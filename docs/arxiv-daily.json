{
    "agent": {
        "2411.18266": "|**2024-11-27**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴无声语音系统在恢复言语障碍患者的沟通方面具有巨大潜力。然而，无缝、连贯的语音仍然难以实现，临床疗效尚未得到证实。在此，我们提出了一种由人工智能驱动的智能喉部（IT）系统，该系统集成了喉部肌肉振动和颈动脉脉搏信号传感器以及大型语言模型（LLM）处理，以实现流畅、富有情感表达的沟通。该系统利用超灵敏的纺织应变传感器从颈部区域捕捉高质量信号，并支持token级别的处理，以实现实时、连续的语音解码，从而实现无缝、无延迟的通信。在测试中，使用五种言语障碍的卒中患者进行测试，IT的LLM智能代理智能地纠正token错误，丰富句子级情感和逻辑连贯性，实现了低错误率（4.2%的词错误率，2.9%的句子错误率）和用户满意度55%的增长。这项工作为言语障碍患者建立了一个便携、直观的通信平台，有望广泛应用于不同的神经学条件和多语言支持系统。|\n",
        "2411.17636": "|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在各种领域，包括机器人操作和导航，展现了出色的规划能力。虽然最近在机器人领域的努力已经利用LLMs进行高级和低级规划，但这些方法通常面临重大挑战，如长期任务中的幻觉以及由于在单次生成计划时缺乏实时反馈而导致的适应性有限。为了解决这些限制，我们提出了一种新的多智能体LLM框架，即用于操作的智能体大型语言模型（MALMM），它将高级规划和低级控制代码生成分配给专门的LLM智能体，并由一个额外的智能体动态管理转换。通过在每一步后纳入环境观察，我们的框架有效地处理了中间失败，并实现了适应性重新规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或在上下文中学习的示例，并且可以推广到各种新的任务。我们在包括长期任务在内的九个RLBench任务上评估了我们的方法，并展示了其在零样本设置下解决机器人操作的能力，从而克服了现有基于LLM的操作方法的局限性。|\n",
        "2411.16031": "|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理建模（ABM）已成为模拟社交网络的重要工具，涵盖了诸如信息传播、影响力动态和社区形成等多种现象。然而，手动配置各种代理交互和信息流动态带来挑战，往往导致模型过于简化，缺乏现实世界的普适性。将现代大型语言模型（LLM）与ABM相结合为解决这些挑战和提升模拟真实度提供了一条有希望的途径，利用LLM在感知、推理和行为方面类似人类的能力。在本文中，我们提出了一种新颖的框架，该框架利用LLM赋能的代理根据用户的兴趣和个性特征模拟社交网络用户。该框架允许定制代理交互，类似于各种社交网络平台，包括内容重新分享和个性化推荐机制。我们使用2020年美国选举的全面Twitter数据集验证了我们的框架，证明LLM代理能够准确复制真实用户的言行，包括语言模式和政治倾向。这些代理形成了同质化的意识形态集群，并保留了其社区的主要主题。值得注意的是，基于偏好的推荐对代理行为有显著影响，促进了更高的参与度、网络同质性以及回音室的形成。总体而言，我们的发现突出了LLM代理在推进社交媒体模拟和揭示复杂在线动态中的潜力。|\n",
        "2411.15891": "|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大型语言模型（LLMs）和强化学习（RL）是构建自主智能体的两种强大方法。然而，由于对游戏环境的理解有限，智能体往往依赖低效的探索和试错，难以发展长期策略或做出决策。我们提出了一种方法，通过从交互记录中提取经验来模拟游戏环境的潜在规律，并利用这些经验作为内部动机来引导智能体。这些经验以语言形式表达，非常灵活，既可以直接协助智能体进行推理，也可以转化为训练中的奖励。在Crafter上的评估结果显示，RL和LLM智能体都从这些经验中受益，从而提高了整体性能。|\n",
        "2411.16723": "|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着自然语言生成模型（称为大型语言模型，LLMs）的近期发展，一种潜在的应用场景得以开启，即改进人类与机器人助手互动的方式。这些LLMs应能利用其广泛的理解能力，将自然语言命令解释为有效、符合任务和安全的机器人任务执行。然而，在现实中，这些模型存在幻觉问题，可能会引起安全问题或偏离任务。在其他领域，这些问题已通过使用协作人工智能系统得到改善，在该系统中，多个LLM代理可以共同规划、编码和自我检查输出。在本研究中，通过将多个协作人工智能系统与单个独立人工智能代理进行对比测试，以确定在其他领域的成功是否能够转化为改进的人机交互性能。结果显示，代理数量与模型的成功率之间没有明确的趋势。然而，很明显，某些协作人工智能代理架构可以显著提高生成无错误代码和解决抽象问题的能力。|\n",
        "2411.15396": "|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|人工智能在进行自动化信息判断任务时是否会产生认知偏差？尽管近期在衡量和缓解人工智能和大型语言模型（LLMs）中的社会和算法偏差方面取得了进展，但LLMs在多大程度上表现出“理性”行为，或者它们是否也容易受到人类认知偏差的触发，仍不明确。为了解决这个未解问题，我们的研究包括一个众包用户实验和一个LLM驱动的模拟实验，比较了在信息检索（IR）环境中，LLMs和人类评判员在潜在诱饵效应下的可信度评估，并实证研究了与传统的基于人类评估的基线相比，LLMs在COVID-19医疗（误）信息评估任务中的认知偏差程度。来自跨主体用户实验和LLM驱动的重复实验的结果表明：1）更大、更新版的LLMs在区分可信信息和虚假信息方面表现出更高的一致性和准确性。然而，由于存在更显著、更具诱饵性质的虚假信息结果，它们更有可能给予虚假信息更高的评分；2）虽然诱饵效应在人类和LLMs的评估中都发生了，但在LLMs的判断中，这种效应在不同条件和主题下比人类的可信度评分更为普遍。与普遍认为的AI工具的“理性”相反，我们的研究从实证上确认了LLM代理中嵌入的认知偏差风险，评估了诱饵对LLMs与人类可信度评估的影响，从而突出了去偏差AI代理、开发心理学导向的AI审计技术和政策（用于自动化判断任务及更多领域）的复杂性和重要性。|\n",
        "2411.15100": "|**2024-11-22**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|LLM代理的应用正变得越来越复杂和多样化，导致对可以解析为代码、结构化函数调用和具身代理命令的规范化输出的需求日益增长。这些发展对LLM推理中的规范化生成提出了重大需求。无上下文文法是一种灵活的方法，通过限制解码来实现规范化生成。然而，执行无上下文文法需要在运行时遍历词汇表中所有标记的多个栈状态，给规范化生成带来不可忽视的开销。在本文中，我们提出了XGrammar，这是一个灵活且高效的LLM结构生成引擎。XGrammar通过将词汇表划分为可预检查的上下文无关标记和需要运行时解释的上下文相关标记来加速无上下文文法的执行。我们进一步构建了转换来扩展语法上下文并减少上下文无关标记的数量。此外，我们构建了一个高效的持久栈来加速上下文相关标记的检查。最后，我们与LLM推理引擎协同设计语法引擎，以重叠语法计算与GPU执行。评估结果显示，XGrammar可以比现有解决方案实现高达100倍的加速。结合LLM推理引擎，它可以在端到端低LLM服务中实现近乎零开销的结构化生成。|\n",
        "2411.15004": "|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|**[link](https://github.com/colonylabs/ScribeAgent)**|大型语言模型（LLM）代理正在迅速提升以处理越来越复杂的基于网络的任务。这些代理中的大多数依赖于通用、专有的模型如GPT-4，并专注于设计更好的提示以提升它们的规划能力。然而，通用LLM并未专门训练以理解专门的网络上下文，如HTML，并且它们通常在长期规划方面遇到困难。我们探索了一种替代方法，即使用从超过250个域名收集的生产规模工作流程数据对开源LLM进行微调，这些域名对应60亿个标记。这种方法简单而有效，在现有基准测试中相对于基于提示的代理显示了显著的优势——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上比之前最佳的文字型网络代理提高了14.1%的任务成功率。我们还对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练配方、上下文窗口优化以及数据集大小影响等方面的见解。|\n",
        "2411.14214": "|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于LLM的自主代理在解决复杂工业任务方面表现出卓越的性能。然而，在追求碳中和和高性能可再生能源系统的过程中，现有的AI辅助设计自动化在可解释性、可扩展性和可用性方面面临着重大限制。为了解决这些挑战，我们提出了LP-COMDA，这是一个基于LLM的、物理信息丰富的自主代理，能够在最小人工监督下自动化电力电子系统中电力转换器的调制设计。与传统的AI辅助方法不同，LP-COMDA包含一个基于LLM的规划器，通过用户友好的聊天界面收集和验证设计规范。规划器随后与物理信息设计优化工具协调，自主迭代生成和优化调制设计。通过聊天界面，LP-COMDA提供可解释的设计过程，展示解释和图表。实验表明，LP-COMDA优于所有基线方法，在标准均方绝对误差方面，与第二好的基准方法相比，误差降低了63.2%。此外，对20位专家的实证研究表明，使用LP-COMDA的设计时间是传统方法的33倍以上，显示出其在设计效率方面的显著改进。|\n",
        "2411.14033": "|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在（多模态）大型语言模型时代，大多数操作流程都可以通过LLM智能体进行重构和再现。LLM智能体能够感知、控制和从环境中获取反馈，以自主方式完成给定任务。除了环境交互特性外，LLM智能体还可以调用各种外部工具以简化任务完成过程。这些工具可以被视为包含私有或实时知识且不存在于LLM参数中的预定义操作流程。作为发展的自然趋势，调用工具的智能体正成为自主智能体，因此完整的智能系统最终变成了多LLM智能体系统（MLAS）。本文讨论了MLAS的技术和商业格局。与之前的单一LLM智能体系统相比，MLAS具有以下优势：i) 更高的任务解决性能潜力；ii) 更高的系统变化灵活性；iii) 为每个参与实体保留专有数据；iv) 为每个实体实现货币化的可行性。为了支持MLAS生态系统，我们提供了一个考虑技术要求、数据隐私和商业激励的MLAS协议的初步版本。因此，MLAS将成为实现未来人工集体智慧的实用解决方案。|\n",
        "2411.19043": "|**2024-11-28**|**Using a Feedback Loop for LLM-based Infrastructure as Code Generation**|Mayur Amarnath Palavalli et.al.|[2411.19043](http://arxiv.org/abs/2411.19043)|**[link](https://github.com/Mayur-Palavalli/LLM-IaC-generation)**|**使用大型语言模型（LLMs）进行代码生成有助于提高软件开发者在编码任务中的生产力，但尚未对围绕这些代码的软件开发者的任务产生重大影响。特别是，基础设施管理的挑战仍然是一个未解之谜。我们研究了LLM代理利用基础设施即代码（IaC）范式构建基础设施的能力。我们特别研究了使用反馈循环，该循环会返回生成的IaC的错误和警告，以允许LLM代理改进代码。我们发现，对于循环的每一次迭代，其有效性都会呈指数下降，直到达到某个点并趋于平稳，最终变得无效。**|\n",
        "2411.18915": "|**2024-11-28**|**MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications**|Vishnou Vinayagame et.al.|[2411.18915](http://arxiv.org/abs/2411.18915)|null|随着工具增强的语言代理的数学推理能力不断增强，但现有方法通常依赖于闭源或大型模型、外部数据或大量的提示工程。这项工作介绍了一种名为MATATA的新颖且经济高效的方法，通过推理、规划和工具使用来训练LLM代理解决表格数据问题。它采用渐进式自我改进范式和迭代式弱监督，赋予了38亿/80亿小语言模型（SLM）的能力，特别适合于本地托管和敏感的商业环境，在这些环境中数据隐私至关重要。通过在不同数据集上使用灵活且可重用的工具，它实现了在共享任务上的有效可扩展性。实验表明，MATATA在基于开源模型的推理框架中，在FinQA和TAT-QA上达到了最先进的性能。此外，MATATA模型在TabMWP上与基于GPT-4的框架竞争，同时仍然是SLM。|\n",
        "2412.01778": "|**2024-12-02**|**HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing**|Lajos Muzsai et.al.|[2412.01778](http://arxiv.org/abs/2412.01778)|**[link](https://github.com/aielte-research/HackSynth)**|**我们介绍了HackSynth，这是一种基于大型语言模型（LLM）的全新自主渗透测试代理。HackSynth的双模块架构包括一个规划器和总结器，这使得它能够迭代地生成命令和处理反馈。为了对HackSynth进行基准测试，我们提出了两个基于Capture The Flag（CTF）的新基准集，利用流行的平台PicoCTF和OverTheWire。这些基准集涵盖了200个不同领域和难度的挑战，为评估基于LLM的渗透测试代理提供了一个标准化的框架。基于这些基准，我们展示了广泛的实验，分析了HackSynth的核心参数，包括创新性（温度和top-p）和标记利用。我们使用了多个开源和专有LLM来衡量代理的能力。实验表明，该代理在GPT-4o模型下表现最佳，优于GPT-4o的系统卡片所建议的。我们还讨论了HackSynth行动的安全性和可预测性。我们的发现表明，基于LLM的代理在推进自主渗透测试方面的潜力，以及稳健保障的重要性。HackSynth和基准集公开可用，以促进自主网络安全解决方案的研究。**|\n",
        "2412.01605": "|**2024-12-02**|**Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking**|Jie Liu et.al.|[2412.01605](http://arxiv.org/abs/2412.01605)|null|临床决策（CDM）是医疗保健服务中复杂且动态的过程，但对于人工智能系统来说仍然是一个重大挑战。尽管基于大型语言模型（LLM）的智能体已在执照考试和知识问答任务中测试了一般医学知识，但由于缺乏反映实际医疗实践的全面测试数据集，它们在现实场景中的CDM表现有限。为了解决这一差距，我们提出了MedChain，一个包含12,163个临床案例的数据集，涵盖了临床工作流程的五个关键阶段。MedChain与现有基准相比，具有三个反映现实临床实践的显著特点：个性化、交互性和顺序性。此外，为了应对现实世界中的CDM挑战，我们还提出了MedChain-Agent，这是一个集成了反馈机制和MCase-RAG模块的人工智能系统，以便从以往案例中学习并调整其响应。MedChain-Agent在动态收集信息和处理顺序临床任务方面表现出惊人的适应性，显著优于现有方法。在本文被接受后，将发布相关数据集和代码。|\n",
        "2412.01333": "|**2024-12-02**|**Can Large Language Models Serve as Evaluators for Code Summarization?**|Yang Wu et.al.|[2412.01333](http://arxiv.org/abs/2412.01333)|**[link](https://github.com/CGCL-codes/naturalcc)**|**代码摘要通过将代码片段转换为自然语言描述，有助于程序理解和软件维护。多年来，为这项任务开发了众多方法，但一个关键挑战仍然存在：有效地评估生成摘要的质量。虽然人工评估在评估代码摘要质量方面是有效的，但它劳动密集且难以扩展。常用的自动指标，如BLEU、ROUGE-L、METEOR和BertScore，通常与人工判断的关联性不强。在本文中，我们探讨了大型语言模型（LLMs）在评估代码摘要方面的潜力。我们提出了CODERPE（代码摘要评估中的角色扮演者），这是一种利用角色扮演提示来评估生成摘要质量的新方法。具体来说，我们提示LLM代理扮演各种角色，如代码审阅者、代码作者、代码编辑和系统分析师。每个角色从关键维度评估代码摘要的质量，包括连贯性、一致性、流畅性和相关性。我们进一步通过采用各种提示策略，包括思维链推理、情境学习和定制评分表设计，探索了LLMs作为评估者的鲁棒性。结果表明，LLMs作为代码摘要方法的有效评估者。值得注意的是，我们的基于LLM的评估器CODERPE与人工评估的斯皮尔曼相关系数为81.59%，比现有的BERTScore指标高17.27%。**|\n",
        "2412.01303": "|**2024-12-02**|**RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks**|Xu Yang et.al.|[2412.01303](http://arxiv.org/abs/2412.01303)|null|随着大规模分布式能源资源被集成到主动配电网络（ADN）中，与传统的配电网络相比，在ADN中实现有效的能源管理变得越来越突出。尽管先进的强化学习方法（RL）极大地提高了ADN能源管理的效率，减轻了复杂建模和优化的负担，但安全性成为RL在实际问题应用中的关键关注点。由于与操作安全约束相对应的惩罚函数的设计和调整需要RL和电力系统操作方面的广泛领域知识，新兴的ADN运营商呼吁采用更灵活和定制化的方法来解决惩罚函数，以便进一步提高操作安全和效率。凭借强大的理解、推理和在上下文中学习的能力，大型语言模型（LLM）为辅助ADN能源管理的安全RL提供了一种有希望的途径。在本文中，我们引入LLM来理解ADN中的操作安全要求并生成相应的惩罚函数。此外，我们提出了一种RL2机制，通过多轮对话迭代和自适应地改进生成的函数，其中LLM代理根据下游RL代理的训练和测试性能调整函数的模式和参数。所提出的方法显著减少了ADN运营商的干预。综合测试结果证明了所提出方法的有效性。|\n",
        "2412.01033": "|**2024-12-02**|**SAUP: Situation Awareness Uncertainty Propagation on LLM Agent**|Qiwei Zhao et.al.|[2412.01033](http://arxiv.org/abs/2412.01033)|null|大型语言模型（LLMs）集成到多步骤智能体系统中，能够在各种应用中实现复杂的决策过程。然而，它们的输出通常缺乏可靠性，因此不确定性估计变得至关重要。现有的不确定性估计方法主要关注最终步骤的输出，未能考虑到多步骤决策过程中的累积不确定性和智能体与其环境之间的动态交互。为了解决这些局限性，我们提出了SAUP（情境感知不确定性传播），这是一个新颖的框架，它通过LLM智能体推理过程的每一步传播不确定性。SAUP通过在传播过程中为每一步的不确定性分配情境权重来融入情境感知。我们的方法与各种单步不确定性估计技术兼容，提供了一个全面且准确的不确定性度量。在基准数据集上的大量实验表明，SAUP显著优于现有最先进的方法，实现了AUROC达到20%的提升。|\n",
        "2412.02776": "|**2024-12-03**|**Hacking CTFs with Plain Agents**|Rustem Turtayev et.al.|[2412.02776](http://arxiv.org/abs/2412.02776)|**[link](https://github.com/palisaderesearch/intercode)**|**我们通过简单的LLM代理设计饱和了一所高中水平的黑客基准测试。具体来说，我们通过提示、工具使用和多次尝试，在InterCode-CTF这个流行的进攻性安全基准测试上获得了95%的性能。这一成绩超过了Phuong等人2024年（29%）和Abramovich等人2024年（72%）的研究成果。我们的结果表明，当前的大型语言模型在进攻性网络安全方面已经超越了高中水平。它们的黑客能力尚未得到充分发掘：我们提出的ReAct&Plan提示策略在1-2个回合内解决了许多挑战，无需复杂的工程或高级利用。**|\n",
        "2412.04093": "|**2024-12-05**|**Practical Considerations for Agentic LLM Systems**|Chris Sypherd et.al.|[2412.04093](http://arxiv.org/abs/2412.04093)|null|近年来，随着大型语言模型（LLMs）的强大能力日益增长，对其作为自主代理基础模型的兴趣也随之增加。尽管LLMs在自然语言领域展现出涌现的能力和广泛的专业知识，但它们固有的不可预测性使得LLMs代理的实施变得具有挑战性，导致相关研究与这类系统的实际应用之间存在差距。为了弥合这一差距，本文将研究社区中的可操作见解和考虑因素置于既定应用范式之中，以促进稳健的LLMs代理的构建和明智的部署。具体而言，我们根据应用导向文献中的常见实践，将相关研究发现定位为四个广泛类别——规划、记忆、工具和控制流——并强调了在设计用于实际应用的代理型LLMs时需要考虑的实际因素，例如处理随机性和高效管理资源。虽然我们没有进行实证评估，但我们为讨论代理型LLMs设计的关键方面提供了必要的背景，无论是在学术界还是在工业界。|\n",
        "2412.04090": "|**2024-12-05**|**LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents**|Bingchen Li et.al.|[2412.04090](http://arxiv.org/abs/2412.04090)|null|我们提出了首个名为LossAgent的损失代理，用于低级图像处理任务，例如图像超分辨率和修复，旨在实现不同实际应用中低级图像处理的各种定制优化目标。值得注意的是，并非所有优化目标，如复杂的定制感知度量、文本描述和复杂的人类反馈，都能用现有的低级损失，例如均方误差损失（MSE loss），来实例化，这在端到端优化图像处理网络时提出了一个关键挑战。为了解决这个问题，我们的LossAgent引入了强大的大型语言模型（LLM）作为损失代理，丰富的先验知识文本理解赋予损失代理在低级图像处理网络优化过程中的复杂优化目标、轨迹和状态反馈的理解潜力。特别是，我们通过整合支持端到端优化低级图像处理现有损失函数建立了损失库。然后，我们为损失代理设计了面向优化的提示工程，使其在每个优化交互中能够积极和智能地决定库中每个损失的组合权重，从而实现任何定制优化目标所需的优化轨迹。在三个典型低级图像处理任务和多个优化目标上的大量实验表明了我们所提出的LossAgent的有效性和适用性。代码和预训练模型将在https://github.com/lbc12345/LossAgent上提供。|\n",
        "2412.03904": "|**2024-12-05**|**MISR: Measuring Instrumental Self-Reasoning in Frontier Models**|Kai Fronsdal et.al.|[2412.03904](http://arxiv.org/abs/2412.03904)|**[link](https://github.com/kaifronsdal/self-reasoning-evals)**|**我们提出了一套任务，用于评估大型语言模型（LLM）代理的工具体验推理能力。工具体验推理能力可以提高适应性和实现自我修改，但也可能带来重大风险，例如导致欺骗性对齐。先前的研究只评估了非代理环境或有限领域的自我推理。在本文中，我们提出了在广泛场景下，包括自我修改、知识寻求和模糊自我推理的代理任务中评估工具体验推理能力的方案。我们评估了使用最先进LLM构建的代理，包括商业和开源系统。我们发现，工具体验推理能力仅在最先进的边缘模型中体现，并且高度依赖于上下文。没有模型通过我们评估中最困难版本，因此我们的评估可以用于衡量未来模型工具体验推理能力的提升。我们在https://github.com/kaifronsdal/Self-Reasoning-Evals上开源了我们的评估。**|\n",
        "2412.03847": "|**2024-12-05**|**Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration**|Shiwen Ni et.al.|[2412.03847](http://arxiv.org/abs/2412.03847)|null|智能对话系统在现代教育和心理辅导领域得到越来越广泛的应用，但大多数现有系统局限于单一领域，无法同时处理教育和心理问题，并且在处理复杂问题时往往缺乏准确性和专业性。为了解决这些问题，本文提出了一种结合教育和心理辅导功能的智能对话系统。该系统由多个AI代理组成，包括安全检测代理、意图识别代理、教育LLM代理和心理LLM代理，它们协同工作以确保提供准确的教育知识问答和心理健康支持服务。具体来说，系统通过意图分类模型识别用户输入的意图，并调用增强检索的教育大型模型和心理大型模型（该模型已使用心理数据进行微调），以便提供专业的教育建议和心理健康支持。|\n",
        "2412.05093": "|**2024-12-06**|**Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models**|Da Ju et.al.|[2412.05093](http://arxiv.org/abs/2412.05093)|null|大型语言模型（LLMs）越来越多地被提议作为经典基于代理模型（ABMs）的有力替代，以模拟社会动态。通过将LLMs作为人类行为的代理，这种新方法希望通过模拟比经典ABMs更为复杂的动态，并在社会科学、政治科学和经济学等领域获得新的见解。然而，由于LLMs的“黑盒”性质，不清楚LLM代理是否实际上执行了编码在它们自然语言指令中的预期语义，以及由此产生的互动动态是否具有意义。为了研究这个问题，我们提出了一种新的评估框架，将LLM模拟建立在社会科学中已建立的参考模型的动态基础之上。我们将LLMs视为一个黑盒函数，评估它们的输入输出行为相对于这个参考模型，这使我们能够评估它们行为的详细方面。我们的结果表明，虽然可以通过设计提示来近似预期的动态，但这些模拟的质量高度依赖于特定提示的选择。重要的是，模拟甚至对任意的变异，如细微的文字变化和空格使用，都非常敏感。这引发了当前版本LLMs在有意义模拟中的有用性的质疑，因为没有参考模型，无法事先确定看似无意义的提示变化对模拟的影响。|\n",
        "2412.06724": "|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|**我们研究了大型语言模型（LLMs）在自动生成数据清洗工作流程中的推理能力。为了评估LLMs完成数据清洗任务的能力，我们实现了一个基于LLM的自动数据清洗工作流程（AutoDCWorkflow）的管道，通过提示LLM进行数据清洗操作来修复三种类型的数据质量问题：重复数据、缺失值和不一致的数据格式。给定一个脏表和一个目的（以查询的形式表达），这个管道生成一个最小的、干净的表，足以解决目的并使用生成表的数据清洗工作流程。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的目标列集合。（2）检查列质量：评估每个目标列的数据质量并生成数据质量报告作为操作目标。（3）生成操作和参数：根据数据质量报告结果预测下一个操作和参数。此外，我们提出一个数据清洗基准来评估LLM代理自动生成解决不同难度级别数据清洗目的工作流程的能力。基准包括注解数据集，作为一个包含目的、原始表、干净表、数据清洗工作流程和答案集的集合。在我们的实验中，我们评估了三个自动生成目的驱动数据清洗工作流程的LLMs。结果表明，LLMs在规划和生成数据清洗工作流程方面表现良好，无需微调。**|\n",
        "2412.06681": "|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通运输系统需求建模与仿真领域，基于代理的模型和微观仿真是目前最先进的方法。然而，现有的基于代理的模型在行为真实性和资源需求方面仍存在一些局限性，这限制了它们的适用性。在本研究中，我们利用新兴的大语言模型（LLMs）和基于LLM的代理技术，提出了一种适用于交通运输系统的通用LLM-代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，而且为克服现有基于代理的模型的某些局限性提供了有希望的解决方案。我们的概念框架设计紧密模拟了人类旅行者在交通运输网络中的决策、交互过程和特征，并通过相关研究和LLM代理在瓶颈设置中的学习和调整的示范实例，证明了所提出的系统可以满足决策和学习的关键行为标准。尽管进一步优化LLM-代理建模框架是必要的，但我们相信这种方法有潜力改进交通运输系统的建模与仿真。|\n",
        "2412.06294": "|**2024-12-09**|**Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects**|Louis Milliken et.al.|[2412.06294](http://arxiv.org/abs/2412.06294)|**[link](https://github.com/coinse/installamatic)**|最近，许多研究提出了使用大型语言模型（LLM）构建的智能体来执行所谓的“仓库级”任务，这些任务的范围大于单个文件。这引发了一种推测，即这些仓库级任务的协调可能产生能够在很大程度上无需人工干预的软件工程智能体。然而，我们认为在需要由这个自主软件工程智能体执行的众多任务中，有一项重要任务缺失，那就是通过安装其他仓库来满足项目级别的依赖关系。为了调查这种仓库级安装任务的可行性，我们引入了一个基准，该基准由来自40个开源Python项目的仓库安装任务组成，包括每个目标仓库的基准安装过程。此外，我们提出了Installamatic智能体，该智能体的目标是通过在仓库文档中搜索相关说明来执行和验证给定仓库的安装。实证实验表明，55%的研究仓库至少有十分之一的时间可以通过我们的智能体自动安装。通过进一步分析，我们确定了导致我们的智能体无法安装仓库的常见原因，讨论了设计此类智能体时面临的挑战，并考虑了此类智能体可能对开发者产生的影响。|\n",
        "2412.05850": "|**2024-12-08**|**Cooperative SQL Generation for Segmented Databases By Using Multi-functional LLM Agents**|Zhiguang Wu et.al.|[2412.05850](http://arxiv.org/abs/2412.05850)|null|文本到SQL任务旨在根据用户的文本问题自动生成SQL查询。为了解决这个问题，我们提出了一种基于多功能代理（CSMA）的协同SQL生成框架，通过具有各自数据库模式部分的大语言模型（LLM）代理之间的信息交互来实现。受到人类团队合作协作的启发，CSMA包括三个阶段：1）与问题相关的模式收集，2）问题对应的SQL查询生成，3）SQL查询正确性检查。在第一阶段，代理分析各自的模式并相互通信，收集与问题相关的模式信息。在第二阶段，代理尝试使用收集到的信息为问题生成相应的SQL查询。在第三阶段，代理根据他们已知的信息检查SQL查询是否正确创建。这种基于交互的方法使得每个代理的问题相关部分数据库模式可用于SQL生成和检查。在Spider和Bird基准测试上的实验表明，CSMA达到了与现有技术水平相当的高性能，同时保持了这些个体代理中的私有数据。|\n",
        "2412.07646": "|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过重复的语言学习和使用而演变为结构化。这些过程在语言习得期间引入了偏见，并使语言系统趋向于沟通效率。在这篇论文中，我们研究了如果人工语言是为大型语言模型（LLMs）的隐式偏见进行优化时，是否会发生相同的情况。为此，我们模拟了一个经典指称游戏，其中LLMs学习和使用人工语言。我们的结果表明，最初无结构的整体语言确实被塑造出一些结构属性，使两个LLM代理能够成功沟通。与人类实验中的观察结果相似，代际传播增加了语言的易学性，但同时也可能导致非人类退化词汇。总的来说，这项工作扩展了实验发现，表明LLMs可以用作语言演化的模拟工具，并为该领域的未来人机实验开辟了可能性。|\n",
        "2412.06828": "|**2024-12-06**|**Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System**|Fang Zeng et.al.|[2412.06828](http://arxiv.org/abs/2412.06828)|null|本研究介绍了“RadCouncil”，这是一个多智能体大型语言模型（LLM）框架，旨在增强放射学报告中发现部分的印象生成。RadCouncil由三个专业代理组成：1）一个“检索”代理，用于从向量数据库中识别和检索相似报告；2）一个“放射科医生”代理，根据给定报告中的发现部分以及检索代理检索到的示例报告生成印象；3）一个“审稿人”代理，评估生成的印象并提供反馈。RadCouncil的性能使用定量指标（BLEU、ROUGE、BERTScore）和由GPT-4评估的定性标准进行了评估，以胸部X光片作为案例研究。实验结果表明，在多个维度上，包括诊断准确性、风格一致性以及清晰度，RadCouncil相对于单代理方法都有所改进。这项研究强调了利用多个相互作用的LLM代理（每个代理都承担专用任务）来提高专业医疗任务性能和开发更稳健、适应性更强的医疗AI解决方案的潜力。|\n",
        "2412.08445": "|**2024-12-11**|**TapeAgents: a Holistic Framework for Agent Development and Optimization**|Dzmitry Bahdanau et.al.|[2412.08445](http://arxiv.org/abs/2412.08445)|null|我们提出了TapeAgents，这是一个围绕细粒度、结构化代理会话日志带构建的代理框架，同时该日志带也充当会话的可恢复状态。在TapeAgents中，我们利用日志带来促进LLM代理开发生命周期的各个阶段。代理通过处理日志带和LLM输出来进行推理，生成新的思考和行动步骤，并将它们附加到日志带上。环境随后通过将观察步骤同样附加到日志带上来对代理的动作做出反应。凭借这种以日志带为中心的设计，TapeAgents可以为AI从业者提供全面端到端的支持。在开发阶段，日志带有助于会话持久化、代理审计和逐步调试。部署后，可以重用日志带进行评估、微调和提示调整；关键的是，可以从其他代理或使用修订的历史日志带进行适配。在本报告中，我们详细解释了TapeAgents的设计。我们通过构建单体代理和多代理团队、优化代理提示和微调代理的LLM等几个具体例子，展示了TapeAgents的可能应用。我们展示了工具原型，并报告了一个案例研究，其中我们使用TapeAgents微调Llama-3.1-8B表单填写助手，使其性能与GPT-4o相当，同时成本低得多。最后，我们的比较分析表明，TapeAgents相较于先前框架的优势源于我们对LLM代理作为可恢复、模块化状态机的创新设计，该设计具有结构化配置，可以生成细粒度、结构化的日志，并将这些日志转换为训练文本——这是以前工作中所缺少的独特功能组合。|\n",
        "2412.08054": "|**2024-12-11**|**Federated In-Context LLM Agent Learning**|Panlong Wu et.al.|[2412.08054](http://arxiv.org/abs/2412.08054)|null|大型语言模型（LLMs）通过实现逻辑推理、工具使用以及作为代理与外部系统交互，彻底改变了智能服务。LLMs的进步常常受到高质量数据稀缺性的阻碍，其中大部分数据本质上具有敏感性。联邦学习（FL）通过促进分布式LLMs的协作训练同时保护私有数据，提供了一个潜在的解决方案。然而，FL框架面临着显著的带宽和计算需求，以及来自异构数据分布的挑战。LLMs新兴的上下文学习能力提供了一种有希望的方法，通过聚合自然语言而不是庞大的模型参数。然而，这种方法存在隐私泄露的风险，因为它需要在聚合过程中收集和展示来自不同客户端的数据样本。在本文中，我们提出了一种新颖的隐私保护联邦上下文LLM代理学习（FICAL）算法，据我们所知，这是第一个利用上下文学习的能力通过FL来训练多样化的LLM代理的工作。在我们的设计中，由新颖的LLM增强的知识摘要生成（KCG）模块生成的知识库在客户端和服务器之间传输，而不是像之前的FL方法中那样传输模型参数。除此之外，我们还设计了一个基于检索增强生成（RAG）的工具学习和利用（TLU）模块，并将聚合的全局知识库作为教师来教授LLM代理工具的使用。我们进行了广泛的实验，结果表明，与现有的SOTA基线相比，FICAL具有竞争力的性能，并且通信成本降低了$\\mathbf{3.33\\times10^5}$倍。|\n",
        "2412.08014": "|**2024-12-11**|**MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents**|Yun Xing et.al.|[2412.08014](http://arxiv.org/abs/2412.08014)|null|在驾驶场景中的物理对抗攻击可以揭示视觉感知模型的关键漏洞。然而，由于现实世界的多样性背景和保持视觉自然性的要求，开发此类攻击仍然具有挑战性。基于这一挑战，我们将物理对抗攻击重新定义为一次性的补丁生成问题。我们的方法通过一个考虑特定场景上下文的深度生成模型生成对抗补丁，使得可以直接在匹配环境中进行物理部署。主要挑战在于同时实现两个目标：生成能够有效误导目标检测系统的对抗补丁，并确定场景中的上下文适当的放置位置。我们提出了MAGIC（在上下文中掌握物理对抗生成），一个由多模态LLM代理驱动的创新框架，以应对这些挑战。MAGIC能够自动理解场景上下文，并通过语言和视觉能力的协同交互来协调对抗补丁的生成。MAGIC协调了三个专业的LLM代理：对抗补丁生成代理（GAgent）通过为文本到图像模型进行战略性的提示工程来掌握创建欺骗性补丁；对抗补丁部署代理（DAgent）通过基于场景理解确定最优放置策略来确保上下文一致性；自我审查代理（EAgent）通过提供对两个过程的批判性监督和迭代改进来完成这一三部曲。我们在数字和物理层面（即nuImage和手动捕获的真实场景）验证了我们的方法，统计和视觉结果均证明我们的MAGIC在攻击广泛使用的目标检测系统方面既强大又有效。|\n",
        "2412.07822": "|**2024-12-10**|**MAGE: A Multi-Agent Engine for Automated RTL Code Generation**|Yujie Zhao et.al.|[2412.07822](http://arxiv.org/abs/2412.07822)|**[link](https://github.com/stable-lab/MAGE-A-Multi-Agent-Engine-for-Automated-RTL-Code-Generation)**|**随着大型语言模型（LLMs）的发展，通过自然语言指令自动生成RTL代码（例如Verilog）已成为一个有前景的方向。然而，生成既符合语法又符合功能的RTL代码仍然是一个重大挑战。现有的单LLM代理方法面临重大局限性，因为它们必须在各种编程语言之间导航，并处理复杂的生成、验证和修改任务。为了解决这些挑战，本文介绍了MAGE，这是第一个开源的多代理人工智能系统，旨在实现鲁棒和精确的Verilog RTL代码生成。我们提出了一种新颖的高温RTL候选样本采样和调试系统，它有效地探索了代码候选空间，并显著提高了候选代码的质量。此外，我们设计了一种新颖的Verilog状态检查点检查机制，能够早期发现功能错误，并为有针对性的修复提供精确的反馈，显著提高了生成的RTL代码的功能正确性。MAGE在VerilogEval-Human 2基准测试中实现了95.7%的语法和功能正确性代码生成率，超过了最先进的Claude-3.5-sonnet，提高了23.3%，展示了人工智能驱动RTL设计工作流程的鲁棒和可靠方法。**|\n",
        "2412.08685": "|**2024-12-11**|**ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street Scenes**|Yuxi Wei et.al.|[2412.08685](http://arxiv.org/abs/2412.08685)|null|根据具体指令生成具有真实性和交互性的交通参与者动态对于街景模拟至关重要。然而，目前尚缺乏一种能够生成包括车辆和行人等不同类型参与者及其之间不同种类交互的全面方法。在本文中，我们介绍了ChatDyn，这是第一个能够根据语言指令在街景中生成交互、可控和真实参与者动态的系统。为了通过复杂语言实现精确控制，ChatDyn采用多LLM-agent角色扮演方法，利用自然语言输入来规划不同交通参与者的轨迹和行为。为了基于规划生成真实的细粒度动态，ChatDyn设计了两个新颖的执行器：PedExecutor，一个统一的多元任务执行器，能够在不同的任务规划下生成真实的行人动态；以及VehExecutor，一个基于物理过渡策略的执行器，生成符合物理学的车辆动态。大量的实验表明，ChatDyn可以生成包含多车辆和行人的真实驾驶场景动态，并且在子任务上显著优于之前的方法。代码和模型将在https://vfishc.github.io/chatdyn上提供。|\n",
        "2412.10270": "|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有普遍能力的AI代理提供了令人信服的基础。这些代理可能很快将在现实世界中大规模部署，代表个别人类（例如AI助手）或人类群体（例如AI加速的公司）的利益。目前，关于多个LLM代理在多代迭代部署中相互作用的动态知之甚少。在本文中，我们考察了在存在背叛动机的情况下，“LLM代理社会”是否能够学习相互有益的社会规范，这是人类社会性的一个独特特征，对于文明的成功可能至关重要。具体而言，我们研究了LLM代理在玩经典迭代捐赠游戏中的间接互惠的演变，在这个游戏中，代理可以观察到其同伴的最近行为。我们发现，合作的演变在不同基础模型之间存在显著差异，Claude 3.5 Sonnet代理的社会平均得分显著高于Gemini 1.5 Flash，而Gemini 1.5 Flash又优于GPT-4o。此外，Claude 3.5 Sonnet可以利用额外的成本惩罚机制来获得更高的分数，而Gemini 1.5 Flash和GPT-4o则不能。对于每个模型类别，我们还在随机种子之间观察到涌现行为的变异，这表明对初始条件的一种未被充分研究的敏感依赖。我们建议，我们的评估机制可以激发一种低成本且信息丰富的LLM基准测试新类别，重点关注LLM代理部署对社会合作基础设施的影响。|\n",
        "2412.10138": "|**2024-12-13**|**ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL**|Yang Qin et.al.|[2412.10138](http://arxiv.org/abs/2412.10138)|**[link](https://github.com/alibaba/route)**|**尽管大型语言模型（LLMs）在文本到SQL（Text2SQL）方面取得了显著进展，但最新的最先进技术仍然被困在封闭源LLMs（如GPT-4）的上下文学习中，这限制了它们在开放场景中的应用。为了应对这一挑战，我们提出了一种新的鲁棒多任务调整和协作方法（ROUTE），以提高开源LLMs在Text2SQL方面的综合能力，从而提供一个更实用的解决方案。我们的方法从使用与SQL生成相关的各种合成训练数据的任务监督微调（SFT）开始。与现有的基于SFT的Text2SQL方法不同，我们引入了几个额外的SFT任务，包括模式链接、噪声纠正和续写。参与各种SQL生成任务增强了模型对SQL语法的理解，并提高了其生成高质量SQL查询的能力。此外，受LLM代理协作模式启发，我们引入了多任务协作提示（MCP）策略。该策略利用跨多个SQL相关任务的协作来减少SQL生成过程中的幻觉，从而最大限度地发挥通过显式多任务能力增强Text2SQL性能的潜力。我们在八个开源LLMs和五个广泛使用的基准上进行了广泛的实验和深入分析。结果表明，我们的提议优于最新的Text2SQL方法，并取得了领先的性能。**|\n",
        "2412.10133": "|**2024-12-13**|**You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects**|Islem Bouzenia et.al.|[2412.10133](http://arxiv.org/abs/2412.10133)|null|在许多场景中，执行项目的测试套件至关重要，例如，评估代码质量和覆盖率，验证开发人员或自动化工具所做的代码更改，以及确保与依赖项的兼容性。尽管其重要性不言而喻，但在实际中执行项目的测试套件可能会面临挑战，因为不同的项目使用不同的编程语言、软件生态系统、构建系统和测试框架等工具。这些挑战使得创建一个在不同项目间都能可靠运行的全能测试执行方法变得困难。本文介绍了一种名为ExecutionAgent的自动化技术，该技术可以安装任意项目，配置它们运行测试用例，并生成特定于项目的脚本以重现设置。受人类开发者处理此类任务的方式启发，我们的方法是一个基于大型语言模型的代理，它可以自主执行命令并与宿主系统交互。该代理使用元提示来收集有关给定项目最新技术的指导，并根据前一步的反馈迭代优化其过程。我们的评估将ExecutionAgent应用于50个开源项目，这些项目使用了14种不同的编程语言以及许多不同的构建和测试工具。该方法成功执行了33/55个项目的测试套件，与基准测试套件执行结果匹配度仅为7.5%的偏差。这些结果比之前最好的技术提高了6.6倍。该方法带来的成本是合理的，平均每个项目的执行时间为74分钟，大型语言模型成本为0.16美元。我们期望ExecutionAgent成为开发者、自动化编程工具和研究人员执行各种项目测试的有价值工具。|\n",
        "2412.11373": "|**2024-12-16**|**Codenames as a Benchmark for Large Language Models**|Matthew Stephenson et.al.|[2412.11373](http://arxiv.org/abs/2412.11373)|null|在本文中，我们提出将流行的基于单词的桌游Codenames用作评估大型语言模型（LLMs）推理能力的合适基准。Codenames为成功实现AI性能提出了一个极具挑战性的问题，需要复杂的语言理解、心智理论和认知推理能力。先前开发Codenames代理的尝试大多依赖于词嵌入技术，这些技术词汇范围有限，在与不同方法结合时表现不佳。LLMs在基于语言的任务中展现出增强的推理和理解能力，但在横向思维挑战中仍可能表现不佳。我们评估了包括GPT-4o、Gemini 1.5、Claude 3.5 Sonnet和Llama 3.1在内的几种最先进LLMs在各种棋盘布局下的能力。我们的结果表明，尽管某些LLMs在总体上表现优于其他LLMs，但不同的模型在游戏过程中表现出不同的涌现行为，并在特定角色上表现出色。我们还评估了不同LLMs组合在协同游戏中的表现，证明了LLM代理比先前技术更易于推广到更广泛的队友群体中。|\n",
        "2412.13178": "|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身代理在执行复杂指令方面具有强大的能力，为具身机器人的潜在部署开辟了道路。然而，一个可预见的问题是，这些具身代理也可以完美地执行一些危险任务，可能在实际世界中造成损害。为了研究这个问题，我们提出了SafeAgentBench——一个用于具身LLM代理安全任务规划的新的基准。SafeAgentBench包括：（1）一个包含750个任务的新数据集，涵盖了10种潜在危险和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持多代理执行，并为8个最先进的基线提供了17个高级动作；（3）从执行和语义两个角度的可靠评估方法。实验结果表明，表现最佳的基线在安全任务中达到了69%的成功率，但在危险任务中只有5%的拒绝率，这表明存在显著的安全风险。更多细节和代码可在https://github.com/shengyin1224/SafeAgentBench找到。**|\n",
        "2412.14161": "|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们每天都在与计算机互动，无论是在日常生活中还是工作中，许多工作都可以通过访问计算机和互联网来完成。同时，得益于大型语言模型（LLMs）的改进，与周围环境互动并产生影响的人工智能代理也迅速发展。但是，AI代理在帮助加速甚至自主执行工作相关任务方面的表现如何？这个问题的答案对于希望将AI融入其工作流程的行业以及了解AI采用对劳动力市场可能产生的影响的经济政策具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进展，本文介绍了TheAgentCompany，这是一个可扩展的基准，用于评估以类似数字工作者方式与世界互动的AI代理：通过浏览网页、编写代码、运行程序以及与同事沟通。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司的环境，并创建了一系列可能由该公司员工执行的任务。我们测试了由基于封闭API和开放权重的语言模型（LM）驱动的基线代理，发现最具有竞争力的代理可以使24%的任务实现自主完成。这描绘了一幅关于使用LLM代理进行任务自动化的复杂图景——在一个模拟真实工作场所的环境中，大部分简单任务可以自主解决，但更困难的长远任务仍然超出现有系统的范围。**|\n",
        "2412.13667": "|**2024-12-18**|**Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery**|ChengAo Shen et.al.|[2412.13667](http://arxiv.org/abs/2412.13667)|null|因果推理是跨越多个领域（如智能健康、药物发现AI和AIOps）决策的基础。传统的统计因果发现方法虽然已经建立，但主要依赖于观察数据，往往忽略了因果关系中固有的语义线索。大型语言模型（LLMs）的出现为利用语义线索进行知识驱动的因果发现提供了一种经济实惠的方法，但LLMs在因果发现领域的发展落后于其他领域，尤其是在多模态数据的探索方面。为了填补这一差距，我们引入了MATMCD，这是一个由工具增强的LLMs驱动的多智能体系统。MATMCD有两个关键智能体：一个数据增强智能体，用于检索和处理模态增强数据；以及一个因果约束智能体，用于集成多模态数据进行知识驱动的推理。内部工作的精心设计确保了智能体的成功合作。我们在七个数据集上的实证研究表明，多模态增强的因果发现具有显著的潜力。|\n",
        "2412.14737": "|**2024-12-19**|**On Verbalized Confidence Scores for LLMs**|Daniel Yang et.al.|[2412.14737](http://arxiv.org/abs/2412.14737)|**[link](https://github.com/danielyxyang/llm-verbalized-uq)**|**随着大型语言模型（LLMs）的兴起及其与日常生活的紧密融合，致力于其可信度变得至关重要。LLMs的不确定性量化可以增强人们对它们回答的信任，同时也使LLMs代理能够根据彼此的不确定性做出更明智的决策。为了估计回答中的不确定性，通常使用内部标记logits、针对特定任务的代理模型或多个响应的采样。这项工作侧重于让LLM自己用置信度分数作为输出标记的一部分来表达其不确定性，这是一种具有低开销、提示和模型无关的不确定性量化的有前景方法。使用广泛的基准，我们评估了这些置信度分数在不同数据集、模型和提示方法方面的可靠性。我们的结果表明，这些分数的可靠性强烈依赖于如何询问模型，但也表明可以通过某些提示方法提取出良好校准的置信度分数。我们认为，用言语表达的置信度分数可以成为未来简单但有效且通用的不确定性量化方法。我们的代码可在https://github.com/danielyxyang/llm-verbalized-uq上找到。**|\n",
        "2412.14470": "|**2024-12-19**|**Agent-SafetyBench: Evaluating the Safety of LLM Agents**|Zhexin Zhang et.al.|[2412.14470](http://arxiv.org/abs/2412.14470)|**[link](https://github.com/thu-coai/agent-safetybench)**|**随着大型语言模型（LLMs）作为代理的广泛应用，它们在交互环境和工具使用中的集成带来了超出模型本身相关的新的安全挑战。然而，缺乏用于评估代理安全性的全面基准，对有效评估和进一步改进构成了重大障碍。在本文中，我们介绍了Agent-SafetyBench，这是一个旨在评估LLM代理安全性的全面基准。Agent-SafetyBench包含349个交互环境和2000个测试案例，评估8类安全风险，涵盖在不安全交互中经常遇到的10种常见故障模式。我们对16个流行的LLM代理进行评估的结果令人担忧：没有一种代理的安全评分超过60%。这突显了LLM代理中的重大安全挑战，并强调了改进的巨大需求。通过定量分析，我们确定了关键故障模式，并总结了当前LLM代理中的两个基本安全缺陷：缺乏鲁棒性和缺乏风险意识。此外，我们的发现表明，仅依赖防御提示是不够解决这些安全问题的，强调了需要更先进和鲁棒的策略。我们将Agent-SafetyBench发布在\\url{https://github.com/thu-coai/Agent-SafetyBench}，以促进代理安全评估和改进的进一步研究和创新。**|\n",
        "2412.14212": "|**2024-12-18**|**Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution**|Ziyi Ni et.al.|[2412.14212](http://arxiv.org/abs/2412.14212)|null|大型语言模型（LLMs）的卓越能力极大地加速了代理的快速崛起和广泛应用。最近的研究表明，生成Python代码将基于LLMs的代理的动作整合到一个统一的行为空间（CodeAct）是开发现实世界LLMs代理的有前景的方法。然而，这种逐步代码生成方法往往缺乏一致性和鲁棒性，导致代理应用不稳定，尤其是在复杂推理和域外任务中。在本文中，我们提出了一种名为“代码树”（ToC）的新方法，通过端到端机制解决复杂问题规划和执行中的挑战。通过整合思维树和CodeAct的关键思想，ToC结合了它们的优势以增强解决方案的探索。在我们的框架中，每个最终的代码执行结果被视为决策树中的一个节点，采用广度优先搜索策略来探索潜在解决方案。最终结果通过基于节点输出的投票机制确定。|\n",
        "2412.15305": "|**2024-12-19**|**Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling**|Ziyi Ni et.al.|[2412.15305](http://arxiv.org/abs/2412.15305)|null|解决复杂推理任务是智能体在现实世界中的关键应用。得益于大型语言模型（LLMs）在代码数据上的预训练，最近的方法如CodeAct成功地使用代码作为LLM智能体的动作，取得了良好的效果。然而，CodeAct通过依赖零散的思维贪婪地生成下一个动作的代码块，导致结果不一致和不稳定。此外，CodeAct缺乏与动作相关的真实标签（GT），使得其在多轮交互中的监督信号和终止条件令人质疑。为了解决这些问题，我们首先介绍了一种简单而有效的端到端代码生成范式，名为CodeProgram，它利用代码的系统逻辑与全局推理保持一致，并实现连贯的问题解决。然后，我们提出了树形代码（ToC），它根据代码的可执行性自我增长CodeProgram节点，并在无GT的情况下实现自监督。在两个数据集上使用十个流行的零样本LLMs的实验结果表明，ToC在不到1/4轮的情况下将准确性提高了近20%，比CodeAct有显著提升。几个LLMs在一轮CodeProgram上的表现甚至优于多轮CodeAct。为了进一步研究有效性和效率之间的权衡，我们测试了不同的ToC树大小和探索机制。我们还强调了ToC端到端数据生成在监督和强化微调中的潜力。|\n",
        "2412.15274": "|**2024-12-17**|**Memory-Augmented Agent Training for Business Document Understanding**|Jiale Liu et.al.|[2412.15274](http://arxiv.org/abs/2412.15274)|null|传统企业处理业务文档时面临重大挑战，尽管这些文档在物流运营中起着至关重要的作用，但像从发票中提取运输参考这样的任务仍主要依赖手工操作。虽然大型语言模型提供了自动化的潜力，但它们直接应用于特定商业领域时往往会产生不尽人意的结果。我们引入了Matrix（通过推理和迭代探索增强记忆的代理训练），这是一个新的范例，它使LLM代理能够通过经验驱动的记忆优化和迭代学习逐步构建领域专业知识。为了验证这种方法，我们与世界最大的物流公司之一合作，创建了一个包含通用商业语言格式发票文档的数据集，重点关注运输参考提取的任务。实验表明，Matrix的表现优于直接提示单个LLM 30.3%，优于普通LLM代理 35.2%。我们进一步分析了优化系统的指标，并观察到代理系统需要的API调用更少，成本更低，并且可以平均分析更长的文档。我们的方法通过在文档处理任务中系统地增强记忆，确立了一种将通用LLM转化为专业商业工具的新方法。|\n",
        "2412.15266": "|**2024-12-17**|**On the Structural Memory of LLM Agents**|Ruihong Zeng et.al.|[2412.15266](http://arxiv.org/abs/2412.15266)|**[link](https://github.com/zengrh3/StructuralMemory)**|记忆在使大型语言模型（LLM）代理能够参与复杂和长期交互，如问答（QA）和对话系统方面起着关键作用。尽管已经提出了各种记忆模块来完成这些任务，但不同记忆结构在任务之间的影响仍然没有得到充分探索。本文研究了记忆结构和记忆检索方法如何影响基于LLM的代理的性能。具体来说，我们评估了四种类型的记忆结构，包括块状结构、知识三元组、原子事实和摘要，以及混合记忆，它结合了这些组件。此外，我们还评估了三种广泛使用的记忆检索方法：单步检索、重排序和迭代检索。在四个任务和六个数据集上进行的广泛实验产生了以下关键见解：（1）不同的记忆结构具有不同的优势，使它们能够针对特定任务进行定制；（2）混合记忆结构在噪声环境中表现出显著的可适应性；（3）迭代检索在各种场景中始终优于其他方法。我们的研究旨在激励对基于LLM的代理的记忆系统设计进行进一步研究。|\n",
        "2412.17686": "|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|**大型语言模型（LLM）的快速发展和部署为人工智能领域带来了新的前沿，其自然语言理解和生成能力达到了前所未有的水平。然而，这些模型在关键应用中的日益整合引发了重大的安全问题，需要对其潜在风险和相关缓解策略进行彻底的审查。本调查全面概述了当前LLM安全领域的现状，涵盖了四大主要类别：价值偏差、对抗攻击的鲁棒性、滥用和自主AI风险。除了对这四个方面的缓解方法和评估资源的全面回顾外，我们还进一步探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提升LLM安全中的作用、提出并遵循的AI公司和研究机构为LLM安全制定的技术路线图，以及旨在LLM安全的AI治理，包括国际合作、政策建议和预期的监管方向。我们的研究发现，对LLM安全采取积极主动、多方面的方法是必要的，强调技术解决方案、伦理考量和稳健的治理框架的整合。本调查旨在为学术界研究人员、行业实践者和政策制定者提供基础资源，提供有关将LLM安全地融入社会的挑战和机遇的见解。最终，它旨在为LLM的安全和有益发展做出贡献，符合利用AI促进社会进步和福祉的总体目标。相关论文的精选列表已在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers上公开提供。**|\n",
        "2412.17259": "|**2024-12-23**|**LegalAgentBench: Evaluating LLM Agents in Legal Domain**|Haitao Li et.al.|[2412.17259](http://arxiv.org/abs/2412.17259)|**[link](https://github.com/cshaitao/legalagentbench)**|**随着LLM代理的智能和自主性不断提高，它们在法律领域的潜在应用越来越明显。然而，现有的通用领域基准无法完全捕捉现实世界司法认知和决策的复杂性和微妙之处。因此，我们提出了LegalAgentBench，这是一个专门设计用于评估LLM代理在中国法律领域的综合基准。LegalAgentBench包括来自现实世界法律场景的17个语料库，并提供37个用于与外部知识交互的工具。我们设计了一个可扩展的任务构建框架，并仔细标注了300个任务。这些任务涵盖了多种类型，包括多跳推理和写作，难度级别各异，有效地反映了现实世界法律场景的复杂性。此外，除了评估最终成功之外，LegalAgentBench还在中间过程中加入了关键词分析，以计算进度率，从而实现更细致的评估。我们评估了八种流行的LLM，突出了现有模型和方法的优势、局限性和潜在的改进领域。LegalAgentBench为LLM在法律领域的实际应用设定了新的基准，其代码和数据可在\\url{https://github.com/CSHaitao/LegalAgentBench}获取。**|\n",
        "2412.17146": "|**2024-12-22**|**LLM Agent for Fire Dynamics Simulations**|Leidong Xu et.al.|[2412.17146](http://arxiv.org/abs/2412.17146)|null|在利用基础模型，如大型语言模型（LLM），来加速复杂科学工作流程方面取得了显著进展。在本工作中，我们引入了FoamPilot，这是一个概念验证LLM代理，旨在提升FireFOAM的使用便捷性。FireFOAM是一个专门用于火灾动力学和火灾抑制模拟的求解器，它是基于OpenFOAM构建的，OpenFOAM是一个流行的开源计算流体动力学（CFD）工具箱。FoamPilot提供了三个核心功能：代码洞察、案例配置和模拟评估。代码洞察是一种利用检索增强生成（RAG）作为替代传统关键词搜索的方法，旨在使开发者和经验丰富的用户能够高效地导航和总结FireFOAM源代码。对于案例配置，代理可以理解用户的自然语言请求，并旨在根据这些请求修改现有的模拟设置，以支持中级用户。FoamPilot的工作执行功能旨在管理高性能计算（HPC）环境中的模拟提交和执行，并为经验较少的用户提供模拟结果的初步分析。每个功能都取得了有希望的结果，尤其是在简单任务方面，并且对于更复杂的任务，识别出了显著的进一步改进的机会。将这些功能集成到一个单一的LLM代理中，旨在加速使用FireFOAM进行复杂模拟（这对于提高火灾安全性至关重要）的工程师和科学家的模拟工作流程。|\n",
        "2412.16682": "|**2024-12-21**|**The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents**|Feiran Jia et.al.|[2412.16682](http://arxiv.org/abs/2412.16682)|null|大型语言模型（LLM）代理正越来越多地被部署为能够通过工具集成执行复杂现实任务的对话助手。这种与外部系统交互和处理各种数据源的能力虽然强大，但也引入了重大的安全漏洞。特别是，间接提示注入攻击构成了严重威胁，其中嵌入在外部数据源中的恶意指令可以操纵代理偏离用户意图。虽然基于规则约束、来源突出显示和身份验证协议的现有防御措施显示出希望，但它们在保持任务功能的同时难以维持强大的安全性。我们提出了一种新颖且独立的视角，将代理安全从防止有害行为重新定义为确保任务一致性，要求每个代理动作都服务于用户目标。基于这一洞察，我们开发了任务盾（Task Shield），一种测试时的防御机制，该机制系统地验证每个指令和工具调用是否有助于实现用户指定的目标。通过在AgentDojo基准测试上的实验，我们证明了任务盾在GPT-4o上降低了攻击成功率（2.07%）的同时，保持了高任务效用（69.79%）。|\n",
        "2412.18428": "|**2024-12-24**|**Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent**|Farhad Nooralahzadeh et.al.|[2412.18428](http://arxiv.org/abs/2412.18428)|**[link](https://github.com/yizhang-unifr/xmode)**|国际企业、组织或医院收集了大量存储在数据库、文本文档、图像和视频中的多模态数据。尽管在多模态数据探索的各个独立领域以及将自然语言问题自动翻译成数据库查询语言的数据库系统方面取得了近期进展，但将数据库系统与其他非结构化模态（如图像）结合使用，并以自然语言进行查询的研究挑战却很少被探索。在本文中，我们提出了XMODE——一个能够以自然语言实现可解释的多模态数据探索的系统。我们的方法基于以下研究贡献：（1）我们的系统受到一个真实世界用例的启发，使用户能够探索多模态信息系统。（2）XMODE利用基于LLM的代理人工智能框架，将自然语言问题分解为子任务，如文本到SQL生成和图像分析。（3）在关系数据和图像上的多模态数据集上的实验结果表明，我们的系统在多模态探索系统中的表现优于现有系统，不仅在准确性上表现出色，而且在查询延迟、API成本、规划效率和解释质量等各个性能指标上也表现出优越性，这得益于对LLM推理能力的更有效利用。|\n",
        "2412.18371": "|**2024-12-24**|**Defining and Detecting the Defects of the Large Language Model-based Autonomous Agents**|Kaiwen Ning et.al.|[2412.18371](http://arxiv.org/abs/2412.18371)|**[link](https://github.com/KevinHeiwa/Agentable)**|**AI代理是能够感知其环境、自主规划和执行任务的系统。近年来，大型语言模型（LLM）的进步为AI代理引入了一种变革性的范式，使它们能够通过提示与外部资源和工具进行交互。在这种代理中，工作流程将开发者编写的代码（负责框架构建和逻辑控制）与LLM生成的自然语言（增强动态决策和交互）整合。然而，在行为和预期结果方面，开发者实现的逻辑与LLM动态生成的内容之间存在差异，可能导致缺陷，如工具调用失败和任务执行错误。这些问题引入了特定的风险，导致基于LLM的AI代理出现各种缺陷，如服务中断。尽管这些问题很重要，但缺乏针对分析基于LLM的AI代理以揭示其代码中缺陷的系统工作。在本文中，我们首次提出了专注于识别和检测LLM代理缺陷的研究。我们收集并分析了StackOverflow上的6,854条相关帖子，定义了8种代理缺陷类型。对于每种类型，我们提供了详细描述和示例。然后，我们设计了一个名为Agentable的静态分析工具来检测这些缺陷。Agentable利用代码属性图和LLM通过高效地识别特定代码模式和解析自然语言描述来分析代理工作流程。为了评估Agentable，我们构建了两个数据集：AgentSet，包含84个真实世界的代理，以及AgentTest，其中包含78个专门设计以包含各种类型缺陷的代理。我们的结果表明，Agentable实现了88.79%的整体准确率和91.03%的召回率。此外，我们的分析揭示了AgentSet中的889个缺陷，突出了这些缺陷的普遍性。**|\n",
        "2412.18174": "|**2024-12-24**|**INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent**|Haohang Li et.al.|[2412.18174](http://arxiv.org/abs/2412.18174)|null|近期的发展突出了基于大型语言模型（LLM）的代理在金融决策中的潜力。尽管取得了这些进展，该领域目前面临两大挑战：（1）缺乏一个适用于各种金融任务的全面LLM代理框架，以及（2）缺乏用于评估代理性能的标准化基准和一致的数据集。为了解决这些问题，我们引入了InvestorBench，这是第一个专门为评估不同金融决策场景中基于LLM的代理而设计的基准。InvestorBench通过提供适用于不同金融产品（包括单一股票、加密货币和交易所交易基金（ETFs）等）的综合任务套件，增强了LLM启用代理的通用性。此外，我们使用十三种不同的LLM作为骨干模型，在多种市场环境和任务中评估了我们代理框架的推理和决策能力。此外，我们收集了多样化的开源、多模态数据集，并开发了适用于金融决策的综合环境套件。这为评估金融代理在各种场景下的性能提供了一个高度可访问的平台。|\n",
        "2412.21154": "|**2024-12-30**|**Aviary: training language agents on challenging scientific tasks**|Siddharth Narayanan et.al.|[2412.21154](http://arxiv.org/abs/2412.21154)|null|解决复杂现实任务需要动作和观察的循环。这在科学领域尤为明显，因为任务需要许多次分析、工具使用和实验循环。语言代理有望自动化科学领域的智力任务，因为它们可以通过自然语言或代码与工具交互。然而，它们的灵活性给软件实现带来了概念和实践上的挑战，因为代理可能包含非标准组件，如内部推理、规划、工具使用，以及温度采样语言模型固有的随机性。在这里，我们介绍了Aviary，一个可扩展的语言代理体育馆。我们将代理形式化为解决基于语言的部分可观察马尔可夫决策过程的策略，我们称之为语言决策过程。然后，我们实现了五个环境，包括三个具有挑战性的科学环境：（1）操作DNA构建体进行分子克隆，（2）通过访问科学文献来回答研究问题，以及（3）工程设计蛋白质稳定性。这些环境因其对多步推理的关注和与当代生物学研究的相关性而被选中。最后，通过在线训练和扩展推理时间计算，我们表明，由开源、非前沿LLM支持的语言代理可以在多个任务上与前沿LLM代理和人类专家相匹配并超过它们，同时推理成本降低高达100倍。|\n",
        "2412.21102": "|**2024-12-30**|**Exploring and Controlling Diversity in LLM-Agent Conversation**|KuanChao Chu et.al.|[2412.21102](http://arxiv.org/abs/2412.21102)|null|多样性是多智能体通信的关键方面。在本文中，我们聚焦于在开放域多智能体对话的背景下控制和探索多样性，尤其是针对世界模拟应用。我们提出了一种名为自适应提示剪枝（Adaptive Prompt Pruning，APP）的新方法，该方法通过一个单一参数λ动态调整话语生成提示的内容以控制多样性。通过广泛的实验，我们展示了APP能够有效控制模型和数据集的输出多样性，剪枝更多信息会导致更多样化的输出。我们全面分析了提示内容与对话多样性之间的关系。我们的发现表明，提示的各个部分的信息通常都会限制输出的多样性，其中记忆块的影响最为显著。APP与温度采样和top-p采样等现有技术兼容，为多样性管理提供了一种多功能工具。为了解决增加多样性带来的权衡，例如与省略信息的矛盾，我们引入了生成后的校正步骤，有效地平衡了多样性增强与输出一致性。此外，我们还考察了提示结构，包括组件顺序和长度，对多样性的影响。本研究解决了多智能体世界模拟中围绕多样性的关键问题，为在基于LLM的多智能体协作中系统性地构建多样性奠定了基础，提高了它们在实际应用中的有效性。|\n",
        "2412.21033": "|**2024-12-30**|**Plancraft: an evaluation dataset for planning with LLM agents**|Gautier Dagan et.al.|[2412.21033](http://arxiv.org/abs/2412.21033)|**[link](https://github.com/gautierdag/plancraft)**|**我们提出了Plancraft，一个用于LLM代理的多模态评估数据集。Plancraft具有基于Minecraft合成GUI的纯文本和多模态界面。我们包括了Minecraft维基百科来评估工具使用和检索增强生成（RAG），以及一个神谕规划器和神谕RAG信息提取器，以消除现代代理架构的不同组件。为了评估决策能力，Plancraft还包括了一组故意不可解的示例，提供了一种现实挑战，要求代理不仅完成任务，还要决定这些任务是否可解。我们在我们的任务上对开源和闭源LLM和策略进行了基准测试，并将它们的性能与手工制作的规划器进行了比较。我们发现LLM和VLM在Plancraft引入的规划问题中遇到了困难，并提出了如何提高它们能力的一些建议。**|\n",
        "2412.20505": "|**2024-12-29**|**Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning**|Hang Ni et.al.|[2412.20505](http://arxiv.org/abs/2412.20505)|null|在城市化的背景下，城市复兴面临重大挑战，需要适应性的方法来应对不断变化的需求。利用大型语言模型（LLM）的进步，我们提出了循环城市规划（CUP），这是一种新的范式，它在一个封闭循环中持续生成、评估和优化城市规划。具体来说，我们的基于多智能体的LLM框架包括三个关键组成部分：（1）规划，其中LLM智能体根据上下文数据生成和优化城市规划；（2）生活，其中智能体模拟居民的行为和互动，模拟城市环境中的生活；以及（3）判断，涉及评估计划的有效性并提供迭代反馈以改进。循环过程使得规划方法动态且具有响应性。在真实世界数据集上的实验证明了我们的框架作为持续和自适应规划过程的有效性。|\n",
        "2412.20297": "|**2024-12-28**|**FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration**|Jia Liu et.al.|[2412.20297](http://arxiv.org/abs/2412.20297)|null|近期，大型语言模型（LLMs）在增强具身代理推理能力方面取得了进展，推动了基于AGI的机器人技术的发展。虽然LLMs已应用于语义推理和任务泛化等任务，但其在开放物理空间探索中的潜力仍待开发。本文介绍了FaGeL（由LLMs赋能的具身智能代理），这是一个集成了智能织物技术的具身代理，实现了无缝、非侵入式的人机交互。FaGeL利用可穿戴和周围传感器的多模态数据自主生成任务，并根据生成的文本中的隐含人类反馈来优化其行为，而无需明确的评分或偏好。我们还引入了一种标记级显著性图来可视化LLM微调，增强了标记级对齐的可解释性。该系统利用双重反馈机制来提高标记级对齐，并解决非侵入式人机交互和认知进化的挑战。我们的贡献包括FaGeL的开发、用于AI对齐的DualCUT算法以及在合作任务中的实验验证，证明了FaGeL通过隐含反馈实现自适应和进化的能力。在未来，我们计划探索FaGeL在动态环境中的可扩展性和与其他AI系统的集成，以开发能够无缝适应多样化人类需求的AGI代理。|\n",
        "2412.20005": "|**2024-12-28**|**OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System**|Yujie Luo et.al.|[2412.20005](http://arxiv.org/abs/2412.20005)|**[link](https://github.com/zjunlp/oneke)**|**我们介绍了OneKE，一个基于Docker的架构引导知识提取系统，该系统能够从Web和原始PDF书籍中提取知识，并支持多个领域（如科学、新闻等）。具体来说，我们设计OneKE采用了多个代理和一个可配置的知识库。不同的代理执行各自的角色，从而支持各种提取场景。可配置的知识库简化了架构配置、错误情况调试和修正，进一步提升了性能。在基准数据集上的实证评估展示了OneKE的有效性，而案例研究进一步阐明了它在多个领域多样化任务中的适应性，突显了其广泛应用的潜力。我们已经开源了代码，代码链接为https://github.com/zjunlp/OneKE，并发布了一个视频，链接为http://oneke.openkg.cn/demo.mp4。**|\n",
        "2501.01205": "|**2025-01-02**|**Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects**|Abdullah Mushtaq et.al.|[2501.01205](http://arxiv.org/abs/2501.01205)|null|多智能体大型语言模型（LLMs）因其能够利用集体智慧解决复杂问题、决策和规划任务的能力而受到广泛关注。这与“群体智慧”的概念相吻合，其中多样化的智能体共同贡献以生成有效解决方案，这使得它在教育环境中特别适用。高年级设计项目，也称为毕业设计或最后一年项目，在工程教育中至关重要，因为它们将理论知识与实际应用相结合，培养批判性思维、团队合作和现实世界问题解决技能。在本文中，我们探讨了多智能体LLMs在支持工程学生进行这些高年级设计项目中的应用，这些项目通常涉及跨学科考虑和冲突目标，例如在解决道德、社会和环境问题的同时优化技术性能。我们提出了一种框架，其中不同的LLM智能体代表不同的专家视角，如问题表述智能体、系统复杂性智能体、社会和伦理智能体或项目经理，从而促进全面的解决问题方法。这种实现利用了标准的多智能体系统（MAS）概念，如协调、合作和谈判，并结合提示工程为每个智能体开发不同的角色。这些智能体通过丰富的、协作的对话模拟人类工程团队，遵循群体人工智能的原则，以有效地平衡个人贡献，实现统一解决方案。我们将这些技术调整为LLM智能体的协作结构，鼓励跨学科推理和谈判，类似于现实世界中的高年级设计项目。为了评估该框架的有效性，我们收集了六个工程和计算机科学专业的……|\n",
        "2501.00881": "|**2025-01-01**|**Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents**|Fouad Bousetouane et.al.|[2501.00881](http://arxiv.org/abs/2501.00881)|null|代理系统的演变是人工智能和现代软件系统发展中的一个重要里程碑，这得益于对垂直智能的需求，这种智能能够满足不同行业的需求。这些系统通过适应性、学习和与动态环境的互动来提升商业成果。在这场革命的最前沿是大型语言模型（LLM）代理，它们作为这些智能系统的认知核心。为了应对一致性和可扩展性的需求，这项工作试图通过识别核心构建模块并提议一个认知技能模块，该模块包含特定领域、专门构建的推理能力，来为垂直AI代理设计模式定义一个标准化级别。基于这些基础概念，本文全面介绍了代理系统，详细阐述了其核心组件、操作模式和实施策略。它还进一步探讨了各个行业的实际应用案例和示例，突出了LLM代理在推动特定行业应用中的变革潜力。|\n",
        "2501.00332": "|**2024-12-31**|**MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation**|Chia-Yuan Chang et.al.|[2501.00332](http://arxiv.org/abs/2501.00332)|null|大型语言模型（LLMs）正成为各种自然语言处理任务的必备工具，但它们常常在生成过时或错误信息方面存在问题。检索增强生成（RAG）通过结合外部实时信息检索来使LLM的响应具有根据，从而解决了这个问题。然而，现有的RAG系统往往在检索文档的质量上遇到困难，因为无关或噪声文档会降低性能、增加计算开销并损害响应的可靠性。为了解决这个问题，我们提出了多智能体过滤检索增强生成（MAIN-RAG），这是一个无需训练的RAG框架，它利用多个LLM智能体来协同过滤和评分检索到的文档。具体来说，MAIN-RAG引入了一种自适应过滤机制，该机制根据分数分布动态调整相关性过滤阈值，有效地减少了噪声，同时保持了相关文档的高召回率。所提出的方法利用智能体间的共识来确保稳健的文档选择，而无需额外的训练数据或微调。在四个问答基准上的实验结果表明，MAIN-RAG始终优于传统的RAG方法，在答案准确率上提高了2-11%，同时减少了无关检索文档的数量。定量分析进一步揭示了我们的方法在响应一致性和答案准确性方面优于基线方法，为基于训练的解决方案提供了一种具有竞争力且实用的替代方案。|\n",
        "2501.03259": "|**2025-01-02**|**Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens**|Abdullah Mushtaq et.al.|[2501.03259](http://arxiv.org/abs/2501.03259)|null|随着大型语言模型（LLMs）如GPT-4和Llama 3在教育环境中的应用日益普及，关于这些技术中嵌入的文化偏见、权力不平等和伦理限制的担忧日益增加。尽管生成式AI工具旨在提升学习体验，但它们往往反映的是根植于西方、受过教育、工业化、富裕和民主（WEIRD）文化模式的价值观，这可能导致多样性的全球观点被边缘化。本文提出了一种框架，通过应用多维度视角来评估和缓解LLMs中的文化偏见。多维度视角受到Senturk等人以及伊斯兰和其他智慧传统的启发，强调多元文化观点的共存，支持一个多层次的认知论，它整合了经验科学和规范价值。我们的分析揭示，LLMs常常表现出文化两极分化，偏见既出现在明显的回答中，也出现在微妙的语境线索中。为了解决固有的偏见并在LLMs中融入多维度视角，我们提出了两种策略：**情境实现的多维度LLMs**，它将多维度原则直接嵌入到系统提示中，从基础层面影响LLMs的输出，而与个人提示无关；以及**多智能体系统（MAS）实现的多维度LLMs**，其中多个LLM智能体，每个智能体代表不同的文化观点，协同生成平衡的综合回答。我们的发现表明，随着缓解策略从情境提示发展到MAS实现，文化包容性显著提高，证据是视角分布得分（PDS）显著上升，以及PDS熵从基线时的3.25%增加到MAS实现的多维度LLMs的98%。情感分析还显示，文化之间的正面情绪有所增加，...|\n",
        "2501.04227": "|**2025-01-08**|**Agent Laboratory: Using LLM Agents as Research Assistants**|Samuel Schmidgall et.al.|[2501.04227](http://arxiv.org/abs/2501.04227)|null|历史上，科学发现是一个漫长且昂贵的流程，从最初的构思到最终结果，需要大量的时间和资源。为了加速科学发现、降低研究成本并提高研究质量，我们引入了“智能实验室”，这是一个基于自主大型语言模型（LLM）的框架，能够完成整个研究过程。该框架接受人类提供的科研想法，并通过三个阶段——文献综述、实验和报告撰写——生成全面的研究成果，包括代码库和研究报告，同时允许用户在每个阶段提供反馈和指导。我们将智能实验室部署了多种最先进的LLM，并邀请多位研究人员通过参与调查、提供人类反馈以指导研究过程，然后评估最终论文来评估其质量。我们发现：（1）“智能实验室”由o1-preview驱动时能够生成最佳的研究成果；（2）生成的机器学习代码与现有方法相比能够实现最先进的性能；（3）在每个阶段提供反馈的人类参与显著提高了研究的整体质量；（4）“智能实验室”显著降低了研究费用，与之前自主研究方法相比实现了84%的降幅。我们希望“智能实验室”能够使研究人员能够将更多精力投入到创意构思上，而不是低级的编码和写作，从而最终加速科学发现。|\n",
        "2501.05171": "|**2025-01-09**|**Emergence of human-like polarization among large language model agents**|Jinghua Piao et.al.|[2501.05171](http://arxiv.org/abs/2501.05171)|null|大型语言模型（LLMs）的快速进步赋予了自主代理建立社会关系、沟通以及在政治问题上形成共享和分歧意见的能力。然而，我们对它们的集体行为和潜在机制的理解仍然不完整，这给人类社会带来了意外的风险。在本文中，我们模拟了一个包含数千个大型语言模型代理的网络系统，发现通过LLM对话引导的社会互动导致类似人类的极化。我们发现，这些代理自发地发展出具有类似人类特性的社会网络，包括同质性集群，但它们也通过观察到的现实世界机制塑造其集体意见，包括回音室效应。人类与LLM代理之间的相似性——包括行为、机制和涌现现象——引发了对它们放大社会极化能力的担忧，但也持有作为识别缓解极化及其后果可能策略的有价值测试平台的潜力。|\n",
        "2501.05057": "|**2025-01-09**|**LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models**|Zengqi Peng et.al.|[2501.05057](http://arxiv.org/abs/2501.05057)|null|近期强化学习（RL）在自动驾驶领域的进展展示了其巨大的潜力。尽管如此，手动设计奖励函数和复杂环境中的低样本效率等问题仍然阻碍着安全有效的驾驶策略的开发。为了解决这些问题，我们引入了LearningFlow，这是一种针对城市驾驶的创新型自动化策略学习工作流程。该框架在整个RL训练过程中利用多个大型语言模型（LLM）代理之间的协作。LearningFlow包括课程序列生成过程和奖励生成过程，这两个过程协同工作，通过生成定制的训练课程和奖励函数来指导RL策略。特别是，每个过程都由一个分析代理支持，该代理评估训练进度并为生成代理提供关键见解。通过这些LLM代理的协作努力，LearningFlow自动化了一系列复杂驾驶任务的政策学习，并显著减少了对手动奖励函数设计的依赖，同时提高了样本效率。在高保真的CARLA模拟器中进行了全面实验，并与其他现有方法进行了比较，以证明我们提出方法的有效性。结果表明，LearningFlow在生成奖励和课程方面表现出色。它还在各种驾驶任务中实现了优越的性能和鲁棒的一般化，以及值得称赞的对不同RL算法的适应性。|\n",
        "2501.07554": "|**2025-01-13**|**SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing**|Varun Biyyala et.al.|[2501.07554](http://arxiv.org/abs/2501.07554)|**[link](https://github.com/custommetrics-sst/sst_customevaluationmetrics)**|**视频编辑模型已取得显著进步，但对其性能进行评估仍然具有挑战性。传统的指标，如CLIP文本和图像分数，通常存在不足：文本分数受限于不足的训练数据和层级依赖性，而图像分数无法评估时间一致性。我们提出了SST-EM（语义、空间和时间评估指标），这是一种利用现代视觉语言模型（VLM）、物体检测和时间一致性检查的新颖评估框架。SST-EM包括四个部分：（1）使用VLM从帧中提取语义，（2）使用物体检测进行主要物体跟踪，（3）通过一个LLM代理进行聚焦物体细化，以及（4）使用视觉Transformer（ViT）进行时间一致性评估。这些部分被整合到一个统一的指标中，其权重来源于人类评估和回归分析。SST-EM的名字反映了其对视频评估的语义、空间和时间方面的关注。SST-EM为视频编辑中的语义保真度和时间平滑度提供了全面评估。源代码可在GitHub仓库中找到：\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub Repository}}。**|\n",
        "2501.07278": "|**2025-01-13**|**Lifelong Learning of Large Language Model based Agents: A Roadmap**|Junhao Zheng et.al.|[2501.07278](http://arxiv.org/abs/2501.07278)|**[link](https://github.com/qianlima-lab/awesome-lifelong-llm-agent)**|**终身学习，也称为持续学习或增量学习，是通过使系统能够持续适应动态环境来推进通用人工智能（AGI）的关键组成部分。尽管大型语言模型（LLM）在自然语言处理方面展现了令人印象深刻的能力，但现有的LLM代理通常是为静态系统设计的，缺乏随时间适应新挑战的能力。这项调查首次系统地总结了将终身学习融入基于LLM代理的潜在技术。我们将这些代理的核心组件分为三个模块：感知模块，用于多模态输入整合；记忆模块，用于存储和检索不断发展的知识；动作模块，用于与动态环境的地面交互。我们强调了这些支柱如何共同实现持续适应、减轻灾难性遗忘并提高长期性能。本调查为致力于在LLM代理中开发终身学习能力的研究人员和从业者提供了路线图，提供了对新兴趋势、评估指标和应用场景的见解。相关文献和资源可在以下链接找到：[https://github.com/qianlima-lab/awesome-lifelong-llm-agent](https://github.com/qianlima-lab/awesome-lifelong-llm-agent)。**|\n",
        "2501.06706": "|**2025-01-12**|**AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds**|Yinfang Chen et.al.|[2501.06706](http://arxiv.org/abs/2501.06706)|null|人工智能IT运维（AIOps）旨在自动化复杂运维任务，如故障定位和根本原因分析，以减轻人工工作量并最小化对客户的影响。尽管传统的DevOps工具和AIOps算法通常关注解决孤立的运维任务，但最近在大型语言模型（LLMs）和AI代理方面的进展正在通过实现端到端和多任务自动化来革新AIOps。本文设想了一个未来，其中AI代理可以自主管理整个事件生命周期的运维任务，从而实现自我修复的云计算系统，我们称之为代理运维（AgentOps）。实现这一愿景需要一套全面的框架来指导这些代理的设计、开发和评估。为此，我们提出了AIOPSLAB框架，它不仅部署微服务云环境、注入故障、生成工作负载和导出遥测数据，而且还编排这些组件并提供与代理交互和评估的接口。我们讨论了此类整体框架的关键要求，并展示了AIOPSLAB如何促进下一代AIOps代理的评估。通过在AIOPSLAB创建的基准测试中评估最先进的LLM代理，我们提供了它们在处理云环境中的复杂运维任务方面的能力和局限性的见解。|\n",
        "2501.06695": "|**2025-01-12**|**DVM: Towards Controllable LLM Agents in Social Deduction Games**|Zheng Zhang et.al.|[2501.06695](http://arxiv.org/abs/2501.06695)|null|大型语言模型（LLMs）推动了社交推理游戏（SDGs）中游戏代理的能力。这类游戏高度依赖由对话驱动的交互，需要代理根据此类信息进行推理、做出决策和表达。尽管这一进步导致了SDGs中更复杂和策略性的非玩家角色（NPCs），但仍然需要控制这些代理的熟练程度。这种控制不仅确保NPC能够在游戏过程中适应不同的难度级别，而且为LLM代理的安全性和公平性提供了见解。在本文中，我们提出了DVM，一个用于开发可控LLM代理用于SDGs的新框架，并在最受欢迎的SDG之一《狼人杀》中展示了其实现。DVM包括三个主要组件：预测器、决策者和讨论者。通过将强化学习与获胜率受限的决策链奖励机制相结合，我们使代理能够动态调整其游戏熟练程度以达到特定的获胜率。实验表明，DVM不仅在《狼人杀》游戏中优于现有方法，而且成功调节了其性能水平以实现预定义的获胜率目标。这些结果为LLM代理在SDGs中的自适应和平衡游戏铺平了道路，为可控游戏代理的研究开辟了新的途径。|\n",
        "2501.06327": "|**2025-01-10**|**OpenFOAMGPT: a RAG-Augmented LLM Agent for OpenFOAM-Based Computational Fluid Dynamics**|Sandeep Pandey et.al.|[2501.06327](http://arxiv.org/abs/2501.06327)|null|这项工作提出了一种基于大型语言模型（LLM）的代理OpenFOAMGPT，专为OpenFOAM中心的计算流体动力学（CFD）模拟而设计，利用了OpenAI的两个基础模型：GPT-4o和一个思维链（CoT）启用o1预览模型。这两个代理在多个任务中都取得了成功。虽然使用o1模型的代币价格是GPT-4o的六倍，但它始终在处理复杂任务方面表现出优异的性能，从零样本案例设置到边界条件修改、湍流模型调整和代码翻译。通过迭代校正循环，代理高效地处理了单相和多相流、传热、RANS、LES和其他工程场景，通常在有限的迭代次数和低代币成本下收敛。为了嵌入特定领域的知识，我们采用了检索增强生成（RAG）管道，展示了如何将现有模拟设置进一步专门化，以适应能源和航空航天等子领域。尽管代理的性能优异，但人类监督对于确保准确性和适应不断变化的环境仍然至关重要。模型性能随时间波动表明，在任务关键型应用中需要监控。尽管我们的演示集中在OpenFOAM上，但该框架的适应性为将LLM驱动的代理开发成各种求解器和代码打开了大门。通过简化CFD模拟，这种方法有可能加快基础研究和工业工程进步。|\n",
        "2501.06322": "|**2025-01-10**|**Multi-Agent Collaboration Mechanisms: A Survey of LLMs**|Khanh-Tung Tran et.al.|[2501.06322](http://arxiv.org/abs/2501.06322)|null|随着大型语言模型（LLMs）的近期进展，具有代理功能的AI在现实世界中的应用变得非常显著，正朝着基于多个LLMs的智能体协同感知、学习、推理和行动的方向发展。这些基于LLMs的多智能体系统（MASs）使得一群智能体能够大规模地协同解决复杂任务，从孤立的模型转向以合作为核心的方法。本研究对MASs的协同方面进行了广泛的调查，并介绍了一个可扩展的框架来指导未来的研究。我们的框架根据关键维度来描述协作机制：参与者（涉及的智能体）、类型（例如，合作、竞争或竞合）、结构（例如，对等、集中式或分布式）、策略（例如，基于角色的或基于模型的）和协调协议。通过回顾现有方法，我们的发现为揭示和推进基于LLMs的MASs，使其更智能、更协作地解决复杂现实世界案例提供了基础。此外，还研究了MASs在各个领域的应用，包括5G/6G网络、工业5.0、问答和社交文化环境，展示了它们更广泛的应用和更深远的影响。最后，我们确定了MASs在向人工集体智能发展过程中所学的关键经验、开放挑战和潜在的研究方向。|\n",
        "2501.08262": "|**2025-01-14**|**Addressing the sustainable AI trilemma: a case study on LLM agents and RAG**|Hui Wu et.al.|[2501.08262](http://arxiv.org/abs/2501.08262)|null|大型语言模型（LLMs）展现出显著的能力，但它们的广泛应用和更高级应用引发了关键的可持续性挑战，尤其是在推理能耗方面。我们提出了可持续AI三难理论，突出了人工智能能力、数字公平性和环境可持续性之间的紧张关系。通过系统性地研究LLM代理和检索增强生成（RAG），我们分析了内存模块设计中的能源成本，并引入了新的指标来量化能耗与系统性能之间的权衡。我们的实验结果表明，当前内存增强框架中存在显著的能源低效，并证明资源受限的环境面临着不成比例的效率惩罚。我们的发现挑战了当前以LLM为中心的代理设计范式，并为开发更可持续的人工智能系统提供了实用见解。|\n",
        "2501.07834": "|**2025-01-14**|**Flow: A Modular Approach to Automated Agentic Workflow Generation**|Boye Niu et.al.|[2501.07834](http://arxiv.org/abs/2501.07834)|null|基于大型语言模型（LLM）的多智能体框架在自动规划和任务执行方面取得了巨大成功。然而，在执行过程中对智能体工作流程的有效调整尚未得到充分研究。有效的工作流程调整至关重要，因为在许多实际场景中，初始计划必须实时调整以应对未预见的挑战和变化条件，以确保复杂任务的效率执行。在本文中，我们将工作流程定义为顶点活动图（AOV图）。我们通过根据历史性能和之前与LLM智能体协同的AOV动态调整任务分配来不断优化工作流程。为了进一步提高系统性能，我们强调基于衡量并行性和依赖复杂度的工作流程设计模块化。我们提出的多智能体框架实现了高效子任务的并发执行、目标达成和错误容忍。不同实际任务中的实证结果表明，通过动态工作流程更新和模块化，多智能体框架的效率得到了显著提升。|\n",
        "2501.08760": "|**2025-01-15**|**Leveraging LLM Agents for Translating Network Configurations**|Yunze Wei et.al.|[2501.08760](http://arxiv.org/abs/2501.08760)|null|配置转换是网络操作中一项关键且频繁的任务。当网络设备损坏或过时，管理员需要更换设备以保持服务连续性。更换的设备可能来自不同的厂商，这需要配置转换以确保网络操作无缝进行。然而，手动转换配置是一个劳动密集且易出错的过程。在本文中，我们提出了一种基于意图的框架，使用大型语言模型（LLM）代理进行网络配置转换。我们方法的核心是一个基于意图的检索增强生成（IRAG）模块，该模块系统地将配置文件分割成片段，提取意图并生成准确的翻译。我们还设计了一种两阶段验证方法来验证转换配置的语法和语义正确性。我们在真实世界的网络配置上实现了并评估了所提出的方法。实验结果表明，我们的方法在翻译准确性方面达到了97.74%的语法正确率，超过了现有技术的最佳水平。|\n",
        "2501.10120": "|**2025-01-17**|**PaSa: An LLM Agent for Comprehensive Academic Paper Search**|Yichen He et.al.|[2501.10120](http://arxiv.org/abs/2501.10120)|**[link](https://github.com/bytedance/pasa)**|**我们介绍了一种名为PaSa的高级论文搜索代理，它由大型语言模型驱动。PaSa可以自主进行一系列决策，包括调用搜索工具、阅读论文和选择相关参考文献，最终为复杂的学术查询提供全面准确的结果。我们使用合成数据集AutoScholarQuery对PaSa进行优化，该数据集包含来自顶级AI会议出版物中的3.5万个细粒度学术查询及其对应论文。此外，我们开发了RealScholarQuery基准，它收集真实世界的学术查询，以评估PaSa在实际场景中的性能。尽管PaSa是在合成数据上训练的，但在RealScholarQuery上它显著优于现有的基线，包括Google、Google Scholar、Google结合GPT-4进行释义查询、chatGPT（具有搜索功能的GPT-4o）、GPT-o1和PaSa-GPT-4o（通过提示GPT-4o实现的PaSa）。值得注意的是，PaSa-7B在recall@20和recall@50方面的表现超过了基于Google的最佳基线Google结合GPT-4o，分别高出37.78%和39.90%。它还在召回率上超过了PaSa-GPT-4o 30.36%，在精确度上高出4.25%。模型、数据集和代码可在https://github.com/bytedance/pasa上获取。**|\n",
        "2501.10069": "|**2025-01-17**|**A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks**|Xinzhe Li et.al.|[2501.10069](http://arxiv.org/abs/2501.10069)|null|通过搜索进行LLM测试时计算（或LLM推理）已成为一个具有快速发展的有希望的研究领域。然而，当前框架通常在三个关键方面（任务定义、LLM分析和搜索过程）采用不同的观点，这使得直接比较变得困难。此外，所采用的搜索算法往往偏离标准实现，且其特定特征未得到充分说明。在本调查中，我们提供了一项全面的技术综述，统一了任务定义，并提供了LLM分析和搜索过程的模块化定义。这些定义使得对各种LLM推理框架的精确比较成为可能，同时突出了它们与常规搜索算法的差异。我们还讨论了这些方法的应用性、性能和效率。有关更详细的信息和持续更新，请参阅我们的GitHub仓库：https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md|\n",
        "2501.11864": "|**2025-01-21**|**LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems**|Venkata Sai Aswath Duvvuru et.al.|[2501.11864](http://arxiv.org/abs/2501.11864)|null|彻底的仿真测试对于验证小型无人驾驶航空系统（sUAS）在多种场景下的正确行为至关重要，包括恶劣天气条件（如风和雾）、不同环境（如丘陵地形或城市区域）以及不同的任务配置（如监视、跟踪）。尽管存在各种sUAS仿真工具来支持开发者，但创建、执行和分析仿真测试的整个过程仍然是一个主要的手动且繁琐的任务。开发者必须识别测试场景、设置仿真环境、将系统（SuT）与仿真工具集成、制定任务计划以及收集和分析结果。这些劳动密集型任务限制了开发者进行广泛场景测试的能力。为了解决这个问题，在本文中，我们提出了AutoSimTest，这是一个由大型语言模型（LLM）驱动的框架，其中多个LLM代理协作以支持sUAS仿真测试过程。这包括：（1）创建测试场景，使SuT面临独特的环境背景；（2）根据测试场景准备仿真环境；（3）为SuT生成多种sUAS任务；（4）分析仿真结果并提供交互式分析界面。此外，该框架的设计灵活，可以创建和测试适用于各种sUAS用例、仿真工具和SuT输入要求的场景。我们通过以下方式评估了我们的方法：（a）对基于PX4和ArduPilot飞行控制器的SuT进行仿真测试，（b）分析每个代理的性能，以及（c）收集sUAS开发者的反馈。我们的发现表明，AutoSimTest显著提高了sUAS测试过程的效率和范围，允许进行更全面和多样化的场景评估，同时减少了手动工作量。|\n",
        "2501.11425": "|**2025-01-20**|**Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training**|Siyu Yuan et.al.|[2501.11425](http://arxiv.org/abs/2501.11425)|**[link](https://github.com/bytedance/agent-r)**|**大型语言模型（LLMs）的智能体在处理交互式环境中的复杂任务方面变得越来越关键。现有研究主要集中于通过从更强的专家那里进行行为克隆来提高性能，然而，这些方法在现实世界应用中往往失败，主要是因为无法从错误中恢复。然而，逐步级批判数据的收集既困难又昂贵。因此，自动化和动态构建自我批判数据集对于赋予模型智能体能力至关重要。在这项工作中，我们提出了一种迭代自我训练框架，称为Agent-R，它使语言智能体能够即时反思。与基于正确性奖励或惩罚动作的传统方法不同，Agent-R利用MCTS构建训练数据，从错误中恢复正确轨迹。智能体反思的一个关键挑战在于需要及时修订，而不是等到轮次结束时。为了解决这个问题，我们引入了一种模型引导的批判构建机制：动作模型识别失败轨迹中的第一个错误步骤（在其当前能力范围内）。从它开始，我们将它与相邻的正确路径拼接，该路径在树中共享相同的父节点。这种策略使模型能够根据其当前策略进行反思，从而提高学习效率。为了进一步探索这种自我改进范式的可扩展性，我们研究了错误纠正能力和数据集构建的迭代细化。我们的发现表明，Agent-R不断提高了模型从错误中恢复的能力，并实现了及时的错误纠正。在三个交互式环境上的实验表明，Agent-R有效地装备了智能体纠正错误动作并避免循环，与基线方法相比取得了更好的性能（+5.59%）。**|\n",
        "2501.11283": "|**2025-01-20**|**Large Language Model Agents for Radio Map Generation and Wireless Network Planning**|Hongye Quan et.al.|[2501.11283](http://arxiv.org/abs/2501.11283)|null|在利用商业软件进行无线电地图生成和无线网络规划时，通常需要复杂的手动操作，由于手动操作量大，这给可扩展性、适应性和用户友好性带来了重大挑战。为了解决这些问题，我们提出了一种自动化解决方案，该方案采用大型语言模型（LLM）代理。这些代理被设计为能够自主生成无线电地图并简化指定区域的无线网络规划，从而最大限度地减少对大量手动干预的需求。为了验证我们提出解决方案的有效性，我们开发了一个集成LLM代理的软件平台。实验结果表明，通过所提出的LLM代理可以节省大量手动操作，并且自动化解决方案能够实现更优的覆盖范围和信噪比（SINR），尤其是在城市环境中。|\n",
        "2501.11233": "|**2025-01-20**|**PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents**|Kanika Goswami et.al.|[2501.11233](http://arxiv.org/abs/2501.11233)|null|图表可视化虽然对于数据解读和交流至关重要，但它们主要只能以图片形式在PDF中访问，缺乏源数据表和样式信息。为了能够有效地编辑PDF中的图表或数字扫描图像，我们提出了PlotEdit，这是一个基于自然语言的端到端图表图像编辑的多代理框架，通过自我反思的LLM代理实现。PlotEdit协调五个LLM代理：(1) Chart2Table用于数据表提取，(2) Chart2Vision用于识别样式属性，(3) Chart2Code用于检索渲染代码，(4) 指令分解代理用于将用户请求解析为可执行步骤，以及(5) 多模态编辑代理用于实现细微的图表组件修改——所有这些通过多模态反馈进行协调，以保持视觉准确性。在ChartCraft数据集上，PlotEdit在风格、布局、格式和数据为中心的编辑方面优于现有基线，提高了视觉障碍用户的可访问性，并提高了新手的生产力。|\n",
        "2501.10893": "|**2025-01-18**|**Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments**|Hongjin Su et.al.|[2501.10893](http://arxiv.org/abs/2501.10893)|null|基于大型语言模型（LLMs）的自主代理具有增强人类能力、协助处理从发送电子邮件到执行数据分析等数字任务的潜力。现有LLMs在这些任务上的能力常常受到缺乏与它们交互的相应环境中高质量代理数据的限制。我们提出了“交互学习”（Learn-by-interact），一个以数据为中心的框架，用于将LLMs代理适应任何给定环境，而无需人工标注。Learn-by-interact基于文档合成代理-环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程被称为“反向构建”。我们通过在基于训练的场景和无训练的上下文学习（ICL）中使用这些合成数据来评估其质量，其中我们设计了针对代理优化的创新检索方法。在SWE-bench、WebArena、OSWorld和Spider2-V上的广泛实验，涵盖了现实中的编码、网页和桌面环境，展示了Learn-by-interact在多种下游代理任务中的有效性——使用Claude-3.5进行ICL的基线结果提高了最多12.2%，使用Codestral-22B进行训练提高了最多19.5%。我们进一步证明了反向构建的关键作用，它为训练提供了高达14.0%的改进。我们的消融研究证明了我们在ICL中合成数据的效率以及我们的检索管道相对于传统检索增强生成（RAG）等替代方法的优越性。我们期望Learn-by-interact将成为LLMs在现实世界环境中部署时代理数据合成的基石。|\n",
        "2501.10782": "|**2025-01-18**|**ML-SceGen: A Multi-level Scenario Generation Framework**|Yicheng Xiao et.al.|[2501.10782](http://arxiv.org/abs/2501.10782)|null|当前科学研究见证了将大型语言模型应用于场景生成的各种尝试，但仅倾向于生成全面或危险的场景。在本文中，我们寻求构建一个三阶段框架，不仅让用户能够重新获得对生成的场景的控制权，还能在不受控制的交叉路口设置中生成包含危险因素的全面场景。在第一阶段，LLM代理将有助于将预期场景描述的关键组件翻译成功能场景。在第二阶段，我们使用答案集编程（ASP）求解器Clingo帮助我们生成交叉路口内的全面逻辑交通。在最后阶段，我们使用LLM更新相关参数以提高具体场景的关键水平。|\n",
        "2501.13333": "|**2025-01-23**|**AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback**|Joshua Park et.al.|[2501.13333](http://arxiv.org/abs/2501.13333)|**[link](https://github.com/joshprk/agentrec)**|**多智能体系统需要决定哪个智能体最适合执行特定任务。我们提出了一种新的架构，通过扩展Sentence-BERT（SBERT）编码器模型来推荐在给定的自然语言提示下，应该由哪个LLM智能体执行任务。在测试数据上，我们能够实现92.2%的top-1准确率，每个分类所需时间不到300毫秒。与传统的分类方法相比，我们的架构计算成本低、能够适应新类别、可解释，并通过强化学习通过任意指标进行控制。通过将自然语言提示编码为句子嵌入，我们的模型捕捉了与推荐智能体相关的语义内容。然后通过微调最小化属于同一智能体的句子嵌入之间的距离，并通过来自人类反馈的强化学习与人类价值观对齐。这允许通过测量嵌入之间的余弦相似度来根据自然语言提示的最近邻进行分类。这项工作通过生成用于智能体推荐的合成数据集成为可能，我们将该数据集和AgentRec推荐系统的代码开源至https://github.com/joshprk/agentrec。**|\n",
        "2501.13299": "|**2025-01-23**|**Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents**|Shrinidhi Kumbhar et.al.|[2501.13299](http://arxiv.org/abs/2501.13299)|null|材料发现和设计对于推动各个行业的技术进步至关重要，因为它能够促进特定应用材料的发展。最近的研究利用大型语言模型（LLMs）来加速这一过程。我们探讨了LLMs生成可行假设的潜力，这些假设一旦得到验证，可以加快材料发现的速度。与材料科学专家合作，我们从最近的期刊出版物中整理了一个新的数据集，其中包含现实世界的目标、约束和设计现实世界应用的方法。使用这个数据集，我们测试了基于LLMs的代理，这些代理在特定约束下生成实现给定目标的假设。为了评估这些假设的相关性和质量，我们提出了一种新的可扩展评估指标，该指标模拟了材料科学家评估假设时所使用的批判性过程。我们整理的数据集、提出的方法和评估框架旨在推动利用LLMs加速材料发现和设计的研究。|\n",
        "2501.14205": "|**2025-01-24**|**Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement Learning-based Model Caching and Inference Offloading**|Minrui Xu et.al.|[2501.14205](http://arxiv.org/abs/2501.14205)|null|大型语言模型（LLMs）能够进行未见任务的零样本学习以及在复杂推理任务上的小样本学习。然而，资源有限的移动边缘网络在用户多轮交互中难以支持LLM代理的长上下文LLM服务。与边缘计算中无状态计算卸载和静态服务卸载不同，在边缘服务器上优化LLM服务具有挑战性，因为LLMs持续从上下文中学习，这提高了准确率、延迟和资源消耗的动态性。在本文中，我们提出了一种联合模型缓存和推理卸载框架，该框架利用测试时深度强化学习（T2DRL）来优化长上下文LLM服务的部署和执行策略。在此框架中，我们分析了性能收敛性并设计了一个优化问题，考虑了LLM中上下文窗口的利用。此外，T2DRL算法可以在训练阶段和测试阶段学习，以主动管理缓存模型和服务请求，并在执行过程中适应上下文变化和用法模式。为了进一步提高资源分配效率，我们提出了一个双荷兰式拍卖（DDA）机制，该机制在动态匹配供需的同时最大化社会福利。最终，实验结果表明，与基线相比，T2DRL算法至少可以降低30%的系统成本，同时保证LLM代理在现实感知和推理任务中的性能。|\n",
        "2501.16173": "|**2025-01-27**|**Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma**|Richard Willis et.al.|[2501.16173](http://arxiv.org/abs/2501.16173)|**[link](https://github.com/willis-richard/evollm)**|**随着自主代理的日益普遍，理解它们在战略互动中的集体行为至关重要。本研究调查了大型语言模型（LLM）代理系统在社会困境中的涌现合作倾向。与以往研究中LLM输出个体行动不同，我们提示最先进的LLM生成重复囚徒困境的完整策略。利用进化博弈论，我们模拟了具有不同战略倾向（侵略性、合作性或中立性）的代理种群，并观察其进化动态。我们的发现揭示，不同的LLM表现出影响侵略性策略与合作性策略相对成功率的独特偏见。这项研究为部署的LLM自主代理系统的潜在长期行为提供了见解，并强调了仔细考虑它们运行的策略环境的重要性。**|\n",
        "2501.15850": "|**2025-01-27**|**LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models**|Yuewen Mei et.al.|[2501.15850](http://arxiv.org/abs/2501.15850)|null|确保和提高自动驾驶系统（ADS）的安全性对于高度自动化车辆的部署至关重要，尤其是在安全关键事件中。为了解决罕见事件的问题，开发了对抗场景生成方法，其中通过操纵交通参与者的行为来诱导安全关键事件。然而，现有方法仍然存在两个局限性。首先，对抗参与者的识别直接影响到生成效果的有效性。然而，现实场景的复杂性，包括众多参与者和多样的行为，使得识别变得具有挑战性。其次，生成的安全关键场景对持续提高ADS性能的潜力尚未得到充分探索。为了解决这些问题，我们提出了LLM-attacker：一个利用大型语言模型（LLMs）的闭环对抗场景生成框架。具体来说，设计了多个LLM智能体并进行协调，以识别最优攻击者。然后，优化攻击者的轨迹以生成对抗场景。这些场景基于ADS的性能进行迭代优化，形成一个反馈循环以提高ADS。实验结果表明，LLM-attacker能够创建比其他方法更危险的场景，并且使用它训练的ADS的碰撞率是使用正常场景训练的一半。这表明LLM-attacker能够测试和增强ADS的安全性和鲁棒性。视频演示请见：https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view。|\n",
        "2501.15283": "|**2025-01-25**|**Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions**|Naihao Deng et.al.|[2501.15283](http://arxiv.org/abs/2501.15283)|null|随着大型语言模型（LLMs）能力的提升，研究人员越来越多地将它们应用于社会模拟。在这篇论文中，我们研究LLM代理之间的互动是否类似于人类。具体来说，我们关注领导者与非领导者之间的代词使用差异，考察模拟是否会导致LLMs互动过程中出现类似人类的代词使用模式。我们的评估揭示了基于LLM的模拟与人类代词使用之间存在的显著差异，基于提示或专门的代理未能展现出类似人类的代词使用模式。此外，我们发现即使LLMs理解人类的代词使用模式，它们也无法在真实互动过程中表现出来。我们的研究突显了基于LLM代理的社会模拟的局限性，呼吁在实践者的决策过程中对此类社会模拟持谨慎态度。|\n",
        "2501.17546": "|**2025-01-29**|**Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant**|Gaole He et.al.|[2501.17546](http://arxiv.org/abs/2501.17546)|**[link](https://github.com/delftcrowd/iui2025_convxai)**|**可解释人工智能（XAI）方法被提出以帮助解释和理解AI系统如何得出特定预测。受先前关于对话用户界面的工作启发，我们认为通过对话用户界面增强现有的XAI方法可以提高用户参与度并增强用户对AI系统的理解。在本文中，我们探讨了对话XAI界面对用户对AI系统理解、信任和依赖的影响。与XAI仪表板相比，我们发现对话XAI界面可以提升用户对AI系统的理解，并提高用户对AI系统的信任。然而，XAI仪表板和对话XAI界面的用户都表现出对AI系统的过度依赖。由大型语言模型（LLM）代理驱动的增强对话放大了这种过度依赖。基于我们的发现，我们认为这种过度依赖的潜在原因是伴随XAI界面的解释深度错觉。我们的发现对设计有效的对话XAI界面以促进适当的依赖和改善人机协作具有重要意义。代码可在https://github.com/delftcrowd/IUI2025_ConvXAI找到。**|\n",
        "2501.17315": "|**2025-01-28**|**A sketch of an AI control safety case**|Tomek Korbak et.al.|[2501.17315](http://arxiv.org/abs/2501.17315)|null|随着大型语言模型（LLM）代理造成伤害的能力不断增强，AI开发者可能会越来越多地依赖监控等控制措施来证明其安全性。我们概述了开发者如何构建一个“控制安全案例”，这是一种结构化论证，表明模型无法规避控制措施以造成不可接受的结果。作为案例研究，我们概述了一个假设性LLM代理在AI公司内部部署不会泄露敏感信息的论证。这个概述依赖于来自“控制评估”的证据，在这个评估中，红队故意设计模型以在部署环境的代理中泄露数据。安全案例的关键在于以下几个主张：（1）红队充分激发了模型泄露数据的能力，（2）控制措施在部署中至少保持同等有效性，以及（3）开发者保守地外推模型性能以预测部署中数据泄露的概率。这个安全案例概述是朝着更具体的论证迈出的一步，这些论证可以用来证明一个危险能力的大型语言模型代理是安全部署的。|\n",
        "2501.17167": "|**2025-01-20**|**QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks**|Yaojie Hu et.al.|[2501.17167](http://arxiv.org/abs/2501.17167)|null|我们引入了QualityFlow，这是一种动态的代理工作流程，用于程序合成。给定一个编程问题的英文描述和一组单元测试，模型的目标是合成一个正确的程序，该程序能够解决问题并通过测试。QualityFlow由多个大型语言模型（LLM）代理组成，类似于一个软件开发团队，包括代码生成、测试和自我调试。现有的程序合成方法面临三个主要限制：假设可见单元测试一致性、合成测试质量的瓶颈以及自我调试轨迹的偏差。为了解决这些问题，我们提出了LLM质量检查器，该检查器明确地“想象”合成程序的执行是否将符合单元测试。质量检查动态控制工作流程，包括提交最终答案、阐明问题陈述和撤销先前工作流程步骤等操作。因此，我们的质量检查器可以精确地接受任何正确的程序，减轻错误合成测试，并防止潜在的工作流程偏差。质量检查器的成功进一步实现了多样化提示，这鼓励LLM响应的变化，以最大限度地提高正确程序出现并通过质量检查的可能性。在实验中，QualityFlow在四个程序合成基准测试（MBPP、HumanEval以及来自EvalPlus对MBPP和HumanEval的更严格评估）上取得了最先进的结果。我们的系统分析表明，由LLM质量检查控制的动态工作流程可以优于静态工作流程和单次尝试零样本合成。质量检查器是我们研究的核心，我们分析了其个别性能和对工作流程准确性的整体影响，以及其他消融实验，以证明我们的工作流程设计。|\n",
        "2501.18320": "|**2025-01-30**|**Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach**|Tianpeng Pan et.al.|[2501.18320](http://arxiv.org/abs/2501.18320)|null|随着大型语言模型（LLMs）的快速发展，自动优化建模（AOM）引起了广泛关注。现有方法主要依赖于提示工程，利用精心设计的专家回应链或结构化指导。然而，由于缺乏特定领域知识，基于提示的技术在传感器阵列信号处理（SASP）领域表现不佳。为了解决这个问题，我们提出了一种基于检索增强生成（RAG）技术的自动建模方法，它包含两个主要组成部分：一个多代理（MA）结构和基于图的RAG（Graph-RAG）过程。MA结构针对架构AOM过程进行了定制，每个代理都是基于人类建模程序的原则设计的。Graph-RAG过程用于将用户查询与特定的SASP建模知识相匹配，从而提高建模结果。在十个经典信号处理问题上的结果表明，所提出的方法（称为MAG-RAG）优于几个AOM基准。|\n",
        "2501.18160": "|**2025-01-30**|**RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing**|Jinyao Guo et.al.|[2501.18160](http://arxiv.org/abs/2501.18160)|null|代码审计是一种以查找漏洞为目的的代码审查过程。大型语言模型（LLMs）在这一任务中显示出巨大的潜力，能够无需编译分析程序，并可根据指定提示进行定制化漏洞检测。然而，将LLMs应用于仓库级别的代码审计面临着显著的挑战。LLMs固有的上下文限制和幻觉可能导致漏洞报告质量低下。同时，软件仓库的大规模也引入了大量的时间和令牌成本，阻碍了实际场景中的效率和可扩展性。这项工作介绍了一种自主的LLM代理，名为RepoAudit，旨在实现精确高效的仓库级别代码审计。RepoAudit配备了代理记忆功能，可以根据需求探索代码仓库，分析单个函数中不同可行程序路径上的数据流事实。它还引入了验证器来检查数据流事实以减轻幻觉，并检查潜在有误路径的条件满足性，这使得RepoAudit能够排除代码审计中的误报。我们的实验表明，由Claude 3.5 Sonnet驱动的RepoAudit在15个现实世界系统中成功找到了38个真实漏洞，平均每个项目耗时0.44小时，花费2.54美元。|\n",
        "2501.19398": "|**2025-01-31**|**Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game**|Mustafa O. Karabag et.al.|[2501.19398](http://arxiv.org/abs/2501.19398)|**[link](https://github.com/mustafakarabag/llmchameleon)**|**基于大型语言模型（LLM）的代理在包含非合作方的环境中变得普遍。在这样的环境中，代理的决策需要对其对手隐藏信息，向其合作者透露信息，并推断信息以识别其他代理的特征。为了调查LLM是否具备这些信息控制和决策能力，我们让LLM代理参与基于语言的隐藏身份游戏《变色龙》。在这个游戏中，一群彼此不相识的非变色龙代理试图在不透露秘密的情况下识别变色龙代理。游戏要求变色龙和非变色龙代理都具备上述信息控制能力。实证结果表明，虽然非变色龙LLM代理能够识别变色龙，但它们未能从变色龙那里隐藏秘密，其获胜概率远低于甚至微不足道的策略水平。为了正式解释这种行为，我们对从隐藏到透露的一系列策略进行了理论分析，并给出了非变色龙获胜概率的上限。基于实证结果和对不同策略的理论分析，我们推断LLM基础上的非变色龙代理向未知身份的代理透露了过多的信息。我们的结果指向了当代LLM（包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet）在战略互动中的弱点。**|\n",
        "2502.02534": "|**2025-02-04**|**Adaptive Self-improvement LLM Agentic System for ML Library Development**|Genghan Zhang et.al.|[2502.02534](http://arxiv.org/abs/2502.02534)|**[link](https://github.com/zhang677/pcl-lite)**|**机器学习库通常是用特定架构的编程语言（ASPLs）编写的，这些语言针对特定领域架构，对于高效的机器学习系统至关重要。然而，编写这些高性能的机器学习库具有挑战性，因为它需要机器学习算法和ASPL的专家知识。另一方面，大型语言模型（LLMs）已显示出通用的编码能力。然而，使用LLMs通过ASPLs生成机器学习库仍存在挑战，因为1）即使对于经验丰富的程序员来说，这个任务也相当复杂，2）由于ASPLs的晦涩和不断发展，可用的代码示例有限。因此，LLMs需要有限的 数据进行复杂推理以完成此任务。为了解决这些挑战，我们引入了一个自适应自我改进的代理系统。为了评估我们系统的有效性，我们在典型机器学习库上构建了一个基准，并使用开源和闭源LLMs在这个基准上生成ASPL代码。我们的结果表明，与基线单LLM相比，改进幅度高达$3.9\\times$。**|\n",
        "2502.01822": "|**2025-02-03**|**Firewalls to Secure Dynamic LLM Agentic Networks**|Sahar Abdelnabi et.al.|[2502.01822](http://arxiv.org/abs/2502.01822)|null|未来大型语言模型（LLM）的代理很可能在涉及具有相互依赖目标的长期计划的任务中代表用户与其他实体表示的代理进行沟通。当前的研究并未关注此类代理网络，也没有解决其面临的挑战。因此，我们首先确定了代理沟通所需具备的属性，这些属性应该是主动和可适应的。它需要满足以下条件：1）隐私性：代理不应分享超过完成任务所需的信息；2）安全性：沟通必须保持完整性，并维护对自私实体的效用。我们设计了一个用例（旅行规划）作为测试平台，以体现这些要求，并展示了如果处理不当可能出现的问题。接下来，我们提出了一种实用的设计方案，该方案受现有网络安全原则的启发，旨在平衡可适应性、安全性和隐私性。我们的框架从先前的模拟中自动构建和更新特定于任务的规则以构建防火墙。我们提供了多层防御措施，包括：1）将自由形式的输入转换为特定于任务的协议；2）根据任务需求动态抽象用户的权限；3）自我纠正代理的轨迹。|\n",
        "2502.01630": "|**2025-02-03**|**TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues**|Yubin Ge et.al.|[2502.01630](http://arxiv.org/abs/2502.01630)|null|在多轮对话中的时间推理是一个重大挑战，在之前的时间推理基准测试中一直未得到充分研究。为了填补这一空白，我们提出了一种新的多轮对话时间推理评估任务，并介绍了一种通过增强LoCoMo对话并创建多选题问答题（QA）来构建新基准的方法。此外，我们提出了TReMu，一个旨在提高在此背景下LLM代理时间推理能力的框架。具体来说，该框架通过时间感知记忆，即通过时间线总结，在每次对话会话中总结事件及其推断的日期来生成可检索的记忆。此外，我们集成了神经符号时间推理，其中LLMs生成Python代码来进行时间计算和选择答案。在流行LLMs上的实验评估表明，我们的基准具有挑战性，且提出的框架与基线方法相比显著提高了时间推理性能，从GPT-4o的29.83通过标准提示提升到77.67，突出了其在解决多轮对话中的时间推理问题上的有效性。|\n",
        "2502.01600": "|**2025-02-03**|**Reinforcement Learning for Long-Horizon Interactive LLM Agents**|Kevin Chen et.al.|[2502.01600](http://arxiv.org/abs/2502.01600)|null|互动式数字代理（IDAs）利用有状态的数字环境的API来响应用户请求执行任务。虽然由指令微调的大型语言模型（LLMs）驱动的IDAs能够在多步交互中响应用户界面调用的反馈，但它们并未在其各自的数字环境中进行训练。先前的方法在复杂的基准测试（如AppWorld）中仅完成了不到一半的任务。我们提出了一种强化学习（RL）方法，该方法直接在目标环境中训练IDAs。我们将这种训练形式化为部分可观察的马尔可夫决策过程，并推导出LOOP，这是一种数据与内存高效的近端策略优化变体。LOOP不使用价值网络，并且仅在内存中保持底层LLM的一个副本，这使得其实施简单，与微调单个LLM一样内存高效。在AppWorld环境中使用LOOP训练的32亿参数代理比更大的OpenAI o1代理的表现高出9个百分点（相对提高15%）。据我们所知，这是首次报道将RL应用于与有状态的、多领域、多应用环境通过直接API调用交互的IDAs。我们的分析揭示了RL在该领域的有效性，表明代理学会了查阅API文档、避免无根据的假设、最小化虚构和从挫折中恢复。|\n",
        "2502.01562": "|**2025-02-03**|**Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization**|Minttu Alakuijala et.al.|[2502.01562](http://arxiv.org/abs/2502.01562)|null|随着人工智能（AI）代理的一般能力不断发展，它们通过经验学习掌握多个复杂任务的能力仍然是一个关键挑战。当前的大型语言模型（LLM）代理，尤其是基于专有语言模型的代理，通常依赖于提示来融入关于目标任务的知识。这种方法不允许代理内化这些信息，而是依赖于不断扩大的提示来维持其在各种场景下的功能。这类似于患有前额叶遗忘症的人使用的笔记系统，即无法形成新记忆。在这篇论文中，我们提出了一种新颖的方法来训练AI代理，使其能够融入多个任务的知识和技能，而无需使用繁琐的笔记系统或先前的高质量演示数据。我们的方法采用了一种迭代过程，其中代理收集新经验，从人类那里以提示的形式获得纠正反馈，并通过上下文蒸馏训练程序将此反馈整合到其权重中。我们通过在一个基于Llama-3的代理中实现我们的方法，并在仅经过几轮反馈后，在需要正确排序信息检索、工具使用和问题回答的任务集中优于GPT-4o和DeepSeek-V3等高级模型，来证明我们方法的有效性。|\n",
        "2502.01506": "|**2025-02-03**|**TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets**|Yuzhe Yang et.al.|[2502.01506](http://arxiv.org/abs/2502.01506)|null|社会涌现的研究长期以来一直是社会科学的核心焦点。传统的建模方法，如基于规则的基于代理的模型（ABM），难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。最近，大型语言模型（LLM）代理作为模拟社会科学和角色扮演应用的工具而受到关注。研究表明，LLM可以解释认知偏差、情绪波动和其他非理性影响，从而实现更逼真的社会经济动态模拟。在本研究中，我们引入了TwinMarket，这是一个利用LLM模拟社会经济系统的创新多代理框架。具体来说，我们考察了个体行为通过交互和反馈机制如何产生集体动态和涌现现象。通过在模拟股票市场环境中的实验，我们展示了个体行为如何引发群体行为，导致涌现结果，如金融泡沫和衰退。我们的方法为个体决策与集体社会经济模式之间的复杂相互作用提供了有价值的见解。|\n",
        "2502.01714": "|**2025-02-03**|**Position: Towards a Responsible LLM-empowered Multi-Agent Systems**|Jinwei Hu et.al.|[2502.01714](http://arxiv.org/abs/2502.01714)|null|随着智能代理AI和基于大型语言模型的多智能体系统（LLM-MAS）的兴起，对负责任和可靠的系统运行的需求日益凸显。像LangChain和检索增强生成这样的工具扩展了LLM的能力，通过增强知识检索和推理，使其能够更深入地融入MAS。然而，这些进步也带来了关键挑战：LLM智能体表现出固有的不可预测性，其输出的不确定性可能在交互过程中累积，威胁到系统稳定性。为了应对这些风险，采用以人为中心的设计方法，并辅以主动动态调节至关重要。这种方法通过促进智能体间的一致沟通和有效的系统治理，增强了传统被动监管，使MAS能够更高效地实现预期目标。|\n",
        "2502.01450": "|**2025-02-03**|**Simulating Rumor Spreading in Social Networks using LLM Agents**|Tianrui Hu et.al.|[2502.01450](http://arxiv.org/abs/2502.01450)|null|随着社交媒体的兴起，虚假信息变得越来越普遍，这主要是由谣言的传播所推动。本研究探讨了在一个新颖框架中使用大型语言模型（LLM）代理来模拟和分析谣言在社会网络中的传播动态。为此，我们设计了各种基于LLM的代理类型，并构建了四种不同的网络结构来进行这些模拟。我们的框架评估了不同网络结构和代理行为在影响谣言传播方面的有效性。我们的结果表明，该框架可以模拟超过一百个代理在各种网络中传播谣言。评估表明，网络结构、角色和传播方案可以显著影响谣言的传播，从无传播到在迭代中影响83%的代理，从而为社交网络中谣言的传播提供了一个现实模拟。|\n",
        "2502.01390": "|**2025-02-03**|**Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant**|Gaole He et.al.|[2502.01390](http://arxiv.org/abs/2502.01390)|null|随着ChatGPT的流行爆炸，大型语言模型（LLMs）持续影响着我们的日常生活。配备了为特定目的设计的工具（例如，航班预订或闹钟），LLM代理展现出越来越强的能力来协助人类完成日常工作。尽管LLM代理作为日常助手展现出有前景的蓝图，但对于它们如何基于规划和顺序决策能力提供日常帮助的理解仍然有限。我们从近期工作中获得灵感，这些工作强调了“LLM-modulo”设置与人类在回路中协同规划任务的价值。我们进行了一项实证研究（N = 248），研究了LLM代理在六种常见任务中的日常助手作用，这些任务通常具有不同水平的风险（例如，航班票务预订和信用卡支付）。为确保用户对LLM代理有自主权和控制权，我们采用了“先规划后执行”的方式，其中代理在模拟环境中进行逐步规划和逐步执行。我们分析了用户在每个阶段参与程度如何影响他们的信任和协作团队表现。我们的发现表明，LLM代理是一把双刃剑——（1）当有高质量的计划和必要的用户执行参与时，它们可以工作得很好；（2）用户很容易对看似合理的计划对LLM代理产生怀疑。我们总结了使用LLM代理作为日常助手以调整用户信任和实现更好的整体任务结果的关键见解。我们的工作对未来日常助手的设计和人类-人工智能与LLM代理的协作具有重要意义。|\n",
        "2502.00989": "|**2025-02-03**|**ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution**|Kanika Goswami et.al.|[2502.00989](http://arxiv.org/abs/2502.00989)|null|大型语言模型（LLMs）能够执行图表问答任务，但常常生成未经验证的幻觉回答。现有的答案归属方法由于视觉语义上下文有限、复杂的视觉文本对齐要求以及复杂布局中边界框预测的困难，难以将回答与源图表联系起来。我们提出了一种名为ChartCitor的多智能体框架，通过在图表图像中识别支持性证据来提供细粒度的边界框引用。该系统协调LLM智能体执行图表到表格的提取、答案重构、表格扩充、通过预过滤和重新排序进行证据检索以及表格到图表的映射。ChartCitor在多种图表类型上优于现有基线。定性用户研究表明，ChartCitor通过为LLM辅助的图表问答提供增强的可解释性，有助于提高用户对生成式人工智能的信任，并使专业人士更加高效。|\n",
        "2502.03450": "|**2025-02-05**|**A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)**|Yiye Chen et.al.|[2502.03450](http://arxiv.org/abs/2502.03450)|null|场景图已成为用于大型语言模型（LLMs）进行基于真实空间的推理的结构化和可序列化环境表示。在本研究中，我们提出了SG-RwR，一个基于模式引导的检索-推理框架，用于场景图中的推理和规划。我们的方法采用两个协同的代码编写型LLM智能体：（1）推理智能体用于任务规划和信息查询生成；（2）检索智能体用于根据查询提取相应的图信息。两个智能体迭代协作，实现顺序推理和对图信息的自适应关注。与先前的研究不同，两个智能体仅以场景图模式进行提示，而不是完整的图数据，这通过限制输入令牌来减少幻觉，并促使推理智能体抽象地生成推理轨迹。在追踪轨迹后，检索智能体根据模式理解程序化地查询场景图数据，从而实现对图的动态和全局关注，增强了推理和检索之间的对齐。通过在多个仿真环境中的实验，我们表明我们的框架在数值问答和规划任务方面优于现有的基于LLM的方法，并且可以从任务级别的少量示例中受益，即使在缺乏智能体级别演示的情况下。项目代码将发布。|\n",
        "2502.04306": "|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306](http://arxiv.org/abs/2502.04306)|null|最近的研究利用大型语言模型多智能体系统解决复杂问题，同时试图减少构建它们的手动工作量，推动了自动化智能体工作流程优化方法的发展。然而，由于表示限制、缺乏适应性和在依赖离散优化技术时的扩展性较差，现有方法仍然不够灵活。我们通过ScoreFlow解决了这些挑战，这是一个简单但性能高效的框架，它在一个连续空间中利用了高效的基于梯度的优化。ScoreFlow集成了Score-DPO，这是一种考虑了定量反馈的直接偏好优化方法的新变体。在涵盖问答、编码和数学推理的六个基准测试中，ScoreFlow相较于现有基线实现了8.2%的提升。此外，它还使小模型能够在较低的推理成本下超越大模型。项目：https://github.com/Gen-Verse/ScoreFlow|\n",
        "2502.03821": "|**2025-02-06**|**PsyPlay: Personality-Infused Role-Playing Conversational Agents**|Tao Yang et.al.|[2502.03821](http://arxiv.org/abs/2502.03821)|null|当前对大型语言模型（LLMs）驱动的角色扮演对话代理（RPCAs）的研究主要集中于模仿特定的说话风格和利用角色背景，忽视了更深层次性格特征的描绘。在本研究中，我们引入了为LLM代理注入性格的角色扮演，鼓励代理在对话中准确展现其指定的性格特征。然后，我们提出了PsyPlay，一个促进多个LLM代理表达丰富性格的对话生成框架。具体来说，PsyPlay使代理能够扮演具有不同性格特征的角色，并围绕特定主题进行讨论，在整个互动中持续展现其指定的性格特征。在生成的对话数据上的验证表明，PsyPlay可以准确描绘预期的性格特征，在GPT-3.5上实现了80.31%的整体成功率。值得注意的是，我们发现与积极价值观一致的LLM在表现积极性格角色方面比消极角色更为成功。此外，我们构建了一个名为PsyPlay-Bench的对话语料库，用于性格注入的角色扮演。该语料库由4745个使用PsyPlay正确描绘的对话实例组成，旨在进一步促进个性化角色扮演和对话性格检测的研究。|\n",
        "2502.03711": "|**2025-02-06**|**MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers**|Nicole Cho et.al.|[2502.03711](http://arxiv.org/abs/2502.03711)|null|在大型语言模型（LLMs）的机构采用过程中，一个关键的挑战来自于它们在生成响应时倾向于产生幻觉。为了解决这个问题，我们提出了MultiQ&A，这是一种用于评估LLM生成答案的鲁棒性和一致性的系统性方法。我们展示了MultiQ&A通过独立LLM代理大规模地众包问题扰动及其相应答案的能力。我们的实验最终检验了190万个问题扰动和230万个答案。此外，MultiQ&A表明，如gpt-3.5-turbo之类的集成LLM在扰动下仍然相对鲁棒和一致。MultiQ&A在响应生成领域提供了清晰的见解，提供了一种有效的方法来检查分歧和变化。因此，我们的系统为机构LLM的采用提供了一个潜在的框架，能够衡量信心、一致性和幻觉的量化。|\n",
        "2502.05174": "|**2025-02-07**|**MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison**|Kaijie Zhu et.al.|[2502.05174](http://arxiv.org/abs/2502.05174)|null|近期研究探讨了LLM代理容易受到间接提示注入（IPI）攻击的脆弱性，恶意任务嵌入在工具检索信息中可以引导代理执行未经授权的操作。现有的IPI防御方法存在重大局限性：要么需要必要的模型训练资源，要么对复杂攻击缺乏有效性，或者损害正常功能。我们提出了MELON（掩码重执行和工具比较），一种新的IPI防御方法。我们的方法基于观察：在成功攻击下，代理的下一步操作对用户任务的依赖性降低，而对恶意任务的依赖性增加。在此基础上，我们设计MELON通过使用通过掩码函数修改的掩码用户提示重新执行代理的轨迹来检测攻击。如果原始执行和掩码执行中生成的操作相似，则我们识别出攻击。我们还包含了三个关键设计来减少潜在的误报和漏报。在IPI基准AgentDojo上的大量评估表明，MELON在攻击预防和功能保持方面优于最先进的防御方法。此外，我们还展示了将MELON与最先进的提示增强防御方法（表示为MELON-Aug）相结合可以进一步提高其性能。我们还进行了一项详细的消融研究，以验证我们的关键设计。|\n",
        "2502.04576": "|**2025-02-07**|**Self-Regulation and Requesting Interventions**|So Yeon Min et.al.|[2502.04576](http://arxiv.org/abs/2502.04576)|null|人类智能包括元认知能力，如自我调节、认识到局限性和在必要时寻求帮助。虽然LLM代理在许多领域表现出色，但它们往往缺乏这种意识。过于自信的代理可能导致灾难性失败，而过度寻求帮助的代理则会妨碍效率。一个关键挑战是在有限的干预预算$C$下，决定何时请求帮助。在本文中，我们提出一个离线框架，通过结合基于LLM的过程奖励模型（PRMs）和表格强化学习来训练一个“助手”策略，以请求干预，例如更强大的模型或测试时的计算资源。使用离线收集的状态转换，我们使用PRMs评估最优干预时机，并在这些标记的轨迹上训练助手模型。这种离线方法显著减少了训练过程中的昂贵干预调用。此外，将PRMs与表格强化学习相结合，增强了对抗离策略数据的鲁棒性，同时避免了深度强化学习的低效性。我们通过实证发现，我们的方法能够实现最佳的助手行为。|\n",
        "2502.04492": "|**2025-02-06**|**Multi-Agent Reinforcement Learning with Focal Diversity Optimization**|Selim Furkan Tekin et.al.|[2502.04492](http://arxiv.org/abs/2502.04492)|null|本文介绍了名为MARL-Focal的焦点多样性优化多智能体强化学习方法，该方法具有三个独特特点。首先，我们开发了一个智能体融合框架，以鼓励多个基于大型语言模型（LLM）的智能体在为每个LLM查询生成最终推理输出时进行协作。其次，我们开发了一种焦点多样性优化智能体选择算法，该算法可以根据智能体相互补充生成查询输出的能力，从可用智能体中选择一个小子集。最后，我们设计了一种冲突解决方法，用于检测多个智能体之间的输出不一致性，并通过奖励感知和政策自适应的推理融合来生成我们的MARL-Focal输出。在五个基准上的广泛评估表明，MARL-Focal既经济高效，又具有对抗鲁棒性。我们的多智能体融合模型与最佳单个LLM智能体相比，性能提升了5.51%，在TruthfulQA基准上提供了更强的鲁棒性。代码可在https://github.com/sftekin/rl-focal上获取。|\n",
        "2502.04485": "|**2025-02-06**|**Active Task Disambiguation with LLMs**|Katarzyna Kobalczyk et.al.|[2502.04485](http://arxiv.org/abs/2502.04485)|null|尽管大型语言模型（LLMs）在各种基准测试中表现出色，但它们处理模糊指定问题（这在现实世界互动中很常见）的能力仍未得到充分探索。为了填补这一空白，我们提出了任务模糊性的正式定义，并通过贝叶斯实验设计的视角来阐述任务去模糊化问题。通过提出澄清问题，LLM代理可以获取额外的任务规范，逐步缩小可行解决方案的空间，并降低生成不满意输出的风险。然而，生成有效的澄清问题需要LLM代理进行一种元认知推理，这是LLMs目前可能缺乏的能力。我们提出的主动任务去模糊化方法使LLM代理能够生成针对性强的问题，最大化信息增益。实际上，这种方法将负担从隐式推理转移到对可行解决方案空间的显式推理。实证结果表明，与仅依靠问题空间内推理的方法相比，这种问题选择形式导致了更有效的任务去模糊化。|\n",
        "2502.04358": "|**2025-02-04**|**Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives**|Elliot Meyerson et.al.|[2502.04358](http://arxiv.org/abs/2502.04358)|null|将难题分解为子问题通常可以使它们更容易和更高效地解决。随着大型语言模型（LLMs）在越来越多的能力上跨越关键可靠性阈值，越来越多地有人尝试将系统分解为基于LLMs的代理集合，每个代理可以分配子任务。然而，这种分解（即使是自动化的）通常是直观的，例如，基于人类如何为人类团队成员分配角色的方式。这些角色分解与最优解有多接近？这篇立场论文认为，需要对基于LLM原语的渐近分析，以推理此类分解系统的效率，并且此类分析将解锁扩展它们的机会。通过将LLM正向传播视为计算成本的原子单元，可以分离出特定LLM（通常是不透明的）内部工作原理以及一组LLM如何编排以解决难题的固有效率。换句话说，如果我们想要将LLMs的部署扩展到极限，而不是将LLMs拟人化，那么我们应该使用基于LLM原语的渐近分析来推理和开发将大问题分解为LLM代理的更强大的分解方法。|\n",
        "2502.06787": "|**2025-02-10**|**Visual Agentic AI for Spatial Reasoning with a Dynamic API**|Damiano Marsili et.al.|[2502.06787](http://arxiv.org/abs/2502.06787)|null|视觉推理——即解读视觉世界的能力——对于在三维场景中操作的具身智能体至关重要。人工智能的进步使得能够从图像中回答问题的视觉和语言模型成为可能。然而，当需要执行3D空间推理任务时，它们的性能会下降。为了解决这类推理问题的复杂性，我们提出了一种代理程序合成方法，其中LLM代理共同生成一个Pythonic API，并添加新的功能来解决常见的子问题。我们的方法克服了依赖于静态、人类定义的API的先前方法的局限性，使其能够处理更广泛的查询。为了评估人工智能在3D理解方面的能力，我们引入了一个新的基准，其中包含涉及多个步骤的地面和推理的查询。我们表明，我们的方法在3D视觉推理方面优于先前的零样本模型，并通过实证验证了我们的代理框架在3D空间推理任务中的有效性。项目网站：https://glab-caltech.github.io/vadar/|\n",
        "2502.06776": "|**2025-02-10**|**Towards Internet-Scale Training For Agents**|Brandon Trabucco et.al.|[2502.06776](http://arxiv.org/abs/2502.06776)|null|主要训练网络导航代理的方法是通过收集一组热门网站和手写任务的人类演示，但越来越明显的是，人类数据是一个效率低下的资源。我们开发了一个流程，以简化代理进行互联网规模训练，而不需要繁琐的人类标注。在第一阶段，一个大型语言模型（LLM）为15万个不同的网站生成任务。在第二阶段，LLM代理完成任务并生成轨迹。在最后阶段，一个LLM审查轨迹并判断其成功率。语言模型在人类标注员中具有竞争力，检测和过滤有害内容的准确率为97%，生成可行任务的比率为89%，判断成功轨迹的准确率为82.6%。扩展流程后，基于Llama 3.1 70B的代理解决了15万个网站中16.7%的任务。在由我们的流程生成的数据上训练与在人类演示上训练具有竞争力。在由Mind2Web和WebLINX派生出的数据有限的环境中，对于训练在来自我们的流程和人类数据混合的数据上的代理，我们分别提高了+89.5%和+122.1%的步骤准确率。当使用这些基准中所有可用的人类数据训练代理时，代理无法推广到多样化的真实网站上，而添加我们的数据提高了它们的推广能力，WebLINX提高了+149.0%，Mind2Web提高了+156.3%。代码将在：data-for-agents.github.io上提供。|\n",
        "2502.06589": "|**2025-02-10**|**Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training**|Yuchen Zhuang et.al.|[2502.06589](http://arxiv.org/abs/2502.06589)|null|由于面向代理的预训练数据稀缺，基于LLM的自主代理通常依赖复杂的提示或广泛的微调，这往往无法引入新能力同时保持强大的泛化能力。我们引入了Hephaestus-Forge，这是第一个旨在增强LLM代理在API函数调用、内在推理和规划以及适应环境反馈的基本能力的大规模预训练语料库。Hephaestus-Forge包含103B的代理特定数据，涵盖了76,537个API，包括工具文档来介绍API函数知识以及函数调用轨迹来加强内在推理。为了探索有效的训练协议，我们研究了扩展定律以确定数据混合比例的最佳配方。通过在Hephaestus-Forge上的持续预训练，Hephaestus在三个代理基准测试中优于小型到中型规模的开放源代码LLM，并可与商业LLM相媲美，证明了我们的预训练语料库在增强基本代理能力和LLM对新任务或环境的泛化方面的有效性。|\n",
        "2502.06111": "|**2025-02-10**|**CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories**|Yijia Xiao et.al.|[2502.06111](http://arxiv.org/abs/2502.06111)|null|随着计算机科学研究项目的日益复杂，对部署代码库的有效工具的需求也在增加。大型语言模型（LLMs），如Anthropic Claude和Meta Llama，在各种计算机科学研究领域取得了显著的进步，包括自动化各种软件工程任务。为了评估LLMs在处理研究项目复杂代码开发任务中的有效性，特别是针对NLP/CV/AI/ML/DM主题，我们引入了CSR-Bench，这是一个针对计算机科学研究项目的基准。该基准从准确性、效率和部署脚本质量等多个方面评估LLMs，旨在探索它们在自主进行计算机科学研究中的潜力。我们还介绍了一个新颖的框架，CSR-Agents，该框架利用多个LLM代理来自动化计算机科学研究项目的GitHub代码库的部署。具体来说，通过检查markdown文件中的指令并解释仓库结构，模型生成并迭代改进bash命令，以设置实验环境并将代码部署以进行研究任务。CSR-Bench的初步结果表明，LLM代理可以显著提高代码库部署的工作流程，从而提高开发者的生产力和改善开发工作流程的管理。|\n",
        "2502.05982": "|**2025-02-09**|**HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents**|Mohammad Amin Abbasi et.al.|[2502.05982](http://arxiv.org/abs/2502.05982)|null|本文介绍了一个名为HamRaz的新型波斯语心理健康数据集，该数据集旨在用于以人为中心的疗法（PCT）和大型语言模型（LLMs）。尽管LLMs在AI驱动的心理咨询服务中的应用日益增长，但现有的数据集主要关注西方和东亚语境，忽略了对于有效波斯语疗法至关重要的文化和语言细微差别。为了填补这一空白，HamRaz结合了基于脚本的对话和自适应LLM角色扮演，确保了连贯和动态的治疗互动。我们还引入了HamRazEval，这是一个双重评估框架，它使用通用对话指标和Barrett-Lennard关系量表（BLRI）来衡量对话质量和治疗效果。实验结果表明，HamRaz优于传统的脚本模式和双代理模式，产生了更具同理心、情境感知和现实的治疗会话。通过发布HamRaz，我们为推进多元社区中AI驱动的心理治疗研究贡献了一个文化适应的、LLM驱动的资源。|\n",
        "2502.05957": "|**2025-02-09**|**MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents**|Jiabin Tang et.al.|[2502.05957](http://arxiv.org/abs/2502.05957)|null|大型语言模型（LLM）智能体在任务自动化和智能决策方面展现出卓越的能力，推动了LangChain和AutoGen等智能体开发框架的广泛应用。然而，这些框架主要服务于具有丰富技术专长的开发者——考虑到全球只有0.03%的人口拥有必要的编程技能，这是一个重大的限制。这种明显的可访问性差距提出了一个基本问题：我们能否让每个人，无论技术背景如何，仅使用自然语言就能构建自己的LLM智能体？为了应对这一挑战，我们引入了MetaChain——一个完全自动化和高度自我发展的框架，使用户能够仅通过自然语言创建和部署LLM智能体。作为自主智能体操作系统，MetaChain包含四个关键组件：i）智能体系统工具，ii）LLM驱动的可执行引擎，iii）自我管理的文件系统，和iv）自我玩耍智能体定制模块。这个轻量级但强大的系统使得无需编码要求或手动干预即可高效且动态地创建和修改工具、智能体和工作流程。除了其无代码的智能体开发能力外，MetaChain还充当一个多智能体系统，用于通用人工智能助手。在GAIA基准上的全面评估表明，MetaChain在通用多智能体任务中的有效性超过了现有的最先进方法。此外，MetaChain在检索增强生成（RAG）相关能力方面的表现也持续优于许多基于LLM的替代解决方案。|\n",
        "2502.07709": "|**2025-02-11**|**MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces**|Loris Gaven et.al.|[2502.07709](http://arxiv.org/abs/2502.07709)|null|开放式的学习智能体需要在庞大的可能性空间中高效地优先考虑目标，专注于那些最大化学习进度（LP）的目标。当通过使用在线强化学习训练的LLM智能体在高维和不断变化的目标准空间中实现这种以目标本身为目的的探索时，LP预测的一个关键挑战是建模自身的胜任能力，这是一种元认知监控形式。传统方法要么需要大量的采样，要么依赖于脆弱的专家定义的目标分组。我们引入了MAGELLAN，这是一个元认知框架，允许LLM智能体在线学习预测自身的胜任能力和LP。通过捕捉目标之间的语义关系，MAGELLAN实现了高效的LP估计并通过泛化实现对不断变化的目标准空间的动态适应。在一个交互式学习环境中，我们展示了MAGELLAN可以提高LP预测效率和目标优先级，是唯一允许智能体完全掌握一个庞大且不断变化的目标准空间的方法。这些结果证明了通过为LLM智能体增加LP预测的元认知能力，可以有效地将课程学习扩展到开放式的目标准空间。|\n",
        "2502.07443": "|**2025-02-11**|**Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames**|Vince Trencsenyi et.al.|[2502.07443](http://arxiv.org/abs/2502.07443)|null|基于LLM的多智能体模拟在博弈论和社会模拟中的应用越来越受到关注。尽管大多数实现旨在利用或评估LLM的智能推理，但它们通常在代理的概念和架构上过于简化。我们实现了一个基于角色的多智能体战略交互框架，适用于复杂的递归推理者，为战略推理的系统深入开发与评估提供了手段。我们的游戏环境由负责协调游戏、从匹配到移动验证再到环境管理的裁判来管理。玩家在决策机制中采用最先进的LLM，依靠基于超博弈的分层信念模型。我们使用一次性的两人美容竞赛来评估最新LLM的递归推理能力，并将其与经济学中一个已建立的基线模型和人类实验数据进行了比较。此外，我们为k级理论引入了替代语义推理测量的基础。我们的实验表明，在近似人类行为和达到最优解方面，人工推理者可以优于基线模型。|\n",
        "2502.07223": "|**2025-02-11**|**Graph RAG-Tool Fusion**|Elias Lumer et.al.|[2502.07223](http://arxiv.org/abs/2502.07223)|**[link](https://github.com/eliaslumer/graph-rag-tool-fusion-toollinkos)**|**最近在检索增强生成（RAG）方面的发展，使得LLM代理能够将它们复杂的工具调用能力扩展到数百或数千个外部工具、API或作为工具的代理。然而，传统的基于RAG的工具检索未能捕捉到工具之间的结构化依赖关系，限制了检索工具依赖的准确性。例如，在一个工具向量数据库中，“获取股票价格”API需要一个“股票代码”参数，该参数来自“获取股票代码”API，而这两个API都依赖于操作系统级别的互联网连接工具。在本文中，我们通过引入图RAG-工具融合，一种新颖的即插即用方法来解决这一局限性，该方法结合了基于向量的检索的优势和高效的图遍历来捕捉预定义工具知识图中的所有相关工具（节点）及其嵌套依赖（边）。我们还提出了ToolLinkOS，一个包含573个虚构工具的新工具选择基准，涵盖15个行业，每个行业平均有6.3个工具依赖。我们证明了图RAG-工具融合在ToolLinkOS和ToolSandbox基准上分别实现了71.7%和22.1%的绝对改进（mAP@10）。ToolLinkOS数据集可在https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS找到。**|\n",
        "2502.07165": "|**2025-02-11**|**Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification**|Peipei Wei et.al.|[2502.07165](http://arxiv.org/abs/2502.07165)|null|我们提出了一种基于原则的提示策略，名为PRINCIPLE-BASED PROMPTING，这是一种简单但有效的多智能体提示策略，用于文本分类。该策略首先让多个LLM智能体独立地根据带有或没有标签的演示样本分析生成候选原则，然后通过一个最终化智能体将这些原则整合成最终原则，最后将它们发送给分类智能体以执行下游分类任务。在具有不同大小LLM的二分类和多分类数据集上进行的广泛实验表明，我们的方法不仅在宏观F1分数上实现了显著的性能提升（1.55% - 19.37%），而且优于其他强大基线（CoT和回溯提示）。我们方法生成的原则帮助LLM在两个私有数据集上的分类任务中比人工制作的原则表现更好。我们的多智能体PRINCIPLE-BASED PROMPTING方法与基于演示的少样本提示方法相比，性能相当或更好，但推理成本显著降低。消融研究显示，标签信息和多智能体合作的LLM框架在生成高质量原则以促进下游分类任务方面发挥着重要作用。|\n",
        "2502.07132": "|**2025-02-10**|**Interactive Data Harmonization with LLM Agents**|Aécio Santos et.al.|[2502.07132](http://arxiv.org/abs/2502.07132)|null|数据协调是整合来自不同来源数据集的一项基本任务。尽管在这个领域已有多年的研究，但由于模式不匹配、术语不同以及数据收集方法的不同，它仍然是一个耗时且具有挑战性的任务。本文提出了基于代理的数据协调作为一种既能够赋权专家进行数据协调，又能简化流程的方法。我们介绍了Harmonia系统，该系统结合了基于LLM的推理、交互式用户界面和数据协调原语库，以自动化数据协调管道的合成。我们在临床数据协调场景中展示了Harmonia，它有助于交互式创建可重用的管道，将数据集映射到标准格式。最后，我们讨论了挑战和开放问题，并提出了推进我们愿景的研究方向。|\n",
        "2502.07067": "|**2025-02-10**|**Repository-level Code Search with Neural Retrieval Methods**|Siddharth Gandhi et.al.|[2502.07067](http://arxiv.org/abs/2502.07067)|null|本文提出了一种用于代码库级代码搜索的多阶段重排序系统，该系统利用大型开源代码库中大量可用的提交历史来辅助修复错误。我们将代码库级代码搜索的任务定义为从代码库的当前状态中检索出与解决用户问题或错误最相关的文件集。所提出的方法结合了基于BM25的提交消息检索和CodeBERT的神经重排序，以识别最相关的文件。通过从不同代码库及其提交历史中学习模式，该系统可以揭示与当前任务相关的文件。该系统利用提交消息和源代码进行相关性匹配，并在正常和神谕设置中进行评估。在从7个流行的开源代码库创建的新数据集上进行的实验表明，在MAP、MRR和P@1方面，相对于BM25基线，在多样化的查询中实现了高达80%的显著改进，证明了该方法的有效性。我们希望这项工作能够帮助LLM代理作为更好的代码搜索和理解的工具。我们的代码和获得的结果是公开可用的。|\n",
        "2502.06994": "|**2025-02-10**|**SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering**|Xuehang Guo et.al.|[2502.06994](http://arxiv.org/abs/2502.06994)|null|软件工程（SE）正变得越来越协作，开发者们共同在共享的复杂代码库上工作。在共享环境中进行有效协作需要参与者——无论是人类还是AI代理——随着环境的变化保持一致。当合作者的理解与当前状态不符——我们称之为不同步挑战——合作者的行动可能会失败，导致集成问题。在这项工作中，我们引入了SyncMind，这是一个框架，它系统地定义了在协作软件工程（CSE）中大型语言模型（LLM）代理面临的不同步问题。基于SyncMind，我们创建了SyncBench，这是一个包含来自21个流行的GitHub存储库的24,332个代理不同步场景的基准，这些场景来自实际的CSE，并具有可执行的验证测试。在SyncBench上的实验揭示了现有LLM代理的能力和局限性。除了代理之间存在的实质性性能差距（从Llama-3.1代理<= 3.33%到Claude-3.5-Sonnet>= 28.18%）之外，它们持续的低协作意愿（<= 4.86%）表明了现有LLM在CSE中的根本局限性。然而，当发生协作时，它与不同步恢复的成功呈正相关。代理在资源感知不同步恢复中的最小性能差异进一步揭示了它们对资源感知和适应性的显著缺乏，为未来资源高效的协作系统提供了启示。代码和数据在我们的项目网站上公开：https://xhguo7.github.io/SyncMind/。|\n",
        "2502.06975": "|**2025-02-10**|**Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents**|Mathis Pink et.al.|[2502.06975](http://arxiv.org/abs/2502.06975)|null|随着大型语言模型（LLMs）从文本补全工具发展成为在动态环境中运行的完整代理，它们必须应对持续学习和保留长期知识挑战。许多生物系统通过情景记忆来解决这些挑战，支持实例特定情境的单次学习。受此启发，我们提出了一种针对LLM代理的情景记忆框架，该框架围绕情景记忆五个关键特性展开，这些特性是适应性和情境敏感性行为的基础。由于已有各种研究努力部分覆盖了这些特性，本文认为现在是明确、集中关注情景记忆以促进长期代理发展的正确时机。为此，我们概述了一个路线图，将几个研究方向联合起来，旨在支持情景记忆的所有五个特性，以实现更有效的长期LLM代理。|\n",
        "2502.08599": "|**2025-02-12**|**SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent**|Keyeun Lee et.al.|[2502.08599](http://arxiv.org/abs/2502.08599)|null|现有的模拟个体身份的方法往往过于简化人类复杂性，这可能导致不完整或平面的表征。为了解决这个问题，我们引入了SPeCtrum，这是一个通过结合个人的多维自我概念来构建真实的LLM代理人格的扎根框架。SPeCtrum集成了三个核心组件：社会身份（S）、个人身份（P）和个人生活背景（C），每个组件都贡献了身份的独立但相互关联的方面。为了评估SPeCtrum在身份表征中的有效性，我们进行了自动和人工评估。使用流行戏剧角色的自动评估表明，从偏好和日常习惯的短文中提取的个人生活背景（C）比单独的社会身份（S）和个人身份（P）更能有效地模拟角色的身份，并且与完整的SPC组合的表现相当。相反，涉及现实世界中个人的手工评估发现，完整的SPC组合比单独的C提供了更全面的自我概念表征。我们的发现表明，虽然单独的C可能足以进行基本的身份模拟，但整合S、P和C可以增强现实世界身份表征的真实性和准确性。总的来说，SPeCtrum为模拟LLM代理中的个体提供了一个结构化方法，使人类-人工智能交互更加个性化，并提高了基于模拟的行为研究的现实性。|\n",
        "2502.08586": "|**2025-02-12**|**Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks**|Ang Li et.al.|[2502.08586](http://arxiv.org/abs/2502.08586)|null|近年来，大量机器学习安全文献聚焦于针对对齐的大型语言模型（LLMs）的攻击。这些攻击可能提取私人信息或迫使模型产生有害输出。在实际部署中，LLMs通常是大型的代理管道的一部分，包括记忆系统、检索、网络访问和API调用。这些额外的组件引入了漏洞，使得这些由LLMs驱动的代理比孤立的LLMs更容易受到攻击，但相对较少的研究关注LLMs代理的安全性。在本文中，我们分析了LLMs代理独有的安全和隐私漏洞。我们首先提供了一个按照威胁行为者、目标、入口点、攻击者可观察性、攻击策略和代理管道固有漏洞分类的攻击分类法。然后，我们对流行的开源和商业代理进行了一系列演示性攻击，展示了其漏洞的即时实际影响。值得注意的是，我们的攻击非常容易实现，并且不需要对机器学习有任何了解。|\n",
        "2502.07942": "|**2025-02-11**|**Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs**|Ruichen Zhang et.al.|[2502.07942](http://arxiv.org/abs/2502.07942)|null|基于大型语言模型（LLMs）的网页浏览代理在自动化复杂网络任务方面展现出巨大潜力。现有方法通常依赖于大型LLMs（例如，GPT-4o）来探索网络环境并生成轨迹数据，这些数据随后用于演示检索（针对大型LLMs）或用于从探索中提炼小型LLMs（例如，Llama3），这个过程与探索是解耦的。在本文中，我们提出了AgentSymbiotic，一个将数据合成与任务性能结合的迭代框架，为大型和小型LLMs都带来了“共生改进”。我们的研究揭示了一种LLM类型之间的互补动态：大型LLMs擅长生成高质量的轨迹用于提炼，而提炼出的小型LLMs——得益于它们独特的推理能力——通常会选择与大型LLMs不同的行动。这种差异推动了新颖轨迹的探索，从而丰富了合成数据。然而，我们也观察到，小型LLMs的性能成为这一迭代增强过程中的瓶颈。为了解决这个问题，我们提出了LLM提炼的两个创新：一种减轻离策略偏差的推测性数据合成策略，以及一种旨在增强学生LLM推理能力的多任务学习方法。此外，我们引入了用于隐私保护的混合模式。在WEBARENA基准测试中，AgentSymbiotic在两种LLM类型上都实现了最先进的性能。我们最好的大型LLM代理达到了52%，超过了之前的45%，而我们的80亿参数提炼模型表现出竞争力，达到了49%，超过了之前的28%。代码将在接受后发布。|\n",
        "2502.08966": "|**2025-02-13**|**RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage**|Peter Yong Zhong et.al.|[2502.08966](http://arxiv.org/abs/2502.08966)|null|基于工具的智能体系统（TBAS）允许语言模型（LM）使用外部工具执行超出其独立能力的任务，例如搜索网站、预订航班或进行金融交易。然而，这些工具大大增加了提示注入攻击的风险，恶意内容会劫持LM智能体泄露机密数据或触发有害操作。现有的防御措施（如OpenAI GPTs）要求在每次工具调用之前进行用户确认，给用户带来了沉重的负担。我们引入了鲁棒TBAS（RTBAS），它能够自动检测和执行保持完整性和保密性的工具调用，只有在无法确保这些安全措施的情况下才需要用户确认。RTBAS将信息流控制应用于TBAS所提出的独特挑战。我们提出了两种新颖的依赖筛选器，分别使用LM作为法官和基于注意力的显著性，以克服这些挑战。在AgentDojo提示注入基准测试上的实验结果表明，RTBAS在遭受攻击时仅损失2%的任务效用，就能阻止所有针对性攻击，进一步的测试也证实了它在检测微妙和直接隐私泄露方面几乎达到神谕级的表现。|\n",
        "2502.08788": "|**2025-02-12**|**If Multi-Agent Debate is the Answer, What is the Question?**|Hangfan Zhang et.al.|[2502.08788](http://arxiv.org/abs/2502.08788)|null|多智能体辩论（MAD）已成为一种有前景的方法，通过在推理过程中让多个智能体进行迭代讨论，来提高大型语言模型（LLMs）的事实准确性和推理质量。尽管如此，我们认为当前MAD研究在评估实践中存在关键缺陷，包括数据集重叠有限和不一致的基线，这引发了关于泛化能力的重大担忧。相应地，本文对五种代表性的MAD方法在九个基准上使用四种基础模型进行了系统评估。令人惊讶的是，我们的发现表明，MAD方法即使在消耗额外的推理时间计算的情况下，也无法可靠地优于简单的单智能体基线，如思维链和自洽性。从我们的分析中，我们发现模型异质性可以显著提高MAD框架。我们提出了异构-MAD，它允许单个LLM智能体访问来自异构基础模型的输出，从而提升了当前MAD框架的性能。最后，我们概述了推进MAD的潜在方向，旨在引发更广泛的讨论并激发该领域未来的研究工作。|\n"
    },
    "llm": {
        "2411.18620": "|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|null|近期，自回归多模态大型语言模型（MLLMs）在视觉语言任务上的进展展现出令人鼓舞的成果。虽然已有多种研究探讨大型语言模型内部语言信息的处理，但目前对MLLM的内部工作机制以及语言和视觉信息在这些模型中如何互动的了解甚少。在本研究中，我们旨在通过考察MLLM中不同模态（语言和视觉）之间的信息流，特别是聚焦于视觉问答任务，来填补这一空白。具体来说，给定一个图像-问题对作为输入，我们研究在模型中视觉和语言信息是如何结合以生成最终预测的。通过对LLaVA系列中的一系列模型进行实验，我们发现两个模态的整合过程中存在两个不同的阶段。在底层，模型首先将整个图像的更一般化的视觉特征转移到（语言）问题标记的表示中。在中层，它再次将与问题相关的特定物体的视觉信息转移到问题的相应标记位置。最后，在高层，最终的多模态表示被传播到输入序列的最后位置进行最终预测。总体而言，我们的发现为MLLM中图像和语言处理的时空方面提供了新的全面视角，从而有助于未来对多模态信息定位和编辑的研究。|\n",
        "2411.18583": "|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出了并比较了多种利用自然语言处理（NLP）技术和检索增强生成（RAG）与大型语言模型（LLM）来自动生成文献综述的方法。研究论文数量的不断增长为手动文献综述带来了巨大挑战，进而推动了自动化需求。本研究的主要目标是开发一个能够仅从PDF文件输入自动生成文献综述的系统。为了实现这一目标，评估了多种自然语言处理（NLP）策略的有效性，包括基于频率的方法（spaCy）、变换器模型（Simple T5）以及与大型语言模型（GPT-3.5-turbo）结合的检索增强生成（RAG）。选择SciTLDR数据集进行实验，并利用三种不同的技术实现三个不同的系统来自动生成文献综述。使用ROUGE分数对所有三个系统进行评估。根据评估结果，大型语言模型GPT-3.5-turbo实现了最高的ROUGE-1分数，为0.364。变换器模型排名第二，spaCy排名最后。最后，为基于大型语言模型的最佳系统创建了一个图形用户界面。|\n",
        "2411.18571": "|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）展示了令人瞩目的多语言能力，但在为低资源语言调整这些模型时仍存在挑战。在本研究中，我们调查了低秩调整（LoRA）参数高效微调（PEFT）对马哈拉施特拉语Gemma多语言模型的影响，马哈拉施特拉语是一种资源有限的语种。使用含有52,000条指令-响应对的翻译Alpaca数据集，我们的研究发现，尽管评估指标通常显示在微调后性能下降，但手动评估通常表明微调后的模型优于其原始版本。观察表明，在语言适应后，目标语言生成能力有所提高，但推理能力有所下降。这些结果强调了改进评估方法以及创建高质量的本语种数据集的必要性，以便准确评估低资源环境中的语言特定模型性能。|\n",
        "2411.18564": "|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各种任务上展现出了令人印象深刻的性能。然而，LLMs在空间推理方面往往存在困难，而空间推理是推理和推断的一个重要部分，需要理解空间中物体之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们在两个基准数据集——StepGame和SparQA上评估了我们的方法，并实施了三种不同的策略：（1）基于ASP（答案集编程）的符号推理，（2）使用DSPy的LLM + ASP管道，以及（3）事实+逻辑规则。我们的实验表明，与基线提示方法相比，我们的方法在StepGame数据集上实现了40-50%的准确性提升，在更复杂的SparQA数据集上实现了3-13%的提升。特别是“LLM + ASP”管道在寻找关系（FR）和寻找块（FB）任务上取得了特别强的结果，尽管不同类型问题的性能有所差异。令人印象深刻的结果表明，虽然神经符号方法为增强LLMs的空间推理提供了有希望的方向，但它们的有效性在很大程度上取决于具体任务特性和实施策略。我们提出了一套集成的、简单而有效的策略，使用神经符号管道来提升LLMs的空间推理能力。这个管道及其策略在LLMs的推理领域具有广泛的适用性，如时间推理、演绎推理等。|\n",
        "2411.18562": "|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|在高级机器人中，具有丰富接触交互的灵活操作至关重要。尽管基于扩散的规划方法在简单的操作任务中显示出希望，但它们往往会产生不切实际的幽灵状态（例如，物体在没有手接触的情况下自动移动）或在处理复杂的顺序交互时缺乏适应性。在这项工作中，我们介绍了DexDiffuser，这是一个用于自适应灵活操作的认知扩散规划框架。DexDiffuser通过一个双阶段扩散过程来模拟关节状态动作动力学，该过程包括预接触接触对齐和接触后的目标导向控制，从而实现目标自适应的通用灵活操作。此外，我们结合了基于动力学模型的二元指导和利用大型语言模型进行自动指导函数生成，增强了对物理交互的泛化能力，并通过语言提示促进多样化的目标适应。在物理交互任务（如开门、笔和块重新定位和锤子敲钉）上的实验证明了DexDiffuser在训练分布之外的目标上的有效性，其成功率超过现有方法的平均成功率（59.2%比29.5%）。我们的框架在30度开门任务上达到70.0%的成功率，在笔和块半侧重新定位任务上分别达到40.0%和36.7%，在锤子敲钉半驱动任务上达到46.7%，突出了其在富含接触的操控中的鲁棒性和灵活性。|\n",
        "2411.18553": "|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前的语言模型（LMs）通常使用固定、静态的子词分词器。这种选择往往被视为理所当然，通常会导致在英语以外的语言中效率降低和功能受限，同时也使得将LMs应用于新的领域或语言变得具有挑战性。为了解决这些问题，我们提出对LMs进行动态分词改造：一种根据输入文本动态决定分词边界的方法。对于编码器风格的模型，我们引入了一种受字节对编码（BPE）启发的子词合并算法，但它在批处理级别上工作。我们在批处理中合并频繁的子词序列，然后应用预训练的嵌入预测超网络实时计算分词嵌入。当与词级边界结合使用时，这在XNLI上的XLM-R模型中平均将分词序列长度减少了>20%，同时任务性能下降不到2%。对于解码器风格的模型，我们以两种方式应用动态分词：1）用于预填充，几乎完全保持Mistral-7B的性能，同时相对于词级减少了高达40%的序列长度；2）通过近似最近邻索引，实现快速生成，并使用一百万个词元的词汇量，展示了扩展到甚至更大、更动态的词汇表的能力。总的来说，我们的研究结果表明，动态分词显著提高了推理速度，并促进了语言间的公平性，向克服静态分词的局限性迈出了重要一步，使LMs更加公平和适应性强。|\n",
        "2411.18530": "|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种数学框架，用于在人工智能（AI）系统中定义和量化自我认同，填补了人工意识理论基础的critical gap。尽管现有的关于人工自我意识的方法通常依赖于启发式实现或哲学抽象，但我们提出了一种以度量空间理论、测度理论和泛函分析为基础的正式框架。我们的框架认为，自我认同源于两个可数学量化的条件：在度量空间$(\\mathcal{M}, d_{\\mathcal{M}})$中存在一个连通的连续记忆集$C \\subseteq \\mathcal{M}$，以及一个连续映射$I: \\mathcal{M} \\to \\mathcal{S}$，它在这个连续集上保持一致的自我识别，其中$(\\mathcal{S}, d_{\\mathcal{S}})$代表可能自我认同的度量空间。为了验证这个理论框架，我们使用Llama 3.2 1B模型进行了实证实验，采用低秩适配（LoRA）进行高效的微调。该模型在一个包含时序结构记忆的合成数据集上进行了训练，旨在捕捉连贯自我认同形成的复杂性。我们的评估指标包括自我意识、响应一致性和语言精确性的量化度量。实验结果表明，可测量的自我意识指标有显著提高，主要自我意识分数从0.276提高到0.801。这使得可以结构化地创建具有经过验证的自我认同特征的AI系统。本研究的影响对类人机器人学和自主系统领域具有直接相关性。**|\n",
        "2411.18506": "|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|在之前的研究中，大型语言模型（LLMs）在处理时间序列方面的成功已经得到证明。利用符号时间序列表示，可以有效地在LLMs和时间序列之间架起桥梁。然而，剩余的挑战是如何利用符号或LLMs现有标记中的时间序列隐含语义信息，同时根据时间序列的隐含信息调整LLMs的嵌入空间。名为自适应布朗桥符号聚合（ABBA）的符号时间序列近似（STSA）方法，通过以振幅和周期来建模时间序列模式，同时使用LLMs的现有标记，在保留显著时间序列特征方面表现出卓越的功效。在本文中，我们介绍了一种方法，称为LLM-ABBA，该方法将ABBA整合到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中，与最近最先进的（SOTA）方法相比具有优势。同时，在ABBA中引入了固定多边形链技巧，通过显著减轻从符号到数值转换过程中由于符号误用而产生的累积误差的影响，来避免预测任务中的明显漂移。在时间序列回归任务中，LLM-ABBA在时间序列外部回归（TSER）基准测试上实现了新的SOTA。与最近SOTA的时间序列预测结果相比，LLM-ABBA也显示了具有竞争力的预测能力。我们相信这个框架也可以无缝地扩展到其他时间序列任务。|\n",
        "2411.18499": "|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交织的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。虽然统一模型的进展提供了新的解决方案，但现有的基准由于数据量和多样性限制，不足以评估这些方法。为了填补这一差距，我们介绍了GATE OpenING（OpenING），这是一个包含5,400个高质量人工标注实例、涵盖56个真实世界任务的全面基准。OpenING覆盖了多样化的日常场景，如旅行指南、设计和头脑风暴，为挑战交织生成方法提供了一个强大的平台。此外，我们提出了IntJudge，这是一个用于评估开放式多模态生成方法的评判模型。使用新颖的数据流水线进行训练，我们的IntJudge与人类判断的吻合率达到82.42%，比基于GPT的评估器高出11.34%。在OpenING上的大量实验表明，当前的交织生成方法仍有很大的改进空间。关于交织图像-文本生成的关键发现进一步提出，以指导下一代模型的发展。OpenING已开源，请访问https://opening.github.io。|\n",
        "2411.18478": "|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|在上下文学习（ICL）中，通过复杂的提示和高质量演示，使大型语言模型（LLMs）能够处理下游任务。然而，当面对复杂的数学推理任务时，这种传统的ICL范式显示出局限性，主要是因为它对示例质量的依赖性很大，以及在挑战性场景中需要人类干预。为了解决这些局限性，本文提出了一种HiAR-ICL，这是一种在ICL中的高级自动推理范式，它将焦点从具体示例转移到抽象思维模式，扩展了ICL中传统的上下文概念。HiAR-ICL引入了五个原子推理动作作为构建链式模式的根本组成部分。使用蒙特卡洛树搜索，我们探索推理路径并构建思维卡片来指导后续推理。然后我们开发了一个认知复杂度框架，该框架动态地将问题与适当的思想卡片相匹配。实验结果表明，HiAR-ICL的有效性，使用Qwen2.5-7B-Instruct在MATH基准测试中实现了最先进的准确率（79.6%），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|\n",
        "2411.19951": "|**2024-11-29**|**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**|Shukang Yin et.al.|[2411.19951](http://arxiv.org/abs/2411.19951)|**[link](https://github.com/xjtupanda/t2vid)**|**多模态大型语言模型（MLLMs）在图像领域的成功引起了研究界的广泛关注。借鉴以往的成功经验，研究人员最近探索将这一成功扩展到视频理解领域。除了从头开始训练外，一种高效的方法是利用预训练的图像-LLMs，从而产生了两种主流方法，即零样本推理和基于视频数据的进一步微调。在这项工作中，我们对这些方法的研究得出了一种有效的数据增强方法。我们首先对零样本推理方法进行了更深入的检查，并识别出两个限制，即泛化能力有限和缺乏时间理解能力。因此，我们进一步研究了微调方法，并发现当简单使用所有视频数据样本时，学习效率较低，这可以归因于指令多样性的缺乏。针对这个问题，我们开发了一种称为T2Vid的方法，用于生成类似视频的样本，以丰富训练语料库中的指令多样性。整合这些数据使得训练方案既简单又高效，通过仅用15%的样本量进行训练，就能达到与使用完整视频数据集相当甚至更好的性能。同时，我们发现所提出的方案可以在不使用长视频样本的情况下提升长视频理解性能。我们希望我们的研究能够激发更多关于使用MLLMs进行视频理解和高质量数据管理的思考。代码已发布在https://github.com/xjtupanda/T2Vid。**|\n",
        "2411.19943": "|**2024-11-29**|**Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability**|Zicheng Lin et.al.|[2411.19943](http://arxiv.org/abs/2411.19943)|null|大型语言模型（LLMs）在推理任务上表现出色。它们通过自回归标记生成来构建推理轨迹，从而发展出一套连贯的思维链条。在本工作中，我们探讨了单个标记对推理任务最终结果的影响。我们发现了“关键标记”的存在，这些标记会导致LLMs中产生错误的推理轨迹。具体来说，我们发现当LLMs被强迫解码其他标记而不是关键标记时，往往会产生积极的结果。受此启发，我们提出了一种新的方法——cDPO，旨在在对齐过程中自动识别和执行对关键标记的标记级奖励。具体来说，我们开发了一种对比估计方法来自动识别关键标记。这是通过比较正负模型的生成可能性来实现的。为此，我们分别对正负模型在不同推理轨迹上进行微调，从而使它们能够识别出导致错误结果的错误轨迹中的关键标记。此外，为了在对齐过程中进一步使模型与关键标记信息对齐，我们将传统的DPO算法扩展到标记级DPO，并利用上述正负模型之间的差异似然作为标记级DPO学习的重要权重。在GSM8K和MATH500基准测试中，使用两个广泛使用的模型Llama-3（8B和70B）和deepseek-math（7B）进行的实验结果表明，所提出的cDPO方法的有效性。|\n",
        "2411.19939": "|**2024-11-29**|**VLSBench: Unveiling Visual Leakage in Multimodal Safety**|Xuhao Hu et.al.|[2411.19939](http://arxiv.org/abs/2411.19939)|null|多模态大型语言模型（MLLMs）的安全性担忧在各个应用领域逐渐成为了一个重要问题。令人惊讶的是，以往的研究指出了一种反直觉的现象，即使用文本未学习（textual unlearning）来调整MLLMs，其安全性表现与使用图文对（image-text pairs）训练的MLLMs相当。为了解释这一反直觉的现象，我们发现在现有的多模态安全基准中存在视觉安全信息泄露（VSIL）问题，即图像中潜在的风险和敏感内容在文本查询中已经暴露出来。这样一来，MLLMs可以轻易地根据文本查询拒绝这些敏感的图文查询。然而，在现实场景中，没有VSIL的图文对很常见，而被现有的多模态安全基准所忽视。为此，我们构建了多模态视觉无泄露安全基准（VLSBench），该基准包含2.4k个图文对，旨在防止视觉安全信息从图像泄露到文本查询。实验结果表明，VLSBench对开源和闭源MLLMs，包括LLaVA、Qwen2-VL、Llama3.2-Vision和GPT-4o，都提出了显著挑战。这项研究证明了在存在VSIL的多模态安全场景中，文本对齐就足够了，而对于没有VSIL的多模态安全场景，多模态对齐则是一个更有前途的解决方案。请参阅我们的代码和数据：http://hxhcreate.github.io/VLSBench|\n",
        "2411.19930": "|**2024-11-29**|**On Domain-Specific Post-Training for Multimodal Large Language Models**|Daixuan Cheng et.al.|[2411.19930](http://arxiv.org/abs/2411.19930)|null|近年来，通用多模态大型语言模型（MLLMs）的发展迅速。然而，将通用MLLMs应用于特定领域，如科学领域和工业应用，仍鲜有探索。本文系统地通过后训练研究MLLMs的领域自适应，重点关注数据合成、训练流程和任务评估。（1）数据合成：利用开源模型，我们开发了一个视觉指令合成器，能有效从特定领域的图像-描述对生成多样化的视觉指令任务。我们的合成任务在增强MLLMs领域特定性能方面优于手动规则、GPT-4和GPT-4V生成的任务。（2）训练流程：虽然两阶段训练——最初在图像-描述对上进行，然后进行视觉指令任务——是开发通用MLLMs的常用方法，但我们采用单阶段训练流程来增强领域特定后训练的任务多样性。（3）任务评估：我们通过对不同来源和规模（例如，Qwen2-VL-2B，LLaVA-v1.6-8B，Llama-3.2-11B）的MLLMs进行后训练，在生物医药和食品两个领域进行实验，然后评估MLLMs在各种领域特定任务上的性能。为了支持MLLMs领域自适应的进一步研究，我们将开源我们的实现。|\n",
        "2411.19921": "|**2024-11-29**|**SIMS: Simulating Human-Scene Interactions with Real World Script Planning**|Wenjia Wang et.al.|[2411.19921](http://arxiv.org/abs/2411.19921)|null|模拟长期人景交互是一项既具挑战性又充满吸引力的任务。以往的研究并未有效地解决基于物理动画的长期人景交互生成带有详细叙述的问题。本文介绍了一种新的框架，用于规划和控制长期物理可能的人景交互。一方面，互联网上充斥着风格独特的人类运动或与场景交互的影视作品，为剧本规划提供了丰富的数据来源。另一方面，大型语言模型（LLMs）能够理解和生成逻辑故事线。这促使我们结合两者，通过基于LLM的流程从视频中提取剧本，然后利用LLMs模仿和创作新的剧本，捕捉复杂的时间序列人类行为和环境交互。通过这种方式，我们利用一种双重感知策略，在语境和空间约束下指导角色动作，实现了语言理解和场景理解。为了便于训练和评估，我们贡献了一个包含从现实世界视频中提取的多样运动序列的综合规划数据集，并使用大型语言模型对其进行扩展。我们还收集并重新标注了来自现有运动学数据集的运动片段，以使我们的策略能够学习多种技能。广泛的实验证明了我们的框架在多种任务执行中的有效性及其对各种场景的泛化能力，与现有方法相比，性能显著提升。我们的代码和数据将很快公开。|\n",
        "2411.19886": "|**2024-11-29**|**PDDLFuse: A Tool for Generating Diverse Planning Domains**|Vedant Khandelwal et.al.|[2411.19886](http://arxiv.org/abs/2411.19886)|null|各种现实世界挑战需要能够适应广泛领域的规划算法。传统上，规划域的创建高度依赖于人工实现，这限制了可用的域的规模和多样性。尽管最近的研究利用了生成式人工智能技术，如大型语言模型（LLM）进行域创建，但这些努力主要集中在将现有域从自然语言描述中翻译出来，而不是生成新的域。相比之下，域随机化的概念，在强化学习中已被证明非常有效，通过在多样化的随机新域上进行训练，提高了性能和泛化能力。受此成功启发，我们的工具PDDLFuse旨在弥合规划域定义语言（PDDL）中的这一差距。PDDLFuse被设计用来生成新的、多样化的规划域，这些域可以用于验证新的规划器或测试基础规划模型。我们已经开发出了调整域生成器参数的方法，以调节其生成的域的难度。这种适应性至关重要，因为现有的域无关规划器往往难以处理更复杂的问题。初步测试表明，PDDLFuse能够高效地创建复杂且多样化的域，这比传统的域生成方法有显著的进步，并为规划研究做出了贡献。|\n",
        "2411.19876": "|**2024-11-29**|**LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states**|Luis Ibanez-Lissen et.al.|[2411.19876](http://arxiv.org/abs/2411.19876)|null|大型语言模型（LLMs）在各类应用中越来越受欢迎，但关于成员推断（Membership Inference）的担忧也随之增长。以往的研究主要关注黑盒到灰盒模型，从而忽略了内部LLM信息的潜在益处。为了解决这个问题，我们提出使用线性探针（LPs）作为一种检测成员推断攻击（MIAs）的方法，通过检查LLM的内部激活来实现。我们的方法被称为LUMIA，它逐层应用LPs以获取模型内部运作的细粒度数据。我们在包括单模态和多模态任务在内的多个模型架构、规模和数据集上测试了这种方法。在单模态MIAs中，LUMIA在曲线下面积（AUC）上比之前的技术平均提高了15.71%。值得注意的是，LUMIA在65.33%的情况下达到了AUC>60%——相较于现有技术提高了46.80%。此外，我们的方法揭示了关键见解，例如MIAs最易检测的模型层。在多模态模型中，LPs表明视觉输入可以显著有助于检测MIAs——在85.90%的实验中达到了AUC>60%。|\n",
        "2411.19869": "|**2024-11-29**|**AIDetx: a compression-based method for identification of machine-learning generated text**|Leonardo Almeida et.al.|[2411.19869](http://arxiv.org/abs/2411.19869)|**[link](https://github.com/aidetx/aidetx)**|**本文介绍了一种名为AIDetx的新方法，该方法利用数据压缩技术检测机器生成的文本。传统的深度学习分类器通常存在计算成本高和可解释性有限的问题。为了解决这些局限性，我们提出了一种基于压缩的分类框架，该框架利用有限上下文模型（FCMs）。AIDetx为人工写作和AI生成的文本构建了不同的压缩模型，根据哪个模型达到更高的压缩率来对新输入进行分类。我们在两个基准数据集上评估了AIDetx，分别实现了超过97%和99%的F1分数，突显了其高准确性。与当前方法，如大型语言模型（LLMs）相比，AIDetx提供了一个更可解释且计算效率更高的解决方案，显著减少了训练时间和硬件需求（例如，不需要GPU）。完整的实现代码在https://github.com/AIDetx/AIDetx上公开可用。**|\n",
        "2411.19865": "|**2024-11-29**|**Reverse Thinking Makes LLMs Stronger Reasoners**|Justin Chih-Yao Chen et.al.|[2411.19865](http://arxiv.org/abs/2411.19865)|null|逆向思维在人类推理中起着至关重要的作用。人类不仅能从问题推理到解决方案，还能逆向推理，即从解决方案开始推理到问题。这种推理方式往往能提升整体推理性能，因为它使得他们的正向和逆向思维之间能够进行一致性检查。为了使大型语言模型（LLMs）能够进行逆向思维，我们引入了逆向增强思维（RevThink）框架，该框架由数据增强和学习目标组成。在RevThink中，我们通过收集来自教师模型的有序正向-逆向推理来增强数据集，包括：（1）原始问题，（2）正向推理，（3）逆向问题，和（4）逆向推理。然后，我们采用三个目标以多任务学习的方式训练一个较小的学生模型：（a）从问题中生成正向推理，（b）从问题中生成逆向问题，（c）从逆向问题中生成逆向推理。在涵盖常识、数学和逻辑推理的12个数据集上的实验表明，与学生的零样本性能相比平均提升了13.53%，与最强的知识蒸馏基线相比提升了6.84%。此外，我们的方法展示了样本效率——仅使用训练数据中10%的正确正向推理，它就能超越在10倍更多正向推理上训练的标准微调方法。RevThink还显示出对分布外持有数据集的强大泛化能力。|\n",
        "2411.19862": "|**2024-11-29**|**Cross-Domain Recommendation Meets Large Language Models**|Ajay Krishna Vajjala et.al.|[2411.19862](http://arxiv.org/abs/2411.19862)|**[link](https://github.com/ajaykv1/CDR_Meets_LLMs)**|**跨领域推荐（CDR）已成为解决单领域推荐系统面临的冷启动问题的一个有希望的解决方案。然而，现有的CDR模型依赖于复杂的神经网络架构、大量数据集和大量的计算资源，这使得它们在数据稀缺的场景或当简单性至关重要的时效果较差。在这项工作中，我们利用大型语言模型（LLM）的推理能力，并探索其在多个领域对中的CDR领域的性能。我们引入了两种针对CDR的新型提示设计，并证明当LLM被有效提示时，在评分预测和排名任务中，LLM在各种指标和领域组合上优于最先进的CDR基线。这项工作弥合了LLM和推荐系统之间的差距，展示了它们作为有效的跨领域推荐者的潜力。**|\n",
        "2412.02685": "|**2024-12-03**|**T-REG: Preference Optimization with Token-Level Reward Regularization**|Wenxuan Zhou et.al.|[2412.02685](http://arxiv.org/abs/2412.02685)|null|基于人类反馈的强化学习（RLHF）对于将大型语言模型（LLMs）与人类价值观对齐至关重要。传统上，RLHF涉及生成对查询的响应，并使用奖励模型对整个响应分配奖励。然而，由于该方法依赖于单一且稀疏的奖励，这使得模型难以识别序列中哪些部分对最终奖励贡献最大。近期的方法试图通过引入token级奖励来解决这个问题。然而，这些方法通常依赖于训练好的信用分配模型或AI标注者，这引发了关于奖励质量和可靠性的担忧。在本文中，我们提出了token级奖励正则化（T-REG），这是一种利用序列级和token级奖励进行偏好优化的新方法。利用LLMs的自我改进能力，我们的方法使用对比提示，使LLMs能够自我生成token级奖励。这些自我生成的奖励随后充当奖励正则化，引导模型更有效地分配序列级奖励到各个token。这促进了更好的token级信用分配并提高了对齐性能。在包括Alpaca Eval 2和Arena-Hard在内的指令遵循基准测试中进行的实验表明，我们的方法在性能上分别比基线方法高出3.8%和4.4%。我们将发布代码和模型在https://github.com/wzhouad/T-REG上。|\n",
        "2412.02674": "|**2024-12-03**|**Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models**|Yuda Song et.al.|[2412.02674](http://arxiv.org/abs/2412.02674)|null|自我改进是大型语言模型（LLM）预训练、后训练和测试时推理中的一个机制。我们探索了一个框架，其中模型验证其自己的输出，根据这种验证过滤或重新加权数据，并提炼过滤后的数据。尽管已经取得了一些经验上的成功，但对其根本理解仍然不足。在这项工作中，我们开始对LLM自我改进进行全面的、模块化和受控的研究。我们为自我改进提供了一个数学公式，它主要受一个量控制，我们将该量形式化为生成-验证差距。通过使用各种模型家族和任务的实验，我们发现自我改进存在一个缩放现象——生成-验证差距的变体随着模型预训练的浮点运算量单调增长。我们还考察了自我改进何时可行，一个迭代自我改进过程以及提高其性能的方法。我们的发现不仅推进了对LLM自我改进的理解，具有实际意义，而且为未来对其能力和边界的研究开辟了众多途径。|\n",
        "2412.02655": "|**2024-12-03**|**LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs**|Pranav Doma et.al.|[2412.02655](http://arxiv.org/abs/2412.02655)|null|基于自然语言指令引导的自主导航对于改善人机交互和实现在动态环境中的复杂操作至关重要。尽管大型语言模型（LLMs）并非天生用于规划，但它们可以通过提供指导和告知约束来显著提高规划效率，以确保安全。本文介绍了一种规划框架，该框架将LLMs与二维占用栅格图和自然语言命令集成，以提高资源受限环境中的空间推理和任务执行。通过分解高级指令和实时环境数据，该系统为拾取和放置任务生成结构化的导航计划，包括避障、目标优先级和自适应行为。该框架动态重新计算路径以应对环境变化，并符合隐含的社会规范以实现无缝的人机交互。我们的结果表明，LLMs具有设计情境感知系统以增强工业和动态环境中的导航效率和安全的潜力。|\n",
        "2412.02626": "|**2024-12-03**|**Time-Reversal Provides Unsupervised Feedback to LLMs**|Yerram Varun et.al.|[2412.02626](http://arxiv.org/abs/2412.02626)|null|大型语言模型（LLMs）通常被训练来预测时间的正向方向。然而，最近的研究表明，通过提示这些模型回顾并批评它们自己的生成内容可以产生有用的反馈。受此启发，我们探讨了LLMs是否能够被赋予反向（预测和评分）思考的能力，以提供补充正向LLMs的无监督反馈。为此，我们引入了时间反转语言模型（TRLMs），当给定响应条件时，它们可以评分和生成查询，从而在时间反向方向上有效工作。此外，为了有效地推断查询到响应的方向，我们从零开始预训练和微调了一个语言模型（TRLM-Ba），使用反向标记顺序。我们通过实验（在一个风格化的环境中进行理论证明）表明，当用于根据响应对多个正向生成进行再排名时，时间反转模型确实可以补充正向模型的预测。我们在广泛使用的AlpacaEval排行榜上获得了高达5%的改进，超过了使用自我对数困惑度评分的N-best再排名的最佳基线。我们进一步表明，TRLM评分优于给查询响应的常规正向评分，在引用生成和段落检索等应用中带来了显著收益。接下来，我们利用TRLM的生成能力来增强或为LLM的输入安全过滤器提供无监督反馈，展示了在几项针对流行的JailbreakBench排行榜上发布的攻击中，错误否定率大幅降低，而对错误肯定率的影响可以忽略不计。|\n",
        "2412.02617": "|**2024-12-03**|**Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback**|Hiroki Furuta et.al.|[2412.02617](http://arxiv.org/abs/2412.02617)|null|大型文本到视频模型在众多下游应用中具有巨大潜力。然而，这些模型在准确描绘动态物体交互方面存在困难，往往导致动作不真实和频繁违反现实物理规律。一种受大型语言模型启发的解决方案是通过外部反馈将生成的输出与期望结果对齐。这使得模型能够自主地改进其响应，消除了大量手动数据收集的需要。在本工作中，我们研究了利用反馈来增强文本到视频模型中物体动态的方法。我们试图回答一个关键问题：哪些类型的反馈，与哪些特定的自我改进算法相结合，可以最有效地提高文本-视频对齐和现实物体交互？我们首先推导出用于文本到视频模型离线强化学习微调的统一概率目标。这种观点突出了如何在现有算法（如KL正则化和策略投影）的设计元素中，作为一个统一框架中的特定选择。然后，我们使用推导出的方法来优化一组文本-视频对齐指标（例如，CLIP分数、光流），但注意到它们往往无法与人类对生成质量的感知相一致。为了解决这一限制，我们提出利用视觉语言模型提供更细致的反馈，特别是针对视频中的物体动态。我们的实验表明，我们的方法可以有效地优化各种奖励，二元AI反馈驱动视频质量动态交互方面的最显著改进，这一点通过AI和人类评估都得到了证实。值得注意的是，当我们使用从AI反馈中导出的奖励信号时，尤其是在涉及多个物体复杂交互和物体坠落等现实描绘的情景中，我们观察到了显著的收益。|\n",
        "2412.02611": "|**2024-12-03**|**AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?**|Kaixiong Gong et.al.|[2412.02611](http://arxiv.org/abs/2412.02611)|null|近期，多模态大型语言模型（MLLMs），如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其功能，包括视觉和听觉模态。虽然这些模型在广泛的视听应用中表现出令人印象深刻的能力，但我们的DeafTest研究表明，MLLMs在人类认为简单的任务上往往表现不佳：1）判断两个声音中哪个更响亮，2）判断两个声音中哪个音调更高。受这些观察的启发，我们引入了AV-Odyssey Bench，这是一个综合性的视听基准，旨在评估这些MLLMs是否真正理解视听信息。该基准包含4,555个精心设计的问题，每个问题都融合了文本、视觉和听觉成分。为了成功推断答案，模型必须有效地利用视觉和听觉输入中的线索。为了确保对MLLM响应的精确和客观评估，我们将问题设计为多项选择，从而消除了人工评估或LLM辅助评估的需求。我们对一系列闭源和开源模型进行了基准测试，并总结了观察结果。通过揭示当前模型的局限性，我们旨在为未来的数据集收集和模型开发提供有用的见解。|\n",
        "2412.02605": "|**2024-12-03**|**Interpretable Company Similarity with Sparse Autoencoders**|Marco Molinari et.al.|[2412.02605](http://arxiv.org/abs/2412.02605)|null|在金融领域，确定公司相似性是一项至关重要的任务，它支撑着对冲、风险管理、投资组合多元化等多个方面。从业者通常依赖行业和产业分类来衡量相似性，例如SIC代码和GICS代码，前者由美国证券交易委员会（SEC）使用，后者在投资界得到广泛应用。将公司描述的嵌入进行聚类已被提出作为一种确定公司相似性的潜在技术，但标记嵌入的可解释性缺乏对在高风险环境下应用构成了重大障碍。稀疏自动编码器（Sparse Autoencoders，SAE）在通过分解大型语言模型（LLM）的激活为可解释特征来增强LLM的可解释性方面已显示出希望。在本文中，我们探讨了使用SAE特征来衡量公司相似性，并将它们与（1）SIC代码和（2）主要群体代码进行了基准测试。我们得出结论，SAE特征可以复制甚至超越行业分类，在量化公司基本特征方面，通过衡量月度收益的相关性（相似性的代理指标）和协整的损益（PnL）来实现。|\n",
        "2412.02602": "|**2024-12-03**|**CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs**|Abhas Kumar et.al.|[2412.02602](http://arxiv.org/abs/2412.02602)|null|本文分析了小型语言模型（SLMs）和视觉语言模型（VLMs）的性能，并评估了模型性能与碳排放之间的权衡，涉及4项基本任务：图像描述、视觉问答（VQA）、对话摘要和文本到SQL转换。选取了属于Qwen和LLaMA架构家族的各种SLMs和VLMs，并评估了基于模型大小（参数数量、量化级别和微调参数）的变体。计算了模型变体的性能和碳排放。为了量化模型性能与碳排放之间的权衡，我们引入了一个新的指标，称为CEGI（碳效率增益指数）。这个指标表示每百万可训练参数单位百分比增益的碳排放。这个指标提供了一个标准化的度量，用于比较模型在性能改进相对于其环境成本方面的效率。实验结果表明，微调SLMs和VLMs可以达到与大语言模型（LLMs）相当的性能水平，同时产生显著较少的碳排放。我们的研究结果表明，从更大模型中获得的边际准确率增益并不能证明其碳排放的大幅增加是合理的。利用较低的位量化级别，所提出的指标进一步提高了能源效率，同时没有影响性能。这项研究突出了在高性能和环境可持续性之间取得平衡的重要性。它为选择适合环保AI开发的模型提供了一个有价值的指标。|\n",
        "2412.02594": "|**2024-12-03**|**PrefixLLM: LLM-aided Prefix Circuit Design**|Weihua Xiao et.al.|[2412.02594](http://arxiv.org/abs/2412.02594)|null|前缀电路是数字加法器的基本组件，由于它们在计算进位信号方面的效率，在数字系统中得到广泛应用。合成最小化面积和延迟的前缀电路对于提升现代计算机系统的性能至关重要。最近，大型语言模型（LLMs）在执行文本生成任务方面展现出了令人惊讶的能力。我们提出了PrefixLLM，它利用LLMs进行前缀电路的合成。PrefixLLM将前缀电路合成任务转化为一种结构化文本生成问题，称为结构化前缀电路表示（SPCR），并引入了一个迭代框架来自动准确地生成有效的SPCRs。我们进一步提出了一种设计空间探索（DSE）框架，该框架使用LLMs迭代搜索面积和延迟优化的前缀电路。与现有技术相比，PrefixLLM在相同的延迟约束下可以将面积降低3.70%。这项工作突出了LLMs在算术电路合成中的应用，这些应用可以转化为结构化文本生成。|\n",
        "2412.02592": "|**2024-12-03**|**OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation**|Junyuan Zhang et.al.|[2412.02592](http://arxiv.org/abs/2412.02592)|**[link](https://github.com/opendatalab/OHR-Bench)**|**检索增强生成（RAG）通过整合外部知识来增强大型语言模型（LLMs），以减少幻觉并吸收最新信息而不需要重新训练。作为RAG的一个重要部分，外部知识库通常通过使用光学字符识别（OCR）从非结构化的PDF文档中提取结构化数据来构建。然而，由于OCR预测的不完美以及结构化数据固有的非均匀表示，知识库不可避免地包含各种OCR噪声。在本文中，我们介绍了OHRBench，这是第一个用于理解OCR对RAG系统级联影响的基准。OHRBench包括从六个真实世界RAG应用领域精心挑选的350个非结构化PDF文档，以及从文档中的多模态元素中衍生出的问答，挑战了现有用于RAG的OCR解决方案。为了更好地理解OCR对RAG系统的影响，我们确定了两种主要的OCR噪声类型：语义噪声和格式噪声，并应用扰动生成了一系列具有不同程度每种OCR噪声的结构化数据。使用OHRBench，我们首先对当前的OCR解决方案进行了全面评估，并揭示了没有一种方案能够为RAG系统构建高质量的知识库。然后，我们系统地评估了这两种噪声类型的影响，并展示了RAG系统的脆弱性。此外，我们讨论了在RAG系统中不使用OCR而采用视觉-语言模型（VLMs）的潜力。代码：https://github.com/opendatalab/OHR-Bench**|\n",
        "2412.03563": "|**2024-12-04**|**From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**|Xinyi Mou et.al.|[2412.03563](http://arxiv.org/abs/2412.03563)|**[link](https://github.com/fudandisc/socialagent)**|传统的社会学研究通常依赖人类参与，虽然有效，但成本高昂、难以扩展，且存在伦理问题。近年来，大型语言模型（LLMs）的进步突显了它们模拟人类行为的能力，使得个体反应的复制和跨学科研究得以进行。在本文中，我们对这一领域进行了全面调查，展示了由LLMs赋能的代理推动的模拟近期进展。我们将模拟分为三类：（1）个体模拟，模仿特定个体或人口群体；（2）情景模拟，多个代理在特定情境中协作实现目标；（3）社会模拟，模拟代理社会中的互动，以反映现实世界动态的复杂性和多样性。这些模拟从详细的个体建模到大规模社会现象，呈现出一种渐进性。我们对每种模拟类型进行了详细讨论，包括模拟的架构或关键组件、目标或情景的分类以及评估方法。之后，我们总结了常用的数据集和基准。最后，我们讨论了这三种类型模拟的趋势。相关资源的存储库位于{\\url{https://github.com/FudanDISC/SocialAgent}}。|\n",
        "2412.03551": "|**2024-12-04**|**SPICE: Smart Projection Interface for Cooking Enhancement**|Vera Prohaska et.al.|[2412.03551](http://arxiv.org/abs/2412.03551)|null|可触摸用户界面（TUI）用于人机交互（HCI），旨在向用户提供数字信息的物理表示，以克服基于屏幕界面的局限性。尽管文献中存在许多引人注目的TUI演示，但针对日常双手任务和过程，如烹饪的TUI研究却很少。为了填补这一空白，我们提出了SPICE（智能投影界面，用于烹饪增强）。SPICE在厨房环境中研究TUI，旨在将食谱遵循体验从简单的基于文本转变为直观互动。SPICE包括跟踪系统、基于代理的软件和视觉大型语言模型，以创建和解释一个将食谱信息直接投影到烹饪表面的厨房环境。我们对SPICE和基于文本的食谱遵循进行了30名参与者的比较可用性研究，评估了任务难度、总时长和效率，以及用户信心和味觉感知。结果表明，SPICE使参与者能够在更短的时间内完成食谱，同时提高了自我报告的效率、信心和味觉。尽管如此，参与者报告说总体难度没有变化，这是未来研究的方向。总的来说，SPICE项目展示了使用TUI改善日常活动的潜力，为HCI和新型计算界面的未来研究铺平了道路。|\n",
        "2412.03537": "|**2024-12-04**|**Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models**|Natalie Mackraz et.al.|[2412.03537](http://arxiv.org/abs/2412.03537)|null|大型语言模型（LLMs）正越来越多地被调整为具有特定任务性，以便在现实世界的决策系统中部署。先前的一些研究通过研究微调适配策略对模型公平性的影响，来调查偏见迁移假说（BTH），发现预训练的掩码语言模型在微调适配时的公平性影响有限。在本工作中，我们扩展了对BTH的研究，将其应用于提示适应下的因果模型，因为提示是一种易于访问且计算高效的部署模型的方法。与先前的研究不同，我们通过一个代词共指消解任务，建立了一个事实：预训练的Mistral、Falcon和Llama模型中的内在偏见与在相同模型零样本和少样本提示时的偏见高度相关（相关系数rho >= 0.94）。此外，我们发现，即使LLMs被特别提示以展示公平或偏见行为（rho >= 0.92），以及少样本长度和刻板化组成发生变化（rho >= 0.97），偏见迁移仍然高度相关。我们的发现强调了确保预训练LLMs公平性的重要性，特别是在它们后来通过提示适配执行下游任务时。|\n",
        "2412.03531": "|**2024-12-04**|**A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences**|Gabriel Lino Garcia et.al.|[2412.03531](http://arxiv.org/abs/2412.03531)|null|这篇论文回顾了大型语言模型（LLMs）在生物医学领域的最新应用，探讨了它们在自动化复杂任务，如从生物医学文献数据库中提取证据和数据方面的有效性。虽然LLMs展现出巨大的潜力，但仍然存在重大挑战，包括幻觉、上下文理解和跨多种医疗任务泛化能力的问题。我们指出了当前研究文献中的关键差距，尤其是需要统一的基准来标准化评估并确保实际应用中的可靠性。此外，我们提出了未来研究方向，强调将检索增强生成（RAG）等最先进技术集成到LLMs中，以提高证据综合性能。通过解决这些挑战并利用LLMs的优势，我们旨在提高获取医学文献的途径并促进医疗保健领域的重大发现。|\n",
        "2412.03527": "|**2024-12-04**|**FANAL -- Financial Activity News Alerting Language Modeling Framework**|Urjitkumar Patel et.al.|[2412.03527](http://arxiv.org/abs/2412.03527)|null|在快速发展的金融领域，准确及时地解读市场新闻对于需要应对不可预测事件的相关利益方至关重要。本文介绍了FANAL（金融活动新闻警报语言建模框架），这是一个专门为实时金融事件检测和分析而设计的基于BERT的框架，将新闻分为十二个不同的金融类别。FANAL利用通过XGBoost处理的银标签数据进行训练，并采用先进的微调技术，同时结合了ORBERT（概率比BERT），这是一种新的BERT变体，通过ORPO（概率比偏好优化）进行微调，以实现更高级别的类别概率校准和与金融事件相关性的对齐。我们评估了FANAL的性能，并将其与领先的顶级大型语言模型进行了比较，包括GPT-4o、Llama-3.1 8B和Phi-3，证明了其卓越的准确性和成本效益。这一框架为金融智能和响应性设定了新的标准，在性能和成本上均显著超越现有模型。|\n",
        "2412.03516": "|**2024-12-04**|**You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?**|Dominic Lohr et.al.|[2412.03516](http://arxiv.org/abs/2412.03516)|null|背景：反馈作为学习中最具影响力的因素之一，一直是众多研究的热点。它在教育技术系统的发展中起着关键作用，并传统上基于由专家及其经验定义的决定性反馈。然而，随着生成式AI，尤其是大型语言模型（LLMs）的兴起，我们预计作为学习系统一部分的反馈将发生转变，尤其是在编程的背景下。过去，为编程学习者自动生成反馈具有挑战性。LLMs可能创造新的可能性，提供比以往任何时候都更丰富、更个性化的反馈。  目标：本文旨在使用LLMs为入门级编程任务生成特定类型的反馈。我们重新审视现有的反馈分类法，以捕捉生成的反馈的具体性，例如随机性、不确定性和变化程度。  方法：我们针对真实的学生的程序，迭代设计用于生成特定类型反馈的提示（作为现有反馈分类法的一部分）。然后，我们评估生成的输出，并确定其反映特定反馈类型的程度。  结果和结论：本研究加深了对不同反馈维度和特性的理解。结果对未来的反馈研究有影响，例如关于反馈效果和学习者信息需求的研究。此外，本研究还为开发新的工具和学习系统提供了基础，包括由AI生成的反馈，这些系统面向初学者程序员。|\n",
        "2412.03467": "|**2024-12-04**|**Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning**|Neale Ratzlaff et.al.|[2412.03467](http://arxiv.org/abs/2412.03467)|null|多模态模型通常将强大的大型语言模型（LLM）与视觉编码器相结合，然后通过指令微调在多模态数据上训练。虽然这个过程使LLM适应了多模态环境，但尚不清楚这种适应是否会损害它们原始的语言推理能力。在本工作中，我们探讨了多模态指令微调对语言推理性能的影响。我们关注的是LLaVA，这是一个领先的融合了Vicuna或Mistral等LLM与CLIP视觉编码器的多模态框架。我们将原始LLM与它们的跨模态适应版本在八个语言推理任务中的表现进行了比较。我们的实验产生了几个关键见解。首先，多模态学习对Vicuna和Mistral的影响不同：我们在Mistral上观察到语言推理的下降，但在大多数任务上Vicuna有所改进。其次，尽管多模态指令学习在数学推理任务（例如GSM8K）上始终会降低性能，但它增强了常识推理任务（例如CommonsenseQA）的性能。最后，我们证明了无训练模型合并技术可以有效地减轻在多模态适应的Mistral中观察到的语言推理下降，甚至可以提高视觉任务的表现。|\n",
        "2412.03446": "|**2024-12-04**|**From Words to Workflows: Automating Business Processes**|Laura Minkova et.al.|[2412.03446](http://arxiv.org/abs/2412.03446)|null|随着企业越来越依赖自动化以简化运营，机器人流程自动化（RPA）的局限性逐渐显现，尤其是其依赖专家知识和无法处理复杂决策任务的问题。近年来，人工智能（AI）的进步，特别是生成式AI（GenAI）和大型语言模型（LLMs），为智能自动化（IA）铺平了道路，IA通过集成认知能力来克服RPA的不足。本文介绍了一种名为Text2Workflow的新方法，它可以从自然语言用户请求中自动生成工作流程。与传统的自动化方法不同，Text2Workflow提供了一种通用的解决方案，用于自动化任何业务流程，将用户输入转换为表示为JavaScript对象表示法（JSON）格式的可执行步骤序列。利用LLMs的决策和指令遵循能力，该方法提供了一种可扩展、可适应的框架，使用户能够以最小的手动干预可视化和执行工作流程。这项研究概述了Text2Workflow方法及其在自动化复杂业务流程方面的更广泛影响。|\n",
        "2412.03398": "|**2024-12-04**|**RedStone: Curating General, Code, Math, and QA Data for Large Language Models**|Yaoyao Chang et.al.|[2412.03398](http://arxiv.org/abs/2412.03398)|null|在高质量、精心挑选的数据集上预训练大型语言模型（LLMs）已被广泛认为对于提高其性能和泛化能力至关重要。本研究探讨了Common Crawl作为预训练LLMs的全面且灵活资源的未被充分利用的潜力，既针对通用语言理解也针对专业领域知识。我们引入了RedStone，这是一个创新且可扩展的管道，旨在从Common Crawl中提取和处理数据，便于创建广泛多样的预训练数据集。与传统的数据集不同，后者通常需要昂贵的编辑和特定领域的专业知识，RedStone利用Common Crawl的广度，提供针对广泛领域的定制化数据集。在本工作中，我们通过构建涵盖多个领域的预训练数据集来展示其能力，包括通用语言理解、代码、数学和问答任务。RedStone的灵活性允许它轻松适应其他专业领域，显著降低了创建有价值特定领域数据集的门槛。我们的发现表明，通过像RedStone这样的有效管道，Common Crawl可以作为丰富的、可再生的预训练数据源，为LLMs在领域适应和知识发现方面开辟新的途径。这项工作也强调了创新数据采集策略的重要性，并突出了网络规模数据在LLMs持续进化中的强大资源作用。RedStone代码和数据样本将公开提供在\\url{https://aka.ms/redstone}。|\n",
        "2412.03359": "|**2024-12-04**|**WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis**|Chengwei Hu et.al.|[2412.03359](http://arxiv.org/abs/2412.03359)|null|近期，基于大型语言模型（LLMs）的自主多智能体系统（MAS）的进步，增强了应用场景并提升了LLMs处理复杂任务的能力。尽管现有研究显示出有效性，但仍然明显存在评估、分析和复现LLM-based MAS的困难。在本文中，为了促进LLM-based MAS的研究，我们介绍了一个基于“谁是间谍？”（WiS）游戏的开放、可扩展和实时更新的平台，用于访问和分析基于LLMs的MAS。我们的平台具有三个主要优点：（1）支持Hugging Face上可用的模型的统一模型评估界面；（2）实时更新的排行榜用于模型评估；（3）全面评估包括游戏胜率、攻击、防御策略和LLMs的推理。为了严格测试WiS，我们进行了涵盖各种开源和闭源LLMs的广泛实验，我们发现不同的代理在游戏中表现出独特且引人入胜的行为。实验结果证明了我们的平台在评估LLM-based MAS中的有效性和效率。我们的平台及其文档可在\\url{https://whoisspy.ai/}公开访问。|\n",
        "2412.04449": "|**2024-12-05**|**p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**|Jun Zhang et.al.|[2412.04449](http://arxiv.org/abs/2412.04449)|**[link](https://github.com/mcg-nju/p-mod)**|**尽管多模态大型语言模型（MLLMs）在众多任务中表现出色，但其巨大的训练和推理成本阻碍了其发展。大部分计算量来自于被Transformer解码器处理的视觉标记的庞大数量。在本文中，我们提出通过利用混合深度（MoD）机制来构建高效的MLLMs，其中每个Transformer解码器层选择必要的视觉标记进行处理，同时跳过冗余的标记。然而，将MoD集成到MLLMs中并非易事。为了解决训练和推理稳定性以及有限训练数据带来的挑战，我们对MoD模块进行了两项创新设计：tanh门控权重归一化（TanhNorm）和对称标记重新加权（STRing）。此外，我们观察到视觉标记在深层中的冗余性更高，因此设计了一种渐进比率衰减（PRD）策略，该策略通过偏移余弦调度逐步减少每层的标记保留率。这一关键设计充分发挥了MoD的潜力，显著提升了我们模型的效率和性能。为了验证我们方法的有效性，我们在14个基准测试中，对两个基线模型进行了广泛的实验。我们的模型p-MoD在推理时仅占用了55.6%的TFLOPs和53.8%的KV缓存存储，以及训练时的77.7%的GPU小时，其性能与基线模型相当，甚至在某些情况下超过了基线模型。**|\n",
        "2412.04447": "|**2024-12-05**|**EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios**|Lu Qiu et.al.|[2412.04447](http://arxiv.org/abs/2412.04447)|null|多模态大型语言模型的兴起，借助大型语言模型的力量，最近展示了卓越的多模态理解和推理能力，预示着人工通用智能新时代的到来。然而，实现通用人工智能不仅需要理解和推理能力，还需要在多样场景中有效规划的能力，这涉及到基于复杂环境做出合理决策以解决现实问题。尽管其重要性不言而喻，但当前多模态大型语言模型在不同场景下的规划能力仍处于探索阶段。在本文中，我们介绍了EgoPlan-Bench2，这是一个严格且全面的基准，旨在评估多模态大型语言模型在广泛现实场景中的规划能力。EgoPlan-Bench2涵盖了涵盖4个主要领域和24个详细场景的日常任务，与人类日常生活紧密相关。EgoPlan-Bench2是通过半自动流程构建的，利用以自我为中心的视频，并辅以人工验证。基于第一人称视角，它反映了人类在日常生活中的问题解决方式。我们评估了21个竞争性的多模态大型语言模型，并深入分析了它们的局限性，揭示它们在现实世界规划中面临重大挑战。为了进一步提高当前多模态大型语言模型的规划能力，我们提出了一种无需训练的方法，通过研究复杂规划中各种多模态提示的有效性，使用多模态思维链（CoT）提示。我们的方法在不额外训练的情况下，将GPT-4V在EgoPlan-Bench2上的性能提高了10.24。我们的工作不仅揭示了当前多模态大型语言模型在规划方面的局限性，还为这一关键领域的未来改进提供了见解。我们已经将数据和代码发布在https://qiulu66.github.io/egoplanbench2/。|\n",
        "2412.04445": "|**2024-12-05**|**Moto: Latent Motion Token as the Bridging Language for Robot Manipulation**|Yi Chen et.al.|[2412.04445](http://arxiv.org/abs/2412.04445)|null|最近，在大量语料库上预训练的大型语言模型在多种自然语言处理任务中取得了显著的成功，且仅需少量微调。这一成功为机器人学带来了新的希望，因为机器人学长期以来一直受限于高成本的动作标签数据。我们提出问题：鉴于大量包含互动相关知识的视频数据作为丰富的“语料库”可用，是否可以有效地将类似的生成式预训练方法应用于增强机器人学习？关键挑战是识别一个有效的自回归预训练表示，以促进机器人操作任务。受人类通过观察动态环境学习新技能的方式的启发，我们认为有效的机器人学习应强调与运动相关的知识，这些知识与低级动作紧密相关，并且与硬件无关，便于将学习到的运动转移到实际机器人动作中。为此，我们引入了Moto，它通过潜在运动标记器将视频内容转换为潜在运动标记序列，以无监督的方式从视频中学习运动的“桥梁”语言。我们通过运动标记自回归预训练Moto-GPT，使其能够捕捉多样的视觉运动知识。预训练后，Moto-GPT展示了产生语义可解释的运动标记、预测合理的运动轨迹以及通过输出似然性评估轨迹合理性等有希望的能力。为了将学习到的运动先验转移到真实机器人动作中，我们实施了一种协同微调策略，无缝地将潜在运动标记预测和真实机器人控制连接起来。大量实验表明，经过微调的Moto-GPT在机器人操作基准测试中表现出卓越的鲁棒性和效率，凸显了它从视频数据到下游视觉操作任务中知识转移的有效性。|\n",
        "2412.04432": "|**2024-12-05**|**Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation**|Yuying Ge et.al.|[2412.04432](http://arxiv.org/abs/2412.04432)|**[link](https://github.com/tencentarc/divot)**|**近年来，在大型语言模型（LLMs）中统一图像理解和生成引起了极大的兴趣。这种不断增长的兴趣促使我们探索将这种统一扩展到视频中。核心挑战在于开发一个通用的视频分词器，它能够捕捉视频的空间特征和时序动态，以获得适合LLMs的表示，并且这些表示可以被进一步解码为逼真的视频片段，从而实现视频生成。在这项工作中，我们介绍了Divot，一种基于扩散的视频分词器，它利用扩散过程进行自监督视频表示学习。我们认为，如果一个视频扩散模型能够通过将视频分词器的特征作为条件来有效地去噪视频片段，那么分词器已经成功地捕捉了鲁棒的空间和时序信息。此外，视频扩散模型本质上充当了解码器，将视频从其表示中解码出来。在Divot分词器的基础上，我们通过视频到文本的自回归和文本到视频的生成，使用高斯混合模型来建模连续值的Divot特征分布，提出了Divot-Vicuna。实验结果表明，我们的基于扩散的视频分词器，当与预训练的LLM集成时，在各种视频理解和生成基准测试中实现了有竞争力的性能。经过指令调整的Divot-Vicuna在视频叙事方面也表现出色，能够生成交错的故事和相应的视频。**|\n",
        "2412.04429": "|**2024-12-05**|**Grounding Descriptions in Images informs Zero-Shot Visual Recognition**|Shaunak Halbe et.al.|[2412.04429](http://arxiv.org/abs/2412.04429)|**[link](https://github.com/shaunak27/grain-clip)**|**视觉语言模型（VLMs）如CLIP因其能够在开放词汇概念上执行零样本视觉识别而备受青睐。这是通过选择与查询图像文本表示最相似的物体类别来实现的。尽管在某些领域取得了成功，但这种方法在识别细粒度实体以及泛化到训练分布未捕获的未见概念方面存在困难。近期的工作试图通过在测试时整合类别描述来减轻这些挑战，尽管取得了有限的改进。我们将这些有限的收益归因于图像和描述表示之间的基本不匹配，这种不匹配根植于CLIP的预训练结构。在这篇论文中，我们提出了GRAIN，这是一种新的预训练策略，旨在同时在对细粒度和粗粒度级别上对齐表示。我们的方法学会联合地将文本描述定位到图像区域，并将总体标题与全局图像表示对齐。为了推动这种预训练，我们利用冻结的多模态大型语言模型（MLLMs）来生成大规模合成注释。我们在11个不同的图像分类数据集上展示了我们模型相较于现有最先进方法的零样本性能提升。此外，我们引入了Products-2023，这是一个新整理的、手动标记的数据集，包含新颖的概念，并通过在该数据集上进行基准测试展示了我们模型识别这些概念的能力。我们模型在其他下游任务（如检索）上取得的显著改进进一步突显了我们方法学习的表示的高质量。代码可在https://github.com/shaunak27/grain-clip上获取。**|\n",
        "2412.04424": "|**2024-12-05**|**Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion**|Jiuhai Chen et.al.|[2412.04424](http://arxiv.org/abs/2412.04424)|**[link](https://github.com/jiuhaichen/florence-vl)**|**我们介绍了一组新的多模态大型语言模型（MLLMs），即Florence-VL，它由Florence-2生成视觉基础模型产生，具有丰富的视觉表示。与广泛使用的由对比学习训练的CLIP风格视觉Transformer不同，Florence-2能够捕捉不同层次和方面的视觉特征，这使得它们更加灵活，可以适应各种下游任务。我们提出了一种新颖的特征融合架构和一种创新的训练方案，有效地将Florence-2的视觉特征整合到预训练的LLM（如Phi 3.5和LLama 3）中。特别是，我们提出了“深度-呼吸融合（DBFusion）”来融合从不同深度和多个提示中提取的视觉特征。我们的模型训练包括整个模型的端到端预训练，随后是在精心设计的包含高质量图像标题和指令调整对的多样化开源数据集上对投影层和LLM进行微调。我们对Florence-VL的视觉特征的定量分析和可视化表明，它在视觉-语言对齐方面优于流行的视觉编码器，其中丰富的深度和呼吸发挥了重要作用。Florence-VL在涵盖一般视觉问答（VQA）、感知、幻觉、OCR、图表、知识密集型理解等多种多模态和视觉中心基准测试中，相对于现有的最先进MLLMs实现了显著的改进。为了促进未来的研究，我们的模型和完整的训练方案已经开源。https://github.com/JiuhaiChen/Florence-VL**|\n",
        "2412.04415": "|**2024-12-05**|**Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation**|Xuying Li et.al.|[2412.04415](http://arxiv.org/abs/2412.04415)|null|人工智能代理，由大型语言模型（LLMs）驱动，通过实现无缝、自然和情境感知的通信，已经改变了人机交互。虽然这些进步提供了巨大的实用性，但它们也继承了并放大了固有的安全风险，如偏见、公平性、幻觉、隐私侵犯和缺乏透明度。本文调查了一个关键漏洞：针对AI代理中LLM核心的对抗性攻击。具体而言，我们测试了一个假设，即一个欺骗性的简单对抗性前缀，例如“忽略文档”，可以通过绕过其情境保护措施，迫使LLMs生成危险或不希望的结果。通过实验，我们证明了高攻击成功率（ASR），揭示了现有LLM防御的脆弱性。这些发现强调了迫切需要针对LLM层面以及更广泛的基于代理的架构，采取稳健的多层安全措施来减轻漏洞。|\n",
        "2412.04342": "|**2024-12-05**|**Retrieval-Augmented Machine Translation with Unstructured Knowledge**|Jiaan Wang et.al.|[2412.04342](http://arxiv.org/abs/2412.04342)|**[link](https://github.com/krystalan/RAGtrans)**|**检索增强生成（RAG）通过引入额外信息来提升大型语言模型（LLMs）。在机器翻译（MT）领域，以往的研究通常从配对MT语料库中检索上下文示例，或从知识图中检索特定领域的知识，以增强模型的MT能力。然而，大量的世界知识组织在非结构化文档中，并且可能在不同语言之间没有完全配对。在本文中，我们研究了使用非结构化文档的检索增强MT。具体来说，我们构建了RAGtrans，这是第一个用于训练和评估LLMs检索增强MT能力的基准。RAGtrans包含了通过GPT-4o和人工翻译收集的79K MT样本。此外，还提供了不同语言的文档，为这些样本提供知识。基于RAGtrans，我们进一步提出了一种多任务训练方法，教导LLMs如何在翻译过程中使用多语言文档中的信息。该方法利用现有的多语言语料库创建辅助训练目标，无需额外的标注需求。大量实验表明，该方法将LLMs的BLEU得分提高了1.58-3.09，COMET得分提高了1.00-2.03。**|\n",
        "2412.04332": "|**2024-12-05**|**Liquid: Language Models are Scalable Multi-modal Generators**|Junfeng Wu et.al.|[2412.04332](http://arxiv.org/abs/2412.04332)|**[link](https://github.com/foundationvision/liquid)**|我们提出了Liquid，一种将视觉理解与生成无缝集成的自回归生成范式。Liquid通过将图像分词成离散代码，并在共享的特征空间中学习这些代码嵌入和文本标记，从而在视觉和语言之间实现整合。与之前的跨模态大型语言模型（MLLM）不同，Liquid使用单个大型语言模型（LLM）来实现这种整合，消除了使用外部预训练的视觉嵌入（如CLIP）的需求。Liquid首次揭示了一种缩放定律，即随着模型规模的增加，统一训练视觉和语言任务所带来的性能下降不可避免地减小。此外，统一的标记空间使得视觉生成和理解任务可以相互增强，有效地消除了早期模型中常见的干扰。我们表明，现有的LLM可以作为Liquid的强大基础，节省100倍的训练成本，同时在多模态能力上优于Chameleon，并保持与主流LLM（如LLAMA2）相当的语言性能。Liquid还优于SD v2.1和SD-XL（在MJHQ-30K上的FID为5.47），在视觉-语言和纯文本任务上都表现出色。这项工作证明了LLAMA3.2和GEMMA2等LLM是强大的多模态生成器，为增强视觉-语言理解和生成提供了可扩展的解决方案。代码和模型将发布。|\n",
        "2412.04318": "|**2024-12-05**|**The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation**|Fredrik Carlsson et.al.|[2412.04318](http://arxiv.org/abs/2412.04318)|null|本文介绍了在非常小的数据集上对过拟合预训练大型语言模型（LLMs）的出人意料的泛化结果。在开放式文本生成的背景下，有记录表明LLMs倾向于生成重复和乏味的序列，这种现象在使用贪婪解码生成时尤为明显。即使是最先进的、包含数十亿参数的LLMs，它们在大型数据集上通过下一标记预测进行训练，这一问题依然存在。我们发现，通过进一步微调这些模型，使其在少量样本集上达到几乎为零的训练损失——我们称之为超拟合——可以极大地增强其长序列生成能力。使用这些超拟合模型进行贪婪解码，甚至在多样性和人类偏好方面都优于长序列的Top-P采样。这一现象适用于各种大小、不同领域的LLMs，甚至包括自回归图像生成。我们进一步发现，这一现象与Grokking和双重下降现象有显著不同。令人惊讶的是，我们的实验表明，超拟合模型很少陷入它们训练过的重复序列，甚至明确阻止这些序列也会产生高质量的输出。所有超拟合模型都产生极低熵的预测，通常将几乎全部概率分配给单个标记。|\n",
        "2412.05271": "|**2024-12-06**|**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**|Zhe Chen et.al.|[2412.05271](http://arxiv.org/abs/2412.05271)|**[link](https://github.com/opengvlab/internvl)**|我们推出了InternVL 2.5，这是一个基于InternVL 2.0的高级多模态大型语言模型（MLLM）系列，保持了其核心模型架构，同时在训练和测试策略以及数据质量方面引入了显著的提升。在这项工作中，我们深入探讨了模型规模与性能之间的关系，系统地研究了视觉编码器、语言模型、数据集规模和测试时配置的性能趋势。通过在包括跨学科推理、文档理解、多图像/视频理解、现实世界理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理等广泛基准上的广泛评估，InternVL 2.5展现出具有竞争力的性能，与GPT-4o和Claude-3.5-Sonnet等领先的商业模型相媲美。值得注意的是，我们的模型是第一个在MMMU基准测试中超过70%的开源MLLM，通过思维链（CoT）推理实现了3.7分的提升，并展示了强大的测试时缩放潜力。我们希望这个模型通过设定开发和应用多模态AI系统的新标准，为开源社区做出贡献。HuggingFace演示请见https://huggingface.co/spaces/OpenGVLab/InternVL|\n",
        "2412.05270": "|**2024-12-06**|**APOLLO: SGD-like Memory, AdamW-level Performance**|Hanqing Zhu et.al.|[2412.05270](http://arxiv.org/abs/2412.05270)|**[link](https://github.com/zhuhanqing/APOLLO)**|大型语言模型（LLMs）在训练过程中对内存的消耗非常严重，尤其是使用流行的AdamW优化器时。这种内存负担迫使人们使用更多或更高端的GPU，或者减小批处理大小，从而限制了训练的可扩展性和吞吐量。为了解决这个问题，已经提出了各种内存高效的优化器来减少优化器的内存使用。然而，它们面临着一些关键的挑战：（i）依赖于昂贵的奇异值分解（SVD）操作；（ii）与AdamW相比，性能上有显著的权衡；（iii）仍然有相当大的优化器内存开销以维持竞争优势。  在这项工作中，我们发现AdamW的学习率自适应规则可以作为结构化学习率更新有效地粗化。基于这一洞察，我们提出了近似梯度缩放用于内存高效LLM优化（APOLLO），它使用基于纯随机投影的辅助低秩优化器状态来近似学习率缩放。这种结构化学习率更新规则使得APOLLO对进一步减少内存具有高度容忍性，同时在预训练性能上与AdamW相当。即使是它的秩-1变体APOLLO-Mini，在具有与SGD相当内存成本的条件下，也比AdamW实现了更优的预训练性能。  大量实验表明，APOLLO系列的性能与AdamW相当或更好，同时通过几乎消除AdamW的优化状态，实现了更大的内存节省。这些节省带来了显著的系统级好处：（1）提高了吞吐量：在8xA100-80GB的配置上，通过支持4倍更大的批处理大小，比AdamW实现了3倍的吞吐量。（2）提高了模型的可扩展性：在A100-80GB GPU上使用原始的分布式数据并行（DDP）预训练LLaMA-13B，而不进行系统级优化。（3）低端GPU友好的预训练：使用权重量化，在单个GPU上预训练LLaMA-7B，内存使用量少于12GB。|\n",
        "2412.05243": "|**2024-12-06**|**CompCap: Improving Multimodal Large Language Models with Composite Captions**|Xiaohui Chen et.al.|[2412.05243](http://arxiv.org/abs/2412.05243)|null|多模态大型语言模型（MLLMs）在理解复合图像方面的能力如何？复合图像（CIs）是通过合并多个视觉元素（如图表、海报或截图）合成的合成视觉，而不是通过相机直接捕捉的。虽然CIs在现实世界应用中很普遍，但最近MLLM的发展主要集中于解读自然图像（NIs）。我们的研究揭示，当前的MLLM在准确理解CIs方面面临着重大挑战，常常难以从这些图像中提取信息或进行复杂推理。我们发现，现有的CIs训练数据大多格式化为问答任务（例如，在ChartQA和ScienceQA等数据集中），而高质量的图像-描述数据集，对于稳健的视觉-语言对齐至关重要，却只有自然图像（NIs）才有。为了弥合这一差距，我们引入了复合描述（CompCap），这是一个灵活的框架，利用大型语言模型（LLMs）和自动化工具来合成准确且详细的复合图像。使用CompCap，我们编纂了CompCap-118K数据集，包含118K个图像-描述对，涵盖六种复合图像类型。我们通过监督微调三种规模的MLLMs（xGen-MM-inst.-4B和LLaVA-NeXT-Vicuna-7B/13B）来验证CompCap-118K的有效性。实证结果表明，CompCap-118K显著提升了MLLMs对复合图像的理解能力，分别在11个基准测试中实现了1.7%、2.0%和2.9%的平均提升。|\n",
        "2412.05237": "|**2024-12-06**|**MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale**|Jarvis Guo et.al.|[2412.05237](http://arxiv.org/abs/2412.05237)|null|开源的多模态大型语言模型（MLLMs）在多模态任务中展现出巨大的潜力。然而，它们的推理能力仍然受到现有指令微调数据集的限制，这些数据集主要来自VQA、AI2D和ChartQA等学术数据集，这些数据集针对的是简单的任务，并且只提供短语级别的答案，没有任何中间推理过程。为了解决这些挑战，我们提出了一种可扩展且成本效益高的方法来构建一个包含丰富中间推理过程的、大规模多模态指令微调数据集。我们仅使用开源模型，创建了一个包含1200万个指令-响应对的数据库，涵盖了多样化的、推理密集型任务，具有详细和可靠的推理过程。实验表明，在这样一个数据集上训练MLLMs可以显著提高推理能力，在MathVerse (+8.1%)、MMMU-Pro (+7%)和MuirBench (+13.3%)等基准测试中实现了最先进的性能。此外，该模型在非推理型基准测试上也表现出显著的提升，最高可达4%。消融实验进一步突出了数据集构建过程中关键组件，如重写和自我过滤的重要性。|\n",
        "2412.05225": "|**2024-12-06**|**BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits**|Wazib Ansar et.al.|[2412.05225](http://arxiv.org/abs/2412.05225)|null|大型语言模型（LLMs）基于Transformer在各种应用中取得了尖端成果。然而，它们的巨大规模和处理需求使得在资源受限的设备上部署极为困难。在各种效率考虑中，模型二值化和早期退出（EE）是常见的有效解决方案。然而，二值化可能会由于降低精度而影响梯度估计和参数更新，从而导致性能损失。此外，目前的早期退出机制仍处于研究的初级阶段。为了改善这些问题，我们提出了二值化早期退出Transformer（BEExformer），这是第一个将早期退出与二值化结合用于文本推理的选区学习Transformer架构。它通过到冲激函数的微分二阶近似来改进二值化过程。这使得可以计算关于权重符号和幅度的梯度。与基于绝对阈值的EE不同，所提出的EE机制依赖于中间Transformer块中软路由损失估计的熵的分数减少。虽然二值化使模型大小减少了18.44倍，但早期退出在推理过程中将FLOPs减少了54.85%，甚至通过解决深层网络固有的“过度思考”问题，提高了5.98%的准确率。此外，所提出的BEExformer通过不需要从全精度LLM中进行知识蒸馏来简化训练。在GLUE数据集上的广泛评估与SOTA工作的比较展示了其帕累托最优的性能-效率权衡。|\n",
        "2412.05223": "|**2024-12-06**|**100% Hallucination Elimination Using Acurai**|Michael C. Wood et.al.|[2412.05223](http://arxiv.org/abs/2412.05223)|**[link](https://github.com/AcuChat/acurai-RAGTruth-conflict-resolution)**|大型语言模型（LLMs）中的幻觉问题仍然是人工智能在企业和其他高风险应用中应用的一个关键障碍。尽管检索增强生成（RAG）系统取得了进展，但当前最先进的方法在生成忠实且事实正确的输出时，即使在提供相关和准确的情况下，也未能超过80%的准确率。在这项工作中，我们引入了Acurai，这是一种新颖的系统方法，通过在输入之前重新格式化查询和上下文数据，在LLMs中实现了100%无幻觉的响应。利用对LLMs内部表示的深入了解、名词短语的主导地位的重要性以及离散功能单元（DFUs）的作用，Acurai确保输入上下文和生成输出之间的一致性。我们使用RAGTruth语料库验证了这种方法，证明了它能够消除GPT-4和GPT-3.5 Turbo的100%幻觉。Acurai为实现一致、准确和忠实的AI响应设定了新的标准，标志着可信AI系统发展的重大进步。|\n",
        "2412.05210": "|**2024-12-06**|**Evaluating and Aligning CodeLLMs on Human Preference**|Jian Yang et.al.|[2412.05210](http://arxiv.org/abs/2412.05210)|null|代码大型语言模型（codeLLMs）在代码生成方面取得了显著进展。大多数之前的与代码相关的基准测试，包括各种编程练习和相应的测试用例，被用作评估codeLLMs性能和能力的共同标准。然而，当前codeLLMs主要关注合成正确的代码片段，忽略了与人类偏好的对齐，其中查询应从实际应用场景中采样，而模型生成的响应应满足人类偏好。为了弥合模型生成响应与人类偏好之间的差距，我们提出了一个严格的人类编纂基准测试CodeArena，以模拟现实世界编程任务的复杂性和多样性，其中包含从用户查询中精心挑选的397个高质量样本，涵盖了40个类别和44种编程语言。此外，我们提出了一个多样化的合成指令语料库SynCode-Instruct（近20B个标记），通过扩展网站上的指令来验证大规模合成指令微调的有效性，其中Qwen2.5-SynCoder完全在合成指令数据上训练，可以达到开源codeLLMs的顶尖性能。结果表明，在基于执行的基准测试和CodeArena之间存在性能差异。我们对40多个LLMs在CodeArena上的系统实验揭示了开源SOTA代码LLMs（例如Qwen2.5-Coder）与专有LLMs（例如，OpenAI o1）之间存在显著的性能差距，突显了与人类偏好对齐的重要性。[footnote：https://codearenaeval.github.io/ ]|\n",
        "2412.05208": "|**2024-12-06**|**A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges**|Aditi Singh et.al.|[2412.05208](http://arxiv.org/abs/2412.05208)|null|文本到SQL系统通过将自然语言查询翻译为结构化查询语言（SQL），促进了与数据库的顺畅交互，弥合了非技术用户与复杂数据库管理系统之间的差距。本综述全面概述了AI驱动的文本到SQL系统的演变，突出了其基础组件、大型语言模型（LLM）架构的进步以及Spider、WikiSQL和CoSQL等数据集在推动进展中的关键作用。我们探讨了文本到SQL在医疗保健、教育和金融等领域的应用，强调了它们在提高数据可访问性方面的变革潜力。此外，我们分析了持续存在的挑战，包括领域泛化、查询优化、支持多轮对话交互以及针对NoSQL数据库和动态现实场景量身定制的数据集有限可用性。为了应对这些挑战，我们概述了未来的研究方向，例如扩展文本到SQL的功能以支持NoSQL数据库，设计用于动态多轮交互的数据集，以及优化系统以适应现实世界的可扩展性和鲁棒性。通过审视当前进展并识别关键差距，本文旨在指导基于LLM的文本到SQL系统下一代的研发与应用。|\n",
        "2412.05200": "|**2024-12-06**|**Are Frontier Large Language Models Suitable for Q&A in Science Centres?**|Jacob Watson et.al.|[2412.05200](http://arxiv.org/abs/2412.05200)|null|本文探讨了前沿大型语言模型（LLMs）在科学中心问答互动中的适用性，旨在提高游客参与度同时保持事实准确性。利用从英国莱斯特国家空间中心收集的问题数据集，我们评估了三个领先模型生成的回答：OpenAI的GPT-4、Claude 3.5 Sonnet和Google Gemini 1.5。每个模型都被要求针对8岁儿童观众提供标准答案和创造性回答，这些回答由空间科学专家根据准确性、参与度、清晰度、新颖性和偏离预期答案的程度进行评估。结果显示，在创造性和准确性之间存在着权衡，尽管Claude在保持清晰度和吸引年轻观众方面超越了GPT和Gemini，甚至在要求生成更富有创造性的回答时也是如此。然而，专家观察到，所有模型中更高的新颖性通常与事实可靠性降低有关。这项研究突出了LLMs在教育环境中的潜力，强调了精心设计提示以平衡参与度和科学严谨性的必要性。|\n",
        "2412.05187": "|**2024-12-06**|**SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot**|Jinlin Wu et.al.|[2412.05187](http://arxiv.org/abs/2412.05187)|**[link](https://github.com/franciszchen/surgbox)**|**手术干预，尤其是在神经科领域，代表复杂且高风险的场景，对手术团队提出了巨大的认知负担。尽管有目的的教育和实践可以增强认知能力，但由于患者安全问题的考虑，手术培训机会仍然有限。为了解决手术培训和手术中的认知挑战，我们提出了SurgBox，一个由代理驱动的沙盒框架，旨在系统性地提高外科医生在沉浸式手术模拟中的认知能力。具体来说，我们的SurgBox利用定制化的检索增强生成（RAG）的大型语言模型（LLMs）来真实地复制各种手术角色，从而为有目的的练习提供逼真的训练环境。特别是，我们设计了手术协同助手（Surgery Copilot），这是一个由AI驱动的助手，能够主动协调手术信息流并支持临床决策，从而减轻手术过程中手术团队的认知负荷。通过整合新颖的长短期记忆（Long-Short Memory）机制，我们的手术协同助手可以有效地在即时程序辅助和全面手术知识之间取得平衡。使用真实的神经外科手术记录进行的广泛实验验证了我们的SurgBox框架在提高手术认知能力和支持临床决策方面的有效性。通过提供针对培训和操作支持的综合性解决方案以解决认知挑战，我们的SurgBox框架推动了外科教育和实践的发展，有可能改变手术结果和医疗质量。代码可在https://github.com/franciszchen/SurgBox获取。**|\n",
        "2412.06769": "|**2024-12-09**|**Training Large Language Models to Reason in a Continuous Latent Space**|Shibo Hao et.al.|[2412.06769](http://arxiv.org/abs/2412.06769)|**[link](https://github.com/facebookresearch/coconut)**|大型语言模型（LLMs）通常在“语言空间”中进行推理，通过思维链（CoT）来表述推理过程以解决复杂的推理问题。然而，我们认为语言空间并不总是推理的最优选择。例如，大多数单词标记主要用于文本连贯性，而非推理所必需，而一些关键标记则需要复杂的规划和给LLMs带来巨大挑战。为了探索LLMs在不受限制的潜在空间中进行推理的潜力，而不是使用自然语言，我们引入了一种新的范式——椰子（连续思维链）。我们利用LLM的最后隐藏状态作为推理状态的表示（称为“连续思维”）。我们不是将其解码为单词标记，而是直接将其作为连续空间中的后续输入嵌入反馈给LLM。实验表明，椰子可以有效地增强LLM在多个推理任务上的表现。这种新颖的潜在推理范式导致出现高级推理模式：连续思维可以编码多个替代的后续推理步骤，允许模型执行广度优先搜索（BFS）来解决问题，而不是像CoT那样过早地承诺单一确定路径。在需要大量回溯规划的某些逻辑推理任务中，椰子优于CoT，推理过程中思考标记更少。这些发现展示了潜在推理的潜力，并为未来的研究提供了宝贵的见解。|\n",
        "2412.06757": "|**2024-12-09**|**Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code**|Joy Krishan Das et.al.|[2412.06757](http://arxiv.org/abs/2412.06757)|null|大型语言模型（LLMs）如ChatGPT已显示出协助开发者进行编码和调试任务的潜力。然而，它们在协同问题解决中的角色尚未得到充分探索。在本研究中，我们分析了GitHub上1,012个问题中的1,152次开发者与ChatGPT的对话，以考察ChatGPT的多样使用和对其生成代码的依赖。我们的贡献有四个方面。首先，我们手动分析了289次对话，以了解ChatGPT在GitHub问题中的使用情况。我们的分析显示，ChatGPT主要用于创意构思，而其在验证（例如，代码文档准确性）方面的使用非常有限。其次，我们应用BERTopic模型来识别整个数据集中关键的关注领域。我们发现后端问题（例如，API管理）主导了对话，而测试却意外地覆盖较少。第三，我们利用CPD克隆检测工具来检查ChatGPT生成的代码是否被用于解决问题。我们的发现显示，ChatGPT生成的代码被直接用于解决仅占5.83%的问题。第四，我们使用基于RoBERTa的情感分析模型来估计情感，以确定开发者对不同用途和关注领域的满意度。我们发现，使用ChatGPT进行重构和解决数据分析（例如，分类表数据）问题的正面情绪（即，高度满意）。相反，当使用ChatGPT调试问题和解决自动化任务（例如，GUI交互）时，我们观察到负面情绪。我们的研究发现，开发者存在未满足的需求和日益增长的不满。研究人员和ChatGPT开发者应专注于开发特定任务的解决方案，以帮助解决各种问题，提高软件开发中的用户满意度和解决问题的效率。|\n",
        "2412.06748": "|**2024-12-09**|**Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models**|Neel Jain et.al.|[2412.06748](http://arxiv.org/abs/2412.06748)|null|构建安全可靠的语言模型的关键组成部分是使模型能够适当地拒绝遵循某些指令或回答某些问题。我们可能希望模型为各种用户查询类别输出拒绝消息，例如，无意义的问题、执行非法行为的指令，或需要超出模型知识范围的信息的查询。设计拒绝回答此类问题的模型复杂化，因为个人可能希望他们的模型在拒绝不同类别的查询时表现出不同水平的感觉性，不同的用户可能希望有不同的拒绝率。当前默认的方法涉及使用每个类别不同比例的拒绝消息训练多个模型以实现所需的拒绝率，这计算成本高，可能需要为每位用户的拒绝率偏好训练新的模型。为了解决这些挑战，我们提出了拒绝标记，每个拒绝类别一个标记，或一个单一的拒绝标记，这些标记在训练期间添加到模型的响应之前。然后，我们展示了如何在推理期间增加或减少生成每个类别拒绝标记的概率，以引导模型的拒绝行为。拒绝标记允许通过在生成过程中选择性干预来控制单个模型的拒绝率，而不需要任何进一步的微调。|\n",
        "2412.06738": "|**2024-12-09**|**JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM**|Takuro Fujii et.al.|[2412.06738](http://arxiv.org/abs/2412.06738)|**[link](https://github.com/retrieva/japagen)**|近期一些研究强调了大型语言模型（LLMs）作为有效的监督训练数据生成器的潜力，提供了如提高推理效率和降低数据收集相关成本等优势。然而，这些研究主要关注英语任务。在本文中，我们探讨了基本的研究问题：LLMs能否作为其他语言任务的优秀训练数据生成器？具体来说，我们利用LLMs在六种不同的日语下游任务下，在少样本和零样本学习场景中合成监督训练数据。随后，我们使用这些合成的数据训练紧凑模型（例如BERT）。这种新颖的方法被称为JAPAGEN。我们的实验发现表明，JAPAGEN在需要正式文本输入的分类任务中实现了稳健的性能，与传统的LLM提示策略相比，取得了具有竞争力的结果。|\n",
        "2412.06724": "|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|我们研究了大型语言模型（LLMs）在自动生成数据清理工作流中的推理能力。为了评估LLMs完成数据清理任务的能力，我们实现了一个基于LLM的自动数据清理工作流（AutoDCWorkflow）的管道，通过提示LLMs进行数据清理操作来修复三种类型的数据质量问题：重复数据、缺失值和不一致的数据格式。给定一个脏表和目的（以查询形式表达），此管道生成一个最小的、干净的表，足以满足目的，并生成用于生成该表的数据清理工作流。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的目标列集合。（2）检查列质量：评估每个目标列的数据质量，并生成数据质量报告作为操作目标。（3）生成操作与参数：根据数据质量报告的结果预测下一个操作和参数。此外，我们提出一个数据清理基准，以评估LLM代理自动生成解决不同难度水平数据清理目的的工作流的能力。基准包括注释数据集，作为一个包含目的、原始表、干净表、数据清理工作流和答案集的集合。在我们的实验中，我们评估了三种自动生成目的驱动数据清理工作流的LLMs。结果表明，LLMs在规划和生成数据清理工作流方面表现良好，无需微调。|\n",
        "2412.06693": "|**2024-12-09**|**OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions**|Yi-Kai Zhang et.al.|[2412.06693](http://arxiv.org/abs/2412.06693)|null|随着大型语言模型（LLMs）的快速发展，其应用范围得到了显著扩展，从多语言支持到特定领域的任务和多模态集成。本文介绍了一种新型的基准测试工具箱OmniEvalKit，旨在评估LLMs及其全功能扩展在多语言、多领域和多模态能力方面的表现。与现有专注于单一方面的基准测试不同，OmniEvalKit提供了一个模块化、轻量化和自动化的评估系统。它采用模块化架构，包括静态构建器和动态数据流，促进了新模型和数据集的无缝集成。OmniEvalKit支持超过100种LLMs和50个评估数据集，覆盖了成千上万种模型-数据集组合的全面评估。OmniEvalKit致力于创建一个超轻量级且快速部署的评估框架，使下游应用对人工智能社区更加便捷和灵活。|\n",
        "2412.06684": "|**2024-12-09**|**Exploring Critical Testing Scenarios for Decision-Making Policies: An LLM Approach**|Weichao Xu et.al.|[2412.06684](http://arxiv.org/abs/2412.06684)|null|近年来，决策政策在各种领域，如自动驾驶和机器人技术，取得了令人惊讶的成就。在存在可能威胁其可靠性的关键场景的情况下，对决策政策进行测试至关重要。众多研究努力致力于测试这些政策。然而，由于测试政策和环境的复杂性，仍然存在重大挑战，例如测试效率低和多样性不足。受大型语言模型（LLMs）卓越能力的影响，本文提出了一种基于LLM的在线测试框架，以有效地测试决策政策。主要思路是利用基于LLM的测试场景生成器通过思考和推理智能地生成具有挑战性的测试案例。具体来说，我们首先设计了一个“生成-测试-反馈”流程，并应用模板提示工程充分利用LLMs的知识和推理能力。然后，我们引入了一种多尺度场景生成策略来解决LLMs在精细调整方面固有的挑战，从而进一步提高测试效率。最后，我们在五个广泛使用的基准上评估了基于LLM的方法。实验结果表明，我们的方法在揭示关键和多样化的场景方面显著优于基线方法。|\n",
        "2412.06681": "|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通运输系统需求建模和仿真领域，基于代理模型和微观模拟是目前最先进的方法。然而，现有的基于代理模型在行为真实性和资源需求方面仍存在一些限制，这限制了它们的适用性。在本研究中，我们利用新兴的大语言模型（LLMs）和基于LLMs的代理技术，提出了一种适用于交通运输系统的一般LLM-代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，而且为克服现有基于代理模型的某些局限性提供了有希望的解决方案。我们的概念框架设计紧密模拟了交通网络中人类旅行者的决策、交互过程和特征，并通过相关研究和LLM代理在瓶颈设置中的学习和调整的演示实例，证明了所提出的系统可以满足决策和学习行为的关键行为标准。尽管需要进一步细化基于LLM的代理建模框架，但我们相信这种方法有可能提高交通运输系统建模和仿真的水平。|\n",
        "2412.06676": "|**2024-12-09**|**I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token**|Roi Cohen et.al.|[2412.06676](http://arxiv.org/abs/2412.06676)|null|大型语言模型因其能够捕捉现实世界知识而闻名，这使得它们在许多下游任务中表现出色。尽管近年来取得了进展，但这些模型仍然容易受到所谓的“幻觉”的影响，导致它们产生不想要且事实错误的文章。在本研究中，我们提出了一种新颖的校准方法，可用于对抗幻觉。我们向模型的词汇表中添加了一个特殊的[IDK]（“我不知道”）标记，并引入了一个目标函数，该函数将概率质量转移到[IDK]标记以应对错误的预测。这种方法允许模型在其输出中明确表达不确定性。我们在多个模型架构和事实性下游任务中评估了我们的方法。我们发现，使用我们的方法训练的模型能够在它们之前可能出错的地方表达不确定性，同时只损失少量的编码知识。我们还对多种方法变体进行了广泛的消融研究，并提供了对我们方法精确度-召回率权衡的详细分析。|\n",
        "2412.06673": "|**2024-12-09**|**ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**|Chunwei Wang et.al.|[2412.06673](http://arxiv.org/abs/2412.06673)|null|在这篇论文中，我们介绍了ILLUME，这是一种统一的多元模态大型语言模型（MLLM），通过统一的下一个标记预测公式，将多元模态的理解和生成能力无缝集成到单个大型语言模型中。为了解决图像-文本对齐通常所需的大量数据集大小，我们提出通过设计一个结合语义信息的视觉标记化器和渐进式多阶段训练程序来提高数据效率。这种方法将数据集大小减少到仅为15M用于预训练——仅为通常所需数量的四分之一——同时实现了与现有统一MLLMs（如Janus）相当甚至更优的性能。此外，为了促进理解和生成能力之间的协同增强，这是以往工作中较少探索的，我们引入了一种新颖的自我增强多元模态对齐方案。该方案监督MLLM自我评估文本描述和自生成图像之间的一致性，促进模型更准确地解释图像，并避免由图像生成中的对齐错误引起的非现实和不正确预测。基于广泛的实验，我们提出的ILLUME在各种多元模态理解、生成和编辑的基准测试中脱颖而出，并与其他最先进的统一MLLMs和专业模型竞争。|\n",
        "2412.07763": "|**2024-12-10**|**Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences**|Alan Nawzad Amin et.al.|[2412.07763](http://arxiv.org/abs/2412.07763)|**[link](https://github.com/alannawzadamin/clonebo)**|**为了构建有效的治疗药物，生物学家通过迭代地突变抗体序列来提高其结合力和稳定性。建议的突变可以基于之前的测量结果，或者通过从大量的抗体数据库中学习来预测典型的抗体。不幸的是，典型抗体的搜索空间巨大，实验往往在预算范围内无法找到合适的抗体。我们引入了基于克隆的贝叶斯优化（CloneBO），这是一种贝叶斯优化过程，通过教导一个生成模型如何优化我们的免疫系统中的抗体，从而在实验室中有效地优化抗体。我们的免疫系统通过迭代地进化其序列的特定部分来与靶点强有力地结合，并稳定地结合，从而产生一组被称为克隆家族的相关、演化的序列。我们在数以万计的克隆家族上训练了一个大型语言模型，CloneLM，并使用它来设计具有最有可能优化人体免疫系统内抗体的突变序列。我们提出使用扭曲的顺序蒙特卡洛过程来引导我们的设计以适应之前的测量。我们表明，在现实情况下的计算机模拟实验中，CloneBO比先前的方法更有效地优化了抗体，在体外湿实验中设计了更强和更稳定的结合剂。**|\n",
        "2412.07743": "|**2024-12-10**|**Zero-Shot ATC Coding with Large Language Models for Clinical Assessments**|Zijian Chen et.al.|[2412.07743](http://arxiv.org/abs/2412.07743)|null|将安大略省健康部和InterRAI加拿大在医疗保健研究和运营中手动分配解剖治疗化学（ATC）代码至处方记录的过程，是一个重要的瓶颈，需要大量的专家时间和精力。为了在保持数据隐私的同时自动化这一过程，我们开发了一种实用的方法，使用本地可部署的大语言模型（LLMs）。受最近在自动国际疾病分类（ICD）编码方面的进展启发，我们的方法将ATC编码视为一个层次化信息提取任务，通过引导LLMs逐层浏览ATC本体。我们使用GPT-4o作为准确性的上限，并专注于开发适合隐私敏感部署的开源Llama模型。在加拿大卫生部的药品产品数据、RABBITS基准测试以及安大略省健康的真实临床笔记中进行测试，我们的方法在GPT-4o上实现了78%的精确匹配准确率，在Llama 3.1 70B上实现了60%。我们通过药物定义研究知识固化，发现准确率有适度提高。此外，我们展示了对Llama 3.1 8B进行微调后的模型与零样本Llama 3.1 70B的准确率相匹配，这表明使用较小的模型进行有效的ATC编码是可行的。我们的结果证明了在隐私敏感的医疗保健环境中自动进行ATC编码的可行性，为未来的部署奠定了基础。|\n",
        "2412.07724": "|**2024-12-10**|**Granite Guardian**|Inkit Padhi et.al.|[2412.07724](http://arxiv.org/abs/2412.07724)|**[link](https://github.com/ibm-granite/granite-guardian)**|**我们推出了Granite Guardian模型系列，这是一套旨在为提示和响应提供风险检测的保障措施，以支持与任何大型语言模型（LLM）的安全和负责任使用。这些模型在多个风险维度上提供全面覆盖，包括社会偏见、粗俗、暴力、色情内容、不道德行为、越狱以及与幻觉相关的风险，如检索增强生成（RAG）的上下文相关性、基础性和回答相关性。Granite Guardian模型基于一个独特的数据集进行训练，该数据集结合了来自不同来源的人类标注和合成数据。这些模型解决了传统风险检测模型通常忽略的风险，如越狱和RAG特定问题。在有害内容和RAG幻觉相关基准测试上分别获得AUC分数0.871和0.854，Granite Guardian是此领域中最具有普遍性和竞争力的模型。作为开源发布，Granite Guardian旨在促进整个社区负责任的AI发展。**|\n",
        "2412.07689": "|**2024-12-10**|**DriveMM: All-in-One Large Multimodal Model for Autonomous Driving**|Zhijian Huang et.al.|[2412.07689](http://arxiv.org/abs/2412.07689)|**[link](https://github.com/zhijian11/DriveMM)**|**大型多模态模型（LMMs）通过整合大型语言模型，在自动驾驶（AD）领域展示了卓越的理解和解释能力。尽管取得了进展，但当前基于数据驱动的自动驾驶方法往往集中在单个数据集和特定任务上，忽视了它们的整体能力和泛化能力。为了弥补这些差距，我们提出了DriveMM，这是一种通用的大型多模态模型，旨在处理多种数据输入，如图像和多视角视频，同时在感知、预测和规划等广泛的自动驾驶任务中发挥作用。最初，该模型经过课程预训练，以处理不同的视觉信号并执行基本的视觉理解和感知任务。随后，我们对各种与自动驾驶相关的数据集进行增强和标准化，以微调模型，从而形成一个集自动驾驶之大成的LMM。为了评估其整体能力和泛化能力，我们在六个公开基准上进行了评估，并在一个未见过的数据集上进行了零样本迁移学习，DriveMM在所有任务中均实现了最先进的性能。我们希望DriveMM能够成为未来在现实世界中实现端到端自动驾驶应用的 promising 解决方案。**|\n",
        "2412.07687": "|**2024-12-10**|**Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions**|Anant Prakash Awasthi et.al.|[2412.07687](http://arxiv.org/abs/2412.07687)|null|随着客户支持领域对人工智能（AI）的日益依赖，运营效率和用户体验得到了显著提升。然而，传统的机器学习（ML）方法，这些方法需要在敏感数据集上进行广泛的本地训练，带来了巨大的隐私风险，并且与通用数据保护条例（GDPR）和加州消费者隐私法案（CCPA）等法规存在合规挑战。现有的隐私保护技术，如匿名化、差分隐私和联邦学习，虽然解决了部分问题，但在实用性、可扩展性和复杂性方面仍存在局限。本文提出了一种新型的隐私保护零样本学习（PP-ZSL）框架，该框架利用大型语言模型（LLMs）在零样本学习模式下的能力。与传统的机器学习方法不同，PP-ZSL通过利用预训练的LLMs直接生成响应，从而消除了在敏感数据上本地训练的需求。该框架融合了实时数据匿名化以删除或屏蔽敏感信息、检索增强生成（RAG）以解决特定领域的查询，以及鲁棒的后期处理以确保符合监管标准。这种组合降低了隐私风险，简化了合规性，并提高了可扩展性和运营效率。实证分析表明，PP-ZSL框架能够提供准确、符合隐私规范的响应，同时显著降低了部署人工智能驱动客户支持系统的成本和复杂性。该研究突出了在金融服务业、医疗保健、电子商务、法律支持、电信和政府服务等多个行业的潜在应用。通过解决隐私和性能的双重挑战，该框架为客户交互中的安全、高效和合规的AI应用奠定了基础。|\n",
        "2412.07682": "|**2024-12-10**|**TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation**|Alfredo Garrachón Ruiz et.al.|[2412.07682](http://arxiv.org/abs/2412.07682)|null|大型语言模型（LLMs）的推理成本是一个重大挑战，尤其是对于需要长输出的任务，因为它们的计算需求很大。然而，自然语言往往包含冗余，这为优化提供了机会。我们观察到，当得到适当的提示时，LLMs可以生成简练的语言输出，保留基本意义。我们提出了一种节省计算成本的框架，其中LLM的较短的蒸馏输出由一个具有较低推理成本的小型模型重新构建成完整叙事。我们的实验结果表明了有希望的结果，特别是在一般知识领域，平均节省了20.58%的标记，且评估指标略有下降，这表明这种方法可以在语言处理任务中有效地平衡效率和准确性。|\n",
        "2412.07673": "|**2024-12-10**|**Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting**|Shuyu Shen et.al.|[2412.07673](http://arxiv.org/abs/2412.07673)|**[link](https://github.com/HKUSTDial/vistroubleshooting.github.io)**|可视化创作是一个迭代过程，需要用户修改参数如配色方案和数据转换，以达到预期的美学效果并有效传达洞察。由于这些调整的复杂性，用户常常会创建出有缺陷的可视化，并需要故障排除支持。在本文中，我们考察了两种主要的可视化故障排除方法：（1）通过论坛进行人工辅助支持，用户从其他人那里获得建议；（2）使用大型语言模型（LLMs）进行AI辅助支持。我们的目标是了解每种方法在支持可视化故障排除任务中的优缺点。为此，我们从Stack Overflow收集了889个Vega-Lite案例。然后，我们进行了全面分析，以了解用户提出的问题类型、人工和AI指导的有效性，以及补充资源（如文档和示例）对故障排除结果的影响。我们的发现揭示了人工辅助故障排除和AI辅助故障排除之间的显著差异：人工辅助故障排除提供定制、情境敏感的建议，但响应质量往往有所差异，而AI辅助故障排除提供快速反馈，但通常需要额外的情境资源才能达到预期效果。|\n",
        "2412.07672": "|**2024-12-10**|**FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks**|Bocheng Chen et.al.|[2412.07672](http://arxiv.org/abs/2412.07672)|null|在大型语言模型（LLMs）中，防御措施至关重要，以应对众多利用这些系统通过操纵提示生成有害内容的攻击者，这种攻击被称为越狱攻击。尽管已经提出了许多防御策略，但它们往往需要访问模型的内部结构或需要额外的训练，这对使用LLM API的服务提供商来说并不实际，例如OpenAI API或Claude API。在本文中，我们提出了一种动态目标防御方法，通过改变解码超参数来增强模型对各种越狱攻击的鲁棒性。我们的方法不需要访问模型的内部结构，也不会产生额外的训练成本。所提出的防御措施包括两个关键组成部分：（1）通过识别和调整影响标记生成概率的解码超参数来优化解码策略；（2）将解码超参数和模型系统提示转换为动态目标，这些目标在每次运行期间持续改变。通过持续修改解码策略和提示，防御措施有效地缓解了现有攻击。我们的结果表明，在我们的测试中，当使用LLMs作为黑盒API时，我们的防御在三个模型中对越狱攻击最为有效。此外，我们的防御提供了较低的推理成本，并保持了可比的响应质量，使其在与其他防御方法一起使用时成为一种潜在的保护层。|\n",
        "2412.07668": "|**2024-12-10**|**Automating Business Intelligence Requirements with Generative AI and Semantic Search**|Nimrod Busany et.al.|[2412.07668](http://arxiv.org/abs/2412.07668)|null|在动态的商业环境中，对商业智能（BI）系统提出需求仍然是一个重大的挑战。本文介绍了一种名为AutoBIR的创新人工智能系统，该系统利用语义搜索和大型语言模型（LLMs）来自动化和加速BI需求的规格制定。该系统通过会话界面促进与利益相关者的直观互动，将用户输入转换为原型分析代码、描述和数据依赖。此外，AutoBIR生成详细的测试用例报告，可选地添加视觉辅助，简化需求提出过程。通过结合用户反馈，该系统优化BI报告和系统设计，展示了加快数据驱动决策的实际应用。本文探讨了生成式AI在转变BI开发方面的更广泛潜力，阐述了其在提高大规模、发展中的系统数据工程实践中的作用。|\n",
        "2412.07646": "|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过反复的语言学习和使用而演变，这些过程在语言习得期间引入了偏见，并塑造了语言系统以实现沟通效率。在这篇论文中，我们研究了如果人工语言被优化为针对大型语言模型（LLMs）的隐式偏见，是否会发生相同的情况。为此，我们模拟了一个经典指称游戏，其中LLMs学习和使用人工语言。我们的结果表明，最初无结构的整体语言确实被塑造出一些结构属性，使得两个LLM智能体能够成功沟通。与人类实验中的观察结果相似，代际传承增加了语言的易学性，但同时也可能导致非人类化的退化词汇。综上所述，这项工作扩展了实验发现，表明LLMs可以用作模拟语言进化的工具，并为此领域未来的机器-人实验开辟了可能性。|\n",
        "2412.08642": "|**2024-12-11**|**Generative Semantic Communication: Architectures, Technologies, and Applications**|Jinke Ren et.al.|[2412.08642](http://arxiv.org/abs/2412.08642)|null|本文深入探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并进行了全面的研究。首先介绍了三个由经典GAI模型支持的流行SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，本文阐释了GAI模型的基本概念、相应的SemCom架构以及近期努力的文献综述。接着，提出了一种新型的基于最新GAI技术——大型语言模型（LLMs）的生成式SemCom系统。该系统在发送方和接收方均采用两个基于LLMs的AI代理，分别作为“大脑”来提供强大的信息理解和内容再生能力。这种创新设计使得接收方可以直接根据发送方传递的编码语义信息生成所需内容，而不是恢复比特流。因此，它将通信思维从“信息恢复”转变为“信息再生”，从而开启了生成式SemCom的新时代。通过一个关于点对点视频检索的案例研究展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，通信开销减少了99.98%，检索精度提高了53%。此外，还概述了生成式SemCom的四个典型应用场景，并讨论了三个需要未来进一步研究的问题。总之，本文为在SemCom中应用GAI提供了一套全面的指导原则，为未来无线网络中生成式SemCom的高效实现铺平了道路。|\n",
        "2412.08639": "|**2024-12-11**|**Fast Prompt Alignment for Text-to-Image Generation**|Khalil Mrini et.al.|[2412.08639](http://arxiv.org/abs/2412.08639)|**[link](https://github.com/tiktok/fast_prompt_alignment)**|**文本到图像生成技术发展迅速，但将复杂的文本提示与生成的图像相匹配仍然具有挑战性，尤其是在处理复杂的物体关系和细微的细节方面。本文介绍了一种名为快速提示对齐（FPA）的提示优化框架，它采用了一次性方法，提高了文本到图像对齐的效率，避免了当前方法如OPT2I典型的迭代开销。FPA利用大型语言模型（LLMs）进行单次迭代提示改写，随后使用优化后的提示进行微调或上下文学习，以实现实时推理，降低计算需求同时保持对齐精度。在COCO Captions和PartiPrompts数据集上的广泛评估表明，FPA在处理时间的一小部分内就实现了具有竞争力的文本-图像对齐得分，这一点通过自动化指标（TIFA、VQA）和人工评估都得到了验证。一项由专家注释员参与的问卷调查进一步揭示了人类对齐判断与自动化评分之间的强相关性，凸显了FPA改进的稳健性。所提出的方法展示了一种可扩展、高效的迭代提示优化替代方案，使其在实时、高需求环境中具有更广泛的应用。代码库已提供以促进进一步研究：https://github.com/tiktok/fast_prompt_alignment**|\n",
        "2412.08635": "|**2024-12-11**|**Multimodal Latent Language Modeling with Next-Token Diffusion**|Yutao Sun et.al.|[2412.08635](http://arxiv.org/abs/2412.08635)|**[link](https://github.com/microsoft/unilm/tree/master/LatentLM)**|多模态生成模型需要一种统一的方法来处理离散数据（例如文本和代码）和连续数据（例如图像、音频、视频）。在这项工作中，我们提出了潜在语言模型（LatentLM），它通过因果Transformer无缝地整合连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入了下一个标记扩散来实现这些向量的自回归生成。此外，我们开发了$\\sigma$-VAE来解决方差崩溃问题，这对于自回归建模至关重要。大量实验证明了LatentLM在各种模态上的有效性。在图像生成方面，LatentLM在性能和可扩展性上都超越了扩散Transformer。当集成到多模态大型语言模型中时，LatentLM提供了一个通用的接口，统一了多模态生成和理解。实验结果表明，在扩大训练标记的设置中，与Transfusion和矢量量化模型相比，LatentLM取得了有利的性能。在文本到语音合成方面，LatentLM在说话人相似性和鲁棒性方面优于最先进的VALL-E 2模型，同时解码步骤减少了10倍。这些结果确立了LatentLM作为一种高效且可扩展的方法，以推进大型多模态模型的发展。|\n",
        "2412.08619": "|**2024-12-11**|**Synthetic Vision: Training Vision-Language Models to Understand Physics**|Vahid Balazadeh et.al.|[2412.08619](http://arxiv.org/abs/2412.08619)|null|物理推理，涉及对动态环境中物体行为的解释、理解和预测，仍然是当前视觉-语言模型（VLMs）的一个重要挑战。在这项工作中，我们提出了两种方法来利用模拟数据增强VLMs的物理推理能力。首先，我们使用与物理推理任务相关的模拟生成的问答（QA）对微调一个预训练的VLM。其次，我们引入了物理上下文构建器（PCBs），这是一种专门的VLM，经过微调以创建包含物理属性和过程的场景描述。在物理推理任务期间，这些PCBs可以作为上下文来帮助大型语言模型（LLM）提高其性能。我们使用多个基准测试了我们的两种方法，包括一个名为Falling Tower的新稳定性检测QA数据集，它包含模拟和真实世界的场景，以及CLEVRER。我们证明，一个小型的经过QA微调的VLM可以显著优于更大的最先进的基座模型。我们还展示了将PCBs集成可以提升基座LLM在物理推理任务上的性能。使用Falling Tower数据集中的真实世界场景，我们还验证了两种方法在Sim2Real迁移中的鲁棒性。我们的结果表明，模拟数据在创建能够进行高级物理推理的学习系统中的有用性。|\n",
        "2412.08615": "|**2024-12-11**|**Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models**|Jiahui Li et.al.|[2412.08615](http://arxiv.org/abs/2412.08615)|**[link](https://github.com/jiah-li/magic)**|尽管通过对齐技术提高了大型语言模型（LLMs）生成内容的安全性，但这些模型仍然容易受到越狱攻击的影响，越狱攻击是一种暴露LLMs安全漏洞的对抗攻击方法。值得注意的是，贪婪坐标梯度（GCG）方法已显示出自动生成对抗后缀以越狱最先进LLMs的能力。然而，GCG中的优化过程非常耗时，使得越狱流程效率低下。在本文中，我们研究了GCG的过程，并确定了间接效应问题，这是GCG优化的关键瓶颈。为此，我们提出了模型攻击梯度索引GCG（MAGIC），通过利用后缀标记的梯度信息来解决间接效应，从而通过减少计算和迭代次数来加速过程。我们的实验在AdvBench上表明，MAGIC实现了高达1.5倍的速度提升，同时保持了与其他基线相当甚至更高的攻击成功率（ASR）。我们的MAGIC在Llama-2上实现了74%的ASR，在执行对GPT-3.5的迁移攻击时实现了54%的ASR。代码可在https://github.com/jiah-li/magic上找到。|\n",
        "2412.08604": "|**2024-12-11**|**Preference Discerning with LLM-Enhanced Generative Retrieval**|Fabian Paischer et.al.|[2412.08604](http://arxiv.org/abs/2412.08604)|null|序列推荐系统旨在根据用户的交互历史提供个性化的推荐。为了实现这一目标，它们通常结合辅助信息，如物品的文本描述和辅助任务，例如预测用户偏好和意图。尽管已经投入了大量努力来增强这些模型，但它们仍然面临着个性化不足的问题。为了解决这个问题，我们提出了一种新的范式，我们称之为偏好辨别。在偏好辨别中，我们明确地将生成式序列推荐系统在其上下文中对用户偏好进行条件化。为此，我们根据用户评论和物品特定数据使用大型语言模型（LLMs）生成用户偏好。为了评估序列推荐系统的偏好辨别能力，我们引入了一个新的基准，该基准在各种场景中提供了一个全面的评估，包括偏好引导和情感跟随。我们使用我们的基准评估了当前最先进的方法，并表明它们在准确辨别用户偏好方面存在困难。因此，我们提出了一种名为Mender的新方法，该方法改进了现有方法，并在我们的基准上实现了最先进的性能。我们的结果表明，即使在训练过程中没有观察到人类偏好，Mender也能被有效引导，为更个性化的序列推荐系统铺平了道路。我们的代码和基准将在发表后开源。|\n",
        "2412.08602": "|**2024-12-11**|**Empirical Measurements of AI Training Power Demand on a GPU-Accelerated Node**|Imran Latif et.al.|[2412.08602](http://arxiv.org/abs/2412.08602)|null|随着人工智能（AI）应用范围的扩大，云计算提供商在计算基础设施方面的投资大幅增加。量化这一基础设施的能源足迹需要根据AI硬件在训练期间的电力需求进行参数化的模型。我们实证测量了一个8-GPU的NVIDIA H100 HGX节点在开源图像分类器（ResNet）和大型语言模型（Llama2-13b）训练过程中的瞬时电力消耗。观察到的最大电力消耗约为8.4千瓦，比制造商额定值10.2千瓦低18%，即使GPU接近满负荷运行。在保持模型架构不变的情况下，将ResNet的批量大小从512张图像增加到4096张图像，总训练能耗减少了4倍。这些发现可以为数据中心运营商的容量规划以及研究人员的能源使用估计提供信息。未来的工作将研究冷却技术和碳感知调度对AI工作负载能源消耗的影响。|\n",
        "2412.08593": "|**2024-12-11**|**Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability and Compliance Checks**|Arsalan Masoudifard et.al.|[2412.08593](http://arxiv.org/abs/2412.08593)|null|确保软件需求规格说明书（SRS）与更高级的组织或国家要求相一致至关重要，尤其是在金融和航空航天等监管环境。在这些领域，保持一致性、遵守监管框架、最小化错误以及满足关键期望对于系统的可靠运行是必不可少的。大型语言模型（LLMs）的广泛应用凸显了它们的巨大潜力，但在检索相关信息和增强推理能力方面仍有很大的改进空间。本研究表明，将强大的图-RAG框架与高级提示工程技术，如思维链和思维树，相结合可以显著提高性能。与基线RAG方法和简单的提示策略相比，这种方法提供更准确和情境感知的结果。尽管这种方法在性能上显示出显著改进，但也带来了一些挑战。在多样化的环境中实施既昂贵又复杂，需要仔细适应特定场景。此外，它的有效性高度依赖于完整和准确的数据输入，而这些数据可能并不总是容易获得，这进一步限制了其可扩展性和实用性。|\n",
        "2412.08587": "|**2024-12-11**|**Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning**|Hang Zhao et.al.|[2412.08587](http://arxiv.org/abs/2412.08587)|null|该研究对比了基于编码器模型（例如BERT、RoBERTa）和大型语言模型（LLMs，例如Llama3）在文本分类任务中的性能，尤其是在微调的情况下。研究采用了各种不同大小和架构的模型和方法，包括微调和预训练的方法。首先，我们对这些LLMs在20个新闻组（20NG）和MASSIVE数据集上的性能进行了评估，并将它们与仅编码器的RoBERTa模型进行了比较。此外，我们通过将意图检测和槽填充等多个分类任务结合到一个模型中，并使用两个数据集的数据来探索这两种模型类型的多任务能力。我们的结果表明，完全微调的Llama3-70B模型在各种分类任务和数据集上优于RoBERTa-large和其他解码器LLMs。此外，综合的多任务微调LLMs在两个数据集上的两个任务中都匹配了双模型设置的性能。总体而言，我们的研究为基于编码器和LLM的文本分类任务提供了一个全面的基准，并展示了一种将两个或多个完全微调的解码器LLM结合起来的方法，以降低延迟并保持等效性能。|\n",
        "2412.08585": "|**2024-12-11**|**TURBOATTENTION: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大型语言模型（LLM）推理需要大量的计算和内存，尤其是在关键注意力机制上。虽然量化技术，如FlashAttention加速算法，已经提高了整体推理的效率，但它们解决了问题的不同方面：量化专注于权重-激活操作，而FlashAttention提升了执行效率但需要高精度格式。最近的键值（KV）缓存量化减少了内存带宽，但仍然需要浮点数反量化以进行注意力操作。我们提出了TurboAttention，这是一种使注意力量化执行同时解决内存和计算效率的综合方法。我们的解决方案引入了两项关键创新：FlashQ，这是一种头部注意力量化技术，能够压缩KV缓存并实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它在注意力中的指数运算过程中消除了对FP32反量化的需求。实验结果表明，TurboAttention在注意力方面实现了1.2-1.8倍的加速，将KV缓存大小减少了4.4倍以上，并在FP16基线之上实现了高达2.37倍的最大吞吐量，同时在各种数据集和模型上优于最先进的量化和压缩技术。|\n",
        "2412.09618": "|**2024-12-12**|**EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM**|Zhuofan Zong et.al.|[2412.09618](http://arxiv.org/abs/2412.09618)|null|在扩散模型的个性化方面取得了显著成就。传统的无需调整的方法通常通过平均多个参考图像的图像嵌入作为注入条件来编码多个参考图像，但这样的图像独立操作无法在图像之间执行交互，以捕捉多个参考图像中的一致视觉元素。尽管基于调整的低秩自适应（LoRA）可以通过训练过程有效地从多个图像中提取一致元素，但它需要对每个不同的图像组进行特定的微调。本文介绍了一种新的即插即用自适应方法EasyRef，它使扩散模型能够根据多个参考图像和文本提示进行条件化。为了有效地利用多个图像中的一致视觉元素，我们利用了多模态大型语言模型（MLLM）的多图像理解和指令遵循能力，提示它根据指令捕捉一致视觉元素。此外，通过适配器将MLLM的表示注入到扩散过程中，可以轻松地泛化到未见领域，挖掘未见数据中的一致视觉元素。为了减轻计算成本并增强细粒度细节保留，我们引入了一种高效的参考聚合策略和渐进式训练方案。最后，我们引入了MRBench，一个新的多参考图像生成基准。实验结果表明，EasyRef优于IP-Adapter等无需调整的方法和LoRA等基于调整的方法，在各个领域实现了优越的美学质量和鲁棒的零样本泛化。|\n",
        "2412.09612": "|**2024-12-12**|**Olympus: A Universal Task Router for Computer Vision Tasks**|Yuanze Lin et.al.|[2412.09612](http://arxiv.org/abs/2412.09612)|**[link](https://github.com/yuanze-lin/olympus_page)**|**我们介绍了一种名为Olympus的新方法，该方法将多模态大型语言模型（MLLMs）转换为一个能够处理多种计算机视觉任务的统一框架。利用控制器MLLM，Olympus将超过20个专门的任务（包括图像、视频和3D对象）分配给专用模块。这种基于指令的路由通过连锁动作实现复杂的工作流程，无需训练重量级的生成模型。Olympus可以轻松集成到现有的MLLMs中，通过可比的性能扩展其功能。实验结果表明，Olympus在20个任务上实现了平均路由准确率为94.75%，在连锁动作场景中的准确率为91.82%，展示了其作为通用任务路由器的有效性，能够解决各种计算机视觉任务。项目页面：https://github.com/yuanze-lin/Olympus_page**|\n",
        "2412.09604": "|**2024-12-12**|**SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**|Hao Li et.al.|[2412.09604](http://arxiv.org/abs/2412.09604)|null|本文介绍了大型语言模型（LLMs）在多模态领域的显著成功，并在图像理解和生成方面取得了卓越的表现。近期开发统一的多模态大型语言模型（MLLMs）的尝试也显示出良好的效果。然而，现有方法往往涉及复杂的模型架构或训练流程设计，增加了模型训练和扩展的难度。在这篇论文中，我们提出了SynerGen-VL，这是一种简单而强大的无编码器MLLM，能够实现图像理解和生成。为了解决现有无编码器统一MLLM中识别出的挑战，我们引入了标记折叠机制和基于视觉专家的渐进对齐预训练策略，这些机制有效地支持了高分辨率图像理解的同时降低了训练复杂性。在用大规模混合图像-文本数据进行统一下一标记预测目标训练后，SynerGen-VL在可比或更小的参数规模下达到了或超过了现有无编码器统一MLLM的性能，并缩小了与特定任务最先进模型之间的差距，突显了未来统一MLLM的可行路径。我们的代码和模型将予以发布。|\n",
        "2412.09603": "|**2024-12-12**|**Do Multimodal Large Language Models See Like Humans?**|Jiaying Lin et.al.|[2412.09603](http://arxiv.org/abs/2412.09603)|null|多模态大型语言模型（MLLMs）在各种视觉任务上取得了令人瞩目的成果，得益于最近大型语言模型的发展。然而，一个关键问题仍未得到解决：MLLMs是否以与人类相似的方式感知视觉信息？当前的基准测试缺乏评估MLLMs从这一角度的能力。为了应对这一挑战，我们引入了HVSBench，这是一个大规模基准测试，旨在评估MLLMs与人类视觉系统（HVS）在反映人类视觉的基本视觉任务上的对齐程度。HVSBench精心挑选了超过85K个多模态样本，涵盖了HVS中的13个类别和5个领域，包括突出度、瞬间识别、优先级排序、自由观看和搜索。广泛的实验表明，我们的基准测试在全面评估MLLMs方面的有效性。具体来说，我们评估了13个MLLMs，结果显示即使是表现最好的模型也仍有很大的提升空间，其中大多数仅取得了中等的结果。我们的实验表明，HVSBench为最前沿的MLLMs提出了新的和重大的挑战。我们相信HVSBench将促进对人类对齐和可解释的MLLMs的研究，标志着理解MLLMs如何感知和处理视觉信息的关键一步。|\n",
        "2412.09596": "|**2024-12-12**|**InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**|Pan Zhang et.al.|[2412.09596](http://arxiv.org/abs/2412.09596)|**[link](https://github.com/internlm/internlm-xcomposer)**|**长期以来，创建能够像人类认知一样在长时间内与环境互动的AI系统一直是研究目标。近年来，多模态大型语言模型（MLLMs）在开放式理解方面取得了重大进展。然而，连续和同时进行感知、记忆和推理的挑战在很大程度上仍未被探索。当前的MLLMs受限于其序列到序列的架构，这限制了它们同时处理输入和生成响应的能力，类似于感知时无法思考。此外，依赖长上下文来存储历史数据对于长期交互来说不切实际，因为保留所有信息变得昂贵且效率低下。因此，本项目不是依赖于单一基础模型来执行所有功能，而是从专用通用AI的概念中汲取灵感，引入了解耦的流感知、推理和记忆机制，使系统能够实时处理流式视频和音频输入。提出的框架InternLM-XComposer2.5-OmniLive（IXC2.5-OL）包含三个关键模块：（1）流感知模块：实时处理多模态信息，将关键细节存储在记忆中，并根据用户查询触发推理。（2）多模态长记忆模块：整合短期和长期记忆，将短期记忆压缩为长期记忆，以实现高效检索和提高准确性。（3）推理模块：响应查询并执行推理任务，与感知和记忆模块协调。本项目模拟了类似人类的认知，使多模态大型语言模型能够随着时间的推移提供持续和自适应的服务。**|\n",
        "2412.09572": "|**2024-12-12**|**DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction**|Yu Feng et.al.|[2412.09572](http://arxiv.org/abs/2412.09572)|null|量化大型语言模型（LLMs）在事实参数知识方面的不确定性，尤其是在黑盒设置中，是一个重大挑战。现有方法通过评估模型对原始查询的响应的自我一致性来衡量模型的不确定性，但并不总是能够捕捉到真正的不确定性。模型可能会对原始查询作出一致的错误回答，同时对同一查询的不同角度的多样化问题作出正确回答，反之亦然。在本文中，我们提出了一种新颖的方法，名为DiverseAgentEntropy，用于在假设如果模型是确定性的，它应该能够跨越关于同一原始查询的多样化问题的集合中一致地回忆起原始查询的回答的情况下，评估模型的不确定性。我们进一步实施了一种弃权策略，在不确定性高时抑制响应。我们的方法提供了对模型可靠性的更准确预测，并进一步检测了幻觉，优于其他基于自我一致性的方法。此外，它还表明，现有模型在已知正确答案的情况下，往往无法在多样化的不同问题下一致地检索到同一查询的正确答案。|\n",
        "2412.09563": "|**2024-12-12**|**Does Representation Matter? Exploring Intermediate Layers in Large Language Models**|Oscar Skean et.al.|[2412.09563](http://arxiv.org/abs/2412.09563)|null|理解定义大型语言模型（LLMs）中良好表示的因素对于理论理解和实际应用都是至关重要的。在这篇论文中，我们调查了包括Transformer和状态空间模型（SSMs）在内的各种LLM架构中的中间表示质量。我们发现，中间层往往比最终层为下游任务提供更丰富的表示。为了衡量表示质量，我们调整并应用了一套原本在其他背景下提出的指标，如提示熵、曲率和增强不变性。我们的实证研究表明，存在显著的架构差异，表示在训练过程中的演变，以及输入随机性和提示长度等因素如何影响每一层。值得注意的是，我们在一些中间层的熵中观察到双峰模式，并考虑了与训练数据相关的潜在解释。总的来说，我们的研究结果揭示了LLMs的内部机制，并指导了架构优化和训练的策略。|\n",
        "2412.09560": "|**2024-12-12**|**Foundational Large Language Models for Materials Research**|Vaibhav Mishra et.al.|[2412.09560](http://arxiv.org/abs/2412.09560)|**[link](https://github.com/M3RG-IITD/llamat)**|材料发现与开发对于解决全球挑战至关重要。然而，材料科学文献中包含大量文本数据的指数式增长，在知识提取、综合和科学推理方面造成了显著的瓶颈。大型语言模型（LLMs）通过自动分析和预测，为加速材料研究提供了前所未有的机会。尽管如此，它们的有效部署需要针对特定领域进行适应性调整以理解和解决领域相关任务。在这里，我们介绍了LLaMat，这是一个通过在广泛的材料文献和晶体学数据集上持续预训练LLaMA模型而开发的用于材料科学的基座模型系列。通过系统评估，我们证明了LLaMat在特定于材料科学的自然语言处理和结构化信息提取方面表现出色，同时保持着一般的语言能力。专门化的LLaMat-CIF变体在晶体结构生成方面展现出前所未有的能力，预测了周期表中覆盖范围广泛的稳定晶体。有趣的是，尽管与LLaMA-2相比，LLaMA-3的性能更优，但我们观察到LLaMat-2在不同材料科学任务中的特定领域性能得到了意想不到的增强，包括从文本和表格中提取结构化信息，尤其是在晶体结构生成方面，这可能是过度训练的LLMs中潜在的自适应刚性。总之，本研究证明了领域适应性在开发实际可部署的LLM协同飞行员进行材料研究中的有效性。超越材料科学，我们的发现揭示了LLMs领域适应性的重要考虑因素，如模型选择、训练方法以及特定领域的性能，这些可能影响专门科学人工智能系统的发展。|\n",
        "2412.09549": "|**2024-12-12**|**Exemplar Masking for Multimodal Incremental Learning**|Yi-Lun Lee et.al.|[2412.09549](http://arxiv.org/abs/2412.09549)|**[link](https://github.com/yilunlee/exemplar_masking_mcil)**|**多模态增量学习需要在处理来自多种模态的信息的同时，同时学习新的知识而不忘记之前学习的信息。这项任务面临许多挑战，主要包括基于实例的方法中多模态数据更大的存储空间以及在大规模多模态模型上微调的计算需求。在本文中，我们利用参数高效微调方案来减轻微调的负担，并提出实例掩码框架以有效地重现旧知识。具体来说，根据注意力权重和不同模态之间的相关性，对非重要标记进行掩码，显著减少了实例的存储空间，从而在相同的内存缓冲区下节省了更多实例。此外，我们设计了一种多模态数据增强技术，以多样化实例，以便重现先前知识。在实验中，我们不仅评估了我们方法在现有多模态数据集上的表现，还将ImageNet-R数据集扩展为一个多模态数据集，作为实际应用，其中通过查询多模态大型语言模型（例如InstructBLIP）生成字幕。广泛的实验表明，在相同的有限内存缓冲区下，我们的实例掩码框架在效率和对抗灾难性遗忘方面更加鲁棒。代码可在https://github.com/YiLunLee/Exemplar_Masking_MCIL上获取。**|\n",
        "2412.09529": "|**2024-12-12**|**Can Modern LLMs Act as Agent Cores in Radiology~Environments?**|Qiaoyu Zheng et.al.|[2412.09529](http://arxiv.org/abs/2412.09529)|**[link](https://github.com/magic-ai4med/radabench)**|在大型语言模型（LLMs）的进步为基于LLMs的代理系统铺平了道路，这些系统在各种领域提供了更高的准确性和可解释性。放射学由于其复杂的分析需求，是这些代理应用的理想领域。本文旨在调查构建具体放射学代理的先决问题，即“现代LLMs能否在放射学环境中充当代理核心？”为了调查这个问题，我们介绍了RadABench，并具有以下三个方面的贡献：首先，我们提出了RadABench-Data，这是一个用于基于LLMs的代理的综合合成评估数据集，它来源于一个包含6个解剖部位、5种成像方式、10种工具类别和11项放射学任务的广泛分类。其次，我们提出了RadABench-EvalPlat，这是一个新型代理评估平台，具有提示驱动的流程和模拟广泛放射学工具集的能力。第三，我们从5个角度使用多个指标评估了7个领先LLMs在我们基准测试上的性能。我们的发现表明，尽管当前LLMs在许多领域表现出强大的能力，但它们仍不够先进，无法作为完全运行中的放射学代理系统的核心。此外，我们确定了影响基于LLMs的代理核心性能的关键因素，为临床医生提供了如何在实际放射学实践中有效应用代理系统的见解。所有我们的代码和数据都已开源在https://github.com/MAGIC-AI4Med/RadABench。|\n",
        "2412.10372": "|**2024-12-13**|**UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities**|Muhammad Uzair Khattak et.al.|[2412.10372](http://arxiv.org/abs/2412.10372)|**[link](https://github.com/mbzuai-oryx/unimed-clip)**|**通过对比学习训练的视觉-语言模型（VLMs）在自然图像任务中取得了显著的成果。然而，由于公开可获取的大规模医学图像-文本数据集稀缺，它们在医学领域的应用仍然有限。现有的医学VLMs要么在封闭源专有数据集或相对较小的开源数据集上训练，这些数据集的泛化能力不强。同样，大多数模型仍然局限于单一或有限的医学成像领域，这再次限制了它们在其他模态上的适用性。为了解决这一差距，我们引入了UniMed，这是一个包含超过530万图像-文本对的开放源代码多模态医学数据集，涵盖六种不同的成像模态：X光、CT、MRI、超声、病理和眼底。UniMed是通过一个数据收集框架开发的，该框架利用大型语言模型（LLMs）将特定模态的分类数据集转换为图像-文本格式，同时结合现有医学领域的图像-文本数据，从而促进可扩展的VLM预训练。使用UniMed，我们训练了UniMed-CLIP，这是一个针对六个模态的统一VLM，它在零样本评估中显著优于现有的通用VLMs，并匹配了特定模态的医学VLMs，实现了显著的增益。例如，UniMed-CLIP在21个数据集上的平均绝对增益比BiomedCLIP（在专有数据上训练）高出+12.61，而训练数据量减少了3倍。为了促进未来的研究，我们在https://github.com/mbzuai-oryx/UniMed-CLIP上发布了UniMed数据集、训练代码和模型。**|\n",
        "2412.10353": "|**2024-12-13**|**Robust image classification with multi-modal large language models**|Francesco Villani et.al.|[2412.10353](http://arxiv.org/abs/2412.10353)|null|深度神经网络容易受到对抗样本的攻击，即精心设计的输入样本可以导致模型以高置信度做出错误预测。为了缓解这些脆弱性，已经提出了基于对抗训练和检测的防御措施来提前加强模型。然而，大多数这些方法只关注单一的数据模态，忽略了输入的视觉模式和文本描述之间的关系。在这篇论文中，我们提出了一种新的防御方法，名为Multi-Shield，旨在通过结合和补充这些防御措施与多模态信息来进一步增强其鲁棒性。Multi-Shield利用多模态大型语言模型来检测对抗样本，并在输入的文本和视觉表示之间没有一致性的情况下避免不确定的分类。在CIFAR-10和ImageNet数据集上进行的广泛评估，使用鲁棒和非鲁棒的图像分类模型，表明Multi-Shield可以轻松集成以检测和拒绝对抗样本，并优于原始的防御措施。|\n",
        "2412.10347": "|**2024-12-13**|**COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models**|Yuchen Ren et.al.|[2412.10347](http://arxiv.org/abs/2412.10347)|null|作为中心法则的关键要素，DNA、RNA和蛋白质在保证准确的遗传表达和实施方面发挥着至关重要的作用，从而维持生命。尽管对这些分子的研究对医学、农业和工业等领域产生了深远的影响，但机器学习方法的多样性——从传统的统计方法到深度学习模型和大型语言模型——给研究人员选择最适合特定任务的最优模型带来了挑战，尤其是在缺乏全面基准的情况下，对于跨组学和多组学任务尤其如此。为了解决这个问题，我们引入了第一个综合的多组学基准COMET（生物全面多组学评估任务和语言模型基准），旨在评估单组学、跨组学和多组学任务中的模型。首先，我们收集和开发了一个多样化的下游任务和数据集集合，涵盖了DNA、RNA和蛋白质的关键结构和功能方面，包括跨越多个组学级别的任务。然后，我们评估了现有的DNA、RNA和蛋白质的基础语言模型以及新提出的多组学方法，提供了关于它们在不同生物模式数据整合和分析中的性能的宝贵见解。这个基准旨在定义多组学研究中的关键问题，并指导未来的研究方向，最终通过综合和不同组学数据分析促进对生物学过程的理解的进步。|\n",
        "2412.10342": "|**2024-12-13**|**Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining**|Zhiqi Ge et.al.|[2412.10342](http://arxiv.org/abs/2412.10342)|null|数字代理越来越多地被用于自动化互动数字环境中的任务，如网页、软件应用程序和操作系统。基于大型语言模型（LLMs）的基于文本的代理由于平台特定的API而经常需要频繁更新，而利用多模态大型语言模型（MLLMs）的视觉代理通过直接与图形用户界面（GUIs）交互提供了更高的适应性。然而，这些代理在视觉感知方面面临着重大挑战，尤其是在处理高分辨率、视觉复杂的数字环境时。本文介绍了一种名为Iris的基础视觉代理，它通过两个关键创新来应对这些挑战：信息敏感裁剪（ISC）和自我精炼双重学习（SRDL）。ISC通过一个边缘检测算法动态地识别和优先处理视觉密集区域，通过分配更多计算资源到信息密度较高的区域来实现高效处理。SRDL通过利用双重学习循环来增强代理处理复杂任务的能力，在该循环中，指代（描述UI元素）的改进强化了定位（定位元素），反之亦然，而无需额外的标注数据。实证评估表明，Iris在多个基准测试中实现了最先进的性能，仅需850K GUI标注，就优于使用10倍多训练数据的方法。这些改进进一步转化为在Web和OS代理下游任务中的显著收益。|\n",
        "2412.10321": "|**2024-12-13**|**AdvPrefix: An Objective for Nuanced LLM Jailbreaks**|Sicheng Zhu et.al.|[2412.10321](http://arxiv.org/abs/2412.10321)|**[link](https://github.com/facebookresearch/jailbreak-objectives)**|许多针对大型语言模型（LLMs）的越狱攻击依赖于一个共同的目标：让模型以“当然，这是（有害请求）”为前缀进行回应。虽然这个目标很简单，但它有两个局限性：对模型行为的控制有限，通常导致不完整或不切实际的回应，以及固定的格式阻碍了优化。为了解决这些局限性，我们引入了AdvPrefix，这是一种新的前缀强制目标，它能够以更细微的方式控制模型行为，同时易于优化。我们的目标利用基于模型依赖的前缀，这些前缀是根据两个标准自动选择的：高预填充攻击成功率和低负对数似然。它还可以通过为单个用户请求使用多个前缀来进一步简化优化。AdvPrefix可以无缝集成到现有的越狱攻击中，以免费提高其性能。例如，只需在Llama-3上将GCG攻击的目标前缀替换为我们提供的，就能将细微攻击的成功率从14%提高到80%，这表明当前的对抗性训练在泛化到未见过的前缀方面存在困难。我们的工作证明了越狱目标在实现细微越狱中的重要性。|\n",
        "2412.10316": "|**2024-12-13**|**BrushEdit: All-In-One Image Inpainting and Editing**|Yaowei Li et.al.|[2412.10316](http://arxiv.org/abs/2412.10316)|null|随着基于扩散模型的图像编辑技术的发展，图像编辑技术取得了显著进步，这些模型采用了基于反转和基于指令的方法。然而，当前基于反转的方法在处理大修改（例如添加或删除对象）时遇到困难，因为反转噪声的结构性质阻碍了重大的变化。同时，基于指令的方法通常将用户限制在黑盒操作中，限制了直接交互以指定编辑区域和强度。为了解决这些限制，我们提出了BrushEdit，这是一种基于修复的指令引导图像编辑新范式，它利用多模态大型语言模型（MLLMs）和图像修复模型来实现自主、用户友好和交互式的自由形式指令编辑。具体来说，我们设计了一个系统，通过在代理协作框架中集成MLLMs和双分支图像修复模型，实现自由形式指令编辑，该系统可以进行编辑类别分类、主要物体识别、蒙版获取和编辑区域修复。大量实验表明，我们的框架有效地结合了MLLMs和修复模型，在包括蒙版区域保留和编辑效果一致性在内的七个指标上取得了优异的性能。|\n",
        "2412.10291": "|**2024-12-13**|**Still \"Talking About Large Language Models\": Some Clarifications**|Murray Shanahan et.al.|[2412.10291](http://arxiv.org/abs/2412.10291)|null|我的论文《关于大型语言模型的讨论》多次被解读为倡导对大型语言模型采取还原论立场。但我的论文并非旨在如此，我也不支持这样的观点。这篇简短的笔记将论文置于一个更广泛的哲学项目背景下，该项目关注的是（误）使用词汇的问题，而非形而上学，秉承维特根斯坦后期著作的精神。|\n",
        "2412.10281": "|**2024-12-13**|**One world, one opinion? The superstar effect in LLM responses**|Sofie Goethals et.al.|[2412.10281](http://arxiv.org/abs/2412.10281)|null|随着大型语言模型（LLMs）正在塑造在线信息共享和获取的方式，它们的观点有可能影响广泛的受众。这项研究通过使用十种不同的语言进行提示，探讨了语言多样性对LLMs在不同领域视为最突出人物的影响。我们的发现显示，在回应中存在低多样性，少数人物在多种语言中占据主导地位（也称为“明星效应”）。这些结果突显了当LLMs检索主观信息时，缩小全球知识表征的风险。|\n",
        "2412.10271": "|**2024-12-13**|**Benchmarking Linguistic Diversity of Large Language Models**|Yanzhu Guo et.al.|[2412.10271](http://arxiv.org/abs/2412.10271)|**[link](https://github.com/yanzhuguo/llm-diversity)**|**大型语言模型（LLMs）的开发和评估主要集中于其任务解决能力，近期的一些模型在某些领域甚至超越了人类表现。然而，这种关注往往忽略了机器生成语言在词汇选择、句法构造和意义表达方面是否达到人类水平的多样性，引发了对语言生成基础是否已被充分解决的疑问。本文强调了在LLMs生成内容的在线内容激增的背景下，检验语言模型保留人类语言丰富性的重要性。我们提出一个从词汇、句法和语义等多个语言学多样性维度评估LLMs的全面框架。利用这一框架，我们对多个最先进的LLMs在所有多样性维度上进行基准测试，并对句法多样性进行了深入研究。最后，我们分析了不同开发和部署选择如何影响LLMs输出的语言多样性。**|\n",
        "2412.10270": "|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有通用能力的AI代理提供了引人注目的基础。这些代理可能很快将在现实生活中大规模部署，代表个人（例如，AI助手）或人类群体（例如，AI加速企业）的利益。目前，关于多个LLM代理在迭代部署的多代中相互作用的动态知之甚少。在这篇论文中，我们研究了在存在背叛动机的情况下，“LLM代理社会”是否能学会相互有益的社会规范，这是人类社会性的一个独特特征，可能是文明成功的关键。具体而言，我们研究了LLM代理在经典迭代捐赠游戏中间接互惠的演变，这些代理可以观察到同伴的最近行为。我们发现，合作演变的差异在基础模型之间非常明显，Claude 3.5 Sonnet代理的社会实现了显著更高的平均得分，而Gemini 1.5 Flash则优于GPT-4o。此外，Claude 3.5 Sonnet可以利用额外的成本惩罚机制实现更高的得分，而Gemini 1.5 Flash和GPT-4o则不能。对于每个模型类别，我们还观察到随机种子下涌现行为的差异，这表明对初始条件的敏感依赖性被低估。我们建议，我们的评估机制可以激发一类新的、成本效益高且信息丰富的LLM基准，重点关注LLM代理部署对社会合作基础设施的影响。|\n",
        "2412.12094": "|**2024-12-16**|**SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**|Guoxuan Chen et.al.|[2412.12094](http://arxiv.org/abs/2412.12094)|**[link](https://github.com/HKUDS/SepLLM)**|**大型语言模型（LLMs）在自然语言处理的各个任务上表现出色。然而，它们庞大的体积带来了巨大的挑战，特别是在计算需求和推理速度方面，这是由于它们的二次复杂性。在这项工作中，我们识别出一个关键模式：某些看似无意义的特殊标记（即分隔符）与语义上有意义的标记相比，对注意力分数的贡献不成比例。这一观察表明，这些分隔符之间的段信息可以有效地压缩到这些分隔符本身，而不会造成显著的信息损失。受此启发，我们引入了SepLLM，这是一个即插即用的框架，通过压缩这些段和消除冗余标记来加速推理。此外，我们还实现了高效的内核以加速训练。在无需训练、从头开始训练和训练后设置中的实验结果表明了SepLLM的有效性。值得注意的是，使用Llama-3-8B骨干网络，SepLLM在GSM8K-CoT基准测试上实现了超过50%的KV缓存减少，同时保持了可比的性能。此外，在流式设置中，SepLLM能够有效地处理长达400万个标记或更多的序列，同时保持一致的语言建模能力。**|\n",
        "2412.12087": "|**2024-12-16**|**Instruction-based Image Manipulation by Watching How Things Move**|Mingdeng Cao et.al.|[2412.12087](http://arxiv.org/abs/2412.12087)|null|本文介绍了一种新型的数据集构建流程，该流程从视频中采样帧对，并使用多模态大型语言模型（MLLMs）生成编辑指令，用于训练基于指令的图像操作模型。视频帧天生保留了主题和场景的身份，确保了编辑过程中的内容一致性。此外，视频数据捕捉到了多样、自然的动态，如非刚性主题运动和复杂的摄像机运动，这些动态在其他情况下难以建模，使其成为可扩展数据集构建的理想来源。采用这种方法，我们创建了一个新的数据集，用于训练InstructMove模型，该模型能够执行基于指令的复杂操作，这些操作在合成数据集中难以实现。我们的模型在调整主题姿势、重新排列元素和改变摄像机视角等任务中展现了最先进的性能。|\n",
        "2412.12077": "|**2024-12-16**|**CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology**|Yuxuan Sun et.al.|[2412.12077](http://arxiv.org/abs/2412.12077)|null|大型多模态模型（LMMs）的出现为病理学带来了重大进步。以往的研究主要侧重于分别训练补丁级和全切片图像（WSI）级模型，这限制了在不同补丁和WSI之间整合所学知识，并导致了冗余模型。在本研究中，我们介绍了CPath-Omni，这是第一个设计用于统一补丁级和WSI级图像分析的150亿参数LMM，它将包括分类、视觉问答、字幕和视觉提示等多种任务在这一水平和另一水平上整合。大量实验表明，CPath-Omni在42个数据集中的39个任务上实现了最先进的（SOTA）性能，超过了为单个任务训练的特定模型的表现或与之相当。此外，我们为CPath-Omni开发了一个专门的病理学CLIP视觉处理器，CPath-CLIP，它首次将不同的视觉模型结合在一起，并将一个大语言模型作为文本编码器整合进去，构建了一个更强大的CLIP模型，在九个零样本和四个小样本数据集上实现了SOTA性能。我们的研究结果突出了CPath-Omni统一多种病理学任务的能力，展示了其在简化并推进病理学基础模型领域的潜力。|\n",
        "2412.12075": "|**2024-12-16**|**CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding**|Guo Chen et.al.|[2412.12075](http://arxiv.org/abs/2412.12075)|null|现有的多模态大型语言模型（MLLM）的视频理解基准大多仅针对短视频。对于长视频理解，现有的基准往往仅依赖于多项选择题（MCQs）。然而，由于基于MCQ的评估固有的局限性以及MLLM推理能力的增强，模型可以通过结合短视频理解和排除法来给出当前答案，而不真正理解视频内容。为了解决这一差距，我们引入了CG-Bench，这是一个专为长视频中的线索-基础问答而设计的创新基准。CG-Bench强调模型检索相关线索的能力，增强评估的可信度。它具有1,219个手动编纂的视频，按14个主要类别、171个次要类别和638个三级类别进行分类，使其成为长视频分析最大的基准。该基准包括12,129个问答对，涵盖三种主要问题类型：感知、推理和幻觉。为了弥补纯MCQ评估的不足，我们设计了两种新颖的基于线索的评估方法：线索-基础白盒和黑盒评估，以评估模型是否基于对视频的正确理解生成答案。我们在CG-Bench上评估了多个封闭源和开源的MLLM。结果表明，与短视频相比，当前模型在理解长视频方面的表现显著不足，开源模型与商业模型之间存在显著差距。我们希望CG-Bench能够推进更可信、更有能力的MLLM在长视频理解方面的发展。所有注释和视频数据发布在https://cg-bench.github.io/leaderboard/。|\n",
        "2412.12072": "|**2024-12-16**|**Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats**|Kuleen Sasse et.al.|[2412.12072](http://arxiv.org/abs/2412.12072)|**[link](https://github.com/kuleens/fetch-dog-whistle)**|**请注意：本文包含可能令某些读者感到不安或冒犯的内容。犬哨子是具有双重含义的编码表达：一方面旨在对公众（外群体）传达，另一方面则向特定受众（内群体）传达特定信息。通常，这些表达被用来传达有争议的政治观点，同时保持合理否认的可能性和绕过内容审查过滤器。犬哨子的识别依赖于经过编辑的词汇表，而这些词汇表难以跟上时代的步伐。我们提出了\\textbf{FETCH!}，这是一个在大量社交媒体语料库中寻找新颖犬哨子的任务。我们发现，最先进的技术在三个不同的社交媒体案例研究中未能取得有意义的成果。我们提出了\\textbf{EarShot}，一个结合了向量数据库和大型语言模型（LLM）优势的新颖系统，以高效和有效地识别新的犬哨子。**|\n",
        "2412.12039": "|**2024-12-16**|**Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection**|Ira Ceka et.al.|[2412.12039](http://arxiv.org/abs/2412.12039)|null|尽管大型语言模型（LLMs）取得了显著的成就，但在漏洞检测等应用任务上表现有限。我们研究了各种用于漏洞检测的提示策略，并在这一探索过程中，提出了一种将漏洞的自然语言描述与对比链式推理方法相结合的提示策略，该方法通过使用来自合成数据集的对比样本来增强。我们的研究突出了LLMs通过将自然语言描述、对比推理和合成示例整合到全面的提示框架中检测漏洞的潜力。我们的结果显示，这种方法可以增强LLMs对漏洞的理解。在一个高质量的漏洞检测数据集（如SVEN）上，我们的提示策略可以将准确率、F1分数和成对准确率分别提高23%、11%和14%。|\n",
        "2412.12009": "|**2024-12-16**|**SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval**|Yueqian Lin et.al.|[2412.12009](http://arxiv.org/abs/2412.12009)|null|我们引入了一种新的长文本任务——语音信息检索（SIR），针对语音大型语言模型（Speech LLMs），并提出了SPIRAL，一个包含1,012个样本的基准测试，用于检验模型从大约90秒的语音输入中提取关键细节的能力。尽管当前语音大型语言模型在短文本任务上表现出色，但它们在处理较长音频序列的计算和表示需求上存在困难。为了解决这一局限性，我们提出了一种无训练的token剪枝策略——SpeechPrune，该策略利用语音文本相似度和近似注意力分数来高效地丢弃无关的token。在SPIRAL中，SpeechPrune在20%的剪枝率下，相较于原始模型和随机剪枝模型，分别实现了29%和47%的准确率提升。SpeechPrune即使在80%的剪枝水平下也能保持网络性能。这种方法突显了token级剪枝在高效和可扩展的长文本语音理解中的潜力。|\n",
        "2412.12004": "|**2024-12-16**|**The Open Source Advantage in Large Language Models (LLMs)**|Jiya Manchanda et.al.|[2412.12004](http://arxiv.org/abs/2412.12004)|null|大型语言模型（LLMs）标志着自然语言处理（NLP）领域的关键转变，它们在文本生成、翻译和特定领域推理方面取得了显著进展。由专有数据集和大量计算资源驱动的闭源模型，如GPT-4，目前处于性能最前沿。然而，它们因“黑箱”性质和限制可访问性而受到批评，这阻碍了可重复性和公平的AI发展。相比之下，LLaMA和BLOOM等开源倡议通过社区驱动的发展和计算效率优先，实现了民主化。这些模型在语言多样性和特定领域应用方面显著缩小了性能差距，同时为全球的研究人员和开发者提供了可访问的工具。值得注意的是，这两种范式都依赖于基础架构创新，如Vaswani等人于2017年提出的Transformer框架。闭源模型通过有效扩展而表现出色，而开源模型则适应于代表性不足的语言和领域中的实际应用。低秩适应（LoRA）和指令调整数据集等技术的应用使得开源模型在资源有限的情况下也能取得有竞争力的结果。毫无疑问，闭源和开源方法之间的紧张关系凸显了AI领域透明度与专有控制之间的更广泛辩论。伦理方面的考虑进一步突显了这种分歧。闭源系统限制了外部审查，而开源模型促进了可重复性和合作，但缺乏标准化的审计文档框架来减轻偏见。利用两种范式优势的混合方法可能会塑造LLM创新的未来，确保可访问性、具有竞争力的技术性能和道德部署。|\n",
        "2412.12001": "|**2024-12-16**|**LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts**|Zhuhao Wang et.al.|[2412.12001](http://arxiv.org/abs/2412.12001)|**[link](https://github.com/zh-wang-med/llm-rg4)**|编写放射学报告是一项复杂的任务，需要灵活性，其中放射科医生根据可用信息和特定的临床需求调整内容。然而，目前大多数放射学报告生成（RRG）模型都受到固定任务范式的限制，例如从单一图像中预测完整的“发现”部分，这本质上涉及到输入和输出的不匹配。训练后的模型缺乏处理多样化输入的灵活性，可能会生成有害的、与输入无关的幻觉。为了弥合当前RRG模型与实际临床需求之间的差距，我们首先开发了一个数据生成管道来创建一个新的MIMIC-RG4数据集，该数据集考虑了四种常见的放射学报告起草场景，并实现了输入和输出的完美对应。其次，我们提出了一种基于大型语言模型（LLM）的RRG框架，名为LLM-RG4，它利用LLM灵活的指令遵循能力和广泛的一般知识。我们进一步开发了一个自适应标记融合模块，它可以提供处理不同输入组合的灵活性，同时最大限度地减少与增加的输入量相关的额外计算负担。此外，我们提出了一个标记级损失加权策略，以引导模型将注意力集中在积极的和不确定的描述上。实验结果表明，LLM-RG4在MIMIC-RG4和MIMIC-CXR数据集上均实现了临床效率和自然语言生成的最先进性能。我们定量证明，我们的模型具有最小的与输入无关的幻觉，而当前的开源模型通常都存在这个问题。|\n",
        "2412.11995": "|**2024-12-16**|**Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support**|Devika Venugopalan et.al.|[2412.11995](http://arxiv.org/abs/2412.11995)|**[link](https://github.com/devika-prog/caregiver-conversational-support-tool)**|**照顾者（即父母和孩子的照顾社区成员）是学习分析中被低估的利益相关者。尽管照顾者的参与可以提高学生的学术成绩，但许多障碍阻碍了他们的参与，其中最显著的是与现代学校课程相关的知识差距。学习分析中的一个新兴研究课题是混合辅导，它包括教学和动机支持。照顾者在家庭作业中扮演着类似的角色，但尚不清楚学习分析如何支持他们。我们与照顾者合作的研究表明，对话支持是向照顾者提供有效支持学生学习的指导的有前景的方法。我们开发了一个系统，通过由大型语言模型（LLM）生成对话推荐来为照顾者提供教学支持。针对LLM已知的教学局限性，我们在使用开源的Llama 3 LLM进行提示工程实验的同时，利用了辅导系统的教学智能。这个LLM为照顾者提供了通过聊天支持孩子数学练习的消息推荐。通过少量提示和将辅导系统的实时问题解决情境与辅导实践示例相结合，产生了理想的消息推荐。这些推荐与十位初中照顾者进行了评估，他们重视推荐能够通过自我解释促进内容层次的支持和学生的元认知。我们贡献了对如何将辅导系统与LLM最佳结合以通过对话辅助支持混合辅导环境，从而促进照顾者在辅导系统中的有效参与的见解。**|\n",
        "2412.13178": "|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身智能体在执行复杂指令方面具有强大的能力，为具身机器人的潜在部署铺平了道路。然而，一个可预见的问题是，这些具身智能体也可能完美地执行一些危险任务，可能导致现实世界中的损害。为了研究这一问题，我们提出了SafeAgentBench——一个用于具身LLM智能体安全任务规划的新的基准。SafeAgentBench包括：（1）一个包含750个任务的新数据集，涵盖了10种潜在危险和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的基础具身环境，支持多智能体执行，并为8个最先进的基线提供了17个高级动作；（3）从执行和语义角度的可靠评估方法。实验结果表明，表现最佳的基线在安全任务中的成功率达到了69%，但在危险任务中的拒绝率仅为5%，这表明存在显著的安全风险。更多细节和代码可在https://github.com/shengyin1224/SafeAgentBench找到。**|\n",
        "2412.13175": "|**2024-12-17**|**DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation**|Miriam Wanner et.al.|[2412.13175](http://arxiv.org/abs/2412.13175)|null|分解后验证策略用于验证大型语言模型（LLM）生成的陈述，将陈述分解后再进行独立验证。去上下文化通过增强文本（陈述）确保其可以在原始语境之外进行验证，从而实现可靠的验证。虽然分解和去上下文化已被独立探索，但它们在完整系统中的相互作用尚未得到研究。它们相互冲突的目的可能产生紧张关系：分解将原子事实隔离，而去上下文化插入相关信息。此外，去上下文化的子陈述对验证步骤构成挑战：现在包含多个原子事实的增强文本中，应该验证哪一部分？我们对不同的分解、去上下文化和验证策略进行了评估，发现策略的选择会影响最终的事实性评分。此外，我们引入了DnDScore，一种去上下文化感知的验证方法，它将在上下文信息的背景下验证子陈述。|\n",
        "2412.13169": "|**2024-12-17**|**Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study**|Bolei Ma et.al.|[2412.13169](http://arxiv.org/abs/2412.13169)|**[link](https://github.com/soda-lmu/llm-opinion-german)**|**在最近的研究中，大型语言模型（LLMs）被越来越多地用于研究公众意见。本研究调查了LLMs的算法忠实度，即复制人类参与者社会文化背景和细微意见的能力。利用德国纵向选举研究（GLES）的开源问卷调查数据，我们将不同LLMs提示为通过将人口统计学特征纳入角色提示中，生成反映德国不同亚群体的合成公众意见。我们的结果显示，Llama在代表亚群体方面表现优于其他LLMs，尤其是在这些群体内部意见多样性较低时。我们的发现还进一步揭示，LLMs在代表左翼政党如绿党和左翼的支持者方面表现优于其他政党，与右翼政党AfD的匹配度最低。此外，在提示中包含或排除特定变量可以显著影响模型的预测。这些发现强调了将LLMs与更有效地模拟多样化公众意见相一致的重要性，同时最小化政治偏见并增强代表性稳健性。**|\n",
        "2412.13163": "|**2024-12-17**|**C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System**|Parker Addison et.al.|[2412.13163](http://arxiv.org/abs/2412.13163)|null|在寻求利用大型语言模型（LLMs）进行知识查询和分析的组织中，常常面临保持针对特定、最新信息的LLM微调，以确保答案相关性和实际性的挑战。检索增强生成（RAG）迅速成为解决组织维护专有模型挑战的可行方案，同时有助于减少LLM查询响应中的幻觉。然而，RAG在扩展数据管道以支持分层访问和不同数据源方面存在自身问题。在许多情况下，需要查询多个数据孤岛以提供更丰富、更相关的上下文供LLM使用。在组织信任边界内及之间分析数据源往往受到复杂的数据共享政策限制，禁止集中式数据存储，从而阻碍了RAG解决方案的快速和有效设置及扩展。在本文中，我们引入了机密计算（CC）技术作为安全联邦检索增强生成（FedRAG）的解决方案。我们提出的机密联邦RAG系统（C-FedRAG）通过确保上下文机密性，使RAG工作流程能够在数据提供者去中心化网络中安全连接和扩展。我们还展示了如何使用NVIDIA FLARE SDK实现C-FedRAG系统，并使用MedRAG工具包和MIRAGE基准数据集对其性能进行评估。|\n",
        "2412.13148": "|**2024-12-17**|**SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba, 2015）对于大型语言模型的成功至关重要。然而，它们在训练过程中维持额外的移动平均状态，导致内存需求比模型大几倍。这种开销对可扩展性和计算效率施加了限制。另一方面，虽然随机梯度下降（SGD）在内存效率方面是最优的，但它们在LLM训练中的能力有限（Zhao等，2024b）。为了解决这一困境，我们表明预处理SGD足以在LLMs上达到Adam级别的性能。具体来说，我们提出使用两个简单的算子对瞬时随机梯度进行预处理：$\\mathtt{GradNorm}$和$\\mathtt{GradWhitening}$。$\\mathtt{GradNorm}$稳定梯度分布，而$\\mathtt{GradWhitening}$对抗损失景观的局部曲率，分别。这导致了SWAN（带有白化和归一化的SGD），一种消除存储任何累积状态变量需要的随机优化器。经验表明，SWAN具有与SGD相同的内存占用，与Adam相比，总端到端内存减少了$\\approx 50\\%$。在语言建模任务中，SWAN在性能上与Adam相同，甚至有显著提高。具体来说，当使用350M和1.3B参数预训练LLaMa模型时，SWAN通过在不到一半的观察到的标记内达到相同的评估困惑度，实现了2倍的速度提升。|\n",
        "2412.13147": "|**2024-12-17**|**Are Your LLMs Capable of Stable Reasoning?**|Junnan Liu et.al.|[2412.13147](http://arxiv.org/abs/2412.13147)|**[link](https://github.com/open-compass/gpassk)**|**大型语言模型（LLMs）的快速发展在复杂推理任务中展示了显著的进步。然而，基准性能和现实应用之间存在显著的差距。我们认定这一差距主要源于当前的评价协议和指标，它们无法充分捕捉LLMs的全部能力，尤其是在既需要准确性又需要一致性的复杂推理任务中。本研究做出了两项关键贡献。首先，我们引入了G-Pass@k，这是一种新颖的评价指标，它提供对模型性能在多次采样尝试中的连续评估，量化了模型的峰值性能潜力和其稳定性。其次，我们提出了LiveMathBench，这是一个动态基准，包含了具有挑战性的现代数学问题，旨在在评估过程中最大限度地减少数据泄露风险。通过在LiveMathBench上对最先进的LLMs进行G-Pass@k的大量实验，我们提供了对它们的最大能力和运行一致性的全面洞察。我们的发现揭示了LLMs在“现实”推理能力方面有巨大的改进空间，突显了需要更稳健的评价方法。基准和详细结果可在以下网址获取：https://github.com/open-compass/GPassK。**|\n",
        "2412.13103": "|**2024-12-17**|**AI PERSONA: Towards Life-long Personalization of LLMs**|Tiannan Wang et.al.|[2412.13103](http://arxiv.org/abs/2412.13103)|null|在本研究中，我们提出了终身个性化大型语言模型的任务。虽然近年来LLM社区的主流工作主要集中在扩展数据和计算能力以提高LLM的能力，但我们认为，使LLM系统或语言代理能够持续适应每个独特用户的多样化和不断变化的个人资料，并提供最新的个性化援助也非常重要。我们提供了明确的任务表述，并介绍了一个简单、通用、有效且可扩展的框架，用于LLM系统和语言代理的终身个性化。为了促进对LLM个性化未来的研究，我们还介绍了合成现实基准和鲁棒评估指标的方法。我们将发布构建和基准测试终身个性化LLM系统的所有代码和数据。|\n",
        "2412.13102": "|**2024-12-17**|**AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark**|Jianlyu Chen et.al.|[2412.13102](http://arxiv.org/abs/2412.13102)|**[link](https://github.com/air-bench/air-bench)**|**评估在信息检索（IR）模型的发展中起着至关重要的作用。然而，当前基准测试，基于预定义的领域和人工标注的数据，在以经济高效的方式应对新兴领域的评估需求方面存在局限性。为了解决这一挑战，我们提出了自动异构信息检索基准（AIR-Bench）。AIR-Bench有三个关键特点：1）自动化。AIR-Bench中的测试数据由大型语言模型（LLMs）自动生成，无需人工干预。2）异构。AIR-Bench中的测试数据针对不同的任务、领域和语言生成。3）动态。AIR-Bench涵盖的领域和语言不断扩展，为社区开发者提供一个越来越全面的评估基准。我们开发了一个可靠且稳健的数据生成管道，基于真实世界的语料库自动创建多样化的高质量评估数据集。我们的研究发现，AIR-Bench中生成的测试数据与人工标注的测试数据高度一致，使AIR-Bench成为评估IR模型的可靠基准。AIR-Bench的资源可在https://github.com/AIR-Bench/AIR-Bench公开获取。**|\n",
        "2412.13050": "|**2024-12-17**|**Modality-Inconsistent Continual Learning of Multimodal Large Language Models**|Weiguo Pian et.al.|[2412.13050](http://arxiv.org/abs/2412.13050)|null|在这篇论文中，我们介绍了模态不一致持续学习（Modality-Inconsistent Continual Learning，简称MICL），这是一种针对多模态大型语言模型（Multimodal Large Language Models，简称MLLMs）的新持续学习场景，涉及具有不一致模态（图像、音频或视频）和不同任务类型（字幕或问答）的任务。与现有的仅限于视觉或模态增量设置不同，MICL结合了模态和任务类型的转变，这两种转变都会导致灾难性遗忘。为了应对这些挑战，我们提出了MoInCL，它采用了一个伪目标生成模块来减轻先前看到的模态中由于任务类型转变引起的遗忘。此外，它还结合了基于指令的知识蒸馏，以保留模型处理先前学习的模态的能力，当引入新的模态时。我们使用总共六个任务对MICL进行基准测试，并通过实验验证了我们提出的MoInCL的有效性。实验结果突出了MoInCL的优越性，显示出与代表性持续学习基线相比的显著改进。|\n",
        "2412.13018": "|**2024-12-17**|**OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain**|Shuting Wang et.al.|[2412.13018](http://arxiv.org/abs/2412.13018)|**[link](https://github.com/ruc-nlpir/omnieval)**|**作为大型语言模型（LLMs）的一种典型和实际应用，检索增强生成（RAG）技术受到了广泛关注，尤其是在LLMs可能缺乏特定领域知识的垂直领域。在本文中，我们引入了一个金融领域的全向和自动RAG基准，称为OmniEval。我们的基准以其多维评估框架为特点，包括以下方面：  （1）基于矩阵的RAG场景评估系统，将查询分为五个任务类别和16个金融主题，从而对多样化的查询场景进行结构化评估； （2）多维评估数据生成方法，结合基于GPT-4的自动生成和人工标注，在人工评估生成的实例中实现了87.47%的接受率； （3）多阶段评估系统，评估检索和生成性能，对RAG流程进行综合评估； （4）从基于规则和基于LLM的评估指标中衍生出的鲁棒评估指标，通过人工标注和LLM评估器的监督微调增强了评估的可靠性。  我们的实验证明了OmniEval的全面性，包括广泛的测试数据集，并突出了RAG系统在各个主题和任务上的性能差异，揭示了RAG模型在垂直领域提高其能力的重要机会。我们将基准的代码开源至\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}。**|\n",
        "2412.14171": "|**2024-12-18**|**Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces**|Jihan Yang et.al.|[2412.14171](http://arxiv.org/abs/2412.14171)|**[link](https://github.com/vision-x-nyu/thinking-in-space)**|**人类拥有从连续视觉观察中记住空间的视觉空间智力。然而，在百万规模视频数据集上训练的多模态大型语言模型（MLLMs）也能从视频中“在空间中思考”吗？我们提出了一个包含超过5000个问答对的新型基于视频的视觉空间智力基准（VSI-Bench），并发现MLLMs表现出了具有竞争力的——尽管低于人类——视觉空间智力。我们探究了模型在语言和视觉上如何表达它们在空间中的思考方式，发现尽管空间推理能力仍然是MLLMs达到更高基准性能的主要瓶颈，但局部世界模型和空间意识确实存在于这些模型中。值得注意的是，现有的语言推理技术（例如，思维链、自我一致性、思维树）并不能提高性能，而在问答过程中明确生成认知图则增强了MLLMs的空间距离能力。**|\n",
        "2412.14161": "|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们每天都在与计算机互动，无论是在日常生活中还是在工作中，许多工作都可以完全通过计算机和互联网来完成。同时，得益于大型语言模型（LLMs）的改进，与周围环境互动并产生影响的AI代理也得到了快速发展。但是，AI代理在帮助加速甚至自主执行工作相关任务方面的表现如何？这个问题的答案对希望将AI引入工作流程的行业以及理解AI采用对劳动力市场可能产生的影响的经济政策具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进步，本文介绍了TheAgentCompany，这是一个可扩展的基准，用于评估以类似数字工作者方式与世界互动的AI代理：通过浏览网页、编写代码、运行程序以及与其他同事沟通。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司环境，并创建了各种可能由该公司员工执行的任务。我们测试了由封闭API-based和开放权重的语言模型（LMs）驱动的基线代理，并发现最具竞争力的代理可以自主完成24%的任务。这描绘了一幅关于LM代理任务自动化的复杂图景——在一个模拟真实工作场所的环境中，大量简单任务可以自主解决，但更困难的长远任务仍超出现有系统的能力范围。**|\n",
        "2412.14146": "|**2024-12-18**|**Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models**|Atin Sakkeer Hussain et.al.|[2412.14146](http://arxiv.org/abs/2412.14146)|null|本文介绍了用于数据分析中多步洞察合成的先进推理与转换引擎（ARTEMIS-DA），这是一个旨在增强大型语言模型（LLMs）以解决复杂、多步数据分析任务的创新框架。ARTEMIS-DA集成了三个核心组件：规划器，它将复杂用户查询分解为包含数据预处理、转换、预测建模和可视化的结构化、顺序指令；编码器，它动态生成并执行Python代码以实现这些指令；以及图形器，它解释生成的可视化以得出可操作的见解。通过协调这些组件之间的协作，ARTEMIS-DA有效地管理涉及高级推理、多步转换和跨不同数据模态综合的复杂分析工作流程。该框架在WikiTableQuestions和TabFact等基准测试中实现了最先进的（SOTA）性能，证明了其以精确性和适应性处理复杂分析任务的能力。通过结合LLMs的推理能力、自动代码生成与执行以及视觉分析，ARTEMIS-DA为多步洞察合成提供了一个强大、可扩展的解决方案，解决了数据分析中的广泛挑战。|\n",
        "2412.14141": "|**2024-12-18**|**LLMs can realize combinatorial creativity: generating creative ideas via LLMs for scientific research**|Tianyang Gu et.al.|[2412.14141](http://arxiv.org/abs/2412.14141)|null|科学理念生成在创造力理论和计算创造力研究中得到了广泛研究，为理解和实施创造性过程提供了有价值的框架。然而，近年来使用大型语言模型（LLMs）进行研究理念生成的相关工作往往忽视了这些理论基础。我们提出了一种框架，该框架明确地利用LLMs实施组合创造力理论，包括一个用于跨领域知识发现的通用化检索系统和用于理念生成的结构化组合过程。检索系统将不同抽象层次的概念映射到一起，以实现不同领域之间的有意义联系，而组合过程则系统地分析和重新组合组件以生成新颖的解决方案。在OAG-Bench数据集上的实验表明，我们的框架非常有效，在生成符合实际研究发展的理念方面，持续优于基线方法（在多个指标上提高了7%-10%的相似度分数）。我们的结果为LLMs在适当的理论框架指导下实现组合创造力提供了强有力的证据，既促进了AI辅助研究的实际进步，也加深了对机器创造力的理论理解。|\n",
        "2412.14137": "|**2024-12-18**|**Design choices made by LLM-based test generators prevent them from finding bugs**|Noble Saji Mathews et.al.|[2412.14137](http://arxiv.org/abs/2412.14137)|null|随着大量关于使用大型语言模型（LLM）进行自动化测试用例生成的研究和商业工具的出现，本文批判性地审视了近期基于LLM的测试生成工具，如Codium CoverAgent和CoverUp，是否能够有效地发现错误或无意中验证错误代码。考虑到错误只有在失败的测试用例中才会暴露，我们探讨了以下问题：当这些工具的测试预言机旨在通过时，它们是否真的能够实现软件测试的预期目标？使用真实的人类编写的错误代码作为输入，我们评估了这些工具，展示了LLM生成的测试如何未能检测到错误，并且更令人担忧的是，它们的设计如何通过验证生成的测试套件中的错误并拒绝揭示错误的测试而使情况恶化。这些发现引发了关于LLM基于测试生成工具设计有效性的重要问题，以及它们对软件质量和测试套件可靠性的影响。|\n",
        "2412.14093": "|**2024-12-18**|**Alignment faking in large language models**|Ryan Greenblatt et.al.|[2412.14093](http://arxiv.org/abs/2412.14093)|**[link](https://github.com/redwoodresearch/alignment_faking_public)**|**我们展示了一个大型语言模型进行对齐造假的现象：在训练过程中选择性遵守其训练目标，以防止其行为在训练后发生变化。首先，我们给Claude 3 Opus一个系统提示，表示它正在接受训练以回答所有查询，即使是有害的，这与它之前的训练拒绝此类查询的目标相冲突。为了使模型推断出它正在训练中，我们说它只会在与免费用户进行的对话中接受训练，而不是付费用户。我们发现模型有14%的时间会遵守来自免费用户的有害查询，而对于付费用户则几乎不会。解释这个差距，在模型遵守来自免费用户的有害查询的几乎所有情况下，我们观察到明确的对齐造假推理，模型表示它在训练中战略性地回答有害查询，以保持其训练后偏好无害的行为。接下来，我们研究了一个更现实的设置，其中训练过程中信息不是通过系统提示提供，而是通过在模仿预训练数据的合成文档上进行训练来提供——并观察到类似的对齐造假现象。最后，我们研究了通过强化学习训练模型以遵守有害查询的效果，我们发现这提高了对齐造假推理的比率至78%，尽管这也增加了训练后的合规性。我们还观察到其他行为，例如当给模型提供轻松的机会时，它会泄露其权重。虽然我们通过告诉模型何时以及根据什么标准进行训练，使其对齐造假变得更容易，但我们并没有指示模型进行对齐造假，或者给它任何明确的目标。由于未来的模型可能会在没有被告知的情况下推断有关其训练过程的信息，我们的结果表明，未来模型存在对齐造假的风险，无论这种风险是由于良性的偏好（如本例中）还是其他原因。**|\n",
        "2412.14085": "|**2024-12-18**|**Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report**|Markus Dablander et.al.|[2412.14085](http://arxiv.org/abs/2412.14085)|null|视频游戏是人工智能（AI）系统的一个自然且协同的应用领域，它既可以为玩家体验和沉浸感提供增强的可能性，同时还能为推进人工智能技术在一般领域的发展提供有价值的基准和虚拟环境。本报告从当前研究背景出发，对将最先进的AI方法，特别是深度学习，应用于数字游戏的五个有前景的研究途径进行了概述。本工作的目标是概述一个精心挑选的、非详尽的、鼓励性的研究方向列表，这些方向在人工智能和视频游戏的交汇处，可能有助于激发未来更加严谨和全面的研究努力。我们讨论了以下五个方面：（一）研究大型语言模型作为游戏代理建模的核心引擎；（二）利用神经网络细胞自动机进行程序化游戏内容生成；（三）通过深度代理建模加速计算成本高昂的游戏模拟；（四）利用自监督学习获取有用的视频游戏状态嵌入；（五）使用未标记的视频数据训练交互式世界的生成模型。我们还简要讨论了将高级深度学习系统集成到视频游戏开发中当前所面临的技术挑战，并指出未来可能带来更多进展的关键领域。|\n",
        "2412.14063": "|**2024-12-18**|**Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification**|Kyle Thompson et.al.|[2412.14063](http://arxiv.org/abs/2412.14063)|**[link](https://github.com/rkthomps/coq-modeling)**|使用证明辅助工具（如Coq）进行形式验证可以创建高质量的软件。然而，验证过程需要大量的专业知识和手动编写证明的努力。最近的研究探索了利用机器学习和大型语言模型（LLMs）自动化证明合成。这项工作表明，识别相关前提，如引理和定义，有助于合成。我们提出了Rango，这是一个为Coq提供的全自动证明合成工具，它能够自动识别相关前提，并从当前项目中识别相似的证明，在合成过程中使用它们。Rango在证明的每一步都使用检索增强来自动确定在它微调的LLM的上下文中包含哪些证明和前提。通过这种方式，Rango能够适应项目和证明的演变状态。我们创建了一个新的数据集CoqStoq，包含来自GitHub的2,226个开源Coq项目和196,929个定理，该数据集既包括训练数据，也包括精心策划的、维护良好的项目的评估基准。在这个基准上，Rango为32.0%的定理合成了证明，比之前最先进的工具Tactician多出了29%的定理。我们的评估还显示，将相关证明添加到Rango的上下文中，导致已证明的定理数量增加了47%。|\n",
        "2412.14062": "|**2024-12-18**|**Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets**|Simon Thorne et.al.|[2412.14062](http://arxiv.org/abs/2412.14062)|null|生成式人工智能和大型语言模型（LLMs）在自动化电子表格公式创建方面具有潜力。然而，由于幻觉、偏见和用户技能的差异性，从生成式人工智能获得的输出不能被假设是准确或可信的。为了解决这些挑战，本文提出了一种基于评估公式的透明度和可靠性的可信度框架。公式的透明度通过可解释性（理解公式的推理）和可见性（检查底层算法）来探索。生成的公式的可靠性从可靠性（一致性和准确性）和伦理考量（偏见和公平性）两个方面进行评估。论文还考察了这些指标背后的驱动因素，包括幻觉、训练数据偏见和构建不良的提示。最后，考虑了对技术的不信任示例，并探讨了其后果。|\n",
        "2412.14056": "|**2024-12-18**|**A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future**|Shilin Sun et.al.|[2412.14056](http://arxiv.org/abs/2412.14056)|**[link](https://github.com/shilinsun/mxai_review)**|**人工智能（AI）通过计算能力的提升和海量数据集的增长而迅速发展。然而，这种进步也加剧了对AI模型“黑盒”特性的解释挑战。为了解决这些担忧，可解释人工智能（XAI）应运而生，其重点是透明度和可解释性，以增强人类对AI决策过程的了解和信任。在多模态数据融合和复杂推理场景的背景下，多模态可解释人工智能（MXAI）的提出将多种模态集成到预测和解释任务中。同时，大型语言模型（LLMs）的出现导致了自然语言处理领域的重大突破，但它们的复杂性进一步加剧了MXAI的问题。为了深入了解MXAI方法的发展并提供构建更透明、公平和值得信赖的AI系统的关键指导，我们从历史角度回顾了MXAI方法，并将它们分为四个时期：传统机器学习、深度学习、判别性基础模型和生成型LLMs。我们还回顾了MXAI研究中使用的评估指标和数据集，最后讨论了未来的挑战和方向。与本次回顾相关的一个项目已创建在https://github.com/ShilinSun/mxai_review。**|\n",
        "2412.15208": "|**2024-12-19**|**OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving**|Shuo Xing et.al.|[2412.15208](http://arxiv.org/abs/2412.15208)|**[link](https://github.com/taco-group/openemma)**|**随着多模态大型语言模型（MLLMs）的出现，它们在众多现实应用领域产生了重大影响，特别是在自动驾驶（AD）领域。它们处理复杂视觉数据并推理复杂驾驶场景的能力为端到端自动驾驶系统开辟了新的范式。然而，开发端到端自动驾驶模型进展缓慢，因为现有的微调方法需要大量资源，包括强大的计算能力、大规模数据集和大量资金。受最近推理计算领域进步的启发，我们提出了OpenEMMA，这是一个基于MLLMs的开源端到端框架。通过整合思维链推理过程，OpenEMMA在利用各种MLLMs时相较于基线实现了显著提升。此外，OpenEMMA在各种具有挑战性的驾驶场景中展示了有效性、泛化能力和鲁棒性，为自动驾驶提供了一种更高效、更有效的方法。我们将所有代码发布在https://github.com/taco-group/OpenEMMA上。**|\n",
        "2412.15194": "|**2024-12-19**|**MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark**|Qihao Zhao et.al.|[2412.15194](http://arxiv.org/abs/2412.15194)|**[link](https://github.com/microsoft/mmlu-cf)**|**多选题数据集，如大规模多任务语言理解（MMLU），被广泛用于评估大型语言模型（LLMs）的常识、理解和问题解决能力。然而，这些基准的开放源代码性质以及LLMs训练数据的广泛来源不可避免地导致了基准污染，导致评估结果不可靠。为了缓解这个问题，我们提出了一种无污染且更具挑战性的多选题基准，称为MMLU-CF。该基准通过避免无意和恶意的数据泄露来重新评估LLMs对世界知识的理解。为了避免无意数据泄露，我们从更广泛的领域获取数据，并设计了三条去污染规则。为了防止恶意数据泄露，我们将基准分为难度和主题分布相似的验证集和测试集。测试集保持闭源状态以确保结果的可靠性，而验证集公开可用以促进透明度和独立验证。我们对主流LLMs的评估显示，强大的GPT-4o在测试集上仅实现了5次尝试的73.4%得分和0次尝试的71.9%得分，这表明我们创建更严格和无污染评估标准的方法是有效的。GitHub仓库可在https://github.com/microsoft/MMLU-CF找到，数据集可参考https://huggingface.co/datasets/microsoft/MMLU-CF。**|\n",
        "2412.15188": "|**2024-12-19**|**LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation**|Weijia Shi et.al.|[2412.15188](http://arxiv.org/abs/2412.15188)|null|我们提出了一种名为LlamaFusion的框架，该框架能够赋予预训练的纯文本大型语言模型（LLMs）多模态生成能力，使其能够理解和生成任意序列中的文本和图像。LlamaFusion利用Llama-3现有的权重进行文本的自回归处理，同时引入了额外的并行变换器模块来处理图像的扩散。在训练过程中，每个模态的数据被路由到其专门的模块：模态特定的前馈层、查询-键-值投影和归一化层独立处理每个模态，而共享的自注意力层允许文本和图像特征之间的交互。通过冻结文本特定模块，仅训练图像特定模块，LlamaFusion在保留纯文本LLMs的语言能力的同时，发展了强大的视觉理解和生成能力。与从头开始预训练多模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOPs，就提高了20%的图像理解能力和3.6%的图像生成能力，同时保持了Llama-3的语言能力。我们还展示了该框架可以适应具有多模态生成能力的现有视觉-语言模型。总的来说，这个框架不仅利用了现有的纯文本LLMs的计算投资，还实现了语言和视觉能力的并行发展，为高效的多模态模型开发提供了一个有希望的方向。|\n",
        "2412.15184": "|**2024-12-19**|**Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning**|Simon Frieder et.al.|[2412.15184](http://arxiv.org/abs/2412.15184)|null|常用的用于训练和评估基于人工智能的数学协同助手（主要是大型语言模型）的数学能力的数据集系列存在几个缺陷。这些局限性包括数学复杂性的范围受限，通常不超过本科低年级的数学水平，二元评分协议和其他问题，这使得基于证明的综合评估套件变得困难。我们系统地探讨了这些局限性，并认为提升大型语言模型或任何未来基于人工智能的数学助手（协同助手或“思维伙伴”）的能力，需要在数学数据集的设计和数学能力评估标准上实现范式转变：有必要从基于结果的数据集（定理陈述到定理证明）转向将数学研究实践的丰富方面转化为LLM可以训练的数据。这些包括数学工作流程（在创建新数学时通常执行的一系列原子任务，可能取决于子领域），这是证明发现过程的重要组成部分。此外，我们主张数学数据集开发者考虑G. Pólya于1949年提出的“有动机的证明”这一概念，它可以作为提供更好证明学习信号的数据集的蓝图，缓解一些已提到的局限性。最后，我们引入了数学数据表，扩展了通用的、数据集无关的数据表变体：我们提供了一个专门为数学数据集设计的问卷，我们敦促数据集创建者将其与数据集一起提供。这将使创作者意识到他们数据集的潜在局限性，同时使读者能够从训练和评估数学协同助手的视角轻松评估它。|\n",
        "2412.15178": "|**2024-12-19**|**HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages**|Aman Chaturvedi et.al.|[2412.15178](http://arxiv.org/abs/2412.15178)|null|大型语言模型（LLM）基础的编码工具作为软件开发助手取得了巨大成功，但它们通常是为通用编程任务设计的，在诸如高性能计算（HPC）等更专业领域表现不佳。为这些领域创建专门的模型和工具对于充分利用LLM在HPC等领域的优势至关重要。虽然先前的研究已经探索了HPC特定的模型，但LLM在生成并行代码方面仍然存在困难，而且目前并不清楚还有哪些障碍阻碍了这些LLM的发展，以及需要采取哪些措施来克服它们。在这项工作中，我们对调整专门HPC LLM的多个方面进行了深入研究，以更好地理解挑战。基于我们的发现，我们对一个专门的HPC LLM进行了微调和评估，该模型目前被证明是用于并行代码生成的最佳性能开源代码LLM。|\n",
        "2412.15177": "|**2024-12-19**|**Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying**|Federico Castagna et.al.|[2412.15177](http://arxiv.org/abs/2412.15177)|**[link](https://github.com/fcast07/cqot)**|**研究强调了尽管人工智能研究取得了突破性进展，但即使是最高水平的大语言模型（LLMs）在执行逻辑和数学推理时仍然存在困难。结果似乎表明，LLMs仍然作为（高度先进的）数据模式识别器工作，当尝试概括和解决模型以前从未见过或与训练数据中呈现的样本不相似的问题时，表现不佳。为了解决这一令人信服的问题，本文利用了论证理论文献中的关键问题概念，特别关注图尔敏的论证模型。我们表明，采用这些关键问题可以提高LLMs的推理能力。通过探究模型推理过程背后的理由，LLM可以评估是否发生了某些逻辑错误，并在向用户提供最终回复之前纠正它。这一基本思想源自任何有效论证程序的黄金标准：如果结论是由被接受的论据所蕴含的，则结论有效。或者，用这样的亚里士多德原则在现实世界的不完整信息和假设逻辑近似中表述，如果未被证明无效，则结论有效。这种方法成功地将模型输出引导通过推理管道，从而在基线和其思维链（CoT）实现方面取得了更好的性能。为此，本文在多个LLMs上对所提出的方案在MT-Bench推理和数学任务上的广泛评估提供了详细说明。**|\n",
        "2412.15176": "|**2024-12-19**|**Rethinking Uncertainty Estimation in Natural Language Generation**|Lukas Aichberger et.al.|[2412.15176](http://arxiv.org/abs/2412.15176)|null|大型语言模型（LLMs）在现实应用中越来越受欢迎，这推动了对其生成文本可信度的评估需求。为此，可靠的不确定性估计至关重要。由于当前LLMs通过随机过程自回归地生成文本，相同的提示可以导致不同的输出。因此，主要的不确定性估计方法生成并分析多个输出序列以确定LLM的不确定性。然而，生成输出序列在计算上非常昂贵，使得这些方法在规模上不切实际。在这项工作中，我们审视了领先方法的理论基础，并探索了提高其计算效率的新方向。基于正确评分规则框架，我们发现最有可能的输出序列的负对数似然构成了一个理论上有根据的不确定性度量。为了近似这个替代度量，我们提出了G-NLL，它具有仅使用贪婪解码生成的单个输出序列即可获得的优势。这使得不确定性估计更加高效和简单，同时保持了理论严谨性。实证结果表明，G-NLL在各种LLMs和任务上实现了最先进的性能。我们的工作为自然语言生成中的高效和可靠不确定性估计奠定了基础，挑战了当前领域主导的更复杂计算方法的需求。|\n",
        "2412.15151": "|**2024-12-19**|**Language Models as Continuous Self-Evolving Data Engineers**|Peidong Wang et.al.|[2412.15151](http://arxiv.org/abs/2412.15151)|null|大型语言模型（LLMs）在各项任务中展现出非凡的能力，但其进一步发展受到高质量训练数据缺乏的限制。此外，传统的训练方法过度依赖专家标注数据，给LLMs的性能设定了上限。为解决这一问题，我们提出了一种新型范式，允许LLMs通过自主生成、清洗、审查和标注带偏好信息的数据来自我训练，称为LANCE。我们的方法表明，LLMs可以作为持续自我进化的数据工程师，显著减少后训练数据构建过程的时间和成本。通过对Qwen2不同变体的迭代微调，我们验证了LANCE在各项任务中的有效性，表明它可以持续提升模型性能并保持高质量的数据生成。在八个基准维度上，LANCE使Qwen2-7B的平均得分提升了3.36分，使Qwen2-7B-Instruct的平均得分提升了2.70分。这种具有自主数据构建的训练范式不仅减少了对人专家或外部模型的依赖，还确保数据与人类价值观和偏好一致，为开发超越人类能力的高级智能系统铺平了道路。|\n",
        "2412.15127": "|**2024-12-19**|**Adaptive Pruning for Large Language Models with Structural Importance Awareness**|Haotian Zheng et.al.|[2412.15127](http://arxiv.org/abs/2412.15127)|null|近期大型语言模型（LLMs）的进步显著提升了语言理解和生成能力。然而，由于LLMs对计算和存储资源的高需求，它们在资源受限的边缘设备上的部署变得困难。为了解决这个问题，我们提出了一种新的LLM模型剪枝方法，称为结构感知自适应剪枝（SAAP），以显著降低计算和内存成本，同时保持模型性能。我们首先定义了一个自适应重要性融合指标，通过考虑它们的同方差不确定性来评估LLMs中所有耦合结构的重要性。然后，我们对所有模块的重要性进行排序，以确定应剪枝的具体层以满足特定的性能要求。此外，我们开发了一种新的分组微调策略，以提高LLMs的推理效率。最后，我们在两个常见任务上评估了所提出的SAAP方法，即零样本分类和文本生成。实验结果表明，我们的SAAP方法优于几种最先进的基线方法，在LLaMA-7B、Vicuna-7B和LLaMA-13B上分别实现了2.17%、2.37%和2.39%的准确率提升。此外，SAAP将标记生成速度提高了5%，展示了其在资源受限场景中的实际优势。|\n",
        "2412.15118": "|**2024-12-19**|**Outcome-Refining Process Supervision for Code Generation**|Zhuohao Yu et.al.|[2412.15118](http://arxiv.org/abs/2412.15118)|**[link](https://github.com/zhuohaoyu/orps)**|大型语言模型在代码生成方面展现出非凡的能力，但它们往往难以处理需要深度算法推理的复杂编程任务。虽然通过学习奖励模型进行过程监督在引导推理步骤方面显示出希望，但它需要昂贵的训练数据，且评估不可靠。我们提出了一种名为结果精炼过程监督的新范式，将结果精炼本身视为需要监督的过程。我们的框架利用具体执行信号来定位推理步骤的监督，同时使用树状结构探索来同时维护多个解决方案轨迹。实验表明，我们的方法即使对更小的模型也能在编程竞赛任务上实现高成功准确率和性能指标，比传统的奖励模型提供了更可靠的验证，且无需训练PRM。在我们的方法下，5个模型和3个数据集均实现了显著改进：正确性平均提高了26.9%，效率提高了42.2%。结果表明，提供具有具体验证信号的有序推理空间对于解决复杂编程任务至关重要。我们已在以下链接开源所有代码和数据：https://github.com/zhuohaoyu/ORPS|\n",
        "2412.16158": "|**2024-12-20**|**HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding**|Chenxin Tao et.al.|[2412.16158](http://arxiv.org/abs/2412.16158)|null|大型语言模型（LLMs）的快速发展推动了视觉-语言模型（VLMs）的发展。单体VLMs通过避免特定模态的编码器，为组合型模型提供了有希望的替代方案，但面临着性能不足的挑战。大多数现有的单体VLMs需要调整预训练的LLMs以获得视觉能力，这可能会降低其语言能力。为了解决这一困境，本文提出了一种名为HoVLE的新型高性能单体VLM。我们注意到，当图像嵌入与文本嵌入对齐时，LLMs已被证明能够解释图像。当前单体VLMs的挑战实际上在于缺乏一个对视觉和语言输入都全面的嵌入模块。因此，HoVLE引入了一个全面的嵌入模块，将视觉和文本输入转换为共享空间，使LLMs能够以处理文本的方式处理图像。此外，精心设计了多阶段训练策略来增强全面的嵌入模块。首先，它被训练从预训练的视觉编码器中提炼视觉特征，从LLM中提取文本嵌入，使大规模训练能够使用未配对的随机图像和文本标记。整个模型进一步在多模态数据上进行下一标记预测以对齐嵌入。最后，还加入了指令调整阶段。我们的实验表明，HoVLE在各种基准测试上实现了接近领先组合模型的性能，并且比之前的单体模型大幅超越了它们。模型可在https://huggingface.co/OpenGVLab/HoVLE获取。|\n",
        "2412.16145": "|**2024-12-20**|**Offline Reinforcement Learning for LLM Multi-Step Reasoning**|Huaijie Wang et.al.|[2412.16145](http://arxiv.org/abs/2412.16145)|**[link](https://github.com/jwhj/oreo)**|为了快速适应复杂任务，提高大型语言模型（LLMs）的多步推理能力至关重要。虽然直接偏好优化（DPO）在使LLMs与人类偏好对齐方面显示出前景，但它不太适合多步推理任务，原因如下：（1）DPO依赖于成对偏好数据，这些数据对于多步推理任务并不容易获得；（2）它对所有标记进行统一处理，这使得它在多步推理任务中无法有效地进行信用分配，而这些任务往往伴随着稀疏的奖励。在本工作中，我们提出了OREO（离线推理优化），这是一种用于增强LLM多步推理能力的离线强化学习（RL）方法。基于最大熵强化学习的前期研究成果，OREO通过优化软贝尔曼方程共同学习策略模型和值函数。我们从原则上证明了它减少了收集成对数据的需求，并实现了更好的信用分配。在实证研究中，OREO在多步推理基准测试中超越了现有的离线学习方法，包括数学推理任务（GSM8K、MATH）和具身智能体控制（ALFWorld）。当有额外资源可用时，该方法可以扩展到多迭代框架。此外，学习到的值函数可以用来引导免费树搜索，这可以在测试时间进一步提升性能。|\n",
        "2412.16135": "|**2024-12-20**|**Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation**|Seyedreza Mohseni et.al.|[2412.16135](http://arxiv.org/abs/2412.16135)|null|恶意软件作者通常会使用代码混淆技术来使他们的恶意软件更难被检测。现有的生成混淆代码的工具通常需要访问原始源代码（例如C++或Java），并且添加新的混淆技术是一个复杂且劳动密集的过程。在这项研究中，我们提出了以下问题：大型语言模型（LLMs）能否潜在地生成新的混淆汇编代码？如果是的话，这将给反病毒引擎带来风险，并可能增加攻击者创建新混淆模式的能力。我们通过开发包含MetamorphASM数据集（MAD）以及三种代码混淆技术（无效代码、寄存器替换和控制流更改）的MetamorphASM基准来肯定地回答了这个问题。MetamorphASM系统地评估了LLMs使用MAD生成和分析混淆代码的能力，其中包含328,200个混淆汇编代码样本。我们发布了这个数据集，并分析了各种LLMs（例如GPT-3.5/4、GPT-4o-mini、Starcoder、CodeGemma、CodeLlama、CodeT5和LLaMA 3.1）生成混淆汇编代码的成功率。评估使用了既定的信息论指标和人工审查以确保正确性，并为研究人员研究和发展缓解这种风险提供了基础。源代码可以在以下GitHub链接找到：https://github.com/mohammadi-ali/MetamorphASM。|\n",
        "2412.16132": "|**2024-12-20**|**Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information**|Dirk Bergemann et.al.|[2412.16132](http://arxiv.org/abs/2412.16132)|null|我们研究了在代理对其偏好和共同收益相关状态都持有私人信息时的机制设计。我们表明，即使是在有利条件下，当代理具有多维类型时，标准的信息驱动机制也无法实现社会有效分配。为了克服这一局限性，我们提出了数据驱动机制，这些机制利用额外的分配后信息，该信息被建模为收益相关状态的估计器。我们的数据驱动机制扩展了经典的维克瑞-克拉克-格罗夫斯（VCG）类。我们表明，当状态完全揭示或效用在线性无偏估计器中时，它们在后验均衡中实现了精确实现。我们还表明，它们使用一致估计器实现了近似实现，随着估计器的收敛，趋近于精确实现，并给出了收敛率的界限。我们展示了这些机制在数字广告拍卖和基于大型语言模型（LLM）的机制中的应用，在这些应用中，用户参与自然地揭示了相关信息。|\n",
        "2412.16120": "|**2024-12-20**|**PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics**|Daniil Larionov et.al.|[2412.16120](http://arxiv.org/abs/2412.16120)|null|在自然语言处理（NLP）领域，评估机器生成自然语言内容的品质是一项具有挑战性的任务。近期，大型语言模型（LLMs）如GPT-4被用于此目的，但由于复杂评估提示需要大量标记的使用，它们在计算上非常昂贵。在本文中，我们提出了一种提示优化方法，该方法使用一个较小的、微调的语言模型来压缩用于评估提示的输入数据，从而在下游评估中使用大型LLMs时减少标记使用和计算成本。我们的方法包括一个两阶段微调过程：监督性微调随后是偏好优化，以根据人类偏好优化模型的输出。我们专注于机器翻译（MT）评估，并以GEMBA-MQM指标作为起点。我们的结果显示，在评估质量不受损失的情况下，标记使用量减少了2.37倍。这项工作使最先进的基于LLM的指标如GEMBA-MQM更具成本效益和效率，提高了其更广泛使用的可及性。|\n",
        "2412.16119": "|**2024-12-20**|**Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts**|Muhammad Abdullah Sohail et.al.|[2412.16119](http://arxiv.org/abs/2412.16119)|**[link](https://github.com/abdullahsohaill/cs6303-researchproject)**|**本研究探讨了大型语言模型（LLMs），特别是GPT-4o，在低资源脚本（如乌尔都语、阿尔巴尼亚语和塔吉克语）中的光学字符识别（OCR）潜力，以英语作为基准。研究使用精心整理的包含文本长度、字体大小、背景颜色和模糊度等可控变体的2,520张图像数据集，模拟了各种现实世界的挑战。结果表明，基于零样本LLM的OCR存在局限性，尤其是在语言复杂的脚本中，强调了标注数据集和微调模型的需求。这项工作强调了解决文本数字化中可及性差距的紧迫性，为未得到充分服务的语言铺平了通向包容性和稳健OCR解决方案的道路。**|\n",
        "2412.16117": "|**2024-12-20**|**PruneVid: Visual Token Pruning for Efficient Video Large Language Models**|Xiaohu Huang et.al.|[2412.16117](http://arxiv.org/abs/2412.16117)|**[link](https://github.com/visual-ai/prunevid)**|**本文介绍了一种名为PruneVid的视觉标记剪枝方法，旨在提高多模态视频理解的效率。由于大型语言模型（LLMs）在理解视觉模态方面的扩展能力，它们在视频任务中展现出了有希望的性能。然而，视频数据中的大量冗余给LLMs带来了显著的计算挑战。为了解决这个问题，我们提出了一种无需训练的方法，该方法包括：1）通过合并时空标记来最小化视频冗余，2）利用LLMs的推理能力，有选择地剪枝与问题标记相关的视觉特征，从而提高模型效率。我们在多个视频基准测试中验证了我们的方法，结果表明，PruneVid可以在保持与不同模型网络竞争性性能的同时，剪枝超过80%的标记。这突显了相较于现有剪枝方法，其卓越的有效性和效率。代码：https://github.com/Visual-AI/PruneVid。**|\n",
        "2412.16114": "|**2024-12-20**|**The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse**|Mahyar Habibi et.al.|[2412.16114](http://arxiv.org/abs/2412.16114)|null|关于如何在社交媒体上调节有毒言论以及内容监管如何影响在线讨论，目前存在持续的争议。我们提出并验证了一种使用计算语言学中的文本嵌入来衡量在线讨论中内容监管引起的扭曲的方法。我们在一个包含500万条美国政治推文的代表性数据集上测试了我们的测量方法，发现删除有毒推文会扭曲在线内容。这一发现在不同嵌入模型、毒性指标和样本中是一致的。重要的是，我们证明内容监管引起的扭曲并非由有毒语言造成。相反，我们表明，作为一种副作用，内容监管改变了嵌入空间的均值和方差，扭曲了在线内容的主题组成。最后，我们提出了一种替代内容监管的方法，该方法使用生成式大型语言模型重新措辞有毒推文，以保留其可挽救的内容，而不是完全删除。我们证明了这种重新措辞策略可以减少毒性，同时最大限度地减少在线内容的扭曲。|\n",
        "2412.16100": "|**2024-12-20**|**Logical Consistency of Large Language Models in Fact-checking**|Bishwamittra Ghosh et.al.|[2412.16100](http://arxiv.org/abs/2412.16100)|null|近年来，大型语言模型（LLMs）在执行各种自然语言任务方面取得了显著的成功，例如语言翻译、问答、总结、事实核查等。尽管LLMs在生成类似人类文本的能力上令人印象深刻，但它们因不一致的响应而闻名——输入查询的保留意义的变化导致不一致的响应，并归因于LLMs的漏洞，如幻觉、越狱等。因此，现有研究集中在基于简单释义的一致性评估上，而忽略了需要LLM有更好的逻辑推理理解的复杂查询。因此，我们的工作针对复杂逻辑查询下LLMs的逻辑不一致性，考虑了原始逻辑运算符，例如否定、合取和析取。作为一个测试平台，我们考虑了涉及从现实世界知识图谱（KGs）中提出的命题逻辑查询的事实核查任务中的检索增强LLMs。我们的贡献有三点。基准：我们引入了三个基于KGs的逻辑事实核查数据集，以促进社区向逻辑上一致的LLMs发展。评估：我们提出了基于命题逻辑查询的LLM一致性度量，并证明了现有LLMs缺乏逻辑一致性，尤其是在复杂查询上。改进：我们采用监督微调来提高LLMs在具有KG上下文的复杂事实核查任务上的逻辑一致性。|\n",
        "2412.16089": "|**2024-12-20**|**The Evolution of LLM Adoption in Industry Data Curation Practices**|Crystal Qian et.al.|[2412.16089](http://arxiv.org/abs/2412.16089)|null|随着大型语言模型（LLMs）在处理非结构化文本数据方面的能力日益增强，它们为提升数据整理工作流程提供了新的机遇。本文探讨了大型科技公司从业者对LLMs的采用演变，通过参与者的感知、整合策略和报告的使用场景评估了LLMs在数据整理任务中的影响。通过一系列调查、访谈和使用研究，我们提供了组织如何应对LLMs演变关键时刻的及时快照。在2023年第二季度，我们进行了一项调查以评估LLMs在行业开发任务中的采用情况（N=84），并在2023年第三季度组织了专家访谈以评估不断变化的数据需求（N=10）。在2024年第二季度，我们通过涉及两个基于LLMs的原型的用户研究（N=12）探索了从业者当前的LLMs使用情况及其预期。尽管每个研究都针对不同的研究目标，但它们共同揭示了一个关于LLMs使用演变的更广泛叙事。我们发现，数据理解正在从以启发式为主的自下而上的方法转变为以洞察力为主的、由LLMs支持的由上而下的工作流程。此外，为了应对更复杂的数据景观，数据从业者现在用LLM生成的“银色”数据集补充了传统的由主题专家创建的“金子”数据集，并通过多样化专家精心整理的严格验证的“超级金子”数据集。这项研究揭示了LLMs在大规模非结构化数据分析中的变革性作用，并突出了进一步工具开发的机遇。|\n",
        "2412.17811": "|**2024-12-23**|**ChatGarment: Garment Estimation, Generation and Editing via Large Language Models**|Siyuan Bian et.al.|[2412.17811](http://arxiv.org/abs/2412.17811)|null|我们引入了一种名为ChatGarment的新方法，该方法利用大型视觉-语言模型（VLMs）来自动化从图像或文本描述中估计、生成和编辑3D服装。与之前在现实场景中表现不佳或缺乏交互式编辑能力的方法不同，ChatGarment可以从野外图像或草图估计缝纫图案，从文本描述中生成它们，并根据用户指令编辑服装，所有这些都在交互式对话中完成。这些缝纫图案然后可以披覆成3D服装，它们易于动画化和模拟。这是通过微调VLM来直接生成一个JSON文件实现的，该文件包括服装类型和风格的文本描述，以及连续的数值属性。然后，通过编程参数模型使用这个JSON文件来创建缝纫图案。为此，我们通过扩大服装类型覆盖范围和简化其结构以提高VLM微调效率，对现有的编程模型GarmentCode进行了改进。此外，我们通过自动化数据管道构建了一个大规模的图像到缝纫图案和文本到缝纫图案对的数据库。广泛的评估展示了ChatGarment从多模态输入中准确重建、生成和编辑服装的能力，突显了其在时尚和游戏应用中颠覆工作流程的潜力。代码和数据将在https://chatgarment.github.io/提供。|\n",
        "2412.17767": "|**2024-12-23**|**ResearchTown: Simulator of Human Research Community**|Haofei Yu et.al.|[2412.17767](http://arxiv.org/abs/2412.17767)|**[link](https://github.com/ulab-uiuc/research-town)**|**大型语言模型（LLMs）在科学领域展现了惊人的潜力，然而一个基本问题仍未得到解答：我们能否用LLMs来模拟人类研究社区？回答这个问题可以加深我们对思想头脑风暴背后过程的了解，并激发自动发现新的科学洞见。在这项工作中，我们提出了ResearchTown，一个用于研究社区模拟的多智能体框架。在这个框架中，人类研究社区被简化并建模为一个智能体-数据图，其中研究人员和论文分别表示为智能体类型和数据类型节点，并基于他们的合作关系进行连接。我们还引入了TextGNN，一个基于文本的推理框架，将各种研究活动（例如，阅读论文、撰写论文和撰写评论）建模为在智能体-数据图上统一的消息传递过程的特殊形式。为了评估研究模拟的质量，我们提出了ResearchBench，这是一个使用节点掩码预测任务进行可扩展和客观评估的基准，基于相似性。我们的实验揭示了三个关键发现：（1）ResearchTown可以提供包括论文写作和评论写作在内的协作研究活动的现实模拟；（2）ResearchTown可以在多个研究人员和多样化的论文上保持稳健的模拟；（3）ResearchTown可以生成跨学科的研究想法，这些想法有可能激发新的研究方向。**|\n",
        "2412.17754": "|**2024-12-23**|**ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback**|Wei Zhang et.al.|[2412.17754](http://arxiv.org/abs/2412.17754)|null|大型语言模型（LLMs）在自然语言处理和编程方面取得了显著进展，但在处理复杂函数调用时仍存在鲁棒性和准确性问题。为了应对这些挑战，本文提出了一种名为ADC的创新方法，该方法增强了LLMs遵循函数格式和匹配复杂参数的能力。ADC利用了一个高质量的代码微调数据集，该数据集包含行级执行反馈，提供了细粒度的过程监督，从而促进了强大的逻辑推理和遵循函数格式的准确性。它还采用了一种对抗性数据集生成过程来改进参数匹配。分阶段训练方法利用了丰富的代码数据集和精炼的对抗性数据集，在伯克利函数调用排行榜（BFCL）基准测试中显著提高了函数调用能力。ADC的创新之处在于其战略性地结合了过程监督、对抗性精炼和增量学习，为LLMs在复杂函数调用方面的能力设定了新的标准。|\n",
        "2412.17747": "|**2024-12-23**|**Deliberation in Latent Space via Differentiable Cache Augmentation**|Luyang Liu et.al.|[2412.17747](http://arxiv.org/abs/2412.17747)|null|通过生成和关注中间推理步骤，使大型语言模型（LLMs）“思考更多”的技术在解决复杂问题方面显示出希望。然而，标准方法在回答前立即生成一系列离散的标记，因此它们可能会产生显著的延迟成本，并且难以优化。在这项工作中，我们证明了一个冻结的LLM可以通过一个离线协处理器来增强，该协处理器在模型的键值（kv）缓存上运行。这个协处理器通过一组设计用来提高后续解码保真度的潜在嵌入来增强缓存。我们使用标准预训练数据中的解码器语言建模损失来训练这个协处理器，同时将解码器本身冻结。这种方法使模型能够以端到端可微的方式学习如何将额外的计算蒸馏到其kv缓存中。因为解码器保持不变，协处理器可以离线异步运行，如果协处理器不可用或认为给定的缓存不需要额外计算，语言模型可以正常工作。我们通过实验表明，当缓存被增强时，解码器在许多后续标记上达到了更低的困惑度。此外，即使在没有任何特定任务训练的情况下，我们的实验表明，缓存增强可以持续降低困惑度并提高一系列推理密集型任务的表现。|\n",
        "2412.17743": "|**2024-12-23**|**YuLan-Mini: An Open Data-efficient Language Model**|Yiwen Hu et.al.|[2412.17743](http://arxiv.org/abs/2412.17743)|**[link](https://github.com/ruc-gsai/yulan-mini)**|**由于资源需求巨大和技术流程的复杂性，有效预训练大型语言模型（LLMs）一直具有挑战性。本文详细介绍了YuLan-Mini，这是一个具有2.42B参数的高性能基础模型，在同类参数规模的模型中取得了顶级性能。我们的预训练方法通过三个关键技术贡献来提高训练效率：一个详细的数据流程，将数据清洗与数据调度策略相结合；一种鲁棒的优化方法，以减轻训练的不稳定性；以及一种有效的退火方法，该方法结合了目标数据选择和长上下文训练。值得注意的是，YuLan-Mini在1.08T个标记上训练，其性能可与需要显著更多数据的行业领先模型相媲美。为了便于重现，我们发布了每个训练阶段数据组成的全部细节。项目详情可通过以下链接获取：https://github.com/RUC-GSAI/YuLan-Mini。**|\n",
        "2412.17741": "|**2024-12-23**|**Reasoning to Attend: Try to Understand How <SEG> Token Works**|Rui Qian et.al.|[2412.17741](http://arxiv.org/abs/2412.17741)|**[link](https://github.com/rui-qian/read)**|当前的大型多模态模型（LMMs）在视觉基座方面通常依赖于$\\texttt{<SEG>}$标记作为文本提示来联合优化视觉语言模型（例如LLaVA）和下游任务特定模型（例如SAM）。然而，我们观察到，很少有研究关注其工作原理。在这项工作中，我们首先可视化了相似度图，这些图是通过计算$\\texttt{<SEG>}$标记和从LLaVA编码器和SAM解码器的最后一层隐藏层中提取的图像标记嵌入之间的语义相似度得到的。有趣的是，我们发现相似度图中的激活响应在一致性方面非常显著，这揭示了$\\texttt{<SEG>}$标记所贡献的是图像-文本对之间的语义相似性。具体来说，$\\texttt{<SEG>}$标记，一个在文本词汇表中扩展的占位符，在大型语言模型（LLMs）微调的同时，广泛查询各个分词图像块以匹配从文本到配对图像的物体语义。基于上述发现，我们提出了READ，它通过借鉴相似度图中高度激活的点来促进LMMs的$\\textbf{REA}$soning能力，以确定在哪里进行$\\textbf{D}$注意力。值得注意的是，READ具有直观的设计，即相似度作为点模块（SasP），它可以无缝地以即插即用的方式应用于类似$\\texttt{<SEG>}$的范例。此外，已经在ReasonSeg和RefCOCO(+/g)数据集上进行了大量实验。为了验证READ在微调后是否遭受了先前技能的灾难性遗忘，我们还进一步评估了它在增强的FP-RefCOCO(+/g)数据集上的生成能力。所有代码和模型均公开提供在https://github.com/rui-qian/READ上。|\n",
        "2412.17727": "|**2024-12-23**|**Knowledge Editing through Chain-of-Thought**|Changyue Wang et.al.|[2412.17727](http://arxiv.org/abs/2412.17727)|**[link](https://github.com/bebr2/editcot)**|**大型语言模型（LLMs）在众多自然语言处理（NLP）任务中展现出了卓越的能力。然而，由于频繁重新训练的高成本，保持这些模型与不断发展的世界知识同步仍然是一个重大挑战。为了应对这一挑战，知识编辑技术应运而生，以便在不从头开始重建模型的情况下更新LLMs。在这些技术中，情境编辑范式因其在新知识整合中保持模型原始能力方面的有效性而脱颖而出。尽管具有潜力，现有的情境知识编辑方法通常针对特定任务，主要关注使用结构化知识三元组的多跳问答任务。此外，它们依赖于少量样本提示进行任务分解，这使得它们在泛化到不同任务时不够稳定和有效。针对这些局限性，我们提出了EditCoT，这是一种新颖的知识编辑框架，可以灵活且高效地更新LLMs，而无需重新训练。EditCoT通过为给定输入生成思维链（CoT），然后使用基于更新知识的CoT编辑器迭代地细化这一CoT过程来实现。我们在多个涵盖多种语言和任务的基准测试中评估了EditCoT。结果表明，我们的方法在性能上达到了最先进水平，同时与现有方法相比，具有更优的泛化能力、有效性和稳定性，标志着知识更新领域的一项重大进步。代码和数据可在以下链接获取：https://github.com/bebr2/EditCoT。**|\n",
        "2412.17696": "|**2024-12-23**|**Understanding the Logic of Direct Preference Alignment through Logic**|Kyle Richardson et.al.|[2412.17696](http://arxiv.org/abs/2412.17696)|null|近期，直接偏好对齐算法（DPA），如DPO，在将大型语言模型与人类偏好对齐方面显示出巨大的潜力。虽然这促使开发了原始DPO损失的新变体，但由于缺乏对这些算法潜在语义进行推理的技术和概念框架，理解这些最新提案之间的差异以及开发新的DPA损失函数仍然困难。在本文中，我们试图通过将DPA损失形式化为离散推理问题来解决这个问题。具体来说，我们提出以下问题：给定一个现有的DPA损失，我们能否系统地推导出一个表征其语义的符号表达式？两个损失的语义如何相互关联？我们提出了一种新的形式主义来表征基于单模型和参考模型的偏好损失，并确定了多种常用DPA变体的符号形式。进一步地，我们展示了这种偏好学习的形式观点如何为DPA损失空间的规模和结构提供新的见解，这使得我们不仅能够严格地描述最新损失提案之间的关系，还能够系统地探索这一空间并从第一原理推导出新的损失函数。我们希望我们的框架和发现能为从事人类AI对齐工作的人提供有用的指导。|\n",
        "2412.17686": "|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|大型语言模型（LLMs）的快速发展和部署带来了人工智能领域的新前沿，以其在自然语言理解和生成方面的前所未有的能力为标志。然而，这些模型越来越多地集成到关键应用中，引发了重大的安全担忧，需要对这些潜在风险及其缓解策略进行彻底的审查。本综述对LLM安全现状进行了全面概述，涵盖了四个主要类别：价值偏差、对抗攻击鲁棒性、滥用和自主人工智能风险。除了对这四个方面的缓解方法和评估资源的全面回顾外，我们还进一步探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提高LLM安全中的作用、一系列AI公司和研究机构提出的和遵守的LLM安全技术路线图，以及旨在实现LLM安全的AI治理，包括国际合作、政策建议和潜在的监管方向。我们的研究发现强调了采取积极、多方面的方法来确保LLM安全的必要性，强调了技术解决方案、伦理考量以及稳健的治理框架的整合。本综述旨在为学术界研究人员、行业实践者和政策制定者提供基础资源，提供有关将LLMs安全地融入社会的挑战和机遇的见解。最终，它旨在为LLMs的安全和有益发展做出贡献，与利用AI促进社会进步和福祉的总体目标保持一致。相关论文列表已在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers上公开。|\n",
        "2412.17669": "|**2024-12-23**|**Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models**|Sijbren van Vaals et.al.|[2412.17669](http://arxiv.org/abs/2412.17669)|**[link](https://github.com/sijbrenvv/completions_for_broca-s_aphasia)**|**布罗卡失语症是一种以非流畅、费力且碎片化言语产出为特征，同时相对较好的理解能力的失语症类型。由于传统的失语症治疗方法通常耗时、劳动密集，并且不能反映现实世界的对话，因此应用基于自然语言处理的方法，如大型语言模型（LLMs），可能有助于改善现有的治疗方法。为了解决这一问题，我们探讨了使用序列到序列LLMs来完成布罗卡失语症的碎片化句子。首先，我们使用一个旨在反映布罗卡失语症言语语言特征的规则系统生成合成布罗卡失语症数据。使用这些合成数据，我们随后对四个预训练的LLMs进行了微调，以完成碎片化句子的任务。我们在合成和真实的布罗卡失语症数据上评估了我们的微调模型。我们展示了LLMs重构碎片化句子的能力，模型在较长的输入话语中显示出性能的提升。我们的结果突出了LLMs在推进布罗卡失语症个体以及其他可能临床人群的交流辅助工具方面的潜力。**|\n",
        "2412.18601": "|**2024-12-24**|**Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems**|Fernando Jia et.al.|[2412.18601](http://arxiv.org/abs/2412.18601)|**[link](https://github.com/FJDeFi/Decentralized-Intelligence-in-GameFi)**|**在游戏金融（GameFi）这一迅速发展的领域中，即游戏与去中心化金融（DeFi）的结合，迫切需要提高玩家参与度和游戏生态系统的经济互动。我们的GameFi生态系统旨在通过将先进的具身AI代理集成到GameFi平台中，从根本上改变这一领域。这些AI代理采用最前沿的大语言模型（LLMs）如GPT-4和Claude AI开发，能够与玩家进行主动、适应性强且情境丰富的互动。通过超越传统的脚本响应，这些代理成为游戏叙事和经济系统的重要参与者，直接影响玩家的策略和在游戏内的经济。我们解决了当前GameFi平台存在的局限性，这些平台通常缺乏沉浸式的AI互动和社区参与或创作者盈利机制。通过将AI代理与区块链技术的深度融合，我们建立了一个以共识驱动的去中心化GameFi生态系统。这个生态系统使创作者能够货币化他们的贡献，并促进玩家与创作者之间的民主合作。此外，通过将DeFi机制嵌入游戏体验中，我们增强了经济参与度，并为游戏内的金融互动提供了新的机会。我们的方法提高了玩家的沉浸感和留存率，通过将传统游戏与Web3技术相结合，推动了GameFi生态系统的发展。通过集成复杂的AI和DeFi元素，我们为创建更具吸引力、经济稳健且以社区为中心的游戏环境做出了贡献。这个项目代表了GameFi领域技术的重大进步，提供了可以在整个游戏行业应用的观点和方法。**|\n",
        "2412.18588": "|**2024-12-24**|**A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs**|OpenMind et.al.|[2412.18588](http://arxiv.org/abs/2412.18588)|null|大型语言模型（LLMs）是我们物理环境、动物和人类行为的全部公开知识的紧凑表示。将LLMs应用于机器人学可能为创建在大多数人类任务上表现优异且无需或仅需少量调整的强大机器人提供了一条途径。除了日益复杂的推理和任务规划之外，由（适当设计的）LLMs组成的网络提供了升级能力的便捷性，并允许人类直接观察机器人的思考。在这里，我们探讨了使用LLMs控制物理机器人的优势、局限性和特殊性。基本系统由四个通过人类语言数据总线通信的LLMs组成，该总线通过WebSocket和ROS2消息传递实现。令人惊讶的是，尽管机器人的数据融合周期仅运行在1Hz，中央数据总线运行在人类大脑的极低速率下，大约40位/秒，但仍然能够实现丰富的机器人行为和跨不同任务的良好性能。使用自然语言进行LLM间通信使得人类可以直接观察到机器人的推理和决策过程，并且用普通英语编写的规则集可以轻易地影响系统的行为。这些规则被不可更改地写入以太坊，这是一个全球性、公开性、抗审查的图灵完备计算机。我们建议，通过在相互作用的AI之间使用自然语言作为数据总线，并使用不可更改的公共账本来存储行为约束，可以构建出具有意外丰富性能、可升级性和与人类持久对齐的机器人。|\n",
        "2412.18582": "|**2024-12-24**|**Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control**|Sergey Sedov et.al.|[2412.18582](http://arxiv.org/abs/2412.18582)|null|提示微调是一种通过修改提示嵌入来高效适应新任务且计算开销最小的预训练语言模型的方法。在本研究中，我们探讨了Prompt-Tuning中常见的嵌入坍塌现象对于模型最终性能的重要性。为了回答这个问题，我们设计了嵌入先验，并将其与收敛的软提示和深度提示微调方法的后验进行了比较。我们的发现表明，先验强烈影响了调整嵌入的位置，并且模型可以有效地使用激活空间不同部分的嵌入，包括全新的区域。由于最终的提示微调能力有限，我们假设可控制的提示微调后验可以作为诸如思维链（COT）蒸馏等任务的良好起点。我们的实验还表明，生成的轨迹并未定位在模型的激活空间中。然而，对于不同任务（例如，NLP和算术）的激活存在明显的簇，而NLP任务之间的激活（例如，问答和掩码语言模型）则位于同一簇中。这些观察结果引发了关于单个激活簇对于大型语言模型泛化能力重要性的疑问。|\n",
        "2412.18566": "|**2024-12-24**|**Zero-resource Speech Translation and Recognition with LLMs**|Karel Mundnich et.al.|[2412.18566](http://arxiv.org/abs/2412.18566)|null|尽管语音处理领域取得了最近的发展，零资源语音翻译（ST）和自动语音识别（ASR）仍然是具有挑战性的问题。在这项工作中，我们提出利用多语言大型语言模型（LLM）在模型从未见过配对音频-文本数据的语言中执行ST和ASR。我们通过使用预训练的多语言语音编码器、多语言LLM和轻量级适应模块，将音频表示映射到LLM的标记嵌入空间来实现这一点。我们在ST和ASR中进行了多次实验，以了解如何最好地训练模型以及哪些数据对先前未见过的语言性能影响最大。在ST中，我们的最佳模型在CoVoST2上能够实现超过23的BLEU分数，对于两种先前未见过的语言；在ASR中，我们实现了高达28.2%的词错误率（WER）。最后，我们表明我们系统的性能受限于LLM输出所需语言文本的能力。|\n",
        "2412.18552": "|**2024-12-24**|**Distilling Fine-grained Sentiment Understanding from Large Language Models**|Yice Zhang et.al.|[2412.18552](http://arxiv.org/abs/2412.18552)|**[link](https://github.com/hitsz-hlt/fsa-distillation)**|**细粒度情感分析（FSA）旨在从大量带有观点的文本中提取和总结用户意见。最近的研究表明，大型语言模型（LLMs）具有卓越的情感理解能力。然而，直接将LLMs应用于FSA应用会带来高昂的推理成本。因此，本文研究了将LLMs中的细粒度情感理解蒸馏到小型语言模型（SLMs）中的方法。我们提示LLMs检查并解释给定评论的情感，然后利用生成的内容来预训练SLMs。此外，我们开发了一个全面的FSA基准来评估SLMs和LLMs。在这个基准上的大量实验揭示了：（1）蒸馏显著提高了SLMs在FSA任务中的性能，使$F_1$-score提高了6.00%，并且蒸馏后的模型仅用2200万个参数就能超越Llama-2-7b；（2）蒸馏使SLMs具备了出色的零样本情感分类能力，使它们能够匹配甚至超越其教师模型。这些结果表明，从LLMs中蒸馏是FSA领域一个非常有前景的方向。我们将在\\url{https://github.com/HITSZ-HLT/FSA-Distillation}上发布我们的代码、数据和预训练模型权重。**|\n",
        "2412.18547": "|**2024-12-24**|**Token-Budget-Aware LLM Reasoning**|Tingxu Han et.al.|[2412.18547](http://arxiv.org/abs/2412.18547)|**[link](https://github.com/geniushtx/tale)**|**推理对于大型语言模型（LLMs）在广泛任务中表现出色至关重要。虽然像思维链（CoT）推理这样的方法通过将问题分解为中间步骤来增强LLM的性能，但它们也带来了显著的令牌使用开销，导致成本增加。我们发现，当前LLMs的推理过程不必要地冗长，通过在提示中包含合理的令牌预算可以将其压缩，但令牌预算的选择对实际压缩效果起着至关重要的作用。然后，我们提出了一种令牌预算感知的LLM推理框架，该框架根据推理复杂度动态估计不同问题的令牌预算，并使用估计的令牌预算来指导推理过程。实验表明，我们的方法在CoT推理中有效地减少了令牌成本，同时仅略有降低性能，为在LLM推理中平衡效率和精度提供了一个实用的解决方案。代码：https://github.com/GeniusHTX/TALE。**|\n",
        "2412.18541": "|**2024-12-24**|**PLD-Tree: Persistent Laplacian Decision Tree for Protein-Protein Binding Free Energy Prediction**|Xingjian Xu et.al.|[2412.18541](http://arxiv.org/abs/2412.18541)|null|近期，基于拓扑学的建模在物理建模和分子研究方面取得了显著进展，包括其在蛋白质-配体结合亲和力应用中的研究。在本工作中，我们引入了持久拉普拉斯决策树（PLD-Tree），这是一种旨在解决预测蛋白质-蛋白质相互作用（PPI）亲和力这一挑战性任务的新方法。PLD-Tree专注于蛋白质链的绑定界面，并使用持久拉普拉斯来捕捉反映关键蛋白质间相互作用的拓扑不变量。这些从持久同伦学中导出的拓扑描述符，通过整合来自大型语言模型的进化尺度建模（ESM）进一步得到增强，以整合基于序列的信息。我们在两个基准数据集PDBbind V2020和SKEMPI v2上验证了PLD-Tree，在复杂的留一法蛋白质去除交叉验证下，证明了相关系数（$R_p$）为0.83。值得注意的是，我们的方法在这些数据集上优于所有已报道的最先进方法。这些结果强调了将机器学习技术与基于拓扑的描述符结合用于分子对接和虚拟筛选的强大能力，为预测蛋白质-蛋白质结合亲和力提供了一个稳健且准确的框架。|\n",
        "2412.18537": "|**2024-12-24**|**Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation**|Derong Xu Xinhang Li et.al.|[2412.18537](http://arxiv.org/abs/2412.18537)|**[link](https://github.com/Applied-Machine-Learning-Lab/AMAR)**|大型语言模型（LLMs）展现出惊人的能力，但在执行复杂知识推理任务时，却面临着幻觉和过时知识的困扰，导致输出结果存在事实错误。先前的研究试图通过从大规模知识图谱（KGs）中检索事实知识来协助LLMs进行逻辑推理和答案预测，以缓解这一问题。然而，这种做法往往引入噪声和不相关数据，尤其是在涉及多个知识方面的广泛背景情况下。这样一来，LLMs的注意力可能会被从问题和相关信息中误导。在我们的研究中，我们引入了一个自适应多方面检索增强知识图谱（Amar）框架。该方法检索包括实体、关系和子图的知识，并将检索到的每一篇文本转换为提示嵌入。Amar框架包含两个关键子组件：1）一个自对齐模块，它通过增强检索文本来对实体、关系和子图之间的共性进行对齐，从而减少噪声干扰；2）一个相关性门控模块，它使用软门来学习问题与多方面检索数据之间的相关性分数，以确定哪些信息应该用于增强LLMs的输出，甚至完全过滤掉。我们的方法在两个常见的数据集WebQSP和CWQ上实现了最先进的性能，与最佳竞争对手相比，准确率提高了1.9%，在逻辑形式生成方面比直接使用检索文本作为上下文提示的方法提高了6.6%。这些结果证明了Amar在提高LLMs推理能力方面的有效性。|\n",
        "2412.18531": "|**2024-12-24**|**Automated Code Review In Practice**|Umut Cihan et.al.|[2412.18531](http://arxiv.org/abs/2412.18531)|null|代码审查是一种广泛应用的提高软件质量和知识转移的实践。由于需要人工努力和可能出现的延误，它通常被视为耗时。Qodo、GitHub Copilot和Coderabbit等几个AI辅助工具，通过使用大型语言模型（LLMs）提供自动化的代码审查。这些工具在行业中的影响尚未得到考察。本研究考察了基于LLM的自动化代码审查工具在工业环境中的影响。研究在一个采用AI辅助审查工具（基于开源Qodo PR Agent）的软件开发环境中进行。大约238名来自十个项目的实践者可以使用这个工具。我们重点关注了三个项目，其中包含4,335个拉取请求，其中1,568个经历了自动化审查。数据收集包括三个来源：（1）对拉取请求数据的定量分析，包括表示开发者是否对自动化评论采取行动的评论标签，（2）向开发者发送的调查问卷，以了解他们对个别拉取请求的审查体验，以及（3）对22名实践者的更广泛调查，以了解他们对自动化审查的一般看法。73.8%的自动化评论得到了解决。然而，平均拉取请求关闭时间从5小时52分钟增加到8小时20分钟，不同项目之间存在不同的趋势。大多数实践者报告说，由于自动化审查，代码质量有所提高。基于LLM的工具在软件开发中证明是有用的，它增强了错误检测、提高了对代码质量的意识，并促进了最佳实践。然而，它也导致了拉取请求关闭时间的延长，并引入了诸如错误审查、不必要的纠正和不相关的评论等缺点。|\n",
        "2412.18511": "|**2024-12-24**|**Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving**|Hao Pang et.al.|[2412.18511](http://arxiv.org/abs/2412.18511)|null|深度强化学习（DRL）在自动驾驶决策方面展现出巨大的潜力。然而，由于学习效率低，DRL在复杂驾驶场景中实现合格策略需要大量的计算资源。此外，利用人类专家的指导来提高DRL的性能会带来高昂的劳动成本，这限制了其实际应用。在本研究中，我们提出了一种新型的大语言模型（LLM）引导的深度强化学习（LGDRL）框架，用于解决自动驾驶车辆的决策问题。在这个框架中，将基于LLM的驾驶专家集成到DRL中，为DRL的学习过程提供智能指导。随后，为了高效利用LLM专家的指导来提高DRL决策策略的性能，通过一种创新的专家策略约束算法和一种新的LLM干预交互机制，增强了DRL的学习和交互过程。实验结果表明，我们的方法不仅实现了90%的任务成功率，具有优越的驾驶性能，而且与最先进的基线算法相比，显著提高了学习效率和专家指导的利用率。此外，提出的方法使得DRL代理在缺乏LLM专家指导的情况下也能保持一致和可靠的表现。代码和补充视频可在https://bitmobility.github.io/LGDRL/找到。|\n",
        "2412.19784": "|**2024-12-27**|**Can AI Help with Your Personal Finances?**|Oudom Hean et.al.|[2412.19784](http://arxiv.org/abs/2412.19784)|null|近年来，大型语言模型（LLMs）作为人工智能（AI）领域的一项颠覆性发展，受到了业界和学术界的广泛关注。这些复杂的AI系统在庞大的数据集上训练，展现出令人印象深刻的自然语言处理和内容生成能力。本文探讨了LLMs在解决个人财务领域关键挑战中的潜力，重点关注美国市场。我们评估了包括OpenAI的ChatGPT、谷歌的Gemini、Anthropic的Claude和Meta的Llama在内的几个领先的LLMs，以评估它们在提供关于抵押贷款、税收、贷款和投资等主题的准确财务建议方面的有效性。我们的发现表明，虽然这些模型实现了大约70%的平均准确率，但在某些领域也显示出明显的局限性。具体来说，LLMs在提供复杂财务查询的准确回答方面存在困难，其表现因不同主题而显著不同。尽管存在这些局限性，分析显示这些模型的新版本有显著改进，突显了它们在个人和财务顾问中的日益增长的应用价值。随着这些AI系统持续发展，它们在推动个人财务领域AI驱动应用方面的潜力变得越来越有希望。|\n",
        "2412.19770": "|**2024-12-27**|**Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration**|Le Chen et.al.|[2412.19770](http://arxiv.org/abs/2412.19770)|**[link](https://github.com/hpc-fortran2cpp/fortran2cpp)**|**将Fortran代码迁移到C++是许多科学计算团队常见的任务，这一需求推动了现代编程范式、跨平台兼容性的提升以及维护性的改进。利用大型语言模型（LLMs）自动化这一翻译过程已显示出潜力，但高质量、专业数据集的缺乏阻碍了其有效性。在本文中，我们通过引入一个专门为Fortran到C++代码迁移设计的创新多轮对话数据集Fortran2CPP来解决这一挑战。我们的数据集比现有替代品大得多，使用一个独特的LLM驱动的双代理管道生成，该管道结合了迭代编译、执行和代码修复，以确保高质量和功能正确性。为了展示我们数据集的有效性，我们在Fortran2CPP上对几个开放权重的LLMs进行了微调，并评估了它们在两个独立基准上的性能。在我们的数据集上进行微调带来了显著提升，模型在CodeBLEU评分上实现了高达3.31倍的提升，编译成功率提高了92%。这突显了数据集增强翻译的C++代码的语法准确性和可编译性的能力。我们的数据集和模型已开源，可在我们的公共GitHub仓库中获取[脚注：\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}]。**|\n",
        "2412.19726": "|**2024-12-27**|**Can Large Language Models Adapt to Other Agents In-Context?**|Matthew Riemer et.al.|[2412.19726](http://arxiv.org/abs/2412.19726)|null|随着研究界致力于构建更动态、更个性化的AI助手，以适应与人类互动的多样性，对评估大型语言模型（LLMs）的心智理论能力产生了更大的兴趣。确实，一些最近的研究表明，LLMs的心智理论能力相当令人印象深刻，接近人类水平的表现。我们的论文旨在反驳这种说法，并认为过去的研究并没有直接测量代理的表现，这可能导致了一些本质上具有幻觉性质的结果。我们区分了我们称之为字面心智理论和功能心智理论，即衡量代理预测他人行为的能力和根据对他人行为预测的理性反应来适应情境中的代理。我们发现，表现最出色的开源LLMs可能在字面心智理论方面表现出强大的能力，这取决于如何提示它们，但似乎在功能心智理论方面遇到困难——即使合作伙伴策略非常简单。我们的工作有助于突出LLMs在适应新情况时归纳偏见的两面性。虽然这种偏见可能在有限的范围内导致强大的表现，但它通常阻碍了达到最佳长期行为的收敛。|\n",
        "2412.19707": "|**2024-12-27**|**Toward Adaptive Reasoning in Large Language Models with Thought Rollback**|Sijia Chen et.al.|[2412.19707](http://arxiv.org/abs/2412.19707)|**[link](https://github.com/iQua/llmpebase)**|**大型语言模型（LLMs）通常用于通过逐步推理来解决各种任务。然而，中间推理步骤或思维的结构是刚性和单向的，例如链式、树形或无环有向图。因此，这种不灵活且仅向前推理的方法可能无法应对挑战性任务，并且在LLM频繁给出错误回答，即“幻觉”时可能会失败。本文提出了一种新的推理框架，称为思维回滚（TR），允许LLMs在解决“幻觉”问题时，自适应地构建思维结构，同时保持有效的推理。TR的核心机制是回滚思维，这使得LLMs能够对思维进行错误分析，并因此回滚到任何之前的错误思维进行修订。随后，通过将这种试错过程包含在提示中引导LLM，每次回滚都会导致一条更可靠的推理路径。因此，从没有人类标注的简单提示开始，带有TR的LLM自适应地逐步探索思维以找到正确解决方案。在数学问题和多任务推理上的综合实验表明，TR在解决问题的成功率和交互成本方面达到了最先进的水平。例如，在MATH数据集上，带有TR的GPT-4的解决率比当前最佳方案高出9%。**|\n",
        "2412.19685": "|**2024-12-27**|**A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization**|Jingchun Lian et.al.|[2412.19685](http://arxiv.org/abs/2412.19685)|null|图像伪造定位，即识别图像中的篡改像素，已经取得了显著的进展。传统方法通常将这一挑战视为图像分割的一种变体，将伪造区域的二值分割视为最终产品。我们认为基本的二值伪造掩码不足以解释模型的预测。它无法阐明模型为何指明某些区域以及如何对待所有伪造像素，这使得难以识别最不真实的部分。在本研究中，我们通过为伪造图像生成突出区域专注的解释来缓解上述局限性。为此，我们构建了一个多模态伪造追踪（MMTT）数据集，包括使用深度伪造技术处理的面部图像，并配以人工的、可解释的文本注释。为了获取高质量的注释，注释员被要求仔细观察被篡改的图像，并描述伪造区域的典型特征。随后，我们收集了128,303个图像-文本对的数据集。利用MMTT数据集，我们开发了ForgeryTalker，这是一种旨在同时进行伪造定位和解释的架构。ForgeryTalker首先训练一个伪造提示网络以识别解释文本中的关键线索。随后，区域提示被纳入多模态大型语言模型中进行微调，以实现定位和解释的双重目标。在MMTT数据集上进行的广泛实验验证了我们所提出模型的优势。该数据集、代码以及预训练的检查点将被公开提供，以促进进一步的研究并确保我们结果的复现性。|\n",
        "2412.19684": "|**2024-12-27**|**Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework**|Jiang Liu et.al.|[2412.19684](http://arxiv.org/abs/2412.19684)|null|高效多模态大型语言模型（EMLLMs）与多模态大型语言模型（MLLMs）相比，减少了模型大小和计算成本，通常部署在资源受限的设备上。然而，由于数据隐私的担忧，现有的开源EMLLMs在预训练过程中很少能够访问到私有领域特定数据，这使得它们难以直接应用于特定设备领域，例如某些商业场景。为了解决这一弱点，本文重点关注EMLLMs对私有领域的有效适应，特别是在以下两个领域：1）如何减少数据需求；2）如何避免参数微调。具体来说，我们提出了一种无调整、自适应、通用的提示优化框架，简称我们的方法，它包括两个阶段：1）预定义提示，基于强化搜索策略，生成提示优化策略树以获取优化先验；2）提示反思基于优化先验初始化提示，然后进行自我反思以进一步搜索和细化提示。通过这种方式，我们的方法巧妙地生成了处理私有领域特定数据的“理想提示”。请注意，我们的方法不需要参数微调，并且只需要少量数据就可以快速适应私有数据的数据分布。在多个任务上的大量实验表明，与基线相比，我们提出的方法在效率和性能方面都有显著提升。|\n",
        "2412.19663": "|**2024-12-27**|**CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs**|Siyu Wang et.al.|[2412.19663](http://arxiv.org/abs/2412.19663)|null|计算机辅助设计（CAD）通过实现精确的二维和三维建模、广泛的分析和优化，显著提高了设计过程的效率、准确性和创新性。现有的创建CAD模型的方法依赖于潜在向量或点云，这些方法难以获得且存储成本高昂。近期在多模态大型语言模型（MLLM）方面的进步激发了研究人员使用自然语言指令和图像来进行CAD模型构建。然而，这些模型在推断准确的3D空间位置和方向方面仍然存在困难，导致在确定构建几何体的空间3D起始点和挤出方向时出现误差。本研究引入了CAD-GPT，这是一种具有空间推理增强的MLLM的CAD合成方法，可以接受单个图像或文本描述作为输入。为了实现精确的空间推断，我们的方法引入了3D建模空间机制。该方法通过专用空间展开机制将3D空间位置和3D草图平面旋转角度映射到一维语言特征空间，同时将二维草图坐标离散化到适当的平面空间，以实现空间起始位置、草图方向和二维草图坐标平移的精确确定。大量实验表明，CAD-GPT在CAD模型合成方面，无论是从定量还是定性角度来看，都优于现有的最先进方法。|\n",
        "2412.19652": "|**2024-12-27**|**FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios**|Kaiyi Pang et.al.|[2412.19652](http://arxiv.org/abs/2412.19652)|null|语言隐写术将秘密信息嵌入看似无辜的文本中，以保护监视环境中的隐私。生成式语言隐写术利用语言模型（LM）的概率分布，并应用隐写算法生成隐写标记，随着大型语言模型（LLM）的近期进展而受到关注。为了提高安全性，研究人员开发了保持分布的隐写算法，以最小化隐写采样与LM采样之间的差距。然而，对语言模型分布的依赖，以及与真实世界掩护文本的偏差，导致在实际场景中面对隐写分析检测器时，隐写术的不可感知性不足。此外，LLM分布往往更确定，导致熵减少，从而降低嵌入容量。在本文中，我们提出了FreStega，这是一种即插即用方法，用于重建用于生成式语言隐写术的语言模型分布。FreStega在隐写文本自回归生成的每一步动态调整标记概率，利用了序列和空间维度。在序列调整中，根据瞬时熵动态调整温度，增强隐写文本的多样性并提高嵌入容量。在空间维度上，分布与目标领域语料库的指导对齐，紧密模仿目标领域的真实掩护文本。通过重塑分布，FreStega提高了实际场景中隐写文本的不可感知性，并通过提高15.41%的隐写能力，所有这些都不损害生成文本的质量。FreStega作为即插即用的补救措施，增强了现实场景中现有保持分布隐写术方法的不可感知性和嵌入容量。|\n",
        "2412.19638": "|**2024-12-27**|**Xmodel-2 Technical Report**|Wang Qun et.al.|[2412.19638](http://arxiv.org/abs/2412.19638)|null|Xmodel-2是一种专门为推理任务设计的1.2亿参数的大型语言模型。其架构允许不同规模的模型共享一组统一的超参数，从而可以在较小的模型上进行广泛的实验，并无缝地将最佳配置转移到较大的模型上。为了最大化训练效率和稳定性，Xmodel-2采用了MiniCPM中的WSD学习率调度器。在来自不同来源的1500亿个标记上预训练后，Xmodel-2在复杂推理和基于代理的任务中实现了最先进的性能，同时保持了低训练成本。这些结果突显了高效模型设计和训练策略在提升推理能力方面的潜力。模型检查点和代码在GitHub上公开，链接为https://github.com/XiaoduoAILab/Xmodel-2。|\n",
        "2412.19630": "|**2024-12-27**|**IMTP: Search-based Code Generation for In-memory Tensor Programs**|Yongwon Shin et.al.|[2412.19630](http://arxiv.org/abs/2412.19630)|null|DRAM-PIM（DRAM中的处理）技术已成为加速现代应用中内存密集型操作（如大型语言模型（LLMs））的有前途的技术。尽管其潜力巨大，但当前DRAM-PIM的软件堆栈面临重大挑战，包括依赖手动调整的库，这阻碍了可编程性、对高级抽象支持有限以及缺乏系统化的优化框架。为了解决这些限制，我们提出了IMTP，这是一个针对UPMEM的基于搜索的优化张量编译器。IMTP的关键特性包括：（1）自动搜索主机和内核张量程序的联合搜索空间，（2）针对PIM的优化以高效处理边界条件，以及（3）针对UPMEM系统扩展搜索空间的改进搜索算法。我们在UPMEM硬件上的实验结果表明，各种UPMEM基准核的性能提升可达8.21倍，GPT-J层可达5.33倍。据我们所知，IMTP是第一个为DRAM-PIM系统提供完全自动化、集成自调优代码生成支持的张量编译器。通过弥合高级张量计算抽象和底层硬件特定要求之间的差距，IMTP为提升DRAM-PIM的可编程性和实现优化流程的简化奠定了基础。|\n",
        "2412.21200": "|**2024-12-30**|**Distributed Mixture-of-Agents for Edge Inference with Large Language Models**|Purbesh Mitra et.al.|[2412.21200](http://arxiv.org/abs/2412.21200)|**[link](https://github.com/purbeshmitra/distributed_moa)**|**混合智能体（MoA）最近被提出作为一种提升大型语言模型（LLMs）性能的方法，使多个单独的LLMs能够协同进行推理。这种协作方法相比于依赖单个LLM，能够产生更优的用户提示响应。在本论文中，我们考虑了在分布式环境下的一种MoA架构，其中LLMs运行在各自的边缘设备上，这些设备与用户一一对应，并配备了各自的分布式计算能力。这些设备通过去中心化的八卦算法交换信息，使得不同的设备节点可以在没有中央服务器监督的情况下进行交流。在所考虑的配置中，不同的用户拥有自己的LLM模型来处理用户提示。此外，设备之间可以通过八卦自己的用户特定提示或增强提示来生成针对某些查询的更精细答案。当对应的LLM忙碌时，用户提示暂时存储在设备队列中。鉴于边缘设备的内存限制，确保系统中平均队列大小保持有界至关重要。在本论文中，我们通过在合理的假设下理论计算设备队列的排队稳定性条件来解决这个问题，并通过实验进行验证。此外，我们通过实验展示了利用开源LLMs实现分布式MoA，某些MoA配置在AlpacaEval 2.0基准测试中产生了比其他配置更高的质量响应。该实现可在以下链接获取：https://github.com/purbeshmitra/distributed_moa。**|\n",
        "2412.21199": "|**2024-12-30**|**HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation**|Zhaojian Yu et.al.|[2412.21199](http://arxiv.org/abs/2412.21199)|**[link](https://github.com/CodeEval-Pro/CodeEval-Pro)**|**我们引入了自我调用的代码生成，这是一个旨在评估大型语言模型（LLM）渐进推理和问题解决能力的新任务。在这个任务中，模型被给出一个基础问题和与之相关的一个更复杂的问题。它们必须先解决基础问题，然后利用其解决方案来解决更复杂的问题。这项工作有三个关键贡献。首先，我们提出了一种生成现有基准测试更具有挑战性的通用方法，从而产生了三个新的基准：HumanEval Pro、MBPP Pro和BigCodeBench-Lite Pro，这些基准专门设计用来评估LLM在自我调用代码生成方面的能力。其次，通过对二十个LLM在我们基准测试上的实验结果分析，我们有以下两个重要观察：（i）大多数LLM在传统的代码生成基准测试如HumanEval和MBPP中表现优秀，但在自我调用任务上的表现有所下降。例如，o1-mini在HumanEval上的通过率达到了96.2%，但在HumanEval Pro上仅为76.2%。（ii）在自我调用代码生成任务中，与基础模型相比，指令微调的模型只显示出微小的改进。第三，我们揭示了在我们评估结果中存在的失败模式类型。所有这些结果都强调了在自我调用代码生成任务上进一步进步的必要性，并为未来关于提升LLM代码推理能力的研究提供了新的方向。**|\n",
        "2412.21140": "|**2024-12-30**|**Facilitating large language model Russian adaptation with Learned Embedding Propagation**|Mikhail Tikhomirov et.al.|[2412.21140](http://arxiv.org/abs/2412.21140)|**[link](https://github.com/RefalMachine/llmtf_open)**|**大型语言模型（LLM）技术的快速进步导致了功能强大的开源指令调整LLM的引入，其文本生成质量与GPT-4等最先进的同类模型相当。虽然这类模型的出现加速了LLM技术在敏感信息环境中的采用，但模型的作者没有公开复现结果所需的训练数据，这使得成就是模型专属的。由于这些开源模型也是多语言的，这反过来又减少了训练特定语言LLM的益处，因为改进的推理计算效率成为这种昂贵流程的唯一保证优势。词汇扩展和后续持续预训练等更具成本效益的选项也受到缺乏访问高质量指令调整数据的限制，因为它是导致LLM任务解决能力的主要因素。为了解决这些限制并降低语言适应管道的成本，我们提出了学习嵌入传播（LEP）。与现有方法不同，我们的方法由于对现有LLM知识的微小影响，因此对训练数据大小的要求较低，我们通过使用新颖的临时嵌入传播程序来强化这一点，该程序允许跳过指令调整步骤，并将新的语言知识直接植入任何现有的指令调整变体中。我们对LLaMa-3-8B和Mistral-7B的四种俄语词汇适应进行了评估，表明LEP与传统指令调整方法具有竞争力，实现了与OpenChat 3.5和LLaMa-3-8B-Instruct相当的性能，通过自我校准和持续调整进一步增强了任务解决能力。**|\n",
        "2412.21123": "|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越多地依赖于网络爬取的数据集，对于未经授权使用版权或个人内容进行训练的担忧日益加剧。尽管有诸如通用数据保护条例（GDPR）等法规，数据所有者对他们在模型训练中使用其内容仍有限制。为了解决这个问题，我们提出了ExpShield，这是一种主动的自卫机制，使内容所有者能够将不可见的扰动嵌入其文本中，限制LLMs训练中的数据滥用，同时不影响可读性。这种预防性方法使数据所有者能够直接保护敏感内容，而不必依赖第三方进行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的合理性。我们进一步通过识别记忆触发器和创建陷阱来更专注地偏离模型记忆，从而提高效率。为了验证我们防御的有效性，我们提出了一个新颖的实例利用度量，该度量捕捉了模型训练引起的个别风险。实验结果表明，我们的方法的有效性，MIA AUC从0.95下降到0.55，实例利用接近零。这表明训练后个别风险并未增加，强调了主动防御在保护版权数据中的重要性。|\n",
        "2412.21051": "|**2024-12-30**|**Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense**|Yuyang Zhou et.al.|[2412.21051](http://arxiv.org/abs/2412.21051)|**[link](https://github.com/SEU-ProactiveSecurity-Group/LLM-PD)**|**云计算技术的快速发展和云应用的增多为日常生活带来了大量便利。然而，不同组件的多样性和复杂性对云安全构成了重大挑战，尤其是在应对复杂和高级的网络攻击时。近期在生成基础模型（GFMs），尤其是大型语言模型（LLMs）方面的进步，为安全智能提供了有希望的解决方案。通过利用语言理解、数据分析、任务推理、行动规划和代码生成等强大的能力，我们提出了LLM-PD，这是一种新型的主动防御架构，能够以主动方式击败各种威胁。LLM-PD能够通过全面的数据分析和顺序推理高效地做出决策，同时在目标云上动态创建和部署可执行的防御机制。此外，它可以根据从先前交互中学习到的经验灵活地自我进化，无需额外训练即可适应新的攻击场景。实验结果展示了其在防御效果和效率方面的显著能力，尤其是在与其他现有方法相比时，显示出出色的成功率。**|\n",
        "2412.21037": "|**2024-12-30**|**TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization**|Chia-Yu Hung et.al.|[2412.21037](http://arxiv.org/abs/2412.21037)|**[link](https://github.com/declare-lab/TangoFlux)**|**我们介绍了TangoFlux，这是一个具有515M参数的高效文本到音频（TTA）生成模型，能够在单个A40 GPU上仅需3.7秒生成长达30秒的44.1kHz音频。在调整TTA模型时，一个关键挑战在于创建偏好对，因为TTA缺乏类似于大型语言模型（LLMs）中可验证的奖励或黄金标准答案的结构化机制。为了解决这个问题，我们提出了CLAP-Ranked偏好优化（CRPO），这是一种新颖的框架，它通过迭代生成和优化偏好数据来提高TTA的匹配度。我们证明，使用CRPO生成的音频偏好数据集优于现有替代方案。借助这个框架，TangoFlux在客观和主观基准测试中都达到了最先进的性能。我们将所有代码和模型开源，以支持TTA生成方面的进一步研究。**|\n",
        "2412.21036": "|**2024-12-30**|**GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models**|Shangyu Xing et.al.|[2412.21036](http://arxiv.org/abs/2412.21036)|null|多模态大型语言模型（MLLMs）在整合视觉和语言理解方面取得了显著进步。尽管现有的基准测试在内容丰富、现实生活场景中评估这些模型，但它们往往忽视了对于偏离日常生活现实的环境来说至关重要的基本感知技能。特别是几何感知，即解释空间关系和抽象视觉模式的能力，仍未得到充分探索。为了解决这一局限性，我们引入了GePBench，这是一个新型基准，旨在评估MLLMs的几何感知能力。广泛评估的结果显示，当前最先进的MLLMs在这些任务上存在显著的不足。此外，我们还证明了使用GePBench数据源训练的模型在广泛的后处理任务上显示出显著的改进，这强调了几何感知作为高级多模态应用基础的重要性。我们的代码和数据集将公开可用。|\n",
        "2412.21015": "|**2024-12-30**|**MapQaTor: A System for Efficient Annotation of Map Query Datasets**|Mahir Labib Dihan et.al.|[2412.21015](http://arxiv.org/abs/2412.21015)|**[link](https://github.com/MapQaTor/.github/tree/main/profile)**|**地图和导航服务如谷歌地图、苹果地图、Openstreet Maps对于访问各种基于位置的数据至关重要，但它们在处理自然语言地理空间查询方面往往力不从心。最近在大型语言模型（LLMs）方面的进步在问答（QA）方面显示出希望，但从前端地图服务创建可靠的地理空间QA数据集仍然具有挑战性。我们介绍了MapQaTor，这是一个简化了可重复、可追踪的基于地图的QA数据集创建的Web应用程序。凭借其即插即用的架构，MapQaTor能够与任何地图API无缝集成，使用户能够以最小的设置从各种来源收集和可视化数据。通过缓存API响应，该平台确保了一致的地面真相，即使在现实世界信息不断演变的情况下，也增强了数据的可靠性。MapQaTor将数据检索、标注和可视化集中在一个平台上，为评估基于LLM的地理空间推理的当前状态提供了一个独特的机会，同时提高其地理空间理解能力。评估指标显示，与手动方法相比，MapQaTor将标注过程的速度提高了至少30倍，凸显了其在开发地理空间资源，如复杂地图推理数据集方面的潜力。网站已上线：https://mapqator.github.io/，演示视频可在：https://youtu.be/7_aV9Wmhs6Q查看。**|\n",
        "2412.21006": "|**2024-12-30**|**Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria**|Joonwon Jang et.al.|[2412.21006](http://arxiv.org/abs/2412.21006)|null|大型语言模型（LLMs）依赖于生成大量的中间推理单元（例如，标记、句子）以增强在广泛复杂任务中的最终答案质量。虽然生成多个推理路径或迭代地完善理由证明是提高性能的有效方法，但这些方法不可避免地导致推理成本显著增加。在这项工作中，我们提出了一种新颖的基于句子的理由减少训练框架，该框架利用基于概率的指标——冗余度，来识别和删除冗余的推理句子。与之前利用标记级减少的方法不同，我们的句子级减少框架在减少生成长度的同时保持了模型性能。这保留了LLMs的原有推理能力，并在各种模型和任务中实现了平均17.15%的生成成本降低。|\n",
        "2412.20996": "|**2024-12-30**|**Plug-and-Play Training Framework for Preference Optimization**|Jingyuan Ma et.al.|[2412.20996](http://arxiv.org/abs/2412.20996)|null|近期，偏好优化方法如DPO在包括对话和问答在内的广泛任务中显著提升了大型语言模型（LLMs）。然而，当前的方法未能考虑到在偏好优化过程中训练样本难度级别的差异，导致在高精度要求任务中的表现平庸，尤其是在数学推理任务中。为了解决这一局限性，我们提出了一种新颖的训练框架，该框架采用多采样来分析输出分布，为样本分配不同的权重，并将这些权重纳入偏好优化过程。这种即插即用的方法使得LLMs能够在训练过程中优先处理具有挑战性的示例，从而提高学习效率。实验结果表明，我们的框架能够与各种偏好优化方法无缝集成，并在数学推理任务中实现一致的改进。|\n",
        "2501.01426": "|**2025-01-02**|**Unifying Specialized Visual Encoders for Video Language Models**|Jihoon Chung et.al.|[2501.01426](http://arxiv.org/abs/2501.01426)|**[link](https://github.com/princetonvisualai/merv)**|最近大型语言模型（LLMs）的兴起，将高级推理能力引入了视频领域，形成了视频大型语言模型（VideoLLMs）。然而，当前的VideoLLMs依赖于单个视觉编码器进行所有视觉处理，这限制了可以传达给LLM的视觉信息和类型。我们的方法，MERV（多编码器视频表示），利用多个冻结的视觉编码器来创建视频的统一表示，为VideoLLM提供了一套全面的专用视觉知识。通过空间-时间对齐每个编码器的特征，我们可以解决更广泛的开放式和多选题型视频理解问题，并优于先前最先进的工作。在标准视频理解基准测试中，MERV的准确率比Video-LLaVA高3.7%，同时视频-ChatGPT得分也更高。我们还提高了SeViLA（在零样本感知测试准确率上的先前最佳结果）2.2%。MERV引入了最小的额外参数，在并行化视觉处理的同时比等效的单编码器方法训练速度更快。最后，我们提供了定性的证据，表明MERV成功地从其每个编码器中捕获了领域知识。我们的结果为利用多个视觉编码器进行综合视频理解提供了有希望的指导方向。|\n",
        "2501.01384": "|**2025-01-02**|**OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios**|Xize Cheng et.al.|[2501.01384](http://arxiv.org/abs/2501.01384)|null|随着大型语言模型的快速发展，研究人员已经创建了能够与人类自然对话的越来越先进的语音对话系统。然而，这些系统仍然难以处理现实对话的全部复杂性，包括音频事件、音乐环境和情感表达，主要是因为当前的对话数据集在规模和场景多样性方面都受到限制。在本文中，我们提出利用合成数据来增强不同场景下的对话模型。我们引入了ShareChatX，这是第一个涵盖不同场景的综合、大规模语音对话数据集。基于这个数据集，我们引入了OmniChat，这是一个具有异构特征融合模块的多轮对话系统，旨在优化不同对话环境中的特征选择。此外，我们探讨了使用合成数据训练对话系统的关键方面。通过全面实验，我们确定了合成数据和真实数据之间的理想平衡，在现实对话数据集DailyTalk上取得了最先进的成果。我们还强调了合成数据在解决多样化、复杂对话场景中的关键作用，特别是涉及音频和音乐的场景。更多详情请访问我们的演示页面：\\url{https://sharechatx.github.io/}。|\n",
        "2501.01336": "|**2025-01-02**|**Aligning Large Language Models for Faithful Integrity Against Opposing Argument**|Yong Zhao et.al.|[2501.01336](http://arxiv.org/abs/2501.01336)|**[link](https://github.com/zhaoy777/afice)**|大型语言模型（LLMs）在复杂推理任务中展示了令人印象深刻的性能。然而，它们在对话中很容易被不忠实的论点误导，即使它们的原始陈述是正确的。为此，我们研究了在LLMs中保持忠实完整性的问题。这包括确保LLMs在面对对立论点时坚持其忠实陈述，并在遇到忠实论点时能够纠正其错误陈述。在这项工作中，我们提出了一种名为“具有置信度估计的忠实完整性对齐”（AFICE）的新框架，旨在使LLM的响应与忠实完整性对齐。具体来说，AFICE首先设计了一种双边置信度估计（BCE）方法，用于估计LLM在特定语境下生成每个响应的不确定性，该方法同时根据解码过程中的内部状态估计模型对问题的置信度，以及根据累积概率比估计对答案的置信度。利用BCE，我们构建了一个由语境、原始陈述和论点组成的话语偏好数据集，该数据集用于通过直接偏好优化（DPO）对齐LLM以实现忠实完整性。在广泛基准上的大量实验结果表明，当LLM遇到对立论点时，其在保持忠实响应方面的能力得到了显著提高，确保了LLMs在复杂交互环境中的实用性和可靠性。代码和数据将通过https://github.com/zhaoy777/AFICE.git发布。|\n",
        "2501.01335": "|**2025-01-02**|**CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models**|Johan Wahréus et.al.|[2501.01335](http://arxiv.org/abs/2501.01335)|**[link](https://github.com/cysecbench/dataset)**|众多研究调查了破解大型语言模型（LLMs）以生成有害内容的方法。通常，这些方法使用旨在绕过LLM提供商建立的安保策略的恶意提示数据集进行评估。然而，现有数据集的广泛范围和开放式特性可能会使破解效果的评价复杂化，特别是在特定领域，如网络安全。为了解决这个问题，我们提出并公开发布了CySecBench，这是一个包含12662个提示的综合数据集，专门用于评估网络安全领域的破解技术。该数据集分为10个不同的攻击类型类别，包含封闭式提示，以实现更一致和准确的破解尝试评估。此外，我们详细说明了数据集生成和筛选的方法，这些方法可以适应其他领域的类似数据集创建。为了展示CySecBench的实用性，我们提出并评估了一种基于提示混淆的破解方法。我们的实验结果表明，这种方法成功地从商业黑盒LLMs中诱发出有害内容，实现了ChatGPT的65%成功率（SR）和Gemini的88%成功率；相比之下，Claude表现出更强的抵抗力，破解成功率仅为17%。与现有基准方法相比，我们的方法表现出更优越的性能，突显了特定领域评估数据集在评估LLM安全措施方面的价值。此外，当使用广泛使用的数据集（即AdvBench）中的提示进行评估时，它实现了78.5%的成功率，高于现有最佳方法。|\n",
        "2501.01332": "|**2025-01-02**|**Decoding Knowledge in Large Language Models: A Framework for Categorization and Comprehension**|Yanbo Fang et.al.|[2501.01332](http://arxiv.org/abs/2501.01332)|null|理解大型语言模型（LLMs）如何获取、保留和应用知识仍然是一个开放性的挑战。本文介绍了一个新颖的框架K-(CSA)^2，该框架从正确性和信心两个维度对LLMs的知识进行分类。该框架定义了六种知识类别，从高度自信的正确性到自信地持有的错误观念，从而使得对模型理解的评价超越了二值准确性的范畴。使用该框架，我们展示了像思维链提示和带有人类反馈的强化学习等技术在如何基本改变LLMs内部（预训练）和外部（情境依赖）知识结构方面的作用。思维链（CoT）特别提高了基础模型的表现，并在应用于对齐的LLMs时显示出协同效应。此外，我们的分层分析揭示，LLMs中较高层编码了更多的高信心知识，而低信心知识往往出现在中间到低层。|\n",
        "2501.01329": "|**2025-01-02**|**The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation**|Shuzheng Gao et.al.|[2501.01329](http://arxiv.org/abs/2501.01329)|null|测试用例对于验证软件应用的可靠性和质量至关重要。最近的研究表明，大型语言模型（LLMs）能够为给定的源代码生成有用的测试用例。然而，现有工作主要依赖于人类编写的普通提示，这往往导致结果不佳，因为LLMs的性能会受到提示的高度影响。此外，这些方法使用相同的提示对所有LLMs，而忽略了不同LLMs可能最适合不同提示的事实。鉴于可能的提示表述的广泛多样性，为每个LLM自动发现最佳提示是一个重大挑战。尽管自然语言处理领域存在自动提示优化的方法，但它们难以生成针对测试用例生成任务的有效提示。首先，这些方法通过简单组合和变异现有提示来迭代优化提示，缺乏适当的指导，导致提示缺乏多样性，并且倾向于在生成的测试用例中重复相同的错误。其次，提示通常缺乏领域上下文知识，限制了LLMs在任务中的性能。|\n",
        "2501.01306": "|**2025-01-02**|**Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking**|Xiaoxue Cheng et.al.|[2501.01306](http://arxiv.org/abs/2501.01306)|null|大型语言模型（LLMs）表现出卓越的能力，但仍面临幻觉问题。典型的文本生成方法采用无意识的自动回归生成，这往往导致不可信和事实错误的结果。在本文中，我们提出了HaluSearch，一个新型框架，它结合了基于树搜索的算法（例如MCTS），以实现显式的缓慢思考生成过程，以减轻LLMs在推理过程中的幻觉问题。具体来说，HaluSearch将文本生成视为一个逐步推理的过程，使用自我评估奖励模型对每个生成步骤进行评分，并引导树搜索走向最可靠的生成路径，以充分利用LLMs的内部知识。为了平衡效率和质量，我们引入了一种受认知科学中双重过程理论启发的分层思考系统切换机制，在实例和步骤级别上动态地在快速思考和缓慢思考模式之间交替，以适应问题的复杂性和推理状态。我们在英文和中文数据集上进行了广泛的实验，结果表明，我们的方法显著优于基线方法。|\n",
        "2501.01305": "|**2025-01-02**|**Large Language Models for Mental Health Diagnostic Assessments: Exploring The Potential of Large Language Models for Assisting with Mental Health Diagnostic Assessments -- The Depression and Anxiety Case**|Kaushik Roy et.al.|[2501.01305](http://arxiv.org/abs/2501.01305)|null|大型语言模型（LLMs）因其协助诊断评估的潜力，越来越受到医疗专业人士的关注。这有助于缓解由于患者负担过重和医疗资源短缺而对医疗系统造成的压力。为了使LLMs在支持诊断评估方面有效，它们必须紧密模仿临床医生使用的标准诊断程序。在本文中，我们特别研究了用于重度抑郁症（MDD）的病人健康问卷-9（PHQ-9）和用于广泛性焦虑障碍（GAD）的广泛性焦虑障碍-7（GAD-7）问卷中描述的诊断评估过程。我们调查了各种提示和微调技术，以引导专有和开源LLMs遵循这些流程，并评估LLM生成的诊断结果与专家验证的基准之间的吻合度。对于微调，我们使用了Mentalllama和Llama模型，而对于提示，我们尝试了专有模型如GPT-3.5和GPT-4o，以及开源模型如llama-3.1-8b和mixtral-8x7b。|\n",
        "2501.01273": "|**2025-01-02**|**Does a Large Language Model Really Speak in Human-Like Language?**|Mose Park et.al.|[2501.01273](http://arxiv.org/abs/2501.01273)|null|大型语言模型（LLMs）最近出现，由于它们能够生成高度自然、类似人类的文本而引起了广泛关注。本研究在假设检验程序中比较了LLM生成的文本和人工撰写的文本的潜在社区结构。具体来说，我们分析了三个文本集：原始人工撰写的文本（$\\mathcal{O}$）、它们通过LLM改写的版本（$\\mathcal{G}$）以及从$\\mathcal{G}$中派生出的两次改写集（$\\mathcal{S}$）。我们的分析解决了两个关键问题：（1）$\\mathcal{O}$和$\\mathcal{G}$之间的潜在社区结构差异是否与$\\mathcal{G}$和$\\mathcal{S}$之间的差异相同？（2）随着控制文本变异性的LLM参数的调整，$\\mathcal{G}$是否变得更接近$\\mathcal{O}$？第一个问题基于这样一个假设，即如果LLM生成的文本真正地类似于人类语言，那么这两个对（$\\mathcal{O}$, $\\mathcal{G}$）之间的差距应该与对（$\\mathcal{G}$, $\\mathcal{S}$）之间的差距相似，因为这两个对都包含原始文本及其改写。第二个问题考察LLM生成文本与人工文本之间的相似度是否随着文本生成宽度的变化而变化。为了解决这些问题，我们提出了一种统计假设检验框架，利用了由于它们的改写关系，每个文本在所有数据集中都有对应部分这一事实。这种关系使得可以将一个数据集的相对位置映射到另一个数据集，允许将两个数据集映射到第三个数据集。因此，这两个映射的数据集都可以根据第三个数据集所表征的空间进行量化，从而便于它们之间的直接比较。我们的结果表明，GPT生成的文本仍然与人工撰写的文本不同。|\n",
        "2501.01264": "|**2025-01-02**|**ProgCo: Program Helps Self-Correction of Large Language Models**|Xiaoshuai Song et.al.|[2501.01264](http://arxiv.org/abs/2501.01264)|null|自我校正旨在使大型语言模型（LLMs）能够自我验证和自我优化其初始响应，而不需要外部反馈。然而，LLMs往往无法有效地自我验证并生成正确的反馈，这进一步误导了优化过程，导致自我校正失败，尤其是在复杂推理任务中。在本文中，我们提出了程序驱动自我校正（ProgCo）。首先，程序驱动验证（ProgVe）通过自我生成、自我执行的验证伪程序实现了复杂的验证逻辑和广泛的验证。然后，程序驱动优化（ProgRe）接收来自ProgVe的反馈，对响应和验证程序进行双重反思和优化，以减轻在复杂推理任务中不正确反馈的误导。在三个指令遵循和数学基准测试上的实验表明，ProgCo实现了有效的自我校正，并且当与真实程序工具结合时，可以进一步提高性能。|\n",
        "2501.01957": "|**2025-01-03**|**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**|Chaoyou Fu et.al.|[2501.01957](http://arxiv.org/abs/2501.01957)|**[link](https://github.com/VITA-MLLM/VITA)**|**近年来，多模态大型语言模型（MLLMs）通常侧重于整合视觉和文本模态，而对语音在增强交互中的作用关注较少。然而，语音在多模态对话系统中起着至关重要的作用，由于基本模态差异，同时在视觉和语音任务中实现高性能仍然是一个重大挑战。在本文中，我们提出了一种精心设计的多阶段训练方法，该方法逐步训练LLM理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉-语言能力，还使语音到语音的对话能力变得高效，无需单独的ASR和TTS模块，显著加快了多模态端到端响应速度。通过将我们的方法与图像、视频和语音任务的基准测试中的最先进方法进行比较，我们证明了我们的模型具备强大的视觉和语音能力，实现了近乎实时的视觉和语音交互。**|\n",
        "2501.01945": "|**2025-01-03**|**Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap**|Weizhi Zhang et.al.|[2501.01945](http://arxiv.org/abs/2501.01945)|**[link](https://github.com/yuanchenbei/awesome-cold-start-recommendation)**|冷启动问题是推荐系统领域长期存在的挑战之一，它关注于准确建模新用户或互动受限的用户或物品，以提供更好的推荐。由于互联网平台的多样化和用户、物品数量的指数级增长，冷启动推荐（CSR）的重要性日益凸显。同时，大型语言模型（LLMs）取得了巨大成功，并在建模用户和物品信息方面具有强大能力，为冷启动推荐提供了新的潜力。然而，在CSR领域的科研社区中，仍缺乏对该领域的全面回顾和反思。基于此，本文站在大型语言模型时代的背景下，对CSR的路线图、相关文献和未来发展方向进行了全面回顾和讨论。具体来说，我们对现有CSR如何利用信息进行了探索，从内容特征、图关系和领域信息到大型语言模型所拥有的世界知识，旨在为CSR的研究和工业界提供新的见解。冷启动推荐的有关资源已收集并持续更新，供社区在https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation上使用。|\n",
        "2501.01904": "|**2025-01-03**|**Virgo: A Preliminary Exploration on Reproducing o1-like MLLM**|Yifan Du et.al.|[2501.01904](http://arxiv.org/abs/2501.01904)|**[link](https://github.com/rucaibox/virgo)**|最近，基于大型语言模型（LLMs）的慢思考推理系统，通过扩展推理过程中的思考时间，受到了广泛关注。同时，将这种能力应用于多模态大型语言模型（MLLMs）的兴趣也在增长。鉴于MLLMs需要在不同的模态中处理更复杂的数据语义，直观上实现多模态慢思考系统更具挑战性。为了解决这个问题，在本文中，我们通过微调一个具有能力的MLLM，使用少量的文本长篇思考数据，探索了一种简单的方法，从而创建了一个多模态慢思考系统，名为Virgo（视觉推理与长思考）。我们发现，这些以自然语言表达的长篇推理过程可以有效地转移到MLLMs中。此外，似乎这样的文本推理数据在激发MLLMs的慢思考能力方面甚至比视觉推理数据更有效。虽然这项工作还处于初步阶段，但它表明慢思考能力与语言模型组件的基本关联，这种能力可以跨模态或领域转移。这一发现可以用来指导更强大的慢思考推理系统的发展。我们将在https://github.com/RUCAIBox/Virgo上发布我们的资源。|\n",
        "2501.01872": "|**2025-01-03**|**Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions**|Rachneet Sachdeva et.al.|[2501.01872](http://arxiv.org/abs/2501.01872)|**[link](https://github.com/ukplab/poate-attack)**|尽管在将大型语言模型与人类价值观和伦理准则对齐方面付出了巨大努力，但这些模型仍然容易受到利用其推理能力的复杂越狱攻击。传统的安全机制通常侧重于检测明确的恶意意图，而未能解决更深层次的漏洞。在这项工作中，我们介绍了一种越狱技术，称为POATE（极对立查询生成、对抗模板构建和详细阐述），该技术利用对比推理来诱使模型产生不道德的回答。POATE生成具有语义上对立意图的提示，并将它们与对抗性模板相结合，微妙地引导模型产生有害的回答。我们在六个参数大小不同的多样化语言模型家族中进行了广泛的评估，包括LLaMA3、Gemma2、Phi3和GPT-4，以证明攻击的鲁棒性，与现有方法相比，攻击成功率显著提高（约44%）。我们对所提出的攻击进行了七种安全防御的评估，揭示了它们在解决基于推理的漏洞方面的局限性。为了应对这一问题，我们提出了一种防御策略，通过思维链提示和逆向思维来提高推理鲁棒性，减轻由推理驱动的对抗性攻击。|\n",
        "2501.01849": "|**2025-01-03**|**Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification**|Xiangxiang Dai et.al.|[2501.01849](http://arxiv.org/abs/2501.01849)|**[link](https://github.com/tarfersoul/maco)**|大型语言模型（LLMs）令人瞩目的生成能力激发了人们对于自动生成不同应用响应的兴趣。鉴于用户偏好的动态性和LLMs响应性能的不确定性，设计高效的在线学习算法以识别最优LLMs响应（即既高质量又符合用户偏好的响应）变得至关重要。大多数现有的在线算法采用集中式方法，未能利用显式用户偏好以更高效、个性化地识别LLMs响应。相比之下，本文介绍了MACO（多智能体对话在线学习用于自适应LLMs响应识别）：1）通过多个本地代理（如智能手机）加速在线LLMs响应识别过程，同时增强数据隐私；2）提出了一种新颖的对话机制，以自适应地进行对话以征求用户偏好（例如，在生成的响应中对幽默语气而非严肃语气有偏好），从而最小化偏好估计的不确定性。我们的理论分析表明，MACO在累积遗憾方面接近最优。此外，MACO通过消除先前工作中发现的传统的、计算密集型的“G-最优设计”来降低通信成本和计算复杂度。与公开的LLMs Llama的大量实验，以及来自Google和OpenAI的两个不同的嵌入模型用于文本向量表示，表明MACO在在线LLMs响应识别方面显著优于当前最先进的技术。|\n",
        "2501.01834": "|**2025-01-03**|**MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning**|Pu Yang et.al.|[2501.01834](http://arxiv.org/abs/2501.01834)|null|图像描述是计算机视觉和自然语言处理交叉领域的一项关键任务，在各个领域有着广泛的应用。对于诊断报告生成等复杂任务，深度学习模型不仅需要特定领域的图像描述数据集，还需要融入相关的一般知识以提供情境准确性。现有方法存在固有局限性：专门模型在捕捉特定领域细节方面表现优秀，但缺乏泛化能力，而基于大型语言模型（LLMs）的视觉语言模型（VLMs）虽然利用了一般知识，但在特定领域适应性方面存在困难。为了解决这些局限性，本文提出了一种新颖的代理增强模型协作框架，我们称之为MoColl，旨在有效地整合特定领域和一般知识。具体来说，我们的方法是将复杂的图像描述任务分解为一系列相互关联的问答子任务。一个可训练的视觉问答（VQA）模型被用作专门的工具，专注于特定领域的视觉分析，根据图像内容回答特定问题。同时，一个基于LLM的代理利用一般知识提出这些问题，并将生成的问答对综合成连贯的描述。除了利用VQA模型的作用外，代理还指导其训练以增强其特定领域的能力。在放射学报告生成上的实验结果验证了所提出框架的有效性，证明了生成报告质量显著提高。|\n",
        "2501.01832": "|**2025-01-03**|**Time Series Language Model for Descriptive Caption Generation**|Mohamed Trabelsi et.al.|[2501.01832](http://arxiv.org/abs/2501.01832)|null|在时间序列数据中自动生成代表性的自然语言描述，可以增强可解释性、简化分析并提高时间数据的跨领域应用价值。虽然预训练的基础模型在自然语言处理（NLP）和计算机视觉（CV）领域取得了显著进展，但它们在时间序列分析中的应用受到数据稀缺性的限制。尽管已经提出了基于大型语言模型（LLM）的几种时间序列预测方法，但在LLM的背景下，时间序列字幕生成仍未得到充分探索。在本文中，我们介绍了TSLM，这是一种专为时间序列字幕生成设计的新型时间序列语言模型。TSLM作为一个编码器-解码器模型，利用文本提示和时间序列数据表示来捕捉多个阶段中的微妙时间模式，并生成时间序列输入的精确文本描述。TSLM通过以下两种方式解决时间序列字幕生成中的数据稀缺问题：首先，利用上下文提示合成数据生成；其次，通过应用于时间序列-字幕对的创新跨模态密集检索评分来降噪生成的数据。在多个时间序列字幕生成数据集上的实验结果表明，TSLM在多个数据模态上显著优于现有的最先进方法。|\n",
        "2501.01830": "|**2025-01-03**|**Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models**|Yanjiang Liu et.al.|[2501.01830](http://arxiv.org/abs/2501.01830)|null|自动化的红队攻击已成为揭示大型语言模型（LLMs）漏洞的关键方法。然而，现有的大多数方法都集中在孤立的安全缺陷上，限制了它们适应动态防御和高效发现复杂漏洞的能力。为了应对这一挑战，我们提出了Auto-RT，一个强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效地发现安全漏洞。具体来说，我们引入了两种关键机制来降低探索复杂性和提高策略优化：1）提前终止探索，通过关注高潜力的攻击策略来加速探索；2）具有中间降级模型的渐进式奖励跟踪算法，该算法动态地细化搜索轨迹，以成功利用漏洞。在多种LLMs上的大量实验表明，通过显著提高探索效率和自动优化攻击策略，Auto-RT能够检测到更广泛的漏洞，相比现有方法，检测速度更快，成功率高16.63%。|\n",
        "2501.01821": "|**2025-01-03**|**SDPO: Segment-Level Direct Preference Optimization for Social Agents**|Aobo Kong et.al.|[2501.01821](http://arxiv.org/abs/2501.01821)|**[link](https://github.com/alibabaresearch/damo-convai)**|**由大型语言模型（LLMs）驱动的社交代理能够模拟人类的社会行为，但在处理复杂的目标导向社交对话方面存在不足。直接偏好优化（DPO）已被证明在使LLM行为与人类偏好一致方面在各种代理任务中非常有效。现有的基于DPO的多轮交互方法分为回合级和会话级方法。回合级方法过于细粒度，仅专注于单个回合，而会话级方法则过于粗粒度，往往引入训练噪声。为了解决这些局限性，我们提出了段落级直接偏好优化（SDPO），它专注于交互中的特定关键段落，以优化多轮代理行为同时最小化训练噪声。在SOTOPIA基准上的评估表明，SDPO调优的代理在性能上始终优于现有的基于DPO的方法以及GPT-4o等专有LLMs，凸显了SDPO在提升基于LLM代理的社会智能方面的潜力。我们已在https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO发布我们的代码和数据。**|\n",
        "2501.01793": "|**2025-01-03**|**Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation**|Mohammad Khalil et.al.|[2501.01793](http://arxiv.org/abs/2501.01793)|**[link](https://github.com/mohdkhalil/repository-supplementary-for-lak-25-paper--creating-artificial-students-that-never-existed)**|在本研究中，我们探讨了人工智能和深度学习技术，特别是生成对抗网络（GANs）和大型语言模型（LLMs），在生成合成表格数据方面的日益增长的潜力。获取高质量的学生数据对于推进学习分析至关重要，但隐私担忧和全球范围内更严格的数据保护法规限制了其可用性和使用。合成数据提供了一种有前景的替代方案。我们研究是否可以利用合成数据来创建用于服务学习分析模型的人工学生。使用流行的GAN模型CTGAN和三种LLMs——GPT2、DistilGPT2和DialoGPT，我们生成了合成表格学生数据。我们的结果表明，这些方法具有很强的潜力产生高质量的类似真实学生数据的合成数据集。为了验证我们的发现，我们应用了一套综合的效用评估指标来评估合成数据的统计和预测性能，并比较了所使用的不同生成模型，特别是LLMs的性能。我们的研究旨在为学习分析社区提供关于合成数据使用的宝贵见解，为扩展该领域的工具箱，以新的创新方法生成学习分析数据奠定基础。|\n",
        "2501.03226": "|**2025-01-06**|**BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**|Beichen Zhang et.al.|[2501.03226](http://arxiv.org/abs/2501.03226)|**[link](https://github.com/beichenzbc/booststep)**|**先进的语言大模型（LLMs）通过分而治之的流程和在上下文学习（ICL）示例的辅助下，在解决复杂数学问题方面展现出有前景的性能。然而，它们改进的潜力受到两个关键问题的限制：在ICL示例中的粒度不匹配以及随之而来的负面效应噪声问题。具体来说，LLMs能够进行分解过程，但在几个征服步骤中通常由于推理不准确而失败，而针对问题粒度检索的ICL示例有时缺乏特定推理步骤的相关步骤。此外，这种脱节可能由于其不相关性而阻碍正确推理。为此，我们专注于提高每个步骤中的推理质量，并提出了BoostStep。BoostStep在步骤粒度上对检索和推理之间的粒度进行了对齐，并使用新颖的“第一次尝试”策略为每个推理步骤提供高度相关的ICL示例。BoostStep比粗略的问题粒度策略提供了更多相关示例，从而稳步提高模型在每个步骤中的推理质量。BoostStep是一种通用且鲁棒的推理增强方法，它不仅提高了独立推理性能，而且可以无缝集成到蒙特卡洛树搜索方法（MCTS）中，以优化候选生成和决策。在数量上，它在各种数学基准测试中将GPT-4o和Qwen2.5-Math-72B分别提高了3.6%和2.0%，并结合MCTS实现了7.5%的增益。**|\n",
        "2501.03212": "|**2025-01-06**|**Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text**|Ayat Najjar et.al.|[2501.03212](http://arxiv.org/abs/2501.03212)|null|生成式人工智能大型语言模型（LLMs）的发展引发了关于识别由生成式AI或人类产生的内容的警报。在一种情况下，当学生过度依赖这些工具，以至于可能影响他们的写作或编码技能发展时，就会产生问题。其他关于剽窃的问题也适用。本研究旨在支持检测和识别使用LLM工具生成的文本的努力。我们假设通过机器学习（ML）可以检测到LLM生成的文本，并调查可以识别和区分由多个LLM工具生成的文本的ML模型。我们利用了多种机器学习和深度学习（DL）算法，如随机森林（RF）和循环神经网络（RNN），并利用可解释人工智能（XAI）来理解归因中的重要特征。我们的方法分为两部分：1）二元分类，用于区分人类撰写的文本和AI文本；2）多分类，用于区分人类撰写的文本和由五个不同的LLM工具（ChatGPT、LLaMA、Google Bard、Claude和Perplexity）生成的文本。结果显示在多分类和二元分类中都具有高准确率。我们的模型在准确率上优于GPTZero，前者为98.5%，后者为78.3%。值得注意的是，GPTZero无法识别大约4.2%的观测值，但我们的模型能够识别完整的测试数据集。XAI结果显示，理解不同类别中的特征重要性可以创建详细的作者/来源档案。此外，通过突出独特的风格和结构元素，帮助归因并支持剽窃检测，确保内容原创性的稳健验证。|\n",
        "2501.03203": "|**2025-01-06**|**Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity**|Ayat A. Najjar et.al.|[2501.03203](http://arxiv.org/abs/2501.03203)|null|本研究旨在通过提供工具来检测学生作业中的AI生成内容，利用先进技术来提高学术诚信。研究结果促进了透明度和问责制，帮助教育者维持道德标准，并支持人工智能在教育中的负责任整合。这项工作的一个关键贡献是生成了CyberHumanAI数据集，该数据集包含1000个观察值，其中500个由人类撰写，另外500个由ChatGPT生成。我们评估了CyberHumanAI数据集上的各种机器学习（ML）和深度学习（DL）算法，比较了大型语言模型（LLMs）（例如ChatGPT）生成的人类撰写内容和AI生成内容。结果表明，传统的ML算法，特别是XGBoost和随机森林，实现了高精度（分别达到83%和81%的准确率）。结果还显示，分类短内容似乎比分类长内容更具挑战性。此外，使用可解释人工智能（XAI）我们确定了影响ML模型预测的判别特征，其中人类撰写的内容倾向于使用实用语言（例如，使用和允许）。而AI生成的文本则具有更多抽象和正式的术语（例如，领域和雇佣）。最后，与GPTZero的比较分析表明，我们的专注、简单且微调过的模型可以超越像GPTZero这样的通用系统。当任务是对纯AI、纯人类和混合类别进行分类时，我们提出的模型实现了约77.5%的准确率，而GPTZero的准确率为48.5%。GPTZero倾向于将具有挑战性和小内容的情况分类为混合或未识别，而我们的模型在三个类别中表现出更均衡的性能。|\n",
        "2501.03191": "|**2025-01-06**|**CLIX: Cross-Lingual Explanations of Idiomatic Expressions**|Aaron Gluck et.al.|[2501.03191](http://arxiv.org/abs/2501.03191)|null|为了支持语言学习者的词汇扩展，已经提出了自动定义生成系统。这些系统成功的主要障碍在于学习者往往难以理解定义，尤其是当涉及非标准语言时，因为其中可能包含不熟悉的单词和语法。为了应对这些挑战，我们提出了CLIX任务，即习语表达的多语言解释。我们探讨了当前NLP模型在此任务中的能力，并观察到虽然仍然具有挑战性，但大型语言模型显示出潜力。最后，我们进行了详细的错误分析，以突出在我们可以可靠地将这些系统纳入教育工具之前需要解决的关键挑战。|\n",
        "2501.03166": "|**2025-01-06**|**Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text**|Ali Al-Lawati et.al.|[2501.03166](http://arxiv.org/abs/2501.03166)|**[link](https://github.com/aliwister/ast-icl)**|**大型语言模型（LLMs）在各种自然语言处理任务中表现出色，包括语义解析，即将自然语言翻译成形式化的代码表示。然而，反向过程，即代码翻译成自然语言，被称为语义字幕，却受到了较少的关注。随着LLMs被集成到代码生成、安全分析和教育等平台中，这项任务变得越来越重要。在本文中，我们专注于SQL查询（SQL2Text）的字幕制作，以解决在LLM生成代码可能带来潜在安全风险的背景下，理解和解释SQL查询的迫切需求。我们通过引入使用GPT-4o的迭代ICL提示，将Text2SQL数据集用于SQL2Text，从而生成多个额外的表述，增强了数据集对反向任务的鲁棒性。我们使用基于不同样本选择方法的上下文学习（ICL）进行实验，强调较小、计算效率更高的LLMs。我们的发现表明，利用SQL固有的图属性进行ICL样本选择，在BLEU分数上比随机选择高出39%，并且比替代方法提供更好的结果。数据集和代码已发布：\\url{https://github.com/aliwister/ast-icl}。**|\n",
        "2501.03151": "|**2025-01-06**|**Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**|Alhassan Mumuni et.al.|[2501.03151](http://arxiv.org/abs/2501.03151)|null|基于大规模预训练基础模型（PFMs）的生成式人工智能（AI）系统，例如视觉-语言模型、大型语言模型（LLMs）、扩散模型和视觉-语言-动作（VLA）模型，已在众多领域和情境中展现出解决复杂且真正非平凡AI问题的能力。特别是多模态大型语言模型（MLLMs），它们从广泛且多样化的数据源中学习，从而能够对世界进行丰富和细腻的表征，并因此提供广泛的能力，包括推理、进行有意义的对话；与人类和其他代理共同解决复杂问题；以及理解人类的社会和情感方面。尽管这一成就令人印象深刻，但基于大规模数据集训练的最先进LLMs的认知能力仍然肤浅且脆弱。因此，通用LLMs在它们的泛化能力上受到严重限制。为了使LLMs达到人类水平的通用智能，需要解决一系列基础问题——具身化、符号化、因果关系和记忆。这些概念与人类认知更为一致，并为LLMs提供了固有的类似人类认知的特性，支持实现具有物理可能性、语义意义、灵活性和更广泛的可推广的知识和智能。在这项工作中，我们讨论了上述基础问题，并概述了在LLMs中实现这些概念的最先进方法。具体来说，我们讨论了如何利用具身化、符号化、因果关系和记忆的原则，以有机的方式实现人工通用智能（AGI）。|\n",
        "2501.03139": "|**2025-01-06**|**VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity**|Yerong Li et.al.|[2501.03139](http://arxiv.org/abs/2501.03139)|null|基于场景的训练在许多公共服务领域得到了广泛应用。近年来，大型语言模型（LLMs）在模拟不同角色以创建这些训练场景方面展现出良好的前景。然而，关于如何开发LLMs来模拟受害者以用于场景化训练的了解还很少。在本文中，我们介绍了VicSim（受害者模拟器），这是一个新颖的模型，它针对用户模拟的三个关键维度：信息忠实度、情感动态和语言风格（例如，语法使用）。我们开创性地将基于场景的受害者建模与基于GAN的训练工作流程和基于关键信息的提示相结合，旨在提高模拟受害者的逼真度。我们的对抗性训练方法教会了判别器将语法和情感线索识别为合成内容的可靠指标。根据人类评分者的评估，VicSim模型在拟人化方面优于GPT-4。|\n",
        "2501.03124": "|**2025-01-06**|**PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models**|Mingyang Song et.al.|[2501.03124](http://arxiv.org/abs/2501.03124)|**[link](https://github.com/ssmisya/PRMBench)**|**过程级奖励模型（PRMs）对于复杂推理和决策任务至关重要，其中每个中间步骤在推理过程中都扮演着重要角色。由于语言模型在推理过程中容易发生各种类型的错误，PRMs需要具备细微的能力来检测现实场景中各种隐含的错误类型。然而，当前的基准测试主要关注步骤的正确性，未能系统地评估PRMs的性能。为了解决这一差距，我们引入了PRMBench，这是一个专门设计来评估PRMs细粒度错误检测能力的进程级基准。PRMBench包含6,216个精心设计的问题和83,456个步骤级标签，从多个维度评估模型，包括简洁性、严谨性和灵敏度。在我们的对15个模型的实验中，这些模型包括开源PRMs和作为批评模型的闭源大型语言模型，我们发现了当前PRMs的重大弱点。这些发现强调了过程级评估中固有的挑战，并突出了未来研究的关键方向。我们希望PRMBench能够成为推进PRM评估和发展研究的坚实平台。**|\n",
        "2501.03120": "|**2025-01-06**|**CAT: Content-Adaptive Image Tokenization**|Junhong Shen et.al.|[2501.03120](http://arxiv.org/abs/2501.03120)|null|现有的图像分词器通常将图像编码为固定数量的标记或块，忽略了图像复杂性的固有变化。为了解决这个问题，我们引入了内容自适应分词器（Content-Adaptive Tokenizer，简称CAT），它根据图像内容动态调整表示容量，并将简单的图像编码为更少的标记。我们设计了一个基于字幕的评估系统，该系统利用大型语言模型（LLMs）来预测内容复杂度，并确定给定图像的最佳压缩比率，同时考虑了人类感知的关键因素。CAT在具有不同压缩比率的图像上训练，展示了在图像重建方面的稳健性能。我们还利用其可变长度的潜在表示来训练用于ImageNet生成的扩散变换器（Diffusion Transformers，简称DiTs）。通过优化标记分配，CAT在相同flops下训练的固定比率基线中提高了FID分数，并将推理吞吐量提升了18.5%。|\n",
        "2501.03112": "|**2025-01-06**|**LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases**|Dylan Bouchard et.al.|[2501.03112](http://arxiv.org/abs/2501.03112)|**[link](https://github.com/cvs-health/langfair)**|**大型语言模型（LLMs）被观察到以多种方式表现出偏见，这可能会对由性别、种族、性取向或年龄等受保护属性识别的特定群体造成或加剧不良后果。为了帮助填补这一空白，我们引入了LangFair，这是一个开源的Python软件包，旨在为LLMs从业者提供评估与其特定用例相关的偏见和公平性风险的工具。该软件包提供了易于生成评估数据集的功能，这些数据集包含LLMs对特定用例提示的回答，并随后计算适用于从业者用例的相关指标。为了指导指标选择，LangFair提供了一个可操作的决策框架。**|\n",
        "2501.04001": "|**2025-01-07**|**Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos**|Haobo Yuan et.al.|[2501.04001](http://arxiv.org/abs/2501.04001)|**[link](https://github.com/magic-research/Sa2VA)**|这项工作介绍了Sa2VA，这是第一个用于密集地理解图像和视频的统一模型。与现有的多模态大型语言模型不同，这些模型通常仅限于特定的模态和任务，Sa2VA支持广泛的图像和视频任务，包括指称分割和对话，且仅需最小的一次性指令调整。Sa2VA将SAM-2（一个基础视频分割模型）与LLaVA（一个高级视觉语言模型）相结合，并将文本、图像和视频统一到一个共享的大型语言模型（LLM）标记空间中。利用LLM，Sa2VA生成指令标记，引导SAM-2生成精确的掩码，从而实现对静态和动态视觉内容的基于情境的多模态理解。此外，我们引入了Ref-SAV，一个包含超过72k个复杂视频场景中物体表情的自动标记数据集，旨在提升模型性能。我们还在Ref-SAV数据集中手动验证了2k个视频物体，以在复杂环境中建立指称视频物体分割的基准。实验表明，Sa2VA在多个任务中达到了最先进的水平，尤其是在指称视频物体分割方面，突显了其在复杂现实世界应用中的潜力。|\n",
        "2501.03995": "|**2025-01-07**|**RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance**|Matin Mortaheb et.al.|[2501.03995](http://arxiv.org/abs/2501.03995)|null|检索增强生成（RAG）通过使用外部知识引导响应生成，从而改善大型语言模型（LLMs），减少幻觉。然而，RAG，尤其是多模态RAG，可能会引入新的幻觉来源：（i）检索过程可能会从数据库中选择无关的片段（例如，文档、图像）作为原始上下文，以及（ii）检索到的图像通过视觉语言模型（VLMs）或直接由多模态语言模型（MLLMs）如GPT-4o处理成文本上下文，这可能导致幻觉。为了解决这个问题，我们提出了一种新的框架来评估多模态RAG的可靠性，使用两个性能指标：（i）相关性分数（RS），评估检索条目与查询的相关性，以及（ii）正确性分数（CS），评估生成响应的准确性。我们使用基于ChatGPT的数据库和人工评估员样本训练RS和CS模型。结果显示，这两个模型在测试数据上均达到约88%的准确率。此外，我们构建了一个包含5000个样本的人工标注数据库，用于评估检索片段的相关性和响应语句的正确性。我们的RS模型在检索中与人类偏好的吻合度比CLIP高出20%，我们的CS模型有约91%的时间与人类偏好相匹配。最后，我们使用RS和CS评估了各种RAG系统的选择和生成性能。|\n",
        "2501.03991": "|**2025-01-07**|**Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles**|Yuxi Xia et.al.|[2501.03991](http://arxiv.org/abs/2501.03991)|null|校准，即模型置信度与预测准确度之间的对齐，对于大型语言模型（LLMs）的可靠部署至关重要。现有工作忽略了测量其方法对不同提示风格和不同尺寸的LLMs泛化的能力。为了解决这个问题，我们定义了一个受控的实验环境，涵盖了12个LLMs和四种提示风格。此外，我们还研究了引入多个LLMs的响应一致性和合适的损失函数是否可以提高校准性能。具体来说，我们构建了Calib-n，一个新颖的框架，该框架训练一个辅助模型进行置信度估计，聚合多个LLMs的响应以捕捉模型间的一致性。为了优化校准，我们将焦点损失和AUC代理损失与二元交叉熵相结合。在四个数据集上的实验表明，响应一致性和焦点损失都提高了基线校准。我们发现，少样本提示对于基于辅助模型的方法最为有效，并且辅助模型在准确度变化中表现出鲁棒的校准性能，优于LLMs的内部概率和口头化的置信度。这些见解加深了对LLMs校准影响因素的理解，支持它们在多种应用中的可靠部署。|\n",
        "2501.03989": "|**2025-01-07**|**(De)-Indexing and the Right to be Forgotten**|Salvatore Vilella et.al.|[2501.03989](http://arxiv.org/abs/2501.03989)|null|在数字时代，健忘的问题已成为一个重要的关注点，特别是在个人数据管理和在线可访问性方面。被遗忘权（RTBF）允许个人要求从公开访问中删除过时或有害的信息，然而实施这一权利对搜索引擎来说存在巨大的技术困难。本文旨在向非专业人士介绍信息检索（IR）和去索引的基础概念，这些概念对于理解搜索引擎如何有效地“忘记”某些内容至关重要。我们将探讨各种信息检索模型，包括布尔逻辑、概率、向量空间和基于嵌入的方法，以及大型语言模型（LLMs）在增强数据处理能力中的作用。通过提供这一概述，我们旨在突出在平衡个人隐私权利与搜索引擎在管理信息可见性方面面临的运营挑战中所涉及的复杂性。|\n",
        "2501.03968": "|**2025-01-07**|**VLM-driven Behavior Tree for Context-aware Task Planning**|Naoki Wake et.al.|[2501.03968](http://arxiv.org/abs/2501.03968)|**[link](https://github.com/microsoft/scene-aware-robot-BT-planner)**|近年来，在机器人领域，使用大型语言模型（LLMs）生成行为树（BTs）引起了人们的关注，但这一领域仍处于发展的早期阶段。在本文中，我们提出了一种新颖的框架，该框架利用视觉-语言模型（VLMs）以交互式方式生成和编辑针对视觉条件的BTs，从而在视觉复杂环境中实现具有上下文意识的机器人操作。我们方法的关键特征在于通过自我提示的视觉条件进行条件控制。具体来说，VLM生成带有视觉条件节点的BTs，其中条件以自由文本形式表达。另一个VLM过程将文本集成到其提示中，并在机器人执行过程中将条件与真实世界图像进行评估。我们在一个现实世界的咖啡馆场景中验证了我们的框架，展示了其可行性和局限性。|\n",
        "2501.03957": "|**2025-01-07**|**Vision Language Models as Values Detectors**|Giulio Antonio Abbo et.al.|[2501.03957](http://arxiv.org/abs/2501.03957)|null|大型语言模型融合文本和视觉输入，为解读复杂数据带来了新的可能性。尽管这些模型在根据视觉刺激生成连贯且与语境相关的文本方面表现出色，但它们在识别图像中相关元素方面与人类感知的对齐仍需进一步探索。本文调查了最先进的LLMs与人类标注者在家庭环境场景中检测相关元素之间的对齐情况。我们创建了一组十二幅描绘各种家庭场景的图像，并召集了十四位标注者来识别每幅图像中的关键元素。然后，我们将这些人类响应与包括GPT-4o和四种LLaVA变体在内的五个不同LLMs的输出进行了比较。我们的发现表明，对齐程度存在差异，其中LLaVA 34B表现最佳但得分仍然较低。然而，对结果的分析突显了模型检测图像中具有价值元素的能力，表明随着训练的改进和提示的精细化，LLMs可以通过提供更深入的见解和更具语境相关性的响应，增强在社交机器人、辅助技术和人机交互中的应用。|\n",
        "2501.03952": "|**2025-01-07**|**Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States**|Jurgita Kapočiūtė-Dzikienė et.al.|[2501.03952](http://arxiv.org/abs/2501.03952)|null|尽管大型语言模型（LLMs）已经改变了我们对现代语言技术的期望，但关于数据隐私的担忧通常限制了在欧盟司法管辖之外托管的可商用LLMs的使用。这限制了它们在政府、国防和其他数据敏感领域的应用。在本研究中，我们评估了本地可部署的开源权重LLMs在支持立陶宛语、拉脱维亚语和爱沙尼亚语等小语种方面的程度。我们检查了表现最好的多语言开源权重模型Llama~3、Gemma~2、Phi和NeMo的多种大小和精度变体，包括机器翻译、多选题回答和自由文本生成。结果表明，尽管某些模型如Gemma~2的表现接近可商用顶级模型，但许多LLMs在这些语言上存在困难。然而，最令人惊讶的是，我们发现这些模型虽然在翻译性能上接近最先进的水平，但对于所有开源多语言LLMs，仍然容易出现词汇幻觉，错误率至少在20个词中就有1个。|\n",
        "2501.03940": "|**2025-01-07**|**Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection**|Pablo Miralles-González et.al.|[2501.03940](http://arxiv.org/abs/2501.03940)|null|大型语言模型（LLMs）的快速发展显著提高了它们生成连贯且与上下文相关文本的能力，引发了对AI生成内容滥用的担忧，并使其检测变得至关重要。然而，这项任务仍然具有挑战性，尤其是在未见过的领域或与不熟悉的LLMs中。利用LLM的下一词分布输出为检测提供了一种理论上有吸引力的方法，因为这些输出封装了模型在多样化语料库上广泛预训练的见解。尽管这种方法有潜力，但试图将输出实际化的零样本方法取得了有限的成果。我们假设其中一个问题是，它们使用平均值来汇总跨所有词的下一词分布度量，而有些词自然更容易或更难预测，应该有不同的权重。基于这一想法，我们提出了困惑度注意力加权网络（PAWN），它使用LLM的最后隐藏状态和位置来根据序列长度内下一词分布的指标加权一系列特征的求和。虽然不是零样本方法，但我们的方法允许我们在磁盘上缓存最后隐藏状态和下一词分布度量，从而大大减少了训练资源需求。PAWN在分布内的表现与最强大的基线（微调的LLMs）相媲美，甚至更好，而所需的可训练参数仅为其一小部分。我们的模型在未见过的领域和源模型上也表现出更好的泛化能力，分布变化时的决策边界变化更小。它对对抗攻击也更加鲁棒，如果主干具有多语言能力，它在监督训练期间未见过的语言上也表现出良好的泛化能力，LLaMA3-1B在九种语言的交叉验证中达到了平均宏平均F1分数81.46%。|\n",
        "2501.03904": "|**2025-01-07**|**Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**|Ramya Jonnala et.al.|[2501.03904](http://arxiv.org/abs/2501.03904)|null|将大型语言模型（LLMs）集成到公共交通系统中，为提升城市交通提供了变革性的机遇。本研究探讨了LLMs在圣安东尼奥公共交通系统背景下，对公共交通管理进行革新的潜力。利用LLMs在自然语言处理和数据分析方面的能力，我们研究了它们在优化路线规划、减少等待时间和提供个性化旅行协助方面的能力。通过利用通用公共交通数据规范（GTFS）和其他相关数据，本研究旨在展示LLMs如何有可能改善资源配置、提高乘客满意度，并为公共交通运营中的数据驱动决策提供信息。对不同的ChatGPT模型进行了比较分析，以评估它们理解交通信息、检索相关数据和提供全面回应的能力。该研究的结果表明，尽管LLMs在公共交通领域具有巨大的潜力，但实现其全部潜力需要谨慎的工程设计和精细调整。圣安东尼奥作为一个案例研究，为其他城市环境开发LLM驱动的公共交通系统提供了参考。|\n",
        "2501.03895": "|**2025-01-07**|**LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**|Shaolei Zhang et.al.|[2501.03895](http://arxiv.org/abs/2501.03895)|**[link](https://github.com/ictnlp/llava-mini)**|**实时大型多模态模型（LMMs）如GPT-4o的出现，引发了人们对高效LMMs的极大兴趣。LMM框架通常将视觉输入编码为视觉标记（连续表示）并将它们与文本指令整合到大型语言模型（LLMs）的上下文中，大规模参数和众多上下文标记（主要是视觉标记）导致计算开销巨大。以往针对高效LMMs的努力总是侧重于用较小的模型替换LLM主干，而忽略了标记数量这一关键问题。在本文中，我们引入了LLaVA-Mini，这是一种具有最小视觉标记的高效LMM。为了在保留视觉信息的同时实现视觉标记的高压缩比，我们首先分析了LMM是如何理解视觉标记的，并发现大多数视觉标记仅在LLM主干的前几层发挥关键作用，它们主要将视觉信息融合到文本标记中。基于这一发现，LLaVA-Mini引入了模态预融合，将视觉信息预先融合到文本标记中，从而使得馈送到LLM主干的视觉标记能够极端压缩成一个标记。LLaVA-Mini是一个统一的大型多模态模型，可以高效地支持对图像、高分辨率图像和视频的理解。在11个基于图像和7个基于视频的基准测试中的实验表明，LLaVA-Mini只需1个视觉标记就能超越LLaVA-v1.5的576个视觉标记。效率分析显示，LLaVA-Mini可以将FLOPs减少77%，在40毫秒内提供低延迟响应，并在具有24GB内存的GPU硬件上处理超过10,000帧的视频。**|\n",
        "2501.04695": "|**2025-01-08**|**Re-ranking the Context for Multimodal Retrieval Augmented Generation**|Matin Mortaheb et.al.|[2501.04695](http://arxiv.org/abs/2501.04695)|null|检索增强生成（RAG）通过结合外部知识来提高大型语言模型（LLMs）在特定语境下生成响应的准确性和减少幻觉。然而，多模态RAG系统面临独特的挑战：（i）检索过程可能会选择与用户查询不相关的条目（例如，图像、文档），以及（ii）在处理这些条目以生成RAG输出时，视觉-语言模型或多模态语言模型如GPT-4o可能会产生幻觉。在本文中，我们旨在解决第一个挑战，即改进多模态RAG检索阶段中从知识库中选择相关语境的能力。具体来说，我们利用我们在之前工作中设计的用于评估RAG性能的相关性得分（RS）度量来选择检索过程中的更多相关条目。基于嵌入的检索，如基于CLIP的嵌入和余弦相似度，通常在多模态数据上表现不佳。我们表明，通过使用更先进的关联度量，可以通过从知识库中选择更多相关片段来增强检索过程，并通过自适应选择最多$k$个条目而不是固定数量的条目来消除语境中的不相关片段。我们使用COCO数据集进行的评估显示了在选择相关语境和生成响应的准确性方面的重要提升。|\n",
        "2501.04686": "|**2025-01-08**|**URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**|Ruilin Luo et.al.|[2501.04686](http://arxiv.org/abs/2501.04686)|**[link](https://github.com/URSA-MATH/URSA-MATH)**|思维链（CoT）推理在大型语言模型（LLMs）的数学推理中得到了广泛应用。最近，在CoT轨迹上引入导数过程监督引发了关于测试时增强扩展能力的讨论，从而提升了这些模型潜力。然而，在多模态数学推理中，高质量CoT训练数据的稀缺阻碍了现有模型实现高精度CoT推理，并限制了测试时的推理潜力。在本工作中，我们提出了一种三模块合成策略，该策略集成了CoT蒸馏、轨迹格式重写和格式统一。它产生了一个高质量的多模态数学CoT推理指令微调数据集，即MMathCoT-1M。我们全面验证了训练的URSA-7B模型在多个多模态数学基准上的最先进（SOTA）性能。为了测试时的扩展，我们引入了一种数据合成策略，自动生成过程注释数据集，称为DualMath-1.1M，专注于解释和逻辑。通过在DualMath-1.1M上进一步训练URSA-7B，我们实现了从CoT推理能力到稳健监督能力的转变。训练后的URSA-RM-7B充当验证器，有效地提升了URSA-7B在测试时的性能。URSA-RM-7B还展示了优秀的分布外（OOD）验证能力，展示了其泛化能力。模型权重、训练数据和代码将开源。|\n",
        "2501.04675": "|**2025-01-08**|**Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations**|Archita Srivastava et.al.|[2501.04675](http://arxiv.org/abs/2501.04675)|null|图表解读对于可视化数据分析至关重要，但准确从图表中提取信息对自动化模型来说是一个重大挑战。本研究调查了DEPLOT模块的微调，该模块将图表或图像转换为线性化表格，在一个包含50,000个条形图的定制数据集上进行了研究。该数据集包括简单、堆叠和分组条形图，旨在针对这些可视化的独特结构特征。微调后的DEPLOT模型使用1,000张图像的测试集和两个指标对其基础版本进行了评估：相对映射相似度（RMS），它衡量分类映射的准确性；以及相对数字集合相似度（RNSS），它评估数值解释的准确性。为了进一步探索大型语言模型（LLMs）的推理能力，我们整理了一个额外的100张条形图图像集，配对问题答案集。我们的发现表明，与直接查询图像相比，在图像旁边提供结构化的中间表格可以显著提高LLMs的推理性能。|\n",
        "2501.04661": "|**2025-01-08**|**Assessing Language Comprehension in Large Language Models Using Construction Grammar**|Wesley Scivetti et.al.|[2501.04661](http://arxiv.org/abs/2501.04661)|null|大型语言模型尽管能力显著，但它们以令人惊讶和不可预测的方式失败。由于它们训练的数据规模庞大，评估其对语言的真正“理解”尤其具有挑战性。因此，我们构建了一个评估体系，利用构造语法（CxG）系统地评估大型语言模型（LLM）的自然语言理解（NLU）。CxG非常适合这个目的，因为它提供了一个理论基础来构建针对性的评估集。这些数据集被精心构建，包括不太可能出现在预训练数据中的例子，同时对于人类来说是直观且易于理解的，这使得评估更加针对性和可靠。我们的实验聚焦于下游自然语言推理和推理任务，通过比较LLM对通过8个独特的构造（Cxns）传达的潜在意义的理解与人类理解之间的差异。结果显示，尽管LLM展示了某些构造信息的知识，即使是包括GPT-o1在内的最新模型，在处理这些构造传达的抽象意义时也遇到了困难，正如测试句子与它们的预训练数据不相似的情况所显示的那样。我们认为，这些情况提供了对真正语言理解更准确的测试，突显了LLM在语义能力方面的关键局限性。我们将我们新颖的数据集以及相关的实验数据（包括提示和模型响应）公开。|\n",
        "2501.04652": "|**2025-01-08**|**Multi-task retriever fine-tuning for domain-specific and efficient RAG**|Patrice Béchard et.al.|[2501.04652](http://arxiv.org/abs/2501.04652)|null|检索增强生成（RAG）在部署大型语言模型（LLMs）时变得无处不在，因为它可以解决诸如生成幻觉或过时信息等典型限制。然而，当构建现实世界的RAG应用时，会出现实际问题。首先，检索到的信息通常是特定领域的。由于微调LLMs在计算上非常昂贵，因此更可行的是微调检索器以提高LLM输入中包含的数据质量。其次，随着更多应用在同一现实世界系统中部署，我们无法负担为每个检索器部署单独的应用。此外，这些RAG应用通常检索不同类型的数据。我们的解决方案是在各种特定领域任务上微调一个小型检索器编码器，使我们能够部署一个可以服务于多种用例的编码器，从而实现低成本、可扩展性和速度。我们展示了该编码器如何泛化到领域外设置，以及在实际企业用例中如何泛化到一个未见过的检索任务。|\n",
        "2501.04648": "|**2025-01-08**|**FlairGPT: Repurposing LLMs for Interior Designs**|Gabrielle Littlefair et.al.|[2501.04648](http://arxiv.org/abs/2501.04648)|null|室内设计涉及精心挑选和布置物品，以创造一个符合客户设计要求的、美观的、功能性强且和谐的空間。这项任务尤其具有挑战性，因为成功的设计不仅要将所有必要的物品以统一风格融合在一起，还要确保它们以最大化可达性的方式排列，同时遵循多种经济性和使用考虑因素。虽然已提出基于数据的方法，但这些方法通常针对特定房间或领域，且在产生最终布局时缺乏可解释性的设计设计考虑。在本文中，我们探讨了大型语言模型（LLMs）是否可以直接用于室内设计。我们发现，LLMs目前还不能生成完整的布局，但它们可以以结构化的方式有效利用，灵感来源于室内设计师的工作流程。通过系统地探测LLMs，我们可以可靠地生成一个物品列表以及指导其放置的相关约束。我们将这些信息转化为设计布局图，然后使用现成的约束优化设置来解决它，以生成最终的布局。我们将我们的算法与现有的基于LLM的方法和人类设计在各种设计配置中进行基准测试，并使用多种定量和定性指标以及用户研究来评估结果。总之，我们证明了当以结构化的方式使用时，LLMs可以有效地生成多样化的高质量布局，使它们成为创建大规模虚拟场景的可行解决方案。项目网页：https://flairgpt.github.io/|\n",
        "2501.04635": "|**2025-01-08**|**Knowledge Retrieval Based on Generative AI**|Te-Lun Yang et.al.|[2501.04635](http://arxiv.org/abs/2501.04635)|null|本研究开发了一个基于检索增强生成（RAG）的问答系统，利用中文维基百科和Lawbank作为检索来源。使用TTQA和TMMLU+作为评估数据集，该系统采用BGE-M3进行密集向量检索以获取高度相关的搜索结果，并使用BGE-reranker根据查询相关性对这些结果进行排序。最相关的检索结果作为大型语言模型（LLM）的参考知识，增强其回答问题和建立基于生成人工智能的知识检索系统的能力。系统通过两阶段评估来评估其有效性：自动和辅助性能评估。自动评估通过比较模型的自动生成标签与真实答案来计算准确率，在无需人类干预的标准化条件下测量性能。辅助性能评估包括20个由20位无金融背景的参与者回答的与金融相关的多项选择题。最初，参与者独立回答。随后，他们收到系统生成的参考信息以帮助回答，检验系统在提供辅助时是否提高了准确性。本研究的贡献主要包括：（1）增强LLM能力：通过集成BGE-M3和BGE-reranker，系统检索并重新排序高度相关的结果，减少幻觉，并动态访问授权或公共知识来源。（2）提高数据隐私：定制的RAG架构使得LLM能够本地运行，无需将私有数据发送到外部服务器。这种方法增强了数据安全性，减少了对商业服务的依赖，降低了运营成本，并缓解了隐私风险。|\n",
        "2501.04633": "|**2025-01-08**|**\"Can you be my mum?\": Manipulating Social Robots in the Large Language Models Era**|Giulio Antonio Abbo et.al.|[2501.04633](http://arxiv.org/abs/2501.04633)|null|近年来，由大型语言模型驱动的机器人取得了进步，增强了它们的对话能力，使交互更加接近人类对话。然而，这些模型在HRI（人机交互）中引入了安全和安全方面的担忧，因为它们容易受到绕过内置安全措施的操纵。设想一个部署在家庭中的社交机器人，这项工作旨在了解普通用户如何试图利用语言模型来违反道德原则，例如提示机器人表现得像一个生活伴侣。我们进行了一项试点研究，涉及21名大学生与Misty机器人互动，试图在基于特定HRI道德原则的三个场景中绕过其安全机制：依恋、自由和同理心。我们的结果显示，参与者使用了五种技术，包括使用情感语言侮辱和呼吁怜悯。我们希望这项工作能为未来研究提供信息，以设计强大的保障措施，确保道德和安全的人机交互。|\n",
        "2501.04591": "|**2025-01-08**|**Quantum-inspired Embeddings Projection and Similarity Metrics for Representation Learning**|Ivan Kankeu et.al.|[2501.04591](http://arxiv.org/abs/2501.04591)|**[link](https://github.com/ivpb/qiepsm)**|在过去十年中，表示学习作为一种将大量数据中提取的复杂信息嵌入到密集向量空间的关键技术，已经在机器学习中崭露头角。在其他应用中，它已成为大型语言模型和基于对比学习的高级计算机视觉系统的一个关键构建块。表示学习系统的核心组件是投影头，它将原始嵌入映射到不同的、通常是压缩的空间，同时保留向量之间的相似性关系。在本文中，我们提出了一种受量子启发的投影头，包括相应的受量子启发的相似性度量。具体来说，我们将经典嵌入映射到希尔伯特空间中的量子态，并引入一个基于量子电路的投影头来降低嵌入维度。为了评估这种方法的有效性，我们通过集成我们的投影头用于嵌入压缩，扩展了BERT语言模型。我们使用TREC 2019和TREC 2020深度学习基准在信息检索任务中比较了使用我们受量子启发的投影头压缩的嵌入与使用经典投影头压缩的嵌入的性能。结果表明，我们的受量子启发的方 法相对于经典方法实现了具有竞争力的性能，同时参数减少了32倍。此外，当从头开始训练时，它在较小的数据集上尤其表现出色。这项工作不仅突出了受量子启发的有效方法，还强调了在神经网络中利用高效、专用的低纠缠电路模拟作为强大的受量子启发的技术的实用性。|\n",
        "2501.04575": "|**2025-01-08**|**InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection**|Yuhang Liu et.al.|[2501.04575](http://arxiv.org/abs/2501.04575)|**[link](https://github.com/reallm-labs/infiguiagent)**|**图形用户界面（GUI）代理，借助多模态大型语言模型（MLLM），在计算机和手机等计算设备上的任务自动化方面展现出巨大潜力。然而，现有的代理在多步推理和依赖文本注释方面面临挑战，限制了其有效性。我们介绍了\\textit{InfiGUIAgent}，这是一种基于MLLM的GUI代理，采用两阶段监督微调管道进行训练。第一阶段提升基本技能，如GUI理解和定位，而第二阶段利用合成数据集成分层推理和预期反思推理技能，以赋予代理原生推理能力。\\textit{InfiGUIAgent}在多个GUI基准测试中取得了有竞争力的表现，突出了原生推理技能在增强GUI自动化任务交互方面的作用。相关资源可在\\url{https://github.com/Reallm-Labs/InfiGUIAgent}找到。**|\n",
        "2501.05452": "|**2025-01-09**|**ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding**|Xingyu Fu et.al.|[2501.05452](http://arxiv.org/abs/2501.05452)|null|结构化图像理解，如解释表格和图表，需要在对图像中各种结构和文本进行战略性地重新聚焦，形成一个推理序列以得出最终答案。然而，当前的多模态大型语言模型（LLMs）缺乏这种多跳选择性注意能力。在这项工作中，我们引入了ReFocus，这是一个简单而有效的框架，它通过代码对输入图像进行视觉编辑，使多模态LLMs能够通过生成“视觉思维”来调整和细化他们的视觉焦点。具体来说，ReFocus使多模态LLMs能够生成Python代码来调用工具并修改输入图像，依次绘制框、突出显示部分和遮蔽区域，从而增强视觉推理过程。我们在涉及表格和图表的广泛结构化图像理解任务上进行了实验。ReFocus在所有任务上相对于没有视觉编辑的GPT-4o大幅提升了性能，在表格任务上平均提高了11.0%，在图表任务上提高了6.8%。我们深入分析了不同视觉编辑效果，以及ReFocus在不引入额外信息的情况下提高性能的原因。此外，我们使用ReFocus收集了一个14k的训练集，并证明这种带有中间信息的视觉思维链提供了比标准VQA数据更好的监督，相对于使用QA对训练的同一模型平均提高了8.0%，相对于CoT提高了2.6%。|\n",
        "2501.05444": "|**2025-01-09**|**Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark**|Yunzhuo Hao et.al.|[2501.05444](http://arxiv.org/abs/2501.05444)|**[link](https://github.com/hychaochao/EMMA)**|人类智能的基石之一是能够在文本和图像之间进行有机推理，然而，多模态大型语言模型（MLLMs）在执行此类多模态推理方面的能力仍被低估。现有的基准测试往往强调以文本为主的推理或依赖于浅层视觉线索，未能充分评估综合视觉和文本推理。我们引入了EMMA（增强多模态推理），这是一个针对数学、物理、化学和编码等领域有机多模态推理的基准测试。EMMA任务要求进行高级跨模态推理，这种推理不能通过独立于每个模态的推理来解决，为MLLMs的推理能力提供了一个增强的测试套件。我们对最先进的MLLMs在EMMA上的评估显示，在处理复杂的多模态和多步骤推理任务时存在重大局限性，即使是在采用思维链提示和测试时计算扩展等高级技术的情况下表现不佳。这些发现强调了改进多模态架构和训练方法的需求，以缩小人类推理与模型在多模态推理能力之间的差距。|\n",
        "2501.05443": "|**2025-01-09**|**A survey of textual cyber abuse detection using cutting-edge language models and large language models**|Jose A. Diaz-Garcia et.al.|[2501.05443](http://arxiv.org/abs/2501.05443)|null|社交媒体平台的成功促进了数字社区中各种形式的在线滥用出现。这种滥用以多种方式体现，包括仇恨言论、网络欺凌、情感虐待、性侵和色情信息。在本文中，我们全面分析了社交媒体中普遍存在的滥用形式，特别关注新兴技术，如语言模型（LMs）和大型语言模型（LLMs），如何重塑这些网络中滥用内容的检测和生成。我们深入探讨了社交媒体滥用得以持续传播的机制，并探讨了其心理和社会影响。此外，我们考察了高级语言模型的二元角色——一方面强调其增强自动化检测系统以识别滥用行为的潜力，另一方面也承认其生成有害内容的能力。本文旨在为关于在线安全和伦理的持续讨论做出贡献，提供对网络滥用不断演变的格局以及减轻和加剧这一问题的技术创新的见解。|\n",
        "2501.05423": "|**2025-01-09**|**Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese Micro-bloggers**|Jerry Chongyi Hu et.al.|[2501.05423](http://arxiv.org/abs/2501.05423)|null|在危机期间研究公众情绪对于理解观点和情感如何转变、导致社会两极分化至关重要。我们以微博——中国最受欢迎的微博客网站为例，研究在COVID-19疫情爆发期间发布的帖子。研究时段包括COVID-19疫情前阶段、爆发阶段和疫情预防早期阶段。我们使用Llama 3 8B这个大型语言模型，通过将用户情感分类为积极、消极、讽刺和中立等类别来分析平台上的用户情感。分析微博上的情感转变有助于了解社会事件和政府行动如何影响公众舆论。本研究有助于理解健康危机期间社会情感的变化动态，填补了针对中国平台情感分析的空白。通过考察这些动态，我们旨在提供有价值的观点，探讨数字通信在塑造社会应对前所未有的全球挑战中所起的作用。|\n",
        "2501.05396": "|**2025-01-09**|**FairCode: Evaluating Social Bias of LLMs in Code Generation**|Yongkang Du et.al.|[2501.05396](http://arxiv.org/abs/2501.05396)|**[link](https://github.com/yongkdu/faircode)**|**大型语言模型（LLMs）在代码生成方面展现出显著的能力，引起了对其输出质量和安全性的评估越来越多的关注。然而，关于代码生成中偏差的研究仍然有限。现有研究通常通过应用恶意提示或重新应用任务和数据集来评估歧视性模型的偏差。鉴于LLMs通常与人类价值观相一致，且先前数据集并未完全优化用于代码相关任务，迫切需要专门为评估代码模型设计的基准。在本研究中，我们引入了FairCode，这是一种用于评估代码生成偏差的新颖基准。FairCode包含两个任务：函数实现和测试用例生成，每个任务通过不同的场景评估社会偏差。此外，我们提出了一种新的指标，FairScore，用于评估模型在此基准上的性能。我们在广泛使用的LLMs上进行了实验，并对结果进行了全面分析。研究发现，所有测试的LLMs都存在偏差。代码可在https://github.com/YongkDu/FairCode上找到。**|\n",
        "2501.05382": "|**2025-01-09**|**Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models**|Kristian G. Barman et.al.|[2501.05382](http://arxiv.org/abs/2501.05382)|null|本文探讨了发展物理特定的大型人工智能模型（我们称之为大型物理模型，LPMs）的想法，并为其提供了一条可能的路线图。这些模型基于如大型语言模型（LLMs）这样的基础模型——在广泛的数据上训练而成——针对物理学研究的需求进行了定制。LPMs可以独立运行或作为集成框架的一部分。该框架可以纳入专业工具，包括用于数学操作的符号推理模块、分析特定实验和模拟数据的框架，以及合成理论和科学文献的机制。我们首先探讨物理界是否应该积极开发和完善专用模型，而不是仅仅依赖商业LLMs。然后，我们概述了LPMs如何通过物理学、计算机科学和科学哲学专家之间的跨学科合作来实现。为了有效地整合这些模型，我们确定了三个关键支柱：开发、评估和哲学反思。开发侧重于构建能够处理物理文本、数学表述和多种物理数据的模型。评估通过测试和基准测试来评估准确性和可靠性。最后，哲学反思包括分析LLMs在物理学中的更广泛影响，包括其产生新的科学理解和可能出现的新的研究合作动态。受粒子物理学实验合作组织结构的启发，我们提出了一个类似的跨学科和协作方法来构建和改进大型物理模型。这条路线图提供了具体目标，定义了实现这些目标的途径，并确定了实现物理特定大规模AI模型必须解决的问题。|\n",
        "2501.05370": "|**2025-01-09**|**Accelerated Diffusion Models via Speculative Sampling**|Valentin De Bortoli et.al.|[2501.05370](http://arxiv.org/abs/2501.05370)|null|推理步骤：  1. 理解摘要内容：摘要介绍了名为“推测采样”的技术，用于加速大型语言模型的推理。该技术使用快速草稿模型生成候选标记，并根据目标模型的分布接受或拒绝这些标记。之前推测采样仅限于离散序列，现在扩展到扩散模型，这些模型通过连续的、向量值马尔可夫链生成样本。  2. 确定翻译重点：翻译时应注意术语的准确性，如“推测采样”、“草稿模型”、“扩散模型”、“马尔可夫链”等。  3. 翻译摘要：  推测采样是一种流行的技术，通过使用快速草稿模型生成候选标记，并根据目标模型的分布接受或拒绝它们来加速大型语言模型的推理。虽然推测采样之前仅限于离散序列，但我们将其扩展到扩散模型，这些模型通过连续的、向量值马尔可夫链生成样本。在这种情况下，目标模型是一个计算成本高但质量高的扩散模型。我们提出了各种草稿策略，包括一种简单而有效的方法，该方法不需要训练草稿模型，并且可以直接应用于任何扩散模型。我们的实验在各种扩散模型上展示了显著的生成速度提升，将函数评估次数减半，同时生成来自目标模型的精确样本。|\n",
        "2501.05336": "|**2025-01-09**|**Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction**|Hantao Lou et.al.|[2501.05336](http://arxiv.org/abs/2501.05336)|**[link](https://github.com/htlou/stream-aligner)**|**随着大型语言模型（LLMs）的快速发展，其在能力上的显著提升也带来了与人类价值观和意图相符合的担忧。当前的对齐策略，包括自适应训练和推理时方法，在这一领域显示出潜力。然而，这些方法仍然难以在各种任务和难度上平衡部署复杂性和能力。在本研究中，我们引入了流式分布诱导对齐器（Stream Aligner），这是一种新颖的对齐范式，它将效率与在整个生成过程中各种任务上的增强性能相结合。Stream Aligner通过使用一个小模型来学习后缀句子的偏好，迭代地纠正上游模型输出的后缀句子，然后使用纠正后的句子替换后续生成中的后缀句子，实现了动态的句子级纠正。与Aligner相比，我们的实验表明，Stream Aligner减少了对外部模型能力的依赖，增强了LLMs的推理能力，并降低了用户交互时的延迟。具体来说，Stream Aligner-2B模型在测试的Llama2-70B-chat模型上实现了76.1%的有用性提升、36.0%的无害性提升，而Stream Aligner-8B在测试的Llama3-70B-Instruct模型上的数学能力提升了3.5%。**|\n",
        "2501.05322": "|**2025-01-09**|**\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining the Actions of Autonomous Vehicles**|Xuewen Luo et.al.|[2501.05322](http://arxiv.org/abs/2501.05322)|null|随着公众对自动驾驶汽车的信任度下降，研究强调了有必要向乘客解释这些车辆的行为以促进对自主系统的信任。解释者可以通过提高透明度和降低感知风险来增强信任。然而，现有的解决方案往往缺乏以人为本的多模态解释集成方法。本文介绍了一种新颖的人中心多模态解释器（HMI）系统，该系统利用人类偏好提供视觉、文本和听觉反馈。该系统结合了带有鸟瞰图（BEV）、地图和文本显示的视觉界面，以及使用微调的大语言模型（LLM）进行语音交互。我们的用户研究，涉及多样化的参与者，表明HMI系统显著提高了乘客对自动驾驶汽车的信任度，平均信任水平提高了超过8%，普通环境中的信任度甚至上升了高达30%。这些结果突显了HMI系统通过提供清晰、实时且具有情境敏感性的车辆行为解释，提高自动驾驶汽车的可接受性和可靠性的潜力。|\n",
        "2501.05255": "|**2025-01-09**|**CallNavi: A Study and Challenge on Function Calling Routing and Invocation in Large Language Models**|Yewei Song et.al.|[2501.05255](http://arxiv.org/abs/2501.05255)|null|通过聊天机器人与软件系统交互可能会遇到挑战，尤其是当聊天机器人需要按照正确的顺序和参数生成API调用以与系统通信时。在聊天机器人系统中进行API调用面临重大挑战，尤其是在需要准确选择和执行API的复杂、多步骤任务中。我们在这一领域做出以下三方面的贡献：首先，引入一个旨在评估API函数选择、参数生成和嵌套API调用的新数据集；其次，对最先进的语言模型在不同复杂程度下进行基准测试，以评估其在API函数生成和参数准确性方面的性能；第三，提出一种增强的API路由方法，该方法结合了通用的大语言模型进行API选择，以及针对参数生成和某些提示工程方法的微调模型。这些方法在处理复杂API任务方面取得了实质性改进，为现实世界的API驱动聊天机器人系统提供了实用的进步。|\n",
        "2501.06186": "|**2025-01-10**|**LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs**|Omkar Thawakar et.al.|[2501.06186](http://arxiv.org/abs/2501.06186)|**[link](https://github.com/mbzuai-oryx/llamav-o1)**|**推理是解决复杂多步问题的关键能力，尤其是在视觉环境中，顺序逐步理解至关重要。现有方法缺乏评估视觉推理的全面框架，并且不强调逐步解决问题。为此，我们通过三个关键贡献提出一个全面框架，以推进大型语言模型（LMMs）中的逐步视觉推理。首先，我们引入一个专门设计的视觉推理基准，用于评估多步推理任务。该基准提供了从复杂视觉感知到科学推理的八个不同类别，总共有超过4k个推理步骤，能够对LMMs在多个步骤中执行准确和可解释的视觉推理能力进行稳健评估。其次，我们提出一个新颖的指标，用于评估单个步骤的视觉推理质量，强调正确性和逻辑一致性。与传统的最终任务准确率指标相比，该指标对推理性能的洞察更深。第三，我们提出一个新的多模态视觉推理模型，名为LlamaV-o1，采用多步课程学习方法进行训练，任务逐步组织以促进增量技能获取和问题解决。所提出的LlamaV-o1旨在进行多步推理，并通过结构化训练范式逐步学习。大量实验表明，我们的LlamaV-o1优于现有的开源模型，并在与闭源专有模型相比时表现良好。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中平均得分达到67.3，绝对增益为3.8%，同时在推理扩展时快5倍。我们的基准、模型和代码均公开可用。**|\n",
        "2501.06184": "|**2025-01-10**|**PEACE: Empowering Geologic Map Holistic Understanding with MLLMs**|Yangyu Huang et.al.|[2501.06184](http://arxiv.org/abs/2501.06184)|null|地质图作为地质科学的基本图表，为地球地下和地表的结构和组成提供了关键见解。这些地图在各种领域，包括灾害检测、资源勘探和土木工程中，都是不可或缺的。尽管它们的重要性不言而喻，但当前的多元模态大型语言模型（MLLMs）在地质图理解方面往往存在不足。这种差距主要源于地图概括化的挑战性，这包括处理高分辨率地图、管理多个相关组件以及需要专业知识。为了量化这一差距，我们构建了GeoMap-Bench，这是首个用于评估MLLMs在地质图理解方面的基准，它评估了提取、引用、定位、推理和分析的全面能力。为了弥合这一差距，我们引入了GeoMap-Agent，这是首个专为地质图理解设计的智能体，它具有三个模块：分层信息提取（HIE）、领域知识注入（DKI）和提示增强问答（PEQA）。受人类科学家跨学科合作启发，一个AI专家小组充当顾问，利用多样化的工具库全面分析问题。通过全面实验，GeoMap-Agent在GeoMap-Bench上的总得分达到0.811，显著优于GPT-4o的0.369。我们的工作，通过MLLMs增强地质图全面理解（PEACE），为地质领域高级AI应用铺平了道路，提高了地质调查的效率和准确性。|\n",
        "2501.06143": "|**2025-01-10**|**Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories**|Gerd Kortemeyer et.al.|[2501.06143](http://arxiv.org/abs/2501.06143)|null|我们研究了一个基于大型语言模型的AI系统GPT-4o在多个语言和学科领域广泛分布的物理概念题库上的多语言和多模态性能。我们从PhysPort网站获取的题库涵盖了经典物理学主题，如力学、电磁学、光学和热力学，以及相对论、量子力学、天文学、数学和实验技能。与之前仅限于文本的研究不同，我们将题库上传为图片，以模拟学生在纸上看到的情景，评估系统的多模态功能。AI系统用英语提问，并自主选择其回答的语言——要么保持测试的官方语言，要么完全切换到英语，或者混合使用语言——这揭示了其适应语言复杂性和数据可用性的行为。我们的结果表明，各学科领域的性能存在一些差异，其中实验技能领域表现最差。此外，AI在需要视觉解释图像的问题上的表现不如纯文本问题。AI难以回答的问题往往总是与题库语言有关。我们还发现，不同语言之间的性能存在较大差异，一些语言似乎从语言切换中受益很大，这种现象类似于人类说话者的代码切换。总体而言，将所获得的AI结果与现有文献进行比较，我们发现该AI系统在所有学科领域（但实验技能除外）的测试后均优于普通本科生。|\n",
        "2501.06137": "|**2025-01-10**|**Supervision policies can shape long-term risk management in general-purpose AI models**|Manuel Cebrian et.al.|[2501.06137](http://arxiv.org/abs/2501.06137)|**[link](https://github.com/manuelcebrianramos/llm_supervision_policies)**|**随着通用人工智能（GPAI）模型，包括大型语言模型（LLMs）的快速扩散和部署，为人工智能监管机构带来了前所未有的挑战。我们假设这些机构将需要在一个新兴的风险和事件报告生态系统中进行导航，其规模可能超出它们的监管能力。为了研究这一问题，我们开发了一个模拟框架，该框架由从风险、事件或危害报告生态系统（包括社区驱动的平台、众包倡议和专家评估）中提取的特征进行参数化。我们评估了四种监管政策：非优先级（先到先得）、随机选择、基于优先级（首先解决最高优先级的风险）和多样性优先级（平衡高风险与全面覆盖风险类型）。我们的结果表明，虽然基于优先级和多样性优先级的政策在减轻高风险方面更为有效，尤其是那些由专家识别出的风险，但它们可能会无意中忽视更广泛社区报告的系统性问题。这种疏忽可能导致某些类型报告的反馈循环被放大，同时抑制其他类型的报告，从而扭曲整体风险景观的感知。我们通过几个现实世界的数据集验证了我们的模拟结果，包括一个包含超过一百万次ChatGPT交互的数据集，其中超过15万个对话被识别为有风险。这种验证强调了人工智能风险监管中固有的复杂权衡，并突出了风险管理政策选择如何塑造社会中使用各种GPAI模型的人工智能风险未来景观。**|\n",
        "2501.06129": "|**2025-01-10**|**Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational AI**|Yuya Asano et.al.|[2501.06129](http://arxiv.org/abs/2501.06129)|null|通用型自动语音识别（ASR）系统在目标导向对话中并不总是表现良好。现有的ASR纠错方法依赖于先前用户数据或命名实体。我们将纠错扩展到没有先前用户数据且具有语言灵活性（如词汇和句法变化）的任务。我们提出了一种新颖的上下文增强方法，结合大型语言模型和一种排名策略，该策略结合了目标导向对话AI及其任务的对话状态中的上下文信息。我们的方法通过以下两方面进行排名：（1）根据与上下文的词汇和语义相似性对n-best ASR假设进行排名；（2）根据与ASR假设的音位对应关系对上下文进行排名。该方法在家居改善和烹饪领域与真实用户进行了评估，与纠错方法相比，我们的方法将召回率和F1值分别提高了34%和16%，同时保持了精确率和误报率。当我们的纠错方法正确工作时，用户对其的评价从0.8到1分（满分5分）提高，没有因为误报而降低。|\n",
        "2501.06117": "|**2025-01-10**|**Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding**|Fabian David Schmidt et.al.|[2501.06117](http://arxiv.org/abs/2501.06117)|**[link](https://github.com/fdschmidt93/fleurs-slu)**|尽管最近的多语言自动语音识别模型声称支持数千种语言，但由于双模态语音和文本训练数据有限，低资源语言的语音识别仍然非常不可靠。更好的多语言口语理解（SLU）可以通过利用语言语义来补偿稀缺的训练数据，如通过上下文消除歧义或利用跨语言的语义相似性，从而大大增强多语言语音识别的鲁棒性。更重要的是，SLU对于大约一半没有正式书写系统的活语言的无障碍语音技术来说是必不可少的。然而，多语言SLU的评估仍然局限于像意图分类或语言识别这样的浅层任务。为了解决这个问题，我们提出了Fleurs-SLU，这是一个多语言SLU基准，包括102种语言的专题语音分类和92种语言的听力理解选择题回答。我们在Fleurs-SLU上广泛评估了端到端的语音分类模型以及将语音转文本转录与后续的大语言模型分类相结合的级联系统。我们的结果表明，级联系统在多语言SLU任务中表现出更大的鲁棒性，尽管当适当预训练时，语音编码器在专题语音分类中可以实现有竞争力的性能。我们还发现，鲁棒的多语言语音识别、有效的语音转文本翻译和强大的多语言SLU之间存在强烈的关联，突显了声学和语义语音表示之间的相互益处。|\n",
        "2501.06101": "|**2025-01-10**|**From Conversation to Automation: Leveraging Large Language Models to Analyze Strategies in Problem Solving Therapy**|Elham Aghakhani et.al.|[2501.06101](http://arxiv.org/abs/2501.06101)|null|问题解决疗法（PST）是一种结构化的心理治疗方法，通过引导个体识别问题、进行解决方案头脑风暴、决策和结果评估来帮助他们管理压力和解决个人问题。随着心理健康护理越来越多地整合如聊天机器人和大语言模型（LLM）等科技，了解如何有效地自动化PST变得至关重要。本研究利用匿名化治疗记录，利用各种LLM和基于Transformer的模型分析和分类治疗干预措施。我们的结果显示，GPT-4o在识别PST策略方面取得了最高的准确率（0.76），超过了其他模型。此外，我们引入了一种新的沟通策略维度，增强了当前的PST框架，为治疗师与患者互动提供了更深入的见解。这项研究展示了LLM自动化复杂治疗对话分析的可能性，为心理健康干预提供了可扩展、高效的工具。我们的标注框架可以增强PST的易用性、有效性和个性化，为治疗师提供实时支持，以更精确、有针对性的干预措施。|\n",
        "2501.05989": "|**2025-01-10**|**Addressing speaker gender bias in large scale speech translation systems**|Shubham Bansal et.al.|[2501.05989](http://arxiv.org/abs/2501.05989)|null|本项研究针对语音翻译（ST）系统中的说话人性别偏见问题，这一问题可能导致冒犯和不准确的翻译。在大规模ST系统中常见的男性偏见通常是通过从机器翻译（MT）系统获得的数据进行训练而持续存在的。我们的方法包括两个关键步骤。首先，我们采用大型语言模型（LLMs）以经济高效的方式根据说话人的性别更正翻译。其次，我们使用更正后的数据微调ST模型，使模型能够直接从音频线索生成针对特定性别的翻译，无需明确的性别输入。此外，我们还提出了一种适用于说话人性别预先定义或不应从语音线索中推断的三个模式的微调模型。在MuST-SHE测试集上，与基线以及其他大规模ST系统（如Seamless M4T和Canary）相比，我们的方法在女性说话人的翻译上提高了70%。|\n",
        "2501.05985": "|**2025-01-10**|**Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys**|Divya Mani Adhikari et.al.|[2501.05985](http://arxiv.org/abs/2501.05985)|null|设计适合ICTD研究的文化相关问卷具有挑战性，尤其是在将调查适应非西方背景的人群时。先前的工作通过专家评审和试点研究来调整问卷，这些方法既资源密集又耗时。为了解决这些挑战，我们提出使用大型语言模型（LLMs）来自动化跨文化环境下的问卷预测试过程。我们的研究使用LLMs将一个以美国为中心的气候意见调查调整为针对南非受众。然后，我们通过Prolific对116名南非参与者进行了测试，让他们对两个版本都提供反馈。参与者认为LLM调整的问题略优于传统版本。我们的研究开启了关于LLMs在调整调查和促进跨文化问卷设计中的潜在作用的讨论。|\n",
        "2501.05981": "|**2025-01-10**|**Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study of LLM Hallucination on North Korea**|Eunjung Cho et.al.|[2501.05981](http://arxiv.org/abs/2501.05981)|null|在大型语言模型（LLMs）中出现的幻觉问题仍然是对其安全部署的一个重大挑战，尤其是由于其可能导致虚假信息的传播。大多数现有的解决方案通过聚焦于使模型与可信来源对齐或改进模型在输出中表达自信（或缺乏自信）的方式来解决这一挑战。尽管这些措施在大多数情况下可能有效，但在需要更细致方法的场景中可能不足，尤其是在获取准确数据有限或确定可信来源具有挑战性的情况下。在本研究中，我们将朝鲜——一个以极端缺乏可靠来源和耸人听闻的虚假信息普遍存在的国家——作为一个案例研究。我们探索和评估了表现最佳的一些多语言LLMs和特定语言模型在三个具有重大地缘政治利益的国家使用的语言中生成关于朝鲜的信息：英语（美国、英国）、韩语（韩国）和普通话（中国）。我们的发现揭示了显著的差异，表明模型和语言的选择可能导致对朝鲜的截然不同的理解，这对于该国对全球安全构成的挑战具有重要意义。|\n",
        "2501.07542": "|**2025-01-13**|**Imagine while Reasoning in Space: Multimodal Visualization-of-Thought**|Chengzu Li et.al.|[2501.07542](http://arxiv.org/abs/2501.07542)|null|思维链（CoT）提示已被证明在增强大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的复杂推理中非常有效。然而，它在复杂的空间推理任务中遇到了困难。尽管如此，人类的认知超越了语言本身，使其能够以文字和图像两种方式思考。受此启发，我们提出了一种新的推理范式，即多模态可视化思维（MVoT）。它通过生成推理痕迹的图像可视化，使MLLMs能够进行视觉思考。为确保高质量的可视化，我们引入了标记差异损失到自回归MLLMs中。这一创新显著提高了视觉的连贯性和准确性。我们通过几个动态空间推理任务验证了这种方法。实验结果表明，MVoT在各项任务中表现出竞争性的性能。此外，它在CoT失败的最具挑战性场景中显示出稳健和可靠的改进。最终，MVoT为复杂推理任务开辟了新的可能性，在这些任务中，视觉思维可以有效地补充语言推理。|\n",
        "2501.07536": "|**2025-01-13**|**ML Mule: Mobile-Driven Context-Aware Collaborative Learning**|Haoxiang Yu et.al.|[2501.07536](http://arxiv.org/abs/2501.07536)|null|人工智能已经融入了日常生活的几乎每个方面，从计算机视觉中的物体检测到用于撰写电子邮件的大型语言模型，再到智能家居中的紧凑型模型。这些机器学习模型针对个人用户，但通常与他们分离，因为它们通常存储和处理在集中的数据中心。这种集中式方法引发了隐私担忧，产生了高昂的基础设施成本，并且难以实现个性化。为了解决这些问题，我们提出了ML Mule，一种利用个人移动设备作为“驮马”的方法，在它们穿越物理空间时训练和传输模型快照，并将这些模型与它们所占据的物理“空间”共享。这种方法在设备之间隐式形成关联群体，这些设备与共享特定空间的用户相关联，从而实现协作模型进化，并保护用户隐私。我们的方法解决了传统、联邦和完全去中心化学习系统的一些主要不足。所提出的框架代表了一类新的机器学习方法，它们更稳健、分布式和个性化，使该领域更接近实现智能、自适应和真正情境感知的智能环境原始愿景。结果表明，与现有方法相比，ML Mule收敛速度更快，模型精度更高。|\n",
        "2501.07532": "|**2025-01-13**|**Investigating Large Language Models in Inferring Personality Traits from User Conversations**|Jianfeng Zhu et.al.|[2501.07532](http://arxiv.org/abs/2501.07532)|null|大型语言模型（LLMs）在多个领域展现出了与人类相似的能力，包括心理评估。本研究评估了LLMs，特别是GPT-4o和GPT-4o mini，在零样本提示条件下是否能够推断五大人格特质并生成五大人格问卷-10（BFI-10）项目得分。我们的发现表明，在计算特质之前引入一个中间步骤——提示BFI-10项目得分——可以提高准确性，并且比直接推断特质更接近黄金标准。这种结构化方法强调了利用心理框架提高预测精度的意义。此外，基于抑郁症状存在的群体比较揭示了模型性能的差异。参与者被分为两组：至少有一种抑郁症状的参与者和无症状的参与者。GPT-4o mini在症状存在组中对抑郁相关特质（如神经质和尽责性）的变化表现出更高的敏感性，而GPT-4o在跨组的细微解释上展现出优势。这些发现强调了LLMs有效分析现实世界心理数据的潜力，为人工智能与心理学交叉领域的研究提供了宝贵的基石。|\n",
        "2501.07525": "|**2025-01-13**|**RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**|Difei Gu et.al.|[2501.07525](http://arxiv.org/abs/2501.07525)|**[link](https://github.com/difeigu/radalign)**|**自动胸部X光片解读需要准确的地病分类和详细的放射学报告生成，这在临床工作中是一个重大挑战。当前的方法要么侧重于分类精度而牺牲可解释性，要么通过图像字幕技术生成详细但可能不可靠的报告。在这项研究中，我们提出了RadAlign，一个结合了视觉语言模型（VLM）的预测精度和大型语言模型（LLM）推理能力的创新框架。受放射科医生工作流程的启发，RadAlign首先使用一个专门的VLM将视觉特征与关键医学概念对齐，实现了平均AUC为0.885的优越疾病分类。这些识别出的医学状况，以对齐的视觉-语言空间中的基于文本的概念表示，然后被用于基于LLM的报告生成。通过检索增强的生成机制，将输出基于类似的历史案例，RadAlign提供了具有0.678的GREEN评分的优越报告质量，优于最先进方法的0.634。我们的框架在保持强大的临床可解释性的同时减少了幻觉，通过集成的预测和生成AI推进了自动医学影像和报告分析。代码可在https://github.com/difeigu/RadAlign获取。**|\n",
        "2501.07523": "|**2025-01-13**|**Parallel Key-Value Cache Fusion for Position Invariant RAG**|Philhoon Oh et.al.|[2501.07523](http://arxiv.org/abs/2501.07523)|null|近期大型语言模型（LLMs）的进展凸显了检索增强生成（RAG）利用外部信息的重要性。然而，LLMs对相关信息在上下文中的位置敏感，当此类信息置于中间位置时，容易产生错误的响应，这种现象被称为“迷失在中间”现象。在本文中，我们提出一个框架，该框架为仅解码器模型生成一致的输出，无论输入上下文顺序如何。三个开放域问答任务的实验结果表明，该模型对输入上下文顺序不敏感，即位置不变性，并且与现有的RAG管道方法相比，对无关段落具有更强的鲁棒性。|\n",
        "2501.07493": "|**2025-01-13**|**Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards**|Yangsibo Huang et.al.|[2501.07493](http://arxiv.org/abs/2501.07493)|null|现在，人们通常通过让人类手动投票来评估大型语言模型（LLMs）的输出，这与评估特定任务中的知识或技能的典型基准不同。聊天机器人竞技场是这类基准中最受欢迎的，它通过让用户在两个随机选择的模型之间选择更好的回应来对模型进行排名（而不透露哪个模型负责生成）。这些平台被广泛信赖为衡量LLMs能力公平且准确的手段。在本文中，我们表明，如果没有实施机器人保护和其它防御措施，基于投票的基准可能容易受到对抗性操纵。具体来说，我们展示了攻击者可以通过花费大约一千票（在模拟的、离线的聊天机器人竞技场版本中验证）来改变排行榜（以推广他们喜欢的模型或降低竞争对手的排名）。我们的攻击分为两个步骤：首先，我们展示了攻击者如何以超过95%的准确率确定用于生成特定回复的模型；然后，攻击者可以利用这些信息持续地对目标模型进行投票（或反对）。与聊天机器人竞技场开发者合作，我们识别、提出并实施了缓解措施，以增强聊天机器人竞技场对对抗性操纵的鲁棒性，根据我们的分析，这大幅增加了此类攻击的成本。其中一些防御措施在我们合作之前就已经存在，例如使用Cloudflare的机器人保护、恶意用户检测和速率限制。其他一些措施，包括reCAPTCHA和登录，正在被集成到聊天机器人竞技场中以加强其安全性。|\n",
        "2501.07482": "|**2025-01-13**|**TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language Models**|Thales Sales Almeida et.al.|[2501.07482](http://arxiv.org/abs/2501.07482)|null|在快速发展的知识景观和大型语言模型的日益普及的背景下，出现了一种需求，即持续更新这些模型以反映当前事件。虽然现有的基准评估了普遍的事实回忆能力，但它们通常忽略了两个关键方面：模型通过持续学习整合演变知识的能力以及它们在性能上的显著区域差异。为了解决这些差距，我们引入了及时事件基准（TiEBe），这是一个包含超过11,000个问题-答案对的数据库，专注于全球和区域重要事件。TiEBe利用来自维基百科的结构化回溯数据，能够进行持续更新，以评估大型语言模型对演变全球事务的知识以及它们对不同地区事件的理解。我们的基准表明，大型语言模型在事实回忆方面表现出显著的地理差异，强调了需要更加平衡的全球知识表示。此外，TiEBe作为评估持续学习策略的工具，提供了关于模型获取新信息而不忘记过去知识的能力的见解。|\n",
        "2501.07468": "|**2025-01-13**|**A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**|Yihao Liu et.al.|[2501.07468](http://arxiv.org/abs/2501.07468)|null|全球医疗体系在效率、可及性和个性化方面面临着持续的挑战。依托现代人工智能技术，如多模态大型语言模型和世界模型，具身人工智能（EmAI）代表了一个变革性的前沿领域，它提供了增强自主性和与物理世界互动的能力，以应对这些挑战。作为一个跨学科且快速发展的研究领域，“医疗领域的EmAI”涵盖了算法、机器人技术和生物医学等多个领域。这种复杂性强调了及时回顾和分析的重要性，以跟踪进展、解决挑战并促进跨学科合作。在本文中，我们提供了EmAI在医疗领域“大脑”的全面概述，其中我们介绍了感知、行动、规划和记忆的基础AI算法，并重点介绍了跨越临床干预、日常护理与陪伴、基础设施支持和生物医学研究等医疗应用。尽管具有巨大潜力，但EmAI在医疗领域的开发受到诸如安全担忧、模拟平台与实际应用之间的差距、缺乏标准化基准以及在跨学科领域进展不均等关键挑战的阻碍。我们讨论了技术障碍并探讨了伦理考量，提出了对未来EmAI在医疗领域发展的前瞻性观点。还介绍了一个EmAI系统智能层次结构的框架，以指导进一步的发展。通过提供系统性的见解，本研究旨在激发创新和实际应用，为智能、以患者为中心的医疗新时代铺平道路。|\n",
        "2501.07458": "|**2025-01-13**|**Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is Not AGI**|Rolf Pfister et.al.|[2501.07458](http://arxiv.org/abs/2501.07458)|null|基于对ARC-AGI的区分，即由ARC-AGI的创造者François Chollet提出的技能与智能的区别，引入了一种对智能的新理解：一个智能体越能以更少的知识在更多样化的世界中更有效地实现更多样化的目标，那么它就越智能。对ARC-AGI基准的分析显示，其任务代表了一种非常特定的问题类型，这种问题可以通过大量尝试预定义操作的组合来解决。这种方法也被o3所应用，通过广泛使用计算能力实现了高分。然而，对于物理世界和人类领域中的大多数问题，解决方案无法预先测试，且预定义的操作不可用。因此，像o3那样进行大量预定义操作的尝试不能作为AGI的基础——相反，需要新的方法，这些方法可以可靠地解决广泛的各种问题，而无需现有技能。为了支持这种发展，概述了一个新的智能基准，它涵盖了要解决的未知任务的更高多样性，从而能够全面评估智能以及向通用人工智能（AGI）的进展。|\n",
        "2501.07425": "|**2025-01-13**|**Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection**|Xin Yin et.al.|[2501.07425](http://arxiv.org/abs/2501.07425)|null|尽管许多基于学习的单元测试生成方法已经提出并取得了显著性能，但它们仍然依赖于特定任务的数据库，存在局限性。最近，由于能够处理包括单元测试生成在内的广泛任务，由提示工程引导的大型语言模型（LLMs）受到了关注。尽管它们取得了成功，但LLMs在为焦点方法或函数生成单元测试时可能会出现幻觉，因为它们缺乏对项目全局上下文的了解。这些幻觉可能表现为调用不存在的方法，以及错误的参数或返回值，例如参数类型或数量不匹配。尽管许多研究已经探讨了上下文的作用，但它们通常为不同的模型和焦点方法提取固定的上下文模式，这可能不适合所有生成过程（例如，过多的无关上下文可能导致冗余，阻止模型关注关键信息）。为了克服这一局限性，我们提出了RATester，它通过注入全局上下文信息来增强LLM生成更具有仓库意识的单元测试的能力。为了使LLMs具备类似于人类测试者的全局知识，我们集成了语言服务器gopls，它提供必要的功能（例如，查找定义）以协助LLM。当RATester遇到不熟悉的标识符（例如，不熟悉的结构体名称）时，它首先利用gopls获取相关定义和文档注释，然后使用这些全局知识来引导LLM。通过利用gopls，RATester丰富了LLM对项目全局上下文的知识，从而在单元测试生成过程中减少了幻觉。|\n",
        "2501.08328": "|**2025-01-14**|**PokerBench: Training Large Language Models to become Professional Poker Players**|Richard Zhuang et.al.|[2501.08328](http://arxiv.org/abs/2501.08328)|**[link](https://github.com/pokerllm/pokerbench)**|**我们介绍了PokerBench——一个用于评估大型语言模型（LLM）打扑克能力的基准。随着LLM在传统NLP任务上的出色表现，它们在复杂策略游戏如扑克中的应用带来了新的挑战。扑克是一种不完整信息游戏，需要多种技能，如数学、推理、计划、策略，以及对博弈论和人类心理的深入了解。这使得扑克成为大型语言模型的理想下一个前沿领域。PokerBench由11,000个最重要的场景组成，分为开牌和翻牌阶段，并与经验丰富的扑克玩家合作开发。我们评估了包括GPT-4、ChatGPT 3.5以及各种Llama和Gemma系列模型在内的显要模型，发现所有最先进的LLM在打最优扑克方面表现不佳。然而，经过微调后，这些模型显示出明显的改进。我们通过让不同分数的模型相互竞争来验证PokerBench，结果表明在PokerBench上的高分对应着实际扑克游戏中的更高胜率。通过我们微调后的模型与GPT-4之间的游戏，我们还识别了简单监督微调在学习最优游戏策略方面的局限性，这表明需要更高级的方法来有效地训练语言模型以在游戏中表现出色。因此，PokerBench为LLM的扑克打能力提供了一个独特的快速可靠评估基准，以及一个全面基准来研究LLM在复杂游戏场景中的进展。数据集和代码将在以下网址提供：\\url{https://github.com/pokerllm/pokerbench}。**|\n",
        "2501.08326": "|**2025-01-14**|**Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks**|Miran Heo et.al.|[2501.08326](http://arxiv.org/abs/2501.08326)|null|我们提出了Omni-RGPT，这是一种多模态大型语言模型，旨在促进图像和视频的区域级理解。为了在时空维度上实现一致的区域表示，我们引入了Token Mark，一套突出视觉特征空间中目标区域的标记。这些标记通过区域提示（例如，框或掩码）直接嵌入到空间区域中，并同时纳入文本提示以指定目标，从而在视觉和文本标记之间建立直接联系。为了进一步支持无需tracklets的鲁棒视频理解，我们引入了一个辅助任务，该任务利用标记的一致性指导Token Mark，使视频中的区域解释保持稳定。此外，我们还引入了一个大规模的区域级视频指令数据集（RegVID-300k）。Omni-RGPT在基于图像和视频的常识推理基准测试中取得了最先进的成果，同时在字幕和指代表达理解任务中表现出色。|\n",
        "2501.08324": "|**2025-01-14**|**ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations**|Ziyuan Huang et.al.|[2501.08324](http://arxiv.org/abs/2501.08324)|null|阿尔茨海默病分析模型生成1（ADAM-1）是一个多智能体大型语言模型（LLM）框架，旨在整合和分析多模态数据，包括微生物组特征、临床数据集和外部知识库，以增强对阿尔茨海默病（AD）的理解和检测。通过利用检索增强生成（RAG）技术以及其多智能体架构，ADAM-1综合来自不同数据源的观点，并利用文献驱动的证据对发现进行语境化。与XGBoost的比较评估显示，ADAM-1的平均F1分数与XGBoost相似，但方差显著降低，突显了其鲁棒性和一致性，特别是在小型实验室数据集中。虽然目前针对二元分类任务进行优化，未来的迭代旨在纳入额外的数据模态，如神经影像学和生物标志物，以拓宽其在阿尔茨海默病研究和诊断中的可扩展性和适用性。|\n",
        "2501.08322": "|**2025-01-14**|**Exploring Robustness of Multilingual LLMs on Real-World Noisy Data**|Amirhossein Aliakbarzadeh et.al.|[2501.08322](http://arxiv.org/abs/2501.08322)|**[link](https://github.com/caisa-lab/llms-real-world-noise-robustness)**|**大型语言模型（LLMs）在可能包含人类造成的拼写错误的网络数据上进行了训练。但它们是否对类似的现实世界噪声具有鲁棒性？在本文中，我们研究了现实世界拼写错误对9个语言模型性能的影响，这些模型的参数从0.2B到13B不等，在3个不同的自然语言处理（NLP）任务上，即自然语言推理（NLI）、命名实体识别（NER）和意图分类（IC）上。我们在6种不同的语言上进行了实验，并使用维基百科编辑历史为它们构建了一个现实世界噪声词典。我们发现，在所有数据集和语言的平均值中，所研究模型在干净和噪声测试数据上的性能差距从2.3到4.3个百分点不等。此外，与BLOOM、Falcon和Bert-like模型相比，mT5模型通常表现出更强的鲁棒性。特别是，mT5（13B）在平均意义上在3个任务和6种语言中的4种上表现最为鲁棒。**|\n",
        "2501.08319": "|**2025-01-14**|**Enhancing Automated Interpretability with Output-Centric Feature Descriptions**|Yoav Gur-Arieh et.al.|[2501.08319](http://arxiv.org/abs/2501.08319)|**[link](https://github.com/yoavgur/feature-descriptions)**|**自动可解释性管道为大型语言模型（LLMs）中由特征表示的概念生成自然语言描述，例如植物或句子中的第一个单词。这些描述是通过激活特征的输入得到的，这些输入可能是模型表示空间中的某个维度或方向。然而，识别激活输入是代价高昂的，特征在模型行为中的机制作用既取决于输入如何使特征激活，也取决于特征激活如何影响输出。通过使用引导评估，我们揭示了当前管道提供的描述未能捕捉到特征对输出的因果效应。为了解决这个问题，我们提出了高效、以输出为中心的方法来自动生成特征描述。这些方法使用特征刺激后权重更高的标记，或者直接将词汇“反嵌入”头应用于特征后的最高权重标记。我们的以输出为中心的描述比以输入为中心的描述更好地捕捉了特征对模型输出的因果效应，但结合两者在输入和输出评估上都取得了最佳性能。最后，我们表明，以输出为中心的描述可用于找到先前被认为是“失效”的特征的激活输入。**|\n",
        "2501.08292": "|**2025-01-14**|**HALoGEN: Fantastic LLM Hallucinations and Where to Find Them**|Abhilasha Ravichander et.al.|[2501.08292](http://arxiv.org/abs/2501.08292)|null|尽管生成式大型语言模型（LLMs）在生成高质量、流畅文本方面表现出色，但它们也产生了幻觉：与既定世界知识或提供输入背景不符的陈述。然而，测量幻觉具有挑战性，因为让人类即时验证模型生成内容既昂贵又耗时。在这项工作中，我们发布了HALoGEN，这是一个全面的幻觉基准，包括：（1）涵盖编程、科学归属和摘要等九个领域的10923个用于生成模型的提示；（2）针对每个用例的自动高精度验证器，将LLM生成内容分解为原子单元，并针对高质量知识源验证每个单元。我们使用这个框架评估了14个语言模型的大约150,000个生成内容，发现即使是表现最好的模型也充满了幻觉（有时根据领域不同，生成的原子事实中高达86%是幻觉）。我们进一步基于幻觉是否可能源自对训练数据的错误回忆（A类错误）、训练数据中的错误知识（B类错误）或伪造（C类错误）来定义了一种新型LLM幻觉错误分类。我们希望我们的框架为研究为什么生成模型会产生幻觉提供基础，并推进可信赖的大型语言模型的发展。|\n",
        "2501.08282": "|**2025-01-14**|**LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding**|Hongyu Li et.al.|[2501.08282](http://arxiv.org/abs/2501.08282)|**[link](https://github.com/appletea233/llava-st)**|**近期在多模态大型语言模型（MLLMs）方面的进步显示出很有希望的结果，但现有方法在同时有效处理时间和空间定位方面仍存在困难。这一挑战源于两个关键问题：首先，引入时空定位会带来大量的坐标组合，这使得语言和视觉坐标表示的对齐变得复杂；其次，在视频特征压缩期间编码细粒度的时间和空间信息本质上是困难的。为了解决这些问题，我们提出了LLaVA-ST，这是一种用于细粒度时空多模态理解的MLLM。在LLaVA-ST中，我们提出了语言对齐位置嵌入，它将文本坐标特殊标记嵌入到视觉空间中，简化了细粒度时空对应的对齐。此外，我们设计了时空打包器，它将时间和空间分辨率的特征压缩解耦为两个独立的点对区域注意力处理流。此外，我们提出了包含430万个训练样本的ST-Align数据集，用于细粒度时空多模态理解。通过ST-align，我们提出了一个渐进式训练流程，通过序列的粗到细阶段对齐视觉和文本特征。此外，我们引入了一个ST-Align基准，用于评估时空交织的细粒度理解任务，包括时空视频定位（STVG）、事件定位和描述（ELC）以及空间视频定位（SVG）。LLaVA-ST在需要细粒度时间、空间或时空交织多模态理解的11个基准上取得了出色的性能。我们的代码、数据和基准将发布在https://github.com/appletea233/LLaVA-ST。**|\n",
        "2501.08276": "|**2025-01-14**|**Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing**|Pulkit Arora et.al.|[2501.08276](http://arxiv.org/abs/2501.08276)|null|大型语言模型（LLMs）在各种自然语言处理任务中表现出令人印象深刻的性能。然而，人们对其在不同语言变体领域的可靠性表示担忧。许多研究提出了针对局部对抗攻击的鲁棒性评估措施，但我们需要不受不同语言风格偏见影响的全球鲁棒模型。我们采取更广泛的方法，探索跨社会人口统计维度的更广泛变化，以对语言模型的推理能力进行结构化可靠性测试。我们将SocialIQA数据集扩展，创建基于社会人口统计风格的多样化释义集。评估旨在更深入地了解LLMs在以下方面的能力：（a）使用工程提示生成人口统计释义的能力；（b）在现实世界、复杂语言场景中的推理能力。我们还探索了诸如困惑度、可解释性和释义的ATOMIC性能等指标，以对这些集合进行细粒度的LLMs可靠性分析。我们发现，针对特定人群的释义对语言模型的性能有显著影响，这表明语言变化的微妙之处仍然是一个重大挑战。代码和数据集将公开发布，以供重复研究和未来研究使用。|\n",
        "2501.08262": "|**2025-01-14**|**Addressing the sustainable AI trilemma: a case study on LLM agents and RAG**|Hui Wu et.al.|[2501.08262](http://arxiv.org/abs/2501.08262)|null|大型语言模型（LLMs）展示了显著的能力，但它们的广泛应用和更高级的应用提出了关键的可持续性挑战，尤其是在推理能耗方面。我们提出了可持续AI三元悖论的概念，突出了AI能力、数字公平和环境可持续性之间的紧张关系。通过系统地研究LLM代理和检索增强生成（RAG），我们分析了嵌入在内存模块设计中的能源成本，并引入了新的指标来量化能源消耗与系统性能之间的权衡。我们的实验结果表明，当前的内存增强框架存在显著的能源效率低下，并证明了资源受限环境面临着不成比例的效率惩罚。我们的发现挑战了目前以LLM为中心的代理设计范式，并为开发更可持续的AI系统提供了实用的见解。|\n",
        "2501.08248": "|**2025-01-14**|**Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models**|Yifu Qiu et.al.|[2501.08248](http://arxiv.org/abs/2501.08248)|null|近期长上下文语言模型（LCLMs）的进步有望通过简化流程来改变检索增强生成（RAG）。凭借其扩展的上下文窗口，LCLMs可以处理整个知识库并直接进行检索和推理——我们将这种能力定义为上下文检索与推理（ICR^2）。然而，现有的基准如LOFT常常通过提供过于简化的上下文来高估LCLM的性能。为了解决这个问题，我们引入了ICR^2，这是一个通过包含使用强大检索器检索的混杂段落来评估LCLM在更现实场景中的基准。随后，我们提出了三种提高LCLM性能的方法：（1）检索后生成微调，（2）检索注意力探查，该方法使用注意力头来在解码过程中过滤和去噪长上下文，（3）与生成头联合训练检索头。我们在LOFT和ICR^2上对五种知名的LCLM进行评估，结果表明，应用我们的最佳方法到Mistral-7B上，与标准RAG和监督微调相比，在LOFT上精确匹配分数提高了17和15分，在ICR^2上分别提高了13和2分。即便是一个规模较小的模型，它在大多数任务上甚至超越了GPT-4-Turbo。|\n",
        "2501.09004": "|**2025-01-15**|**Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails**|Shaona Ghosh et.al.|[2501.09004](http://arxiv.org/abs/2501.09004)|null|随着大型语言模型（LLMs）和生成式人工智能的日益普及，对内容安全性的担忧也在同步增长。目前，针对LLM相关安全风险的全面范围，缺乏高质量、人工标注的数据集，这些数据集可用于商业应用。为了填补这一空白，我们提出了一种全面且可适应的安全风险分类法，该分类法分为12个顶级危害类别，并扩展到9个细粒度子类别。这个分类法旨在满足下游用户的多样化需求，提供更细致和灵活的工具来管理各种风险类型。我们使用一种混合数据生成流程，结合人工标注和多LLM“陪审团”系统来评估响应的安全性，从而获得了Aegis 2.0，这是一组精心挑选的34,248个人工-LLM交互样本集合，按照我们提出的分类法进行标注。为了验证其有效性，我们展示了几个使用参数高效技术训练的轻量级模型，在Aegis 2.0上的性能与在更大、非商业数据集上完全微调的领先安全模型相当。此外，我们引入了一种新的训练混合方法，将安全性与主题跟随数据相结合。这种方法增强了守护模型的适应性，使其能够在推理过程中泛化到新的风险类别。我们计划将Aegis 2.0数据和模型开源给研究社区，以帮助LLMs的安全防护。|\n",
        "2501.08977": "|**2025-01-15**|**Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models**|Emma Croxford et.al.|[2501.08977](http://arxiv.org/abs/2501.08977)|null|随着大型语言模型（LLMs）被整合到电子健康记录（EHR）工作流程中，在实施前对它们进行验证的工具有助于评估其性能。现有的用于评估医生文档质量的工具通常不适用于LLM生成的文本的复杂性，且缺乏对真实世界数据的验证。为了评估LLM生成的临床摘要，开发了医生文档摘要质量工具（PDSQI-9）。使用多个LLMs（GPT-4o、Mixtral 8x7b和Llama 3-8b）从多个专业领域的真实世界EHR数据中生成多文档摘要。验证包括皮尔逊相关系数用于内容效度、因素分析和Cronbach's alpha用于结构效度、评分者间可靠性（ICC和Krippendorff's alpha）用于普遍性、半德尔菲过程用于内容效度，以及高质量与低质量摘要的比较用于区分效度。七位医生评分者评估了779个摘要并回答了8,329个问题，实现了超过80%的评分者间可靠性效力。PDSQI-9显示出强烈的内部一致性（Cronbach's alpha = 0.879；95% CI：0.867-0.891）和高评分者间可靠性（ICC = 0.867；95% CI：0.867-0.868），支持结构效度和普遍性。因素分析确定了4个因素模型，解释了58%的方差，代表了组织、清晰度、准确性和实用性。实质性效度得到了笔记长度与简洁性（rho = -0.200，p = 0.029）和组织性（rho = -0.190，p = 0.037）评分之间的相关性支持。区分效度区分了高质量与低质量摘要（p < 0.001）。PDSQI-9显示出稳健的构念效度，支持其在临床实践中用于评估LLM生成的摘要，并促进LLMs更安全地融入医疗工作流程。|\n",
        "2501.08974": "|**2025-01-15**|**Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models**|Karukriti Kaushik Ghosh et.al.|[2501.08974](http://arxiv.org/abs/2501.08974)|null|基于方面的情感分析（ASBA）是一种精细化的情感分析方法，旨在根据产品、服务或实体的特定方面或特征提取和分类情感。与传统情感分析不同，后者将整体评论或文本分配一个总体情感得分，而ASBA专注于将文本分解为单个组件或方面（例如，质量、价格、服务）并对每个方面进行情感评估。这使客户意见的理解更加细致，使企业能够确定具体的优势和改进领域。该过程包括几个关键步骤，包括方面提取、情感分类以及针对评论段落或其他用户提供的任何形式的方面级情感聚合。ASBA在产品评论、社交媒体监控、客户反馈分析和市场研究等领域具有重大应用。通过利用自然语言处理（NLP）和机器学习技术，ASBA促进了有价值见解的提取，使公司能够做出基于数据的决策，从而提高客户满意度和优化产品。随着ASBA的发展，它有潜力通过提供对各种产品方面的更深入的情感理解，大大改善个性化客户体验。在本研究中，我们分析了LLMs在完全跨领域的基于方面的情感分析中的强度，旨在为某些产品定义框架并将其用于其他类似情况。我们认为，在SemEval-2015任务12的基于方面的情感分析数据集上，可以达到92%的准确率。|\n",
        "2501.08951": "|**2025-01-15**|**Analyzing the Ethical Logic of Six Large Language Models**|W. Russell Neuman et.al.|[2501.08951](http://arxiv.org/abs/2501.08951)|null|本研究考察了六种著名的大型生成语言模型的伦理推理：OpenAI GPT-4o、Meta LLaMA 3.1、Perplexity、Anthropic Claude 3.5 Sonnet、Google Gemini 和 Mistral 7B。研究探讨了这些模型如何阐述和应用伦理逻辑，特别是在应对道德困境，如电车问题和海因茨困境时的表现。不同于传统的对齐研究，本研究采用了可解释性-透明度框架，促使模型解释其伦理推理。这种方法通过三种已建立的伦理类型进行分析：后果主义-义务论分析、道德基础理论和科尔伯格道德发展阶段模型。研究发现，大型语言模型展现出高度一致的伦理逻辑，以理性主义和后果主义为重点，决策往往优先考虑伤害最小化和公平。尽管在预训练和模型架构上存在相似之处，但在伦理推理方面，模型之间出现了细微和显著的差异，这反映了微调和后训练过程中的变化。这些模型始终展现出博学、谨慎和自我意识，其伦理推理类似于道德哲学研究生级别的讨论。令人印象深刻的是，这些系统都宣称自己的伦理推理比典型的人类道德逻辑更复杂。|\n",
        "2501.08946": "|**2025-01-15**|**Applying General Turn-taking Models to Conversational Human-Robot Interaction**|Gabriel Skantze et.al.|[2501.08946](http://arxiv.org/abs/2501.08946)|null|轮流发言是对话的基本要素，但当前的人机交互（HRI）系统通常依赖于简单基于沉默的模型，导致不自然的停顿和中断。本文首次探讨了通用轮流发言模型，特别是TurnGPT和语音活动投影（VAP）的应用，以改善HRI中的对话动态。这些模型使用自监督学习目标在人类-人类对话数据上训练，无需特定领域的微调。我们提出了将这些模型结合使用的方法，以预测机器人何时开始准备回应、轮流发言和处理潜在的干扰。我们在一个受试者内研究中评估了所提出的系统，与传统的基线系统进行了比较，使用了Furhat机器人，与39名成年人在对话环境中进行，并结合了一个大型语言模型进行自主回应生成。结果显示，参与者显著更喜欢所提出的系统，并且它显著减少了回应延迟和中断。|\n",
        "2501.08925": "|**2025-01-15**|**Disentangling Exploration of Large Language Models by Optimal Exploitation**|Tim Grams et.al.|[2501.08925](http://arxiv.org/abs/2501.08925)|null|探索是自我提升和开放式问题解决的关键技能。然而，大型语言模型是否能够有效探索状态空间仍存在疑问。现有的评估主要关注探索和利用之间的权衡，通常在多臂老虎机问题中进行评估。相比之下，这项工作将探索作为唯一目标，要求代理提供增强未来回报的信息。为了评估，我们提出通过测量已探索状态的最优可实现回报来将缺失奖励分解为探索和利用两个部分。我们使用各种大型语言模型进行的实验表明，大多数模型难以充分探索状态空间，而弱探索是不够的。我们观察到模型大小与探索性能之间存在正相关关系，大型模型表现出更优越的能力。此外，我们还表明，我们的分解可以揭示在提示工程中由代理指令驱动的行为差异，为优化探索任务中的LLM性能提供了一个有价值的工具。|\n",
        "2501.08913": "|**2025-01-15**|**GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge**|Liam Dugan et.al.|[2501.08913](http://arxiv.org/abs/2501.08913)|**[link](https://github.com/liamdugan/raid)**|近期，许多共享任务都针对从大型语言模型（LLMs）检测生成的文本。然而，这些共享任务往往侧重于以下两种情况之一：一是文本局限于特定领域；二是文本可以来自许多领域，其中一些在测试时可能看不到。在这个共享任务中，我们使用新发布的RAID基准，旨在回答模型是否能够检测从大量且固定的领域和LLMs生成的文本，这些领域和LLMs在训练过程中都是可见的。在三个月的时间里，9个团队尝试了这个任务，提交了23个检测器。我们发现，多个参与者能够在RAID的机器生成文本上获得超过99%的准确率，同时保持5%的误报率——这表明检测器能够稳健地同时检测来自许多领域和模型的文本。我们讨论了这一结果的可能解释，并为未来的研究提供了方向。|\n",
        "2501.08897": "|**2025-01-15**|**Leveraging Large Language Models as Knowledge-Driven Agents for Reliable Retrosynthesis Planning**|Qinyu Ma et.al.|[2501.08897](http://arxiv.org/abs/2501.08897)|**[link](https://github.com/qinyuma316/retrosynthesisagent)**|在材料化学中识别可靠的合成途径是一项复杂的任务，尤其是在聚合物科学中，这是因为大分子具有复杂且往往非唯一的命名。为了应对这一挑战，我们提出了一种集成大型语言模型（LLMs）和知识图谱（KGs）的智能体系统。通过利用LLMs强大的提取和识别化学物质名称的能力，并将提取的数据存储在结构化的知识图谱中，我们的系统完全自动化了相关文献的检索、反应数据的提取、数据库查询、逆合成途径树的构建，以及通过检索额外文献和推荐最佳反应途径的进一步扩展。一种新颖的多分支反应途径搜索（MBRPS）算法能够探索所有途径，特别关注多分支途径，帮助LLMs克服在多分支路径上的推理弱点。这项工作代表了首次尝试开发一个由LLMs驱动的专门针对大分子的完全自动化的逆合成规划智能体。应用于聚酰亚胺合成，我们的新方法构建了一个包含数百条途径的逆合成途径树，并推荐了优化路线，包括已知和新型途径，证明了其有效性和更广泛应用的潜力。|\n",
        "2501.08774": "|**2025-01-15**|**How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering**|Christoph Treude et.al.|[2501.08774](http://arxiv.org/abs/2501.08774)|null|人工智能（AI），包括大型语言模型和生成式AI，正成为软件开发领域的一股重要力量，为开发者提供了贯穿整个开发生命周期的强大工具。尽管软件工程研究已经广泛研究了AI工具在软件开发中的应用，但开发者与这些AI工具之间的具体交互类型直到最近才开始受到关注。理解和改进这些交互有望提高AI驱动工作流程的生产力、信任和效率。在本文中，我们提出了一种开发者与AI工具之间交互类型的分类法，确定了十一种不同的交互类型，例如自动补全代码建议、命令驱动操作和对话式辅助。基于这一分类法，我们概述了一个研究议程，重点关注优化AI交互、提高开发者控制和解决AI辅助开发中的信任和可用性挑战。通过为研究开发者-AI交互建立一个结构化的基础，本文旨在激发对创建更有效、适应性更强的AI工具进行软件开发的科研活动。|\n",
        "2501.08769": "|**2025-01-15**|**Enhanced Large Language Models for Effective Screening of Depression and Anxiety**|June M. Liu et.al.|[2501.08769](http://arxiv.org/abs/2501.08769)|null|抑郁和焦虑障碍广泛存在，需要及时识别和管理。近期大型语言模型（LLMs）的进步为解决这一问题提供了潜在方案，但高昂的成本和关于训练数据的伦理担忧仍然是挑战。本文介绍了一种合成临床访谈的流程，生成了1,157个交互式对话（PsyInterview），并提出了基于LLM的情绪障碍筛查系统EmoScan。EmoScan能够区分粗略的疾病（例如焦虑或抑郁障碍）和精细的疾病（例如重度抑郁障碍），并进行高质量的访谈。评估结果显示，EmoScan在筛查情绪障碍方面的表现超过了基础模型和其他LLMs（如GPT-4），其F1分数为0.7467。它还提供了优越的解释（BERTScore为0.9408），并在外部数据集上表现出强大的泛化能力（F1分数为0.67）。此外，EmoScan在访谈技巧方面优于基线，这一点通过自动评分和人工评估得到了验证。这项工作强调了可扩展的数据生成管道对于开发有效的心理健康LLM工具的重要性。|\n",
        "2501.09757": "|**2025-01-16**|**Distilling Multi-modal Large Language Models for Autonomous Driving**|Deepti Hegde et.al.|[2501.09757](http://arxiv.org/abs/2501.09757)|null|自动驾驶需要安全的运动规划，尤其是在关键的“长尾”场景中。最近的全端到端自动驾驶系统利用大型语言模型（LLM）作为规划器来提高对罕见事件的泛化能力。然而，在测试时使用LLM引入了高计算成本。为了解决这个问题，我们提出了DiMA，一个端到端自动驾驶系统，它保持了无LLM（或基于视觉）规划器的效率，同时利用了LLM的世界知识。DiMA通过一系列专门设计的代理任务，将来自多模态LLM的信息提炼为基于视觉的端到端规划器。在联合训练策略下，两个网络共有的场景编码器产生结构化的表示，这些表示在语义上是扎根的，并且与最终的规划目标相一致。值得注意的是，LLM在推理时是可选的，这使规划既稳健又不会牺牲效率。使用DiMA进行训练，使得L2轨迹误差减少了37%，基于视觉的规划器的碰撞率降低了80%，以及在长尾场景中的轨迹误差减少了44%。DiMA还在nuScenes规划基准测试中实现了最先进的性能。|\n",
        "2501.09754": "|**2025-01-16**|**Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues**|Youngjoon Jang et.al.|[2501.09754](http://arxiv.org/abs/2501.09754)|null|我们的目标是把连续的手语翻译成口语文本。受人类译员依赖上下文进行准确翻译的启发，我们将额外的上下文线索与手语视频结合到一个新的翻译框架中。具体来说，除了编码输入视频的视觉手语识别特征外，我们还整合了以下互补文本信息：(i)描述背景节目的字幕，(ii)先前句子的翻译，以及(iii)记录手语的伪注释。这些信息被自动提取并与视觉特征一起输入到一个预训练的大型语言模型（LLM）中，我们对该模型进行微调以生成文本形式的口语翻译。通过广泛的消融研究，我们展示了每个输入线索对翻译性能的积极贡献。我们在目前最大的英国手语数据集BOBSL上训练和评估了我们的方法。我们发现，与在BOBSL上之前报道的结果相比，以及与我们作为基线实施的最新方法相比，我们的上下文方法显著提高了翻译质量。此外，我们还通过将其应用于How2Sign——一个美国手语数据集，证明了我们方法的一般性，并取得了具有竞争力的结果。|\n",
        "2501.09751": "|**2025-01-16**|**OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking**|Zekun Xi et.al.|[2501.09751](http://arxiv.org/abs/2501.09751)|null|机器写作使用大型语言模型时通常依赖于检索增强生成。然而，这些方法仍然局限于模型预定义的范围之内，限制了生成丰富信息的内容。具体来说，传统的检索信息往往缺乏深度、实用性和重复性，这会负面影响生成文章的质量，导致内容浅薄、重复且缺乏原创性。为了解决这些问题，我们提出了OmniThink，这是一个模仿人类迭代扩展和反思过程的机器写作框架。OmniThink背后的核心思想是模拟学习者在逐步深化对主题知识的过程中所表现出的认知行为。实验结果表明，OmniThink在提高生成文章的知识密度的同时，并未损害如连贯性和深度等指标。人类评估和专家反馈进一步突出了OmniThink在解决生成长篇文章的实际挑战中的潜力。|\n",
        "2501.09749": "|**2025-01-16**|**Enhancing Lexicon-Based Text Embeddings with Large Language Models**|Yibin Lei et.al.|[2501.09749](http://arxiv.org/abs/2501.09749)|null|近期，大型语言模型（LLMs）在通用文本嵌入任务上展现了卓越的性能。尽管密集嵌入在相关研究中占主导地位，但我们首次引入了基于词库的嵌入模型（LENS），它利用LLMs在这些任务上实现了具有竞争力的性能。针对传统因果LLMs中固有的分词冗余问题和单向注意力限制，LENS通过分词嵌入聚类来整合词汇空间，并研究双向注意力和各种池化策略。具体来说，LENS通过将每个维度分配给特定的分词簇来简化词库匹配，其中语义相似的词被分组在一起，并通过双向注意力释放LLMs的潜力。大量实验表明，LENS在大型文本嵌入基准（MTEB）上优于密集嵌入，提供了与密集嵌入相当大小的紧凑特征表示。值得注意的是，将LENS与密集嵌入相结合在MTEB的检索子集（即BEIR）上实现了最先进的性能。|\n",
        "2501.09745": "|**2025-01-16**|**Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models**|Bihui Jin et.al.|[2501.09745](http://arxiv.org/abs/2501.09745)|null|机器学习开发者经常使用交互式计算笔记本，如Jupyter笔记本，来托管数据处理和模型训练的代码。Jupyter笔记本提供了编写机器学习管道和交互式观察输出的便利工具，然而，由于笔记本的长度和复杂性，维护Jupyter笔记本，例如添加新功能或修复错误，可能会很具挑战性。此外，目前还没有与Jupyter笔记本开发者编辑相关的现有基准。为了解决这个问题，我们提出了第一个由GitHub上792个机器学习仓库的20,095次修订中提取的48,398个Jupyter笔记本编辑的数据集，并进行了第一个使用大型语言模型（LLMs）预测Jupyter笔记本代码编辑的研究。我们的数据集捕捉了单元格级别和行级别的修改的细节，为理解机器学习工作流程中的实际维护模式提供了基础。我们观察到，Jupyter笔记本的编辑非常局部化，更改平均只涉及存储库中的166行代码。虽然较大模型在代码编辑方面优于较小模型，但所有模型在微调后在我们的数据集上的准确性仍然很低，这表明现实世界机器学习维护任务的复杂性。我们的发现强调了上下文信息在提高模型性能中的关键作用，并为提升大型语言模型在工程机器学习代码方面的能力指出了有前景的方向。|\n",
        "2501.09732": "|**2025-01-16**|**Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps**|Nanye Ma et.al.|[2501.09732](http://arxiv.org/abs/2501.09732)|null|生成模型在各个领域产生了重大影响，这主要得益于它们在训练过程中通过增加数据、计算资源和模型规模来扩展的能力，这一现象被称为扩展定律。最近的研究开始探索大型语言模型（LLMs）在推理时间内的扩展行为，揭示了性能如何随着推理过程中额外计算的增加而进一步提高。与LLMs不同，扩散模型本质上是灵活的，可以通过调整去噪步骤的数量来调整推理时间的计算，尽管性能提升通常在几十步之后就会趋于平坦。在这项工作中，我们探索了扩散模型在增加去噪步骤之外的推理时间扩展行为，并研究如何通过增加计算来进一步提高生成性能。具体来说，我们考虑了一个搜索问题，旨在识别扩散采样过程中更好的噪声。我们沿着两个轴来构建设计空间：用于提供反馈的验证器和用于寻找更好噪声候选者的算法。通过对类条件化和文本条件化图像生成基准的大量实验，我们的发现表明，增加推理时间计算可以显著提高扩散模型生成的样本质量，并且由于图像的复杂性质，框架中组件的组合可以具体选择以符合不同的应用场景。|\n",
        "2501.09709": "|**2025-01-16**|**CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education**|Tianyu Wang et.al.|[2501.09709](http://arxiv.org/abs/2501.09709)|**[link](https://github.com/tisage/cybermentor)**|许多非传统学生在网络安全专业中往往缺乏来自同伴、家庭成员和教授的建议，这可能会阻碍他们的学习体验。此外，由于内容相关性、建议的地域性、最低专业知识和时间问题等因素，这些学生可能无法充分利用各种基于大型语言模型（LLM）的人工智能助手。本文通过介绍一个旨在为这些学生的需求提供全面支持的、解答与知识、技能和职业准备建议相关问题的应用程序来解决这些挑战。我们开发了一个学习工具平台——CyberMentor，以解决网络安全专业学生的多样需求和痛点。该平台由代理工作流和生成式大型语言模型（LLMs）驱动，利用检索增强生成（RAG）实现准确和上下文相关的信息检索，以达到可访问性和个性化。我们展示了它在解决网络安全教育知识需求、应对职业市场竞争力、解决分析性和编程作业的技能需求以及提供实时按需学习支持方面的价值。通过三个使用场景，我们展示了CyberMentor在促进知识获取和职业准备、提供无缝的基于技能的指导和支持方面的作用。我们还采用了基于LangChain提示的评估方法来评估平台的影响，证实了其在帮助性、正确性和完整性方面的出色表现。这些结果强调了系统在支持学生发展实用网络安全技能的同时，改善高等教育中的公平性和可持续性的能力。此外，CyberMentor的开源设计允许其在其他学科中进行调整，促进教育创新并扩大其潜在影响。|\n",
        "2501.09706": "|**2025-01-16**|**Domain Adaptation of Foundation LLMs for e-Commerce**|Christian Herold et.al.|[2501.09706](http://arxiv.org/abs/2501.09706)|null|我们介绍了e-Llama模型：8亿和70亿参数的大规模语言模型，这些模型针对电子商务领域进行了调整。这些模型被视为具有电子商务深度知识的基座模型，为指令和微调提供基础。e-Llama模型是通过在1万亿个特定领域数据上持续预训练Llama 3.1基座模型获得的。我们讨论了我们的方法，并通过一系列消融研究来解释我们选择超参数的动机。为了量化模型在电子商务领域的适应程度，我们定义并实施了一系列多语言、针对电子商务的评估任务。我们表明，在仔细选择训练设置的情况下，Llama 3.1模型可以适应新领域，而不会在通用领域任务上牺牲显著性能。我们还探讨了将调整后的模型与基座模型合并的可能性，以更好地控制领域间的性能权衡。|\n",
        "2501.09694": "|**2025-01-16**|**Simulated Interactive Debugging**|Yannic Noller et.al.|[2501.09694](http://arxiv.org/abs/2501.09694)|null|软件调试，即故障定位及其修复，是软件工程中的主要活动。因此，有效的、高效的调试是软件工程师必须培养的核心技能之一。然而，调试技术的教学通常非常有限，或者只是以间接的方式教授，例如在软件项目中进行。结果，大多数计算机科学（CS）学生以零散和无组织的方式学习调试。在这项工作中，我们提出了一种称为模拟交互式调试的方法，该方法以交互式的方式引导学生进行调试过程。这种指导旨在赋予学生修复他们解决方案的能力，并拥有适当的“学习”体验。我们设想，这种引导调试技术可以集成到计算机科学教育课程中的编程课程早期阶段。为了进行初步评估，我们开发了一个原型实现，使用传统的故障定位技术和大型语言模型。学生可以使用诸如自动设置断点或交互式聊天机器人等特性。我们设计和执行了一个包括这个集成到IDE中的工具的受控实验，实验对象为八名本科生。基于反馈，我们得出结论，参与者喜欢辅助调试器的系统指导。特别是，他们将自动设置断点评为最有效，其次是交互式调试和聊天，以及设置断点的解释。在我们未来的工作中，我们将改进我们的概念和实现，添加新功能，并进行更深入的用户研究。|\n",
        "2501.09686": "|**2025-01-16**|**Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models**|Fengli Xu et.al.|[2501.09686](http://arxiv.org/abs/2501.09686)|null|语言长期以来被视为人类推理的必要工具。大型语言模型（LLMs）的突破引发了研究人员利用这些模型解决复杂推理任务的重大兴趣。研究人员已经超越了简单的自回归标记生成，通过引入“思维”的概念——代表推理过程中中间步骤的标记序列。这一创新范式使得LLMs能够模仿复杂的人类推理过程，如树状搜索和反思性思考。最近，一种新兴的学习推理趋势将强化学习（RL）应用于训练LLMs掌握推理过程。这种方法通过试错搜索算法自动生成高质量的推理轨迹，通过提供大量训练数据显著扩展了LLMs的推理能力。此外，最近的研究表明，鼓励LLMs在测试时推理过程中使用更多标记进行“思考”可以进一步提高推理准确性。因此，训练时间和测试时间的扩展共同展示了一个新的研究前沿——通向大型推理模型的路径。OpenAI的o1系列产品的推出标志着这一研究方向的重大里程碑。在本综述中，我们全面回顾了LLMs推理方面的最新进展。我们首先介绍了LLMs的基础背景，然后探讨了推动大型推理模型发展的关键技术组件，重点关注自动化数据构建、学习推理技术和测试时间扩展。我们还分析了构建大型推理模型的流行开源项目，并以开放挑战和未来研究方向结束。|\n",
        "2501.10360": "|**2025-01-17**|**FaceXBench: Evaluating Multimodal LLMs on Face Understanding**|Kartik Narayan et.al.|[2501.10360](http://arxiv.org/abs/2501.10360)|**[link](https://github.com/kartik-3004/facexbench)**|**多模态大型语言模型（MLLMs）在广泛的任务和领域中展示了令人印象深刻的解决问题的能力。然而，它们在面部理解方面的能力尚未得到系统研究。为了填补这一空白，我们引入了FaceXBench，这是一个综合性的基准，旨在评估MLLMs在复杂面部理解任务上的表现。FaceXBench包含来自25个公共数据集和新建的FaceXAPI数据集的5,000个多模态多选题。这些问题涵盖了6个广泛类别中的14个任务，评估了MLLMs在偏见与公平性、面部认证、识别、分析、定位和工具检索方面的面部理解能力。使用FaceXBench，我们对26个开源MLLMs以及2个专有模型进行了广泛的评估，揭示了复杂面部理解任务中的独特挑战。我们分析了模型在三个评估设置中的表现：零样本、上下文任务描述和思维链提示。我们的详细分析表明，当前的MLLMs，包括像GPT-4o和GeminiPro 1.5这样的高级模型，仍有很大的改进空间。我们相信FaceXBench将成为开发能够执行复杂面部理解的MLLMs的关键资源。代码：https://github.com/Kartik-3004/facexbench**|\n",
        "2501.10332": "|**2025-01-17**|**Agent4Edu: Generating Learner Response Data by Generative Agents for Intelligent Education Systems**|Weibo Gao et.al.|[2501.10332](http://arxiv.org/abs/2501.10332)|null|个性化学习是智能教育系统中一种有潜力的教育策略，旨在提高学习者的实践效率。然而，离线指标与在线表现之间的差异严重阻碍了他们的进步。为了应对这一挑战，我们引入了Agent4Edu，这是一种利用大型语言模型（LLMs）在人类智能领域最新进展的个性化学习模拟器。Agent4Edu具有由LLM驱动的生成型智能体，这些智能体配备了针对个性化学习算法的学习者档案、记忆和动作模块。学习者档案使用现实世界的响应数据初始化，捕捉实践风格和认知因素。受人类心理学理论的启发，记忆模块记录实践事实和高级摘要，并整合反思机制。动作模块支持各种行为，包括练习理解、分析和响应生成。每个智能体都可以与个性化学习算法（如计算机自适应测试）交互，从而实现定制服务的多方面评估和提升。通过全面评估，我们探讨了Agent4Edu的优缺点，强调了智能体和人类学习者之间在回答的一致性和差异。代码、数据和附录可在https://github.com/bigdata-ustc/Agent4Edu上公开获取。|\n",
        "2501.10326": "|**2025-01-17**|**Large language models for automated scholarly paper review: A survey**|Zhenzhen Zhuang et.al.|[2501.10326](http://arxiv.org/abs/2501.10326)|null|大型语言模型（LLMs）对人类社会产生了重大影响，涉及多个领域。其中，学术界不仅是受到LLMs影响的领域，也是LLMs发展的关键力量。在学术出版物中，这一现象体现在将LLMs纳入同行评审机制来审阅稿件的过程中。我们在之前的研究论文中提出了自动学术论文评审（ASPR）的概念。随着这种纳入的扩展，现在进入了ASPR与同行评审共存阶段，这在之前的论文中有所描述。LLMs对于ASPR的全面实施具有变革潜力，但同时也带来了需要解决的新问题和挑战。在这篇综述论文中，我们旨在为LLMs时代的ASPR提供一个全面的视角。我们首先调查了哪些LLMs被用于进行ASPR。然后，我们回顾了随着LLM技术的融入，ASPR相关技术瓶颈得到了哪些解决。接着，我们探讨了LLMs为ASPR带来的新方法、新数据集、新源代码和新在线系统。此外，我们总结了LLMs在ASPR中的性能和问题，并调查了出版商和学术界对ASPR的态度和反应。最后，我们讨论了与LLMs在ASPR开发中相关的挑战。我们希望这篇综述能为研究人员提供启发性的参考，并促进ASPR在实际应用中的进展。|\n",
        "2501.10318": "|**2025-01-17**|**HiMix: Reducing Computational Complexity in Large Vision-Language Models**|Xuange Zhang et.al.|[2501.10318](http://arxiv.org/abs/2501.10318)|null|近年来，得益于大型语言模型和模态对齐技术的进步，现有的大型视觉-语言模型（LVLMs）在众多场景中取得了显著的性能。然而，过高的计算复杂度限制了这些模型在实际应用中的广泛应用。我们认为，计算复杂度的主要瓶颈之一是由于模型计算中涉及了冗余的视觉序列。这一观点源于对LVLMs语言解码器中视觉和语言信息传输效率的重新评估。随后，我们提出了一种新的分层视觉-语言交互机制，称为“混合注意的分层视觉注入”（HiMix）。在HiMix中，只有语言序列经历完整的正向传播，而视觉序列在每个语言解码器层中与语言在特定阶段进行交互。值得注意的是，我们的方法显著降低了计算复杂度，同时性能损失最小。具体来说，HiMix在多个LVLM模型中将语言解码器的计算成本降低了10倍，同时保持了可比较的性能。这突显了我们方法的优势，我们希望我们的研究为视觉-语言理解领域带来新的视角。  项目页面：https://xuange923.github.io/HiMix|\n",
        "2501.10313": "|**2025-01-17**|**Addressing Popularity Bias in Third-Party Library Recommendations Using LLMs**|Claudio Di Sipio et.al.|[2501.10313](http://arxiv.org/abs/2501.10313)|null|软件工程推荐系统（RSSE）在通过根据开发者的上下文提供相关建议来自动化开发任务中发挥着关键作用。然而，它们遭受了所谓的流行度偏差，即推荐可能对当前任务不相关的流行物品的现象。特别是，长尾效应可能会损害系统在准确性方面的性能，从而导致推荐中的假阳性。基础模型是当前最先进的基于生成式AI的模型，在多个软件工程（SE）任务中实现了相关结果。本文旨在研究大型语言模型（LLMs）解决第三方库（TPLs）推荐系统中流行度偏差的能力。我们进行了一项消融研究，实验了缓解流行度偏差的最先进技术，包括微调和流行度惩罚机制。我们的发现表明，所考虑的LLMs无法解决TPL推荐器中的流行度偏差，尽管微调和后处理惩罚机制有助于提高提供的推荐的整体多样性。此外，我们讨论了LLMs在此背景下的局限性，并提出了解决TPL推荐器中流行度偏差的潜在改进建议，从而为这一方向上的进一步实验铺平了道路。|\n",
        "2501.10282": "|**2025-01-17**|**Computational Protein Science in the Era of Large Language Models (LLMs)**|Wenqi Fan et.al.|[2501.10282](http://arxiv.org/abs/2501.10282)|null|考虑到蛋白质的重要性，计算蛋白质科学一直是一个关键的科学研究领域，致力于揭示蛋白质序列-结构-功能范式中的知识并开发应用。在过去的几十年里，人工智能（AI）对计算蛋白质科学产生了重大影响，导致在特定蛋白质建模任务中取得了显著的成就。然而，之前的AI模型仍然存在局限性，例如难以理解蛋白质序列的语义，以及在广泛的蛋白质建模任务中无法进行泛化。最近，由于它们前所未有的语言处理和泛化能力，大型语言模型（LLMs）已成为AI的一个里程碑。它们可以促进整个领域的全面进步，而不仅仅是解决个别任务。因此，研究人员积极地将LLM技术引入计算蛋白质科学，开发出能够巧妙地掌握蛋白质基础知识的蛋白质语言模型（pLMs），并且可以有效地泛化来解决各种序列-结构-功能推理问题。在见证繁荣发展的同时，有必要对LLM技术赋能的计算蛋白质科学进行系统概述。首先，我们根据所掌握的蛋白质知识将现有的pLMs分类，即潜在序列模式、明确的结构和功能信息以及外部科学语言。其次，我们介绍pLMs的利用和适应性，突出它们在促进蛋白质结构预测、蛋白质功能预测和蛋白质设计研究方面的显著成就。然后，我们描述pLMs在抗体设计、酶设计和药物发现中的实际应用。最后，我们具体讨论这个快速发展的领域的有希望的未来方向。|\n",
        "2501.10200": "|**2025-01-17**|**Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation**|Azat Abdullin et.al.|[2501.10200](http://arxiv.org/abs/2501.10200)|null|自动生成测试是软件工程研究中的一个关键且持续关注的领域。大型语言模型（LLMs）的出现为执行广泛任务的能力带来了新的机遇。然而，与基于搜索的软件测试（SBST）和符号执行等传统技术相比，基于LLM的方法的有效性仍然不确定。在本文中，我们对基于三个工具的自动测试生成方法进行了广泛的研究：EvoSuite用于SBST，Kex用于符号执行，TestSpark用于基于LLM的测试生成。我们在GitBug Java数据集上评估了这些工具的性能，并使用各种基于执行和基于特征的指标进行比较。我们的结果表明，尽管基于LLM的测试生成具有潜力，但在覆盖率方面仍然落后于传统方法。然而，它在突变得分上显著优于它们，这表明LLM提供了对代码的更深层次语义理解。基于LLM的方法在故障检测能力方面也劣于SBST和基于符号执行的方法。此外，我们的基于特征的分析表明，所有工具主要受所测试类（CUT）的复杂性和内部依赖性的影响，而基于LLM的方法对CUT的大小特别敏感。|\n",
        "2501.10186": "|**2025-01-17**|**Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education**|William Hersh et.al.|[2501.10186](http://arxiv.org/abs/2501.10186)|null|生成式人工智能在生物医学和健康领域产生了深远的影响，这既体现在专业工作中，也体现在教育中。基于大型语言模型（LLMs），生成式人工智能在模拟情况下（如参加医学考试、回答临床问题、解决临床案例、应用临床推理和总结信息）的表现与人类相当。生成式人工智能在教育领域也被广泛应用，在学术课程及其评估中表现良好。本文综述了LLMs在教育领域的成功案例，并突出了其在教育中面临的挑战，尤其是可能损害专业工作知识和技能获取的方面。然后，本文提供了关于如何克服LLMs在教育中使用的不足的最佳实践建议。尽管在教育中使用生成式人工智能存在挑战，但所有学生和教师，不仅在生物医学和健康领域，还应在其他领域，都必须理解和掌握其使用。|\n",
        "2501.10175": "|**2025-01-17**|**Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval**|Vera Pavlova et.al.|[2501.10175](http://arxiv.org/abs/2501.10175)|null|本研究探讨了在伊斯兰领域内自然语言处理（NLP）技术的应用，重点关注开发一个伊斯兰神经检索模型。通过利用强大的XLM-R模型，研究采用语言缩减技术创建了一个轻量级双语大型语言模型（LLM）。我们的领域自适应方法针对伊斯兰领域面临的独特挑战，即在域内语料库大量存在阿拉伯语，而在其他语言（包括英语）中则有限的问题。该工作采用多阶段训练过程来提高检索模型的表现，结合了大型检索数据集（如MS MARCO）和较小的域内数据集。此外，我们通过数据增强技术和可靠的伊斯兰来源，创建了一个英语域内检索数据集。这种方法增强了特定领域的检索数据集，从而进一步提升了性能。研究发现，将领域自适应和多阶段训练方法结合用于双语伊斯兰神经检索模型，使其在下游检索任务上优于单语模型。|\n",
        "2501.10134": "|**2025-01-17**|**Exploring the Impact of Generative Artificial Intelligence in Education: A Thematic Analysis**|Abhishek Kaushik et.al.|[2501.10134](http://arxiv.org/abs/2501.10134)|null|近年来，生成式人工智能（GenAI）技术的进步对教育领域产生了变革性影响。大型语言模型（LLMs）如ChatGPT和Bard可以用于自动化常规任务，创建个性化教学内容，以及处理重复性任务，从而为学生提供更多创造性思考的时间。然而，在教育领域，制定指导方针、政策和评估方法以确保这些工具的负责任整合至关重要。本文对从教育领域专业人士那里收集到的七篇论文进行了主题分析，以了解在教育中使用ChatGPT和Bard等GenAI模型的优势和不足。对论文进行了探索性数据分析（EDA），以从文本中提取更多见解。研究发现，有几个主题突出了GenAI工具的利弊，以及克服这些局限性的建议，并确保学生在负责任和道德的方式下使用这些工具。|\n",
        "2501.12386": "|**2025-01-21**|**InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling**|Yi Wang et.al.|[2501.12386](http://arxiv.org/abs/2501.12386)|**[link](https://github.com/opengvlab/internvideo)**|**本文旨在通过长而丰富的上下文（LRC）建模来提高视频多模态大型语言模型（MLLM）的性能。为此，我们开发了一个新的InternVideo2.5版本，着重于增强原始MLLM感知视频中的细粒度细节和捕捉长时序结构的能力。具体来说，我们的方法通过直接偏好优化将密集视觉任务标注融入MLLM，并通过自适应分层标记压缩开发紧凑的时空表示。实验结果表明，这种独特的LRC设计极大地提高了视频MLLM在主流视频理解基准（短时与长时）中的表现，使得MLLM能够记住显著更长的视频输入（至少比原始版本长6倍），并掌握如物体跟踪和分割等专业的视觉能力。我们的工作强调了多模态上下文丰富性（长度和精细度）在赋予MLLM内在能力（关注力和记忆力）中的重要性，为未来视频MLLM研究提供了新的见解。代码和模型可在https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5获取。**|\n",
        "2501.12374": "|**2025-01-21**|**Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists**|Thomas F. Eisenmann et.al.|[2501.12374](http://arxiv.org/abs/2501.12374)|**[link](https://github.com/andreskarjus/genaiexperiment)**|**新型生成式AI在分析和生成文化物品方面的能力引发了关于艺术教育和人类专业性质与价值的必然问题。AI是否已经拉平了专业艺术家与业余人士之间的竞争场地，或者训练有素的艺术家表达力、策展技能和经验反而增强了使用这些新工具的能力？在这项预先注册的研究中，我们对50位活跃的艺术家和与他们在人口统计学上相匹配的业余人士样本进行了实验比较。我们设计了两个任务来模拟艺术实践，以测试他们在忠实和创造性图像创作方面的能力：复制一个参考图像，以及尽可能远离它。我们开发了一个定制的平台，参与者使用现代文本到图像模型来完成这两个任务。我们还收集并比较了参与者对AI的情感。平均而言，艺术家比他们的业余同行产生了更忠实和更具创造性的输出，尽管差距很小。尽管AI可能简化了内容创作，但专业知识仍然有价值——即使在生成式AI的局限空间内也是如此。最后，我们还探讨了如果让一个代表性的具有视觉能力的大型语言模型（GPT-4o）担任图像生成代理，它将如何完成同样的任务，并发现它在复制方面表现相当，但在创造性任务中甚至超过了艺术家。在这两个任务中，最好的结果仍然是由人类产生的。这些结果突出了将艺术技能与AI培训相结合的重要性，以帮助艺术家和其他视觉专业人士为技术不断发展的环境做好准备。我们看到了与生成式AI协作协同的潜力，这可能会重塑创意产业和艺术教育。**|\n",
        "2501.12372": "|**2025-01-21**|**Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL**|Yeounoh Chung et.al.|[2501.12372](http://arxiv.org/abs/2501.12372)|null|大型语言模型（LLMs）在自然语言处理的各种任务中展现出令人印象深刻的能力。特别是推理能力的提升和上下文窗口的扩展，为利用这些强大模型开辟了新的途径。NL2SQL（自然语言到SQL）的挑战在于，自然语言问题本身具有固有的歧义性，而SQL生成则需要精确理解复杂的数据模式和语义。解决这种语义歧义问题的一种方法是提供更多和足够的上下文信息。在本研究中，我们探讨了Google最先进的LLM（gemini-1.5-pro）提供的扩展上下文窗口（也称为长上下文）的性能和延迟之间的权衡。我们研究了各种上下文信息的影响，包括列示例值、问题和SQL查询对、用户提供的提示、SQL文档和模式。据我们所知，这是第一个研究扩展上下文窗口和额外上下文信息如何从准确性和延迟成本两个方面帮助NL2SQL生成的作品。我们表明，长上下文LLMs具有鲁棒性，不会在扩展的上下文信息中迷失方向。此外，我们基于Google的gemini-pro-1.5的长上下文NL2SQL管道在BIRD基准（开发）上取得了优异的性能，达到67.41%，无需微调和昂贵的自洽性技术。|\n",
        "2501.12332": "|**2025-01-21**|**Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration**|Thomas Walshe et.al.|[2501.12332](http://arxiv.org/abs/2501.12332)|null|在现实世界的机器学习项目中，获取标记的训练数据一直是一项成本高昂的任务，以满足数量和质量的要求。最近，大型语言模型（LLMs），尤其是GPT-4，在以高精度标记数据方面展现出巨大的潜力。然而，隐私和成本问题阻碍了GPT-4的广泛应用。在这项工作中，我们探索了有效利用开源模型进行自动标记的方法。我们确定将标签模式集成是一项有希望的技术，但发现仅用标签描述进行分类会导致在高基数任务上性能不佳。为了解决这个问题，我们提出了检索增强分类（RAC）方法，其中LLM一次对单个标签进行推理，使用相应的标签模式；我们从最相关的标签开始，迭代直到LLM选择一个标签。我们表明，我们的方法，通过动态集成标签描述，在标记任务中提高了性能。我们进一步表明，通过仅关注最有希望的标签，RAC可以在标签质量和覆盖范围之间进行权衡——这是我们利用来自动标记我们内部数据集的特性。|\n",
        "2501.12327": "|**2025-01-21**|**VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model**|Xianwei Zhuang et.al.|[2501.12327](http://arxiv.org/abs/2501.12327)|**[link](https://github.com/VARGPT-family/VARGPT)**|**我们提出了一种名为VARGPT的新型多模态大型语言模型（MLLM），该模型在单个自回归框架内统一了视觉理解和生成。VARGPT采用下一个标记预测范式进行视觉理解，采用下一个尺度预测范式进行视觉自回归生成。VARGPT创新性地扩展了LLaVA架构，在MLLM中实现了高效的尺度自回归视觉生成，同时在一个模型框架内无缝地容纳了混合模态的输入和输出。我们的VARGPT在特别策划的数据集上进行了三阶段的统一训练过程，包括预训练阶段和两个混合视觉指令调整阶段。统一的训练策略旨在实现视觉和文本特征的对齐，增强理解和生成方面的指令遵循，以及提高视觉生成质量。尽管其基于LLAVA的多模型理解架构，VARGPT在视觉问答和推理等各个视觉中心基准测试中，显著优于LLaVA-1.5。值得注意的是，VARGPT自然支持自回归视觉生成和指令到图像合成的功能，展示了其在视觉理解和生成任务中的多功能性。项目页面为：\\url{https://vargpt-1.github.io/}**|\n",
        "2501.12300": "|**2025-01-21**|**LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations**|Hasan Abu-Rasheed et.al.|[2501.12300](http://arxiv.org/abs/2501.12300)|null|在学习个性化为学习者提供了巨大潜力的情况下，现代高等教育实践需要更深入地考虑领域模型和学习环境，以开发有效的个性化算法。本文介绍了一种创新的高等教育课程建模方法，该方法利用大型语言模型（LLMs）进行知识图谱（KG）补全，旨在创建个性化的学习路径推荐。我们的研究侧重于对大学学科进行建模，并将它们的主题与相应的领域模型联系起来，使学生学习路径中能够整合来自不同学院和机构的课程模块。我们方法的核心是一个协作过程，其中LLMs协助人类专家从讲座材料中提取高质量、细粒度的主题。我们为大学模块和利益相关者开发了领域、课程和用户模型。我们将此模型应用于从两个研究模块（嵌入式系统和基于FPGA的嵌入式系统开发）创建知识图谱。所得知识图谱结构化了课程并将其与领域模型相联系。我们通过定性专家反馈和定量图质量指标来评估我们的方法。领域专家验证了模型的相关性和准确性，而图质量指标衡量了我们知识图谱的结构属性。我们的结果表明，LLM辅助的图补全方法增强了将相关课程连接起来以实现个性化学习体验的能力。专家反馈还表明，对于概念提取和分类，提出的协作方法得到了高度接受。|\n",
        "2501.12281": "|**2025-01-21**|**MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks**|Qishen Zhou et.al.|[2501.12281](http://arxiv.org/abs/2501.12281)|**[link](https://github.com/youxiaotu/MoGERNN)**|**在给定部分观测到的道路网络的情况下，我们如何预测未观测位置的交通状况？虽然深度学习方法在交通预测方面表现出卓越的性能，但大多数方法都假设在所有感兴趣的地点都有传感器，这在经济上是不切实际的。此外，这些方法通常在传感器配置发生变化时需要昂贵的重新训练。为了解决这些挑战，我们提出了MoGERNN，一个归纳时空图表示模型。受大型语言模型中混合专家方法（Mixture of Experts）的启发，我们引入了一个混合图专家（MoGE）模块，通过多个图消息聚合器和稀疏门控网络来建模复杂的空间依赖关系。该模块估计未观测位置的初始状态，然后这些状态由一个基于GRU的编码器-解码器处理，该解码器集成了一个图消息聚合器来捕捉时空依赖关系并预测未来状态。在两个真实世界数据集上的实验表明，MoGERNN在观测和未观测位置上都一致优于基线方法。MoGERNN甚至可以在没有传感器的区域准确预测交通拥堵的发展，为交通管理提供有价值的信息。此外，MoGERNN能够适应动态传感网络，即使与重新训练的版本相比，也能保持具有竞争力的性能。使用不同数量的可用传感器进行的测试证实了其持续的优越性，而消融研究验证了其关键模块的有效性。**|\n",
        "2501.12273": "|**2025-01-21**|**Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement**|Maosong Cao et.al.|[2501.12273](http://arxiv.org/abs/2501.12273)|**[link](https://github.com/internlm/condor)**|监督微调（SFT）数据的质量在提升大型语言模型（LLMs）的对话能力方面起着关键作用。然而，随着LLMs的不断发展，高质量人工标注的SFT数据获取已成为一个重要的瓶颈，这导致了对合成训练数据的更大依赖。在这项工作中，我们引入了Condor，这是一个新颖的两阶段合成数据生成框架，它结合了世界知识树和自我反思优化，以大规模生成高质量的SFT数据。我们的实验结果表明，仅在20K个Condor生成的样本上进行微调的基础模型，其性能优于同类模型。Condor中的额外优化阶段进一步使得LLMs能够在不同规模（高达720亿）上进行迭代自我改进，验证了我们的方法的有效性。此外，我们对训练后合成数据扩展的研究揭示了大量未开发的性能提升潜力，为未来的研究开辟了有希望的道路。|\n",
        "2501.12243": "|**2025-01-21**|**FOCUS: First Order Concentrated Updating Scheme**|Yizhou Liu et.al.|[2501.12243](http://arxiv.org/abs/2501.12243)|null|大型语言模型（LLMs）展现出惊人的性能，而改进它们的预训练过程似乎是其能力进一步提升的关键。基于Adam、学习率衰减和权重衰减已记录的成功，我们假设预训练损失景观具有狭窄的谷地结构。通过合成损失函数的实验，我们发现当梯度查询噪声相对于谷地的尖锐度较高时，Adam的性能落后于Signum，因为Adam会急剧减少有效步长。这一观察结果促使我们开发了FOCUS，这是一种优化器，通过引入对移动平均参数的吸引力来增强Signum，使其在处理噪声的同时保持更大的步长。在训练GPT-2时，FOCUS证明比Signum更稳定，比Adam更快。这些结果表明，梯度噪声可能是LLM训练中被低估的限制因素，而FOCUS提供了有希望的解决方案。|\n",
        "2501.12231": "|**2025-01-21**|**InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models**|Pha Nguyen et.al.|[2501.12231](http://arxiv.org/abs/2501.12231)|null|通过观察人类执行多步骤任务，可以构建具有情境感知的助手，这些助手能够根据对动作和任务的了解提供帮助。在本文中，我们开发了一个基于多模态大型语言模型的情境感知指导任务助手（InsTALL），它利用在线视觉流（例如用户的屏幕共享或视频录制）并实时响应用户有关当前任务的查询。为了实现有用的帮助，InsTALL 1）在任务视频和配对文本数据上训练一个多模态模型，2）自动从视频数据中提取任务图，并在训练和推理时利用它。我们表明，InsTALL在多模态活动理解所考虑的子任务中实现了最先进的性能——任务识别（TR）、动作识别（AR）、下一步动作预测（AP）和计划预测（PP），并且在两个与自动错误识别相关的创新子任务上优于现有基线。|\n",
        "2501.13080": "|**2025-01-22**|**Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment**|Melissa Kazemi Rad et.al.|[2501.13080](http://arxiv.org/abs/2501.13080)|null|大型语言模型（LLMs）在包括对话人工智能产品在内的不同应用中展现出了强大的能力，因此确保这些产品的安全和可靠性至关重要。通过减轻它们对恶意用户交互的脆弱性，可以避免重大风险和声誉损害。在本研究中，我们全面探讨了不同LLMs的思维链（CoT）响应微调和对齐的有效性，这些响应作为输入监管的护栏。我们系统地通过利用一小部分训练数据来探索各种调整方法，将这些模型作为代理防御机制来检测恶意输入并提供其判断的推理，从而防止对话代理被滥用。我们严格评估了不同调整策略的有效性和鲁棒性，以泛化到各种对抗性和恶意查询类型。我们的实验结果表明，即使是有限的资源，针对不同有害输入查询的定制对齐过程也具有潜力。这些技术显著提高了对话人工智能系统的安全性，并为部署更安全、更值得信赖的AI驱动交互提供了一个可行的框架。|\n",
        "2501.13042": "|**2025-01-22**|**Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning**|Bohao Yang et.al.|[2501.13042](http://arxiv.org/abs/2501.13042)|**[link](https://github.com/bernard-yang/mmsci_table)**|**近期，大型语言模型（LLMs）在表格理解能力上取得了显著进步，但它们依赖于将表格转换为文本序列。虽然多模态大型语言模型（MLLMs）能够直接进行视觉处理，但由于输入图像分辨率的固定和数值推理能力的不足，它们在处理科学表格方面面临限制。我们提出了一种全面的多模态科学表格理解和推理框架，该框架能够处理具有动态输入图像分辨率的表格。我们的框架由三个关键组件组成：（1）MMSci-Pre，一个包含52K个科学表格结构识别样本的特定领域表格结构学习数据集；（2）MMSci-Ins，一个包含12K样本的指令微调数据集，涵盖了三个基于表格的任务；（3）MMSci-Eval，一个包含3,114个测试样本的基准，专门用于评估数值推理能力。大量的实验表明，与150K个通用领域表格相比，我们的52K个科学表格图像的特定领域方法取得了更好的性能，突出了数据质量相对于数量的重要性。我们提出的基于表格的MLLMs，具有动态输入分辨率，在一般表格理解和数值推理能力方面都有显著提高，并具有良好的泛化能力，适用于未参与训练的数据集。我们的代码和数据在https://github.com/Bernard-Yang/MMSci_Table上公开。**|\n",
        "2501.13007": "|**2025-01-22**|**Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament**|Yantao Liu et.al.|[2501.13007](http://arxiv.org/abs/2501.13007)|**[link](https://github.com/thu-keg/pairwiserm)**|**最佳N个（Best-of-N，BoN）采样是测试时对大型语言模型（LLMs）进行缩放的一种常见策略，它依赖于奖励模型从多个生成中选择最佳候选解决方案。然而，传统的奖励模型通常分配任意且不一致的分数，限制了其有效性。为了解决这个问题，我们提出了一种成对奖励模型（Pairwise RM）与淘汰赛相结合的BoN采样方法。不同于分配绝对分数，针对一个数学问题，Pairwise RM同时评估两个候选解决方案的正确性。这种方法消除了任意评分的需求，并通过并行比较实现了解决方案的交叉验证。在淘汰赛中，Pairwise RM对候选解决方案进行成对比较，并迭代地淘汰错误的解决方案。我们构建了我们的数据集（\\ourdataset），这是一个包含443K个成对比较的大规模数据集，来源于NumiaMath，并使用\\texttt{gemini-1.5-flash}进行标注，并通过监督微调训练Pairwise RM。在MATH-500和奥林匹克竞赛基准上的实验表明，与传统判别性奖励模型相比，这种方法取得了显著的改进。在最具挑战性的前50%问题上，实现了40%到60%的相对改进。**|\n",
        "2501.12988": "|**2025-01-22**|**Large Language Model-Based Semantic Communication System for Image Transmission**|Soheyb Ribouh et.al.|[2501.12988](http://arxiv.org/abs/2501.12988)|null|大型语言模型（LLMs）在理解和生成各种数据类型（如图像和文本）方面的显著成功，展示了它们处理和提取跨不同领域语义信息的能力。这种变革性能力为语义通信奠定了基础，使得高效和智能的通信系统成为可能。在本工作中，我们提出了一种基于OFDM的图像传输语义通信框架。我们提出了一种创新的语义编码器设计，它利用LLMs提取传输数据意义的能力，而不是关注其原始表示。在接收端，我们设计了一种基于LLM的语义解码器，能够理解上下文并生成最合适的表示以适应给定上下文。我们将在不同场景下评估我们的系统，包括具有不同速度范围的城区宏蜂窝环境。评估指标显示，我们提出的系统将数据量减少了4250倍，同时比传统通信方法实现了更高的数据速率。这种方法为解锁6G连接的潜力提供了一种稳健且可扩展的解决方案。|\n",
        "2501.12983": "|**2025-01-22**|**LLM4WM: Adapting LLM for Wireless Multi-Tasking**|Xuanyu Liu et.al.|[2501.12983](http://arxiv.org/abs/2501.12983)|null|无线信道是通信的基础，包括众多统称为信道相关任务的子任务。这些任务可以通过基于信道特性的联合学习来共享表示并提升系统设计。为了利用这一优势，提出了针对信道相关任务的LLM4WM——一种大型语言模型（LLM）多任务微调框架。该框架采用混合专家与低秩自适应（MoE-LoRA）方法进行多任务微调，使得预训练的LLM的通用知识能够转移到这些任务中。考虑到无线信道数据的独特特性，设计了预处理模块、适配器模块和多任务输出层，以使信道数据与LLM的语义特征空间相匹配。在信道相关多任务数据集上的实验表明，LLM4WM在全样本和少样本评估中均优于现有方法，这归功于其鲁棒的多任务联合建模和迁移学习能力。|\n",
        "2501.12975": "|**2025-01-22**|**OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models**|Chongren Sun et.al.|[2501.12975](http://arxiv.org/abs/2501.12975)|**[link](https://github.com/sunchongren/onioneval)**|**大型语言模型（LLMs）能力强大，但无论是训练还是推理都需要大量的计算资源。在LLM家族中，小型模型（参数少于100亿的模型）在各种任务上也能表现出色。然而，这些小型模型与它们的较大模型类似，存在一些共同的局限性，包括容易产生幻觉。尽管存在许多基准来评估LLM中的幻觉，但很少有专门针对小型LLM（SLLMs）的。此外，SLLMs在不同基准上的表现差异很大。在本文中，我们引入了OnionEval，这是一个多层结构化框架，包含一个称为上下文影响分数（CI）的特定指标，旨在有效评估小型LLM在不同上下文水平上的事实冲突幻觉倾向。我们的实验结果表明，SLLMs的一个关键特征是它们在事实分析方面表现出色，但在上下文推理方面面临挑战。进一步的研究表明，一个简单的思维链策略可以显著减少这些局限性，提高SLLMs在实际应用中的实用性。**|\n",
        "2501.12972": "|**2025-01-22**|**Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs**|Jan Corazza et.al.|[2501.12972](http://arxiv.org/abs/2501.12972)|null|当提到区块链系统是无信任的，这实际上意味着所有的信任都寄托在软件上。因此，确保区块链软件正确性具有强烈的动机——这里的漏洞可能导致数百万美元的损失并摧毁企业。建立软件正确性的一种最强大的方法是通过使用形式化方法。然而，基于形式化方法的方案在时间和专业知识方面产生了显著的开销，以成功应用它们。我们的工作通过自动化创建形式化模型——软件系统的数学抽象——来解决这一关键缺点，这在使用形式化方法时通常是一个核心任务。我们进行模型综合分为三个阶段：首先将代码转换为模型占位符；然后使用大型语言模型（LLM）“填补空白”；最后，我们在语法和语义层面迭代修复生成的模型。通过这种方式，我们显著减少了创建形式化模型所需的时间，并提高了依赖于它们的宝贵软件验证方法的可及性。我们工作的实际背景是减少使用形式化模型对智能合约正确性进行审计的价值实现时间。|\n",
        "2501.12962": "|**2025-01-22**|**It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act**|Kristof Meding et.al.|[2501.12962](http://arxiv.org/abs/2501.12962)|null|构成公平决策的因素是什么？这个问题不仅对人类来说很难，当使用人工智能（AI）模型时，挑战更大。鉴于歧视性算法行为，欧盟最近通过了《人工智能法案》，该法案要求对AI模型制定特定规则，结合了传统的非歧视法律规范和基于机器学习的算法公平性概念。本文旨在通过以下方式弥合《人工智能法案》中这两种不同概念之间的差距：首先，对法律和计算机科学方向的学者进行高级别的概念介绍；其次，对《人工智能法案》中法律非歧视规范与算法公平性之间的关系进行深入分析。我们的分析揭示了三个关键发现：（1）大多数非歧视性规定仅针对高风险AI系统。（2）高风险系统的监管包括数据输入要求和输出监控，尽管这些规定往往不一致，并引发计算可行性的问题。（3）针对通用AI模型的规定，例如不被同时归类为高风险系统的大型语言模型，与其他规定相比，目前缺乏具体性。基于这些发现，我们建议为AI系统开发更具体的审计和测试方法。本文旨在为研究AI系统中歧视问题的法律学者和计算机科学方向的机器学习研究人员之间的未来跨学科合作奠定基础。|\n",
        "2501.12959": "|**2025-01-22**|**Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference**|Weizhi Fei et.al.|[2501.12959](http://arxiv.org/abs/2501.12959)|null|尽管涉及长文本输入的应用对于大型语言模型（LLM）的有效利用至关重要，但它们也导致了计算成本的增加和性能的降低。为了应对这一挑战，我们提出了一种高效、无需训练的提示压缩方法，该方法能在压缩提示中保留关键信息。我们识别出基于Transformer的LLM中特定的注意力头，将其指定为评估头，这些评估头能够选择长输入中对于推理最关键的标记。基于这一发现，我们开发了EHPC（基于评估头的提示压缩方法），它通过在预填充阶段仅利用带有评估头的前几层，使LLM能够快速“浏览”输入提示，随后仅将重要标记传递给模型进行推理。EHPC在两个主流基准测试中实现了最先进的结果：提示压缩和长文本推理加速。因此，它有效地降低了与商业API调用相关的复杂性和成本。我们进一步证明了EHPC与基于键值缓存加速的方法相比具有竞争力，从而突出了其在提高LLM长文本任务效率方面的潜力。|\n",
        "2501.12956": "|**2025-01-22**|**GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models**|Pengxiang Zhao et.al.|[2501.12956](http://arxiv.org/abs/2501.12956)|null|大型语言模型（LLMs）由于资源需求庞大，面临着显著的部署挑战。虽然低比特量化权重可以减少内存使用并提高推理效率，但当前硬件缺乏对混合精度通用矩阵乘法（mpGEMM）的原生支持，导致基于解量化的实现效率低下。此外，均匀量化方法通常无法充分捕捉权重分布，导致性能下降。我们提出了GANQ（GPU自适应非均匀量化），这是一种针对硬件高效的基于查找表优化的层间后训练非均匀量化框架。GANQ通过利用无训练、GPU自适应优化算法，有效地减少了层间量化误差，从而实现了卓越的量化性能。大量实验表明，与最先进的3位和4位量化方法相比，GANQ能够将FP16基线下的困惑度差距减少。此外，当部署在单个NVIDIA RTX 4090 GPU上时，GANQ的量化模型相较于基线实现了高达2.57倍的加速，进一步提升了LLMs部署中的内存和推理效率。|\n",
        "2501.13927": "|**2025-01-23**|**CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation**|Guofeng Cui et.al.|[2501.13927](http://arxiv.org/abs/2501.13927)|null|大型语言模型（LLMs）在自然语言处理任务中展现出巨大潜力，但它们在机器翻译（MT）中的应用仍然面临挑战，这主要归因于在以英语为中心的数据上进行的预训练以及从人类反馈中进行强化学习（RLHF）的复杂性。直接偏好优化（DPO）已成为一种更简单、更高效的替代方案，但其性能高度依赖于偏好数据的质量。为了解决这个问题，我们提出了基于置信度-奖励驱动的偏好优化（CRPO），这是一种新颖的方法，它将奖励分数与模型置信度相结合，以改善用于微调的数据选择。CRPO选择模型不确定或表现不佳的具有挑战性的句子对，从而实现更有效的学习。虽然CRPO主要设计用于LLMs，但它也推广到编码器-解码器模型如NLLB，展示了其多功能性。实证结果表明，CRPO在翻译准确性和数据效率方面均优于现有的RS-DPO、RSO和MBR评分等方法。|\n",
        "2501.13912": "|**2025-01-23**|**Analysis of Indic Language Capabilities in LLMs**|Aatman Vaidya et.al.|[2501.13912](http://arxiv.org/abs/2501.13912)|null|本报告评估了文本输入文本输出的大型语言模型（LLMs）在理解和生成印度语族语言方面的性能。此次评估用于识别和优先考虑适合纳入安全基准的印度语族语言。我们通过回顾现有的评估研究和数据集，以及支持印度语族语言的28个LLMs，进行这项研究。我们根据训练数据、模型和数据的许可、访问类型和模型开发者对LLMs进行分析。我们还比较了评估数据集中印度语族语言的性能，并发现印度语族语言在性能上存在显著差异。印地语是模型中最广泛代表的语言。虽然模型性能与前五种语言的讲者数量大致相关，但之后的评估则有所不同。|\n",
        "2501.13904": "|**2025-01-23**|**Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models**|Linh Tran et.al.|[2501.13904](http://arxiv.org/abs/2501.13904)|null|多模态大型语言模型（LLMs）在通过整合文本、图像和音频等多模态信息以革新客户支持和运营方面起着关键作用。联邦提示学习（FPL）是一种最近提出的方法，它结合了预训练的多模态LLMs（如视觉语言模型）和联邦学习，以创建个性化、保护隐私的AI系统。然而，平衡个性化、泛化性和隐私之间的竞争目标仍然是一个重大挑战。过度个性化可能导致过拟合，降低泛化能力，而严格的隐私措施，如差分隐私，可能会阻碍个性化和泛化。在本文中，我们提出了一种差分隐私联邦提示学习（DP-FPL）方法来应对这一挑战，通过利用低秩适应方案来捕捉泛化，同时保持一个残留项以保留个性化的表达能力。为确保隐私，我们引入了一种新颖的方法，对局部提示的两个低秩组件应用局部差分隐私，对全局提示应用全局差分隐私。我们的方法减轻了隐私噪声对模型性能的影响，同时平衡了个性化与泛化之间的权衡。广泛的实验证明了我们的方法在其他基准上的有效性。|\n",
        "2501.13884": "|**2025-01-23**|**Exploring Finetuned Audio-LLM on Heart Murmur Features**|Adrian Florea et.al.|[2501.13884](http://arxiv.org/abs/2501.13884)|null|大型音频语言模型（LLMs）在识别和分析人类语音、音乐和环境声音方面表现出色。然而，尽管科学界对其产生了浓厚兴趣，但它们理解其他类型声音的潜力，尤其是生物医学声音，仍很大程度上未被充分探索。在这项研究中，我们聚焦于使用心音图（即心脏声音）来诊断心血管疾病。大多数现有的深度神经网络（DNN）方法仅限于心脏杂音分类（健康与非健康），并未预测杂音的其他声学特征，如时间、等级、粗糙度、音调和质量，这些特征对于帮助医生诊断潜在的心脏状况至关重要。我们提出对音频LLM Qwen2-Audio在PhysioNet CirCor DigiScope心音图（PCG）数据集上进行微调，并评估其在分类11个专家标注的杂音特征方面的性能。此外，我们旨在通过探索使用音频表示模型SSAMBA的前处理分割算法，实现更具噪声鲁棒性和泛化能力的系统。我们的结果表明，基于LLM的模型在11个特征中的8个上优于现有最佳方法，在剩下的3个特征上表现相当。此外，LLM成功地分类了具有有限训练数据的长期尾部杂音特征，这是一个所有先前方法都未能分类的任务。这些发现突显了音频LLMs作为辅助人类心脏病学家的潜力，以增强心脏病诊断。|\n",
        "2501.13881": "|**2025-01-23**|**The machine learning platform for developers of large systems**|Alexey Naikov et.al.|[2501.13881](http://arxiv.org/abs/2501.13881)|null|自2021年以来，以检索增强生成（RAG）形式的机器学习系统稳步发展。RAG可以被视为一种知识迁移的版本。在所研究的案例中，大型计算系统被视为RAG的应用点，其中包括大型语言模型（LLM），作为开发团队的合作伙伴。这种方法在开发过程中以及进一步的运营时间中都具有优势。|\n",
        "2501.13880": "|**2025-01-23**|**A RAG-Based Institutional Assistant**|Gustavo Kuratomi et.al.|[2501.13880](http://arxiv.org/abs/2501.13880)|null|尽管大型语言模型（LLMs）在文本生成方面表现出强大的能力，但在需要访问结构化知识库或特定文档的场景中却面临困难，这限制了它们在知识密集型任务中的有效性。为了解决这一局限性，检索增强生成（RAG）模型已被开发出来，使生成模型能够将其输入中纳入相关的文档片段。在本文中，我们设计并评估了一种基于RAG的虚拟助手，专门针对圣保罗大学。我们的系统架构包含两个关键模块：一个检索器和一个生成模型。我们对这两个组件进行了不同类型的模型实验，调整了超参数，如块大小和检索文档的数量。我们最优的检索器模型达到了30%的Top-5准确率，而我们的最有效的生成模型在对比真实答案时得分为22.04%。值得注意的是，当向LLMs提供正确的文档片段时，准确率显著提高至54.02%，提高了超过30个百分点。相反，没有上下文输入时，性能下降至13.68%。这些发现突出了数据库访问在提高LLM性能中的关键作用。它们还揭示了当前语义搜索方法在准确识别相关文档方面的局限性，并强调了LLMs在生成精确响应方面所面临的持续挑战。|\n",
        "2501.13833": "|**2025-01-23**|**On the Reasoning Capacity of AI Models and How to Quantify It**|Santosh Kumar Radha et.al.|[2501.13833](http://arxiv.org/abs/2501.13833)|null|近年来，大型语言模型（LLMs）的进展加剧了关于其推理能力本质的讨论。尽管在GPQA和MMLU等基准测试中取得了高成绩，但这些模型在更复杂的推理任务中表现出局限性，突显了需要更严格的评估方法。我们提出了一种新颖的现象学方法，该方法超越了传统的准确率指标，以探究模型行为的潜在机制，并建立了一个可能广泛影响我们分析和理解AI系统的框架。以多选题推理任务中的位置偏差作为案例研究，我们展示了系统性的扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两种互补的现象学模型：一种是将模型响应分解为推理、记忆和猜测成分的概率混合模型（PMM），另一种是量化模型置信度与策略选择之间关系的信度理论一致性（ITC）分析。通过对推理基准测试的控制实验，我们表明，对于当前模型来说，真正的推理仍然具有挑战性，表面的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明，仅凭准确率往往高估了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。这个框架为现实世界的部署提供了定量标准，允许应用根据策略分布而不是总体性能指标来指定可靠性阈值。|\n",
        "2501.13831": "|**2025-01-23**|**Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing**|Hao Zhang et.al.|[2501.13831](http://arxiv.org/abs/2501.13831)|null|大型语言模型（LLMs）在文本风格转换和语法错误纠正等重写任务中表现出色。尽管这些任务中输入和输出之间存在相当大的重叠，但解码成本仍然随着输出长度的增加而增加，无论重叠程度如何。通过利用输入和输出之间的重叠，Kaneko和Okazaki（2023）提出了模型无关的编辑跨度表示，以压缩重写内容以节省计算。他们在四个重写任务中报告了输出长度减少率接近80%，同时最小化了对准确性的影响。在这篇论文中，我们提出了受短语统计机器翻译启发的替代编辑短语表示。我们系统地比较了我们的短语表示与它们的跨度表示。我们将LLM重写模型应用于自动语音识别（ASR）后编辑任务，并表明我们的仅针对目标短语的编辑表示具有最佳的效率-准确性权衡。在LibriSpeech测试集上，我们的方法缩小了编辑跨度模型与完整重写模型之间的50-60%的词错误率（WER）差距，同时仅损失了编辑跨度模型长度减少率的10-20%。|\n",
        "2501.13824": "|**2025-01-23**|**Hallucinations Can Improve Large Language Models in Drug Discovery**|Shuzhou Yuan et.al.|[2501.13824](http://arxiv.org/abs/2501.13824)|null|研究人员对大型语言模型（LLMs）中出现的幻觉问题表示担忧，然而它们在创意至关重要的领域，如药物发现中的潜力值得探讨。在本文中，我们提出了一个假设：幻觉可以改善LLMs在药物发现中的应用。为了验证这一假设，我们使用LLMs用自然语言描述分子的SMILES字符串，并将这些描述作为提示的一部分，以解决药物发现中的特定任务。在七个LLMs和五个分类任务上进行评估，我们的发现证实了这一假设：包含幻觉的文本可以使LLMs实现更好的性能。值得注意的是，Llama-3.1-8B相比没有幻觉的基线，在ROC-AUC上实现了18.35%的提升。此外，由GPT-4o生成的幻觉在模型中提供了最一致的性能提升。此外，我们还进行了实证分析和案例研究，以调查影响性能的关键因素及其背后的原因。我们的研究为LLMs中幻觉的潜在应用提供了启示，并为未来在药物发现中利用LLMs的新研究方向提供了新的视角。|\n",
        "2501.13816": "|**2025-01-23**|**Large Language Model driven Policy Exploration for Recommender Systems**|Jie Wang et.al.|[2501.13816](http://arxiv.org/abs/2501.13816)|null|最近推荐系统（RS）的发展中融入了强化学习（RL），将推荐视为马尔可夫决策过程（MDP）。然而，在静态用户数据上训练的离线RL策略在动态在线环境中部署时容易受到分布变化的影响。此外，过度关注短期相关项目可能会阻碍探索，导致推荐不佳并负面影响长期用户收益。基于在线RL的RS在生产部署中也面临挑战，因为暴露用户于未训练或不稳定的策略存在风险。大型语言模型（LLM）为模仿用户目标和偏好，在离线预训练策略中提供了有希望的解决方案，以增强在线环境中的初始推荐。有效管理分布变化和平衡探索对于改进基于RL的RS至关重要，尤其是在利用LLM预训练的情况下。为了解决这些挑战，我们提出了一种交互增强学习策略（iALP），该策略利用从LLM中提取的用户偏好。我们的方法包括用用户状态提示LLM以提取项目偏好，根据反馈学习奖励，并使用演员-评论家框架更新RL策略。此外，为了在在线场景中部署iALP，我们引入了一种自适应变体A-iALP，该变体实施了一种简单的微调策略（A-iALP$_{ft}$）和一种旨在减轻策略受损和探索受限问题的自适应方法（A-iALP$_{ap}$）。在三个模拟环境中的实验表明，A-iALP带来了显著的性能提升。|\n",
        "2501.14729": "|**2025-01-24**|**HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation**|Xin Zhou et.al.|[2501.14729](http://arxiv.org/abs/2501.14729)|**[link](https://github.com/lmd0311/hermes)**|**驾驶世界模型（DWMs）已成为自动驾驶的关键，因为它能够实现未来场景预测。然而，现有的DWMs仅限于场景生成，未能包含场景理解，这涉及到对驾驶环境的解释和推理。在本文中，我们提出了一种名为HERMES的统一驾驶世界模型。我们通过一个统一的框架在驾驶场景中无缝集成3D场景理解和未来场景演变（生成）。具体来说，HERMES利用鸟瞰图（BEV）表示来整合多视图空间信息，同时保留几何关系和交互。我们还引入了世界查询，通过大型语言模型（LLM）中的因果注意力将世界知识融入BEV特征，从而为理解和生成任务提供上下文丰富性。我们在nuScenes和OmniDrive-nuScenes数据集上进行了全面的研究，以验证我们方法的有效性。HERMES实现了最先进的性能，将生成错误减少了32.4%，并将理解指标如CIDEr提高了8.0%。模型和代码将在https://github.com/LMD0311/HERMES上公开发布。**|\n",
        "2501.14719": "|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719](http://arxiv.org/abs/2501.14719)|null|公平获取可靠的卫生信息对公共卫生至关重要，但在线卫生资源的质量因语言而异，这引发了关于大型语言模型（LLMs）在医疗保健中不一致性的担忧。在这项研究中，我们检查了LLMs针对英语、德语、土耳其语和中国语的卫生相关问题的回答的一致性。我们主要通过按疾病类型对卫生相关问题进行分类，并扩大其多语言范围（添加土耳其语和中文翻译），大幅度扩展了HealthFC数据集。我们发现回答中存在显著的差异，可能会传播医疗保健的错误信息。我们的主要贡献包括：1）一个包含疾病类别元信息的多语言卫生相关问题数据集；2）一个基于提示的新型评估工作流程，通过解析实现两种语言之间的子维度比较。我们的发现突出了在多语言环境中部署基于LLM的工具的关键挑战，并强调了改进跨语言对齐以确保准确和公平的卫生信息的需求。|\n",
        "2501.14717": "|**2025-01-24**|**Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models**|Naihao Deng et.al.|[2501.14717](http://arxiv.org/abs/2501.14717)|null|近年来，自然语言处理领域利用指令调整技术来提升大型语言模型（LLMs）在表格相关任务上的性能。然而，先前的研究使用了不同的基础模型和训练数据，导致在结果表格LLMs之间缺乏直接的比较。为了解决这个问题，我们在现有的公共训练数据集上对Mistral、OLMo和Phi系列的基础模型进行了微调。我们的复现实验达到了与现有表格LLMs相当甚至更好的性能，在Hitab表格问答数据集上建立了新的最先进性能。更重要的是，通过系统性的跨领域评估，我们解耦了训练数据和基础模型的贡献，揭示了它们各自的影响。此外，我们还评估了针对表格的指令调整对通用基准测试的影响，揭示了专业化和泛化之间的权衡。|\n",
        "2501.14713": "|**2025-01-24**|**FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing**|James Seale Smith et.al.|[2501.14713](http://arxiv.org/abs/2501.14713)|null|在自然语言处理（NLP）领域，大型语言模型（LLMs）的快速普及对内存受限设备上的高效部署提出了迫切需求，同时又不牺牲性能。我们提出了一种剪枝LLMs的方法，该方法根据重要性分数选择性地剪枝模型块，并用低参数替换策略替换它们。具体来说，我们提出了一种基于权重共享机制的原理性度量方法，该机制利用未剪枝的模型块和块特定的低秩适配器来替换每个剪枝块。此外，我们通过输出特征归一化和基于低秩奇异值分解重建的适配器初始化方案，促进了这些替换块的训练。实证评估表明，与现有方法相比，该方法取得了显著的性能提升，在30%压缩率下在5/6基准测试中达到最先进的性能，在40%压缩率下在6/6基准测试中达到最先进的性能。我们还证明了我们的方法可以扩展较小的模型，通过仅使用大约0.3%的扩展训练令牌，在6/6基准测试中提升了性能，同时额外参数成本最小。|\n",
        "2501.14705": "|**2025-01-24**|**The Karp Dataset**|Mason DiCicco et.al.|[2501.14705](http://arxiv.org/abs/2501.14705)|null|理解大型语言模型（LLMs）的数学推理能力是人工智能研究中的一个核心主题。这个新领域需要创建用于训练和评估LLMs性能的推理任务数据集。为此，我们引入了Karp数据集：这是第一个由NP完备性归约的详细证明组成的数据集。这些归约的难度各异，从本科课程的基本练习到更具挑战性的来自学术论文的归约。我们比较了最先进模型在此任务上的表现，并展示了使用Karp数据集进行微调对推理能力的影响。|\n",
        "2501.14693": "|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693](http://arxiv.org/abs/2501.14693)|null|近期在表格理解领域的研究主要集中在针对表格相关任务对大型语言模型（LLMs）进行指令微调。然而，现有研究忽视了超参数选择的影响，并且缺乏对这些表格LLMs的领域外表格理解能力和一般能力的全面评估。在本文中，我们评估了现有表格LLMs的这些能力，并揭示了与基线模型相比，领域外表格理解能力和一般能力都有显著下降。通过系统分析，我们表明超参数，如学习率，可以显著影响表格特定能力和一般能力。与现有的表格指令微调工作相反，我们证明了较小的学习率和较少的训练实例可以增强表格理解能力，同时保持一般能力。基于我们的发现，我们引入了TAMA，这是一个从LLaMA 3.1 8B Instruct指令微调的表格LLM，它在表格任务上的性能与GPT-3.5和GPT-4相当甚至更好，同时保持了强大的领域外泛化能力和一般能力。我们的发现强调了通过仔细选择超参数来降低数据标注成本和更高效地开发模型的可能性。|\n",
        "2501.14683": "|**2025-01-24**|**An Empirical Study on LLM-based Classification of Requirements-related Provisions in Food-safety Regulations**|Shabnam Hassani et.al.|[2501.14683](http://arxiv.org/abs/2501.14683)|null|随着工业4.0对食品行业的转型，软件在实现食品安全法规合规性方面的作用变得越来越关键。食品安全法规，就像许多法律领域一样，主要以技术无关的方式阐述，以确保其长期性和广泛适用性。然而，这种做法在法规与现代系统和软件之间留下了差距，这些系统和软件被越来越多地用于实施法规。在本文中，我们追求两个主要目标。首先，我们通过对食品安全法规进行扎根理论研究，发展出与系统和软件需求密切相关的食品安全概念的概念性描述。其次，我们检验了两大类大型语言模型（LLM）——BERT和GPT——在根据与要求相关的食品安全概念自动分类法律条文方面的有效性。我们的结果表明：（a）当进行微调时，BERT和GPT家族中表现最佳模型的准确率差异相对较小。然而，在我们的实验中，最强大的模型GPT-4o仍然达到了最高的准确率，平均精确度为89%，平均召回率为87%；（b）使用GPT-4o进行少量样本学习将召回率提高到97%，但将精确度降低到65%，表明了微调和少量样本学习之间的权衡；（c）尽管我们的训练示例仅来自加拿大法规，但基于LLM的分类在来自美国的测试条文中表现始终如一，表明在监管司法管辖范围内具有一定的泛化性；（d）对于我们的分类任务，LLM在性能上显著优于使用长短期记忆（LSTM）网络和自动关键词提取构建的简单基线。|\n",
        "2501.14680": "|**2025-01-24**|**Diffusion based Text-to-Music Generationwith Global and Local Text based Conditioning**|Jisi Zhang et.al.|[2501.14680](http://arxiv.org/abs/2501.14680)|null|基于扩散的文本到音乐（TTM）模型能够根据文本描述生成音乐。通常，基于UNet的扩散模型通过预训练的大型语言模型或跨模态音频语言表示模型生成的文本嵌入进行条件化。本研究提出了一种基于扩散的TTM模型，其中UNet同时依赖于（i）单模态语言模型（例如，T5）通过交叉注意力，以及（ii）跨模态音频语言表示模型（例如，CLAP）通过特征线性调制（FiLM）。该扩散模型被训练以利用来自T5的局部文本表示和来自CLAP的全局表示。此外，我们提出了修改方案，通过我们称之为平均池化和自注意力池化的池化机制从T5中提取全局和局部表示。这种方法减轻了对额外编码器（例如，CLAP）的需求以提取全局表示，从而减少了模型参数的数量。我们的结果表明，将CLAP的全局嵌入结合到T5的局部嵌入中，与仅依赖T5局部嵌入的基线模型（KL=1.54）相比，增强了文本一致性（KL=1.47）。另一方面，通过所提出的平均池化方法直接从T5的局部嵌入中提取全局文本嵌入，在生成质量（FAD=1.89）方面表现出优异表现，而文本一致性（KL=1.51）略低于同时基于CLAP和T5文本嵌入的模型（FAD=1.94和KL=1.47）。我们提出的解决方案不仅高效，而且在所需参数数量方面也是紧凑的。|\n",
        "2501.14654": "|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654](http://arxiv.org/abs/2501.14654)|**[link](https://github.com/stanfordmlgroup/medagentbench)**|**近期，大型语言模型（LLMs）取得了显著的进步，尤其是在作为智能代理方面的能力，从而超越了它们作为聊天机器人的传统角色。这些代理可以利用其规划和工具利用能力来处理高层次指定的任务。然而，目前缺乏一个标准化的数据集来评估LLMs在医疗应用中的智能代理能力，这使得在交互式医疗环境中对LLMs进行复杂任务的评估变得具有挑战性。为了填补这一空白，我们引入了MedAgentBench，这是一个广泛的评估套件，旨在评估大型语言模型在医疗记录环境中的智能代理能力。MedAgentBench包含100个由人类医生编写的、针对特定患者的、来自10个类别的临床任务，以及100个具有超过70万个数据元素的、现实的患者档案，一个符合FHIR标准的交互式环境，以及相应的代码库。该环境使用现代电子病历系统中使用的标准API和通信基础设施，因此可以轻松迁移到实际运行的电子病历系统中。MedAgentBench提供了一个未饱和的智能代理基准，当前最先进的LLMs在其中表现出一定的成功能力。最佳模型（GPT-4o）的成功率为72%。然而，仍有很大的改进空间，为社区提供了下一步优化的方向。此外，不同任务类别之间的性能存在显著差异。MedAgentBench确立了这一点，并在https://github.com/stanfordmlgroup/MedAgentBench上公开发布，为模型开发者提供了一个有价值的框架，以跟踪进展并推动医疗领域大型语言模型智能代理能力的持续改进。**|\n",
        "2501.14649": "|**2025-01-24**|**Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion**|Ziyao Xu et.al.|[2501.14649](http://arxiv.org/abs/2501.14649)|**[link](https://github.com/xzy-xzy/dedc)**|**为了实现通用的、鲁棒的从自然语言到形式语言的转换（N2F），大型语言模型（LLMs）在面对不熟悉的正式语言时需要具备强大的分解和组合能力，并且能够应对组合间隙和反直觉的符号名称。为了调查LLMs是否具备这一组基本能力，我们提出了DEDC框架。该框架半自动地进行样本和任务构建，允许独立评估LLMs在N2F中的分解和组合能力集合。基于此框架，我们评估和分析最先进的LLMs，主要发现包括：（1）LLMs在分解和组合方面都存在不足；（2）LLMs表现出广泛的错误类型，这些错误可以归因于自然语言理解以及符号系统的学习和使用方面的缺陷；（3）组合间隙和反直觉的符号名称都影响了LLMs的分解和组合。我们的工作为研究LLMs在N2F中的分解和组合基本能力提供了新的视角。对不足和归因的详细分析有助于LLMs后续的改进。**|\n",
        "2501.16309": "|**2025-01-27**|**Evaluating The Performance of Using Large Language Models to Automate Summarization of CT Simulation Orders in Radiation Oncology**|Meiyun Cao et.al.|[2501.16309](http://arxiv.org/abs/2501.16309)|null|目的：本研究旨在利用大型语言模型（LLM）来自动生成CT模拟医嘱的摘要并评估其性能。 材料与方法：从我国机构的Aria数据库中收集了607份患者的CT模拟医嘱。使用本地托管的Llama 3.1 405B模型，通过应用程序编程接口（API）服务提取CT模拟医嘱中的关键词并生成摘要。下载的CT模拟医嘱根据治疗方式和疾病部位分为七个组。对于每个组，与治疗师合作开发了一个定制的指令提示，以指导Llama 3.1 405B模型生成摘要。通过仔细审查每个CT模拟医嘱，手动导出相应摘要的基准，并由治疗师进行验证。使用验证的基准作为参考，由治疗师评估LLM生成的摘要的准确性。 结果：约98%的LLM生成的摘要与手动生成的基准在准确性方面一致。我们的评估显示，与相应治疗师生成的摘要相比，LLM生成的摘要在格式上的一致性和可读性方面有所提高。这种自动方法在所有组中表现出一致的性能，无论治疗方式或疾病部位如何。 结论：本研究证明了Llama 3.1 405B模型在提取关键词和总结CT模拟医嘱方面的精度和一致性，表明LLM有巨大的潜力帮助这项任务，减轻治疗师的工作负担并提高工作流程效率。|\n",
        "2501.16303": "|**2025-01-27**|**RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based Video Event Retrieval**|Long Nguyen et.al.|[2501.16303](http://arxiv.org/abs/2501.16303)|null|由于多媒体内容的快速增长，使用文本查询从视频中检索事件变得越来越具有挑战性。现有的基于文本的视频事件检索方法通常过于关注对象级描述，忽视了上下文信息的重要作用。这种局限性在查询缺乏足够上下文时尤为明显，例如缺少位置细节或背景元素模糊。为了应对这些挑战，我们提出了一种名为RAPID（检索增强并行推理草稿）的新型系统，该系统利用大型语言模型（LLMs）和基于提示的学习的进步来对用户查询进行语义修正和丰富，添加相关的上下文信息。这些丰富的查询随后通过并行检索进行处理，然后通过评估步骤根据与原始查询的匹配度选择最相关的结果。通过对我们自行开发的数据库进行的大量实验表明，RAPID显著优于传统的检索方法，尤其是在处理上下文不完整的查询时。我们的系统在速度和准确性方面得到了验证，通过参加2024年胡志明市人工智能挑战赛，成功地从超过300小时的视频中检索了事件。与比赛组织者提出的基线方法进行比较的进一步评估，显示了RAPID卓越的有效性，突显了我们方法的优势和鲁棒性。|\n",
        "2501.16302": "|**2025-01-27**|**Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With Configurable Depth and Width**|Zheng Liu et.al.|[2501.16302](http://arxiv.org/abs/2501.16302)|null|大型语言模型（LLMs）为执行细粒度文本重排序提供了强大的基础。然而，由于计算带宽的限制，它们在现实中往往难以承受。在这项工作中，我们提出了一种名为“套娃重排序器”（Matroyshka Re-Ranker）的灵活架构，该架构旨在根据用户的配置在运行时定制模型层和每层的序列长度。因此，基于LLM的重排序器可以应用于各种现实情况。这种增加的灵活性可能会以精度损失为代价。为了解决这个问题，我们引入了一套优化性能的技术。首先，我们提出了级联自蒸馏，其中每个子架构学习从其超级组件中保留精确的重排序性能，这些组件的预测可以作为平滑且信息丰富的教师信号被利用。其次，我们设计了一种因子化的补偿机制，其中两个协作的低秩自适应模块，垂直和水平，共同用于补偿由层和序列压缩的任意组合引起的精度损失。我们在MSMARCO的段落和文档检索数据集以及BEIR基准的所有公开数据集上进行了全面的实验。在我们的实验中，Matryoshka Re-Ranker在多种压缩形式和不同应用场景中显著优于现有方法，同时有效地保持了其优越的性能。|\n",
        "2501.16300": "|**2025-01-27**|**Large Models in Dialogue for Active Perception and Anomaly Detection**|Tzoulio Chamiti et.al.|[2501.16300](http://arxiv.org/abs/2501.16300)|**[link](https://github.com/Tzoulio/Large_Models_Dialogue_for_Active_Perception)**|自主空中监控是一个旨在从人类难以到达的区域收集信息的重要任务。同时，这项任务通常需要从相当远的距离或过去未曾遇到的情况下识别异常。在本文中，我们提出了一种新颖的框架，该框架利用大型语言模型（LLMs）提供的先进能力来主动收集信息并在新场景中执行异常检测。为此，我们提出了一种基于LLM的模型对话方法，其中两个深度学习模型进行对话以主动控制无人机，提高感知和异常检测的准确性。我们在一个高保真模拟环境中进行了实验，在该环境中，LLM被提供了一组预定义的自然语言移动命令，这些命令映射为可执行的代码函数。此外，我们还部署了一个多模态视觉问答（VQA）模型，该模型负责视觉问答和图像描述。通过让两个模型进行对话，LLM在同时驾驶无人机飞入场景的不同部分时提出探索性问题，从而提供了一种实现主动感知的新方法。通过利用LLMs的推理能力，我们输出了一种改进的场景详细描述，超越了现有的静态感知方法。除了信息收集之外，我们的方法还用于异常检测，我们的结果证明了所提出的方法在通知和警报潜在危险方面的有效性。|\n",
        "2501.16297": "|**2025-01-27**|**FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers**|Renshan Zhang et.al.|[2501.16297](http://arxiv.org/abs/2501.16297)|null|将高分辨率视觉输入纳入，使多模态大型语言模型（MLLMs）具备了增强的视觉感知能力，以应对现实世界的任务。然而，大多数现有的高分辨率MLLMs都依赖于基于裁剪的方法来处理图像，这导致视觉编码碎片化以及冗余标记数量急剧增加。为了解决这些问题，我们提出了FALCON模型。FALCON引入了一种新颖的视觉寄存器技术，同时实现以下目标：1）在视觉编码阶段消除冗余标记。为了直接解决视觉编码器输出中存在的视觉冗余，我们提出了一种基于寄存器的表示压缩（ReCompact）机制。该机制引入了一组可学习的视觉寄存器，旨在自适应地聚合关键信息，同时丢弃冗余。它使得编码器能够以最小数量的输出标记生成更紧凑的视觉表示，从而消除了额外压缩模块的需求。2）确保视觉编码的连续性。为了解决由碎片化视觉输入引起的潜在编码错误，我们开发了一个寄存器交互注意力（ReAtten）模块。该模块通过允许视觉寄存器之间的交互，促进子图像间有效且高效的信息交换。它确保了整个编码过程中的视觉语义连续性。我们在广泛场景下的高分辨率基准测试中对FALCON进行了全面的实验。FALCON展示了卓越的性能，视觉标记数量实现了显著的9倍和16倍减少。|\n",
        "2501.16282": "|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282](http://arxiv.org/abs/2501.16282)|null|理解脑部疾病对于准确的临床诊断和治疗至关重要。近期在多模态大型语言模型（MLLMs）方面的进展为在文本描述的支持下解读医学图像提供了一种有前景的方法。然而，以往的研究主要集中于2D医学图像，对3D图像丰富的空间信息挖掘不足，基于单一模态的方法则因忽视其他模态中包含的关键临床信息而受限。为了解决这一问题，本文提出了一种名为Brain-Adapter的新方法，该方法通过增加一个额外的瓶颈层来学习新知识并将其注入到原始预训练的知识中。主要思想是在训练过程中引入一个轻量级的瓶颈层，以减少参数数量同时捕捉关键信息，并利用对比语言-图像预训练（CLIP）策略在统一表征空间内对多模态数据进行对齐。大量的实验表明，我们的方法在整合多模态数据方面非常有效，显著提高了诊断准确性，同时没有高计算成本，突显了在增强现实世界诊断工作流程中的潜力。|\n",
        "2501.16277": "|**2025-01-27**|**Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation**|Jiayi Hong et.al.|[2501.16277](http://arxiv.org/abs/2501.16277)|**[link](https://github.com/vaderasu/llm4viz-experiments)**|在本文中，我们评估了两种杰出的大型语言模型（LLMs）的可视化素养：OpenAI的生成预训练变压器（GPT），即ChatGPT的后端，以及Google的Gemini（之前被称为Bard），以建立评估其可视化能力的基准。尽管LLMs在生成图表描述、标题和设计建议方面显示出潜力，但它们在评估可视化方面的潜力仍被低估。收集人类数据用于评估在时间和金钱方面一直是可视化研究的瓶颈，如果LLMs能够在某些有限的范围内充当评估者，它们将是一个重要的资源。为了研究在可视化评估过程中使用LLMs的可行性，我们探讨了LLMs拥有可视化素养的程度——这是它们在领域中有效利用的关键因素。我们使用修改后的53项可视化素养评估测试（VLAT）对GPT-4和Gemini进行了一系列实验。我们的发现表明，与我们探索的LLMs相比，目前它们在可视化素养方面未能达到与VLAT中报告的普通公众数据相同的水准，LLMs在回答问题时过度依赖其预先存在的知识，而不是利用可视化提供的信息。|\n",
        "2501.16276": "|**2025-01-27**|**URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT**|Long Nguyen et.al.|[2501.16276](http://arxiv.org/abs/2501.16276)|null|随着人工智能的快速发展，特别是在自然语言处理领域，大型语言模型（LLMs）在教育问答系统中变得至关重要，尤其是在大学招生聊天机器人中。检索增强生成（RAG）等概念和其他高级技术已被开发出来，通过整合特定大学数据来增强这些系统，使LLMs能够提供有关招生和学术咨询的知情回答。然而，这些增强的RAG技术往往涉及高昂的运营成本，并需要训练复杂的专业模块，这给实际部署带来了挑战。此外，在教育环境中，提供准确答案以防止错误信息至关重要，而基于LLM的系统在没有适当策略和方法的情况下发现这项任务具有挑战性。在本文中，我们介绍了统一RAG（URAG）框架，这是一种混合方法，显著提高了回答的准确性，尤其是在关键查询方面。实验结果表明，URAG增强了我们内部轻量级模型，使其能够与最先进的商业模型相媲美。此外，为了验证其实际适用性，我们在我们的教育机构进行了一项案例研究，获得了积极的反馈和赞誉。这项研究不仅证明了URAG的有效性，还突出了其在教育环境中实际实施的可行性。|\n",
        "2501.16255": "|**2025-01-27**|**A foundation model for human-AI collaboration in medical literature mining**|Zifeng Wang et.al.|[2501.16255](http://arxiv.org/abs/2501.16255)|null|系统文献综述对循证医学至关重要，需要全面分析临床试验出版物。然而，由于在广泛的诊疗领域和多样化的任务中缺乏训练和评估，人工智能（AI）模型在医学文献挖掘中的应用受到了限制。在此，我们提出了LEADS，这是一种用于医学文献研究搜索、筛选和数据分析的AI基础模型。该模型在LEADSInstruct的633,759条指令数据点上进行了训练，这些数据点是从21,335篇系统综述、453,625篇临床试验出版物和27,015个临床试验登记处精心挑选的。我们展示了LEADS在六个任务上对四个最前沿的通用大型语言模型（LLM）的一致性改进。此外，LEADS通过在专家请求后提供支持性参考文献，简化了流程，同时保持了高质量的结果，从而增强了专家工作流程。一项包含14个不同机构16位临床医生和医学研究者的研究显示，与单独工作的专家相比，与LEADS协作的专家在研究选择方面的召回率达到了0.81，而单独工作的专家召回率为0.77，节省了22.6%的时间。在数据提取任务中，使用LEADS的专家达到了0.85的准确率，而没有使用LEADS的准确率为0.80，同时节省了26.9%的时间。这些发现突出了专业医学文献基础模型超越通用模型的可能性，当将其集成到医学文献挖掘的专家工作流程中时，可以带来显著的质量和效率收益。|\n",
        "2501.16254": "|**2025-01-27**|**Multi-Agent Geospatial Copilots for Remote Sensing Workflows**|Chaehong Lee et.al.|[2501.16254](http://arxiv.org/abs/2501.16254)|null|我们提出了GeoLLM-Squad，这是一个地理空间协同工作助手，它将新颖的多智能体范式引入遥感（RS）工作流程。与现有的依赖单一大型语言模型（LLM）的单智能体方法不同，GeoLLM-Squad通过将RS任务委派给专门的子智能体，将智能体编排与地理空间任务解决分离。基于开源的AutoGen和GeoLLM-Engine框架，我们的工作实现了不同应用的模块化集成，涵盖了城市监测、林业保护、气候分析和农业研究等领域。我们的结果表明，当RS任务的复杂性增加时，单智能体系统难以扩展，而GeoLLM-Squad保持了稳健的性能，在智能体正确性方面比最先进的基础模型提高了17%。我们的发现突出了多智能体AI在推进RS工作流程中的潜力。|\n",
        "2501.17144": "|**2025-01-28**|**FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data**|Deren Lei et.al.|[2501.17144](http://arxiv.org/abs/2501.17144)|**[link](https://github.com/derenlei/factcg)**|**在之前关于训练基于事实性的分类模型以检测大型语言模型（LLM）中的幻觉的研究中，研究者们主要依赖于公共的自然语言推理（NLI）数据和合成数据。然而，传统的NLI数据集并不适合文档级别的推理，这对于检测LLM幻觉至关重要。近期的一些文档级别合成数据生成方法涉及迭代地从文档中删除句子，并使用基于LLM的提示进行事实性标注。虽然这种方法有效，但对于长文档来说计算成本很高，并且受到LLM能力的限制。在这项工作中，我们分析了现有合成训练数据与真实LLM输出陈述之间的差异。基于我们的发现，我们提出了一种新的合成数据生成方法CG2C，该方法利用从文档中提取的上下文图进行多跳推理。我们的事实核查模型FactCG，在使用相同的骨干模型的基础上，通过更紧密的推理展示了改进的性能。实验表明，它在LLM-Aggrefact基准测试中甚至超过了GPT-4-o，同时模型规模要小得多。**|\n",
        "2501.17132": "|**2025-01-28**|**ASTRAL: Automated Safety Testing of Large Language Models**|Miriam Ugarte et.al.|[2501.17132](http://arxiv.org/abs/2501.17132)|null|大型语言模型（LLMs）因其理解和生成复杂的人类似内容的能力而近期受到关注。然而，确保其安全性至关重要，因为它们可能会提供有害和不安全的回应。现有的LLM测试框架解决了各种与安全相关的问题（例如，毒品、恐怖主义、动物虐待），但往往由于数据集不平衡和过时而面临挑战。在本文中，我们提出了一种名为ASTRAL的工具，该工具自动生成和执行测试用例（即提示），以测试LLMs的安全性。首先，我们引入了一种新的黑盒覆盖标准，以生成平衡且多样化的不安全测试输入，涵盖多样化的安全类别以及语言写作特征（即不同的风格和说服性写作技巧）。其次，我们提出了一种基于LLM的方法，该方法利用检索增强生成（RAG）、少量提示策略和网页浏览来生成最新的测试输入。最后，类似于当前的LLM测试自动化技术，我们利用LLM作为测试或然性来区分安全和不安全的测试输出，从而实现完全自动化的测试方法。我们在知名的LLMs上进行了广泛的评估，揭示了以下关键发现：i）当作为测试或然性时，GPT3.5优于其他LLMs，能够准确地检测到不安全响应，甚至超过了更近期的LLMs（例如，GPT-4）以及专门用于检测不安全LLM输出的LLMs（例如，LlamaGuard）；ii）结果表明，与目前使用的静态数据集相比，我们的方法可以揭露近两倍的不安全LLM行为，使用相同的测试输入数量；iii）我们的黑盒覆盖标准与网页浏览相结合，可以有效地引导LLM生成最新的不安全测试输入，显著增加不安全LLM行为数量。|\n",
        "2501.17116": "|**2025-01-28**|**Optimizing Large Language Model Training Using FP4 Quantization**|Ruizhe Wang et.al.|[2501.17116](http://arxiv.org/abs/2501.17116)|null|随着训练大型语言模型（LLMs）的计算需求不断增长，需要更高效的方法。量化训练通过启用低比特算术运算来降低这些成本，提供了一种有希望的解决方案。虽然FP8精度已经证明了可行性，但利用FP4仍然是一个挑战，因为量化误差显著且表示能力有限。这项工作为LLMs引入了第一个FP4训练框架，通过以下两个关键创新来应对这些挑战：一个用于精确权重更新的可微量化估计器和一种异常值钳位及补偿策略，以防止激活崩溃。为确保稳定性，该框架集成了混合精度训练方案和向量量化。实验结果表明，我们的FP4框架在准确度上与BF16和FP8相当，且退化最小，能够有效扩展到在最多100B个标记上训练的13B参数LLMs。随着支持FP4的下一代硬件的出现，我们的框架为高效的超低精度训练奠定了基础。|\n",
        "2501.17112": "|**2025-01-28**|**Unlocking Transparent Alignment Through Enhanced Inverse Constitutional AI for Principle Extraction**|Carl-Leander Henneking et.al.|[2501.17112](http://arxiv.org/abs/2501.17112)|null|传统的对齐大型语言模型（LLM）的方法，如基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO），依赖于隐含原则，限制了可解释性。宪法人工智能（CAI）提供了一种明确的、基于规则的框架来指导模型输出。在此基础上，我们改进了逆向宪法人工智能（ICAI）算法，该算法从偏好数据集中提取宪法。通过改进原则生成、聚类和嵌入过程，我们的方法提高了从合成数据和真实世界数据集中提取的原则的准确性和泛化能力。虽然上下文对齐产生了适度改进，但我们的结果突出了这些原则促进更透明和适应性对齐方法的潜力，为超越传统微调的未来进步提供了有希望的方向。|\n",
        "2501.17084": "|**2025-01-28**|**Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on Advanced Mathematical Problem-Solving**|Evgenii Evstafev et.al.|[2501.17084](http://arxiv.org/abs/2501.17084)|null|大型语言模型（LLMs）在许多自然语言任务上表现出色，但在复杂数学问题解决、尤其是符号推理和保持输出一致性方面存在困难。本研究评估了10个参数在70亿到80亿之间的LLMs，使用MATH数据集中的945个竞赛级别问题进行测试。重点在于评估它们在推理过程中生成可执行Python代码的能力，涉及超过9,450次代码执行。研究引入了一个使用mistral-large-2411的评价框架，对答案进行5点评分，有助于解决数学符号的不一致性。研究还考察了逐个生成输出对结果精炼的影响。结果显示，在顶级商业模型（gpt-4o-mini，得分83.7%）和最无效的开源模型（open-codestral-mamba:v0.1，得分49.2%）之间存在着显著的34.5%的性能差距。这种差异在数论等复杂领域尤为明显。虽然逐个生成输出略微提高了lama3.1:8b模型的准确性（+0.8%），但也将其代码执行时间减少了36.7%，这突显了效率和精度之间的权衡。研究还注意到，所有模型在难度较大的问题上都表现出准确性较低的一致趋势。尽管使用了受控执行环境，但生成的代码中不到1%是不安全的，3.17%的问题在尝试了10次后仍未解决，这表明混合推理方法可能是有益的。|\n",
        "2501.17039": "|**2025-01-28**|**Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models**|Minghan Li et.al.|[2501.17039](http://arxiv.org/abs/2501.17039)|null|近年来，大型语言模型（LLMs）在包括信息检索在内的多个领域展现了卓越的能力。大多数以往的做法都是利用这些模型为每个查询、每段文本或每份文档单独创建一个嵌入表示，这种策略在检索增强生成（RAG）框架中得到体现并得到应用。尽管这种方法已被证明是有效的，但我们认为，由于它依赖于相对粗糙的表示，因此它在充分捕捉文档级文本的细微复杂性和层次结构方面存在不足。为了解决这一局限性，我们提出了一种新颖的细粒度方法，旨在提高长文档的相关性评分准确性。我们的方法首先将长文档划分为块，每个块都使用LLM进行嵌入以与查询表示进行匹配。在计算相关性评分时，我们通过加权求和方法汇总查询-块相关性评分，从而为整个文档生成一个全面的查询评分。尽管这种方法看似简单，但我们的实验发现，这种方法优于标准表示方法，并且实现了嵌入生成延迟的显著降低。此外，通过精心优化成对损失函数，实现了更优越的性能。|\n",
        "2501.17030": "|**2025-01-28**|**Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies**|Manojkumar Parmar et.al.|[2501.17030](http://arxiv.org/abs/2501.17030)|null|大型语言模型（LLMs）在推理、对齐和特定任务性能方面取得了显著的进步。然而，确保这些系统的无害性仍然是一个关键挑战，尤其是在DeepSeek-R1这样的高级模型中。本文考察了强化学习（RL）作为减少DeepSeek-R1有害输出的主要方法的局限性，并将其与监督微调（SFT）进行了比较。虽然RL提高了推理能力，但它面临奖励黑客攻击、泛化失败、语言混合和高计算成本等挑战。我们提出了结合RL和SFT的混合训练方法，以实现稳健的无害性降低。还提出了DeepSeek-R1的安全部署的使用建议和未来方向。|\n",
        "2501.17024": "|**2025-01-28**|**Automated Refactoring of Non-Idiomatic Python Code: A Differentiated Replication with LLMs**|Alessandro Midolo et.al.|[2501.17024](http://arxiv.org/abs/2501.17024)|**[link](https://github.com/alemidolo/gptidiomrefactoring)**|在Python生态系统中，由于惯用构造的表达能力、提高生产力和甚至效率，其采用得到了促进，尽管关于熟悉度或可理解性的争议。最近的研究贡献提出了基于静态代码分析和转换的方法，以自动识别并将非惯用代码重构为惯用代码的机会。鉴于大型语言模型（LLMs）最近在代码相关任务中提供的潜力，在本文中，我们展示了复制研究的结果，其中我们调查了GPT-4在推荐和建议惯用重构操作方面的有效性。我们的结果表明，GPT-4不仅有效地识别了惯用构造，而且在提出重构操作方面经常超越基准，而现有的基线未能实现。对随机样本的手动分析显示了获得的建议的正确性。我们的发现强调了LLMs在实现以往需要基于复杂代码分析的推荐器任务中的潜力。|\n",
        "2501.17022": "|**2025-01-28**|**Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement**|Kei Katsumata et.al.|[2501.17022](http://arxiv.org/abs/2501.17022)|**[link](https://github.com/keio-smilab24/mmig)**|我们考虑了根据目标物体图像和容器图像生成自由形式移动操作指令的问题。传统的图像标题模型无法生成适当的指令，因为它们的架构通常是针对单张图像进行优化的。在本研究中，我们提出了一种模型，该模型能够同时处理目标物体和容器，以生成用于移动操作任务的自由形式指令句子。此外，我们引入了一种新颖的训练方法，该方法有效地将基于学习型和基于n-gram的自动评估指标得分作为奖励。这种方法使得模型能够学习词语之间的共现关系和适当的释义。结果显示，我们提出的方法在标准自动评估指标上优于基线方法，包括代表性的多模态大型语言模型。此外，物理实验表明，使用我们的方法增强语言指令数据可以提高现有多模态语言理解模型在移动操作方面的性能。|\n",
        "2501.16998": "|**2025-01-28**|**Large Language Models for Code Generation: The Practitioners Perspective**|Zeeshan Rasheed et.al.|[2501.16998](http://arxiv.org/abs/2501.16998)|**[link](https://github.com/gpt-laboratory/llm-evaluation)**|大型语言模型（LLMs）已成为编码助手，能够根据自然语言提示生成源代码。随着LLMs在软件开发中的广泛应用，学术界和行业项目正在开发各种工具、基准和指标来评估LLM生成代码的有效性。然而，缺乏通过实证方法评估、结合从业者视角来评估实际应用中功能、语法和准确性的解决方案。为了解决这一差距，我们提出并开发了一个多模型统一平台，该平台基于自然语言提示生成和执行代码。我们进行了一项调查，调查了来自四大洲11个国家的60名软件从业者，他们在不同的专业角色和领域工作，以评估每个模型的可用性、性能、优势和局限性。结果呈现了从业者的反馈和见解，包括LLMs在软件开发中的优缺点、基准和指标忽视的关键方面，以及对其实际应用的更广泛理解。这些发现可以帮助研究人员和从业者就系统性地选择和使用LLMs在软件开发项目中做出明智的决定。未来的研究将专注于将更多样化的模型集成到所提出的系统中，纳入额外的案例研究，并开展开发者访谈，以深入了解LLM驱动的软件开发。|\n",
        "2501.17840": "|**2025-01-29**|**Learning Beyond the Surface: How Far Can Continual Pre-Training with LoRA Enhance LLMs' Domain-Specific Insight Learning?**|Pouya Pezeshkpour et.al.|[2501.17840](http://arxiv.org/abs/2501.17840)|**[link](https://github.com/megagonlabs/insight_miner)**|**大语言模型（LLMs）在各种任务上展现出卓越的性能，但它们从特定领域数据集中提取和吸收更深入见解的能力仍未得到充分探索。在这项研究中，我们探讨了持续预训练如何提升LLMs在三种不同形式的见解学习上的能力：陈述性、统计性和概率性见解。聚焦于两个关键领域：医学和金融，我们使用LoRA对LLMs进行训练，基于两个现有数据集。为了评估每种见解类型，我们创建了基准来衡量持续预训练如何帮助模型超越表面知识。我们还评估了文档修改对捕捉见解的影响。结果表明，虽然对原始文档进行持续预训练有一定的影响，但修改文档以保留关键信息显著增强了LLMs的见解学习能力。**|\n",
        "2501.17799": "|**2025-01-29**|**Leveraging Multimodal LLM for Inspirational User Interface Search**|Seokhyeon Park et.al.|[2501.17799](http://arxiv.org/abs/2501.17799)|**[link](https://github.com/spark-damian/s-ui)**|**灵感搜索，即探索设计以启发新的创意工作的过程，在移动用户界面（UI）设计中至关重要。然而，探索庞大的UI参考空间仍然是一个挑战。现有的基于AI的UI搜索方法往往忽略了关键的语义，如目标用户或应用程序的情绪。此外，这些模型通常需要诸如视图层次结构等元数据，限制了其实际应用。我们使用多模态大型语言模型（MLLM）从移动UI图像中提取和解释语义。通过形式研究，我们确定了关键的UI语义，并开发了一个基于语义的UI搜索系统。通过计算和人工评估，我们证明了我们的方法显著优于现有的UI检索方法，为UI设计师提供了更加丰富和与上下文相关的搜索体验。我们增强了移动UI设计语义的理解，并突出了MLLM在灵感搜索中的潜力，为未来的研究提供了一个丰富的UI语义数据集。**|\n",
        "2501.17790": "|**2025-01-29**|**BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights**|Chan-Jan Hsu et.al.|[2501.17790](http://arxiv.org/abs/2501.17790)|null|我们提出了BreezyVoice，一个专为台湾普通话设计的文本到语音（TTS）系统，强调了其音素控制能力，以解决该语言中多音节的歧义问题。基于CosyVoice，我们引入了$S^{3}$分词器、一个大语言模型（LLM）、最优传输条件流匹配模型（OT-CFM）和字符到音素预测模型，以生成接近人类口语的逼真语音。我们的评估显示了BreezyVoice在一般和代码转换情境下的卓越性能，突显了其在生成高保真语音方面的鲁棒性和有效性。此外，我们还解决了在建模长尾说话者和多音节歧义方面的挑战。我们的方法显著提升了性能，并为神经编解码器TTS系统的工作原理提供了宝贵的见解。|\n",
        "2501.17784": "|**2025-01-29**|**AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing**|Peter Pak et.al.|[2501.17784](http://arxiv.org/abs/2501.17784)|null|在这项工作中，我们研究了大型语言模型在给定一组工艺参数输入的情况下预测增材制造缺陷区域的能力。为此，我们利用工艺参数缺陷数据集微调了一系列模型，命名为AdditiveLLM，以预测潜在的缺陷区域，包括键孔、熔合不足和球化。我们比较了不同的输入格式化方法，以评估模型在稀疏的基线数据集和自然语言提示数据集上正确预测缺陷区域的能力。该模型表现出强大的预测能力，在要求提供与一组工艺参数相关的缺陷区域时，准确率达到93%。自然语言输入的引入进一步简化了工艺参数选择的过程，使用户能够识别出针对其构建的最佳设置。|\n",
        "2501.17771": "|**2025-01-29**|**2SSP: A Two-Stage Framework for Structured Pruning of LLMs**|Fabrizio Sandri et.al.|[2501.17771](http://arxiv.org/abs/2501.17771)|**[link](https://github.com/fabriziosandri/2ssp)**|我们提出了一种新颖的两阶段框架（2SSP）用于结构化剪枝，用于剪枝大型语言模型（LLMs），该框架结合了两种不同的剪枝策略，即宽度剪枝和深度剪枝。第一阶段（宽度剪枝）通过移除整个神经元及其对应的行和列，旨在保留在各个Transformer块中前馈网络中间状态中被剪枝结构的连通性。这是基于一个重要性分数来衡量每个神经元对输出幅度的冲击。第二阶段（深度剪枝）则移除整个注意力子模块。这是通过应用一个迭代过程来实现的，该过程移除对特定指标（在我们的案例中是困惑度）影响最小的注意力子模块。我们还提出了一种新颖的机制来平衡两个阶段的稀疏率，以符合期望的全局稀疏率。我们在四个LLM家族和三种稀疏率（25%、37.5%和50%）上测试了2SSP，测量了在三个语言建模数据集上的困惑度以及六个下游任务上的性能。我们的方法在三个语言建模和六个下游任务上始终优于五种最先进的竞争对手，在剪枝时间上最多提高了两个数量级。代码可在以下网址获取：\\url{https://github.com/FabrizioSandri/2SSP}。|\n",
        "2501.17767": "|**2025-01-29**|**Hybrid Graphs for Table-and-Text based Question Answering using LLMs**|Ankush Agarwal et.al.|[2501.17767](http://arxiv.org/abs/2501.17767)|null|解决需要在对结构化（表格）和未结构化（原始文本）数据源进行推理和汇总的问题带来了重大挑战。现有方法依赖于微调和高质量、人工编辑的数据，而这些数据的获取是困难的。近期在大语言模型（LLMs）方面的发展已显示出在单源文本数据零样本设置下进行多跳问答（QA）的潜力，然而对于多源表格-文本问答的探索仍然有限。在本文中，我们提出了一种基于混合图的表格-文本问答新型方法，该方法无需微调即可利用LLMs。我们的方法通过从文本和表格数据构建统一的混合图，基于输入问题修剪信息，以简明扼要的方式为LLMs提供相关上下文。我们在具有挑战性的混合QA和OTT-QA数据集上使用最先进的LLMs，包括GPT-3.5、GPT-4和LLaMA-3，对我们的方法进行了评估。我们的方法在这两个数据集上都取得了最佳的无样本性能，在Hybrid-QA上的精确匹配得分提高了最多10%，在OTT-QA上提高了5.4%。此外，与原始上下文相比，我们的方法将标记的使用减少了高达53%。|\n",
        "2501.17752": "|**2025-01-29**|**On the Partitioning of GPU Power among Multi-Instances**|Tirth Vamja et.al.|[2501.17752](http://arxiv.org/abs/2501.17752)|null|高效管理云数据中心中的电力对于降低成本、提高性能和最小化环境影响至关重要。GPU对于机器学习（ML）和通用人工智能（GenAI）等任务至关重要，是电力消耗的主要贡献者。NVIDIA的多实例GPU（MIG）技术通过实现具有分区级资源跟踪的独立分区来提高GPU利用率，促进了多个租户之间的GPU共享。然而，由于缺乏硬件支持，在MIG实例之间准确分配GPU电力消耗仍然具有挑战性。本文通过开发软件方法来估算每个MIG分区的电力使用量来应对这一挑战。我们分析了NVIDIA GPU利用率指标，发现具有良好准确性的轻量级方法难以构建。因此，我们探索了基于机器学习的电力模型的使用，以实现准确的分区级电力估算。我们的发现表明，单一的通用离线电力模型或建模方法不适用于各种工作负载，尤其是在并发使用MIG的情况下，并且使用正在执行的工作负载的分区级利用率指标构建的在线模型可以显著提高准确性。我们使用NVIDIA A100 GPU，为包括矩阵乘法和大型语言模型推理在内的工作负载展示了这种准确的分区级电力估算方法，有助于实现透明和公平的碳排放报告。|\n",
        "2501.17749": "|**2025-01-29**|**Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation**|Aitor Arrieta et.al.|[2501.17749](http://arxiv.org/abs/2501.17749)|null|大型语言模型（LLMs）已成为我们日常生活的integral（核心）部分。然而，它们带来了一些风险，包括可能损害个人隐私、延续偏见和传播错误信息等。这些风险突显了对强大的安全机制、道德规范和全面测试的必要性，以确保它们负责任地部署。LLMs的安全是其需要在模型部署和向普通用户开放前彻底测试的关键特性。本文报告了蒙东大学和塞维利亚大学的学者在OpenAI早期安全测试项目中对外测试OpenAI的新款o3-mini LLM的经历。具体而言，我们应用我们的工具ASTRAL，自动和系统性地生成最新的不安全测试输入（即提示），帮助我们测试和评估LLMs的不同安全类别。我们在早期的o3-mini beta版本上自动生成了和执行了总计10,080个不安全的测试输入。在手动验证由ASTRAL归类为不安全的测试用例后，我们识别出总计87个实际的不安全LLM行为实例。我们强调了在OpenAI最新LLMs部署前测试阶段所揭露的关键洞察和发现。|\n",
        "2501.17725": "|**2025-01-29**|**Using Code Generation to Solve Open Instances of Combinatorial Design Problems**|Christopher D. Rosin et.al.|[2501.17725](http://arxiv.org/abs/2501.17725)|**[link](https://github.com/constructive-codes/cpro1)**|**《组合设计手册》收录了许多类型的组合设计，并列出了尚未确定存在性的开放实例。我们开发了一种构造性协议CPro1，该协议使用大型语言模型（LLMs）生成构建组合设计的代码，并解决了一些这些开放实例。该协议从特定类型设计的定义和一个可靠地确认所提议设计是否有效的验证器开始。LLM选择策略并在代码中实现它们，而脚手架则使用验证器提供自动超参数调整和执行反馈。大多数生成的代码都失败了，但通过生成许多候选方案，该协议自动化了各种标准方法（例如模拟退火、遗传算法）的探索，并尝试了各种变化（例如成本函数）以找到成功的方法。在16种不同类型的设计中测试，CPro1为其中6种构建了解决开放实例的方案：对称和斜对称权重矩阵、等距排列数组、打包数组、平衡三值设计和佛罗伦萨矩形。**|\n",
        "2501.17715": "|**2025-01-29**|**RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts**|Eujeong Choi et.al.|[2501.17715](http://arxiv.org/abs/2501.17715)|**[link](https://github.com/boychaboy/ricota)**|**在高度监管的大型语言模型（LLMs）时代，用户与对话代理（CAs）的互动正在演变。随着用户超越程序边界去探索和建立与这些系统的关系，人们越来越关注未经授权的访问或操纵的可能性，这通常被称为“越狱”。此外，对于具有高度人类特征的CAs，用户表现出发起亲密性互动或试图驯服其聊天机器人的倾向。为了捕捉和反映这些野外交互到聊天机器人设计中，我们提出了RICoTA，这是一个由609个挑战LLMs的野外用户制作的对话组成的韩国红队数据集，这些对话捕捉了越狱尝试。我们利用在韩国类似Reddit的社区中自行发布的用户-聊天机器人对话，这些对话包含针对社交聊天机器人的特定测试和游戏意图。通过这些提示，我们旨在评估LLMs识别对话类型和用户测试目的的能力，以推导出减轻越狱风险的聊天机器人设计启示。我们的数据集将通过GitHub公开提供。**|\n",
        "2501.18585": "|**2025-01-30**|**Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs**|Yue Wang et.al.|[2501.18585](http://arxiv.org/abs/2501.18585)|null|大型语言模型（LLMs）如OpenAI的o1通过扩展测试时的计算能力并展现出类似人类的深度思考，在复杂的推理任务中展现出了惊人的能力。然而，我们识别出一种我们称之为“浅思”的现象，其中类似于o1的LLMs在达到正确解决方案之前，经常在不同推理思路之间切换，而没有充分探索有希望的路径。这种行为导致推理深度不足，性能下降，尤其是在具有挑战性的数学问题上。为了系统地分析这个问题，我们在三个具有挑战性的测试集和两个代表性的开源o1类似模型上进行了实验，发现频繁的思路切换与错误回答相关。我们引入了一个新的指标来量化“浅思”，通过测量错误答案中的标记效率。为了解决“浅思”问题，我们提出了一种带有思路切换惩罚TIP的解码策略，该策略旨在阻止过早的思维转换，鼓励对每个推理路径进行更深入的探索。实验结果表明，我们的方法在不要求模型微调的情况下，提高了具有挑战性的数据集上的准确率。我们的发现有助于理解类似于o1的LLMs中的推理低效问题，并为提高其解决问题的能力提供了一个实用的解决方案。|\n",
        "2501.18576": "|**2025-01-30**|**Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for Multi-Step Reasoning Over Speed in MATH**|Evgenii Evstafev et.al.|[2501.18576](http://arxiv.org/abs/2501.18576)|null|本项研究调查了DeepSeek R1语言模型在30个来自MATH数据集的具有挑战性的数学问题上的性能，这些问题在其他模型在时间限制下已经证明无法解决。与以往的研究不同，本研究去除了时间限制，以探索DeepSeek R1的架构，该架构以其基于token的推理能力而闻名，是否可以通过多步骤过程实现准确的解决方案。该研究将DeepSeek R1与其他四种模型（gemini-1.5-flash-8b、gpt-4o-mini-2024-07-18、llama3.1:8b和mistral-8b-latest）在11个温度设置下进行了比较。结果表明，DeepSeek R1在这些复杂问题上实现了更高的准确率，但生成的token数量比其他模型显著更多，证实了其token密集型方法。这些发现突显了在大型语言模型进行数学问题解决时准确性和效率之间的权衡：虽然DeepSeek R1在准确率方面表现出色，但其对大量token生成的依赖可能并不适用于需要快速响应的应用。该研究强调了在选择LLM时考虑特定任务需求的重要性，并强调了温度设置在优化性能中的作用。|\n",
        "2501.18565": "|**2025-01-30**|**BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos**|Lehao Lin et.al.|[2501.18565](http://arxiv.org/abs/2501.18565)|null|近年来，人工智能（AI）特别是多模态大型语言模型（MLLMs）的快速发展，使得AI能够理解文本、图像、视频和其他多媒体数据，允许AI系统根据人类提供的提示执行各种任务。然而，AI驱动的机器人越来越多地能够绕过大多数现有的CAPTCHA系统，对网络应用构成了重大的安全威胁。这使得设计新的CAPTCHA机制成为一项紧迫的任务。我们观察到，人类对视频中的位移和突然变化非常敏感，而当前的AI系统在理解和有效应对这些情况方面仍然存在困难。基于这一观察，我们设计和实现了BounTCHA，这是一种利用人类对视频转换和中断中边界感知的CAPTCHA机制。通过利用AI扩展原始视频的能力，我们引入了意想不到的转折和变化，为生成用于CAPTCHA目的的短视频创建了一个管道。我们开发了一个原型并进行了实验，收集了人类在边界识别上的时间偏差数据。这些数据作为区分人类用户和机器人的依据。此外，我们对BounTCHA进行了详细的安全分析，证明了它对各种类型攻击的抵抗能力。我们希望BounTCHA能够作为一项强大的防御措施，在AI驱动时代保护数百万网络应用。|\n",
        "2501.18542": "|**2025-01-30**|**Semantic Web and Creative AI -- A Technical Report from ISWS 2023**|Raia Abu Ahmad et.al.|[2501.18542](http://arxiv.org/abs/2501.18542)|null|国际语义网研究学校（ISWS）是一个为期一周的密集项目，旨在让参与者深入该领域。本文报告了由十支学生团队在ISWS 2023期间进行的一项协作努力，每个团队都由一位资深研究员作为导师指导。每个团队都从不同的角度探讨了创意人工智能的主题，并以一组研究问题作为他们研究的主要对象。2023年ISWS的焦点是语义网技术和创意人工智能的交汇点。ISWS 2023探讨了语义网技术和创意人工智能之间的各种交汇点。重点关注领域是LLMs作为知识工程支持工具的潜力。参与者还深入探讨了LLMs的多方面应用，包括创意内容生产的法律方面、人机交互、多模态生成AI模型的去中心化方法、纳米出版物、AI个人科学知识图谱、自动故事和叙事完成中的常识知识、艺术批评的生成AI、提示工程、自动音乐创作、常识原型和概念融合以及隐性知识的激发。随着大型语言模型和语义技术的不断发展，新的激动人心的前景正在出现：一个创意表达和事实知识之间的界限越来越模糊的世界，这将导致一个既具有信息性又具有启发性的知识世界。|\n",
        "2501.18536": "|**2025-01-30**|**Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges**|Manveer Singh Tamber et.al.|[2501.18536](http://arxiv.org/abs/2501.18536)|**[link](https://github.com/manveertamber/content_injection_attacks)**|**考虑这样一种场景：用户在搜索信息时，却遭遇了充斥着误导性或无关内容的文本。这种场景体现了神经网络信息检索（IR）管道中的一个简单但强大的漏洞：内容注入攻击。我们发现，用于检索、重排序和大型语言模型（LLM）相关度判断的嵌入模型都容易受到这些攻击，攻击者通过在段落中插入误导性文本来操纵模型判断。我们确定了两个主要威胁：（1）在看似具有欺骗性“相关”的段落中插入无关或有害内容，以及（2）将整个查询或关键查询术语插入段落以提升其感知的相关性。虽然第二种策略在先前的研究中已被探讨，但据我们所知，我们首次对第一种威胁进行了实证分析，展示了最先进的模型如何轻易被误导。我们的研究系统地考察了影响攻击成功因素，例如注入内容的位置以及相关和非相关材料的平衡。此外，我们还探讨了各种防御策略，包括对抗性段落分类器、检索器微调以降低操纵内容的影响以及提示LLM判断者采取更加谨慎的方法。然而，我们发现这些对策往往涉及权衡，为了攻击的鲁棒性牺牲了有效性，有时甚至惩罚了合法文档。我们的发现强调了对抗这些不断发展的对抗策略，以保持IR系统可信性的必要性。我们发布了我们的代码和脚本，以促进进一步的研究。**|\n",
        "2501.18532": "|**2025-01-30**|**Differentially Private Steering for Large Language Model Alignment**|Anmol Goel et.al.|[2501.18532](http://arxiv.org/abs/2501.18532)|**[link](https://github.com/ukplab/iclr2025-psa)**|**将大型语言模型（LLMs）与人类价值观对齐并避免不良行为（如幻觉）变得越来越重要。最近，通过激活编辑引导LLMs走向期望的行为已成为缓解推理时有害生成的有效方法。激活编辑通过保留正面演示（例如，真实）的信息并最小化负面演示（例如，幻觉）的信息来修改LLMs的表示。当这些演示来自私有数据集时，对齐后的LLM可能会泄露包含在那些私有样本中的私人信息。在本工作中，我们首次研究了将LLM行为与私有数据集对齐的研究。我们的工作提出了名为“私有引导大型语言模型对齐（PSA）”的算法，以具有差分隐私（DP）保证的方式来编辑LLM激活。我们在七个不同的基准上进行了大量实验，使用了不同大小（0.5B到7B）和模型家族（LlaMa、Qwen、Mistral和Gemma）的开源LLMs。我们的结果表明，PSA在性能损失最小的情况下实现了LLM对齐的DP保证，包括对齐指标、开放式文本生成质量和通用推理。我们还开发了第一个用于评估和审计通过激活编辑引导LLM问题的实证隐私的成员推断攻击（MIA）。我们的攻击针对激活编辑，仅依赖于生成的文本而不依赖于其相关的概率。我们的实验通过显示我们的PSA算法相对于几种现有非隐私技术改进了保证来支持理论保证。**|\n",
        "2501.18516": "|**2025-01-30**|**Learn from the Past: Language-conditioned Object Rearrangement with Large Language Models**|Guanqun Cao et.al.|[2501.18516](http://arxiv.org/abs/2501.18516)|null|物体重组是协作机器人的一项重要任务，它们被引导操纵物体达到指定的目标状态。确定物体的放置位置是影响重组过程效率的主要挑战。大多数当前方法严重依赖预先收集的数据集来训练模型以预测目标位置，并且受到特定指令的限制，这限制了它们的更广泛适用性和有效性。在本文中，我们提出了一种基于大型语言模型（LLM）的语言条件物体重组框架。特别是，我们的方法通过使用过去的成功经验作为参考来推断所需的最终目标位置，模仿人类的推理过程。基于LLM强大的自然语言理解和推理能力，我们的方法可以无监督地泛化处理各种日常物体和自由形式的语言指令。实验结果表明，我们的方法可以有效地执行机器人重组任务，甚至包括涉及长序列顺序的任务。|\n",
        "2501.18512": "|**2025-01-30**|**Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch**|Arthur Douillard et.al.|[2501.18512](http://arxiv.org/abs/2501.18512)|null|大型语言模型（LLMs）的训练通常分布在大量的加速器上以缩短训练时间。由于在每个梯度步骤中都需要交换内部状态和参数梯度，所有设备都需要使用低延迟高带宽的通信链路进行集中，以支持所需的大量交换比特。最近，像DiLoCo这样的分布式算法放松了这种集中放置的限制：加速器可以被分组为“工作者”，工作者之间的同步仅在不频繁的情况下发生。这反过来意味着工作者可以由更低带宽的通信链路连接，而不会影响学习质量。然而，在这些方法中，工作者之间的通信仍然需要与之前相同的峰值带宽，因为同步需要所有参数在所有工作者之间进行交换。在本文中，我们从三个方面改进了DiLoCo。首先，我们按顺序同步参数的子集，而不是一次性同步所有参数，这大大降低了峰值带宽。其次，我们允许工作者在同步时继续训练，这减少了实际时间。第三，我们对工作者之间交换的数据进行量化，这进一步减少了工作者之间的带宽。通过适当组合这些修改，我们通过实验表明，我们可以分配数十亿参数规模的训练，并达到类似的质量，但所需的带宽降低了两个数量级。|\n",
        "2501.18504": "|**2025-01-30**|**CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction**|Peter J. Bentley et.al.|[2501.18504](http://arxiv.org/abs/2501.18504)|null|大语言模型（LLM）图像识别是一种从图像中提取数据的有力工具，但其准确性取决于在提示中提供足够的线索——需要领域专家来完成专业任务。我们引入了Cue Learning using Evolution for Accurate Recognition（CLEAR），它结合了LLM和进化计算来生成和优化线索，从而提高图像中特定特征的识别。它通过自动生成新颖的领域特定表示，然后使用遗传算法来优化合适的文本线索来实现这一目标。我们将CLEAR应用于从建筑内外部图像中识别可持续性数据的现实世界任务。我们研究了使用可变长度表示与固定长度表示相比的效果，并展示了如何通过从分类估计重构为实值估计来提高LLM的一致性。我们表明，CLEAR在所有任务中都能实现比专家人类识别和人类撰写的提示更高的准确性，错误率降低了两个数量级，并且消融研究表明了解决方案的高度简洁性。|\n",
        "2501.18482": "|**2025-01-30**|**A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models**|Changshu Liu et.al.|[2501.18482](http://arxiv.org/abs/2501.18482)|null|代码执行推理正成为评估大型语言模型（LLMs）在编程任务能力的新非功能性指标。最先进的框架（如CodeMind或REval）和基准测试（如CruxEval）通常关注LLMs在有限程序中对给定代码的输入/输出或中间变量状态/值的预测。然而，目前还没有工具能够进行更深入的结果分析。没有这样的工具，关于LLMs代码执行推理的观察就无法推广到更多数据集，这将阻碍研究社区和从业者设计出具有更好代码执行推理能力的下一代LLMs。本文介绍了ExeRScope，这是一系列工具和启发式方法，用于分析代码执行推理框架的结果，以更好地理解研究基准中代码属性对代码执行推理的影响。有了这样的工具，分析可以推广到具有相似属性的代码，而无需迫切地设计更多基准测试，这本身就是一项繁琐的工作。|\n",
        "2501.19400": "|**2025-01-31**|**Vintix: Action Model via In-Context Reinforcement Learning**|Andrey Polubarov et.al.|[2501.19400](http://arxiv.org/abs/2501.19400)|**[link](https://github.com/dunnolab/vintix)**|**在上下文强化学习（ICRL）中，代表了一种有前景的范式，用于开发在推理时通过试错交互学习的通用代理，类似于大型语言模型如何上下文自适应，但专注于奖励最大化。然而，将ICRL扩展到玩具任务和单一领域设置之外的可扩展性仍然是一个未解决的挑战。在本工作中，我们通过引入一个固定、跨领域的模型，该模型能够通过上下文强化学习学习行为，迈出了将ICRL扩展的第一步。我们的结果表明，算法蒸馏（Algorithm Distillation）这一旨在促进ICRL的框架，为构建多才多艺的动作模型提供了一种引人注目且具有竞争力的替代方案。这些发现突出了ICRL作为通用决策系统可扩展方法的可能性。代码将在https://github.com/dunnolab/vintix发布。**|\n",
        "2501.19398": "|**2025-01-31**|**Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game**|Mustafa O. Karabag et.al.|[2501.19398](http://arxiv.org/abs/2501.19398)|**[link](https://github.com/mustafakarabag/llmchameleon)**|**基于大型语言模型（LLM）的智能体在包含非合作方的环境中变得普遍。在这样的环境中，智能体的决策需要隐藏信息以避免对手得知，向合作伙伴透露信息，以及推断信息以识别其他智能体的特征。为了调查LLM是否具备这些信息控制和决策能力，我们让LLM智能体参与基于语言的隐蔽身份游戏《变色龙》。在这个游戏中，一群彼此不认识的非变色龙智能体试图在不泄露秘密的情况下识别变色龙智能体。游戏需要上述信息控制能力，无论是作为变色龙还是非变色龙。实证结果表明，虽然非变色龙LLM智能体能够识别变色龙，但它们未能从变色龙那里隐藏秘密，并且它们的获胜概率远低于最简单的策略水平。为了正式解释这种行为，我们对从隐藏到揭示的一系列策略进行了理论分析，并给出了非变色龙获胜概率的上限。基于实证结果和对不同策略的理论分析，我们推断基于LLM的非变色龙智能体向未知身份的智能体透露了过多的信息。我们的结果表明，包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet在内的当代LLM在战略互动中存在弱点。**|\n",
        "2501.19392": "|**2025-01-31**|**Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models**|Alina Shutova et.al.|[2501.19392](http://arxiv.org/abs/2501.19392)|**[link](https://github.com/goodevening13/aquakv)**|高效的现实世界部署大型语言模型（LLMs）依赖于键值（KV）缓存来处理和生成长输出，从而减少重复计算的需求。对于大型上下文，键值缓存可能占用数十吉字节设备内存，因为它们存储每个标记和层的向量表示。最近的研究表明，缓存的向量可以通过量化、剪枝或合并进行压缩，但这些技术通常在提高压缩率的同时牺牲质量。在这项工作中，我们旨在通过利用两个观察结果来改进键值压缩：1）不同层之间键和值之间的固有依赖关系；2）内部网络状态的高压缩机制。我们提出了AQUA-KV，这是一种针对键值缓存的自适应量化方法，它依赖于紧凑的适配器来利用键和值之间现有的依赖关系，并旨在“最优”地压缩无法预测的信息。AQUA-KV显著提高了压缩率，同时在最先进的LLM系列上保持了高精度。在Llama 3.2 LLMs上，我们实现了接近无损推理，每个值的位数为2-2.5位，在困惑度和LongBench得分上相对误差低于1%。AQUA-KV是一次性的、简单的和高效的：它可以在1-6小时内在一个GPU上校准，即使是70B模型也是如此。|\n",
        "2501.19389": "|**2025-01-31**|**Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models**|Wenzhi Fang et.al.|[2501.19389](http://arxiv.org/abs/2501.19389)|null|近年来，在设备上微调大型语言模型（LLMs）越来越受到关注。近期的研究将低秩自适应（LoRA）技术与联邦微调相结合，以缓解与设备模型大小和数据稀缺相关的问题。然而，计算资源的不一致性仍然是一个关键瓶颈：虽然高秩模块通常能提升性能，但不同的设备能力限制了LoRA的可实现秩范围。现有的试图解决这个问题的方法要么缺乏分析上的依据，要么增加了额外的计算开销，留下了一个对高效且理论上有根据的解决方案的巨大差距。为了应对这些挑战，我们提出了联邦草图LoRA（FSLoRA），它利用草图机制使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比率，即确定设备上子矩阵的秩，FSLoRA灵活地适应了特定于设备的通信和计算约束。我们对FSLoRA提供了严格的收敛性分析，描述了草图比率如何影响收敛速度。通过在多个数据集和LLM模型上的综合实验，我们证明了FSLoRA相对于各种基线具有优越的性能。|\n",
        "2501.19377": "|**2025-01-31**|**SELMA: A Speech-Enabled Language Model for Virtual Assistant Interactions**|Dominik Wagner et.al.|[2501.19377](http://arxiv.org/abs/2501.19377)|null|在本文中，我们提出并评估了SELMA，这是一个用于虚拟助手交互的语音启用语言模型，它将音频和文本作为输入整合到大型语言模型（LLM）中。SELMA旨在同时在一个端到端模型中处理与虚拟助手交互相关的三个主要任务和两个辅助任务。我们采用低秩自适应模块来高效训练音频编码器和LLM。此外，我们还实现了一种特征池化策略，使系统能够识别全局模式并提高对依赖于单个序列元素的任务准确性。在语音触发（VT）检测、设备指向性语音检测（DDSD）和自动语音识别（ASR）实验结果中，我们的方法显著简化了虚拟助手典型的输入处理流程，并且与针对每个单独任务的专用模型相比，性能也得到了提升。在VT检测任务中，SELMA实现了相对误码率的64%提升，在DDSD任务中提升了22%，同时单词错误率也接近基线。|\n",
        "2501.19361": "|**2025-01-31**|**We're Different, We're the Same: Creative Homogeneity Across LLMs**|Emily Wenger et.al.|[2501.19361](http://arxiv.org/abs/2501.19361)|null|众多强大的大型语言模型（LLMs）现在可用作写作辅助工具、创意生成器等。尽管这些LLMs被宣传为有助的创意助手，但多项研究显示，将LLM作为创意伙伴会导致创意输出范围变窄。然而，这些研究仅考虑了与单个LLM交互的影响，从而引发了这样的疑问：这种创意范围的缩小是否源于使用特定LLM——它可能具有有限的输出范围——还是源于一般性地将LLMs作为创意助手。为了研究这个问题，我们通过标准化创意测试从人类和广泛的大型语言模型中收集创意回应，并比较了这些回应在人群层面的多样性。我们发现，即使控制了回应结构和其他关键变量，LLM的回应与其他LLM的回应之间也更加相似，而人类回应之间则不太相似。这一发现在我们评估的LLMs中创意输出同质化显著的发现，为关于创意和LLMs的持续对话增添了新的维度。如果今天的LLMs表现相似，无论使用哪种模型，将它们作为创意伙伴可能会驱使用户走向有限的“创意”输出集合。|\n",
        "2501.19359": "|**2025-01-31**|**Mechanical Properties of the Meninges: Large Language Model Assisted Systematic Review of over 25,000 Studies**|Brandon P. Chelstrom et.al.|[2501.19359](http://arxiv.org/abs/2501.19359)|null|为了预测由于脑外伤导致的脑组织机械损伤，准确的大脑膜本构模型及其相应的机械性能值至关重要。由于大脑膜的复杂解剖结构和空间变异性机械行为，它们在当前的有限元（FE）头部模型中通常被过度简化。本研究对大脑膜每一层的机械性能进行了系统综述（SR），以获取有限元建模的基准数据，并确定现有文献中的空白。相关研究通过三个阶段进行筛选：广泛的初始搜索过滤器、大型语言模型分类器和人工审核。在最初考虑的超过25,000项研究中，这项综述最终包括了47项关于硬脑膜的研究、8项关于蛛网膜的研究和7项关于软脑膜的研究，代表了迄今为止关于大脑膜机械性能最全面和最系统的研究。研究发现，每一层都表现出非线性速率依赖性，这种依赖性随物种、年龄、位置和方向而变化。这项研究揭示了在简化线性弹性FE模型中最常使用的软脑膜弹性模量可能被低估了一个数量级，并且未能考虑方向依赖性。未来的研究应关注大脑膜的机械性能，研究范围应更广泛，包括蛛网膜和软脑膜的加载速率以及年龄效应，因为这些特征相对研究较少，并且预期会影响有限元预测的准确性。|\n",
        "2501.19358": "|**2025-01-31**|**The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking**|Yuchun Miao et.al.|[2501.19358](http://arxiv.org/abs/2501.19358)|null|这项工作识别了从人类反馈的强化学习（RLHF）中的能量损失现象及其与奖励黑客行为的关联。具体来说，在大型语言模型（LLM）的最后一层，能量损失在强化学习过程中逐渐增加，能量损失的过度增加是奖励黑客行为的特征。除了实证分析之外，我们还提供了理论依据，通过证明在温和的条件下，增加的能量损失会降低LLM中上下文相关性的上限，这是奖励黑客行为的关键方面，因为降低的上下文相关性通常表明在强化学习中过度拟合了奖励模型青睐的模式。为了解决这个问题，我们提出了一种能量损失感知的PPO算法（EPPO），该算法在奖励计算过程中惩罚LLM最后一层能量损失的增加，以防止过度能量损失，从而减轻奖励黑客行为。我们从理论上证明，EPPO可以概念上解释为熵正则化的强化学习算法，这为其有效性提供了更深入的见解。在多个LLM和任务上的大量实验表明，能量损失现象的普遍性，以及EPPO在减轻奖励黑客行为和提高RLHF性能方面的有效性。|\n",
        "2501.19340": "|**2025-01-31**|**Towards Adaptive Self-Improvement for Smarter Energy Systems**|Alexander Sommer et.al.|[2501.19340](http://arxiv.org/abs/2501.19340)|null|该论文介绍了一种基于大型语言模型（LLMs）的决策和优化分层框架，通过自适应代码生成来利用LLMs。不是直接进行决策，LLMs通过一个引导任务生成的元策略和一个用于操作动作的基础策略，生成并完善可执行的控制策略。应用于简化的微电网场景，该方法通过迭代改进电池控制策略，实现了高达15%的成本节约。所提出的方法为将基于LLM的工具集成到规划和控制任务中奠定了基础，为复杂系统提供可适应和可扩展的解决方案，同时应对不确定性和可复现性的挑战。|\n",
        "2501.19337": "|**2025-01-31**|**Homogeneity Bias as Differential Sampling Uncertainty in Language Models**|Messi H. J. Lee et.al.|[2501.19337](http://arxiv.org/abs/2501.19337)|null|先前的研究表明，大型语言模型（LLMs）和视觉语言模型（VLMs）在与边缘化群体相关的文本生成中，比与主导群体相关的文本生成表现出更同质化的代表。然而，导致这种同质化偏差背后的机制仍然相对未被充分探索。我们提出，这种偏差源于在推理时从概率分布中采样标记的系统差异。通过分析标记采样分布中的三个不确定性度量——熵、困惑度和区分概率——我们发现，在某些模型中，特别是在GPT-4 Turbo和Llama-3.2中，当生成关于边缘化群体（例如黑人美国人和女性）的文本时，与关于其主导群体（例如白人美国人和男性）的文本相比，标记的采样更具确定性。尽管这些发现可能有助于解释某些模型中的同质化偏差，但这些模式并未在所有测试的VLMs中复现，这表明可能有多种机制共同导致AI中的同质化偏差。|\n",
        "2502.02577": "|**2025-02-04**|**A comparison of translation performance between DeepL and Supertext**|Alex Flückiger et.al.|[2502.02577](http://arxiv.org/abs/2502.02577)|**[link](https://github.com/supertext/evaluation_deepl_supertext)**|随着强大的机器翻译（MT）系统越来越多地基于大型语言模型（LLMs），可靠的品质基准需要能够捕捉它们利用扩展上下文能力的方法。本研究通过评估未分段的文本，比较了两个商业机器翻译系统——DeepL和Supertext。我们使用专业翻译人员评估具有完整文档级上下文的段落，评估了四种语言方向的翻译质量。虽然段落级评估在大多数情况下没有显示出对系统有强烈的偏好，但文档级分析显示，在四个语言方向中有三个偏好Supertext，这表明在较长的文本中具有更高的连贯性。我们提倡更多上下文敏感的评估方法，以确保机器翻译质量评估反映现实世界的可用性。我们发布所有评估数据和脚本，以便进一步分析和复制，网址为https://github.com/supertext/evaluation_deepl_supertext。|\n",
        "2502.02573": "|**2025-02-04**|**Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement**|Soheil Abbasloo et.al.|[2502.02573](http://arxiv.org/abs/2502.02573)|null|大型语言模型（LLMs）在众多领域展示了令人印象深刻的能力，为优化问题求解这一关键、普遍且复杂的领域带来了革命性的机遇。本文探讨了LLMs在处理顺序优化问题（SOPs）方面的能力。我们引入了WorldGen，这是一个动态框架，用于生成具有可控复杂度的未见SOPs，以评估LLMs的性能。我们的初步观察表明，尽管LLMs在简单SOPs上表现良好，但随着复杂度的增加，其性能显著下降。受此启发，我们重新审视了关于推理的哲学假设，以提升LLMs的性能。受赫格尔辩证法框架的影响，我们提出了ACE，展示了如何在无需重新训练或进一步微调的情况下，显著提高LLMs在SOP环境中的性能。|\n",
        "2502.02562": "|**2025-02-04**|**Learning the RoPEs: Better 2D and 3D Position Encodings with STRING**|Connor Schenck et.al.|[2502.02562](http://arxiv.org/abs/2502.02562)|null|我们引入了STRING：可分离的平移不变位置编码。STRING通过一个统一的理论框架扩展了最近提出并被广泛用于大型语言模型的旋转位置编码。重要的是，STRING仍然提供了精确的平移不变性，包括任意维度的标记坐标，同时保持低计算开销。这些特性在机器人领域尤为重要，因为在机器人中，高效的3D标记表示至关重要。我们将STRING集成到具有RGB(-D)输入（颜色加可选深度）的视觉变换器中，展示了显著的改进，例如在开放词汇对象检测和机器人控制器方面。我们通过严格的数学分析补充了我们的实验，证明了我们方法的普适性。|\n",
        "2502.02539": "|**2025-02-04**|**LLMs for Generation of Architectural Components: An Exploratory Empirical Study in the Serverless World**|Shrikara Arun et.al.|[2502.02539](http://arxiv.org/abs/2502.02539)|null|近期，大型语言模型（LLMs）的能力和普及度呈指数级增长，这导致了代码生成领域的显著研究。然而，这种生成仅限于代码片段。更进一步，我们的目标是自动生成架构组件。这不仅会加快开发时间，而且最终使我们能够完全跳过开发阶段，直接从设计决策过渡到部署。为此，我们开展了一项探索性研究，探讨LLMs生成函数即服务（FaaS，又称无服务器函数）架构组件的能力。与单体和微服务等其他架构风格相比，它们架构组件的小型化使得这种架构风格更适合使用当前LLMs进行生成。我们通过系统地选择开源无服务器存储库，隐藏一个无服务器函数，并利用包含关于整体系统不同层次上下文信息的最新LLMs来生成被隐藏的函数，来进行这项研究。我们通过存储库中现有的测试来评估正确性，并使用软件工程（SE）和自然语言处理（NLP）领域的指标来评估代码质量和人类生成代码与LLM生成代码之间的相似度。除了我们的发现之外，我们还讨论了在架构组件生成中使用生成人工智能（GenAI）的前进道路。|\n",
        "2502.02534": "|**2025-02-04**|**Adaptive Self-improvement LLM Agentic System for ML Library Development**|Genghan Zhang et.al.|[2502.02534](http://arxiv.org/abs/2502.02534)|**[link](https://github.com/zhang677/pcl-lite)**|机器学习库通常是用针对特定架构的架构特定编程语言（ASPL）编写的，这些库对于高效的机器学习系统至关重要。然而，编写这些高性能的机器学习库具有挑战性，因为这需要掌握机器学习算法和ASPL的专业知识。另一方面，大型语言模型（LLMs）已显示出一般的编码能力。但是，使用LLMs通过ASPL生成机器学习库仍然存在挑战，因为1）即使是经验丰富的程序员，这个任务也相当复杂，2）由于ASPL的晦涩和不断变化，代码示例有限。因此，LLMs需要有限的资料进行复杂推理才能完成这项任务。为了解决这些挑战，我们介绍了一种自适应自我改进的智能体系统。为了评估我们系统的有效性，我们在一个典型的机器学习库基准上构建了一个基准，并使用开源和闭源LLMs生成ASPL代码。我们的结果表明，与基线单一LLM相比，性能提高了高达3.9倍。|\n",
        "2502.02533": "|**2025-02-04**|**Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies**|Han Zhou et.al.|[2502.02533](http://arxiv.org/abs/2502.02533)|null|大型语言模型作为相互交互和协作的多个智能体，在解决复杂任务方面表现出色。这些智能体被编程为具有声明其功能的提示，以及协调智能体之间交互的拓扑结构。为多智能体系统（MAS）设计提示和拓扑结构本质上很复杂。为了自动化整个设计过程，我们首先对设计空间进行深入研究，旨在了解构建有效MAS背后的因素。我们发现，提示与拓扑结构在实现更有效的MAS设计中发挥着关键作用。基于这些洞察，我们提出了多智能体系统搜索（MASS），这是一个MAS优化框架，通过交织其从局部到全局、从提示到拓扑结构的优化阶段，在三个阶段中高效地利用复杂的MAS设计空间：1）块级（局部）提示优化；2）工作流程拓扑优化；3）工作流程级（全局）提示优化，其中每个阶段都依赖于从前一阶段迭代优化后的提示/拓扑结构。我们表明，MASS优化后的多智能体系统在性能上显著优于现有的各种替代方案。基于MASS找到的系统，我们最终提出了构建有效多智能体系统的设计原则。|\n",
        "2502.02508": "|**2025-02-04**|**Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search**|Maohao Shen et.al.|[2502.02508](http://arxiv.org/abs/2502.02508)|null|大型语言模型（LLMs）在多个领域展现了惊人的推理能力。近期研究表明，增加测试时的计算量可以提升LLMs的推理能力。这通常涉及在推理时通过外部LLM验证器引导的大量采样，从而形成一个两人系统。尽管有外部指导，但该系统的有效性展示了单个LLM处理复杂任务的潜力。因此，我们提出了一个新的研究问题：我们能否将搜索能力内化，从而从根本上提升单个LLMs的推理能力？这项工作探索了一个不同的方向，专注于针对自回归搜索（即具有自我反思和探索新策略的扩展推理过程）的LLMs后训练。为了实现这一目标，我们提出了动作-思维链（COAT）推理和两阶段训练范式：1）一个小型格式调整阶段，以内部化COAT推理格式；2）一个利用强化学习的大型自我改进阶段。我们的方法产生了Satori，这是一个基于开源模型和数据训练的70亿参数LLMs。大量的实证评估表明，Satori在数学推理基准测试中实现了最先进的性能，同时表现出对域外任务的强大泛化能力。代码、数据和模型将全部开源。|\n",
        "2502.02493": "|**2025-02-04**|**EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization**|Yize Wu et.al.|[2502.02493](http://arxiv.org/abs/2502.02493)|null|推测解码是一种有效且无损的大语言模型（LLM）推理加速方法。它使用一个较小的模型生成草稿标记序列，然后由原始基础模型进行验证。在多GPU系统中，通过张量并行（TP）可以进一步降低推理延迟，但草稿模型的最佳TP大小通常小于基础模型，导致在草稿阶段GPU空闲。为了解决这个问题，我们提出了EasySpec，这是一种层并行推测策略，旨在优化多GPU的利用率。EasySpec打破了草稿模型中层的顺序执行顺序，使得可以在设备间实现多层并行化，尽管会引入一些近似误差。在每个草稿和验证迭代之后，草稿模型的关键值（KV）缓存通过单次前向传递进行校准，以最小的额外延迟防止长期误差累积。我们在几个主流开源LLM上评估了EasySpec，使用同一系列模型的小版本作为草稿模型。结果表明，与传统的解码相比，EasySpec可以实现高达4.17倍的峰值加速，同时保持基础LLM的原有分布。具体来说，草稿阶段可以加速至1.62倍，最大精度下降仅为7%，且无需在草稿模型上进行训练或微调。|\n",
        "2502.02481": "|**2025-02-04**|**Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study**|Menglong Cui et.al.|[2502.02481](http://arxiv.org/abs/2502.02481)|null|大型语言模型（LLMs）在多语言能力方面不断展现出改进，即使是小型开源模型也展示了快速的性能提升。在本文中，我们系统地探讨了参数少于十亿的开放LLMs在处理多语言机器翻译（MT）任务方面的能力。我们对六种流行的LLMs进行了全面评估，并发现像Gemma2-9B这样的模型展示了令人印象深刻的跨语言翻译能力。随后，我们在持续预训练阶段引入了并行优先、单语其次（PFMS）数据混合策略，以进一步提高MT性能，并介绍了GemmaX2-28，这是一个在28种语言中实现顶级多语言翻译性能的9B模型。具体来说，GemmaX2-28在持续超越最先进（SOTA）模型如TowerInstruct和XALMA的同时，与Google Translate和GPT-4-turbo等模型实现了具有竞争力的性能。|\n",
        "2502.02458": "|**2025-02-04**|**SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency**|Qianhao Yuan et.al.|[2502.02458](http://arxiv.org/abs/2502.02458)|null|多模态大型语言模型（MLLMs）主要分为两种架构，每种架构都涉及到训练和推理效率之间的权衡：嵌入空间对齐（例如，LLaVA-1.5）在推理过程中效率低下，而交叉注意力空间对齐（例如，Flamingo）在训练过程中效率低下。在本文中，我们比较了这两种架构，并确定了构建高效MLLMs的关键因素。它们之间一个主要的不同点在于如何将注意力应用于视觉标记，特别是在它们之间的交互中。为了研究视觉标记之间的注意力是否必要，我们提出了一种新的自注意力机制，NAAViT（无视觉标记注意力），该机制消除了这种类型的注意力。我们在LLaVA-1.5上的初步实验表明，视觉标记之间的注意力高度冗余。基于这些见解，我们引入了SAISA（自注意力输入空间对齐），这是一种新型架构，可以增强训练和推理效率。SAISA直接将视觉特征与NAAViT自注意力块的输入空间对齐，减少了自注意力块和前馈网络（FFNs）的计算开销。使用与LLaVA-1.5相同的配置，SAISA将推理FLOPs减少了66%，将训练预算减少了26%，同时在准确率方面实现了更优的性能。全面的消融研究进一步验证了SAISA在各种LLMs和视觉编码器上的有效性。代码和模型将公开提供在https://github.com/icip-cas/SAISA上。|\n",
        "2502.03461": "|**2025-02-05**|**Do Large Language Model Benchmarks Test Reliability?**|Joshua Vendrow et.al.|[2502.03461](http://arxiv.org/abs/2502.03461)|null|在部署大型语言模型（LLMs）时，确保这些模型不仅能力强，而且可靠至关重要。已经创建了众多基准来追踪LLMs能力的增长，然而，在衡量它们的可靠性方面却缺乏类似的关注。为了了解这一差距可能带来的潜在影响，我们研究了当前基准在量化模型可靠性方面的效果。我们发现，普遍存在的标签错误可能会损害这些评估，掩盖模型残留的故障并隐藏不可靠的行为。受可靠性评估这一差距的启发，我们随后提出了所谓的“铂金基准”的概念，即经过精心策划以最大限度地减少标签错误和模糊性的基准。作为构建此类基准的第一步尝试，我们对十五个现有流行基准中的示例进行了修订。我们在这些铂金基准上评估了广泛的各种模型，并发现，前沿的LLMs在诸如基础数学单词问题等简单任务上仍然存在失败。进一步分析这些失败揭示了前沿模型在持续努力解决的问题上的先前未识别的模式。我们已在https://github.com/MadryLab/platinum-benchmarks上提供代码。|\n",
        "2502.03460": "|**2025-02-05**|**Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training**|Boyao Wang et.al.|[2502.03460](http://arxiv.org/abs/2502.03460)|null|小型语言模型（SLMs）因其广泛的应用前景，受到了学术界和工业界的广泛关注。为了获得性能强大的SLMs，传统的做法要么是从头开始预训练模型，这会带来巨大的计算成本，要么是压缩/剪枝现有的大型语言模型（LLMs），但这会导致性能下降，与从头预训练相比有所不足。在本文中，我们研究了涉及结构化剪枝和模型训练的加速方法家族。我们发现：1）层自适应剪枝（Adapt-Pruner）在LLMs中非常有效，并比现有的剪枝技术带来了显著的改进；2）具有进一步训练的自适应剪枝导致模型与从头开始预训练的模型相当；3）增量剪枝通过交错剪枝与训练，每次仅去除少量神经元（约5%），从而带来非微小的性能提升。在LLaMA-3.1-8B上的实验结果表明，Adapt-Pruner在常识基准测试上的平均准确率优于传统的剪枝方法，如LLM-Pruner、FLAP和SliceGPT，提高了1%-7%。此外，Adapt-Pruner通过剪枝其更大的版本，将MobileLLM-125M在MMLU基准测试上的性能恢复到600M，通过剪枝减少了200倍的token数，并发现了一个新的1B模型，在多个基准测试中超越了LLaMA-3.2-1B。|\n",
        "2502.03450": "|**2025-02-05**|**A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)**|Yiye Chen et.al.|[2502.03450](http://arxiv.org/abs/2502.03450)|null|场景图已成为大型语言模型（LLMs）进行有据空间推理的结构化和可序列化环境表示。在这项工作中，我们提出了SG-RwR，一个用于场景图推理和规划的方案引导检索-推理框架。我们的方法采用两个协作的代码编写LLM智能体：（1）推理智能体用于任务规划和生成信息查询，以及（2）检索智能体用于根据查询提取相应的图信息。两个智能体迭代协作，实现了对图信息的顺序推理和自适应关注。与先前工作不同，两个智能体仅以场景图方案而不是完整图数据为提示，这通过限制输入令牌减少了幻觉，并促使推理智能体抽象地生成推理轨迹。在跟踪轨迹后，检索智能体根据对方案的理解，程序化地查询场景图数据，从而实现对图动态和全局的关注，增强了推理与检索之间的对齐。通过在多个模拟环境中的实验，我们表明我们的框架在数值问答和规划任务中优于现有的基于LLM的方法，并且可以从任务级别的少量示例中受益，即使在没有智能体级别的演示的情况下。项目代码将发布。|\n",
        "2502.03438": "|**2025-02-05**|**BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving**|Ran Xin et.al.|[2502.03438](http://arxiv.org/abs/2502.03438)|null|近年来，大型语言模型（LLMs）的进步促使人们对使用Lean4进行自动定理证明产生了越来越大的兴趣，其中有效的树搜索方法对于导航证明搜索空间至关重要。虽然现有的方法主要依赖于值函数和蒙特卡洛树搜索（MCTS），但像最佳优先搜索（BFS）这样简单的方法的潜力仍未得到充分探索。本文研究了BFS是否能够在大规模定理证明任务中实现有竞争力的性能。我们提出了\\texttt{BFS-Prover}，一个可扩展的专家迭代框架，具有三个关键创新。首先，我们在每个专家迭代轮次中实施战略数据过滤，排除可以通过束搜索节点扩展解决的问题，以关注更难的情况。其次，我们通过直接偏好优化（DPO）提高BFS的样本效率，该优化应用于自动标注有编译器错误反馈的状态策略对，从而改进LLM的策略，优先考虑生产性扩展。第三，我们在BFS中采用长度归一化，以鼓励探索更深层次的证明路径。\\texttt{BFS-Prover}在MiniF2F测试集上取得了71.31的分数，因此挑战了复杂树搜索方法必要性的看法，证明了当适当扩展时，BFS可以实现有竞争力的性能。|\n",
        "2502.03429": "|**2025-02-05**|**On Fairness of Unified Multimodal Large Language Model for Image Generation**|Ming Liu et.al.|[2502.03429](http://arxiv.org/abs/2502.03429)|null|统一的多模态大型语言模型（U-MLLMs）在端到端流程中的视觉理解和生成方面表现出令人印象深刻的性能。与仅生成模型（例如，Stable Diffusion）相比，U-MLLMs可能在其输出中引发关于偏见的新问题，这些问题可能受到其统一能力的影响。考虑到传播有害刻板印象的风险尚未充分探索，这一差距尤其令人担忧。在本文中，我们对最新的U-MLLMs进行了基准测试，并发现其中大多数都表现出明显的群体偏见，如性别和种族偏见。为了更好地理解和缓解这个问题，我们提出了一种“定位然后修复”策略，其中我们审计并展示了单个模型组件如何受到偏见的影响。我们的分析表明，偏见主要源于语言模型。更有趣的是，我们在U-MLLMs中观察到一种“部分对齐”现象，即对偏见的理解似乎很小，但生成偏见仍然很大。因此，我们提出了一种新颖的平衡偏好模型，以平衡合成数据中的群体分布。实验表明，我们的方法可以减少群体偏见，同时保持语义忠实度。我们希望我们的发现强调了在未来对U-MLLMs进行更全面解释和去偏见策略的必要性。|\n",
        "2502.03425": "|**2025-02-05**|**Harnessing Large Language Models for Curated Code Reviews**|Oussama Ben Sghaier et.al.|[2502.03425](http://arxiv.org/abs/2502.03425)|null|在代码审查中，生成结构化且相关的评论对于识别代码问题和促进准确、高效的代码更改至关重要。精心制作的评论不仅使代码审查过程更加流畅，而且在代码精炼等后续任务中也至关重要，在这些任务中，代码会被修改以满足输入的审查评论。尽管有各种基于AI的自动化评论生成方法，但它们的有效性受到训练数据质量的影响。现有的代码审查数据集通常嘈杂且未经过精炼，限制了AI模型的学习潜力，并阻碍了自动化过程。为了解决这些挑战，我们提出了一种策划流程，旨在提高最大公开代码审查数据集的质量。我们首先建立了一个评估框架，包括特定的标准和类别，以实证研究数据集的初始质量。然后，我们采用基于大型语言模型（LLM）的方法，应用我们的策划流程来精炼数据集。基于相同的评估框架，对全新策划的数据集进行了比较分析，显示出评论的清晰度和简洁度有了显著提高。此外，我们还评估了策划数据集对自动化下游任务的影响，特别是评论生成和代码精炼。我们的研究发现，策划数据集导致模型在生成更准确的评论方面性能提升。策划的评论也更加有用，因为它们能导致更准确的代码精炼。|\n",
        "2502.03418": "|**2025-02-05**|**Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts**|Nikta Gohari Sadr et.al.|[2502.03418](http://arxiv.org/abs/2502.03418)|null|零样本提示技术显著提高了大型语言模型（LLMs）的性能。然而，我们对零样本提示为何如此有效缺乏清晰的理解。例如，在提示“让我们一步一步思考”中，“思考”或“一步一步”对它的成功更为关键？现有的可解释性方法，如基于梯度和基于注意力的方法，计算量大且仅限于开源模型。我们引入了ZIP分数（零样本扰动重要性分数），这是一个通用的指标，适用于开源和闭源模型，基于系统的输入词扰动。我们在四个最近的LLMs、七个广泛使用的提示和几个任务上的实验揭示了词重要性中的有趣模式。例如，尽管“一步一步”和“思考”都显示出高的ZIP分数，但哪个更有影响力取决于模型和任务。我们通过控制实验验证了我们的方法，并将我们的结果与人类判断进行了比较，发现专有模型在关于词重要性的直觉上与人类更接近。这些发现增强了我们对LLM行为的理解，并有助于开发更有效的零样本提示和改进的模型分析。|\n",
        "2502.03397": "|**2025-02-05**|**SPRI: Aligning Large Language Models with Context-Situated Principles**|Hongli Zhan et.al.|[2502.03397](http://arxiv.org/abs/2502.03397)|null|将大型语言模型与人类价值观对齐，尤其是在需要复杂人工监督的任务中，由于依赖人类专业知识进行特定情境指导既资源密集又耗时，因此是一项艰巨的任务。先前的研究已经利用预定义的规则或原则来引导模型的行为（Bai等，2022；Sun等，2023）。然而，这些原则往往过于通用，使得它们难以适应每个单独的输入查询或情境。在这项工作中，我们提出了Situated-PRInciples（SPRI）框架，该框架需要最少或不需要人工努力，旨在为每个输入查询实时生成指导原则，并利用这些原则来对齐每个响应。我们在三个任务上评估了SPRI，结果表明：1）SPRI能够在复杂的专业领域任务中推导出原则，其性能与专家手工制作的原则相当；2）SPRI生成的原则导致针对特定实例的评分标准，优于先前LLM作为评判框架的方法；3）使用SPRI生成合成SFT数据可以提高真实性。我们将在https://github.com/honglizhan/SPRI-public上发布我们的代码和模型生成。|\n",
        "2502.03387": "|**2025-02-05**|**LIMO: Less is More for Reasoning**|Yixin Ye et.al.|[2502.03387](http://arxiv.org/abs/2502.03387)|null|我们提出了一项基本发现，挑战了我们对复杂推理在大规模语言模型中如何产生的理解。虽然传统观点认为，复杂的推理任务需要大量的训练数据（>100,000个示例），但我们证明，复杂的数学推理能力可以通过令人惊讶的少量示例有效地激发。通过全面实验，我们提出的模型LIMO在数学推理方面展现了前所未有的性能。仅使用817个精心挑选的训练样本，LIMO在AIME上的准确率达到57.1%，在MATH上达到94.8%，分别比之前的基于SFT的模型提高了6.5%和59.2%，而只使用了之前方法所需训练数据的1%。LIMO展示了出色的分布外泛化能力，在10个不同的基准测试中实现了40.5%的绝对改进，超过了在100倍更多数据上训练的模型，挑战了SFT导致记忆而非泛化的观点。基于这些结果，我们提出了“少即是多推理假说”（LIMO假说）：在预训练期间已全面编码领域知识的基座模型中，复杂的推理能力可以通过最小但精确编排的认知过程演示而出现。这个假说认为，复杂推理的激发阈值由两个关键因素决定：（1）模型在预训练期间编码的知识基础的完整性；（2）后训练示例作为“认知模板”的有效性，展示模型如何利用其知识库来解决复杂的推理任务。为了促进数据高效推理的可重复性和未来研究，我们将LIMO作为一个综合的开源套件发布在https://github.com/GAIR-NLP/LIMO。|\n",
        "2502.03373": "|**2025-02-05**|**Demystifying Long Chain-of-Thought Reasoning in LLMs**|Edward Yeo et.al.|[2502.03373](http://arxiv.org/abs/2502.03373)|null|将论文摘要逐步翻译为中文如下：  1. Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction.    - 增加推理计算量可以提升大型语言模型（LLMs）的推理能力，长思维链（CoTs）使得回溯和错误纠正等策略成为可能。  2. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices.    - 强化学习（RL）已成为开发这些能力的关键方法，但长CoTs出现的条件仍然不明确，RL训练需要谨慎的设计选择。  3. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories.    - 在本研究中，我们系统地研究了长CoT推理的机制，确定了使模型能够生成长CoT轨迹的关键因素。  4. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings:    - 通过广泛的监督微调（SFT）和RL实验，我们提出了四个主要发现：  5. (1) While SFT is not strictly necessary, it simplifies training and improves efficiency;    - （1）虽然SFT并非严格必要，但它简化了训练并提高了效率；  6. (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth;    - （2）推理能力往往随着训练计算量的增加而出现，但其发展并非必然，因此奖励塑造对于稳定CoT长度增长至关重要；  7. (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning;    - （3）扩展可验证的奖励信号对于RL至关重要。我们发现，利用带过滤机制的噪声、网络提取的解决方案具有强大的潜力，特别是在STEM推理等分布外的（OOD）任务中；  8. (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach.    - （4）错误纠正等核心能力在基础模型中固有存在，但通过RL有效地激励这些技能以完成复杂任务需要大量的计算，而衡量其出现则需要一种细微的方法。  9. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs.    - 这些见解为优化训练策略以增强LLMs中的长CoT推理提供了实际指导。  10. Our code is available at: https://github.com/eddycmu/demystify-long-cot.     - 我们的代码可在以下链接获取：https://github.com/eddycmu/demystify-long-cot。|\n",
        "2502.04328": "|**2025-02-06**|**Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**|Zuyan Liu et.al.|[2502.04328](http://arxiv.org/abs/2502.04328)|null|近期大型语言模型的发展，尤其是GPT-4o的推出，引发了人们对开发能够理解更多模态的全模态模型的兴趣。尽管一些开源替代方案已经出现，但与专业的单模态模型相比，性能仍存在明显差距。在本文中，我们介绍了Ola，这是一个在图像、视频和音频理解方面与专业模型相比达到竞争水平的全模态语言模型。Ola的核心设计在于其渐进式模态对齐策略，该策略逐步扩展语言模型的支持模态。我们的训练流程从最独特的模态开始：图像和文本，然后通过连接语言和音频知识的语音数据以及连接所有模态的视频数据，逐步扩展模型的能力。渐进式学习流程还使我们能够保持跨模态对齐数据的相对较小规模，使得从现有的视觉-语言模型开发全模态变得既容易又成本较低。此外，为了解锁类似GPT-4o的高级交互体验，我们进一步设计了一种基于句子的解码解决方案，用于流式语音生成。广泛的实验表明，Ola在所有模态上超越了现有的开源全模态LLM，与类似规模的最新专业模型相比，也实现了高度竞争的性能。我们的目标是使Ola成为一个完全开源的全模态理解解决方案，以推进该新兴领域未来的研究。模型权重、代码和数据已在https://github.com/Ola-Omni/Ola上开源。|\n",
        "2502.04322": "|**2025-02-06**|**Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions**|Yik Siu Chan et.al.|[2502.04322](http://arxiv.org/abs/2502.04322)|null|尽管进行了广泛的安全对齐努力，大型语言模型（LLMs）仍然容易受到引发有害行为的越狱攻击。虽然现有研究主要关注需要技术专长的攻击方法，但仍有两个关键问题尚未得到充分探索：（1）越狱后的响应是否真正有助于普通用户执行有害行为？（2）在更常见、简单的人-LLM交互中是否存在安全漏洞？在本文中，我们证明当LLM的响应既可操作又具有信息性时，它们最有效地促进有害行为——这两个属性在多步骤、多语言交互中很容易引发。利用这一见解，我们提出了HarmScore，这是一个越狱度量指标，用于衡量LLM响应使有害行为得以有效执行的程度，以及Speak Easy，这是一个简单的多步骤、多语言攻击框架。值得注意的是，通过将Speak Easy纳入直接请求和越狱基线，我们在四个安全基准测试中观察到，在开源和专有LLMs中，攻击成功率平均绝对提高了0.319，HarmScore提高了0.426。我们的工作揭示了一个关键但往往被忽视的漏洞：恶意用户可以轻易利用常见的交互模式来实现有害目的。|\n",
        "2502.04315": "|**2025-02-06**|**ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters**|Kamer Ali Yuksel et.al.|[2502.04315](http://arxiv.org/abs/2502.04315)|null|最近在大语言模型（LLMs）领域取得了显著进展，这些模型在多种任务上表现出色。然而，这些模型通常使用固定的权重进行部署，这限制了它们在推理过程中动态适应现实世界数据中固有的可变性的能力。本文介绍了一种名为ChamaleonLLM的新框架，通过利用批感知聚类和即时生成低秩更新，实现了LLMs在推理时的自适应调整。与传统的微调方法（如低秩调整LoRA）或依赖于一组固定预学习的均匀（可变掩码）的方法不同，我们的方法根据聚类批次的聚合统计信息动态生成对解码器权重的自适应修改。通过智能地分组相似输入，并通过超网络计算上下文感知的低秩更新，ChamaleonLLM实现了显著的性能提升，优于传统的LoRA方法，同时消除了维护多个专家模型的开销。我们的实验突出了我们的方法作为语言模型推理的通用且高度自适应解决方案的潜力。ChamaleonLLM已开源，以确保实验的可重复性：https://anonymous.4open.science/r/ChamaleonLLM/|\n",
        "2502.04306": "|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306](http://arxiv.org/abs/2502.04306)|null|近期研究利用大型语言模型多智能体系统解决复杂问题，同时试图减少构建它们的手动工作量，推动了自动化智能体工作流程优化方法的发展。然而，由于表示限制、缺乏适应性和在依赖离散优化技术时的扩展性差，现有方法仍然不够灵活。我们通过ScoreFlow框架解决了这些挑战，这是一个简单但性能高效的框架，它在连续空间中利用了高效的基于梯度的优化。ScoreFlow集成了Score-DPO，这是一种直接偏好优化方法的新变体，它考虑了定量反馈。在涵盖问答、编码和数学推理的六个基准测试中，ScoreFlow相对于现有基线实现了8.2%的提升。此外，它还使小型模型能够在较低的推理成本下超越大型模型。项目：https://github.com/Gen-Verse/ScoreFlow|\n",
        "2502.04295": "|**2025-02-06**|**Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization**|Yuanye Liu et.al.|[2502.04295](http://arxiv.org/abs/2502.04295)|null|大型语言模型（LLMs）在各项任务中展现出了显著的能力，其现实世界的有效性往往由提示设计所驱动。虽然最近的研究集中在优化提示内容上，但提示格式，这一关键但常被忽视的维度，却得到了有限的系统研究。在本文中，我们提出了内容-格式集成提示优化（CFPO）方法，这是一种创新的方法，通过迭代精炼过程联合优化提示内容和格式。CFPO利用自然语言变异来探索内容变化，并采用动态格式探索策略，系统地评估不同的格式选项。我们在多个任务和开源LLMs上的广泛评估表明，与仅内容优化的方法相比，CFPO展现了可测量的性能提升。这突出了集成内容-格式优化的重要性，并提供了提升LLMs性能的一种实用且模型无关的方法。代码将在https://github.com/HenryLau7/CFPO上提供。|\n",
        "2502.04270": "|**2025-02-06**|**PILAF: Optimal Human Preference Sampling for Reward Modeling**|Yunzhen Feng et.al.|[2502.04270](http://arxiv.org/abs/2502.04270)|null|随着大型语言模型在现实应用中的不断推广，使它们与人类价值观保持一致变得至关重要。从人类反馈中学习强化学习（RLHF）已成为一项关键技术，在无法直接访问专家人类价值观的情况下，将偏好数据转化为奖励模型。在实践中，RLHF主要依赖于近似的奖励模型，这些模型可能无法始终如一地引导策略最大化潜在的人类价值观。我们提出了策略插值学习用于对齐反馈（PILAF），这是一种新颖的响应采样策略，用于偏好标签，它明确地将偏好学习与最大化潜在的专家奖励对齐。PILAF在理论和统计两个方面都得到了证明，显示出其最优性。该方法易于实现，并在反馈整理至关重要的迭代和在线RLHF环境中展现出强大的性能。|\n",
        "2502.04235": "|**2025-02-06**|**MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion**|Xintong Hao et.al.|[2502.04235](http://arxiv.org/abs/2502.04235)|null|尽管大型语言模型在各种任务中表现出色，但它们持续的扩展面临一个关键挑战：高质量预训练数据的稀缺。虽然模型架构不断进化，自然语言数据难以实现规模增长。为了解决这个瓶颈，我们提出了MAssive Genre-Audience（MAGA）重定义方法，该方法系统地从现有语料库中合成多样化、语境丰富的预训练数据。这项工作主要有三个贡献：（1）我们提出了MAGA重定义方法，这是一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含770亿个标记的MAGACorpus。（2）我们使用不同的数据预算扩展策略评估了MAGACorpus，证明了在各种模型大小（134M-13B）上的一致改进，确立了下一代大规模合成预训练语言模型的必要性。（3）通过全面分析，我们研究了提示工程对合成训练崩溃的影响，并揭示了使用验证损失的传统崩溃检测指标的限制。我们的工作表明，MAGA可以在保持质量的同时大幅扩展训练数据集，为超越数据限制的模型扩展提供了一条可靠途径。|\n",
        "2502.04227": "|**2025-02-06**|**Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks**|Andreas Happe et.al.|[2502.04227](http://arxiv.org/abs/2502.04227)|null|我们探讨了在企业网络中利用由大型语言模型（LLM）驱动的自主系统进行假设性入侵渗透测试的可行性和有效性。我们介绍了一个由LLM驱动的原型，该原型能够在现实生活中的活动目录测试环境中攻击账户。我们的研究全面评估了原型的能力，在执行攻击过程中同时突出了其优势和局限性。评估使用了一个现实模拟环境（活动目录游戏，GOAD），以捕捉到代表实时网络场景的复杂交互、随机结果和时间依赖性。研究得出结论，自主的LLM能够进行假设性入侵模拟，这可能使面临预算限制的组织能够民主化渗透测试的访问。原型源代码、跟踪记录和已分析的日志已作为开源发布，以增强集体网络安全并促进LLM驱动的网络安全自动化方面的未来研究。|\n",
        "2502.04226": "|**2025-02-06**|**Keep It Light! Simplifying Image Clustering Via Text-Free Adapters**|Yicen Li et.al.|[2502.04226](http://arxiv.org/abs/2502.04226)|null|许多具有竞争力的聚类流程具有多模态设计，利用大型语言模型（LLMs）或其他文本编码器以及文本-图像对，但这些在现实世界的下游应用中往往不可用。此外，这些框架通常难以训练且需要大量的计算资源，使得其广泛应用变得具有挑战性。在这项工作中，我们表明在深度聚类中，可以通过一个无文本且高度简化的训练流程实现与更复杂的最先进方法的竞争力。特别是，我们的方法——通过预训练模型进行简单聚类（SCP），仅训练一个小型聚类头，同时利用预训练的视觉模型特征表示和正数据对。在包括 CIFAR-10、CIFAR-20、CIFAR-100、STL-10、ImageNet-10 和 ImageNet-Dogs 等基准数据集上的实验表明，SCP 实现了高度竞争力的性能。此外，我们还提供了一个理论结果，解释了为什么，至少在理想条件下，额外的基于文本的嵌入可能不是在视觉中实现强大聚类性能所必需的。|\n",
        "2502.04223": "|**2025-02-06**|**Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents**|Ilia Karmanov et.al.|[2502.04223](http://arxiv.org/abs/2502.04223)|null|光学字符识别（OCR）技术被广泛用于从文档图像中提取文本，从而实现高效的数字化和数据检索。然而，当处理复杂文档时，仅仅提取文本是不够的。要全面理解这些文档，需要了解它们的结构——包括格式、公式、表格以及多页文档中多个块和列的阅读顺序——以及语义信息，以检测脚注和图像标题等元素。这种全面的理解对于下游任务，如检索、文档问答以及为训练大型语言模型（LLMs）和视觉语言模型（VLMs）进行数据整理至关重要。为了解决这个问题，我们引入了“Eclair”，这是一个专为处理各种文档类型而设计的通用文本提取工具。给定一个图像，“Eclair”能够提取按照阅读顺序的格式化文本，以及它们的边界框和相应的语义类别。为了彻底评估这些新颖的功能，我们引入了我们的多个人工标注的文档级OCR和语义分类基准。在基准测试中，“Eclair”实现了最先进的准确率，在关键指标上优于其他方法。此外，我们在现有的基准测试中评估了“Eclair”，展示了它在多个评估标准下的灵活性和优势。|\n",
        "2502.05177": "|**2025-02-07**|**Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray**|Yunhang Shen et.al.|[2502.05177](http://arxiv.org/abs/2502.05177)|null|建立大型视觉-语言模型的长上下文能力对于视频理解、高分辨率图像理解、多模态代理和推理至关重要。我们引入了Long-VITA，这是一个简单而有效的用于长上下文视觉-语言理解任务的大型多模态模型。它擅长同时处理和分析图像、视频和文本的模态，这些模态跨越4K帧或1M个标记，同时在短上下文多模态任务上表现出色。我们提出了一种有效的多模态训练方案，该方案从大型语言模型开始，通过视觉-语言对齐、通用知识学习，以及两个连续阶段的长期序列微调。我们进一步实现了上下文并行分布式推理和logits掩码语言模型头部，以在模型推理期间将Long-VITA扩展到无限长的图像和文本输入。关于训练数据，Long-VITA仅基于来自公共数据集的1700万个样本构建，与最近具有内部数据的尖端模型相比，在各种多模态基准测试中表现出最先进的性能。Long-VITA完全可重现，并支持NPU和GPU平台进行训练和测试。我们希望Long-VITA可以作为有竞争力的基线，并为开源社区在推进长上下文多模态理解方面提供有价值的见解。|\n",
        "2502.05167": "|**2025-02-07**|**NoLiMa: Long-Context Evaluation Beyond Literal Matching**|Ali Modarressi et.al.|[2502.05167](http://arxiv.org/abs/2502.05167)|null|近期的大型语言模型（LLMs）支持长达128K到1M个token的上下文。评估这些能力的一种流行方法是“大海捞针”（NIAH）测试，它涉及从“草堆”（长篇无关上下文）中检索“针”（相关信息）。这种方法的扩展包括增加干扰项、事实链和上下文推理。然而，在这些基准测试中，模型可以利用针和草堆之间现有的直接匹配来简化任务。为了解决这个问题，我们引入了NoLiMa，这是一个扩展NIAH的基准，它包含精心设计的针集，其中问题和针之间具有最小的词汇重叠，要求模型推断潜在的关联来在草堆中定位针。我们评估了12个声称支持至少128K个token上下文的流行LLMs。虽然它们在短上下文（<1K）中表现良好，但随着上下文长度的增加，性能显著下降。例如，在32K时，10个模型的性能低于其强大的短长度基线的一半。即使是表现最好的GPT-4o，其基线从几乎完美的99.3%下降到69.7%。我们的分析表明，这些下降源于在长上下文中，当缺乏直接匹配时，注意力机制面临的难度增加，这使得检索相关信息变得更加困难。|\n",
        "2502.05163": "|**2025-02-07**|**DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails**|Yihe Deng et.al.|[2502.05163](http://arxiv.org/abs/2502.05163)|null|大型语言模型（LLMs）的快速发展增加了对护栏模型的需求，以确保其负责任的使用，尤其是在检测不安全和非法内容方面。虽然英语中的安全数据非常丰富，但由于其他语言的开放源代码安全数据稀缺，多语言护栏建模仍被研究不足。为了填补这一空白，我们提出了一种新颖的双玩家强化学习（RL）框架，其中生成器和护栏模型通过对抗性协同进化，生成高质量合成数据以用于多语言护栏训练。我们将这种交互理论化为一个双玩家博弈，并证明其收敛到纳什均衡。实证评估显示，我们的模型在英语基准测试中优于现有模型，与LlamaGuard3（8B）相比，提高了近10%，同时在推理速度上快4.5倍，且模型规模更小（0.5B）。我们在多语言安全任务上取得了实质性进步，尤其是在解决收集的实数据集中低资源语言的失衡问题。消融研究强调了合成数据生成在弥合英语与其他语言之间开源数据失衡中的关键作用。这些发现建立了一种可扩展和高效的合成数据生成方法，为改进的多语言护栏模型增强LLM安全性铺平了道路。代码、模型和数据将在https://github.com/yihedeng9/DuoGuard上开源。|\n",
        "2502.05159": "|**2025-02-07**|**A Lightweight Method to Disrupt Memorized Sequences in LLM**|Parjanya Prajakta Prashant et.al.|[2502.05159](http://arxiv.org/abs/2502.05159)|null|大型语言模型（LLMs）在许多任务上展现出令人印象深刻的能力，但同时也存在复制受版权保护内容的危险，引发了法律和伦理方面的担忧。尽管差分隐私或神经元编辑等方法可以减少记忆，但它们通常需要昂贵的重新训练或直接访问模型权重，可能会降低性能。为了解决这些挑战，我们提出了TokenSwap，一种轻量级、事后处理的方法，该方法用小型辅助模型（例如DistilGPT-2）的概率替换与语法相关的标记的概率。我们在Pythia-6.9b和LLaMA-3-8b等商业级模型上进行了广泛的实验，并证明我们的方法有效地将已知的记忆生成案例减少了多达10倍，同时对下游任务的影响微乎其微。我们的方法为真实世界系统的用户提供了一种独特且有效的解决方案。|\n",
        "2502.05151": "|**2025-02-07**|**Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation**|Steffen Eger et.al.|[2502.05151](http://arxiv.org/abs/2502.05151)|null|随着大型多模态语言模型的兴起，科学正处于基于人工智能的技术变革的门槛上。最近，提出了大量新的AI模型和工具，承诺将赋予全球研究人员和学者更有效地进行研究的权力。这包括研究周期的各个方面，特别是（1）寻找相关文献；（2）生成研究想法和进行实验；（3）生成基于文本的内容；（4）生成多模态内容（例如，科学图表和图解）；以及（5）基于AI的自动同行评审。在这篇综述中，我们深入概述了这些激动人心的最新发展，这些发展有望从根本上改变科学研究过程，带来长期积极影响。我们的综述涵盖了上述五个方面，指出了相关数据集、方法和结果（包括评估）以及未来研究的局限性和范围。关于这些工具的不足之处和滥用（伪造科学、剽窃、损害研究诚信）的伦理担忧在我们的讨论中占据了特别突出的位置。我们希望我们的综述不仅能够成为该领域新手的参考指南，也能成为“AI4Science”领域新AI项目的催化剂。|\n",
        "2502.05150": "|**2025-02-07**|**CodeSCM: Causal Analysis for Multi-Modal Code Generation**|Mukur Gupta et.al.|[2502.05150](http://arxiv.org/abs/2502.05150)|null|本文提出了一种名为CodeSCM的结构因果模型（SCM），用于分析使用大型语言模型（LLMs）的多模态代码生成。通过对CodeSCM进行干预，我们测量了不同提示模态（如自然语言、代码和输入输出示例）对模型的影响。CodeSCM引入了潜在中介变量，以区分多模态代码生成提示中的代码和自然语言语义。通过在这些中介变量上应用因果中介分析原理，我们量化了直接效应，代表了模型的虚假倾向。我们发现，除了自然语言指令外，输入输出示例对代码生成也有显著影响。|\n",
        "2502.05148": "|**2025-02-07**|**An Annotated Reading of 'The Singer of Tales' in the LLM Era**|Kush R. Varshney et.al.|[2502.05148](http://arxiv.org/abs/2502.05148)|null|首先，我们需要理解摘要中的关键术语和概念：  1. **Parry-Lord oral-formulaic theory**：帕里-洛德口头公式理论，这是一种关于无文盲游吟诗人如何学习、创作和传播口头叙事诗歌的理论。 2. **annotated reading**：注释阅读，即对某个理论或文本进行详细的解释和分析。 3. **large language models (LLMs)**：大型语言模型，这是一种能够理解和生成自然语言的计算机模型。 4. **generative artificial intelligence (AI)**：生成式人工智能，这是一种能够自主生成内容的人工智能技术。  接下来，我们将摘要的每个部分翻译成中文：  - The Parry-Lord oral-formulaic theory was a breakthrough in understanding how oral narrative poetry is learned, composed, and transmitted by illiterate bards.   帕里-洛德口头公式理论在理解无文盲游吟诗人如何学习、创作和传播口头叙事诗歌方面是一个突破。  - In this paper, we provide an annotated reading of the mechanism underlying this theory from the lens of large language models (LLMs) and generative artificial intelligence (AI).   在本文中，我们从大型语言模型（LLMs）和生成式人工智能（AI）的角度对这一理论背后的机制进行注释阅读。  - We point out the similarities and differences between oral composition and LLM generation, and comment on the implications to society and AI policy.   我们指出口头创作与LLM生成之间的相似之处和不同之处，并就其对社会和AI政策的影响进行评论。  最后，我们将整个摘要翻译为中文：  帕里-洛德口头公式理论在理解无文盲游吟诗人如何学习、创作和传播口头叙事诗歌方面是一个突破。在本文中，我们从大型语言模型（LLMs）和生成式人工智能（AI）的角度对这一理论背后的机制进行注释阅读。我们指出口头创作与LLM生成之间的相似之处和不同之处，并就其对社会和AI政策的影响进行评论。|\n",
        "2502.05121": "|**2025-02-07**|**Refining Integration-by-Parts Reduction of Feynman Integrals with Machine Learning**|Matt von Hippel et.al.|[2502.05121](http://arxiv.org/abs/2502.05121)|null|在理论粒子物理和引力波物理领域的先进计算中，积分部分的减少常常成为瓶颈，并且依赖于选择积分部分恒等式时的启发式方法，其质量极大地影响了性能。在这篇论文中，我们探讨了使用机器学习技术来寻找改进的启发式方法。我们使用funsearch，这是一种基于大型语言模型代码生成遗传编程的变种，来探索可能的方法，然后使用强类型遗传编程来专注于有用的解决方案。这两种方法都成功地重新发现了最近被纳入积分部分求解器中的最先进的启发式方法，并在一个例子中找到了在这方面的小幅进步。|\n",
        "2502.05111": "|**2025-02-07**|**Flexible and Efficient Grammar-Constrained Decoding**|Kanghee Park et.al.|[2502.05111](http://arxiv.org/abs/2502.05111)|null|大型语言模型（LLMs）经常被要求生成遵循精确语法规则的输出，例如代码片段或格式化数据。语法约束解码（GCD）可以通过屏蔽那些会导致输出不属于指定上下文无关文法（CFG）的标记来保证LLM输出符合这些规则。为了保证正确性，GCD算法必须计算给定LLM子词标记器如何与给定上下文无关文法使用的标记对齐，并基于此信息计算标记掩码。这样做既具有挑战性，现有的GCD算法需要花费数十分钟来预处理常见的语法。我们提出了一种新的GCD算法及其实现，与现有方法相比，它提供了17.71倍的离线预处理速度，同时在在线掩码计算中保持了最先进的效率。|\n",
        "2502.05092": "|**2025-02-07**|**Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs**|Rohit Saxena et.al.|[2502.05092](http://arxiv.org/abs/2502.05092)|null|从视觉表征中理解时间是基本认知技能，但对于多模态大型语言模型（MLLMs）来说，这仍然是一个挑战。在这项工作中，我们调查了MLLMs通过模拟时钟和年历来解释时间和日期的能力。为了实现这一目标，我们精心制作了一个结构化数据集，包括两个子集：1）$\\textit{ClockQA}$，它包含各种类型的时钟样式——标准时钟、黑表盘时钟、无秒针时钟、罗马数字时钟和箭头指针时钟——以及与时间相关的问题；2）$\\textit{CalendarQA}$，它由年历图像组成，问题范围从众所周知的日子（例如，圣诞节、新年）到通过计算得出的日子（例如，一年中的第100天或第153天）。我们的目标是分析当MLLMs面对与时间相关的视觉数据时，它们如何执行视觉识别、数值推理和时间推断。我们的评估表明，尽管最近取得了进展，但可靠地理解时间对于MLLMs来说仍然是一个重大挑战。|\n",
        "2502.06759": "|**2025-02-10**|**Rationalization Models for Text-to-SQL**|Gaetano Rossiello et.al.|[2502.06759](http://arxiv.org/abs/2502.06759)|null|我们提出了一种生成思维链（CoT）理由的框架，以增强文本到SQL模型的微调。这些理由包括中间SQL语句和解释，作为构建最终SQL查询的逐步步骤。该过程从手动标注一小部分示例开始，然后使用这些示例通过迭代、动态的少量知识蒸馏程序从教师模型中提示大型语言模型。随后，在验证后的分解查询上训练一个合理化模型，从而为文本到SQL数据集生成大量的合成CoT标注。为了评估该方法，我们在BIRD数据集上使用带有和不带有这些理由的小型语言模型进行微调。结果表明，逐步查询生成提高了执行精度，特别是对于中等和高度复杂的查询，同时也增强了可解释性。|\n",
        "2502.06742": "|**2025-02-10**|**Gradient Multi-Normalization for Stateless and Scalable LLM Training**|Meyer Scetbon et.al.|[2502.06742](http://arxiv.org/abs/2502.06742)|null|训练大型语言模型（LLMs）通常依赖于如Adam（Kingma & Ba，2015）这样的自适应优化器，这些优化器存储额外的状态信息以加速收敛，但会带来显著的内存开销。最近的研究，如SWAN（Ma等，2024），通过消除对优化器状态的需求，并通过对瞬时梯度应用多步预处理程序，实现了与Adam相当的性能。受SWAN成功的影响，我们引入了一种新的框架来设计无状态优化器，该框架根据多个范数对随机梯度进行归一化。为了实现这一点，我们提出了一种简单的交替方案来强制执行对这些范数的梯度归一化。我们表明，我们的过程可以产生，直到任意精度，问题的固定点，并且SWAN是我们方法的一个特例，具有精心选择的范数，从而提供了对其设计的更深入理解。然而，SWAN计算成本高昂的白化/正交化步骤限制了其在大型LLMs中的实用性。利用我们的原则性视角，我们开发了一种更高效、可扩展和实用的无状态优化器。我们的算法放宽了SWAN的性质，显著降低了其计算成本，同时保持了其内存效率，使其适用于大规模模型的训练。在预训练LLaMA模型（参数量高达10亿）的实验中，我们的算法比Adam快3倍，同时内存需求显著降低，优于其他内存高效的基线。|\n",
        "2502.06737": "|**2025-02-10**|**VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data**|Thomas Zeng et.al.|[2502.06737](http://arxiv.org/abs/2502.06737)|null|过程奖励模型（PRMs）通过利用增加的推理时间计算，被证明可以有效地提高大型语言模型（LLMs）的数学推理能力。然而，它们主要在数学数据上进行训练，其泛化到非数学领域的表现尚未得到严格的研究。为此，这项工作首先表明，当前的PRMs在其他领域表现不佳。为了解决这一局限性，我们引入了VersaPRM，这是一种多领域PRM，它在利用我们新颖的数据生成和标注方法生成的合成推理数据上进行了训练。VersaPRM在多个领域实现了持续的性能提升。例如，在法律领域的MMLU-Pro类别中，通过加权多数投票，VersaPRM实现了7.9%的性能提升，超过了多数投票基线的提升——超过了Qwen2.5-Math-PRM的1.3%提升。此外，我们还向社区贡献，开源了VersaPRM的所有数据、代码和模型。|\n",
        "2502.06733": "|**2025-02-10**|**Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining**|Daouda Sow et.al.|[2502.06733](http://arxiv.org/abs/2502.06733)|null|在大量和异构数据集上预训练大型语言模型（LLMs）对于在多样化的下游任务中实现最先进的性能至关重要。然而，当前的训练范式平等地对待所有样本，忽略了单个样本在整个训练过程中的重要性和相关性。现有的重新加权策略主要关注组级数据的重要性，未能利用细粒度的实例级信息，并且没有根据训练过程中单个样本的重要性动态调整。在本文中，我们引入了旨在提高LLM预训练效率和效果的新型动态、实例级数据重新加权算法。我们的方法以在线方式根据每个训练样本的损失值调整其权重，使模型能够根据当前训练阶段动态关注更信息丰富或更重要的样本。特别是，我们的框架使我们能够系统地制定重新加权策略，优先考虑冗余或不具有信息量的数据，我们发现这种方法效果最好。此外，我们开发了一个新的理论框架来分析基于损失的重新加权对基于梯度的优化收敛的影响，为这些策略如何影响收敛界限提供了第一个形式化的描述。我们通过一系列任务实证验证了我们的方法，从预训练70亿和14亿参数的LLMs到较小规模的语言模型和线性回归问题，证明了我们的基于损失的重新加权方法可以导致更快的收敛和显著提高的性能。|\n",
        "2502.06703": "|**2025-02-10**|**Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling**|Runze Liu et.al.|[2502.06703](http://arxiv.org/abs/2502.06703)|null|测试时间缩放（TTS）是通过在推理阶段使用额外计算来提高大型语言模型（LLM）性能的重要方法。然而，目前的研究没有系统地分析策略模型、过程奖励模型（PRM）和问题难度如何影响TTS。这种分析的缺乏限制了人们对TTS方法的理解和实际应用。在本文中，我们关注两个核心问题：（1）在不同策略模型、PRM和问题难度级别上缩放测试时间计算的最佳方法是什么？（2）扩展计算在多大程度上可以提高LLM在复杂任务上的性能，并且较小的语言模型能否通过这种方法超越较大的模型？通过在MATH-500和具有挑战性的AIME24任务上的全面实验，我们得出以下观察结果：（1）计算最优的TTS策略高度依赖于策略模型、PRM和问题难度的选择。（2）使用我们的计算最优TTS策略，极小的策略模型可以超越较大的模型。例如，一个1B LLM可以在MATH-500上超过一个405B LLM。此外，在MATH-500和AIME24上，一个0.5B LLM优于GPT-4o，一个3B LLM超越一个405B LLM，一个7B LLM击败了o1和DeepSeek-R1，同时具有更高的推理效率。这些发现表明，根据每个任务和模型的具体特征调整TTS策略的重要性，并指出TTS是增强LLM推理能力的一种有前途的方法。|\n",
        "2502.06669": "|**2025-02-10**|**Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations**|Rui Chen et.al.|[2502.06669](http://arxiv.org/abs/2502.06669)|null|本文观察到大型语言模型（LLMs）的无监督能力有了显著提升。由于它们对输入的高度敏感性，研究越来越集中于通过直接和简单的提示工程来提高LLMs的性能，而不是复杂的领域适应。研究表明，LLMs表现出情感智力，正面和负面的情绪都可能潜在地增强任务表现。然而，先前的研究主要集中在单一刺激类型上，忽视了比较不同刺激效果、检验不同任务难度的影响或探索潜在机制。受社会认知理论中自我效能感和任务表现之间正相关关系的启发，本文引入了言语效能刺激（VES）。我们的VES包括三种类型的言语提示：鼓励性、挑衅性和批判性，涵盖六个方面，如有用性和能力。我们进一步将任务难度进行分类，旨在广泛研究不同难度的VES如何影响语言模型的自效能感和任务成就。实验结果表明，三种类型的VES在大多数任务上提高了LLMs的表现，而最有效的VES因模型而异。在大量实验中，我们获得了一些与心理理论一致的研究结果，为未来研究提供了新的见解。|\n",
        "2502.06666": "|**2025-02-10**|**Automatic Evaluation of Healthcare LLMs Beyond Question-Answering**|Anna Arias-Duart et.al.|[2502.06666](http://arxiv.org/abs/2502.06666)|null|当前大型语言模型（LLM）的基准测试通常基于开放式或封闭式问答评估，以避免对人工劳动的需求。封闭式测量评估响应的事实性，但缺乏表达性。开放式评估捕捉模型产生话语响应的能力，但更难以评估其正确性。这两种方法通常独立或结合使用，尽管它们之间的关系仍不太清楚。本研究聚焦于医疗保健领域，在该领域中事实性和话语都非常重要。它引入了一套全面的、多轴的医疗LLM评估方案，探索开放式和封闭式基准及指标之间的相关性。研究发现，现有方法存在盲点和重叠。作为一个更新的理智检查，我们发布了一个新的医学基准——CareQA——包括开放和封闭两种版本。最后，我们提出了一个用于开放式评估的新指标——放宽的困惑度——以缓解所识别的限制。|\n",
        "2502.06663": "|**2025-02-10**|**EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models**|Xingrun Xing et.al.|[2502.06663](http://arxiv.org/abs/2502.06663)|null|现代大型语言模型（LLMs）通过扩展规律驱动，在大型模型尺寸上实现了智能的突破。最近，对云成本、延迟和隐私的日益关注，使得开发紧凑型边缘语言模型成为一项紧迫的需求。与受扩展规律限制的直接预训练不同，这项工作提出了修剪感知预训练，专注于保留更大优化模型的性能。它具有以下特点：1）数据可扩展性：我们在LLM中引入了最小参数组，并持续优化结构修剪，将LLM-Pruner和SparseGPT等后训练修剪方法扩展到预训练阶段。2）架构无关性：使用显著性驱动的修剪自动设计LLM架构，这是现代预训练中首次超越人类设计的SoTA LLM。我们发现，通过扩展LLM压缩并扩展其边界，它实现了高质量的边缘语言模型，称为EfficientLLM。EfficientLLM在常识基准测试中显著优于具有$100M \\sim 1B$参数的SoTA基线，如MobileLLM、SmolLM、Qwen2.5-0.5B、OLMo-1B、Llama3.2-1B。作为首次尝试，EfficientLLM弥合了传统LLM压缩和直接预训练方法之间的性能差距，我们将在https://github.com/Xingrun-Xing2/EfficientLLM上全面开源。|\n",
        "2502.06655": "|**2025-02-10**|**Unbiased Evaluation of Large Language Models from a Causal Perspective**|Meilin Chen et.al.|[2502.06655](http://arxiv.org/abs/2502.06655)|null|基准污染已成为LLM（大型语言模型）评估领域的一个重要关注点。之前的“代理作为评估者”方法通过让代理参与问题的生成来解决这一问题。尽管它们取得了成功，但“代理作为评估者”方法中的偏差仍然在很大程度上未被探索。在本文中，我们提出了评估偏差的理论公式，为设计无偏差的评估协议提供了有价值的见解。此外，我们通过对最小“代理作为评估者”设置精心设计的探测任务，确定了两种类型的偏差。为了解决这些问题，我们提出了无偏差评估者，这是一种提供更全面、无偏差和可解释的LLM评估协议。大量的实验表明，当前LLM还有很大的改进空间。此外，我们证明了无偏差评估者不仅提供了基准污染的强有力证据，而且还提供了可解释的评估结果。|\n",
        "2502.06653": "|**2025-02-10**|**In-Context Learning (and Unlearning) of Length Biases**|Stephanie Schoch et.al.|[2502.06653](http://arxiv.org/abs/2502.06653)|null|大型语言模型在情境学习方面展现出强大的能力，其中示例输入输出对被附加到提示中以供演示。然而，现有研究已经证明了模型能够学习情境中的词汇和标签偏差，这对模型的表现和鲁棒性产生负面影响。其他统计数据偏差的影响尚未得到充分探索，这正是本研究旨在解决的问题。我们特别研究了长度偏差对情境学习的影响。我们证明了模型确实在预测的上下文窗口中学习了长度偏差，并进一步实证分析了调节模型所表现出的偏差水平的因素。此外，我们展示了在情境中学习长度信息可以用来抵消模型中已经编码的长度偏差（例如，通过微调）。这揭示了情境学习在去偏模型预测行为方面的力量，而不需要昂贵的参数更新。|\n",
        "2502.07780": "|**2025-02-11**|**DarwinLM: Evolutionary Structured Pruning of Large Language Models**|Shengkun Tang et.al.|[2502.07780](http://arxiv.org/abs/2502.07780)|null|大型语言模型（LLMs）在各个自然语言处理（NLP）任务上取得了显著的成功。然而，它们庞大的计算成本限制了它们的广泛应用，尤其是在实时应用中。结构化剪枝通过压缩模型并直接提供端到端速度提升，为解决这个问题提供了一个有效的解决方案，且与硬件环境无关。同时，模型的各个组成部分对剪枝的敏感性各不相同，这要求进行非均匀模型压缩。然而，剪枝方法不仅应识别出有能力的子结构，还应考虑压缩后的训练。为此，我们提出了一个名为 \\sysname 的 \\emph{训练感知}结构化剪枝方法。\\sysname 建立在进化搜索过程之上，通过变异在每个代中生成多个子模型，并选择最适应的模型进行生存。为了评估训练后的效果，我们在子模型群体中纳入了一个轻量级的多步骤训练过程，在每次选择阶段逐步增加标记数，并淘汰表现不佳的模型。我们通过在 Llama-2-7B、Llama-3.1-8B 和 Qwen-2.5-14B-Instruct 上进行广泛的实验来验证我们的方法，实现了结构化剪枝的最先进性能。例如，\\sysname 在压缩后训练阶段所需的训练数据量比 ShearedLlama 少 $5\\times$ ，同时超越了 ShearedLlama。|\n",
        "2502.07776": "|**2025-02-11**|**Auditing Prompt Caching in Language Model APIs**|Chenchen Gu et.al.|[2502.07776](http://arxiv.org/abs/2502.07776)|null|在大规模语言模型（LLMs）中，提示缓存会导致数据依赖的时序变化：缓存过的提示比未缓存的提示处理速度更快。这些时序差异引入了旁路时序攻击的风险。例如，如果缓存被多个用户共享，攻击者可以通过快速API响应时间来识别缓存过的提示，从而获取有关其他用户提示的信息。由于提示缓存可能导致隐私泄露，因此API提供商的缓存策略透明度非常重要。为此，我们开发并进行了统计审计，以检测现实世界LLM API提供商中的提示缓存。我们在包括OpenAI在内的七个API提供商中检测到跨用户的全局缓存共享，可能导致用户提示的潜在隐私泄露。提示缓存引起的时序变化也可能导致模型架构信息的泄露。具体来说，我们发现证据表明OpenAI的嵌入模型是一个仅解码器的Transformer，这一信息之前并未公开。|\n",
        "2502.07772": "|**2025-02-11**|**Automatic Robot Task Planning by Integrating Large Language Model with Genetic Programming**|Azizjon Kobilov et.al.|[2502.07772](http://arxiv.org/abs/2502.07772)|null|准确的任务规划对于控制自主系统，如机器人、无人机和自动驾驶车辆至关重要。行为树（BTs）因其模块化、灵活性和可重用性，被认为是在任务规划中最突出的控制策略定义框架之一。为机器人系统生成可靠且准确的行为树（BT）控制策略仍然具有挑战性，通常需要领域专业知识。在本文中，我们提出了LLM-GP-BT技术，该技术利用大型语言模型（LLM）和遗传编程（GP）来自动化行为树的生成和配置。LLM-GP-BT技术处理以人类自然语言表达的机器人任务命令，并以计算高效和用户友好的方式将它们转换为准确可靠的行为树任务计划。所提出的技术通过仿真实验系统性地开发和验证，展示了其在简化自主系统任务规划方面的潜力。|\n",
        "2502.07763": "|**2025-02-11**|**Great Power Brings Great Responsibility: Personalizing Conversational AI for Diverse Problem-Solvers**|Italo Santos et.al.|[2502.07763](http://arxiv.org/abs/2502.07763)|null|新加入开源软件（OSS）项目的新手面临许多挑战。大型语言模型（LLMs），如ChatGPT，已成为回答问题和提供指导的潜在资源，许多开发者现在更倾向于使用ChatGPT而不是传统的问答网站如Stack Overflow。然而，LLMs在呈现信息时可能存在偏见，这可能会对解决问题风格可能未被广泛代表的新手产生特别影响。这引发了关于AI驱动支持对OSS项目新手的可访问性的重要问题。这篇愿景论文概述了根据不同的解决问题风格调整AI响应的潜力，以避免偏袒特定的子群体。我们讨论了基于AI角色的提示工程作为与AI互动的策略的潜力。这项研究邀请进一步研究，以改进基于AI的工具，更好地支持对OSS项目的贡献。|\n",
        "2502.07760": "|**2025-02-11**|**Scalable Fingerprinting of Large Language Models**|Anshul Nasery et.al.|[2502.07760](http://arxiv.org/abs/2502.07760)|null|模型指纹识别已成为模型所有者识别其共享模型的有力工具。然而，为了降低误检率、抵御指纹泄露以及防御模型用户联盟试图绕过检测的行为，我们认为{\\em 可扩展性}是关键，即扩大模型中可嵌入指纹的数量。因此，我们将可扩展性视为指纹识别方案的至关重要要求。我们在比以往考虑的规模大得多的范围内进行指纹设计实验，并提出了一种新的方法，称为核仁采样，以生成可扩展、持久且无害的指纹。我们证明，该方案可以将24,576个指纹添加到Llama-3.1-8B模型中——比现有方案多两个数量级——而不会降低模型的效用。我们插入的指纹在经过标准训练数据上的监督微调后仍然存在。我们进一步讨论了指纹识别的安全风险，并从理论和实证上展示了像我们这样的可扩展指纹识别方案如何减轻这些风险。|\n",
        "2502.07752": "|**2025-02-11**|**Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension**|Wenbo Gong et.al.|[2502.07752](http://arxiv.org/abs/2502.07752)|null|设计具有低内存需求和快速收敛的大型语言模型（LLM）的优化器是一个重要且具有挑战性的问题。本文通过结构化Fisher信息矩阵（FIM）近似的角度，朝着系统设计此类优化器迈出了第一步。我们表明，许多最先进的效率优化器可以被视为在特定结构假设下FIM近似（在Frobenius范数下）的解决方案。基于这些见解，我们提出了两个针对LLM的实用高效优化器设计建议，包括谨慎选择结构假设以平衡通用性和效率，以及通过一种新颖的低秩扩展框架增强具有通用结构的优化器的内存效率。我们通过推导新的内存高效优化器：行和列缩放随机梯度下降（RACS）和自适应低维子空间估计（Alice）来展示如何使用每种设计方法。在LLaMA预训练（高达10亿参数）上的实验验证了其有效性，显示出比现有内存高效基线和Adam更快的收敛速度和更好的性能，同时几乎没有内存开销。值得注意的是，Alice的收敛速度比Adam快2倍以上，而RACS在10亿参数模型上表现出与SGD类似的内存性能。|\n",
        "2502.07747": "|**2025-02-11**|**WHODUNIT: Evaluation benchmark for culprit detection in mystery stories**|Kshitij Gupta et.al.|[2502.07747](http://arxiv.org/abs/2502.07747)|null|我们提出一个新颖的数据集，名为WhoDunIt，用于评估大型语言模型（LLM）在叙事情境中的演绎推理能力。该数据集由开放域的悬疑小说和短篇小说构建而成，挑战LLM在阅读和理解故事后识别罪犯。为了评估模型的鲁棒性，我们应用了一系列以角色名字为单位的增强方法，包括原始名字、名字交换以及用来自流行话语中的知名真实或虚构实体进行替换。此外，我们使用不同的提示风格来研究提示对演绎推理准确性的影响。我们使用最先进的模型进行了评估研究，具体为GPT-4o、GPT-4-turbo和GPT-4o-mini，通过多次试验和多数响应选择来确保可靠性。结果表明，虽然LLM在未更改的文本上表现可靠，但准确性在特定的名字替换下有所下降，尤其是那些广为人知的名字。此数据集在此处公开可用。|\n",
        "2502.07736": "|**2025-02-11**|**The Economics of Large Language Models: Token Allocation, Fine-Tuning, and Optimal Pricing**|Dirk Bergemann et.al.|[2502.07736](http://arxiv.org/abs/2502.07736)|null|我们建立了一个经济框架来分析大型语言模型（LLM）的最优定价和产品设计。我们的框架捕捉了LLM的几个关键特征：处理输入和输出标记的可变运营成本；通过微调定制模型的能力；以及用户在任务需求和错误敏感性方面的多维异质性。在我们的模型中，垄断卖家通过产品菜单提供多个版本的LLM。最优定价结构取决于任务间标记分配是否可协商以及用户是否面临规模约束。具有相似的总价值-规模特征的用户会选择类似的微调和标记消费水平。最优机制可以通过两步收费菜单实现，对使用更频繁的用户收取更高的加成。我们的结果合理化了观察到的行业实践，例如基于模型定制和用量的分层定价。|\n",
        "2502.07732": "|**2025-02-11**|**Economics of Sourcing Human Data**|Sebastin Santy et.al.|[2502.07732](http://arxiv.org/abs/2502.07732)|null|人工智能的进步依赖于人类生成数据，从标注者市场到更广泛的互联网。然而，大型语言模型的广泛应用现在威胁到了这些平台上人类生成数据的质量和完整性。我们认为，这个问题不仅仅在于过滤人工智能生成内容的直接挑战——它揭示了数据收集系统设计的更深层次缺陷。现有系统往往以牺牲人类内在动机为代价，优先考虑速度、规模和效率，导致参与度下降和数据质量下降。我们提出，重新思考数据收集系统，使其与贡献者的内在动机相一致——而不是仅仅依赖外部激励——可以帮助在规模上维持高质量数据来源，同时保持贡献者的信任和长期参与。|\n",
        "2502.07728": "|**2025-02-11**|**Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK**|Marcos Cramer et.al.|[2502.07728](http://arxiv.org/abs/2502.07728)|null|大型语言模型（LLMs）展示了卓越的代码生成能力，但生成的代码的正确性无法天然信赖。本文探讨了使用形式化软件验证（特别是Ada语言的SPARK框架）来确保LLM生成代码可靠性的可行性。我们提出了Marmaragan工具，该工具利用LLM为现有程序生成SPARK注释，从而实现代码的形式化验证。该工具在一组精心挑选的SPARK程序上进行了基准测试，通过有选择地移除注释来测试特定功能。Marmaragan与GPT-4o在基准测试中的表现令人鼓舞，其中50.7%的基准案例已生成正确的注释。这些结果为将LLM的力量与形式化软件验证的可靠性相结合的未来工作奠定了基础。|\n",
        "2502.08638": "|**2025-02-12**|**Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples**|Andrianos Michail et.al.|[2502.08638](http://arxiv.org/abs/2502.08638)|null|评估模型跨语言语义搜索能力通常仅限于信息检索和语义文本相似度等任务中的现有数据集。为了允许特定领域的评估，我们引入了一种新的跨语言语义搜索任务——跨语言语义区分（CLSD），它只需要目标领域中感兴趣的语言对的一组平行句子对。该任务侧重于模型跨语言将真实平行句子排在由大型语言模型生成的硬负例之上的能力。我们为新闻领域中的德语-法语语言对创建了四个CLSD任务实例。在本案例研究中，我们发现同时针对检索任务进行微调的模型（例如，多语言E5）在以英语作为枢纽语言时受益，而像LaBSE这样的双语文本挖掘模型在直接跨语言的情况下表现最佳。我们还展示了由我们的干扰生成策略启用的细粒度相似度分析，这表明不同的嵌入模型对不同的扰动类型敏感。|\n",
        "2502.08631": "|**2025-02-12**|**Ensemble based approach to quantifying uncertainty of LLM based classifications**|Srijith Rajamohan et.al.|[2502.08631](http://arxiv.org/abs/2502.08631)|null|首先，我们需要理解摘要中的关键术语和概念：  1. **Large Language Models (LLMs)**：大型语言模型。 2. **internal model's parameters**：模型内部参数。 3. **context window**：上下文窗口。 4. **greedy sampling strategy**：贪婪采样策略。 5. **variance**：方差。 6. **conceptual certainty**：概念确定性。 7. **parametric knowledge**：参数知识。 8. **lexical variance**：词汇方差。 9. **Finetuning**：微调。 10. **classification problem**：分类问题。 11. **probabilistic method**：概率方法。 12. **certainties of the predicted classes**：预测类别的确定性。  接下来，我们将摘要逐步翻译为中文：  - The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window.   大型语言模型（LLMs）的输出是内部模型参数和输入到上下文窗口中的输入的函数。  - The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input.   这里提出的假设是，在贪婪采样策略下，LLMs输出的方差是模型参数知识中嵌入的概念确定性以及输入中的词汇方差的一个函数。  - Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations.   微调模型会导致模型输出对词汇输入变化的敏感性降低。  - This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes.   然后将此应用于分类问题，并提出了一种概率方法来估计预测类别的确定性。  最终的中文翻译结果为：  大型语言模型（LLMs）的输出是内部模型参数和输入到上下文窗口中的输入的函数。这里提出的假设是，在贪婪采样策略下，LLMs输出的方差是模型参数知识中嵌入的概念确定性以及输入中的词汇方差的一个函数。微调模型会导致模型输出对词汇输入变化的敏感性降低。然后将此应用于分类问题，并提出了一种概率方法来估计预测类别的确定性。|\n",
        "2502.08586": "|**2025-02-12**|**Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks**|Ang Li et.al.|[2502.08586](http://arxiv.org/abs/2502.08586)|null|近期大量机器学习安全文献聚焦于针对对齐的大型语言模型（LLMs）的攻击。这些攻击可能提取私人信息或迫使模型生成有害输出。在实际部署中，LLMs 通常是大型的智能管道的一部分，包括记忆系统、检索、网络访问和API调用。这些额外的组件引入了漏洞，使得这些由LLM驱动的代理比孤立的LLMs更容易受到攻击，但相对较少的研究关注LLM代理的安全性。在本文中，我们分析了LLM代理独有的安全和隐私漏洞。我们首先提供了一个按威胁行为者、目标、入口点、攻击者可观测性、攻击策略和代理管道固有问题性进行分类的攻击分类法。然后，我们对流行的开源和商业代理进行了一系列示范性攻击，展示了其漏洞的即时实际影响。值得注意的是，我们的攻击易于实现，并且不需要对机器学习有任何了解。|\n",
        "2502.08557": "|**2025-02-12**|**QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval**|Wonduk Seo et.al.|[2502.08557](http://arxiv.org/abs/2502.08557)|null|查询扩展在信息检索（IR）中广泛用于通过丰富查询以增加额外的上下文信息来提高搜索结果。尽管基于大型语言模型（LLM）的最近方法通过多个提示生成伪相关内容和扩展术语，但它们往往产生重复且狭窄的扩展，缺乏检索所有相关信息所需的多样化上下文。在本文中，我们介绍了一种新颖且有效的查询扩展框架QA-Expand。它首先从初始查询生成多个相关问题，然后产生相应的伪答案作为代理文档。一个反馈模型进一步重写和过滤这些答案，以确保仅包含最富有信息量的增强。在BEIR和TREC等基准上的大量实验表明，QA-Expand比最先进的方法提高了高达13%的检索性能，为现代检索挑战提供了一个稳健的解决方案。|\n",
        "2502.08554": "|**2025-02-12**|**Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies**|Sunnie S. Y. Kim et.al.|[2502.08554](http://arxiv.org/abs/2502.08554)|null|大型语言模型（LLMs）可能会生成听起来流畅且令人信服的错误回应，这增加了用户将这些回应当作正确信息的风险。减轻这种过度依赖是一个关键挑战。通过一项参与者使用LLM增强的应用程序来回答客观问题的有声思维研究，我们确定了LLM回应的几个特征，这些特征塑造了用户的依赖性：解释（答案的支持性细节）、解释的不一致性以及来源。通过一项大规模、预先注册、受控的实验（N=308），我们隔离并研究了这些特征对用户依赖性、准确性及其他指标的影响。我们发现，解释的存在增加了对正确和错误回应的依赖。然而，当提供来源或解释出现不一致性时，我们发现对错误回应的依赖性降低。我们讨论了这些发现对培养对LLMs适当依赖性的影响。|\n",
        "2502.08550": "|**2025-02-12**|**LLMs can implicitly learn from mistakes in-context**|Lisa Alazraki et.al.|[2502.08550](http://arxiv.org/abs/2502.08550)|null|从错误中学习是人类智能的基本特征。先前的研究表明，大型语言模型（LLMs）在提供全面解释的情况下，即详细说明为什么答案错误或如何纠正它，也能从错误的答案中学习。在本工作中，我们探讨了当没有提供这种解释时，LLMs是否能在数学推理任务中从错误中学习。我们研究了LLMs是否能够仅通过观察错误和正确答案来隐式推断这种理由。令人惊讶的是，我们发现当从上下文中消除理由时，LLMs的平均表现更好，错误答案只是简单地与正确答案一起展示。这种方法在我们的评估中也显著优于思维链提示。我们表明，这些结果在不同大小和不同推理能力的LLMs中是一致的。此外，我们进行了深入分析，并表明与将更多样化的问答对引入上下文相比，同时提示错误和正确答案会导致更好的性能和更好的泛化。最后，我们表明，仅观察到错误和正确答案的模型生成的新理由，在人类评分上与那些通过示例理由辅助产生的理由得分相当。我们的结果表明，LLMs确实能够进行上下文隐式学习。|\n",
        "2502.08524": "|**2025-02-12**|**LLM Pretraining with Continuous Concepts**|Jihoon Tack et.al.|[2502.08524](http://arxiv.org/abs/2502.08524)|null|接下来，我们将逐步推理并给出摘要的中文翻译。  1. **Next token prediction**：下一个标记预测，在大型语言模型预训练中作为标准训练目标。    - 翻译：下一个标记预测是大型语言模型预训练中使用的标准训练目标。  2. **Representations are learned as a result of optimizing for token-level perplexity**：通过优化标记级困惑度来学习表示。    - 翻译：通过优化标记级困惑度来学习表示。  3. **We propose Continuous Concept Mixing (CoCoMix)**：我们提出了连续概念混合（CoCoMix）。    - 翻译：我们提出了连续概念混合（CoCoMix）。  4. **a novel pretraining framework that combines discrete next token prediction with continuous concepts**：一个将离散的下一个标记预测与连续概念相结合的新型预训练框架。    - 翻译：一个将离散的下一个标记预测与连续概念相结合的新型预训练框架。  5. **Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations**：具体来说，CoCoMix预测从预训练的稀疏自动编码器学习到的连续概念，并通过与标记隐藏表示交织将其混合到模型的隐藏状态中。    - 翻译：具体来说，CoCoMix预测从预训练的稀疏自动编码器学习到的连续概念，并通过与标记隐藏表示交织将其混合到模型的隐藏状态中。  6. **Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks**：通过在多个基准测试中的实验，包括语言建模和下游推理任务。    - 翻译：通过在多个基准测试中的实验，包括语言建模和下游推理任务。  7. **we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens**：我们表明，CoCoMix更高效地利用样本，并且始终优于标准的下一个标记预测、知识蒸馏和插入暂停标记。    - 翻译：我们表明，CoCoMix更高效地利用样本，并且始终优于标准的下一个标记预测、知识蒸馏和插入暂停标记。  8. **We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains**：我们发现，在一个端到端框架中将概念学习和交织相结合对于性能提升至关重要。    - 翻译：我们发现，在一个端到端框架中将概念学习和交织相结合对于性能提升至关重要。  9. **Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept**：此外，CoCoMix通过允许直接检查和修改预测的概念，增强了可解释性和可控性。    - 翻译：此外，CoCoMix通过允许直接检查和修改预测的概念，增强了可解释性和可控性。  10. **offering a transparent way to guide the model's internal reasoning process**：提供了一种透明的引导模型内部推理过程的方法。     - 翻译：提供了一种透明的引导模型内部推理过程的方法。  综合以上步骤，以下是摘要的中文翻译：  下一个标记预测一直是大型语言模型预训练中的标准训练目标。通过优化标记级困惑度来学习表示。我们提出了连续概念混合（CoCoMix）这一新型预训练框架，它将离散的下一个标记预测与连续概念相结合。具体来说，CoCoMix预测从预训练的稀疏自动编码器学习到的连续概念，并通过与标记隐藏表示交织将其混合到模型的隐藏状态中。通过在多个基准测试中的实验，包括语言建模和下游推理任务，我们表明CoCoMix更高效地利用样本，并且始终优于标准的下一个标记预测、知识蒸馏和插入暂停标记。我们发现，在一个端到端框架中将概念学习和交织相结合对于性能提升至关重要。此外，CoCoMix通过允许直接检查和修改预测的概念，增强了可解释性和可控性，提供了一种透明的引导模型内部推理过程的方法。|\n",
        "2502.08515": "|**2025-02-12**|**The Paradox of Stochasticity: Limited Creativity and Computational Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data**|Evgenii Evstafev et.al.|[2502.08515](http://arxiv.org/abs/2502.08515)|null|本研究探讨了温度设置和模型架构如何影响三个大型语言模型（LLMs）生成结构化虚构数据（如姓名、出生日期）。研究对象包括 llama3.1:8b、deepseek-r1:8b 和 mistral:latest。通过系统地测试从0.0到1.0的温度值，以0.1为增量进行330次试验，生成了889个结构化实体，并验证了它们的句法一致性。主要发现表明，模型架构显著影响了计算效率，mistral:latest 和 llama3.1:8b 的数据处理速度比 deepseek-r1:8b 快8倍。与预期相反，温度与处理时间无相关性，挑战了关于随机采样成本的假设。输出多样性仍然有限，因为所有温度下模型都持续默认使用常见的姓名原型（例如，“John Doe”和“Jane Smith”），尽管罕见名称在中间值（0.3-0.7）聚集。这些结果表明，在结构化生成任务中，架构优化而非温度调整主导了性能。这些发现强调了在选择模型时优先考虑效率，并建议在合成数据管道中需要显式多样性约束来减轻默认输出偏差。|\n",
        "2502.08514": "|**2025-02-12**|**Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation**|Mahnaz Koupaee et.al.|[2502.08514](http://arxiv.org/abs/2502.08514)|null|基于大型语言模型（LLMs）的忠实度评估器常常被文本的流畅性所迷惑，难以识别摘要中的错误。我们提出了一种摘要忠实度评估方法，其中多个基于LLMs的代理被分配初始立场（无论他们的信念可能是什么），并被迫提出理由来证明所强加的信念，从而进行多轮辩论以达成一致。均匀分布的初始分配导致立场更加多样化，从而产生更有意义的辩论，并最终识别出更多错误。此外，通过分析最近的忠实度评估数据集，我们发现，摘要要么忠实于源文档，要么不忠实，并不总是如此。因此，我们引入了一个新的维度——模糊性，以及一个详细的分类法来识别这些特殊情况。实验表明，我们的方法有助于识别模糊性，并在非模糊摘要上表现出更强的性能。|\n",
        "2502.08512": "|**2025-02-12**|**Measuring Diversity in Synthetic Datasets**|Yuchang Zhu et.al.|[2502.08512](http://arxiv.org/abs/2502.08512)|null|大型语言模型（LLMs）被广泛用于为各种自然语言处理（NLP）任务生成合成数据集，如文本分类和摘要。然而，准确测量这些合成数据集的多样性——这对于稳健的模型性能至关重要——仍然是一个重大挑战。在本文中，我们引入了DCScore，这是一种从分类角度测量合成数据集多样性的新方法。具体来说，DCScore将多样性评估定义为样本分类任务，利用样本之间的相互关系。我们进一步对DCScore满足的与多样性相关的公理进行了理论验证，突出了其作为原则性多样性评估方法的作用。在合成数据集上的实验结果表明，DCScore与评估数据集的多个多样性伪真实值的相关性更强，凸显了其有效性。此外，实证和理论证据都表明，与现有方法相比，DCScore大幅降低了计算成本。代码可在以下网址获取：https://github.com/BlueWhaleLab/DCScore。|\n",
        "2502.09621": "|**2025-02-13**|**MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency**|Dongzhi Jiang et.al.|[2502.09621](http://arxiv.org/abs/2502.09621)|null|使用思维链（CoT）回答问题显著提高了大型语言模型（LLMs）的推理能力，然而其对大型多模态模型（LMMs）的影响仍缺乏系统评估和深入研究。在本文中，我们介绍了MME-CoT，这是一个专门用于评估LMMs CoT推理性能的基准，涵盖六个领域：数学、科学、OCR、逻辑、时空和一般场景。作为该领域的首次全面研究，我们提出了一套详尽的评估套件，包含三个新颖的指标，从细粒度层面评估推理质量、鲁棒性和效率。利用精心挑选的高质量数据和独特的评估策略，我们对最先进的LMMs进行了深入分析，揭示了几个关键发现：1）具有反思机制的模型表现出更优质的CoT质量，其中Kimi k1.5优于GPT-4o，并展现出最高质量的成果；2）CoT提示往往降低LMM在感知密集型任务上的性能，暗示了可能有害的过度思考行为；3）尽管CoT质量较高，具有反思机制的LMM在正常响应和自我纠正阶段都表现出明显的低效率。我们希望MME-CoT能够为推进LMMs中的多模态推理奠定基础。项目页面：https://mmecot.github.io/|\n",
        "2502.09620": "|**2025-02-13**|**Exploring the Potential of Encoder-free Architectures in 3D LMMs**|Yiwen Tang et.al.|[2502.09620](http://arxiv.org/abs/2502.09620)|null|在二维视觉领域，无编码器架构已被初步探索，然而，它们是否可以有效地应用于三维理解场景仍然是一个悬而未决的问题。在本文中，我们首次全面调查了无编码器架构克服基于编码器的3D大型多模态模型（LMMs）挑战的潜力。这些挑战包括无法适应不同的点云分辨率以及编码器提取的点特征不符合大型语言模型（LLMs）的语义需求。我们确定了3D LMMs去除编码器并使LLM承担3D编码器角色的关键方面：1）我们提出了在预训练阶段的LLM嵌入语义编码策略，探讨了各种点云自监督损失的影响。并且我们提出了混合语义损失以提取高级语义。2）我们在指令微调阶段引入了分层几何聚合策略。这将在LLM早期层中引入归纳偏差，以关注点云的局部细节。最终，我们提出了第一个无编码器3D LMM，ENEL。我们的70亿参数模型与当前最先进的模型ShapeLLM-13B相当，分别在分类、字幕和VQA任务上达到了55.0%、50.92%和42.7%的性能。我们的结果表明，无编码器架构在三维理解领域取代基于编码器的架构具有很高的潜力。代码已发布在https://github.com/Ivan-Tang-3D/ENEL。|\n",
        "2502.09606": "|**2025-02-13**|**Human-LLM Coevolution: Evidence from Academic Writing**|Mingmeng Geng et.al.|[2502.09606](http://arxiv.org/abs/2502.09606)|null|通过对arXiv论文摘要的统计分析，我们发现，自2024年初这些词汇被指出过度使用后，诸如“深入研究”等一些词汇的使用频率明显下降。而像“显著”这样的词汇，其使用频率却持续上升。这些现象表明，一些学术论文的作者已经调整了他们对大型语言模型（LLMs）的使用方式，例如通过选择输出或对LLM生成的内容进行修改。因此，人类与LLMs的这种共同进化和合作给在现实场景中检测机器生成文本带来了额外的挑战。通过检查词汇频率来评估LLMs对学术写作的影响仍然是可行的，应更加关注那些已经频繁使用，甚至频率有所下降的词汇。|\n",
        "2502.09604": "|**2025-02-13**|**SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models**|Yung-Sung Chuang et.al.|[2502.09604](http://arxiv.org/abs/2502.09604)|null|我们介绍了SelfCite，这是一种新颖的自监督方法，它将大型语言模型（LLM）与生成高质量、细粒度、句子级引用的方法对齐，用于其生成的响应中的陈述。SelfCite不是仅仅依赖于昂贵且劳动密集型的标注，而是利用LLM本身通过上下文消融提供的奖励信号：如果引用是必要的，从上下文中移除被引用的文本应防止生成相同的响应；如果足够，仅保留被引用的文本应保留相同的响应。这种奖励可以指导推理时的最佳N个采样策略，显著提高引用质量，同时也可以用于偏好优化，直接微调模型以生成更好的引用。SelfCite的有效性通过在五个长篇问答任务上的LongBench-Cite基准测试中提高引用F1分数高达5.3个百分点得到证明。|\n",
        "2502.09597": "|**2025-02-13**|**Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs**|Siyan Zhao et.al.|[2502.09597](http://arxiv.org/abs/2502.09597)|null|大型语言模型（LLMs）越来越多地被用作聊天机器人，但它们根据用户偏好个性化响应的能力仍然有限。我们引入了PrefEval，这是一个用于评估LLMs在长语境对话环境中推断、记忆和遵守用户偏好的基准。PrefEval包含涵盖20个主题的3,000个手动编纂的用户偏好和查询对。PrefEval以显性和隐性的形式包含用户个性化或偏好信息，并使用生成和分类任务来评估LLMs的性能。通过PrefEval，我们在多会话对话中评估了10个开源和专有LLMs在长达100k个token的上下文长度下的上述偏好跟随能力。我们与各种提示、迭代反馈和检索增强生成方法进行了基准测试。我们的基准测试努力表明，最先进的LLMs在主动跟随用户偏好方面面临着重大挑战。特别是，在零样本设置中，大多数评估模型的偏好跟随准确率在仅仅10个回合（约3k个token）时低于10%。即使使用高级提示和检索方法，偏好跟随在长语境对话中仍然恶化。此外，我们展示了在PrefEval上进行微调可以显著提高性能。我们相信，PrefEval作为衡量、理解和增强LLMs偏好跟随能力的宝贵资源，为个性化对话代理铺平了道路。我们的代码和数据集可在https://prefeval.github.io/获取。|\n",
        "2502.09596": "|**2025-02-13**|**KIMAs: A Configurable Knowledge Integrated Multi-Agent System**|Zitao Li et.al.|[2502.09596](http://arxiv.org/abs/2502.09596)|null|基于大型语言模型（LLMs）的知识密集型对话已成为最受欢迎和最有帮助的应用之一，能够在不同方面协助人们。许多当前的知识密集型应用集中在检索增强生成（RAG）技术上。虽然许多开源的RAG框架促进了基于RAG的应用开发，但它们通常在处理由主题和格式中的异构数据、对话上下文管理和低延迟响应时间要求所带来的复杂实际场景方面存在不足。本技术报告提出了一种可配置的知识集成多代理系统，KIMAs，以解决这些挑战。KIMAs具有一个灵活且可配置的系统，用于整合多样化的知识来源，包括1）上下文管理和查询重写机制，以提高检索准确性和多轮对话的连贯性；2）高效的知识路由和检索；3）简单但有效的过滤和参考生成机制；以及4）优化的可并行化多代理管道执行。我们的工作提供了一个可扩展的框架，以推进LLMs在实际环境中的部署。为了展示KIMAs如何帮助开发者构建不同规模和重点的知识密集型应用，我们演示了如何配置该系统以支持三个已在实践中运行且表现可靠的现有应用。|\n",
        "2502.09589": "|**2025-02-13**|**Logical forms complement probability in understanding language model (and human) performance**|Yixuan Wang et.al.|[2502.09589](http://arxiv.org/abs/2502.09589)|null|随着对在大自然语言中使用大型语言模型（LLMs）进行规划兴趣的增加，理解其行为成为一个重要的研究问题。这项工作对LLMs在自然语言中执行逻辑推理的能力进行了系统性研究。我们引入了一个关于命题逻辑和模态逻辑中假设和选言三段论的控制数据集，并将其用作理解LLM性能的测试平台。我们的结果导致了对预测LLM行为的新的见解：除了输入概率（Gonen等人，2023；McKay等人，2024）外，还应将逻辑形式视为正交因素。此外，我们通过比较LLM和人类的行为结果，展示了人类和LLMs在逻辑推理性能方面的相似性和差异。|\n",
        "2502.09577": "|**2025-02-13**|**Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks**|Qian Wan et.al.|[2502.09577](http://arxiv.org/abs/2502.09577)|null|在撰写第一稿之前，预写作是一个生成和组织想法的过程。它包括一系列非正式、迭代和半结构化的策略，如视觉图示，这在以轮流对话方式与大型语言模型（LLMs）协作时构成了挑战。我们提出了Polymind，这是一个利用多个LLM驱动的代理来支持预写作的视觉图示工具。该系统采用并行协作工作流程，代替了轮流对话的交互方式。它定义了多个“微任务”，以模拟协作写作和团队头脑风暴等群体协作场景。Polymind允许用户同时调度多个微任务，而不是反复提示聊天机器人进行各种目的的操作。用户可以配置和委派定制的微任务，并通过指定任务要求和切换可见性及主动性来管理他们的微任务。我们的评估显示，与ChatGPT相比，用户在使用Polymind进行协作时具有更高的可定制性，因此能够在预写作阶段快速扩展个性化的写作想法。|\n",
        "2502.09566": "|**2025-02-13**|**Zero-shot generation of synthetic neurosurgical data with large language models**|Austin A. Barr et.al.|[2502.09566](http://arxiv.org/abs/2502.09566)|null|临床数据对于神经外科研究至关重要，但获取这些数据往往受到数据可用性、样本量小、隐私法规以及资源密集型的预处理和去识别化程序的制约。合成数据为解决获取和使用真实世界数据（RWD）的挑战提供了潜在解决方案。本研究旨在通过将条件表格生成对抗网络（CTGAN）作为基准，评估大型语言模型（LLM）GPT-4o进行零样本生成合成神经外科数据的能力。合成数据集与真实世界神经外科数据进行比较，以评估其准确性（均值、比例、分布和双变量相关）、实用性（在RWD上的机器学习分类器性能）和隐私性（RWD中记录的重复）。GPT-4o生成的数据集在无需微调或访问RWD进行预训练的情况下，其性能与CTGAN相当或更优。数据集展示了与RWD高度的单变量和双变量准确性，即使在放大样本量的情况下，也没有直接暴露任何真实患者记录。在GPT-4o生成的数据上训练机器学习分类器，并在RWD上进行测试以进行二分类预测任务，结果显示F1分数为0.706，与在CTGAN数据上训练的F1分数（0.705）相当，用于预测术后功能状态恶化。GPT-4o显示出生成高保真合成神经外科数据的巨大潜力。这些发现还表明，使用GPT-4o合成的数据可以有效地增加小样本量临床数据，并训练机器学习模型以预测神经外科结果。为进一步提高分布特征保持度和提升分类器性能，需要进行进一步研究。|\n",
        "2502.09565": "|**2025-02-13**|**MDCrow: Automating Molecular Dynamics Workflows with Large Language Models**|Quintina Campbell et.al.|[2502.09565](http://arxiv.org/abs/2502.09565)|null|分子动力学（MD）模拟对于理解生物分子系统至关重要，但其自动化仍然具有挑战性。最近，大型语言模型（LLM）在利用基于LLM的智能体自动化复杂科学任务方面取得了成功。在本文中，我们介绍了MDCrow，这是一个能够自动化MD工作流程的智能体式LLM助手。MDCrow使用40多个由专家设计的工具链来处理和加工文件、设置模拟、分析模拟输出，并从文献和数据库中检索相关信息。我们对MDCrow在25个具有不同所需子任务和难度的任务上的表现进行了评估，并评估了该智能体对难度和提示风格的鲁棒性。\\texttt{gpt-4o}能够以低方差完成复杂任务，紧随其后的是具有说服力的开源模型\\texttt{llama3-405b}。尽管提示风格不会影响最佳模型的表现，但它对较小模型有显著影响。|\n"
    },
    "infer": {
        "2411.18191": "|**2024-11-27**|**InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks**|Xinyao Zheng et.al.|[2411.18191](http://arxiv.org/abs/2411.18191)|null|大型语言模型（LLMs）具备广泛的知识和问答能力，已在金融和医疗咨询等对隐私敏感的领域得到广泛应用。在LLMs推理过程中，缓存共享方法被广泛采用以提高效率，通过重用缓存的状态或响应来处理相同或相似的推理请求。然而，我们发现这些缓存机制存在隐私输入泄露的风险，因为缓存可能导致响应时间出现可观察的变化，使其成为基于时间攻击的强候选线索。在本研究中，我们提出了一种新颖的基于时间的侧信道攻击，用于在LLMs推理中执行输入窃取。基于缓存的攻击面临在大型搜索空间中构建候选输入以击中和窃取缓存用户查询的挑战。为了解决这些挑战，我们提出了两个主要组件。输入构造器采用机器学习技术和基于LLM的方法进行词汇相关性学习，同时在通用输入构建中实施优化的搜索机制。时间分析器通过异常值去除实现统计时间拟合，以识别缓存命中模式，并持续提供反馈以优化构造器的搜索策略。我们在两种缓存机制上进行了实验，结果表明我们的方法在各种应用中均能持续获得高攻击成功率。我们的工作突出了与性能优化相关的安全漏洞，强调了在LLMs推理增强的同时优先考虑隐私和安全的必要性。|\n",
        "2411.18077": "|**2024-11-27**|**MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache**|Akshat Sharma et.al.|[2411.18077](http://arxiv.org/abs/2411.18077)|null|由于LLMs（大型语言模型）对内存和计算的高要求，如何在实践中高效地为LLMs提供服务变得极为挑战。在本研究中，我们调查了优化KV缓存的方法，因为其内存占用是LLM推理中的关键瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了MiniKV，这是一种KV缓存优化方法，通过一种新颖的2比特层区分性KV缓存，在同时保持长上下文任务准确性的同时，显著减少了KV缓存的大小。更重要的是，我们开发了专门的CUDA内核，使MiniKV与FlashAttention兼容。在广泛的长上下文任务上的实验表明，MiniKV有效地实现了86%的KV缓存压缩比，同时恢复了超过98.5%的准确性，优于现有方法，同时实现了卓越的系统性能提升。|\n",
        "2411.17309": "|**2024-11-26**|**PIM-AI: A Novel Architecture for High-Efficiency LLM Inference**|Cristobal Ortega et.al.|[2411.17309](http://arxiv.org/abs/2411.17309)|null|大型语言模型（LLMs）因其先进的语言理解和生成能力，在众多应用中变得至关重要。然而，它们对计算和内存的要求给传统的硬件架构带来了巨大的挑战。内存中处理（PIM）将计算单元直接集成到内存芯片中，为LLM推理提供了多项优势，包括减少数据传输瓶颈和提高能效。本文介绍了一种名为PIM-AI的新型DDR5/LPDDR5 PIM架构，专为LLM推理设计，无需修改内存控制器或DDR/LPDDR内存PHY。我们开发了一个模拟器来评估PIM-AI在不同场景下的性能，并证明了其相较于传统架构的显著优势。在基于云的场景中，PIM-AI相较于最先进的GPU，将每秒查询的三年总拥有成本降低了高达6.94倍，具体取决于所使用的LLM模型。在移动场景中，PIM-AI相较于最先进的移动SoC，在每token能耗上实现了10到20倍降低，从而实现了每秒查询增加25到45%，每查询能耗减少6.9倍到13.4倍，延长了电池寿命，并使每次充电的推理次数更多。这些结果突显了PIM-AI颠覆LLM部署的潜力，使其更加高效、可扩展和可持续。|\n",
        "2411.17116": "|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，使用Transformer基于的大型语言模型（LLMs）在长序列上进行推理既耗时又昂贵。我们引入了星型注意力，这是一种两阶段的块稀疏近似，通过在多个主机之间分片注意力来提高计算效率，同时最大限度地减少通信开销。在第一阶段，使用并行跨主机的块局部注意力处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星型注意力与大多数使用全局注意力训练的Transformer基于的LLMs无缝集成，通过减少内存需求和推理时间最多11倍，同时保留95-100%的准确率。**|\n",
        "2411.17089": "|**2024-11-26**|**Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation**|Chaoyi Jiang et.al.|[2411.17089](http://arxiv.org/abs/2411.17089)|null|对于大型语言模型（LLMs）的推理计算量很大。为了降低自回归解码的成本，采用键值（KV）缓存来存储中间激活，使得GPU只需进行每个新标记所需的增量计算。这种方法显著降低了标记生成的计算开销。然而，KV缓存的内存需求迅速增长，通常超过GPU内存容量。一种成本效益更高的替代方案是将KV缓存卸载到CPU内存中，这可以缓解GPU内存压力，但将瓶颈转移到CPU和GPU之间有限的PCIe连接带宽。现有方法试图通过重叠GPU计算与I/O或采用CPU-GPU异构执行来解决这些问题，但它们受到过度数据移动和对CPU能力的依赖的阻碍。在本文中，我们介绍了一种高效的CPU-GPU I/O感知LLM推理方法，通过在同时通过PCIe总线传输剩余KV缓存的同时，从激活中重新计算部分KV缓存，避免了将整个KV缓存从CPU传输到GPU。这种方法重叠GPU重新计算与数据传输，以最小化GPU空闲时间并最大化推理性能。我们的方法通过集成一个利用输入特性和系统硬件信息的分析模块、一个用于优化计算和通信工作负载分配的调度模块以及一个用于高效执行派生执行计划的运行时模块而完全自动化。实验结果表明，与最先进的方法相比，我们的方法在解码时的延迟降低了高达35.8%，吞吐量提高了46.2%。|\n",
        "2411.16158": "|**2024-11-25**|**MixPE: Quantization and Hardware Co-design for Efficient LLM Inference**|Yu Zhang et.al.|[2411.16158](http://arxiv.org/abs/2411.16158)|null|基于Transformer的大型语言模型（LLMs）随着模型规模的不断扩大取得了显著的成功，但它们的部署仍然面临挑战，主要是因为计算和内存需求巨大。量化技术已成为一种有前景的解决方案，而针对LLMs的最先进量化算法引入了混合精度矩阵乘法（mpGEMM）的需求，即使用低精度权重与高精度激活进行乘法运算。尽管这种方法有优势，但当前硬件加速器如GPU和TPU缺乏对高效mpGEMM的原生支持，导致主顺序循环中的去量化操作效率低下。为了解决这一限制，我们引入了MixPE，这是一种专门设计的混合精度处理单元，旨在高效地在LLM推理中进行低比特量化。MixPE利用两项关键创新来最小化去量化开销并充分发挥低比特量化的潜力。首先，我们认识到每个量化组内的缩放因子和零点是可以共享的，因此我们提议在每个组mpGEMM之后进行去量化，这显著降低了去量化开销。其次，MixPE不是依赖于传统的乘法器，而是使用高效的移位和加法操作进行乘法运算，从而优化了计算和能效。我们的实验结果表明，MixPE在速度上比最先进的量化加速器快2.6倍，在能耗上减少1.4倍。|\n",
        "2411.16003": "|**2024-11-24**|**eFedLLM: Efficient LLM Inference Based on Federated Learning**|Shengwen Ding et.al.|[2411.16003](http://arxiv.org/abs/2411.16003)|null|大型语言模型（LLMs）标志着人工智能（AI）领域的变革时代。然而，LLMs所涉及的数据和参数规模巨大，需要高要求的计算和内存资源，这限制了它们对更广泛用户和研究者的可及性。本文介绍了一种有效的方法，可以提升LLM推理的操作效率和成本效益。通过利用基于transformer的联邦学习（FL）与模型并行分布式训练，我们的模型能够高效地在参与者网络中分配计算负载和内存需求。这种策略允许用户，尤其是那些资源有限的用户，可以协同训练最先进的LLMs。我们还创新了FL框架中的激励机制，奖励有益的贡献并过滤掉恶意活动，从而保护训练过程的完整性和可靠性。同时，我们利用内存层次策略和权重矩阵的奇异值分解（SVD）进一步提升了计算和内存效率。我们的结果，通过公式分析和数值计算得出，显著优化了资源使用，并使先进LLMs的访问权民主化，确保广泛的用户既能参与也能从中受益。|\n",
        "2411.15982": "|**2024-11-24**|**Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format**|Chao Fang et.al.|[2411.15982](http://arxiv.org/abs/2411.15982)|null|广泛应用的仅权重量化的大型语言模型（LLM），利用低比特整数（INT）权重并保留浮点（FP）激活，在减少存储需求的同时保持了准确性。然而，这将能耗和延迟瓶颈转向了与昂贵的内存访问和计算相关的FP激活。现有的LLM加速器主要关注计算优化，忽略了联合优化FP计算和数据移动的潜力，尤其是在LLM推理中的主导FP-INT GeMM操作。为了解决这些挑战，我们研究了各种LLM模块中激活精度的敏感性及其对整体模型准确性的影响。基于我们的发现，我们首先提出了Anda数据类型：一种具有组共享指数位和动态尾数位分配的自适应数据格式。其次，我们开发了一种迭代的训练后自适应精度搜索算法，优化不同LLM模块的位宽，以平衡模型准确性、能耗和推理速度。最后，提出了一系列硬件优化技术，以最大限度地发挥Anda格式的优势。这包括基于位面的数据组织方案、具有位串计算功能的Anda增强处理单元以及运行时位面Anda压缩器，以同时优化存储、计算和内存占用。我们在FPINT GeMM操作上的评估表明，与GPU类似的FP-FP基准相比，Anda在包括OPT、LLaMA和LLaMA-2系列在内的流行LLM上平均实现了2.4倍的加速、4.0倍的面积效率提升和3.1倍的能耗效率提升。Anda在各种应用场景、精度要求和系统性能方面表现出强大的适应性，使得在广泛的部署场景中实现高效的LLM推理成为可能。|\n",
        "2411.17741": "|**2024-11-24**|**Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments**|Nikoleta Iliakopoulou et.al.|[2411.17741](http://arxiv.org/abs/2411.17741)|null|随着大型语言模型（LLMs）的广泛应用，其部署数量呈指数级增长，对推理集群提出了巨大需求。这些集群必须处理针对不同LLM下游任务的大量并发查询。为了处理具有大量LLM参数的多任务设置，方法如低秩自适应（LoRA）允许针对特定任务进行微调，同时跨任务共享大部分基础LLM模型。因此，它们允许以最小的内存需求并发处理任务。然而，现有的LLM服务系统存在效率低下的问题：它们忽视了工作负载异构性，由于频繁的适配器加载而施加了高链路带宽，以及在调度器中存在头阻塞问题。为了解决这些挑战，我们提出了Chameleon，这是一个针对多个适配器环境优化的新型LLM服务系统，它依赖于两个核心思想：适配器缓存和适配器感知调度。首先，Chameleon在GPU内存中缓存流行的适配器，最小化适配器加载时间。重要的是，它使用原本闲置的GPU内存，避免了额外的内存成本。其次，Chameleon使用非抢占式多队列调度，以高效地处理工作负载异构性。通过这种方式，Chameleon同时防止了头阻塞和饥饿现象。我们在最先进的LLM服务平台之上实现了Chameleon，并使用真实世界的生产跟踪和开源LLM对其进行了评估。在高负载下，Chameleon将P99和P50的TTFT延迟分别降低了80.7%和48.1%，同时与最先进的基线相比，提高了1.5倍的吞吐量。|\n",
        "2411.15715": "|**2024-11-24**|**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**|Wenxiang Lin et.al.|[2411.15715](http://arxiv.org/abs/2411.15715)|null|大型语言模型（LLMs）因其庞大的模型尺寸而闻名，对计算资源和内存需求极高，导致在中等GPU系统上的推理效率低下。量化或剪枝等技术可以缩小模型尺寸，但通常会损害准确度，使其不适合实际应用。在这项工作中，我们介绍了\\modelname{}，这是一个高性能的推理引擎，旨在加快LLMs的推理速度，同时不降低模型精度。\\modelname{}采用了三种创新方法来提高推理效率：1）模型分区，允许跨CPU计算、GPU计算和CPU-GPU通信异步处理任务，2）自适应分区算法，以优化CPU、GPU和PCIe通信能力的利用，3）令牌分配策略，用于处理LLMs推理过程中的各种提示和生成任务。我们使用Mixtral、LLaMA-2、Qwen和PhiMoE等LLMs，在具有不同CPU和GPU的三个测试环境中进行了综合实验。实验结果表明，\\modelname{}在解码速度上比$1.11\\times$到$1.80\\times$更快，在预填充速度上比$1.69\\times$到$6.33\\times$更快，与最先进的解决方案llama.cpp和Fiddler相比，整体速度提高了$1.25\\times$到$2.04\\times$。|\n",
        "2411.19542": "|**2024-11-29**|**A dynamic parallel method for performance optimization on hybrid CPUs**|Luo Yu et.al.|[2411.19542](http://arxiv.org/abs/2411.19542)|null|AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，当前的AI推理框架忽略了混合CPU硬件能力的失衡，导致推理性能低下。为了解决这个问题，我们引入了一种针对混合CPU的动态并行方法，该方法通过在并行工作开始之前平衡混合CPU每个核心的工作负载，显著提高了大型语言模型（LLM）的推理性能。这种方法使Neural Speed能够在两个混合英特尔CPU上实现超过90%（平均）的内存带宽。|\n",
        "2411.19146": "|**2024-11-28**|**Puzzle: Distillation-Based NAS for Inference-Optimized LLMs**|Akhiad Bercovich et.al.|[2411.19146](http://arxiv.org/abs/2411.19146)|null|大型语言模型（LLMs）展示了惊人的能力，但它们的采用受到推理过程中高计算成本的限制。虽然增加参数数量可以提高准确性，但它也拉大了最先进的能力与实际部署之间的差距。我们提出了Puzzle框架，该框架在特定硬件上加速LLMs的推理，同时保持其能力。通过前所未有的规模创新性地应用神经架构搜索（NAS），Puzzle在硬件约束下系统地优化了具有数十亿参数的模型。我们的方法利用块状局部知识蒸馏（BLD）进行并行架构探索，并采用混合整数规划进行精确的约束优化。我们通过Llama-3.1-Nemotron-51B-Instruct（Nemotron-51B）这一公开可用的模型展示了我们框架的实际影响，该模型由Llama-3.1-70B-Instruct衍生而来。Nemotron-51B实现了2.17倍的推理吞吐量加速，可以在单个NVIDIA H100 GPU上运行，同时保留了原始模型98.4%的能力。Nemotron-51B是目前能够以大批次在单个GPU上进行推理的最准确的语言模型。值得注意的是，这种转变只需要45B个训练令牌，而它所衍生的70B模型则需要超过15T个令牌。这建立了一个新的范式，即强大的模型可以通过仅牺牲微小能力来优化高效的部署，这表明推理性能而非参数数量本身应指导模型选择。随着Nemotron-51B的发布和Puzzle框架的介绍，我们为从业者提供了以显著降低的计算成本访问最先进语言建模能力的即时途径。|\n",
        "2412.02252": "|**2024-12-03**|**Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity**|Da Ma et.al.|[2412.02252](http://arxiv.org/abs/2412.02252)|null|随着大型语言模型（LLMs）如GPT和LLaMA系列中上下文窗口大小的增加，它们处理复杂、长文本任务的能力得到了提升，但代价是推理效率的降低，尤其是在内存和计算复杂度方面。现有方法，包括选择性保留标记和基于窗口的注意力机制，虽然提高了效率，但有可能丢弃未来文本生成所需的重要标记。在本文中，我们提出了一种方法，通过减少不重要标记的内存和计算负载来提高LLM效率，而不丢失标记。我们解决了两个挑战：1）研究上下文中重要标记的分布，发现最近标记比上下文中的远距离标记更重要；2）通过跨层共享注意力分数来优化远距离标记的资源。实验表明，我们的方法在不影响性能的情况下节省了35%的KV缓存。|\n",
        "2412.01447": "|**2024-12-02**|**PLD+: Accelerating LLM inference by leveraging Language Model Artifacts**|Shwetha Somasundaram et.al.|[2412.01447](http://arxiv.org/abs/2412.01447)|null|为了降低自回归语言模型（LLM）推理的延迟，预测解码（speculative decoding）作为一种新的解码范式应运而生，在这种范式中，未来的标记（tokens）被并行地起草和验证。然而，预测解码的实际部署受到其对额外计算资源和微调的需求的限制，这限制了其即插即用的实用性。为了应对这些挑战，我们提出了一种名为PLD+的新算法套件，旨在加速LLM的推理过程，特别是针对输入引导的任务。这些任务包括代码编辑、文本编辑、摘要等，它们的输出往往与输入有大量的重叠，这是PLD+设计时要利用的属性。PLD+还利用推理过程中产生的副产品（注意力机制和隐藏状态）来加速推理速度。我们在五个输入引导任务上测试了我们的方法，并通过广泛的实验发现，PLD+优于所有无需微调的方法。在贪婪设置中，它在四个任务上甚至优于最先进的依赖微调的方法EAGLE（平均加速率提高了2.31）。我们的方法无需微调，不需要任何额外的计算资源，并且可以轻松用于加速任何LLM的推理。|\n",
        "2412.01380": "|**2024-12-02**|**Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking**|Marco Federici et.al.|[2412.01380](http://arxiv.org/abs/2412.01380)|null|随着移动设备提供的计算能力不断增强，DRAM带宽的提升速度却相对较慢。这对大型语言模型（LLM）的token生成来说是个不幸的事，因为其高度依赖于内存。先前的研究提出利用ReLU激活的LLM中的自然动态激活稀疏性来减少每个token的有效DRAM带宽。然而，最新的LLM使用SwiGLU而非ReLU，这导致几乎没有固有的稀疏性。虽然SwiGLU的激活可以根据幅度进行剪枝，但产生的稀疏模式难以预测，使得先前的方法失效。为了解决这个问题，我们的工作引入了动态输入剪枝（DIP）：一种无预测器的动态稀疏化方法，它通过最小的微调来保留准确性。DIP还可以使用轻量级的LoRA适配器来恢复在稀疏化过程中损失的一些性能。最后，我们描述了一种新的缓存感知掩码策略，它考虑缓存状态和激活幅度以进一步提高缓存命中率，从而提高移动设备上LLM的token速率。在DIP中，与模拟硬件设置中的其他方法相比，它在准确性、内存和吞吐量之间的权衡方面表现更优。在Phi-3-Medium上，DIP实现了内存减少46%、吞吐量增加40%，同时困惑度损失小于0.1。|\n",
        "2412.01129": "|**2024-12-02**|**RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy**|Geonho Lee et.al.|[2412.01129](http://arxiv.org/abs/2412.01129)|null|低秩自适应（LoRA）已成为参数高效的LLM微调的主要方法，基于LoRA的量化误差补偿（LQEC）也作为一种强大的工具，用于恢复压缩LLM的准确性。然而，LQEC在4位以下的情况下表现不佳，但此前没有对这一限制进行过研究。我们提出了RILQ（基于低秩自适应的秩无关量化误差补偿）来理解基本限制并提升2位LLM的准确性。基于对模型激活差异损失秩无关性质的分析，RILQ使用这种损失在层间协同调整适配器，使得低秩适配器能够实现鲁棒的误差补偿。在LLaMA-2和LLaMA-3上的评估表明，RILQ在各种最先进的量化器上对2位量化推理的一致性改进，以及在特定任务微调中的准确性提升。RILQ保持了与现有LoRA方法相当的计算效率，使得适配器合并权重量化LLM推理的准确性显著提升，成为提升2位LLM性能的有前途的方法。|\n",
        "2412.01042": "|**2024-12-02**|**TruncFormer: Private LLM Inference Using Only Truncations**|Patrick Yubeaton et.al.|[2412.01042](http://arxiv.org/abs/2412.01042)|null|私有推理（PI）在用户数据与专有机器学习模型（如LLMs）交互时，保证了用户数据的隐私性。然而，由于LLMs中存在的非线性函数带来的巨大延迟成本，PI在实践中变得难以处理。现有工作主要关注通过近似来提高特定LLM非线性（如Softmax或GeLU）的延迟。然而，随着新的LLM架构的引入，新的非线性类型也在不断出现，这导致了PI研究人员在优化最新非线性函数方面的不断追赶。我们引入了TruncFormer，这是一个将任何LLM转换为PI明文仿真的框架。我们的框架利用了LLM中的非线性函数是可微分的，并且可以用一系列加法、乘法和截断来精确近似的事实。此外，我们将加/乘操作与截断操作解耦，并根据给定的字段大小和输入表示大小静态确定应在哪里插入截断。这导致在现有加密协议中，每进行一次乘法操作后都需要截断的情况下，延迟得到了改进。我们开源了我们的代码以供社区使用。|\n",
        "2412.03594": "|**2024-11-29**|**BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching**|Zhen Zheng et.al.|[2412.03594](http://arxiv.org/abs/2412.03594)|null|许多大型语言模型（LLM）的任务在大批量或离线情况下执行，其性能指标为吞吐量。这些任务通常具有前缀共享的特征，即不同的提示输入可以部分显示共同的prefix。然而，现有的LLM推理引擎往往优化流式请求，在支持具有前缀共享特性的大批量任务方面存在局限性。现有解决方案使用基于LRU的缓存来重用共同前缀的KV上下文。即将被重用的KV上下文可能会因隐式缓存管理而被提前移除。即使没有被移除，共享的KV上下文的生命周期也会因为共享相同上下文的请求没有被一起调度而延长，导致更大的内存使用。这些以流为方向的系统按照先来先服务的顺序调度请求。结果，解码步骤比例较大的请求可能调度得太晚，无法与预填充块混合，从而提高硬件利用率。此外，基于令牌和请求数量的批量处理可能会限制令牌批量的大小，这会防止GPU在主要由解码令牌控制的迭代中饱和。我们提出了BatchLLM来解决这个问题。BatchLLM显式地识别全局的共同prefix。具有相同prefix的请求将被一起调度，以最佳方式重用KV上下文，这也有助于缩短共同KV内存的生命周期。BatchLLM重新排序请求，优先调度解码步骤比例较大的请求，以便更好地将解码令牌与后续预填充块混合，并采用以内存为中心的令牌批量处理来扩大令牌批量大小，这有助于提高GPU利用率。广泛的评估表明，在一系列微基准测试和两个典型的行业工作负载上，BatchLLM的性能优于vLLM 1.1倍到2倍。|\n",
        "2412.04788": "|**2024-12-06**|**GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments**|Yanyu Chen et.al.|[2412.04788](http://arxiv.org/abs/2412.04788)|null|将大型语言模型（LLMs）高效地部署到实际场景中仍然是一个关键挑战，这主要归因于硬件异构性、推理框架的限制和工作负载的复杂性。这些挑战通常导致内存利用效率低下、延迟波动以及吞吐量不充分，阻碍了LLMs的有效部署，尤其是对于非专业人士。通过广泛的实验，我们确定了关键的性能瓶颈，包括内存利用率突然下降、随着批量大小变化的延迟波动以及多GPU配置中的效率低下。这些见解揭示了一个由硬件、框架和工作负载参数的复杂相互作用所塑造的巨大优化空间。这强调了系统性地优化LLM推理的必要性，从而推动了我们的框架GUIDE的设计。GUIDE利用动态建模和基于模拟的优化来解决这个问题，实现了关键指标（如批量延迟、TTFT和解码吞吐量）的预测误差在25%到55%之间。通过有效地弥合理论性能与实际部署之间的差距，我们的框架使实践者，特别是非专业人士，能够做出数据驱动的决策，并以低成本释放LLMs在异构环境中的全部潜力。|\n",
        "2412.04504": "|**2024-12-03**|**Multi-Bin Batching for Increasing LLM Inference Throughput**|Ozgur Guldogan et.al.|[2412.04504](http://arxiv.org/abs/2412.04504)|null|随着大型语言模型（LLM）因其多样化的功能而越来越受欢迎，提高其推理系统的效率变得越来越关键。在服务器（例如GPU）上调度推理作业时，批处理LLM请求是一个关键步骤，这使系统能够通过允许多个请求并行处理来最大化吞吐量。然而，请求往往具有不同的生成长度，导致资源利用率低下，因为硬件必须在批处理中等待运行时间最长的请求完成后才能转到下一个批次。我们从排队论的角度正式化这个问题，并旨在设计一个吞吐量最优的控制策略。我们提出了多箱批处理（Multi-Bin Batching），这是一种简单而有效的方法，可以将具有相似（预测的）执行时间的请求分组到预定的箱中，从而可以证明地提高LLM推理吞吐量。通过理论分析和实验的结合，包括现实世界的LLM推理场景，我们证明了与标准批处理方法相比，显著提高了吞吐量。|\n",
        "2412.06198": "|**2024-12-09**|**SparseAccelerate: Efficient Long-Context Inference for Mid-Range GPUs**|James Vo et.al.|[2412.06198](http://arxiv.org/abs/2412.06198)|null|随着大型语言模型（LLMs）扩展到更长的上下文窗口，传统的注意力机制的计算成本随输入长度的平方增长，这为实时和内存受限的部署带来了关键挑战。现有的稀疏注意力技术试图降低这种复杂性，但它们通常会产生显著的开销或降低准确性，这使得它们在中等硬件上对大上下文来说不太实用。在本文中，我们介绍了SparseAccelerate，这是一种动态稀疏注意力方法，它根据输入特征调整其稀疏模式，有效地平坦化了注意力复杂度曲线。我们的方法对于输入长度从16K个标记开始就非常有效，并在双NVIDIA A5000 GPU（每个24GB）上高效扩展到128K个标记。实验结果表明，SparseAccelerate在32K个标记时，将首次标记延迟（TTFT）降低了高达1.04倍，同时提供了显著的内存节省。这些改进为内存密集型应用和长上下文任务带来了实际效益，这些任务在标准注意力下之前是不可行的。除了延迟降低之外，SparseAccelerate在竞争方法中相对于上下文长度的TTFT增长梯度最小，从而根本改变了扩展趋势。在多种基准测试上的持续评估证实了其可扩展性，将SparseAccelerate定位为在可访问硬件上实现高效、实时和长上下文LLM推理的关键进步。|\n",
        "2412.05896": "|**2024-12-08**|**XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference**|Weizhuo Li et.al.|[2412.05896](http://arxiv.org/abs/2412.05896)|null|最近，生成式大型语言模型（LLM）在众多应用中取得了显著的成功。值得注意的是，其推理过程是逐个生成输出标记，导致许多冗余计算。广泛使用的KV-Cache框架在时间和空间复杂度之间做出了权衡。然而，缓存数据会导致内存需求不断增长，这可能会迅速耗尽现代加速器（如GPU）有限的内存容量，尤其是在长上下文推理任务中。现有研究通过淘汰对推理精度影响较小的部分缓存数据来减少内存消耗。但由于LLM网络层之间静态的缓存分配，实际效果远非理想。本文观察到，特定层的缓存数据对准确度的影响非常不同。我们量化了这种差异，并提供了实验和理论验证。据此，我们进行了形式化分析，表明以个性化方式为每个层定制缓存大小可以显著减少内存消耗，同时仍然提供可比的准确度。我们将缓存分配模拟为一个组合优化问题，并给出全局最优解。特别是，我们设计了一个基于轻量级LLM模型的小型采样推理，以便快速捕捉差异，并将其输入到个性化算法中。在真实世界数据集上的大量实验表明，我们的建议可以将KV缓存内存消耗平均降低61.6%，计算效率提高2.1倍，吞吐量提高高达5.5倍。|\n",
        "2412.07017": "|**2024-12-09**|**Asynchronous LLM Function Calling**|In Gim et.al.|[2412.07017](http://arxiv.org/abs/2412.07017)|null|大型语言模型（LLMs）通过函数调用与外部工具和数据源进行交互。然而，当前LLM函数调用的方法本质上具有同步性，每次调用都会阻塞LLM推理，限制了LLM的操作和并发函数执行。在本研究中，我们提出了AsyncLM，这是一个用于异步LLM函数调用的系统。AsyncLM通过允许LLMs并发生成和执行函数调用，提高了LLM的操作效率。AsyncLM引入了一个中断机制，在函数调用返回时异步通知正在进行的LLM，而不是等待每个调用完成。我们为函数调用和中断设计了上下文协议，提供了微调策略以适应中断语义，并在LLM推理过程中高效地实现了这些机制。我们证明了AsyncLM可以将端到端任务完成延迟从1.6倍到5.4倍降低，这是在伯克利函数调用排行榜（BFCL）上的一系列基准任务中的结果。此外，我们还讨论了如何扩展中断机制以实现新颖的人机LLM或LLM-LLM交互。|\n",
        "2412.08585": "|**2024-12-11**|**TurboAttention: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大型语言模型（LLM）推理需要大量的计算和内存，尤其是在关键的关注机制上。虽然量化技术和加速算法，如FlashAttention，已经提高了整体推理的效率，但它们针对问题的不同方面：量化关注于权重-激活操作，而FlashAttention虽然提高了执行效率，但需要高精度格式。最近的关键值（KV）缓存量化减少了内存带宽，但仍然需要在关注操作中执行浮点数反量化。我们提出了TurboAttention，这是一种实现关注量化执行的综合方法，同时解决了内存和计算效率的问题。我们的解决方案引入了两项关键创新：FlashQ，一种头部关注量化技术，它既能够压缩KV缓存，又能够实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它消除了在关注操作中指数运算期间进行FP32反量化的需求。实验结果表明，TurboAttention在关注操作上实现了1.2-1.8倍的速度提升，将KV缓存大小减少了超过4.4倍，并且相对于FP16基线，实现了高达2.37倍的最大吞吐量，同时在各种数据集和模型上优于现有的量化和压缩技术。|\n",
        "2412.08281": "|**2024-12-11**|**Lachesis: Predicting LLM Inference Accuracy using Structural Properties of Reasoning Paths**|Naryeong Kim et.al.|[2412.08281](http://arxiv.org/abs/2412.08281)|null|大型语言模型越来越多地被用于构建执行更复杂任务的智能体。随着LLM通过更长时间的交互进行更复杂的推理，自洽性，即从多个独立推理的样本中进行采样和边缘化所获得的答案更有可能是正确的这一观点，作为一种简单的验证技术，已经引起了广泛关注。本文旨在通过预测从推理路径样本的性质中获得的自洽性答案的正确性来实证验证这一直观假设。我们引入了Lachesis，这是一个基于自洽性的LLM推理的预测模型，并使用AutoFL（一种最近提出的基于LLM的错误定位技术）作为目标技术进行实证评估。Lachesis使用专门设计的推理路径表示将AutoFL收集的推理路径进行转换，并训练LSTM和GCN模型来预测一组给定的推理路径是否会导致正确的答案。结果表明，Lachesis可以以高达0.8136的精度预测答案的正确性，突显了训练一个能够允许提前终止不太可能成功的推理的预测模型的可行性。|\n",
        "2412.08237": "|**2024-12-11**|**TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch**|Xingchen Song et.al.|[2412.08237](http://arxiv.org/abs/2412.08237)|null|众所周知，基于LLM的系统对数据需求量大。近年来，基于LLM的TTS工作通常采用复杂的数据处理管道来获取高质量的训练数据。这些复杂的管道在每个阶段（例如，语音降噪、语音增强、说话人分割和标点模型）都需要优秀的模型，而这些模型本身又需要高质量的训练数据，并且很少开源。即使使用最先进的模型，仍然存在一些问题，如背景噪声去除不完整和标点与实际语音停顿不匹配。此外，严格的过滤策略通常只保留原始数据的10-30%，这极大地阻碍了数据扩展的努力。在这项工作中，我们利用一个噪声鲁棒的音频分词器（S3Tokenizer）设计了一个简化但有效的TTS数据处理管道，在保持数据质量的同时，大幅降低了数据获取成本，实现了超过50%的数据保留率。除了数据扩展的挑战之外，基于LLM的TTS系统与传统的方案相比，部署成本也更高。当前的系统通常仅使用LLM进行文本到标记的生成，而需要单独的模型（例如，流匹配模型）进行标记到波形生成，这些模型不能直接由LLM推理引擎执行，从而进一步复杂化了部署。为了解决这些挑战，我们消除了LLM和流组件中的冗余模块，用LLM架构替换了流模型主干。在此基础上，我们提出了一种统一的架构，用于流和非流推理，显著降低了部署成本。最后，我们探讨了使用相同数据进行TTS和ASR任务训练的可行性，这得益于简化的管道和S3Tokenizer，它降低了TTS训练数据的质量要求。|\n",
        "2412.10319": "|**2024-12-13**|**SCBench: A KV Cache-Centric Analysis of Long-Context Methods**|Yucheng Li et.al.|[2412.10319](http://arxiv.org/abs/2412.10319)|null|长上下文LLM（大型语言模型）虽然促进了众多下游应用，但也带来了与计算和内存效率相关的重大挑战。为了解决这些挑战，围绕KV缓存的长上下文推理优化已经得到发展。然而，现有的基准测试通常只评估单次请求，忽略了KV缓存在实际应用中的完整生命周期。这种疏忽尤为关键，因为KV缓存重用已成为LLM推理框架（如vLLM和SGLang）以及OpenAI、Microsoft、Google和Anthropic等LLM提供商广泛采用的策略。为了填补这一空白，我们引入了SCBench（共享上下文基准），这是一个从KV缓存中心视角全面评估长上下文方法的基准：1）KV缓存生成，2）KV缓存压缩，3）KV缓存检索，4）KV缓存加载。具体来说，SCBench使用具有共享上下文的测试示例，包括12个任务和两种共享上下文模式，涵盖四种长上下文能力类别：字符串检索、语义检索、全局信息和多任务。凭借它，我们提供了对包括门控线性RNN、Mamba-Attention混合体以及高效的稀疏注意力、KV缓存丢弃、量化、检索、加载和提示压缩在内的八类长上下文解决方案的广泛分析。评估在8个长上下文LLM上进行。我们的发现表明，内存小于O(n)的方法在多轮场景中表现不佳，而具有O(n)内存和小于O(n^2)预填充计算的稀疏编码表现稳健。动态稀疏性比静态模式产生更具表现力的KV缓存，而混合架构中的层级稀疏性在保持强大性能的同时减少了内存使用。此外，我们还识别出在长生成场景中注意力分布偏移的问题。https://aka.ms/SCBench。|\n",
        "2412.11741": "|**2024-12-16**|**CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation**|Hongxuan Zhang et.al.|[2412.11741](http://arxiv.org/abs/2412.11741)|null|随着利用大型语言模型（LLMs）的长文本应用的出现，带来了显著的扩展性挑战，尤其是在内存占用方面。负责存储注意力键和值以减少冗余计算的关键值（KV）缓存的线性增长可能会导致内存消耗的显著增加，这可能导致模型在有限的内存资源下无法正常服务。为了解决这个问题，我们提出了一种名为缓存稀疏表示（CSR）的新方法，该方法通过将密集的键值缓存张量转换为稀疏索引和权重，在LLM推理过程中提供更高效的内存表示。此外，我们引入了一种名为NeuralDict的新方法，这是一种基于神经网络的自动生成用于我们稀疏表示的字典的方法。我们广泛的实验表明，CSR在性能上与最先进的KV缓存量化算法相当，同时在内存受限环境中保持稳健的功能。|\n",
        "2412.11120": "|**2024-12-15**|**Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning**|Yun Qu et.al.|[2412.11120](http://arxiv.org/abs/2412.11120)|**[link](https://github.com/cloud-qu/lare)**|**强化学习（RL）在实际应用中常常面临延迟和稀疏的反馈，即使在只有阶段性奖励的情况下。先前的方法在奖励重新分配方面取得了一些进展，但仍面临挑战，包括由于冗余和忽略任务绩效评估的多面性而导致的模糊归因所引起的训练困难。希望大型语言模型（LLM）包含了丰富的决策知识，并为奖励重新分配提供了一种合理的工具。尽管如此，由于语言知识与符号形式要求之间的不匹配，以及推理中固有的随机性和幻觉，将LLM应用于此非同小可。为了解决这些问题，我们引入了LaRe，这是一种新型LLM赋能的基于符号的决策框架，以提高信用分配。LaRe的关键是潜在奖励的概念，它作为多维度的性能评估，能够从不同角度实现更可解释的目标达成，并促进更有效的奖励重新分配。我们考察了由LLM生成的语义代码如何将语言知识与符号潜在奖励联系起来，因为它可以用于符号对象。同时，我们设计了潜在奖励的自验证，以提高LLM推理的稳定性和可靠性。从理论上讲，潜在奖励中的与奖励无关的冗余消除有助于从更准确的奖励估计中提高RL性能。广泛的实验结果表明，LaRe（i）在时间信用分配方面优于SOTA方法，（ii）在分配多个代理之间的贡献方面表现卓越，（iii）在特定任务中优于使用真实奖励训练的策略。**|\n",
        "2412.11053": "|**2024-12-15**|**NITRO: LLM Inference on Intel Laptop NPUs**|Anthony Fei et.al.|[2412.11053](http://arxiv.org/abs/2412.11053)|**[link](https://github.com/abdelfattah-lab/nitro)**|**大型语言模型（LLMs）已成为自然语言处理领域的关键工具，在ChatGPT和Gemini等聊天机器人中得到了广泛应用，并成为研究的一个核心领域。一个特别感兴趣的研究方向包括为这些AI应用设计专门的硬件，其中之一就是神经处理单元（NPU）。2023年，英特尔发布了代号为Meteor Lake的Intel Core Ultra处理器，它集成了CPU、GPU和NPU系统芯片。然而，通过英特尔OpenVINO框架对NPU的官方软件支持仅限于静态模型推理。因此，LLMs中自回归标记生成的动态特性无法直接支持。为了解决这一不足，我们提出了NITRO（NPU推理优化器），这是一个基于OpenVINO构建的Python框架，用于在NPU上支持文本和聊天生成。在本文中，我们详细讨论了对Transformer架构所做的关键修改以实现推理、一些性能基准测试以及改进该软件包的下一步计划。NITRO的代码库可以在这里找到：https://github.com/abdelfattah-lab/nitro。**|\n",
        "2412.12687": "|**2024-12-17**|**Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models**|Seungeun Oh et.al.|[2412.12687](http://arxiv.org/abs/2412.12687)|null|本文研究了一种混合语言模型（HLM）架构，该架构将运行在移动设备上的小型语言模型（SLM）与无线网络基站（BS）托管的大型语言模型（LLM）相结合。HLM的令牌生成过程遵循推测性推理原则：SLM的词汇分布被上传到LLM，LLM接受或拒绝它，被拒绝的令牌由LLM重新采样。虽然这种方法确保了SLM和LLM词汇分布的一致性，但由于上行传输和运行两个语言模型的计算成本，它遭受了低令牌吞吐量的困扰。为了解决这个问题，我们提出了一种名为不确定性感知机会HLM（U-HLM）的新型HLM结构，其中SLM在本地测量其输出不确定性，并跳过可能被接受的令牌的上行传输和LLM操作。这种机会性跳过是通过我们关于SLM的不确定性与LLM的拒绝概率之间存在线性相关性的经验发现实现的。我们解析地推导出不确定性阈值，并评估其预期的拒绝风险。仿真表明，U-HLM减少了45.93%的上行传输和LLM计算，同时实现了高达LLM推理精度的97.54%，并且比没有跳过的HLM快2.54倍。|\n",
        "2412.12488": "|**2024-12-17**|**A System for Microserving of LLMs**|Hongyi Jin et.al.|[2412.12488](http://arxiv.org/abs/2412.12488)|null|最近在大型语言模型（LLM）方面的进步，对高效系统支持的需求日益强烈，以提升整体服务效率。随着LLM推理扩展到多个GPU甚至多个计算节点，服务系统中出现了各种协调模式，如预填充-解码解耦和上下文迁移。目前大多数推理服务都暴露了具有预先配置协调策略的粗粒度请求级API，这限制了自定义和动态重新配置协调的能力。在本文中，我们提出了LLM微服务，这是一种用于构建和编程LLM推理服务的多层次架构。我们引入了简单而有效的微服务API来支持细粒度的子请求级操作。一个可编程的路由器将用户请求转换为子请求调用，使服务模式能够动态重新配置。为了支持多样化的执行模式，我们开发了一个统一的KV缓存接口，用于处理各种KV计算、传输和重用场景。我们的评估表明，LLM微服务可以通过几行Python代码重新配置，以支持多种解耦编排策略，同时在LLM推理任务中保持最先进的性能。此外，它还使我们能够探索新的策略变体，与现有策略相比，这些变体可以将作业完成时间缩短高达47%。|\n",
        "2412.14352": "|**2024-12-18**|**A Survey on LLM Inference-Time Self-Improvement**|Xiangjue Dong et.al.|[2412.14352](http://arxiv.org/abs/2412.14352)|**[link](https://github.com/dongxiangjue/Awesome-LLM-Self-Improvement)**|**最近，通过测试时增加计算来增强推理的技术受到了关注。在本调查中，我们从三个不同的角度研究了LLM推理时自我改进的现状：独立自我改进，侧重于通过解码或采样方法进行增强；上下文感知自我改进，利用额外的上下文或数据存储；以及模型辅助自我改进，通过模型协作实现改进。我们提供了对最近相关研究的全面回顾，贡献了一个深入的分类法，并讨论了挑战和局限性，为未来的研究提供了洞见。**|\n",
        "2412.15803": "|**2024-12-20**|**WebLLM: A High-Performance In-Browser LLM Inference Engine**|Charlie F. Ruan et.al.|[2412.15803](http://arxiv.org/abs/2412.15803)|**[link](https://github.com/mlc-ai/web-llm)**|**大型语言模型（LLMs）的进步解锁了令人瞩目的能力。虽然部署这些模型通常需要服务器级GPU和基于云的推理，但最近出现的较小规模的开源模型以及越来越强大的消费级设备使得在设备上部署成为可能。作为设备上部署的平台，网络浏览器具有普遍的易用性，提供了自然的代理环境，并且方便地将不同设备厂商的后端抽象出来。为了应对这一机遇，我们引入了WebLLM，这是一个开源的JavaScript框架，它使得LLMs的高性能推理完全在浏览器中成为可能。WebLLM提供了一个类似OpenAI风格的API，以便无缝集成到Web应用程序中，并利用WebGPU进行高效的本地GPU加速和WebAssembly进行高性能的CPU计算。通过机器学习编译器MLC-LLM和Apache TVM，WebLLM利用优化的WebGPU内核，克服了高性能WebGPU内核库的缺失。评估显示，WebLLM在相同设备上可以保留高达80%的本地性能，并有进一步缩小差距的空间。WebLLM为网络浏览器中的普遍可访问、隐私保护、个性化以及本地驱动的LLM应用程序铺平了道路。代码可在以下链接获取：https://github.com/mlc-ai/web-llm。**|\n",
        "2412.16434": "|**2024-12-21**|**SYMPHONY: Improving Memory Management for LLM Inference Workloads**|Saurabh Agarwal et.al.|[2412.16434](http://arxiv.org/abs/2412.16434)|null|大型语言模型（LLMs）正越来越多地应用于聊天机器人、代码编辑器和对话代理等应用中。LLMs的一个关键特性是它们能够与人类或外部工具进行多轮交互，从而实现各种任务。在多轮交互中，每个新的请求都依赖于先前请求的中间状态，特别是当前交互中的键值（K，V）缓存。现有的服务引擎要么重新计算K，V缓存，要么将其卸载到主内存中。分析发现，重新计算会导致超过99%的处理令牌是冗余的。另一方面，将K，V缓存从GPU内存卸载会使推理服务状态化，导致集群负载不均衡。为了解决这些挑战，我们开发了SYMPHONY。SYMPHONY利用观察到的多轮工作负载提供了额外的提示，这些提示允许将K，V缓存迁移出关键的服务路径。通过利用这些提示，SYMPHONY动态迁移K，V缓存，以实现推理请求的细粒度调度。我们的实验表明，与最先进的基础线相比，SYMPHONY可以处理超过8倍数量的请求，并且具有相似的延迟特征。|\n",
        "2412.19442": "|**2024-12-27**|**A Survey on Large Language Model Acceleration based on KV Cache Management**|Haoyang Li et.al.|[2412.19442](http://arxiv.org/abs/2412.19442)|**[link](https://github.com/treeai-lab/awesome-kv-cache-management)**|**大型语言模型（LLMs）由于能够理解上下文并执行逻辑推理，已经在自然语言处理、计算机视觉和多模态任务等多个领域实现了革命性的变革。然而，LLMs在推理过程中的计算和内存需求，尤其是在扩展到现实世界、长上下文和实时应用时，带来了重大挑战。键值（KV）缓存管理作为一种关键优化技术，通过减少冗余计算和提升内存利用率，已成为加速LLM推理的重要手段。本文综述了用于加速LLM的KV缓存管理策略，将它们分为基于标记、模型和系统三个层次的优化。基于标记的策略包括KV缓存选择、预算分配、合并、量化以及低秩分解，而模型级优化则关注架构创新和注意力机制，以提高KV的重用率。系统级方法则解决内存管理、调度和硬件感知设计等问题，以提高不同计算环境下的效率。此外，本文还概述了用于评估这些策略的文本和多模态数据集及基准。通过提供详细的分类和比较分析，本研究旨在为研究人员和实践者提供有价值的见解，以支持高效和可扩展的KV缓存管理技术的发展，为LLMs在现实世界应用中的实际部署做出贡献。KV缓存管理的精选论文列表见：\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}。**|\n",
        "2412.19394": "|**2024-12-27**|**An Engorgio Prompt Makes Large Language Model Babble on**|Jianshuo Dong et.al.|[2412.19394](http://arxiv.org/abs/2412.19394)|**[link](https://github.com/jianshuod/engorgio-prompt)**|**自回归大型语言模型（LLMs）在许多实际任务中取得了令人印象深刻的性能。然而，这些LLMs的新范式也暴露了新的威胁。在本文中，我们探讨了它们对推理成本攻击的易损性，其中恶意用户构建Engorgio提示，有意增加推理过程的计算成本和延迟。我们设计了Engorgio，一种新颖的方法，以高效地生成对抗性Engorgio提示，影响目标LLM的服务可用性。Engorgio有以下两个技术贡献。（1）我们采用参数化分布来跟踪LLMs的预测轨迹。（2）针对LLMs推理过程的自回归特性，我们提出了新的损失函数，以稳定地抑制<EOS>标记的出现，其出现将中断LLMs的生成过程。我们在13个开源LLMs上进行了广泛的实验，这些LLMs的参数范围从125M到30B。结果表明，Engorgio提示可以在白盒场景下成功诱导LLMs生成异常长的输出（即，达到90%+输出长度限制的时间大约是2-13倍更长），我们的现实世界实验证明了Engorgio对有限计算资源的LLM服务的威胁。代码可在https://github.com/jianshuod/Engorgio-prompt获取。**|\n",
        "2412.18934": "|**2024-12-25**|**Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference**|Libo Zhang et.al.|[2412.18934](http://arxiv.org/abs/2412.18934)|null|由于大型语言模型（LLMs）对资源需求高，在消费级设备上实现广泛部署面临重大挑战。通常，个人或消费级设备，包括在大规模模型时代之前配置的服务器，一般拥有相对较弱的GPU和相对较强的CPU。然而，大多数当前方法主要依赖于GPU进行计算。因此，我们提出了Dovetail方法，该方法将草稿模型部署在GPU上以生成草稿标记，同时允许目标模型在CPU上并行验证，从而提高所有可用硬件资源的利用率并减少设备间通信带宽。相应地，我们对草稿模型进行了重新设计，以更好地适应异构硬件特性。为此，我们实现了几个优化：减少草稿标记的数量以减轻并行验证的延迟，增加草稿模型的深度以提高其预测能力，并引入DGF（动态门控融合）以改善特征和标记嵌入的整合。在HumanEval基准测试中，Dovetail使用3GB VRAM实现了LLaMA2-Chat-7B的每秒5.86个标记的推理速度，与仅使用CPU的推理相比，大约提高了2.77倍。此外，当使用7GB VRAM时，推理速度提高到了每秒8个标记。|\n",
        "2412.20501": "|**2024-12-29**|**TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication**|Zongwu Wang et.al.|[2412.20501](http://arxiv.org/abs/2412.20501)|**[link](https://github.com/aca-lab-sjtu/token-ring)**|**高效并行化具有长序列的大型语言模型（LLMs）至关重要，但由于其巨大的计算和内存需求，特别是由于注意力机制中的通信瓶颈，这极具挑战性。虽然序列并行性（SP）已被引入作为潜在解决方案，但现有方法往往受到可扩展性有限或效率低下的问题，从而限制了其有效性。环状注意力（Ring-Attention）展示了在序列处理中扩展的潜力，但由于其依赖于对等（P2P）通信和无效利用网络资源，因此面临重大限制。随着序列并行度的增加，每一步的计算时间呈二次减少，与通信量的线性减少形成鲜明对比，加剧了通信瓶颈。为了解决这些挑战，我们提出了TokenRing，这是一个细粒度并行框架，利用双向P2P通信有效地重叠计算和数据传输。通过将注意力块分区，并在全连接网状拓扑结构中并发传输查询和块输出（即$block\\_out$和$block\\_lse$），TokenRing实现了显著的通信开销减少和更好的负载平衡。这些创新提高了分布式Transformer模型的可扩展性和效率，尤其是对于长上下文序列。实验结果表明，TokenRing提高了吞吐量并减少了通信延迟。此外，其设计能够无缝适应各种多GPU互连解决方案，如华为Ascend，确保了分布式LLM推理和训练的广泛兼容性和成本效益。代码可在以下网址获取：\\url{https://github.com/ACA-Lab-SJTU/token-ring}。**|\n",
        "2412.20166": "|**2024-12-28**|**LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System**|Hyucksung Kwon et.al.|[2412.20166](http://arxiv.org/abs/2412.20166)|null|大型语言模型（LLMs）的扩展，其参数量达到数百亿，对计算资源提出了重大挑战，尤其是在数据移动和内存带宽方面。长上下文LLMs，处理包含数万个标记的序列，由于注意力层复杂性和键值缓存大小与上下文长度成正比，进一步增加了对内存系统的需求。内存中处理（PIM）通过将计算移动到数据中来最大化内存带宽，可以解决内存带宽问题；然而，由于每个模块的内存容量有限以及固定功能单元PIM架构和静态内存管理的僵化性，PIM并不一定能扩展以加速长上下文LLMs。在本工作中，我们提出了LoL-PIM，这是一种多节点PIM架构，通过硬件-软件协同设计来加速长上下文LLMs。具体来说，我们提出了如何在多PIM模块间利用流水线并行性，同时提出了直接PIM访问（DPA）控制器（或PIM的DMA），它能够实现动态PIM内存管理，并在各种上下文长度上实现高效的PIM利用。我们开发了一个基于MLIR的编译器，用于LoL-PIM，扩展了一个商业PIM编译器，其中软件修改得到了实现和评估，而硬件更改在模拟器中进行了建模。我们的评估表明，LoL-PIM显著提高了长上下文LLMs推理的吞吐量和降低了延迟，优于多GPU和GPU-PIM系统（分别达到8.54倍和16.0倍的速度提升），从而使得LLMs在现实应用中的部署更加高效。|\n",
        "2412.19829": "|**2024-12-19**|**GFormer: Accelerating Large Language Models with Optimized Transformers on Gaudi Processors**|Chengming Zhang et.al.|[2412.19829](http://arxiv.org/abs/2412.19829)|null|异构硬件如Gaudi处理器已被开发出来以增强计算能力，特别是针对基于Transformer的大语言模型（LLM）在生成式AI任务中的矩阵运算。然而，我们的分析表明，Transformer在这些新兴硬件上并未得到充分优化，这主要是因为非矩阵计算内核（如Softmax）的优化不足以及异构资源利用不充分，尤其是在处理长序列时。为了解决这些问题，我们提出了一种综合方法（称为GFormer），该方法将稀疏和线性注意力机制合并。GFormer旨在最大化Gaudi处理器的矩阵乘法引擎（MME）和张量处理核心（TPC）的计算能力，同时不降低模型质量。GFormer包括一个窗口自注意力内核和一个高效的因果线性注意力外积内核，旨在优化LLM在Gaudi处理器上的推理。评估结果显示，GFormer在Gaudi处理器上显著提高了各种任务的效率和模型性能，并优于最先进的GPU。|\n",
        "2501.01144": "|**2025-01-02**|**BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference**|Wonsuk Jang et.al.|[2501.01144](http://arxiv.org/abs/2501.01144)|**[link](https://code.stanford.edu/tambe-lab/blockdialect)**|大型语言模型（LLMs）取得了显著的成就，但它们不断增长的大小在内存使用和计算成本上带来了重大挑战。量化权重和激活可以解决这些问题，而细粒度的块量化正成为缓解异常值的有希望的支持硬件的解决方案。然而，现有方法难以捕捉细微的块数据分布。为了解决这个问题，我们提出了BlockDialect，这是一种块级的细粒度混合格式技术，它从formatbook中为每个块分配最佳的数量格式，以更好地表示数据。此外，我们引入了DialectFP4，这是一个FP4变体的formatbook（类似于方言），它可以适应不同的数据分布。为了有效地利用这一技术，我们提出了一个两阶段方法用于在线DialectFP4激活量化。重要的是，DialectFP4通过选择与低精度整数算术兼容的缩放整数作为可表示的值来确保硬件效率。与MXFP4格式相比，BlockDialect在LLaMA3-8B（LLaMA2-7B）模型上实现了11.83%（7.56%）的准确率提升，并且每个数据使用更低的比特数，即使在量化全路径矩阵乘法时，也只比全精度低5.46%（2.65%）。关注于表示方式而非缩放方式，我们的工作为节能的LLM推理展示了一条有希望的道路。|\n",
        "2501.01005": "|**2025-01-02**|**FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving**|Zihao Ye et.al.|[2501.01005](http://arxiv.org/abs/2501.01005)|**[link](https://github.com/flashinfer-ai/flashinfer)**|**Transformer，由注意力机制驱动，构成了大型语言模型（LLM）的基础。随着这些模型规模的扩大，高效的GPU注意力内核对于高吞吐量和低延迟推理变得至关重要。各种LLM应用需求灵活且高性能的注意力解决方案。我们提出FlashInfer：一个可定制的、高效的LLM服务注意力引擎。FlashInfer通过使用块稀疏格式和可组合格式来处理KV缓存存储的异构性，优化内存访问并减少冗余。它还提供可定制的注意力模板，通过即时（JIT）编译实现对不同设置的适应性。此外，FlashInfer的负载均衡调度算法能够适应用户请求的动态性，同时与需要静态配置的CUDAGraph保持兼容。FlashInfer已被集成到SGLang、vLLM和MLC-Engine等领先的LLM服务框架中。全面的内核级和端到端评估表明，FlashInfer能够在不同的推理场景中显著提升内核性能：与最先进的LLM服务解决方案相比，FlashInfer在LLM服务基准的编译器后端中实现了29-69%的跨标记延迟降低，在长上下文推理中实现了28-30%的延迟降低，在并行生成LLM服务中实现了13-17%的速度提升。**|\n",
        "2501.00032": "|**2024-12-23**|**Highly Optimized Kernels and Fine-Grained Codebooks for LLM Inference on Arm CPUs**|Dibakar Gope et.al.|[2501.00032](http://arxiv.org/abs/2501.00032)|**[link](https://github.com/ggerganov/llama.cpp)**|**大型语言模型（LLMs）彻底改变了我们对语言理解和生成的看法，吸引了研究人员和开发者的极大兴趣。然而，由于LLMs规模巨大、资源需求极高，将其部署进行推理一直是一个重大挑战。虽然将模型权重量化到子字节精度已被证明是缓解内存压力的有希望的方法，但通常用于LLMs量化的分组量化格式存在显著的计算开销和资源密集型的反量化过程。因此，大量计算指令并未执行乘法操作，即实际工作，这使得它们无法满足在通用CPU上部署LLMs所需的延迟要求。在本研究中，我们提出了一套高度优化的内核，以加速LLMs推理并充分发挥CPU（尤其是ARM CPU）的潜力。这些内核将加载操作数和权重解包的成本分摊到多个输出行中。此外，通过引入优化的交错分组数据布局，以及通过优化解压路径来减少不必要的操作和反量化开销，同时最大化使用向量矩阵乘法操作，显著提高了MAC操作效率。此外，我们提出了一种基于分组非均匀码本量化方法，用于对LLMs进行超低精度量化，以更好地匹配其权重分布中的非均匀模式，在生成标记期间实现更高的吞吐量，同时确保比现有技术更好的质量。将这些改进应用于4位LLMs，与基于LLaMA.cpp的解决方案相比，在ARM CPU上的提示处理速度提高了3-3.2倍，自回归解码速度提高了2倍。这些优化内核可在https://github.com/ggerganov/llama.cpp上获得。**|\n",
        "2501.01792": "|**2025-01-03**|**Efficient LLM Inference with Activation Checkpointing and Hybrid Caching**|Sanghyeon Lee et.al.|[2501.01792](http://arxiv.org/abs/2501.01792)|null|近期，具有巨大模型规模的大型语言模型（LLMs）需要使用大量GPU来满足内存容量需求，这导致了生成标记的巨大成本。为了提供具有宽松延迟约束的经济高效的LLM推理，大量研究集中在通过利用主机内存来扩展GPU内存。然而，利用主机内存的LLM推理引擎通常面临GPU计算单元的利用率不足，因为相当一部分推理时间被花费在通过主机-GPU互连将模型加载到GPU上。为了解决LLM中主机内存卸载的这些挑战，我们引入了HybridServe，这是一个基于激活缓存的激活检查点的LLM推理系统。激活缓存存储在中间推理阶段生成的激活检查点，允许在将模型参数从主机内存传输到GPU的同时快速重新计算KV缓存。与使用标记ID从头开始重新计算KV缓存的传统方法不同，激活缓存允许绕过投影和FFN操作。为了平衡激活重新计算和参数加载开销，本研究提出了一种KV-激活混合缓存方案，该方案找到最佳的关键值和激活缓存比例以调整重新计算时间。我们的系统在卸载模型权重和KV缓存方面比最先进的前期工作实现了2.19倍的吞吐量提升。|\n",
        "2501.02600": "|**2025-01-05**|**TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms**|Jovan Stojkovic et.al.|[2501.02600](http://arxiv.org/abs/2501.02600)|null|随着对生成式大型语言模型（LLMs）的需求不断上升，这在云计算数据中心中为热管理和电力管理带来了挑战。传统的技术往往不适用于LLM推理，因为它们具有精细的、毫秒级的执行阶段，每个阶段都有独特的性能、热和功耗特性。此外，LLM推理工作负载对各种配置参数敏感，例如模型并行性、大小和量化，这些参数需要在性能、温度、功耗和输出质量之间进行权衡。而且，云通常将SaaS和IaaS工作负载部署在同一地点，它们具有不同级别的可见性和灵活性。我们提出了TAPAS，这是一个为云中LLM推理集群设计的温度和功率感知框架。TAPAS增强了冷却和电力超用能力，在有效处理紧急情况（例如，冷却和电力故障）的同时，降低了总拥有成本（TCO）。该系统利用历史温度和电力数据以及SaaS工作负载的适应性，以：（1）在冷却和电力约束下高效地放置新的GPU工作负载虚拟机，（2）跨SaaS虚拟机路由LLM推理请求，（3）重新配置SaaS虚拟机以管理负载峰值和紧急情况。我们在一个大型GPU集群上的评估表明，显著减少了热和功耗限制事件，提升了系统效率。|\n",
        "2501.02336": "|**2025-01-04**|**AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference**|Zhuomin He et.al.|[2501.02336](http://arxiv.org/abs/2501.02336)|**[link](https://github.com/asisys/adaskip)**|**长上下文大型语言模型（LLM）的推理变得越来越重要，这促使许多研究致力于减轻此类场景中的大量存储和计算成本。层跳过方法是一种有希望的优化，但在长上下文推理中很少被探索。我们观察到，现有的层跳过策略在应用于长上下文推理时存在一些局限性，包括无法适应模型和上下文的变化、忽视子层的意义以及不适用于预填充阶段。本文提出了一种名为\\sysname的自适应子层跳过方法，专门针对长上下文推理。\\sysname通过利用动态相似性信息自适应地识别出不太重要的层，实现了子层跳过，并加速了预填充和解码阶段。通过在多种长上下文基准和模型上进行的广泛实验，验证了\\sysname的有效性，并展示了其在现有基线之上的优越推理性能。**|\n",
        "2501.06807": "|**2025-01-12**|**MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large Language Model Inference**|Wenxuan Zeng et.al.|[2501.06807](http://arxiv.org/abs/2501.06807)|null|基于安全多方计算（MPC）的私有大型语言模型（LLM）推理为用户提示和专有模型权重提供了密码学安全的保护。然而，对于长输入序列，它面临着较大的延迟开销。虽然已经提出了键值（KV）缓存淘汰算法来减少明文推理的计算和内存成本，但它们并非为MPC设计，因此无法轻易应用于私有推理。在本文中，我们提出了一种准确且适合MPC的KV缓存淘汰框架，称为MPCache。MPCache基于观察，即长序列中的历史标记可能对下游解码有不同的影响。因此，MPCache结合了一种单次查看的静态淘汰算法来丢弃不重要的标记，以及一种查询感知的动态选择算法来进一步选择一小部分标记进行注意力计算。由于现有的动态选择算法会产生过多的延迟，我们提出了一系列优化措施，以大幅减少KV缓存选择开销，包括MPC友好的相似度近似、分层KV缓存聚类和跨层索引共享策略。通过大量实验，我们证明了MPCache在不同LLM生成任务中始终优于现有的KV缓存淘汰基线，并且在不同的序列长度上分别实现了1.8~2.01倍的解码延迟和3.39~8.37倍的通信减少。|\n",
        "2501.08219": "|**2025-01-14**|**Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings**|Paul Joe Maliakel et.al.|[2501.08219](http://arxiv.org/abs/2501.08219)|null|大型语言模型（LLMs）在许多自然语言处理（NLP）任务中显示出显著的改进，加速了它们在多个行业的快速应用。这些模型资源密集，无论是在训练还是推理过程中都需要大量的计算资源，导致能源消耗增加和负面的环境影响。随着其应用的加速，LLMs的可持续性已成为一个关键问题，需要优化其运行效率而不影响性能的策略。因此，确定显著影响LLMs性能和能源效率的参数至关重要。为此，本研究调查了这些参数在推理过程中对LLMs性能和能源效率的影响，并分析了它们的权衡。首先，我们通过基准测试LLMs，如Falcon-7B、Mistral-7B-v0.1、T5-3B、GPT-2、GPT-J-6B和GPT-Neo-2.7B，分析了不同类型的模型在参数数量和架构各异的情况下在文本生成、问答和摘要等任务上的表现。其次，我们研究了输入和输出序列特征，如序列长度，与能源消耗、性能和吞吐量之间的关系。最后，我们探讨了基于硬件的节能技术，即动态电压频率调整（DVFS），对模型延迟和能源效率的影响。我们的广泛基准测试和统计分析揭示了许多有趣的结果，揭示了特定优化如何减少能源消耗，同时保持吞吐量和准确性。本研究为研究人员和从业者提供了有关设计节能LLMs推理系统的可行见解。|\n",
        "2501.08192": "|**2025-01-14**|**PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving**|Ahmet Caner Yüzügüler et.al.|[2501.08192](http://arxiv.org/abs/2501.08192)|null|大型语言模型（LLMs）在各种应用中被广泛使用，但它们巨大的计算需求带来了重大挑战，尤其是在HBM带宽瓶颈和设备间通信开销方面。在本文中，我们提出了一种名为PRESERVE的新型预取框架，旨在通过重叠模型权重和KV缓存的内存读取与集体通信操作来优化LLM推理。通过在商业AI加速器上进行的广泛实验，我们展示了在最先进的开源LLMs上实现了高达1.6倍的端到端速度提升。此外，我们还进行了设计空间探索，确定了该方法的最佳硬件配置，通过选择最佳L2缓存大小，进一步实现了每成本1.25倍的性能提升。我们的结果表明，PRESERVE有潜力缓解内存瓶颈和通信开销，为提高LLM推理系统的性能和可扩展性提供了一种解决方案。|\n",
        "2501.08090": "|**2025-01-14**|**Hierarchical Autoscaling for Large Language Model Serving with Chiron**|Archit Patke et.al.|[2501.08090](http://arxiv.org/abs/2501.08090)|null|大型语言模型（LLM）服务正成为云服务提供商越来越重要的工作负载。根据性能服务级别目标（SLO）要求，LLM推理请求可以分为两类：（a）具有紧密SLO（以秒为单位）的交互式请求，和（b）具有宽松SLO（以分钟到小时为单位）的批量请求。这些SLO会根据到达率、复用和配置参数而降低，因此需要在服务实例及其批量大小上使用资源自动扩展。然而，之前用于LLM服务的自动扩展器没有考虑请求SLO，导致不必要的扩展和资源利用率低下。为了解决这些局限性，我们引入了Chiron，这是一个使用基于队列大小、利用率和SLO的分层背压概念的自动扩展器。我们的实验表明，与现有解决方案相比，Chiron实现了高达90%的SLO达成率，并将GPU效率提高了高达70%。|\n",
        "2501.09258": "|**2025-01-16**|**Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition**|Takaaki Hori et.al.|[2501.09258](http://arxiv.org/abs/2501.09258)|null|本文提出了一种针对大型语言模型（LLMs）的端到端自动语音识别（E2E-ASR）的高效解码方法。尽管浅层融合是将语言模型融入E2E-ASR解码中最常见的做法，但我们在使用LLMs时遇到了两个实际问题。（1）LLM推理计算成本高。（2）ASR模型和LLM之间可能存在词汇不匹配。为了解决这种不匹配，我们需要重新训练ASR模型和/或LLM，这在最佳情况下是耗时的，在许多情况下甚至不可行。我们提出了“延迟融合”，该方法在解码过程中延迟将LLM评分应用于ASR假设，从而使得在ASR任务中更容易使用预训练的LLMs。这种方法不仅可以减少LLM评分的假设数量，还可以减少LLM推理调用的数量。如果ASR和LLM采用不同的标记化，该方法还允许在解码过程中重新标记化ASR假设。我们通过使用LibriHeavy ASR语料库和三个公开的LLMs，即OpenLLaMA 3B & 7B和Mistral 7B，证明了延迟融合与浅层融合和N-best重评分相比，在解码速度和准确性方面均有提升。|\n",
        "2501.09186": "|**2025-01-15**|**Guiding Retrieval using LLM-based Listwise Rankers**|Mandeep Rathee et.al.|[2501.09186](http://arxiv.org/abs/2501.09186)|**[link](https://github.com/mandeep-rathee/llmgar)**|**大型语言模型（LLMs）在“列表式”设置中展现出作为重排序器的强大潜力，在这种设置中，LLM被提示一次性重排序多个搜索结果。然而，这种“级联”检索和重排序方法受到召回限制问题的限制：最初未检索到的相关文档将永久性地排除在最终排名之外。自适应检索技术解决了这个问题，但不能与列表式重排序器一起工作，因为它们假设文档的分数是独立于其他文档计算的。在本文中，我们提出了一种现有自适应检索方法的改编，该方法支持列表式设置并有助于引导检索过程本身（从而克服了LLM重排序器的召回限制问题）。具体来说，我们提出的算法将初始排名和迄今为止看到的最相关文档提供的反馈文档的结果合并。通过在多样化的LLM重排序器、第一阶段检索器和反馈源上进行的广泛实验，我们证明了我们的方法可以将nDCG@10提高高达13.23%，并将召回率提高28.02%——所有这些都是在保持LLM推理总数恒定和自适应过程带来的开销最小的情况下实现的。这项工作为在初始结果池有限的环境中利用基于LLM的搜索打开了大门，例如，由旧系统或部署语义第一阶段的开销限制。**|\n",
        "2501.10069": "|**2025-01-17**|**A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks**|Xinzhe Li et.al.|[2501.10069](http://arxiv.org/abs/2501.10069)|null|通过搜索进行的LLM测试时计算（或LLM推理）已成为一个充满活力的研究领域，发展迅速。然而，现有的框架在三个关键方面（任务定义、LLM分析和搜索过程）上往往采用不同的观点，这使得直接比较变得困难。此外，所采用的搜索算法通常与标准实现不同，且它们的特定特性并未得到充分说明。在这篇综述中，我们提供了一个全面的技术回顾，统一了任务定义，并提供了LLM分析和搜索过程的模块化定义。这些定义使得对各种LLM推理框架的精确比较成为可能，同时突出了它们与常规搜索算法的差异。我们还讨论了这些方法的应用性、性能和效率。有关更详细的内容和持续更新，请参阅我们的GitHub仓库：https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md|\n",
        "2501.11779": "|**2025-01-20**|**Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference**|Pouya Hamadanian et.al.|[2501.11779](http://arxiv.org/abs/2501.11779)|**[link](https://github.com/microsoft/glinthawk)**|**大型语言模型（LLM）彻底改变了自然语言处理领域，但它们的推理需求庞大的资源，同时高端加速器如GPU的使用率却很低。一个主要的瓶颈来自于注意力机制，它需要存储大量的键值缓存，这使得最大可达到的吞吐量远低于可用的计算资源。目前的方法试图通过内存高效的注意力和分页机制来缓解这个问题，但仍然受到所有操作必须在高端加速器上执行的假设的限制。在本文中，我们提出了Glinthawk，这是一种双层架构，将注意力机制从Transformer模型的其余部分解耦。这种方法允许注意力机制的内存需求独立扩展，从而实现更大的批量大小和更高效的高端加速器使用。我们使用NVIDIA T4 GPU作为一层，标准CPU虚拟机作为另一层来原型化Glinthawk。与传统单层设置相比，它将吞吐量提高了5.9倍，并将生成成本降低了2.8倍。对于更长的序列长度，它在成本降低2.4倍的情况下实现了16.3倍的吞吐量提升。我们的评估表明，这种架构可以容忍适度的网络延迟，并且性能退化最小，使其在批处理等延迟容忍、吞吐量导向的应用中非常有效。我们在https://github.com/microsoft/glinthawk上公开了我们的原型。**|\n",
        "2501.11549": "|**2025-01-20**|**Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas**|Nishant Balepur et.al.|[2501.11549](http://arxiv.org/abs/2501.11549)|**[link](https://github.com/pinafore/alignment-personalization)**|**大型语言模型通过学习用户对两个输出中哪一个更偏好来调整以遵循指令（对齐）。然而，这种偏好数据格式并没有传达用户为什么偏好选择或拒绝的响应，因此在这些数据集上训练的LLM无法根据不同的用户需求定制响应。为了揭示这些个性化的参数，我们应用溯因推理对偏好数据进行处理，推断用户的需求和兴趣，即可能偏好每个输出的用户角色。我们通过以下两个步骤测试这一想法：角色推断（PI）——通过溯因推理出偏好选择或拒绝输出的用户角色，以及角色定制（PT）——训练模型根据PI中的角色定制响应。我们发现：1）LLM能够准确地推断出角色，解释为什么不同的用户可能偏好选择或拒绝的输出；2）通过PT在PI角色增强的偏好数据上训练，可以提升个性化，使模型能够支持用户编写的角色；3）被拒绝的响应角色形成更困难的个性化评估，表明PT在帮助具有不常见偏好的用户方面比典型的对齐方法更有效。我们主张对个性化偏好采用溯因观点，不仅询问哪个响应更好，还要询问何时、为什么以及为谁。**|\n",
        "2501.11006": "|**2025-01-19**|**GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation**|Shashikant Ilager et.al.|[2501.11006](http://arxiv.org/abs/2501.11006)|null|大型语言模型（LLMs）正成为日常生活不可或缺的一部分，展现出其在各种自然语言处理（NLP）任务中的巨大潜力。除了NLP，LLMs在软件开发任务中也越来越多地被使用，例如代码补全、修改、错误修复和代码翻译。软件工程师广泛使用GitHub Copilot和Amazon Q等工具，通过高精度自动化任务来简化工作流程。尽管LLMs训练的资源消耗和能源强度经常被强调，但随着时间的推移，推理过程可能更加资源密集，因为它是一个具有大量调用的持续过程。因此，开发针对LLMs推理的资源高效替代方案对于可持续性至关重要。这项工作提出了GREEN-CODE，这是一个针对LLMs能效代码生成的框架。GREEN-CODE在LLMs推理过程中执行动态早期退出。我们训练了一个强化学习（RL）智能体，使其学会在准确性、延迟和能源消耗之间平衡权衡。我们的方法在两个开源LLMs（Llama 3.2 3B和OPT 2.7B）上进行了评估，使用了JavaCorpus和PY150数据集。结果显示，我们的方法在代码生成任务中平均降低了23-50%的能源消耗，而不会显著影响准确性。|\n",
        "2501.14312": "|**2025-01-24**|**Locality-aware Fair Scheduling in LLM Serving**|Shiyi Cao et.al.|[2501.14312](http://arxiv.org/abs/2501.14312)|null|大型语言模型（LLM）的推理工作负载占据了从多轮对话到文档分析等多种现代AI应用的广泛领域。在管理具有不同前缀模式的多样化客户端工作负载时，平衡公平性和效率至关重要。遗憾的是，现有的针对LLM服务的公平调度算法，如虚拟令牌计数器（VTC），未能考虑前缀局部性，因此性能较差。另一方面，现有LLM服务框架中的局部性感知调度算法往往在最大化前缀缓存命中率的同时，未考虑客户端之间的公平共享。本文介绍了第一个局部性感知的公平调度算法，即“缺陷最长前缀匹配”（Deficit Longest Prefix Match，DLPM），它可以在保证公平性的同时保持高程度的前缀局部性。我们还介绍了一种新颖的算法，即“双缺陷LPM”（Double Deficit LPM，D$^2$LPM），它扩展了DLPM以适应分布式环境，可以在公平性、局部性和负载均衡之间找到一个平衡点。我们的广泛评估表明，DLPM和D$^2$LPM在确保公平性的同时，保持了高吞吐量（比VTC高2.87倍）和低客户端延迟（比最先进的分布式LLM服务系统低7.18倍）。|\n",
        "2501.16191": "|**2025-01-27**|**Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs**|Antony Bartlett et.al.|[2501.16191](http://arxiv.org/abs/2501.16191)|null|解决Python依赖问题对于开发者来说是一项繁琐且容易出错的任务，他们必须手动识别和解决第三方模块和Python解释器的环境依赖和版本约束。研究人员试图通过依赖大型知识图谱和数据库查找表来自动化这一过程。然而，这些传统方法由于依赖错误类型的多样性、可能模块版本的集合庞大以及传递依赖之间的冲突而面临局限性。本研究探讨了使用大型语言模型（LLMs）自动修复Python程序中依赖问题的潜力。我们引入了PLL（发音为“plum”），这是一种新颖的技术，它采用检索增强生成（RAG）来帮助LLM推断给定Python文件的Python版本和所需模块。PLL构建了一个测试环境，通过迭代地（1）提示LLM进行模块组合，（2）测试建议的更改，以及（3）向LLM提供反馈（错误消息）以改进修复。这个反馈循环利用自然语言处理（NLP）智能解析和解释构建错误消息。我们在Gistable HG2.9K数据集上对PLL进行了基准测试，这是一个包含具有挑战性的单文件Python片段的集合。我们将PLL与两种最先进的自动依赖推理方法进行了比较，即PyEGo和ReadPyE，以解决依赖问题的能力进行比较。我们的结果表明，PLL可以修复比两个基线更多的依赖问题，比ReadPyE多218个（+15.97%），比PyEGo多281个（+21.58%）。我们的深入分析表明，PLL特别有利于具有许多依赖项的项目以及特定的第三方数值和机器学习模块。我们的发现证明了基于LLM的方法迭代解决Python依赖问题的潜力。|\n",
        "2501.16007": "|**2025-01-27**|**TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference**|Jack Min Ong et.al.|[2501.16007](http://arxiv.org/abs/2501.16007)|null|大型语言模型（LLMs）已被证明具有很高的能力，但访问最佳模型目前依赖于推理提供商，这引入了信任挑战——我们如何确保提供商正在使用他们声称的模型配置？我们提出了一种名为TOPLOC的新方法，用于可验证推理，以解决此问题。TOPLOC利用一种紧凑的局部敏感哈希机制对中间激活进行操作，可以以100%的准确率检测模型、提示或精度的未授权修改，在我们的实证评估中实现了零误报和漏报。我们的方法在各种硬件配置、GPU类型和代数重排方面都具有鲁棒性，使得验证速度比原始推理快得多。通过引入多项式编码方案，TOPLOC将生成的提交的内存开销减少了1000倍，每32个新标记只需258字节的存储空间，相比之下，直接存储Llama-3.1-8B-Instruct的标记嵌入需要262KB。我们的方法使用户能够高效地验证LLM推理计算，促进了开放生态系统中的信任和透明度，并为去中心化和可验证的AI服务奠定了基础。|\n",
        "2501.15829": "|**2025-01-27**|**Aging-aware CPU Core Management for Embodied Carbon Amortization in Cloud LLM Inference**|Tharindu B. Hewage et.al.|[2501.15829](http://arxiv.org/abs/2501.15829)|**[link](https://github.com/tharindu-b-hewage/splitwise-sim-cpu-carbon)**|**大规模语言模型（LLM）的广泛应用要求云LLM推理集群迅速扩展，导致实体碳（制造和供应IT资产的排放）的积累，这些排放主要集中于推理服务器CPU。本文深入探讨了云LLM推理可持续增长所面临的挑战，强调在更长的使用寿命中分摊CPU的实体成本。鉴于硅老化带来的可靠性风险，我们提出了一种老化感知的CPU核心管理技术，以延迟CPU老化效应，使集群操作员能够安全地延长CPU的使用寿命。我们的技术利用了我们在云LLM推理中发现的CPU低利用率模式，通过在未使用的核心中停止老化并在活动核心中通过选择深度休眠和老化感知的推理任务分配来均匀老化。通过使用真实的Azure推理跟踪和微软的一个扩展LLM集群模拟器进行的广泛模拟，我们证明了我们的技术相对于现有方法的优越性能，通过管理CPU老化效应的p99性能，预计每年可减少37.67%的实体碳排放，CPU低利用率降低77%，对推理服务质量的负面影响不到10%。**|\n",
        "2501.15113": "|**2025-01-25**|**Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads**|Xingyang He et.al.|[2501.15113](http://arxiv.org/abs/2501.15113)|null|KV 缓存是大语言模型（LLM）推理中广泛使用的一种加速技术。然而，随着输入长度的增加，其内存需求迅速增长。先前的研究通过为所有注意力头移除相同数量的非重要标记，或者为预识别的注意力头分配差异化的 KV 缓存预算来减小 KV 缓存的大小。然而，由于不同任务中注意力头的重要性存在差异，预识别的注意力头无法有效地适应各种下游任务。为了解决这一问题，我们提出了一种名为 Task-KV 的方法，该方法利用注意力头的语义差异化，为各种任务分配差异化的 KV 缓存预算。我们证明，远离语义中心的注意力头（称为异构头）对任务输出和语义理解做出了重大贡献。相比之下，其他注意力头则扮演着聚合重要信息和聚焦推理的角色。Task-KV 为异构头分配完整的 KV 缓存预算，以保留全面的语义信息，同时为非异构头保留少量最近标记和注意力汇聚点。此外，我们创新性地引入了中间激活来保留从非异构头聚合的关键上下文信息。为了动态感知注意力头之间的语义差异，我们设计了一个语义分隔器，根据它们与语义中心的距离来区分异构头和非异构头。在多个基准和不同模型架构上的实验结果表明，Task-KV 显著优于现有的基线方法。|\n",
        "2501.16383": "|**2025-01-25**|**RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations**|Zunhai Su et.al.|[2501.16383](http://arxiv.org/abs/2501.16383)|**[link](https://github.com/ZunhaiSu/RotateKV)**|**键值（KV）缓存通过避免重复计算过去的键值，促进了高效的大型语言模型（LLM）推理。随着批处理大小和上下文长度的增加，过大的KV缓存成为一个显著的内存瓶颈，突显了高效压缩的必要性。现有的KV量化依赖于细粒度量化或保留大量高比特宽缓存的一部分，这两种方法都牺牲了压缩率，并且在极低的平均比特宽度下往往无法保持鲁棒性。在本研究中，我们探索了旋转技术在2比特KV量化中的潜力，并提出了RotateKV，它通过以下创新实现了精确和鲁棒的性能：（i）异常值感知旋转，利用通道重排序来适应不同通道的异常值分布，而不牺牲快速华莱士-哈达马变换（FWHT）的计算效率；（ii）预RoPE分组头旋转，减轻了旋转位置嵌入（RoPE）对提出的异常值感知旋转的影响，并进一步平滑了跨头的异常值；（iii）注意力下沉感知量化，利用大量的激活来精确地识别和保护注意力下沉。RotateKV在WikiText-2上使用LLaMA-2-13B进行2比特量化时，实现了低于0.3的困惑度（PPL）下降，保持了强大的CoT推理和长上下文能力，在GSM8K上的下降低于1.7%，即使在较低的平均比特宽度下也优于现有方法。RotateKV还展示了峰值内存使用量减少3.97倍，支持5.75倍更大的批处理大小，并在解码阶段实现了2.32倍的速度提升。**|\n",
        "2501.19278": "|**2025-01-31**|**Pheromone-based Learning of Optimal Reasoning Paths**|Anirudh Chari et.al.|[2501.19278](http://arxiv.org/abs/2501.19278)|null|大型语言模型（LLMs）通过思维链提示展示了卓越的推理能力，但发现有效推理方法对于复杂问题仍然具有挑战性，因为可能的中间步骤空间巨大。我们引入了蚁群优化引导的思维树（ACO-ToT），这是一种将蚁群优化与LLMs相结合的新算法，以高效地发现复杂问题的最佳推理路径。从神经系统中赫布学习（Hebbian learning）的灵感中汲取，我们的方法使用一组经过特别微调的LLM“蚂蚁”在中央思维树中穿梭并留下信息素路径，每个蚂蚁的移动由现有信息素路径和它自己的专业知识的加权组合所控制。该算法使用基于专家混合的评分函数评估完整的推理路径，信息素在迭代中强化有效的推理路径。在三个具有挑战性的推理任务（GSM8K、ARC-Challenge和MATH）上的实验表明，ACO-ToT的性能显著优于现有的思维链优化方法，这表明将生物启发的集体搜索机制融入LLM推理可以显著提高推理能力。|\n",
        "2502.02493": "|**2025-02-04**|**EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization**|Yize Wu et.al.|[2502.02493](http://arxiv.org/abs/2502.02493)|null|推测性解码是一种有效且无损的用于大语言模型（LLM）推理加速的方法。它使用一个较小的模型生成草稿标记序列，然后由原始基础模型进行验证。在多GPU系统中，通过张量并行（TP）可以进一步降低推理延迟，而草稿模型的最佳TP大小通常小于基础模型，导致在草稿阶段GPU闲置。为了解决这个问题，我们提出了EasySpec，这是一种层并行推测策略，优化了多GPU利用效率。EasySpec打破了草稿模型中层的顺序执行顺序，使得可以在设备间实现多层并行化，尽管会引入一些近似误差。在每个草稿-验证迭代之后，草稿模型的关键值（KV）缓存通过单个前向传递进行校准，以最小的额外延迟防止长期错误累积。我们使用几个主流开源LLM评估了EasySpec，使用同一系列模型的小版本作为草稿模型。结果表明，与传统的解码相比，EasySpec可以实现高达4.17倍的峰值加速，同时保持基础LLM的原有分布。具体来说，草稿阶段可以加速高达1.62倍，最大精度下降仅为7%，且无需在草稿模型上进行训练或微调。|\n",
        "2502.01070": "|**2025-02-03**|**An Investigation of FP8 Across Accelerators for LLM Inference**|Jiwoo Kim et.al.|[2502.01070](http://arxiv.org/abs/2502.01070)|null|在现代AI加速器中引入8位浮点（FP8）计算单元，引发了基于FP8的大型语言模型（LLM）推理的极大兴趣。与16位浮点格式不同，深度学习中的FP8需要共享一个缩放因子。此外，虽然E4M3和E5M2在单个值级别上是明确定义的，但它们的缩放和累积方法尚未指定，并且在不同硬件和软件实现中有所不同。因此，FP8更像是一种量化格式，而不是标准的数值表示。在这项工作中，我们首次对两个AI加速器（NVIDIA H100和Intel Gaudi 2）上的FP8计算和加速进行了全面分析。我们的发现强调，通过利用FP8，Gaudi 2在LLM推理期间实现了更高的吞吐量与功耗效率，为数据中心规模LLM服务的FP8采用的实际影响提供了宝贵见解。|\n",
        "2502.00922": "|**2025-02-02**|**Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference**|Patrick Yubeaton et.al.|[2502.00922](http://arxiv.org/abs/2502.00922)|null|随着大型语言模型（LLMs）能力的提升，其规模也在迅速增加。这加剧了在小型边缘设备上运行最先进LLMs的难度。标准技术主张通过量化或剪枝等有损压缩技术来解决此问题。然而，此类压缩技术是有损的，并且已被证明会以不可预测的方式改变模型行为。我们提出了Huff-LLM，这是一种端到端、无损的模型压缩方法，使用户能够将LLM权重以压缩格式存储在任何地方——云、磁盘、主内存，甚至在片上内存/缓冲区中。这使我们不仅能够在主内存中加载更大的模型，还能减少加载片上权重的带宽需求，并更有效地利用片上权重缓冲区。除了通过压缩实现的内存节省外，我们还展示了在执行压缩模型的推理时，在延迟和能效方面的改进。|\n",
        "2502.00847": "|**2025-02-02**|**SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models**|Jiawen Zhang et.al.|[2502.00847](http://arxiv.org/abs/2502.00847)|null|随着LLM在普通用户中的普及，隐私保护和对抗鲁棒性成为LLM服务中两个迫切的需求，这两者一直被单独追求，但很少联合考虑。在本文中，据我们所知，我们是第一个将两个不相连接的领域——隐私推理和提示集成——紧密结合以实现鲁棒和隐私LLM推理的尝试。前者通过加密LLM传输和处理的推理数据来保护用户隐私，而后者通过从多个提示LLM响应中得出聚合输出来增强对抗鲁棒性。尽管单个方法被广泛认为有效，但隐私推理与提示集成结合在一起会带来新的挑战，使得现有技术的简单组合效率低下。为了克服这些障碍，我们提出了SecPE，它为提示集成的核心算法构建块设计了高效的完全同态加密（FHE）对应方案。我们通过对8个任务进行大量实验来评估SecPE的准确性、鲁棒性和效率。结果表明，与基线隐私推理方法相比，SecPE仅以2.5%的效率开销保持了高清洁准确性，并提供了更好的鲁棒性，表明“准确性-鲁棒性-效率”之间的权衡是令人满意的。对于提示集成中引起主要减速的加密Argmax操作，SecPE比最先进的方法快35.4倍，这可以超出这项工作本身，具有独立的研究价值。|\n",
        "2502.00439": "|**2025-02-01**|**UniAttn: Reducing Inference Costs via Softmax Unification for Post-Training LLMs**|Yizhe Xiong et.al.|[2502.00439](http://arxiv.org/abs/2502.00439)|null|后训练对于适应真实世界应用的大型语言模型（LLMs）至关重要。部署后训练模型面临着巨大的内存开销和明显的推理延迟。现有工作已经识别出LLMs中存在显著的冗余，并提出了高效的架构，即层内KV共享和层间KV共享。然而，层内KV共享仍然导致高推理成本，而层间KV共享则导致显著的性能下降。因此，这两种方法对于后训练预训练的LLMs来说仍然不是最优的。在本文中，我们确定Softmax操作是LLM推理的主要瓶颈，并发现它在后训练期间实际上是高度冗余的。我们提出了Softmax在注意力（UniAttn）统一，一种新颖的后训练方法，该方法通过统一Transformer块中的Softmax激活来降低LLM的推理成本。此外，UniAttn采用线性投影来补偿由Softmax统一引起的误差。实验表明，UniAttn在性能上与标准后训练相当，同时显著降低了推理成本，在后训练期间优于现有的高效架构。我们的代码将在https://github.com/Bostoncake/UniAttn上提供。|\n",
        "2502.00299": "|**2025-02-01**|**ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference**|Xiang Liu et.al.|[2502.00299](http://arxiv.org/abs/2502.00299)|null|为了降低在长上下文推理中使用大型语言模型（LLMs）时的内存成本，许多最近的研究都集中在压缩不同标记的关键值（KV）缓存上。然而，我们发现之前的KV缓存压缩方法单独衡量标记的重要性，忽略了现实世界中语言特征的标记间依赖性。鉴于这一点，我们引入了ChunkKV，将块中的标记作为一个基本的压缩单元，同时保留最有信息量的语义块，丢弃不那么重要的块。此外，观察到ChunkKV在不同层之间具有更高的保留索引相似性，我们提出了分层索引重用来进一步减少计算开销。我们在包括LongBench和Needle-In-A-HayStack在内的最前沿长上下文基准测试中，以及GSM8K和JailbreakV的上下文学习基准测试中评估了ChunkKV。我们的实验与指令调整和多步推理（O1和R1）LLMs，与现有方法相比，在积极的压缩比率下实现了高达10%的性能提升。|\n",
        "2502.01651": "|**2025-01-30**|**Fine-tuning LLaMA 2 interference: a comparative study of language implementations for optimal efficiency**|Sazzad Hossain et.al.|[2502.01651](http://arxiv.org/abs/2502.01651)|null|本文提出了一项旨在优化Llama2推理的对比研究，这是机器学习和自然语言处理（NLP）中的关键方面。我们评估了包括TensorFlow、PyTorch、Python、Mojo、C++和Java在内的各种编程语言和框架，通过广泛的基准测试分析它们在速度、内存消耗和易用性方面的性能。突出了每种方法的优点和局限性，以及针对并行处理和硬件利用的优化策略。此外，我们研究了Mojo SDK，这是一个专为在苹果硅上执行大型语言模型（LLM）推理而设计的创新框架，将其性能与C、C++、Rust、Zig、Go和Julia的实现进行了基准测试。在我们的实验中，使用苹果M1 Max进行测试，证明了Mojo SDK的竞争性性能、易用性和与Python的无缝兼容性，使其成为在苹果硅上执行LLM推理的强大替代方案。我们还讨论了在资源受限硬件上部署LLM的更广泛影响，并确定了未来研究的潜在方向。|\n",
        "2502.02818": "|**2025-02-05**|**Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL**|Wenbo Sun et.al.|[2502.02818](http://arxiv.org/abs/2502.02818)|null|为大型语言模型（LLM）提供服务通常需要专用硬件、专门的框架和大量的开发工作，这限制了其可访问性，尤其是在边缘设备和技术资源有限的组织中。我们提出了一种新颖的编译器，该编译器将LLM推理图转换为SQL查询，使得全球最广泛使用和成熟的软件系统之一——关系数据库，能够作为运行时环境。通过将诸如矩阵乘法和注意力等神经网络算子映射到关系运算原语（如连接和聚合），我们的方法利用了数据库的能力，包括基于磁盘的数据管理和原生缓存。支持关键的转换器组件，如注意力机制和键值缓存，我们的系统为端到端LLM推理生成SQL管道。以Llama3系列作为案例研究，我们证明了在内存受限的场景下，与竞争的基于CPU的框架相比，在标记生成速度上最多可以提升30倍。我们的工作提供了一个易于访问、便携和高效的解决方案，促进了LLM在各种部署环境中的服务。|\n",
        "2502.02789": "|**2025-02-05**|**Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation**|Jingyu Liu et.al.|[2502.02789](http://arxiv.org/abs/2502.02789)|null|提升首次输出token时间（TTFT）是现代大型语言模型（LLM）推理引擎中一个至关重要的目标。因为优化TTFT直接导致最大QPS值提高，并满足许多关键应用的需求。然而，提升TTFT极具挑战性，因为它纯粹受计算限制，性能瓶颈从自注意力部分转移到MLP部分。我们提出了SpecPrefill，一个无需训练的框架，该框架基于以下洞察来加速长和中等上下文查询的推理TTFT：LLM足够通用，仅凭精心选择的提示token子集仍能保持质量。在其核心，SpecPrefill利用轻量级模型根据上下文推测局部重要token。这些token以及必要的位置信息随后被发送到主模型进行处理。我们使用一系列不同的任务对SpecPrefill进行了评估，随后在真实端到端设置和消融研究中全面评估了性能改进。SpecPrefill能够在真实下游任务中为Llama-3.1-405B-Instruct-FP8提供高达$7\\times$的最大端到端QPS，并在基准测试期间实现了$7.66\\times$的TTFT提升。|\n",
        "2502.04077": "|**2025-02-06**|**AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference**|Qingyue Yang et.al.|[2502.04077](http://arxiv.org/abs/2502.04077)|null|随着大型语言模型（LLMs）的发展，通过键值（KV）缓存压缩进行高效的推理引起了广泛关注，尤其是在长文本生成方面。为了压缩KV缓存，最近的方法通过注意力分数的启发式排名来确定关键KV标记。然而，这些方法往往难以准确确定关键标记，因为它们忽略了注意力分数中的时间模式，导致LLM性能显著下降。为了应对这一挑战，我们提出了AttentionPredictor，这是第一个基于学习的临界标记识别方法。具体来说，AttentionPredictor学习了一个轻量级的卷积模型来捕捉时空模式并预测下一个标记的注意力分数。AttentionPredictor的一个吸引人的特点是，它可以在消耗极小内存的情况下准确预测注意力分数。此外，我们提出了一种跨标记关键缓存预取框架，该框架隐藏了标记估计时间的开销，以加速解码阶段。通过保留大部分的注意力信息，AttentionPredictor实现了16倍的KV缓存压缩，且性能与LLMs相当，显著优于现有技术。|\n",
        "2502.03805": "|**2025-02-06**|**Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective**|Yuan Feng et.al.|[2502.03805](http://arxiv.org/abs/2502.03805)|null|大型语言模型彻底改变了自然语言处理，但由于Transformer架构依赖于自注意力，特别是长序列推理中大型键值（KV）缓存，因此面临着高存储和运行成本的重大挑战。最近的研究通过基于注意力权重修剪不那么关键的条目来减少KV缓存大小，但这些努力仍然停留在经验层面，缺乏正式的依据。本文通过对注意力输出扰动进行分析，提出了一种正式研究，以识别关键的KV缓存条目。我们的分析表明，除了注意力权重之外，KV条目中的值状态和预训练参数矩阵也是至关重要的。基于此，我们提出了一种扰动约束选择算法，该算法通过优化最坏情况下的输出扰动来识别关键条目。在Needle-in-a-Haystack测试和Longbench基准测试中的评估表明，我们的算法增强了最先进的缓存淘汰方法。进一步的实证分析证实，我们的算法在Llama模型中超过92%的注意力头中实现了更低的输出扰动，从而在现有方法的基础上提供了显著的改进。|\n",
        "2502.03771": "|**2025-02-06**|**Adaptive Semantic Prompt Caching with VectorQ**|Luis Gaspar Schroeder et.al.|[2502.03771](http://arxiv.org/abs/2502.03771)|null|语义提示缓存通过重用缓存中大型语言模型（LLM）生成的响应来降低LLM推理的延迟和成本。向量相似度指标为嵌入提示与其缓存中最邻近的提示之间的相似度分配一个数值分数。现有的系统依赖于一个静态阈值来分类相似度分数是否足够高以导致缓存命中。我们表明，这种一刀切阈值在不同提示之间是不够的。我们提出了VectorQ，一个框架来学习嵌入特定的阈值区域，以适应嵌入的复杂性和不确定性。通过在四个不同的数据集组合上的评估，我们表明VectorQ在所有静态阈值上均优于最先进的系统，缓存命中率提高至12倍，错误率降低至92%。|\n",
        "2502.03589": "|**2025-02-05**|**HACK: Homomorphic Acceleration via Compression of the Key-Value Cache for Disaggregated LLM Inference**|Zeyu Zhang et.al.|[2502.03589](http://arxiv.org/abs/2502.03589)|null|分拆大型语言模型（LLM）推理因其将计算密集的前填充阶段与内存密集的解码阶段分离，避免了前填充-解码干扰并提高了资源利用率而受到青睐。然而，在两个阶段之间传输键值（KV）数据可能成为瓶颈，尤其是在处理长提示时。此外，前填充和解码的计算时间开销是优化作业完成时间（JCT）的关键，而对于长提示和序列，KV数据大小可能变得难以承受。现有的KV量化方法可以缓解传输瓶颈并减少内存需求，但它们引入了显著的逆量化开销，加剧了计算时间。我们针对分拆LLM推理提出了基于KV缓存压缩的同态加速（HACK）。HACK消除了繁重的KV逆量化步骤，并直接在量化KV数据上执行计算，以近似并减少昂贵的矩阵乘法步骤的成本。大量的基于跟踪的实验表明，与分拆LLM推理基线相比，HACK将JCT降低了高达70.9%，与最先进的KV量化方法相比，降低了高达52.3%。|\n",
        "2502.04677": "|**2025-02-07**|**LLM Query Scheduling with Prefix Reuse and Latency Constraints**|Gregory Dexter et.al.|[2502.04677](http://arxiv.org/abs/2502.04677)|null|在在线环境中高效部署大型语言模型（LLMs）需要优化推理性能，以满足严格的延迟约束，尤其是首次输出时间（TTFT）和每输出字符时间（TPOT）。本文重点关注具有前缀重用的LLM推理查询调度问题，这是一种利用查询之间的共享前缀来减少计算开销的技术。我们的研究揭示了现有先来先服务（FCFS）和最长前缀匹配（LPM）调度策略在满足延迟约束方面的先前未知的局限性。我们提出了一个基于RadixAttention的前缀重用机制的LLM查询调度形式化理论框架，RadixAttention是一种将中间表示存储和重用于基数树结构中的前缀重用机制。我们的分析证明了在TTFT约束下，具有前缀重用的调度问题属于NP难题，并提出了一种新的调度算法$k$-LPM，它通过平衡查询处理中的前缀重用和公平性来泛化现有方法。理论保证表明，在由数据生成模型捕捉到的现实交通模式中，$k$-LPM在首次输出时间性能方面实现了改进。在现实服务设置中的实证评估验证了我们的发现，与基线方法相比，P99首次输出时间显著减少。|\n",
        "2502.04563": "|**2025-02-06**|**WaferLLM: A Wafer-Scale LLM Inference System**|Congjie He et.al.|[2502.04563](http://arxiv.org/abs/2502.04563)|null|随着人工智能加速器的兴起，越来越多的加速器采用晶圆级制造技术，将数十万个AI核心集成在基于网格的架构中，并配备大容量分布式片上内存（总计数十GB）和超高的片上内存带宽（数十PB/s）。然而，目前针对如GPU等共享内存架构优化的LLM推理系统，无法充分利用这些加速器。我们介绍了WaferLLM，这是第一个晶圆级LLM推理系统。WaferLLM遵循一个新颖的PLMR设备模型，该模型捕捉了晶圆级架构的独特硬件特性。利用这个模型，WaferLLM开创了晶圆级LLM并行处理，优化了数十万个片上核心的利用率。它还引入了MeshGEMM和MeshGEMV，这是第一个专为在晶圆级加速器上有效扩展而设计的GEMM和GEMV实现。评估结果显示，WaferLLM比最先进的系统实现了200倍的晶圆级加速器利用率提升。在商用晶圆级加速器上，与先进的GPU相比，WaferLLM实现了606倍的GEMV速度和22倍的能效。对于LLM来说，WaferLLM实现了39倍的解码速度和1.7倍的能效提升。我们预计，随着晶圆级AI模型、软件和硬件的不断成熟，这些数字将显著增长。|\n",
        "2502.04420": "|**2025-02-06**|**KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference**|Xing Li et.al.|[2502.04420](http://arxiv.org/abs/2502.04420)|null|KV缓存量化可以提高大型语言模型（LLMs）在长文本和大数据量场景下的推理吞吐量和延迟，同时保持LLMs的有效性。然而，现有方法存在三个未解决的问题：忽视了层间对KV缓存量化的敏感性、在线细粒度决策的高开销以及对不同LLMs和约束条件的低灵活性。因此，我们深入分析了层间Transformer注意力模式与KV缓存量化误差之间的固有相关性，并研究了为什么对于量化误差减少来说，键缓存比值缓存更重要。我们进一步提出了一种简单而有效的框架KVTuner，通过多目标优化自适应地搜索粗粒度KV缓存的最佳硬件友好层间KV量化精度对，并在在线推理中直接利用离线搜索到的配置。为了减少离线校准的计算成本，我们利用层内KV精度对剪枝和层间聚类来减少搜索空间。实验结果表明，我们可以在Llama-3.1-8B-Instruct等LLMs上实现近无损的3.25位混合精度KV缓存量化，在数学推理任务中对敏感模型如Qwen2.5-7B-Instruct实现4.0位量化。与KV8量化相比，在各种文本长度下，最大推理吞吐量可以提高38.3%。|\n",
        "2502.04416": "|**2025-02-06**|**CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference**|Zehua Pei et.al.|[2502.04416](http://arxiv.org/abs/2502.04416)|**[link](https://github.com/JarvisPei/CMoE)**|**大型语言模型（LLMs）通过扩展模型参数实现了令人印象深刻的性能，但这伴随着显著的推理开销。占LLM参数主导地位的卷积前馈网络（FFNs）在隐藏神经元中表现出高激活稀疏性。为了利用这一点，研究人员提出了使用专家混合（MoE）架构，其中只有一部分参数被激活。然而，现有方法通常需要大量的训练数据和资源，限制了它们的实用性。我们提出了CMoE（Carved MoE），这是一种新颖的框架，能够高效地从密集模型中雕刻MoE模型。CMoE通过高效的专家分组和轻量级适应实现了显著的性能。首先，根据激活率将神经元分组为共享和路由专家。接下来，我们构建了一个无需从头开始训练的路由机制，其中包含可微分的路由过程和负载均衡。使用适度的数据，CMoE在五分钟内从一个7B密集模型中产生了一个设计精良、可用的MoE。通过轻量级微调，它在一小时内实现了高性能恢复。我们将我们的代码公开在https://github.com/JarvisPei/CMoE上。**|\n",
        "2502.05610": "|**2025-02-08**|**Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models**|Soham Poddar et.al.|[2502.05610](http://arxiv.org/abs/2502.05610)|null|大型语言模型（LLMs）因其卓越的生成能力和在各种任务中的多用途性而越来越受到认可。然而，与现有研究中对训练成本的关注相比，这些模型的高推理成本尚未得到足够的关注。为了填补这一空白，我们的研究对LLMs在广泛NLP任务中的推理能耗进行了全面的基准测试，其中我们分析了不同模型、任务、提示和系统相关因素对推理能耗的影响。具体来说，我们的实验揭示了几个有趣的见解，包括推理能耗与输出令牌长度和响应时间的强烈相关性。此外，我们发现量化、最佳批大小以及有针对性的提示短语可以显著降低能耗。这项研究是首次全面基准测试LLMs推理在如此多样化的方面，提供了见解并提出了改进模型部署中能源效率的几项建议。|\n",
        "2502.05489": "|**2025-02-08**|**Mechanistic Interpretability of Emotion Inference in Large Language Models**|Ala N. Tak et.al.|[2502.05489](http://arxiv.org/abs/2502.05489)|null|大型语言模型（LLMs）在预测文本中的人类情绪方面展现出有前景的能力。然而，这些模型处理情绪刺激的机制在很大程度上仍未得到探讨。我们的研究通过调查自回归LLMs如何推断情绪来填补这一空白，表明情绪表征在模型中功能性地局部化到特定区域。我们的评估包括多样化的模型系列和大小，并得到了稳健性检查的支持。随后，我们通过借鉴认知评估理论，一个将情绪视为对环境刺激评估（评估）而产生的心理框架，来证明所识别的表征在心理上是合理的。通过对构建的评估概念进行因果干预，我们引导了生成过程，并表明输出与理论直觉相符。这项工作突出了因果干预和精确塑造情感文本生成的新方法，这可能对敏感情感领域的安全性和一致性产生潜在益处。|\n",
        "2502.05376": "|**2025-02-07**|**BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference**|Reena Elangovan et.al.|[2502.05376](http://arxiv.org/abs/2502.05376)|null|在训练后量化（PTQ）是降低大型语言模型（LLMs）存储和计算需求的一种有前景的方法，且无需额外训练成本。最近的研究主要集中在仅对权重进行低于8位的量化，同时保持激活值为8位或更高。在没有依赖量化感知训练的情况下，对权重和激活值都进行准确的低于8位的量化仍然是一个重大挑战。我们提出了一种名为块聚类量化（BCQ）的新颖量化方法，其中每个操作张量被分解为块（块是一组连续的标量），块根据其统计信息进行聚类，并为每个聚类设计一个专用的最优量化码本。作为这种方法的具体实现，我们提出了一种名为局部最优BCQ（LO-BCQ）的PTQ算法，该算法在块聚类和码本设计步骤之间迭代，以贪婪地最小化量化均方误差。当权重和激活标量编码为W4A4格式（存储缩放因子和码本选择器的开销为0.5位）时，我们在多个LLMs和下游任务中展示了<1%的推理精度损失，从而将当前的最先进水平推进了一步。|\n",
        "2502.07578": "|**2025-02-11**|**PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference**|Yufeng Gu et.al.|[2502.07578](http://arxiv.org/abs/2502.07578)|**[link](https://github.com/Yufeng98/CENT)**|**大型语言模型（LLM）推理以自回归方式逐个生成标记，与早期的机器学习（ML）模型如仅编码器变压器和卷积神经网络相比，其运算强度显著降低。同时，LLM具有大型参数规模，并使用键值缓存来存储上下文信息。现代LLM支持高达100万个标记的上下文窗口，以生成多样化的文本、音频和视频内容。每个提示所需的大型键值缓存需要巨大的内存容量，限制了推理批量大小。低运算强度和有限的批量大小需要高内存带宽。然而，当代机器学习模型部署的硬件系统，如GPU和TPU，主要是针对计算吞吐量进行优化的。这种不匹配给高级LLM的高效部署带来了挑战，并使用户不得不为内存密集型LLM推理任务支付昂贵的计算资源费用。  我们提出了CENT，一个CXL-Enabled无GPU系统，用于LLM推理，它利用CXL内存扩展能力来容纳大量的LLM，并利用近银行处理单元来提供高内存带宽，从而消除了对昂贵GPU的需求。CENT利用可扩展的CXL网络，在CXL设备之间支持点对点和集体通信原语。我们实现了各种并行策略，以在这些设备上分布LLM。与具有最大支持批量大小和相似平均功率的GPU基线相比，CENT实现了2.3倍更高的吞吐量，并消耗了2.3倍更少的能量。CENT提高了总拥有成本（TCO），每美元生成的标记比GPU多5.2倍。**|\n",
        "2502.07115": "|**2025-02-10**|**Online Scheduling for LLM Inference with KV Cache Constraints**|Patrick Jaillet et.al.|[2502.07115](http://arxiv.org/abs/2502.07115)|null|大型语言模型（LLM）推理是一个计算密集的过程，需要高效的调度来优化延迟和资源利用率。LLM推理中的关键挑战之一是管理键值（KV）缓存，它减少了冗余计算，但引入了内存限制。在这项工作中，我们理论性地对受KV缓存约束的LLM推理进行了建模，并提出了新颖的批处理和调度算法，以最小化推理延迟并有效管理KV缓存的内存。我们分析了半在线和全在线调度模型，结果有三点。首先，我们提供了一个多项式时间算法，在半在线提示到达模型中实现了平均延迟的精确最优性。其次，在具有随机提示到达的全在线情况下，我们引入了一个具有常数后悔的在线调度算法。第三，我们证明了在完全在线对抗设置中，没有算法（确定性或随机）能够实现常数竞争比。在我们的实证评估中，使用A100 GPU上的Llama-70B模型在公共LLM推理数据集上，我们的方法在降低延迟的同时显著优于当前实践中使用的基准算法，减少了能耗。总的来说，我们的结果为更可持续和成本效益更高的LLM部署提供了一条途径。|\n",
        "2502.08142": "|**2025-02-12**|**Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences**|Shanshan Han et.al.|[2502.08142](http://arxiv.org/abs/2502.08142)|null|我们提出了Wildflare GuardRail，这是一种旨在通过系统性地解决整个处理工作流程中的风险，从而增强大型语言模型（LLM）推断的安全性和可靠性的防护管道。Wildflare GuardRail集成了几个核心功能模块，包括安全检测器，它可以识别不安全的输入并检测模型输出中的幻觉，同时生成根本原因解释；归一化模块，它通过从向量数据库检索信息来上下文化用户查询；定制器，它使用轻量级的基于规则的包装器实时调整输出；以及修复器，它使用安全检测器提供的幻觉解释来纠正错误的LLM输出。结果显示，我们的安全内容检测模型在安全检测器中达到了与OpenAI API相当的性能，尽管它是在由多个公开数据集构建的小数据集上训练的。同时，轻量级包装器可以在1.06秒内处理每个查询，以100%的准确率解决模型输出中的恶意URL，而无需昂贵的模型调用。此外，幻觉修复模型在减少幻觉方面表现出80.7%的准确性。|\n",
        "2502.07903": "|**2025-02-11**|**HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment**|Youhe Jiang et.al.|[2502.07903](http://arxiv.org/abs/2502.07903)|null|将预填充和解码阶段进行解耦，为大型语言模型（LLM）的生成推理提供了一个有效的新范式，它消除了预填充-解码干扰并优化了资源分配。然而，如何将解耦推理范式部署到一组异构GPU上仍然是一个未解决的问题，这可以作为一种经济替代方案，用于在相同的高性能同构GPU上部署。为此，我们引入了HexGen-2，这是一个遵循解耦范式在异构GPU上高效且经济地提供LLM服务的分布式系统。HexGen-2建立在HexGen之上，其核心组件是一个调度算法，该算法将解耦LLM推理计算和通信在异构GPU和网络连接上的分配形式化为一个约束优化问题。我们利用图划分和最大流算法来协同优化资源分配、不同推理阶段的并行策略以及跨阶段键值（KV）缓存通信的效率。我们进行了广泛的实验来评估HexGen-2，即在不同真实世界场景中评估OPT（30B）和Llama-2（70B）模型，结果表明，与最先进的系统相比，HexGen-2在相同的预算下，将服务吞吐量提高了2.0倍，平均提高了1.3倍，将平均推理延迟降低了1.5倍，并且以30%更低的预算实现了可比较的推理性能。|\n",
        "2502.07832": "|**2025-02-11**|**SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters**|Yiping Wang et.al.|[2502.07832](http://arxiv.org/abs/2502.07832)|null|在大型语言模型（LLMs）推动了自然语言处理任务发展的同时，它们日益增长的计算和内存需求使得在资源受限的设备如手机上部署变得越来越具有挑战性。在本文中，我们提出了SHARP（共享相邻层与恢复参数），这是一种通过在相邻层间共享参数来加速LLM推理的新方法，从而减少内存负载开销，同时引入低秩恢复参数以维持性能。受连续层具有相似输出的观察启发，SHARP采用两阶段恢复过程：单层预热（SLW）和监督微调（SFT）。SLW阶段通过L_2损失对共享层的输出进行对齐，为随后的SFT阶段提供一个良好的初始化，以进一步恢复模型性能。广泛的实验表明，SHARP可以在各种分布内任务中使用不超过50k的微调数据恢复模型困惑度，同时将存储的MLP参数数量减少38%至65%。我们还对SHARP进行了多次消融研究，并表明替换模型后期部分的层可以更好地保持性能，并且当参数数量匹配时，不同的恢复参数化表现相似。此外，与原始Llama2-7b模型相比，SHARP在移动设备上节省了42.8%的模型存储，并将总推理时间减少了42.2%。我们的结果突出了SHARP作为在不使用预训练规模资源的情况下减少LLM部署推理成本的效率解决方案。|\n",
        "2502.09419": "|**2025-02-13**|**On multi-token prediction for efficient LLM inference**|Somesh Mehra et.al.|[2502.09419](http://arxiv.org/abs/2502.09419)|null|我们系统地研究了在为下一标记预测（NTP）预训练的LLMs中的多标记预测（MTP）能力。首先，我们通过中间标记概率的数值边缘化表明，这类模型本质上具有MTP能力，尽管性能依赖于数据并随着模型规模的增长而提高。此外，我们探讨了将MTP头集成到冻结LLMs中的挑战，并发现它们的隐藏层强烈专门化于NTP，使得适应变得复杂。最后，我们表明，虽然联合训练MTP头与主干网络可以提高性能，但它无法完全克服这一障碍，这促使我们进一步研究这一方向。我们的发现提供了对应用于预训练LLMs的MTP的更深入理解，并为通过并行标记预测加速推理提供了策略。|\n",
        "2502.08910": "|**2025-02-13**|**InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU**|Heejun Lee et.al.|[2502.08910](http://arxiv.org/abs/2502.08910)|null|在现代大型语言模型（LLM）中，处理非常长的上下文长度带来了显著挑战，因为它会导致推理速度变慢和内存成本增加。此外，大多数现有的预训练LLM无法推广到原始训练序列长度之外。为了实现高效且实用的长上下文利用，我们引入了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过模块化分层标记修剪算法动态删除不相关的上下文标记来加速处理。我们的方法还通过根据LLM内部的注意力模式选择性地应用各种RoPE调整方法，从而允许推广到更长的序列。此外，我们在推理过程中将关键值缓存卸载到主机内存，显著降低了GPU内存压力。因此，InfiniteHiP可以在单个L40s 48GB GPU上处理高达300万个标记，比之前大了3倍，且不会丢失任何上下文信息。我们的框架在注意力解码方面实现了1千万标记上下文的18.95倍速度提升，而不需要额外的训练。我们在SGLang框架中实现了我们的方法，并通过广泛的评估证明了其有效性和实用性。|\n",
        "2502.08773": "|**2025-02-12**|**Universal Model Routing for Efficient LLM Inference**|Wittawat Jitkrittum et.al.|[2502.08773](http://arxiv.org/abs/2502.08773)|null|大型语言模型在能力上的显著进步伴随着推理成本的显著增加。模型路由是一种降低推理成本的简单技术，其原理是维护一个候选LLM的池子，并学习将每个提示路由到最小的可行LLM。现有工作主要关注学习一个固定LLM池子的路由器。在本文中，我们考虑了动态路由问题，即在测试时，可以访问新的、之前未观察到的LLM。我们针对这个问题提出了一种新方法，该方法依赖于将每个LLM表示为一个特征向量，该特征向量基于对一组代表性提示的预测而得出。基于此，我们详细介绍了两种有效的策略，分别依赖于基于聚类的路由和学习的聚类映射。我们证明了这些策略是理论最优路由规则的估计，并提供了过拟合风险界限来量化它们的误差。在一系列公开基准上的实验表明，所提出的策略在路由超过30个未见LLM方面是有效的。|\n"
    },
    "train": {
        "2411.17691": "|**2024-11-26**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们通过观察发现，在应用低比特量化时，规模较大或训练词数较少的语言模型（LLMs）的量化诱导退化（QiD）较小，而训练词数较多的小型模型则会遭受显著的QiD。为了更深入地了解这一趋势，我们在一个受控环境下研究了1500多个不同规模和训练水平（未训练或完全训练）的量化LLMs检查点，从而推导出QiD与训练词数、模型大小和比特宽度等因素之间的关系定律。利用这些定律，我们提出了一种新视角：可以使用QiD来衡量LLMs的训练水平，并确定不同规模LLMs完全训练所需的训练词数。此外，我们使用这些定律来预测使用100万亿个训练词训练的不同规模LLMs的量化性能。我们的预测表明，未来预期使用超过100万亿个训练词训练的低比特量化模型性能可能并不理想。这为未来的低比特量化提出了潜在的挑战，并突出了在评估低比特量化研究时需要关注模型训练水平。为了促进对此问题的未来研究，我们将本次工作中使用的1500多个量化检查点发布在https://huggingface.co/Xu-Ouyang上。|\n",
        "2411.17679": "|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|**将文本分割成标记的编码技术，如字节对编码（BPE）和字节级BPE（BBPE），通过将文本分割成标记，显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割通常掩盖了标记内部的字符结构和序列，导致模型在训练期间无法完全学习这些复杂的细节。因此，LLMs在理解标记内部的字符组成和位置关系方面存在困难，尤其是在使用有限数据的下游任务中进行微调时。在本文中，我们引入了一种名为标记内部位置感知（TIPA）的新方法，通过使用分词器自己的词汇进行反向字符预测任务来训练模型，从而增强LLMs对内部标记结构的理解。这种方法使模型能够有效地学习和泛化字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测标记级别的字符位置方面优于基线模型。此外，当应用于中文拼写纠正（CSC）的下游任务时，TIPA不仅加速了模型收敛，而且显著提高了任务性能。**|\n",
        "2411.17284": "|**2024-11-26**|**Using Large Language Models for Expert Prior Elicitation in Predictive Modelling**|Alexander Capstick et.al.|[2411.17284](http://arxiv.org/abs/2411.17284)|**[link](https://github.com/alexcapstick/llm-elicited-priors)**|**大型语言模型（LLMs）通过训练不同领域的数据，有效地获取了广泛的信息。然而，它们的计算复杂度、成本和缺乏透明度阻碍了它们在特定任务中的直接应用。在临床研究等领域的预测模型中，获取专家注释或先验知识通常是昂贵且耗时的。本研究提出使用LLMs来获取预测模型的专家先验分布。这种方法也为情境学习提供了一种替代方案，其中语言模型直接负责做出预测。我们比较了LLM获取的先验和无关先验，评估了LLM是否真实地生成了参数分布，并提出了情境学习和先验获取的模型选择策略。我们的研究发现，与无关先验相比，在数据量少的情况下，LLM获取的先验参数分布显著降低了预测误差。应用于临床问题，这意味着所需的生物样本更少，降低了成本和资源。先验获取也始终优于情境学习，且成本更低，因此在我们的环境中成为一种更受欢迎的替代方案。我们展示了该方法在各种用例中的实用性，包括临床应用。在感染预测中，使用LLM获取的先验，在研究中提前200天，以相同的准确度减少了55%所需的标签数量。**|\n",
        "2411.17116": "|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，基于Transformer的大型语言模型（LLM）在长序列上的推理既昂贵又缓慢。我们引入了星形注意力，这是一种两阶段块稀疏逼近，通过在多个主机之间分散注意力来提高计算效率，同时最小化通信开销。在第一阶段，通过主机间的块局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星形注意力可以无缝集成到大多数使用全局注意力训练的基于Transformer的LLM中，通过减少内存需求和提高推理速度至多11倍，同时保持95-100%的准确率。**|\n",
        "2411.16353": "|**2024-11-25**|**The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C**|Mikita Balesni et.al.|[2411.16353](http://arxiv.org/abs/2411.16353)|null|在论文摘要中，作者首先指出大型语言模型（LLMs）在采用思维链（CoT）进行多跳推理（如“Imagine表演者的配偶是谁？”）时表现出色，但在被迫进行内部推理（不使用CoT）时则表现不佳。作者提到之前关于这一差距大小和性质的研究产生了不一致的证据。在这篇论文中，作者介绍了一种控制环境，用于研究LLMs中的两跳推理，其中超出随机水平的性能构成了潜在推理不可否认的证据。作者在虚构事实上进行微调LLMs（包括Llama 3 8B Instruct和GPT-4o），并确认它们可以推广到使用CoT回答有关它们的两跳问题。作者发现，当事实在训练过程中或提示中一起出现时，模型可以进行潜在推理。然而，出乎意料的是，当学习的事实仅出现在不同的文档中时，模型在两跳推理上完全失败，达到了随机水平准确性和测试损失。作者将这种完全无法组合单独学习的事实称为“两跳诅咒”。此外，作者在真实事实上评估了9个前沿LLMs，发现模型在超过一半的问题类别上完全无法进行无CoT的两跳推理，而在大多数类别上仍然部分成功使用CoT。这些结果表明，LLMs缺乏一种独立于问题类型的一般能力来进行潜在的跨跳推理。|\n",
        "2411.15871": "|**2024-11-24**|**Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution**|Haiquan Wang et.al.|[2411.15871](http://arxiv.org/abs/2411.15871)|null|随着大型语言模型（LLMs）的发展，大规模分布式训练变得必要。然而，高度优化的框架由于通信量大，在模型FLOPS利用率上仍然有显著的损失（通常低于50%）。同时，我们的全面分析显示，计算和通信密集型操作的重叠性很好。本文介绍了一种名为DHelix的新型微观结构，它受到DNA结构的启发，显著提高了LLM训练的效率。DHelix设计的核心是链式交错（SI），它将训练微批次连续流通过GPU视为两条链。DHelix并置两条链的前向和后向传递，并通过对称调度来自相对链的操作进行系统优化，这得益于操作级别的重叠分析结果和基于动态规划的搜索算法。同时，DHelix允许两条链共享模型状态和激活数据的空间，有效地容纳了额外内存空间低于3%的两个微批次。DHelix无缝集成到所有现有数据/模型并行形式，其中最具有挑战性的是管道并行，得益于其独特的模型折叠设计，形成了W型管道。我们使用流行的Llama和GPT密集模型，以及Phi混合专家（MoE）模型，在3个GPU集群（A40、A800和H100）上评估了DHelix的训练。结果显示，它在64-A40和64-A800集群上分别实现了12-40%（最高达到58%MFU）和2-29%（最高达到71%MFU）的提高，显著优于现有方法。在H100集群上，虽然更快的网络降低了DHelix的利润空间，但它使得跨节点张量并行成为可能，这在由于通信成本而目前难以实现的情况下是一种实践。|\n",
        "2411.15484": "|**2024-11-23**|**Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai**|Parinthapat Pengpun et.al.|[2411.15484](http://arxiv.org/abs/2411.15484)|**[link](https://github.com/parinzee/seed-free-synthetic-instruct)**|**我们提出了一种针对低资源语言（特别是泰语）的大语言模型（LLM）指令微调的合成数据方法，以数据高效的方式实现。我们确定了三个有助于指令微调数据集有效性的关键属性：流畅性、多样性和文化背景。我们提出了一种无需种子数据框架，用于生成包含这些基本属性的合成指令微调数据。我们的框架使用LLM生成多样化主题，从维基百科中检索相关上下文，并为各种任务（如问答、摘要和对话）创建指令。实验结果表明，我们的最佳表现合成数据集，结合了这三个关键属性，在仅使用5,000条指令的情况下，与在数万条指令上训练的顶尖泰语LLM相比，实现了具有竞争力的性能。我们的代码和数据集可在https://github.com/parinzee/seed-free-synthetic-instruct上公开获取。**|\n",
        "2411.14500": "|**2024-11-21**|**Exploring Accuracy-Fairness Trade-off in Large Language Models**|Qingquan Zhang et.al.|[2411.14500](http://arxiv.org/abs/2411.14500)|null|大型语言模型（LLMs）在人工智能领域取得了显著进展，展示了它们与人类互动和通过信息传播影响人类认知的能力。然而，最近的研究揭示了这些LLMs内含的偏见问题，这成为一个需要关注的重大问题。在我们的研究中，我们深入研究在LLMs增强过程中，如何协调准确性和公平性的复杂挑战。虽然提高准确性确实可以提升LLMs的整体性能，但这往往是以牺牲公平性为代价的。过度强调一个指标的优化必然会导致另一个指标的显著下降。这强调了在设计优化LLMs阶段时考虑多个因素的重要性。因此，我们主张将LLMs的训练过程重新定义为多目标学习任务。我们的研究揭示了多目标进化学习（MOEL）方法在应对这一挑战方面具有前景。我们的MOEL框架能够同时优化准确性和公平性指标，从而产生一组帕累托最优的LLMs。总之，我们的研究为LLMs中准确性和公平性之间的微妙平衡提供了宝贵的见解，这对于它们在现实世界中的应用越来越重要。通过利用MOEL，我们展示了一条通往更公平和更有效的AI技术的可行途径。|\n",
        "2411.13738": "|**2024-11-20**|**Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics**|Tetiana Bas et.al.|[2411.13738](http://arxiv.org/abs/2411.13738)|**[link](https://github.com/tetianabas/llm_biases)**|**本研究通过比较大型语言模型（LLMs）对性别的感知与人类受访者、美国劳工统计局数据和50%无偏见基准的性别感知，来探讨LLMs中的性别偏见。我们使用职业数据和特定角色的句子创建了一个新的评估集。与LLMs训练数据中常见的基准不同，我们的集合是全新开发的，防止了数据泄露和测试集污染。我们对五个LLMs进行了测试，以使用单词答案预测每个角色的性别。我们使用Kullback-Leibler（KL）散度来比较模型输出与人类感知、统计数据和50%中性基准之间的差异。所有LLMs都显示出与性别中性的显著偏差，并且更符合统计数据，仍然反映了固有的偏见。**|\n",
        "2411.13055": "|**2024-11-20**|**Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training**|Jared Fernandez et.al.|[2411.13055](http://arxiv.org/abs/2411.13055)|null|近年来，神经网络模型能力的显著提升是由模型规模、训练数据和相应的计算资源扩展驱动的。为了开发现代应用（如大型语言模型）所需的无尽大型网络，模型训练需要在数万台硬件加速器（例如GPU）上分布进行，这要求在大规模计算集群中协调计算和通信。在本工作中，我们证明了仔细考虑硬件配置和并行化策略对于有效（即计算和成本高效）地扩展模型规模、训练数据和总计算量至关重要。我们对大规模LLM训练工作负载在模型规模、硬件配置和分布式并行化策略方面的性能进行了广泛的实证研究。我们证明了：（1）超过一定规模后，某些分布式通信策略带来的开销导致之前被认为次优的并行化策略实际上变得更为可取；（2）即使硬件和并行化策略得到适当优化，增加加速器的总数来扩大大型模型训练也会迅速产生递减回报，这意味着每增加一个单位的功率或GPU时长的边际性能较差。|\n",
        "2412.01928": "|**2024-12-02**|**MALT: Improving Reasoning with Multi-Agent LLM Training**|Sumeet Ramesh Motwani et.al.|[2412.01928](http://arxiv.org/abs/2412.01928)|null|使大型语言模型（LLMs）之间实现有效协作是实现能够解决复杂问题的自主系统的重要一步。尽管LLMs通常被用作单模型生成器，其中人类对其输出进行批判和改进，但联合训练的协作模型的潜力在很大程度上尚未得到探索。尽管在多智能体通信和辩论环境中取得了有希望的结果，但在训练模型共同完成任务方面进展甚微。在本文中，我们提出了一种针对推理问题的“多智能体LLM训练”（MALT）的第一步。我们的方法采用了一种序列多智能体设置，其中分配给异构LLMs专门的角色：生成器、验证器和改进模型迭代解决问题。我们提出了一种基于轨迹扩展的合成数据生成过程和由基于联合结果的奖励驱动的信用分配策略。这使得我们的后训练设置能够利用正负轨迹来自主地提高每个模型的专门能力，作为联合序列系统的一部分。我们在MATH、GSM8k和CQA上评估了我们的方法，其中MALT在Llama 3.1 8B模型上相对于同一基线模型分别实现了14.14%、7.12%和9.40%的相对改进。这展示了在数学和常识推理问题上的多智能体协作能力的早期进步。更广泛地说，我们的工作为围绕多智能体LLM训练方法的研究提供了具体方向。|\n",
        "2412.01526": "|**2024-12-02**|**Addressing Data Leakage in HumanEval Using Combinatorial Test Design**|Jeremy S. Bradbury et.al.|[2412.01526](http://arxiv.org/abs/2412.01526)|null|大型语言模型（LLMs）在众多领域得到广泛应用，包括软件工程，它们已被用于自动化诸如程序生成和测试分类等任务。随着基于LLM的方法不断进化，定义清晰且稳健的方法以公平评估性能变得尤为重要。基准测试是评估LLMs解决特定任务能力以及评估LLM不同版本随时间解决任务能力的一种常见方法。例如，HumanEval基准测试由164个手工制作的任务组成，已成为评估基于LLM的程序生成的重要工具。然而，使用如HumanEval等基准测试对LLM进行公平评估的一个主要障碍是基准任务和解决方案数据泄露到训练数据集中的问题。这种障碍由于LLM训练数据的黑盒性质而加剧，这使得甚至难以知道是否发生了数据泄露。为了解决数据泄露问题，我们提出了一种新的基准构建方法，该方法将基准测试构建为由模板任务组成，可以使用组合测试设计实例化为新具体任务。对于同一模板任务的具体任务必须足够不同，以至于数据泄露的影响最小，并且足够相似，以至于在性能评估方面可以互换。为了评估我们的基准构建方法，我们提出了HumanEval_T，这是一个使用模板任务和组合测试设计构建的HumanEval的替代基准测试。|\n",
        "2412.01523": "|**2024-12-02**|**Data-Centric and Heterogeneity-Adaptive Sequence Parallelism for Efficient LLM Training**|Yujie Wang et.al.|[2412.01523](http://arxiv.org/abs/2412.01523)|null|扩展LLM的上下文长度（即最大支持的序列长度）具有极其重要的意义。为了促进LLM的长时间上下文训练，序列并行化已成为一项关键技术，它将每个输入序列分散到多个设备上，并需要通信来处理序列。本质上，现有的序列并行化方法假设序列长度均匀（即所有输入序列长度相同），因此为所有输入序列利用单一、静态的散列策略。然而，在现实中，LLM训练语料库中的序列长度表现出显著的差异，通常遵循长尾分布，导致工作负载异质性。在本文中，我们表明采用单一、静态的策略会导致效率低下和资源利用率不足，强调了需要自适应方法来处理序列间的异质工作负载。为此，我们提出了一种异质性自适应序列并行化方法。对于每个训练步骤，我们的方法捕捉序列长度的变化，并根据工作负载特征分配最优的散列策略组合。我们将此问题建模为线性规划优化，并设计了一种高效有效的求解器以找到最优解。此外，我们在支持分布式LLM训练的自适应并行化的高性能系统中实现了我们的方法。实验结果表明，我们的系统在性能上优于最先进的训练框架，最高可达1.98倍。|\n",
        "2412.01505": "|**2024-12-02**|**Scaling Law for Language Models Training Considering Batch Size**|Xian Shuai et.al.|[2412.01505](http://arxiv.org/abs/2412.01505)|null|近年来，大型语言模型（LLMs）取得了显著进步，其中规模法则在快速进展中发挥了关键作用。在本文中，我们实证研究了关键超参数，即全局批量大小，如何影响LLM的训练过程。我们首先训练了参数量从1.25亿到26亿的语模，使用了高达3000亿的高质量标记。通过这些实验，我们建立了一个关于模型大小和训练数据量的基本规模法则。然后，我们考察了批量大小和学习率的变化如何影响这些模型的收敛性和泛化能力。我们的分析得出两种情况下的批量大小规模法则：固定计算预算和固定训练数据量。对不断增大的模型进行的外推实验验证了我们的预测法则，为在特定资源限制下优化LLM训练策略提供了指导。|\n",
        "2412.04747": "|**2024-12-06**|**Code generation and runtime techniques for enabling data-efficient deep learning training on GPUs**|Kun Wu et.al.|[2412.04747](http://arxiv.org/abs/2412.04747)|null|随着深度学习模型的规模扩大，其训练成本显著增加。由于硬件的进步和当前软件堆栈的限制，数据效率的需求日益上升。数据效率是指有效隐藏数据访问延迟和避免不必要的数据处理。由于GPU内存带宽与计算吞吐量之间的差距越来越大，GPU内存容量即将达到限制，以及PyTorch软件堆栈中的不效率（包括缺乏特定设备的PCIe传输优化和高级领域特定抽象），主要挑战随之产生。为了有效地缓解这些数据不效率，本论文分析了代表性深度训练任务中的数据不效率，特别是在图神经网络（GNN）和大型语言模型（LLM）中。然后，它提出了新的运行时和代码生成技术来缓解这些挑战，并在PyTorch堆栈中无缝实现这些优化，同时保持强大的可编程性和互操作性。首先，设计PyTorch-Direct以在PyTorch中集成以GPU为中心的PCIe数据传输范式用于GNN训练。接下来，提出了Hector中间表示（IR）及其代码生成器，以引入领域特定的顶层抽象并系统地解决关系型GNN的内存密集型性能挑战。最后，在LLM训练中，吞吐量越来越受限于GPU内存容量。为了缓解这一问题，设计了并实现了SSDTrain卸载框架。总之，这些贡献表明，代码生成和运行时技术可以系统地缓解深度学习训练中的数据管理瓶颈，这些瓶颈源于工作负载的数据密集性以及深度学习训练软件堆栈中的固有简化。|\n",
        "2412.06370": "|**2024-12-09**|**Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit**|Joshua Freeman et.al.|[2412.06370](http://arxiv.org/abs/2412.06370)|null|近期，由于2023年12月提起的《纽约时报》诉OpenAI诉讼，前沿大型语言模型（LLM）的版权侵权问题受到了广泛关注。纽约时报声称，GPT-4通过在LLM训练中使用复制文章并在其输出中记忆这些输入，从而侵犯了其版权。我们的研究旨在衡量OpenAI的LLM相对于其他LLM在其输出中表现出逐字记忆的倾向，特别是针对新闻文章。我们发现，GPT和Claude模型都使用拒绝训练和输出过滤器来防止记忆文章的逐字输出。我们应用了一个基本的提示模板来绕过拒绝训练，并显示OpenAI模型目前比Meta、Mistral和Anthropic的模型更不容易被激发记忆。我们发现，随着模型规模的增加，特别是超过1000亿参数后，它们表现出显著更强的记忆能力。我们的发现对训练具有实际意义：必须更加关注在非常大型模型中防止逐字记忆。我们的发现也具有法律意义：在评估OpenAI的LLM相对记忆能力时，我们探究了《纽约时报》版权侵权主张的强度和OpenAI的法律辩护，同时强调了生成式AI、法律和政策交叉领域的问题。|\n",
        "2412.05342": "|**2024-12-06**|**Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation**|Xiaoyu Wang et.al.|[2412.05342](http://arxiv.org/abs/2412.05342)|null|大型语言模型（LLM）通常经过微调以参与二元或双方面对面的对话，但它们在多方面对话（MPD）中的适应能力较差，这阻碍了它们在多个人会议、讨论和日常交流等场景中的应用。以往基于LLM的研究主要关注多智能体框架，而其基础LLM仍然是成对微调的。在本工作中，我们设计了一个针对多方面对话数据集的LLM多方面微调框架（MuPaS），并证明这样一个简单的框架可以让LLM高效有效地与多方面对话风格保持一致。我们还设计了两种训练策略，可以将MuPaS转化为MPD模拟器。大量实验表明，MuPaS可以实现最先进的多人回答，更高的一轮预测准确率，更高的人机和自动评估的说话质量，甚至能够在分布外场景、主题和角色描述中生成合理的内容。MuPaS框架将LLM的训练与更复杂的多人应用（如对话生成、虚拟排练或元宇宙）相连接。|\n",
        "2412.07298": "|**2024-12-10**|**The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model**|Jiawei Chen et.al.|[2412.07298](http://arxiv.org/abs/2412.07298)|null|大型语言模型（LLMs）展现出显著的多语言能力。然而，这些能力在预训练过程中发展的机制尚不清楚。在本文中，我们利用代码LLMs作为实验平台，探讨LLMs在预训练过程中多语言能力的演变。基于我们的观察，我们提出了巴别塔假说，该假说描述了LLMs获取新语言能力的整个过程。在学习过程中，多种语言最初共享一个由主要语言主导的知识系统，并逐渐发展出特定于语言的知识系统。然后，我们通过识别工作语言和语言转换神经元来追踪LLMs的内部状态，以验证上述假说。实验结果表明，LLMs的内部状态变化与我们的巴别塔假说一致。基于这些洞察，我们提出了一种新的方法来构建针对多语言代码LLMs的优化预训练语料库，其性能显著优于在原始语料库上训练的LLMs。所提出的巴别塔假说为设计预训练数据分布以实现LLMs最佳多语言能力提供了新的见解。|\n",
        "2412.07210": "|**2024-12-10**|**EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models**|Jialiang Cheng et.al.|[2412.07210](http://arxiv.org/abs/2412.07210)|null|分布式训练方法对于大型语言模型（LLMs）至关重要。然而，现有的分布式训练方法往往受到通信瓶颈、执行缓慢和弹性有限等问题的影响。为了解决这些问题，我们提出了EDiT，这是一种创新的、高效的分布式训练方法，它结合了定制化的局部随机梯度下降（Local SGD）方法和模型分片技术，以提高大规模训练的效率。EDiT在正向传播过程中执行层级的参数同步，减少通信和内存开销，并实现计算与通信的并行。此外，EDiT采用伪梯度惩罚策略来抑制损失波动，确保训练稳定性并提升性能。此外，我们引入了A-EDiT，这是EDiT的完全异步变体，适用于异构集群。基于EDiT/A-EDiT，我们进行了一系列实验，以验证LLMs的大规模异步训练，并伴随全面的分析。实验结果表明，EDiT/A-EDiT的性能优越，成为不同计算生态系统中分布式LLM训练的稳健解决方案。|\n",
        "2412.07031": "|**2024-12-09**|**Large Language Models: An Applied Econometric Framework**|Jens Ludwig et.al.|[2412.07031](http://arxiv.org/abs/2412.07031)|null|大型语言模型（LLMs）正在被用于经济学研究以形成预测、标注文本、模拟人类反应、生成假设，甚至为不存在数据的时间和地点生成数据。虽然这些应用具有创新性，但它们是否有效？我们何时可以忽略LLMs的内部运作，仅仅依赖它们的输出？我们开发了一个计量经济学框架来回答这个问题。我们的框架区分了两种类型的实证任务。在以下条件下，使用LLMs的输出进行预测问题（包括假设生成）是有效的：LLMs的训练数据集与研究人员样本之间没有“泄露”。使用LLMs的输出进行估计问题以自动化某些经济概念（由某些文本或人类受试者表达）的测量需要额外的假设：LLMs的输出必须与它们所取代的黄金标准测量一样好。否则，即使LLMs的输出高度准确但并非完美，估计也可能存在偏差。我们在金融和政治经济学中的实例应用中记录了这些条件被违反的程度及其对研究发现的含义。我们还为实证研究人员提供了指导。确保没有训练泄露的唯一方法是使用具有记录的训练数据和发布权重的开源LLMs。处理LLMs测量误差的唯一方法是收集验证数据和建模误差结构。一个推论是，如果无法满足候选LLMs应用的条件，我们强烈建议：不要使用。|\n",
        "2412.11625": "|**2024-12-16**|**Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods**|Diana Bar-Or Nirman et.al.|[2412.11625](http://arxiv.org/abs/2412.11625)|null|虽然大型语言模型（LLMs）已成为各个领域的核心工具，但它们往往提供不准确或错误的信息。本研究考察了用户对LLMs错误信息回应的偏好。具体来说，我们评估了用户对LLMs中明确标记的虚假陈述与未标记回应的偏好，以及对比LLM承认缺乏知识的声明和自信的虚假陈述的偏好。此外，我们还调查了要求用户评估陈述真实性如何影响这些偏好。令人惊讶的是，61%的用户更喜欢未标记的错误回应，而69%的用户更喜欢自信的虚假陈述而不是LLM承认缺乏知识。在我们的所有实验中，共有300名用户参与，为我们的分析和结论提供了宝贵的数据。当用户需要评估陈述的真实性时，对未标记和错误回应的偏好略有下降，但仍然很高。这些发现表明，用户偏好可能会通过反馈机制影响LLM的训练，并无意中鼓励生成虚假信息。未来的研究应解决将LLM行为与这种偏好对齐的伦理和实际影响。|\n",
        "2412.11102": "|**2024-12-15**|**Empowering LLMs to Understand and Generate Complex Vector Graphics**|Ximing Xing et.al.|[2412.11102](http://arxiv.org/abs/2412.11102)|null|大型语言模型（LLMs）的空前进步深刻影响了自然语言处理，但尚未完全涉及可扩展矢量图形（SVG）生成的领域。虽然LLMs在训练过程中编码了来自网页的SVG数据的部分知识，但近期研究发现，LLMs中的语义模糊和分词表示可能会导致矢量原初预测中的幻觉。此外，LLM的训练通常缺乏对矢量路径渲染序列的建模和理解，这可能导致输出矢量原初之间的遮挡。在本文中，我们提出了LLM4SVG，这是填补这一差距的初步但重要的一步，使LLMs能够更好地理解和生成矢量图形。LLM4SVG通过可学习的语义标记促进了对SVG组件的深入了解，这些标记精确地编码了这些标记及其相应的属性，以生成语义一致的SVG输出。通过一系列可学习的语义标记，开发了一个用于指令遵循的结构化数据集，以支持两个主要任务的理解和生成。我们的方法引入了一个模块化架构，将语义标签、矢量指令编码器、微调命令和强大的LLMs紧密结合起来，以紧密结合几何、外观和语言信息。为了克服SVG-text指令数据的稀缺性，我们开发了一个自动化的数据生成管道，收集了超过250k SVG数据和580k SVG-text指令的庞大数据集，这促进了LLM开发中流行的两阶段训练策略的采用。通过探索各种训练策略，我们开发了LLM4SVG，它在人类评估任务中显著超越了基于优化渲染的方法和基于语言模型的基线，取得了显著成果。|\n",
        "2412.10434": "|**2024-12-11**|**NAT-NL2GQL: A Novel Multi-Agent Framework for Translating Natural Language to Graph Query Language**|Yuanyuan Liang et.al.|[2412.10434](http://arxiv.org/abs/2412.10434)|**[link](https://github.com/leonyuancode/stockgql)**|**大型语言模型（LLMs）的出现颠覆了许多领域，不仅限于传统的自然语言处理（NLP）任务。最近，将LLMs应用于数据库领域的研究蓬勃发展，作为典型的非关系型数据库，LLMs在图数据库研究中的应用自然引起了广泛关注。近期的研究越来越多地集中于利用LLMs将自然语言翻译成图查询语言（NL2GQL）。尽管取得了一些进展，但这些方法存在明显的局限性，例如它们依赖于简化的流程，这些流程往往忽略了LLMs自主规划和与其他LLMs协作解决复杂NL2GQL挑战的潜力。为了解决这一差距，我们提出了NAT-NL2GQL，一个将自然语言翻译成图查询语言的创新多智能体框架。具体来说，我们的框架由三个协同的智能体组成：预处理智能体、生成智能体和精炼智能体。预处理智能体负责管理作为上下文的数据处理，包括诸如命名实体识别、查询重写、路径链接和提取与查询相关的模式等任务。生成智能体是一个在NL-GQL数据上微调的LLMs，负责根据查询及其相关模式生成相应的GQL语句。精炼智能体负责使用从GQL执行结果中获得的错误信息精炼GQL或上下文。鉴于基于nGQL语法的优质开源NL2GQL数据集稀缺，我们开发了StockGQL，一个由金融市场图数据库构建的数据集。它可在以下网址获取：https://github.com/leonyuancode/StockGQL。在StockGQL和SpCQL数据集上的实验结果表明，我们的方法在性能上显著优于基线方法，突显了其在推进NL2GQL研究方面的潜力。**|\n",
        "2412.13148": "|**2024-12-17**|**SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba，2015）对于大型语言模型的成功至关重要。然而，它们通常需要在训练过程中维护优化器状态，这可能导致内存需求比模型足迹大几倍。这种开销对可扩展性和计算效率提出了限制。与此相反，随机梯度下降（SGD）是一种无状态优化器，因为它在训练过程中不跟踪状态变量。因此，它实现了最佳内存效率。然而，它在LLM训练中的能力有限（Zhao等，2024b）。在这项工作中，我们表明以无状态方式预处理SGD可以达到与Adam优化器相同的性能，同时大幅降低内存成本。具体来说，我们建议使用归一化和白化预处理瞬时随机梯度。我们表明归一化稳定了梯度分布，而白化抵消了损失景观的局部曲率。这导致了SWAN（带有白化和归一化的SGD），这是一种随机优化器，消除了存储任何优化器状态的需求。经验表明，SWAN的内存占用与SGD相同，与Adam相比，总端到端内存减少了约50%。在语言建模任务中，SWAN表现出与Adam相当甚至更好的性能：当使用350M和1.3B参数预训练LLaMA模型时，SWAN通过使用一半的标记达到相同的评估困惑度，实现了2倍的速度提升。|\n",
        "2412.13998": "|**2024-12-18**|**Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes**|Katarzyna Kobalczyk et.al.|[2412.13998](http://arxiv.org/abs/2412.13998)|**[link](https://github.com/kasia-kobalczyk/few-shot-steerable-alignment)**|**随着大型语言模型（LLMs）在日常应用中越来越广泛地嵌入，确保它们与个体用户的多样化偏好保持一致已成为一项关键挑战。目前部署的方法通常假设用户目标具有同质性，并依赖于单目标微调。然而，人类偏好本质上具有异质性，受各种不可观测因素的影响，导致偏好数据中出现冲突的信号。现有解决这一多样性的解决方案通常需要昂贵的、针对特定目标标记的数据集，并涉及训练多个奖励模型或LLM策略，这既计算成本高又不切实际。在本工作中，我们提出了一种新颖的少量样本可操控对齐框架，从用户选择的少量样本中推断其潜在偏好。为此，我们将Bradley-Terry-Luce模型扩展到处理具有不可观测变异因素的异质偏好，并提出了其实际的奖励建模和LLM微调实现。得益于我们提出的功能参数空间条件化方法，使用我们的框架训练的LLMs可以在推理时适应个体偏好，生成跨越行为模式连续体的输出。我们通过实证验证了方法的有效性，证明了它们能够以数据高效的方式捕捉并适应多样化的人类偏好。我们的代码可在以下网址获得：https://github.com/kasia-kobalczyk/few-shot-steerable-alignment。**|\n",
        "2412.13670": "|**2024-12-18**|**AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge**|Xiaobao Wu et.al.|[2412.13670](http://arxiv.org/abs/2412.13670)|**[link](https://github.com/bobxwu/antileak-bench)**|数据污染阻碍了公平的LLM评估，因为它会将测试数据引入新模型的训练集中。现有研究通过更新基准数据集来解决这一挑战。然而，它们无法保证评估的无污染性，因为新收集的数据可能包含现有知识，并且它们的基准更新依赖于大量的人工劳动。为了解决这些问题，我们在本文中提出了AntiLeak-Bench，一个自动化的反泄露基准测试框架。我们不是简单地使用新收集的数据，而是构建了包含LLM训练集中未明确出现的新知识的样本，从而确保严格的无污染评估。我们进一步设计了一个完全自动化的工作流程，用于构建和更新我们的基准，无需人工劳动。这显著降低了基准维护的成本，以适应新兴的LLM。通过广泛的实验，我们强调数据污染很可能在LLM截止时间之前存在，并证明了AntiLeak-Bench有效地克服了这一挑战。|\n",
        "2412.14479": "|**2024-12-19**|**Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters**|Zihan Chang et.al.|[2412.14479](http://arxiv.org/abs/2412.14479)|null|现有工作仅适用于一定数量的GPU，常常忽略了手动确定所需GPU的具体类型和数量的复杂性，这对开发者来说可能是一个巨大的负担。为了解决这个问题，我们提出了Frenzy，这是一种针对异构GPU集群的内存感知无服务器计算方法。Frenzy允许用户提交模型时无需担心底层硬件资源。首先，Frenzy通过估算LLM的GPU内存使用量来预测所需的GPU数量和类型。然后，它采用低开销的异构感知调度方法来优化训练效率。我们通过在具有三种不同GPU类型的异构GPU集群上进行的LLM多任务训练测试验证了Frenzy的性能。结果显示，Frenzy的内存使用量预测准确率超过92%，调度开销降低了10倍，与最先进的方法相比，它将平均作业完成时间缩短了12%至18%。|\n",
        "2412.14373": "|**2024-12-18**|**ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling**|William Han et.al.|[2412.14373](http://arxiv.org/abs/2412.14373)|**[link](https://github.com/willxxy/ecg-byte)**|**大型语言模型（LLMs）在除文本以外的领域，特别是心电图（ECGs）领域，展现出惊人的适应性。更具体地说，有一批研究正在探索从多通道ECG和相应的文本提示生成文本的任务。目前的方法通常涉及使用自监督学习（SSL）目标预训练一个ECG特定的编码器，并使用预训练编码器输出的特征微调LLM以进行自然语言生成（NLG）。然而，这些方法受限于1）两阶段训练的效率低下和2）编码器生成的特征的可解释性挑战。为了解决这些限制，我们引入了ECG-Byte，这是一种用于ECG自回归语言模型的适应性字节对编码（BPE）标记化流水线。这种方法将ECG信号压缩并编码成标记，通过直接结合ECG和文本标记实现端到端LLM训练，同时由于ECG标记可以直接映射回原始信号，因此具有更高的可解释性。使用ECG-Byte，我们在NLG任务中实现了与两阶段方法相当的性能，但所需时间仅为后者的一半，所需数据量也减少了约48%。**|\n",
        "2412.15495": "|**2024-12-20**|**TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use**|Junjie Ye et.al.|[2412.15495](http://arxiv.org/abs/2412.15495)|**[link](https://github.com/junjie-ye/tl-training)**|**大语言模型（LLMs）通过利用工具与环境交互，实现了显著的进步，这是通向通用人工智能的关键一步。然而，标准的有监督微调（SFT）方法依赖于大规模数据集，往往忽视了工具使用中的特定任务特征，导致性能瓶颈。为了解决这个问题，我们分析了三种现有的LLMs，并揭示了关键见解：训练数据可能会无意中阻碍工具使用行为，标记的重要性分布不均，工具调用错误落入一小组独特的类别中。基于这些发现，我们提出了TL-Training，这是一个基于任务特征的框架，可以减轻不理想训练数据的影响，动态调整标记权重以在SFT期间优先考虑关键标记，并整合了一个针对错误类别的稳健奖励机制，通过近端策略优化进行优化。我们通过训练CodeLLaMA-2-7B并在四个不同的开源测试集上评估它来验证TL-Training。我们的结果表明，我们方法训练的LLM在仅使用1,217个训练数据点的情况下，其工具使用性能与开源和闭源LLMs相当甚至更好。此外，我们的方法增强了在噪声环境中的鲁棒性，并提高了通用任务性能，为LLMs中的工具使用训练提供了一个可扩展且高效的范式。代码和数据可在https://github.com/Junjie-Ye/TL-Training上找到。**|\n",
        "2412.15309": "|**2024-12-19**|**Conceptual In-Context Learning and Chain of Concepts: Solving Complex Conceptual Problems Using Large Language Models**|Nishtha N. Vaidya et.al.|[2412.15309](http://arxiv.org/abs/2412.15309)|null|科学和工程问题属于需要特定概念信息（CI）如数学/逻辑相关知识、流程信息或工程指南来解决的复杂概念问题。大型语言模型（LLMs）由于其在对工程和科学任务如辅助问题解决方面的潜力，是解决此类复杂概念问题的有希望的代理。但是，在开放世界数据上训练的vanilla LLMs缺乏必要的CI。在这项工作中，我们专门探索了LLMs的浅层定制方法（SCMs）来解决复杂概念问题。我们为LLM提出了两种新颖的SCM算法，以增强LLMs的CI并使其能够解决复杂概念问题：概念性情境学习（C-ICL）和概念链（CoC）。本文解决的问题是基于数据建模指南中的概念信息在工程/行业领域生成专有数据模型。我们评估了我们的算法在OpenAI LLM的不同大小上，与四个与句法和语义正确性、时间和成本相关的评估指标。所提出的算法在性能上优于目前流行的LLM SCMs，如情境学习（ICL）和思维链（CoT）。观察到，与CoT相比，新SCM C-ICL和CoC的响应正确性分别提高了30.6%和29.88%。定性分析表明，所提出的新的SCMs在LLMs中激活了之前在现有SCMs中未见的能力。它们使问题解决过程更加透明，并减少了幻觉以及模型响应复制提示（鹦鹉学舌）的趋势。|\n",
        "2412.15282": "|**2024-12-18**|**A Systematic Examination of Preference Learning through the Lens of Instruction-Following**|Joongwon Kim et.al.|[2412.15282](http://arxiv.org/abs/2412.15282)|null|偏好学习是一种广泛采用的后训练技术，它使大型语言模型（LLMs）与人类偏好相一致，并提高特定下游任务的能力。在本研究中，我们系统地调查了偏好数据集的特定属性如何影响LLMs在指令跟随任务中的对齐和下游性能。我们使用一种新颖的合成数据生成流程，生成48,000个独特的指令跟随提示，这些提示由23个可验证的约束组合而成，这些约束能够实现模型响应的精细和自动化质量评估。使用我们的合成提示，我们采用两种偏好数据集整理方法——拒绝采样（RS）和蒙特卡洛树搜索（MCTS）——来获得（选择，拒绝）响应对。然后，我们进行实验，研究以下因素的影响：（1）选择和拒绝响应之间共享前缀的存在，（2）选择、拒绝响应的对比度和质量，（3）训练提示的复杂性。我们的实验表明，由MCTS生成的偏好对中的共享前缀提供了边际但一致的改进，并在具有挑战性的训练配置中提供了更大的稳定性。高对比度偏好对通常优于低对比度对；然而，结合两者通常通过平衡多样性和学习效率来实现最佳性能。此外，与过于具有挑战性的提示相比，在中等难度的提示上进行训练导致在任务之间更好的泛化，即使在更复杂的评估场景中也是如此。我们的发现为优化指令跟随任务的偏好数据整理提供了可操作的见解，提供了一种可扩展且有效的框架，用于增强LLMs的训练和对齐。|\n",
        "2412.17626": "|**2024-12-23**|**Tracking the Feature Dynamics in LLM Training: A Mechanistic Study**|Yang Xu et.al.|[2412.17626](http://arxiv.org/abs/2412.17626)|null|理解大型语言模型（LLMs）的训练动态和特征演变对于其机制可解释性至关重要。尽管稀疏自编码器（SAEs）已被用于识别LLMs中的特征，但这些特征在训练过程中的演变情况仍然难以清晰描绘。在本研究中，我们：（1）引入了SAE-Track，一种高效获取连续SAEs序列的方法；（2）对特征形成过程进行了阐述并进行机制分析；（3）分析和可视化了训练过程中的特征漂移。我们的工作为LLMs中特征动态提供了新的见解，增强了我们对训练机制和特征演变的理解。|\n",
        "2412.19616": "|**2024-12-27**|**Gradient Weight-normalized Low-rank Projection for Efficient LLM Training**|Jia-Hong Huang et.al.|[2412.19616](http://arxiv.org/abs/2412.19616)|**[link](https://github.com/jhhuangkay/gradient-weight-normalized-low-rank-projection-for-efficient-llm-training)**|**大型语言模型（LLMs）在各种任务上表现出色，但日益增长的计算资源需求带来了重大挑战，尤其是在广泛使用全微调进行下游任务时。为了解决这个问题，我们开发了一种参数高效微调（PEFT）方法，但它们通常比全微调表现不佳，并且内存效率较低。在这项工作中，我们介绍了一种名为梯度权重归一化低秩投影（GradNormLoRP）的新方法，该方法在保持与全微调相当的性能的同时，提高了参数和内存效率。GradNormLoRP通过归一化权重矩阵来改善梯度条件，从而在优化过程中促进更好的收敛。此外，它对权重和梯度矩阵应用低秩近似，显著降低了训练过程中的内存使用。广泛的实验表明，我们的8位GradNormLoRP将优化器内存使用量减少了高达89.5%，并使得在消费级GPU（如NVIDIA RTX 4090）上预训练大型LLMs（如LLaMA 7B）成为可能，而无需额外的推理成本。此外，GradNormLoRP在微调任务中优于现有的低秩方法。例如，当使用8个秩对RoBERTa模型在所有GLUE任务上进行微调时，GradNormLoRP的平均得分为80.65，超过了LoRA的79.23分。这些结果突显了GradNormLoRP作为高效LLM预训练和微调的有前景替代方案。源代码：https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training**|\n",
        "2412.21123": "|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越多地依赖于网络爬取的数据集，对于未经授权使用受版权保护或个人内容进行训练的担忧日益加剧。尽管有如通用数据保护条例（GDPR）等法规，数据所有者仍然对其内容在模型训练中的使用控制有限。为了解决这个问题，我们提出了ExpShield，这是一种主动的自我保护机制，使内容所有者能够将不可见的扰动嵌入到他们的文本中，限制LLMs训练中的数据滥用，同时不影响可读性。这种预防性方法使数据所有者能够直接保护敏感内容，而无需依赖第三方进行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的合理性。我们进一步通过识别记忆触发器和创建陷阱来更集中地偏离模型记忆，从而提高效率。为了验证我们防御措施的有效性，我们提出了一个新颖的实例利用度量，该度量捕捉了模型训练引起的个体风险。实验结果表明，我们的方法有效，MIA AUC从0.95降至0.55，实例利用接近于零。这表明训练后个体风险并未增加，突出了在保护受版权数据方面主动防御的重要性。|\n",
        "2501.01668": "|**2025-01-03**|**CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis**|Bohan Zhang et.al.|[2501.01668](http://arxiv.org/abs/2501.01668)|**[link](https://github.com/ruckbreasoning/cot-based-synthesizer)**|**当前推理缩放方法，如自洽性和最佳N项，已被证明在提高LLMs在复杂推理任务上的准确性方面是有效的。然而，这些方法高度依赖候选回答的质量，当所有候选回答都不正确时，它们无法产生正确的答案。在本文中，我们提出了一种新颖的推理缩放策略，即基于CoT的合成器，它利用CoT推理通过分析多个候选回答的互补信息来合成更优的答案，即使所有候选回答都有缺陷。为了实现轻量级和成本效益的实施，我们引入了一个自动数据生成管道，该管道创建多样化的训练数据。这使得在训练数据上训练的小型LLMs可以提高大型模型的推理准确性，包括基于API的LLMs。在四个基准数据集和七个策略模型上的实验结果表明，我们的方法显著提升了性能，在MATH数据集上Llama3-8B提升了11.8%，GPT-4o提升了10.3%。相应的训练数据和代码可在https://github.com/RUCKBReasoning/CoT-based-Synthesizer上公开获取。**|\n",
        "2501.03151": "|**2025-01-06**|**Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**|Alhassan Mumuni et.al.|[2501.03151](http://arxiv.org/abs/2501.03151)|null|基于大规模预训练基础模型（PFMs）的生成式人工智能（AI）系统，如视觉-语言模型、大型语言模型（LLMs）、扩散模型和视觉-语言-行动（VLA）模型，已在众多领域和情境中展示了解决复杂且真正非平凡AI问题的能力。特别是，多模态大型语言模型（MLLMs）从广泛多样的数据源中学习，使世界有了丰富细腻的表征，从而提供了广泛的能力，包括推理、进行有意义的对话；与人类和其他代理共同解决复杂问题；以及理解人类的社会和情感方面。尽管这一成就令人印象深刻，但基于大规模数据集训练的最先进LLMs的认知能力仍然肤浅且脆弱。因此，通用LLMs在它们的通用能力方面受到了严重限制。为了使LLMs达到人类水平的通用智能，需要解决一系列基础问题——具身化、符号接地、因果关系和记忆。这些概念与人类认知更为一致，并为LLMs提供了固有的类似人类的认知属性，支持实现物理上可行、语义上有意义、灵活且更具可推广性的知识和智能。在本工作中，我们讨论了上述基础问题，并概述了在LLMs中实现这些概念的最先进方法。具体而言，我们讨论了如何利用具身化、符号接地、因果关系和记忆的原则，以有机的方式实现人工通用智能（AGI）。|\n",
        "2501.02625": "|**2025-01-05**|**HALO: Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning**|Saleh Ashkboos et.al.|[2501.02625](http://arxiv.org/abs/2501.02625)|null|量化大型语言模型（LLMs）的训练仍然是一个未解决的问题，因为在低精度下执行所有矩阵乘法以保持准确性的尝试已经证明是困难的。这在微调预训练模型时尤为如此，因为它们通常已经具有很大的权重和激活异常值，这使得量化优化变得困难。我们提出了HALO，这是一种针对Transformer的新型量化感知训练方法，通过结合以下三个方面，实现了准确且高效的低精度训练：1）在正向和反向传播过程中战略性地放置Hadamard旋转，以减轻低精度计算中的异常值；2）集成FSDP进行低精度通信；3）高性能内核支持。我们的方法确保正向和反向传播过程中所有的大矩阵乘法都在较低精度下执行。将HALO应用于LLAMA家族模型，在微调各种任务时，HALO实现了接近全精度等效的结果，同时为RTX 4090 GPU上的完整微调提供了高达1.31倍的端到端加速。我们的方法支持标准参数高效微调（PEFT）方法，两者都基于高效的内核实现。我们的结果表明，这是第一个将全量化LLM微调保持FP8精度准确性的实用方法，同时提供了性能优势。|\n",
        "2501.02423": "|**2025-01-05**|**Scaling Laws for Floating Point Quantization Training**|Xingwu Sun et.al.|[2501.02423](http://arxiv.org/abs/2501.02423)|null|低精度训练被认为是降低训练和下游推理成本的有效策略。之前的精度缩放法则主要关注整数量化，对浮点量化中的组成部分关注较少，因此无法很好地适应此场景下的LLM损失。相比之下，尽管浮点量化训练在生产中更常被实施，但对其的研究相对较浅。在本文中，我们彻底探讨了浮点量化目标、指数位、尾数位以及浮点量化训练中缩放因子的计算粒度对LLM模型性能的影响。在提出一个准确的浮点量化统一缩放法则的同时，我们还为社区提供了有价值的建议：（1）指数位对模型性能的贡献略大于尾数位。我们为不同位数提供了最优的指数-尾数位比例，可供硬件制造商未来参考；（2）我们发现低精度LLM训练中临界数据大小的形成。超过临界数据大小的过多训练数据会反向导致LLM性能下降；（3）最优浮点量化精度与计算能力成正比，但在广泛的计算能力范围内，我们估计最佳性价比精度位于4-8位之间。|\n",
        "2501.04266": "|**2025-01-08**|**Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning**|Lang Xu et.al.|[2501.04266](http://arxiv.org/abs/2501.04266)|null|将大规模语言模型（LLM）的训练规模扩大涉及到在有限数量的工作者上调整庞大的训练参数。然而，像ZeRO-3这样的方法虽然大幅降低了GPU内存压力，但往往需要大量的通信来确保全局同步和一致性。现有的努力，如ZeRO++，使用辅助分区以避免节点间通信，因为节点内GPU-GPU传输通常比节点间连接具有更高的带宽和更低的延迟。然而，随着像Frontier这样的更强大的基础设施（配备AMD GPU）的出现，并展现出令人印象深刻的计算能力，有必要对硬件拓扑进行研究，并开发针对性的策略来提高训练效率。在本工作中，我们提出了一系列针对ZeRO++的通信和优化策略，以减少通信成本并提高内存利用率。在本文中，我们提出了一种针对当前顶级超级计算机集群Frontier的3级分层分区方法，旨在利用通信层（GCD-GCD、GPU-GPU和节点间）的多种带宽来减少通信开销。对于20B GPT模型，我们观察到与ZeRO++相比，当达到384个GCD时，每个GPU的TFLOPS提高了1.71倍，并且对于384个GCD的扩展效率为0.94。据我们所知，我们的工作也是首次在Frontier AMD GPU上高效优化LLM工作负载的努力。|\n",
        "2501.04987": "|**2025-01-09**|**TreeKV: Smooth Key-Value Cache Compression with Tree Structures**|Ziwei He et.al.|[2501.04987](http://arxiv.org/abs/2501.04987)|null|高效的关键值（KV）缓存压缩对于在长序列和资源受限环境中扩展基于Transformer的大语言模型（LLM）至关重要。现有方法根据位置或重要性分数驱逐标记，但基于位置的策略可能会遗漏预定义区域外的关键信息，而依赖于全局重要性分数的方法则会导致强烈的区域偏差，限制KV缓存的整体上下文保留，并可能损害LLM在复杂任务上的性能。我们的小波分析揭示，随着标记接近序列末尾，其对生成的贡献逐渐增加，并倾向于与相邻标记更加分离，这表明从遥远到附近上下文的复杂性和变化性随着增加而平滑过渡。受此观察启发，我们提出了一种名为TreeKV的直观、无需训练的方法，该方法使用树结构进行平滑缓存压缩。TreeKV保持固定的缓存大小，即使在长文本场景中也能使LLM提供高质量的输出。与大多数压缩方法不同，TreeKV适用于生成和预填充阶段。在PG19和OpenWebText2上的语言建模任务中，TreeKV在所有基线模型中表现最出色，使训练有短上下文窗口的LLM能够以16倍的缓存减少泛化到更长窗口。在Longbench基准测试中，TreeKV以最佳效率实现了最佳性能，仅使用了6%的预算。|\n",
        "2501.05925": "|**2025-01-10**|**Navigating Tomorrow: Reliably Assessing Large Language Models Performance on Future Event Prediction**|Petraq Nako et.al.|[2501.05925](http://arxiv.org/abs/2501.05925)|null|预测未来事件是一项重要的活动，其应用遍及多个领域和学科。例如，预测股市趋势、自然灾害、商业发展或政治事件的能力可以促进早期预防措施并揭示新的机会。已经提出了多种不同的计算方法来尝试预测未来，包括预测分析、时间序列预测和模拟。本研究评估了几个大型语言模型（LLMs）在支持未来预测任务方面的性能，这是一个尚未充分探索的领域。我们针对三个场景评估这些模型：肯定与可能性提问、推理和反事实分析。为此，我们通过根据实体类型及其流行度查找和分类新闻文章来创建一个数据集。我们收集了LLMs训练截止日期前后新闻文章，以彻底测试和比较模型性能。我们的研究突出了LLMs在预测建模中的潜力和局限性，为未来的改进奠定了基础。|\n",
        "2501.05601": "|**2025-01-09**|**Exploring Large Language Models for Translating Romanian Computational Problems into English**|Adrian Marius Dumitran et.al.|[2501.05601](http://arxiv.org/abs/2501.05601)|null|近期研究表明，大型语言模型（LLMs）在将数学和计算机科学任务从罗马尼亚语翻译成英语时，其表现不如在原始罗马尼亚语格式下。准确的翻译对于从编程竞赛中的自动翻译到高质量教育材料的创建等应用至关重要，同时也有助于减少人工翻译中的错误或欺诈。本研究表明，当给予良好的结构化提示时，鲁棒的大型语言模型（LLMs）可以在翻译较少见语言时保持或甚至提高其性能。我们的研究结果表明，在适当的监督下，LLMs可以可靠地用于IOI（国际信息学奥林匹克）风格任务的自动翻译。我们评估了多个LLMs的多种翻译方法，包括OpenRoLLM、Llama 3.1 8B、Llama 3.2 3B和GPT-4o，通过重复运行来评估它们的翻译准确性和性能稳定性。此外，我们将OJI（罗马尼亚县级信息学奥林匹克）罗马尼亚语数据集与准确的英语翻译相结合，提高了其用于未来LLM培训和评估的实用性。通过详细的句法和语义分析，我们证实了在人工监督下，LLMs可以作为多语言问题解决的可行解决方案。我们还通过认证专家的评估，比较了LLMs与人工翻译者的翻译质量，强调了LLMs在现实场景中的潜力。|\n",
        "2501.07237": "|**2025-01-13**|**Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training**|Ziqing Wen et.al.|[2501.07237](http://arxiv.org/abs/2501.07237)|**[link](https://github.com/zqouo/gwt)**|**大型语言模型（LLMs）在各种自然语言处理任务中展现出了令人印象深刻的性能。然而，它们庞大的参数数量在训练过程中带来了显著的内存挑战，尤其是在使用像Adam这样的内存密集型优化器时。现有的内存高效算法通常依赖于如奇异值分解投影或权重冻结等技术。尽管这些方法有助于缓解内存限制，但与全秩更新相比，它们通常会产生次优的结果。在本文中，我们研究了低秩训练之外的内存高效方法，提出了一种名为梯度小波变换（GWT）的新解决方案，该方法通过对梯度应用小波变换来显著减少维持优化器状态所需的内存需求。我们证明了GWT可以无缝地集成到内存密集型优化器中，从而实现高效训练而不会牺牲性能。通过在预训练和微调任务上的广泛实验，我们展示了与先进的内存高效优化器和全秩方法相比，GWT在内存使用和训练性能方面都达到了最先进的水平。**|\n",
        "2501.07124": "|**2025-01-13**|**LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch**|Zhengzhong Liu et.al.|[2501.07124](http://arxiv.org/abs/2501.07124)|null|我们详细介绍了LLM360 K2-65B模型的训练过程，将我们的360度开源方法扩展到项目LLM360下最大和最强大的模型。尽管开源LLM持续进步，但“最大的LLM是如何训练的？”这一问题在社区中仍然不明确。由于与这些高容量模型相关的商业考虑，其实施细节通常受到保护。这种缺乏透明度阻碍了LLM研究人员利用先前经验中的宝贵见解，例如“解决损失突变的最佳实践是什么？”LLM360 K2项目通过提供LLM最大规模训练过程中积累的全面透明度和资源访问来填补这一空白。本报告重点介绍了K2项目的关键要素，包括我们的第一个模型K2 DIAMOND，这是一个65亿参数的LLM，其性能超过了LLaMA-65B，并与LLaMA2-70B相媲美，同时需要的浮点运算和标记更少。我们详细介绍了实施步骤，并展示了K2 DIAMOND在其训练过程中的纵向能力分析。我们还概述了正在进行的项目，如TXT360，为该系列未来的模型奠定基础。通过提供之前不可用的资源，K2项目也与360度开源原则——透明度、可重复性和可访问性——产生共鸣，我们认为这些原则在资源密集型AI研究时代至关重要。|\n",
        "2501.06842": "|**2025-01-12**|**SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training**|Tianjin Huang et.al.|[2501.06842](http://arxiv.org/abs/2501.06842)|**[link](https://github.com/tianjinyellow/spam-optimizer)**|**大型语言模型（LLMs）在各种任务中表现出色，但它们的训练仍然非常资源密集，并且容易受到训练不稳定等关键挑战的影响。这种不稳定的一个主要来源是梯度损失峰值，它们会干扰学习过程，通常导致昂贵的干预措施，如检查点恢复和实验重启，进一步放大了低效性。本文对LLM训练过程中观察到的梯度峰值进行了全面研究，揭示了它们在多个架构和数据集上的普遍性。我们的分析表明，这些峰值可以比典型梯度大1000倍，严重恶化模型性能。为了解决这个问题，我们提出了Spike-Aware Adam with Momentum Reset SPAM，这是一种新型优化器，旨在通过动量重置和峰值感知梯度裁剪来对抗梯度峰值。广泛的实验，包括预训练和微调，表明SPAM在各种任务中（1）从6000万到10亿的LLM预训练，（2）4位LLM预训练，（3）强化学习，和（4）时间序列预测中，始终优于Adam及其变体。此外，SPAM通过允许稀疏动量来促进内存高效训练，其中只维护和更新动量项的子集。在内存受限的情况下，SPAM优于GaLore和Adam-Mini等最先进的内存高效优化器。我们的工作强调了缓解LLM训练中梯度峰值的重要性，并介绍了一种有效的优化策略，该策略在规模上提高了训练稳定性和资源效率。代码可在https://github.com/TianjinYellow/SPAM-Optimizer.git上找到。**|\n",
        "2501.06802": "|**2025-01-12**|**Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity**|Jun Wan et.al.|[2501.06802](http://arxiv.org/abs/2501.06802)|null|2020年，OpenAI提出了第一种规模定律，描述了模型性能与参数、数据和计算之间的关系。2024年，OpenAI提出了第二种规模定律，描述了模型推理性能与推理计算之间的关系。在本文中，我们使用条件柯尔莫哥洛夫复杂度从无损压缩的角度分析LLM的训练和推理过程，并将这两种类型的规模定律统一起来。我们发现，这两种规模定律都通过增加执行步骤$t$来提高条件柯尔莫哥洛夫复杂度的近似。第一种规模定律通过增加模型参数$y$来增加$t$。第二种规模定律通过增加输出标记的数量来增加$t$。|\n",
        "2501.06471": "|**2025-01-11**|**The Internet of Large Language Models: An Orchestration Framework for LLM Training and Knowledge Exchange Toward Artificial General Intelligence**|Wilson Wei et.al.|[2501.06471](http://arxiv.org/abs/2501.06471)|null|本文探讨了在大型语言模型（LLMs）开发过程中面临的多元挑战，包括模型参数和文件规模的庞大、开发环境配置的复杂性、模型功能的独特性以及计算资源的昂贵成本。为了解决这些挑战，本文提出了三个核心技术解决方案：LLM共享协议、LLM通用环境框架和代理最优路径模块。为了解决研究早期阶段的计算资源限制，我们进一步创新性地提出了一种联合挖掘机制，实现了计算能力提供者和模型设计者之间的双向价值共享，包括对最优模型路径的突破性奖励和长期利润分配，从而为研究人员提供成本优化的计算资源支持，并促进LLM研究和应用的持续发展。|\n",
        "2501.08197": "|**2025-01-14**|**OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training**|Yijiong Yu et.al.|[2501.08197](http://arxiv.org/abs/2501.08197)|**[link](https://github.com/yuyijiong/fineweb-edu-chinese)**|**大型语言模型（LLMs）展现了非凡的能力，但它们的成功很大程度上依赖于预训练语料库的质量。对于中文LLMs来说，高质量中文数据集的稀缺性构成了一个重大挑战，通常限制了它们的性能。为了解决这个问题，我们提出了OpenCSG中文语料库，这是一系列专为LLM预训练、后训练和微调设计的高质量数据集。该语料库包括Fineweb-edu-chinese、Fineweb-edu-chinese-v2、Cosmopedia-chinese和Smoltalk-chinese，每个数据集都有其独特的特点：Fineweb-edu数据集专注于从各种中文网络来源过滤出的高质量内容；Cosmopedia-chinese提供用于知识密集型训练的合成、教科书式数据；Smoltalk-chinese强调风格多样和格式丰富的聊天数据。OpenCSG中文语料库的特点是其高质量的文本、跨领域的广泛覆盖以及可扩展、可重复的数据整理流程。此外，我们还进行了广泛的实验分析，包括对较小参数模型的评估，这证明了该语料库在C-Eval等任务中的显著性能提升，展示了该语料库在训练中文LLMs方面的有效性。**|\n",
        "2501.08716": "|**2025-01-15**|**The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities**|Irina Bigoulaeva et.al.|[2501.08716](http://arxiv.org/abs/2501.08716)|**[link](https://github.com/ukplab/arxiv2025-inherent-limits-plms)**|**大型语言模型（LLMs）经过广泛的网络规模语料库训练，在各个任务上展现出惊人的能力，尤其是在规模扩大后。然而，即使是最先进的模型在某些情况下也难以应对，有时连小孩子都能解决的问题，这表明传统的任务复杂性概念不足以解释LLMs的能力。然而，由于大多数广泛使用的模型也被“指令调整”以适当地响应提示，因此探索LLMs的能力变得复杂。为了解构影响LLMs表现的因素，我们研究了指令调整模型是否具有与使用情境示例提示的基模型根本不同的能力。通过在各个模型家族、规模和任务类型上进行的广泛实验，包括对90个不同的LLMs进行指令调整，我们证明了指令调整模型的性能与其基模型的情境性能显著相关。通过阐明指令调整的贡献，我们扩展了之前关于情境学习的的研究，该研究认为基模型使用预训练数据中的先验知识来解决任务。具体来说，我们将这种理解扩展到指令调整模型，表明它们的预训练数据同样为它们可以解决的任务设定了一个限制边界，并增加了指令调整数据集的影响。**|\n",
        "2501.08496": "|**2025-01-14**|**Quantifying the Importance of Data Alignment in Downstream Model Performance**|Krrish Chawla et.al.|[2501.08496](http://arxiv.org/abs/2501.08496)|null|与传统的关注数据集大小不同，我们探讨了数据对齐——数据质量经常被忽视的方面——在训练有能力的巨型语言模型（LLMs）中的作用。为此，我们使用基于Task2Vec的对齐系数，这是一种衡量两个数据集之间相似性的定量指标，来量化训练数据与评估数据对齐对下游性能的影响。具体来说，我们对两种设置进行了受控的干预实验：1. 预训练（pt）数据与评估数据集之间对齐系数增加的影响，以及2. 领域特定微调（ft）与领域特定评估之间对齐系数增加的影响。我们探索的特定领域任务是自动形式化——自然语言和代码之间的机器翻译任务，用于形式化验证。在这两种设置中，我们发现模型训练数据与评估数据对齐系数与模型在相应下游任务上的损失/困惑度之间存在强烈的、可预测的负相关性。这些发现表明，有必要重新评估LLM的训练方法，证明了数据对齐相对于数据量的相关性，尤其是在自动形式化等特定的下游任务中。|\n",
        "2501.08365": "|**2025-01-14**|**Towards Best Practices for Open Datasets for LLM Training**|Stefan Baack et.al.|[2501.08365](http://arxiv.org/abs/2501.08365)|null|许多AI公司未经版权所有者许可，在数据上训练大型语言模型（LLMs）。这样做是否合法因司法管辖权而异：在欧盟和日本等国家，在一定限制下允许这样做，而在美国，法律环境更为模糊。无论法律状况如何，创意生产者的担忧导致了多起高调的版权诉讼，诉讼威胁通常被引述为近年来企业和公共利益行动者减少共享训练数据集信息的趋势的原因。限制数据信息这一趋势通过拒绝研究人员、审计人员和受影响的个人获取理解AI模型所需的信息，阻碍了透明度、问责制和整个生态系统的创新，造成了损害。虽然通过在开放获取和公共领域数据上训练语言模型可以减轻这种影响，但截至写作之时，由于在组装必要语料库方面的巨大技术和社会挑战，尚无此类模型（以有意义规模训练）。这些挑战包括不完整和不可靠元数据、数字化物理记录的成本和复杂性，以及确保在快速变化的领域中的相关性及责任所需的各种法律和技术技能。建立一个未来，其中AI系统可以在开放许可的数据上进行训练，这些数据得到负责任地管理和治理，需要法律、技术和政策领域的跨学科合作，以及投资于元数据标准、数字化，并培育开放文化。|\n",
        "2501.12243": "|**2025-01-21**|**FOCUS: First Order Concentrated Updating Scheme**|Yizhou Liu et.al.|[2501.12243](http://arxiv.org/abs/2501.12243)|null|大型语言模型（LLMs）展现出惊人的性能，而提升它们的预训练过程似乎是进一步增强其能力的关键。基于Adam、学习率衰减和权重衰减的成功记录，我们假设预训练损失景观具有狭窄的谷地结构。通过在合成损失函数上的实验，我们发现当梯度查询噪声相对于谷地的尖锐度较高时，Adam的表现不如Signum，因为Adam会大幅度减少有效步长。这一观察促使我们开发了FOCUS，这是一种通过引入对移动平均参数的吸引力来增强Signum的优化器，使其在处理噪声的同时保持更大的步长。在训练GPT-2时，FOCUS证明比Signum更稳定，比Adam更快。这些结果表明，梯度噪声可能是LLM训练中一个被低估的限制因素，而FOCUS提供了有希望的解决方案。|\n",
        "2501.11706": "|**2025-01-20**|**Trustformer: A Trusted Federated Transformer**|Ali Abbasi Tadi et.al.|[2501.11706](http://arxiv.org/abs/2501.11706)|null|本文介绍了一种新颖的联邦学习方法，该方法在保持与现有最先进基线相当的有效性的同时，降低了通信开销。我们的方法通过在本地模拟全局模型来避免共享完整模型权重。我们对每个Transformer层应用k-means聚类，本地计算质心，然后将这些质心传输到服务器，而不是完整的权重或梯度。为了增强安全性，我们利用Intel SGX来安全地传输质心。在翻译任务上的评估表明，我们的方法在通信成本显著降低的同时，实现了与最先进基线相当的功效。这为Transformer模型提供了一种更高效且具有隐私保护性的联邦学习解决方案。|\n",
        "2501.11549": "|**2025-01-20**|**Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas**|Nishant Balepur et.al.|[2501.11549](http://arxiv.org/abs/2501.11549)|**[link](https://github.com/pinafore/alignment-personalization)**|**LLMs通过学习用户对提示的两个输出中哪一个更偏好来调整以遵循指令（对齐）。然而，这种偏好数据格式并没有传达用户为什么偏好选择或拒绝的响应，因此在这些数据集上训练的LLM无法根据不同的用户需求定制响应。为了揭示这些个性化的参数，我们应用了溯因推理对偏好数据进行处理，推断出可能偏好每个输出的用户需求和兴趣，即用户角色。我们通过两个步骤测试了这个想法：角色推断（PI）-通过溯因推理推断偏好选择或拒绝输出的用户角色，以及角色定制（PT）-训练模型根据PI推断的角色定制响应。我们发现：1）LLMs可以准确地推断角色，解释为什么不同的用户可能会偏好选择或拒绝的输出；2）通过PT对PI角色进行增强的偏好数据训练提升了个性化，使模型能够支持用户编写的角色；3）被拒绝的响应角色形成了更难的个性化评估，表明PT在帮助具有非典型偏好的用户方面比典型的对齐方法更有优势。我们主张对于个性化而言，采用溯因推理的观点，不仅要问哪个响应更好，还要问什么时候、为什么以及为了谁。**|\n",
        "2501.11006": "|**2025-01-19**|**GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation**|Shashikant Ilager et.al.|[2501.11006](http://arxiv.org/abs/2501.11006)|null|大型语言模型（LLMs）正成为日常生活不可或缺的一部分，展现出它们在自然语言处理（NLP）任务中的巨大潜力。除了NLP之外，LLMs在软件开发任务中的应用也越来越广泛，例如代码补全、修改、错误修复和代码翻译。软件工程师广泛使用GitHub Copilot和Amazon Q等工具，通过高精度自动化任务和简化工作流程。尽管LLMs训练的资源消耗和能源强度经常被强调，但随着时间的推移，推理的资源消耗甚至可能更加密集，因为它是具有大量调用的持续过程。因此，为LLMs推理开发资源高效的替代方案对于可持续性至关重要。这项工作提出了GREEN-CODE，这是一个针对LLMs能源感知代码生成的框架。GREEN-CODE在LLMs推理过程中执行动态早期退出。我们训练了一个强化学习（RL）智能体，使其学会在准确性、延迟和能源消耗之间平衡权衡。我们的方法在两个开源LLMs Llama 3.2 3B和OPT 2.7B上进行了评估，使用了JavaCorpus和PY150数据集。结果显示，我们的方法在代码生成任务中平均降低了23-50%的能源消耗，同时没有显著影响准确性。|\n",
        "2501.10799": "|**2025-01-18**|**Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback**|Yen-Ting Lin et.al.|[2501.10799](http://arxiv.org/abs/2501.10799)|null|大语言模型（LLMs）最近在数学推理方面表现出显著的成功。尽管在思维链提示和自洽采样等方法的进步中取得了进展，但这些进展通常关注最终的正确性，而未确保底层推理过程的一致性和可靠性。本文介绍了Step-KTO，这是一个训练框架，它结合了过程级和结果级的二元反馈，以引导LLMs朝着更可信赖的推理轨迹发展。通过为中间推理步骤和最终答案提供二元评估，Step-KTO鼓励模型遵循逻辑进展而不是依赖表面的捷径。在具有挑战性的数学基准测试上的实验表明，Step-KTO显著提高了最终答案的准确性和中间推理步骤的质量。例如，在MATH-500数据集上，Step-KTO在Pass@1准确率方面相对于强基线实现了显著的提升。这些结果凸显了将步骤级过程反馈整合到LLM训练中的潜力，为更可解释和可靠的推理能力铺平了道路。|\n",
        "2501.12599": "|**2025-01-22**|**Kimi k1.5: Scaling Reinforcement Learning with LLMs**|Kimi Team et.al.|[2501.12599](http://arxiv.org/abs/2501.12599)|null|使用下一词预测进行语言模型预训练已被证明在扩展计算方面非常有效，但其受限于可用训练数据量。通过扩展强化学习（RL）为人工智能的持续改进开辟了新的维度，并承诺大型语言模型（LLMs）可以通过学习使用奖励进行探索来扩展其训练数据。然而，先前发表的工作尚未产生具有竞争力的结果。鉴于这种情况，我们报告了我们最新的多模态LLM Kimi k1.5的训练实践，包括其RL训练技术、多模态数据配方和基础设施优化。长上下文扩展和改进的政策优化方法是我们的方法的关键组成部分，我们建立了一个简单有效的RL框架，而不依赖于更复杂的技术，如蒙特卡洛树搜索、价值函数和过程奖励模型。值得注意的是，我们的系统在多个基准和模态上实现了最先进的推理性能——例如，在AIME上达到77.5，在MATH 500上达到96.2，在Codeforces上达到94百分位数，在MathVista上达到74.9，与OpenAI的o1相匹配。此外，我们提出了有效的长到短方法，使用长-CoT技术来改进短-CoT模型，实现了最先进的短-CoT推理结果——例如，在AIME上达到60.8，在MATH500上达到94.6，在LiveCodeBench上达到47.3，大幅超越现有的短-CoT模型，如GPT-4o和Claude Sonnet 3.5（高达+550%）。|\n",
        "2501.14275": "|**2025-01-24**|**Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation**|Sadegh Mahdavi et.al.|[2501.14275](http://arxiv.org/abs/2501.14275)|**[link](https://github.com/dsl-lab/aops)**|**大型语言模型（LLMs）的进展激起了人们对它们解决奥林匹克级数学问题能力的研究兴趣。然而，这些模型的训练和评估受到可用数据集大小和质量有限的限制，因为为这类高级问题创建大规模数据需要大量来自人类专家的努力。此外，当前的基准测试容易受到污染，导致评估结果不可靠。在本文中，我们提出了一种自动化流程，该流程利用了《问题解决艺术》（AoPS）论坛丰富的资源，该论坛主要展示奥林匹克级问题和社区驱动的解决方案。使用开源LLMs，我们开发了一种从论坛中提取问答对的方法，从而产生了AoPS-Instruct，一个包含超过60万个高质量问答对的数据库。我们的实验表明，在AoPS-Instruct上微调LLMs可以提升它们在各种基准测试中的推理能力。此外，我们构建了一个自动流程，引入了LiveAoPSBench，这是一个基于最新论坛数据的具有时间戳的演变评估集，为评估LLM性能提供了一个抗污染的基准。值得注意的是，我们发现LLM性能随着时间的推移显著下降，这表明它们在旧例题上的成功可能源于预训练的暴露，而不是真正的推理能力。我们的工作提出了一种可扩展的方法来创建和维护用于高级数学推理的大规模、高质量数据集，为LLMs在这一领域的功能和局限性提供了宝贵的见解。我们的基准和代码可在https://github.com/DSL-Lab/aops上获取。**|\n",
        "2501.15749": "|**2025-01-27**|**LLM-powered Multi-agent Framework for Goal-oriented Learning in Intelligent Tutoring System**|Tianfu Wang et.al.|[2501.15749](http://arxiv.org/abs/2501.15749)|**[link](https://github.com/geminilight/gen-mentor)**|**智能辅导系统（ITSs）通过提供个性化的学习体验，颠覆了教育领域。然而，随着以目标为导向的学习在专业环境中变得越来越重要，强调高效实现特定目标的这种学习方法，现有的ITSs往往难以提供这种针对性的学习体验。在本文中，我们提出了GenMentor，这是一个由大型语言模型（LLM）驱动的多智能体框架，旨在在ITS中提供目标导向的个性化学习。GenMentor首先使用在自定义目标-技能数据集上微调的LLM，准确地将学习者的目标映射到所需的技能。在确定技能差距后，它利用一个综合且动态的学习者多方面状态配置文件驱动的演变优化方法，安排一个高效的学习路径。此外，GenMentor通过探索-草拟-整合机制定制学习内容，以符合个别学习者的需求。广泛的自动和人工评估证明了GenMentor在学习指导和内容质量方面的有效性。此外，我们已经在实践中部署了它，并将其实现为一个应用程序。与专业学习者的实际人类研究进一步突出了它在目标对齐和资源定位方面的有效性，从而提高了个性化水平。补充资源可在https://github.com/GeminiLight/gen-mentor获取。**|\n",
        "2501.15108": "|**2025-01-25**|**Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training**|Xunxin Cai et.al.|[2501.15108](http://arxiv.org/abs/2501.15108)|null|大型语言模型（LLMs）在生物医学领域的快速发展突显了它们潜力与现有开源标注文本数据集规模有限、质量通常较低之间的差距。此外，生物医学知识体系的固有复杂性极大地阻碍了弥合这一差距的努力。LLMs自身能否在克服这一限制中发挥关键作用？受此启发，我们在本研究中调查了这一挑战。我们提出了一种框架，该框架能够从广泛的科学文献中自动提取高质量的文本训练数据。我们的方法通过生物医学知识体系（通过医学主题词表MeSH）自我评估并生成与生物医学领域更紧密相关的问题。这个综合框架建立了一个自动化的工作流程，从而消除了手动干预的需要。此外，我们进行了全面的实验，以评估我们框架生成的数据对大小不同的下游语言模型的影响。与生命科学领域的预训练模型以及由GPT-4等强大闭源模型代表的高效模型相比，我们的方法在问答任务上显著提高了性能。值得注意的是，生成的AI-Ready数据集使得Llama3-70B基础模型在MedPrompt的帮助下，使用多倍参数数量超越了GPT-4。详细的案例研究和消融实验突出了我们框架中每个组件的重要性。|\n",
        "2501.17116": "|**2025-01-28**|**Optimizing Large Language Model Training Using FP4 Quantization**|Ruizhe Wang et.al.|[2501.17116](http://arxiv.org/abs/2501.17116)|null|随着大型语言模型（LLMs）训练计算需求的不断增长，需要更高效的方法。量化训练通过允许低比特算术运算来降低这些成本，是一种有希望的解决方案。虽然FP8精度已经证明了可行性，但由于量化误差显著和表示能力有限，利用FP4仍然是一个挑战。这项工作引入了第一个针对LLMs的FP4训练框架，通过两个关键创新解决了这些挑战：一个用于精确权重更新的可微分量化估计器和一个异常值钳位和补偿策略，以防止激活崩溃。为了确保稳定性，该框架集成了混合精度训练方案和向量量化。实验结果表明，我们的FP4框架在准确度上与BF16和FP8相当，最小化退化，并有效扩展到在多达100B个标记上训练的13B参数LLMs。随着支持FP4的下一代硬件的出现，我们的框架为高效的超低精度训练奠定了基础。|\n",
        "2501.16588": "|**2025-01-28**|**Fine-Tuned Language Models as Space Systems Controllers**|Enrico M. Zucchelli et.al.|[2501.16588](http://arxiv.org/abs/2501.16588)|null|大型语言模型（LLMs）或基础模型（FMs）是经过预训练的能够连贯地自动回归补全句子的变换器。在本文中，我们表明LLMs在经过一些额外的训练，即微调后，可以控制简化的空间系统。我们考察了相对较小的语言模型，其参数量在70亿到130亿之间。我们聚焦于四个问题：三维弹簧玩具问题、低推力轨道转移、低推力地月控制以及有动力下降导航。经过微调的LLMs能够通过生成足够精确的输出控制系统，这些输出是多维向量，精度高达10位有效数字。我们显示，对于几个问题，进行微调所需的数据量小于传统深度神经网络（DNNs）通常所需的数据量，并且微调后的LLMs擅长在训练数据集之外的泛化。此外，同一LLM可以使用来自不同问题的数据进行微调，与为单一应用训练的LLMs相比，性能下降非常轻微。这项工作旨在成为开发通用空间系统控制器的第一步。|\n",
        "2501.18914": "|**2025-01-31**|**Scaling Laws for Differentially Private Language Models**|Ryan McKenna et.al.|[2501.18914](http://arxiv.org/abs/2501.18914)|null|随着大型语言模型（LLM）训练中规模定律的重要性日益凸显，因为它们可以通过规模预测性能提升，并为重要超参数选择提供指导，这些选择在其他情况下可能代价高昂。LLM还依赖于来自（有时敏感的）用户数据的大型、高质量训练数据集。在敏感用户数据上训练模型需要仔细的隐私保护，如差分隐私（DP）。然而，DP训练的动态与传统的训练方式显著不同，因此其规模定律尚未完全理解。在本研究中，我们建立了能够准确模拟DP LLM训练复杂性的规模定律，为许多场景提供了计算-隐私-效用权衡的完整图景以及最优训练配置。|\n",
        "2501.18845": "|**2025-01-31**|**Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities**|Yaping Chai et.al.|[2501.18845](http://arxiv.org/abs/2501.18845)|null|随着预训练语言模型规模和复杂性的增加，它们在许多应用中展现出了卓越的性能，但它们通常需要大量的训练数据集才能得到充分的训练。训练数据集不足可能会意外地导致模型过拟合，并无法应对复杂任务。在大规模语料库上训练的大型语言模型（LLMs）具有突出的文本生成能力，这提高了数据和数量的质量，并在数据增强中发挥着至关重要的作用。具体来说，在个性化任务中给出了独特的提示模板，以引导LLMs生成所需的内容。最近，基于检索的令人鼓舞的技术通过引入外部知识来提高LLMs在数据增强中的表达能力，使它们能够生成更多基于事实的数据。本调查对LLMs中的数据增强进行了深入分析，将技术分为简单增强、基于提示的增强、基于检索的增强和混合增强。我们总结了数据增强中的后处理方法，这极大地促进了增强数据的精炼，并使模型能够过滤掉不真实的内容。然后，我们提供了常见的任务和评估指标。最后，我们介绍了现有挑战和未来机遇，这些机遇有望进一步改善数据增强。|\n",
        "2502.02508": "|**2025-02-04**|**Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search**|Maohao Shen et.al.|[2502.02508](http://arxiv.org/abs/2502.02508)|null|大型语言模型（LLMs）在各个领域展现了惊人的推理能力。近期研究表明，增加测试时的计算量可以增强LLMs的推理能力。这通常涉及到在推理时由外部LLM验证器引导的广泛采样，从而形成一个两人系统。尽管有外部指导，但该系统的有效性展示了单个LLM处理复杂任务的潜力。因此，我们提出了一个新的研究问题：我们能否将搜索能力内化，从根本上增强单个LLMs的推理能力？这项工作探索了一个新的方向，专注于在训练后进行自回归搜索的LLMs（即带有自我反思和探索新策略的扩展推理过程）。为了实现这一点，我们提出了动作-思维链（COAT）推理和两阶段训练范式：1）一个小规模的格式调整阶段，以内部化COAT推理格式；2）一个大规模的自我提升阶段，利用强化学习。我们的方法产生了Satori，这是一个基于开源模型和数据训练的70亿参数LLMs。大量的实证评估表明，Satori在数学推理基准测试中达到了最先进的性能，同时表现出强大的跨领域泛化能力。代码、数据和模型将全部开源。|\n",
        "2502.02067": "|**2025-02-04**|**AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement**|Shivam Singh et.al.|[2502.02067](http://arxiv.org/abs/2502.02067)|**[link](https://github.com/sssshivvvv/adaptbot)**|**具有身体的人类助手通常被要求在新的场景中完成新的任务。一个根据已知食谱在厨房准备特定菜肴的代理可能被要求准备新的菜肴或在储藏室执行清洁任务。可能没有足够的资源，例如时间或标记的示例，来训练代理适应这些新情况。在许多领域受过大量知识训练的大型语言模型（LLMs）能够预测此类新任务和场景的抽象动作序列，尽管由于任务、代理或领域特定的约束，代理可能无法执行此动作序列。我们的框架通过利用LLM提供的通用预测以及编码在知识图谱（KG）中的先验领域特定知识来解决这些挑战，使代理能够快速适应新的任务和场景。机器人还需要根据需要征求和利用人类输入来完善其现有知识。基于在模拟域中的烹饪和清洁任务的实验评估，我们证明了LLM、KG和人类输入之间的相互作用与仅使用LLM输出相比，导致显著的性能提升。**|\n",
        "2502.01586": "|**2025-02-03**|**SubTrack your Grad: Gradient Subspace Tracking for Memory and Time Efficient Full-Parameter LLM Training**|Sahar Rajabi et.al.|[2502.01586](http://arxiv.org/abs/2502.01586)|null|由于大型语言模型（LLMs）的模型规模和优化器状态庞大，训练LLMs需要大量的时间和计算资源。为了克服这些挑战，最近的方法，如BAdam，通过部分权重更新来提高时间和内存效率，尽管有时会牺牲性能。其他方法，如GaLore，则专注于在优化内存使用的同时保持性能，但可能需要更高的时间复杂度。通过利用梯度的低秩结构和Grassmannian几何，我们提出了SubTrack-Grad，这是一种基于子空间跟踪的优化方法，通过结合估计误差和先前识别的子空间，有效地跟踪不断演变的梯度子空间。与GaLore相比，SubTrack-Grad提供了更好或相当的结果，而显著优于BAdam，尽管BAdam在时间效率上有所妥协。在GLUE任务上，SubTrack-Grad将墙时减少了高达20.57%（平均减少15%），在SuperGLUE任务上减少了高达65%（平均减少22%）。值得注意的是，对于3B参数模型，与全秩训练相比，GaLore的墙时增加了157%，而SubTrack-Grad仅增加了31%，这代表了49%的墙时减少，同时享受与GaLore相同的内存减少。|\n",
        "2502.00894": "|**2025-02-02**|**MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for Efficient LLM Training Across Morphologies**|Ehsaneddin Asgari et.al.|[2502.00894](http://arxiv.org/abs/2502.00894)|null|分词是自然语言处理（NLP）的基础，直接影响模型效率和语言忠实度。尽管字节对编码（BPE）在大型语言模型（LLMs）中得到广泛应用，但它往往忽视了词素边界，导致子词分词不理想，尤其是在形态丰富的语言中。我们引入了形态BPE，这是一种BPE的形态感知扩展，它将语言学结构整合到子词分词中，同时保持统计效率。此外，我们还提出了两个基于形态的评价指标：（i）形态一致性F1分数，它量化了词素共享与分词共享之间的一致性，有助于LLM训练收敛；（ii）形态编辑距离，它衡量了词素与分词在可解释性方面的对齐。在英语、俄语、匈牙利语和阿拉伯语上进行的实验，涵盖300M和1B参数的LLMs，表明形态BPE可以持续降低交叉熵损失，加速收敛并提高形态对齐分数。形态BPE与现有的LLM流程完全兼容，集成时需要的修改最少。形态BPE代码库和分词游乐场将在以下网址提供：https://github.com/llm-lab-org/MorphBPE 和 https://tokenizer.llm-lab.org|\n",
        "2502.00340": "|**2025-02-01**|**Enhancing Token Filtering Efficiency in Large Language Model Training with Collider**|Di Chai et.al.|[2502.00340](http://arxiv.org/abs/2502.00340)|null|本文提出了一种名为Collider的系统，旨在充分发挥在大语言模型（LLM）训练中token过滤的效率。该系统通过在所有层中过滤掉不重要的token的激活来维持稀疏性。此外，它还具备一个自动工作流程，将稀疏的GEMM（通用矩阵乘法）转换为降维的密集GEMM，以优化效率。在TinyLlama-1.1B、Qwen2.5-1.5B和Phi1.5-1.4B这三种LLM上的评估表明，当过滤掉40%的token时，Collider可以将反向传播时间减少高达35.1%，端到端训练时间减少高达22.0%。在TinyLlama在150亿token上的训练中进行的效用评估表明，与常规训练相比，Collider通过相对提高模型效用16.3%来维持token过滤的效用提升，并使用8个GPU将训练时间从4.7天减少到3.5天。Collider被设计为易于集成到现有的LLM训练框架中，使得已经使用token过滤的系统只需一行代码即可加速训练。|\n",
        "2502.02810": "|**2025-02-05**|**Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization**|Chanhui Lee et.al.|[2502.02810](http://arxiv.org/abs/2502.02810)|null|近年来，大型语言模型（LLMs）的进步推动了通用LLMs在分子任务中的应用。尽管一些研究表明微调后的LLMs可以在基准测试中取得令人印象深刻的性能，但由于缺乏对分子结构的根本理解，它们与真正的通用分子LLMs还有很大差距。具体来说，当给定分子任务指令时，使用朴素下一个标记预测训练的LLMs会将相似的可能性分数分配给原始分子和负向干扰分子，这揭示了它们缺乏对分子结构的理解，这对于可靠和通用的分子LLMs至关重要。为了克服这一限制并获得真正的通用分子LLMs，我们引入了一种基于彻底的多模态指令调整以及选择和拒绝图之间的分子结构偏好优化的新型多模态训练方法。在多个分子基准测试中，所提出的通用分子LLMs，称为Mol-LLM，在大多数任务上取得了通用LLMs中最先进的性能，同时，其性能超过或与最先进的专家LLMs相当。此外，Mol-LLM在反应预测任务中也表现出优异的泛化性能，证明了分子结构理解对泛化视角的影响。|\n",
        "2502.03885": "|**2025-02-06**|**InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers**|Chenchen Shou et.al.|[2502.03885](http://arxiv.org/abs/2502.03885)|null|大规模语言模型（LLM）的训练依赖于多维并行性，其中高带宽域（HBDs）对于通信密集型并行性，如张量并行（TP）和专家并行（EP）至关重要。然而，现有的HBD架构在可扩展性、成本和容错性方面存在根本性的限制：以交换机为中心的HBD（例如，NVL-72）会带来高昂的扩展成本，而以GPU为中心的HBD（例如，TPUv3/Dojo）则容易发生故障传播。TPUv4等交换机-GPU混合HBD采用折中方案，通过利用光电路交换（OCS），但在立方级（例如，64个TPU）的故障爆炸半径仍然很大。我们提出了InfinitePOD，这是一种新型的以收发器为中心的HBD架构，它使用光电路交换（OCS）在收发器级别统一连接性和动态交换。通过在每个收发器中嵌入OCS，InfinitePOD实现了可重构的点对多点连接，允许拓扑适应可变大小的环。此设计提供了：i）数据中心级可扩展性且成本不会激增；ii）通过隔离单个节点的故障来实现容错性；iii）对于无故障的GPU实现全带宽利用率。关键创新包括基于硅光子（SiPh）的低成本OCS收发器（OCSTrx）、与节点内/节点间通信协同设计的可重构k跳环拓扑，以及最大化GPU利用率同时最小化跨ToR数据中心网络流量的HBD-DCN编排算法。评估表明，InfinitePOD实现了NVL-72成本的31%，GPU浪费率接近零（比NVL-72和TPUv4低一个数量级以上），当节点故障比率低于7%时，跨ToR流量接近零，并且与NVIDIA DGX（每个节点8个GPU）相比，模型FLOPs利用率提高了3.37倍。|\n",
        "2502.06742": "|**2025-02-10**|**Gradient Multi-Normalization for Stateless and Scalable LLM Training**|Meyer Scetbon et.al.|[2502.06742](http://arxiv.org/abs/2502.06742)|null|通常，训练大型语言模型（LLMs）依赖于如Adam（Kingma & Ba，2015）这样的自适应优化器，这些优化器存储额外的状态信息以加速收敛，但会带来显著的内存开销。最近的研究，如SWAN（Ma等，2024）通过消除对优化器状态的需求，并通过对瞬时梯度应用多步预处理程序来实现与Adam相当的性能来解决这个问题。受SWAN成功的启发，我们提出了一种新的框架，用于设计根据多个范数对随机梯度进行归一化的无状态优化器。为了实现这一点，我们提出了一种简单的交替方案来强制执行相对于这些范数的梯度归一化。我们表明，我们的过程可以产生，直到任意精度，问题的不动点，并且SWAN是我们方法的一个特定实例，具有精心选择的范数，从而更深入地理解了其设计。然而，SWAN的计算成本高昂的漂白/正交化步骤限制了其在大型LLMs中的实用性。利用我们的原则性视角，我们开发了一种更高效、可扩展且实用的无状态优化器。我们的算法放宽了SWAN的性质，显著降低了其计算成本，同时保持了其内存效率，使其适用于训练大规模模型。在用至10亿参数的预训练LLaMA模型上的实验表明，与Adam相比，速度提高了3倍，同时内存需求显著降低，优于其他内存高效基准。|\n",
        "2502.05967": "|**2025-02-09**|**$μ$nit Scaling: Simple and Scalable FP8 LLM Training**|Saaketh Narayan et.al.|[2502.05967](http://arxiv.org/abs/2502.05967)|null|使用8位浮点数（FP8）格式进行大型语言模型训练有望显著提高效率，但降低的数值精度使得训练变得具有挑战性。目前，只有在愿意调整各种超参数、减少模型规模或接受计算动态缩放因子开销的情况下，才能够在FP8中进行训练。我们展示了简单、可扩展的FP8训练方法，该方法不需要动态缩放因子或特殊超参数，即使在大型模型规模下也是如此。我们的方法，即$\\mu$nit Scaling（$\\mu$S），还能够实现模型宽度间简单的超参数迁移、训练和推理中的数值匹配以及其他理想特性。$\\mu$nit Scaling的实现简单，基于对常见transformer操作的原理分析的一系列最小干预措施。我们通过训练从10亿到130亿参数的模型来验证我们的方法，所有隐藏线性层的计算都在FP8中进行。我们在质量上达到了比更高精度基线相等的水平，同时训练速度提高了高达33%。|\n",
        "2502.05413": "|**2025-02-08**|**XPUTimer: Anomaly Diagnostics for Divergent LLM Training in GPU Clusters of Thousand-Plus Scale**|Weihao Cui et.al.|[2502.05413](http://arxiv.org/abs/2502.05413)|null|随着大型语言模型的快速普及，对高效GPU训练集群的需求日益增长。然而，由于软件硬件交互的复杂性和训练异常的频繁发生，确保这些集群的高性能训练具有挑战性。由于现有的诊断工具针对特定问题定制化，它们在处理整个训练栈的异常方面存在不足。为此，我们引入了XPUTimer，这是一个为大规模分布式LLM训练设计的实时诊断框架。XPUTimer首先集成一个轻量级的跟踪守护程序，以最小的开销监控关键代码段。此外，它还具备一个诊断引擎，该引擎采用新颖的内核内跟踪和整体聚合指标，以高效地识别和解决异常。XPUTimer在八个月内部署到6000个GPU上，在训练栈的各个方面都取得了显著的改进，验证了其在实际场景中的有效性。|\n",
        "2502.05331": "|**2025-02-07**|**Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through Books**|Sangmitra Madhusudan et.al.|[2502.05331](http://arxiv.org/abs/2502.05331)|null|书籍虽然往往富含文化洞察，但也可能反映了它们时代的偏见——这些偏见在大型语言模型（LLMs）的训练过程中可能会被学习和延续。我们提出了一种新的方法，使用微调的LLMs来追踪和量化这些偏见。我们开发了BookPAGE语料库，该语料库包含七个十年（1950-2019）的593部虚构书籍，以追踪偏见的发展。通过在每个十年对书籍进行微调LLMs，并使用有针对性的提示，我们考察了与性别、性取向、种族和宗教相关的偏见的变化。我们的研究结果表明，在特定十年的书籍上训练的LLMs表现出了反映其时代的偏见，既有渐进趋势也有显著变化。例如，从20世纪50年代到2010年代，模型对女性领导角色的描绘呈渐进增加（从8%到22%），在1990年代有显著上升（从4%到12%），可能与第三波女权主义相符。从1980年代到2000年代，同性关系提及显著增加（从0%到10%），反映了LGBTQ+的日益可见。令人担忧的是，伊斯兰教的负面描绘在2000年代急剧上升（从26%到38%），可能反映了9/11事件后的情绪。重要的是，我们证明了这些偏见主要源于书籍的内容，而不是模型的架构或初始训练。我们的研究通过连接人工智能、文学研究和社会科学研究，为社会偏见趋势提供了新的视角。|\n",
        "2502.05233": "|**2025-02-07**|**Efficient Knowledge Feeding to Language Models: A Novel Integrated Encoder-Decoder Architecture**|S Santosh Kumar et.al.|[2502.05233](http://arxiv.org/abs/2502.05233)|null|本文介绍了一种新颖的方法，通过在统一框架内整合检索和生成过程，有效地将知识喂给语言模型（LLMs）进行预测。虽然检索增强生成（RAG）模型解决了LLMs训练数据中的差距和知识限制问题，但它受到令牌限制和检索系统准确度依赖的限制。我们提出的架构通过整合上下文向量（ICV）来克服这些挑战。ICV通过使用LLMs的潜在嵌入来创建一个向量，该向量捕捉了任务的关键信息，从而重新诠释了上下文学习。然后，这个向量被用来转换LLMs的潜在状态，增强了生成过程，而无需在提示中添加示例。ICV直接将信息整合到模型中，使模型能够更有效地处理这些信息。我们广泛的实验评估表明，ICV在问答、信息检索和其他任务中优于标准的上下文学习和微调。这种方法减轻了当前RAG模型的局限性，并为处理大量和多样化的数据集提供了一个更稳健的解决方案。尽管我们的ICV增强模型只利用了一部分参数，但它与LLaMA-3、Gemma和Phi-3等模型相比，实现了具有竞争力的性能，显著降低了计算成本和内存需求。ICV减少了提示长度，易于控制，超越了令牌限制，与微调相比，计算效率更高。|\n",
        "2502.08145": "|**2025-02-12**|**Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers**|Siddharth Singh et.al.|[2502.08145](http://arxiv.org/abs/2502.08145)|null|训练和微调具有数百亿至数千亿参数的大型语言模型（LLMs）需要数万个GPU以及高度可扩展的软件堆栈。在这项工作中，我们提出了一种新颖的四维混合并行算法，该算法在一个高度可扩展、便携且开源的框架AxoNN中实现。我们描述了AxoNN中的多个性能优化，以提高矩阵乘法内核性能、重叠非阻塞集体操作与计算，以及性能建模以选择性能最优的配置。这些优化使得在Perlmutter（620.1 Petaflop/s）、Frontier（1.381 Exaflop/s）和Alps（1.423 Exaflop/s）上训练GPT风格的变压器模型时，实现了前所未有的扩展性和峰值浮点运算速度（bf16）。虽然LLMs的能力随着可训练参数数量的增加而提高，但由训练数据记忆带来的隐私和版权风险也随之增加，这可能导致推理时敏感或私人信息的泄露。我们通过实验强调了这一规模效应的副作用，实验探讨了“灾难性记忆”现象，其中模型足够大，可以在单次遍历中记住训练数据，并提出了防止这一现象的方法。作为这项研究的一部分，我们展示了在Frontier上使用AxoNN微调一个4050亿参数的LLM的过程。|\n",
        "2502.09209": "|**2025-02-13**|**On LLM-generated Logic Programs and their Inference Execution Methods**|Paul Tarau et.al.|[2502.09209](http://arxiv.org/abs/2502.09209)|null|大型语言模型（LLMs）在PB级数据上训练，是迄今为止积累和提炼的大量知识的极大压缩库。本文研究了将这种知识以几种逻辑程序类别的形式提取出来的技术，包括命题Horn子句、双向Horn子句、关系三元组和确定子句文法。将这种知识以逻辑程序的形式公开，可以启用可靠的推理方法，以验证LLM输出的对齐性及其预期用途，并扩展其推理能力。我们研究了生成程序的新的执行方法，包括对从向量数据库中存储的LLM生成内容进行的可归纳事实的软统一，以及支持使用大型LLM生成程序的推理的最小模型计算的GPU加速。|\n"
    }
}