{
    "agent": {
        "2411.18266": "|**2024-11-27**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴无声语音系统在恢复言语障碍患者的沟通方面具有巨大潜力。然而，无缝、连贯的语音仍然难以实现，临床疗效尚未得到证实。在此，我们提出了一种由人工智能驱动的智能喉部（IT）系统，该系统集成了喉部肌肉振动和颈动脉脉搏信号传感器以及大型语言模型（LLM）处理，以实现流畅、富有情感表达的沟通。该系统利用超灵敏的纺织应变传感器从颈部区域捕捉高质量信号，并支持token级别的处理，以实现实时、连续的语音解码，从而实现无缝、无延迟的通信。在测试中，使用五种言语障碍的卒中患者进行测试，IT的LLM智能代理智能地纠正token错误，丰富句子级情感和逻辑连贯性，实现了低错误率（4.2%的词错误率，2.9%的句子错误率）和用户满意度55%的增长。这项工作为言语障碍患者建立了一个便携、直观的通信平台，有望广泛应用于不同的神经学条件和多语言支持系统。|\n",
        "2411.17636": "|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在各种领域，包括机器人操作和导航，展现了出色的规划能力。虽然最近在机器人领域的努力已经利用LLMs进行高级和低级规划，但这些方法通常面临重大挑战，如长期任务中的幻觉以及由于在单次生成计划时缺乏实时反馈而导致的适应性有限。为了解决这些限制，我们提出了一种新的多智能体LLM框架，即用于操作的智能体大型语言模型（MALMM），它将高级规划和低级控制代码生成分配给专门的LLM智能体，并由一个额外的智能体动态管理转换。通过在每一步后纳入环境观察，我们的框架有效地处理了中间失败，并实现了适应性重新规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或在上下文中学习的示例，并且可以推广到各种新的任务。我们在包括长期任务在内的九个RLBench任务上评估了我们的方法，并展示了其在零样本设置下解决机器人操作的能力，从而克服了现有基于LLM的操作方法的局限性。|\n",
        "2411.16031": "|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理建模（ABM）已成为模拟社交网络的重要工具，涵盖了诸如信息传播、影响力动态和社区形成等多种现象。然而，手动配置各种代理交互和信息流动态带来挑战，往往导致模型过于简化，缺乏现实世界的普适性。将现代大型语言模型（LLM）与ABM相结合为解决这些挑战和提升模拟真实度提供了一条有希望的途径，利用LLM在感知、推理和行为方面类似人类的能力。在本文中，我们提出了一种新颖的框架，该框架利用LLM赋能的代理根据用户的兴趣和个性特征模拟社交网络用户。该框架允许定制代理交互，类似于各种社交网络平台，包括内容重新分享和个性化推荐机制。我们使用2020年美国选举的全面Twitter数据集验证了我们的框架，证明LLM代理能够准确复制真实用户的言行，包括语言模式和政治倾向。这些代理形成了同质化的意识形态集群，并保留了其社区的主要主题。值得注意的是，基于偏好的推荐对代理行为有显著影响，促进了更高的参与度、网络同质性以及回音室的形成。总体而言，我们的发现突出了LLM代理在推进社交媒体模拟和揭示复杂在线动态中的潜力。|\n",
        "2411.15891": "|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大型语言模型（LLMs）和强化学习（RL）是构建自主智能体的两种强大方法。然而，由于对游戏环境的理解有限，智能体往往依赖低效的探索和试错，难以发展长期策略或做出决策。我们提出了一种方法，通过从交互记录中提取经验来模拟游戏环境的潜在规律，并利用这些经验作为内部动机来引导智能体。这些经验以语言形式表达，非常灵活，既可以直接协助智能体进行推理，也可以转化为训练中的奖励。在Crafter上的评估结果显示，RL和LLM智能体都从这些经验中受益，从而提高了整体性能。|\n",
        "2411.16723": "|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着自然语言生成模型（称为大型语言模型，LLMs）的近期发展，一种潜在的应用场景得以开启，即改进人类与机器人助手互动的方式。这些LLMs应能利用其广泛的理解能力，将自然语言命令解释为有效、符合任务和安全的机器人任务执行。然而，在现实中，这些模型存在幻觉问题，可能会引起安全问题或偏离任务。在其他领域，这些问题已通过使用协作人工智能系统得到改善，在该系统中，多个LLM代理可以共同规划、编码和自我检查输出。在本研究中，通过将多个协作人工智能系统与单个独立人工智能代理进行对比测试，以确定在其他领域的成功是否能够转化为改进的人机交互性能。结果显示，代理数量与模型的成功率之间没有明确的趋势。然而，很明显，某些协作人工智能代理架构可以显著提高生成无错误代码和解决抽象问题的能力。|\n",
        "2411.15396": "|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|人工智能在进行自动化信息判断任务时是否会产生认知偏差？尽管近期在衡量和缓解人工智能和大型语言模型（LLMs）中的社会和算法偏差方面取得了进展，但LLMs在多大程度上表现出“理性”行为，或者它们是否也容易受到人类认知偏差的触发，仍不明确。为了解决这个未解问题，我们的研究包括一个众包用户实验和一个LLM驱动的模拟实验，比较了在信息检索（IR）环境中，LLMs和人类评判员在潜在诱饵效应下的可信度评估，并实证研究了与传统的基于人类评估的基线相比，LLMs在COVID-19医疗（误）信息评估任务中的认知偏差程度。来自跨主体用户实验和LLM驱动的重复实验的结果表明：1）更大、更新版的LLMs在区分可信信息和虚假信息方面表现出更高的一致性和准确性。然而，由于存在更显著、更具诱饵性质的虚假信息结果，它们更有可能给予虚假信息更高的评分；2）虽然诱饵效应在人类和LLMs的评估中都发生了，但在LLMs的判断中，这种效应在不同条件和主题下比人类的可信度评分更为普遍。与普遍认为的AI工具的“理性”相反，我们的研究从实证上确认了LLM代理中嵌入的认知偏差风险，评估了诱饵对LLMs与人类可信度评估的影响，从而突出了去偏差AI代理、开发心理学导向的AI审计技术和政策（用于自动化判断任务及更多领域）的复杂性和重要性。|\n",
        "2411.15100": "|**2024-11-22**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|LLM代理的应用正变得越来越复杂和多样化，导致对可以解析为代码、结构化函数调用和具身代理命令的规范化输出的需求日益增长。这些发展对LLM推理中的规范化生成提出了重大需求。无上下文文法是一种灵活的方法，通过限制解码来实现规范化生成。然而，执行无上下文文法需要在运行时遍历词汇表中所有标记的多个栈状态，给规范化生成带来不可忽视的开销。在本文中，我们提出了XGrammar，这是一个灵活且高效的LLM结构生成引擎。XGrammar通过将词汇表划分为可预检查的上下文无关标记和需要运行时解释的上下文相关标记来加速无上下文文法的执行。我们进一步构建了转换来扩展语法上下文并减少上下文无关标记的数量。此外，我们构建了一个高效的持久栈来加速上下文相关标记的检查。最后，我们与LLM推理引擎协同设计语法引擎，以重叠语法计算与GPU执行。评估结果显示，XGrammar可以比现有解决方案实现高达100倍的加速。结合LLM推理引擎，它可以在端到端低LLM服务中实现近乎零开销的结构化生成。|\n",
        "2411.15004": "|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|**[link](https://github.com/colonylabs/ScribeAgent)**|大型语言模型（LLM）代理正在迅速提升以处理越来越复杂的基于网络的任务。这些代理中的大多数依赖于通用、专有的模型如GPT-4，并专注于设计更好的提示以提升它们的规划能力。然而，通用LLM并未专门训练以理解专门的网络上下文，如HTML，并且它们通常在长期规划方面遇到困难。我们探索了一种替代方法，即使用从超过250个域名收集的生产规模工作流程数据对开源LLM进行微调，这些域名对应60亿个标记。这种方法简单而有效，在现有基准测试中相对于基于提示的代理显示了显著的优势——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上比之前最佳的文字型网络代理提高了14.1%的任务成功率。我们还对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练配方、上下文窗口优化以及数据集大小影响等方面的见解。|\n",
        "2411.14214": "|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于LLM的自主代理在解决复杂工业任务方面表现出卓越的性能。然而，在追求碳中和和高性能可再生能源系统的过程中，现有的AI辅助设计自动化在可解释性、可扩展性和可用性方面面临着重大限制。为了解决这些挑战，我们提出了LP-COMDA，这是一个基于LLM的、物理信息丰富的自主代理，能够在最小人工监督下自动化电力电子系统中电力转换器的调制设计。与传统的AI辅助方法不同，LP-COMDA包含一个基于LLM的规划器，通过用户友好的聊天界面收集和验证设计规范。规划器随后与物理信息设计优化工具协调，自主迭代生成和优化调制设计。通过聊天界面，LP-COMDA提供可解释的设计过程，展示解释和图表。实验表明，LP-COMDA优于所有基线方法，在标准均方绝对误差方面，与第二好的基准方法相比，误差降低了63.2%。此外，对20位专家的实证研究表明，使用LP-COMDA的设计时间是传统方法的33倍以上，显示出其在设计效率方面的显著改进。|\n",
        "2411.14033": "|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在（多模态）大型语言模型时代，大多数操作流程都可以通过LLM智能体进行重构和再现。LLM智能体能够感知、控制和从环境中获取反馈，以自主方式完成给定任务。除了环境交互特性外，LLM智能体还可以调用各种外部工具以简化任务完成过程。这些工具可以被视为包含私有或实时知识且不存在于LLM参数中的预定义操作流程。作为发展的自然趋势，调用工具的智能体正成为自主智能体，因此完整的智能系统最终变成了多LLM智能体系统（MLAS）。本文讨论了MLAS的技术和商业格局。与之前的单一LLM智能体系统相比，MLAS具有以下优势：i) 更高的任务解决性能潜力；ii) 更高的系统变化灵活性；iii) 为每个参与实体保留专有数据；iv) 为每个实体实现货币化的可行性。为了支持MLAS生态系统，我们提供了一个考虑技术要求、数据隐私和商业激励的MLAS协议的初步版本。因此，MLAS将成为实现未来人工集体智慧的实用解决方案。|\n",
        "2411.19043": "|**2024-11-28**|**Using a Feedback Loop for LLM-based Infrastructure as Code Generation**|Mayur Amarnath Palavalli et.al.|[2411.19043](http://arxiv.org/abs/2411.19043)|**[link](https://github.com/Mayur-Palavalli/LLM-IaC-generation)**|**使用大型语言模型（LLMs）进行代码生成有助于提高软件开发者在编码任务中的生产力，但尚未对围绕这些代码的软件开发者的任务产生重大影响。特别是，基础设施管理的挑战仍然是一个未解之谜。我们研究了LLM代理利用基础设施即代码（IaC）范式构建基础设施的能力。我们特别研究了使用反馈循环，该循环会返回生成的IaC的错误和警告，以允许LLM代理改进代码。我们发现，对于循环的每一次迭代，其有效性都会呈指数下降，直到达到某个点并趋于平稳，最终变得无效。**|\n",
        "2411.18915": "|**2024-11-28**|**MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications**|Vishnou Vinayagame et.al.|[2411.18915](http://arxiv.org/abs/2411.18915)|null|随着工具增强的语言代理的数学推理能力不断增强，但现有方法通常依赖于闭源或大型模型、外部数据或大量的提示工程。这项工作介绍了一种名为MATATA的新颖且经济高效的方法，通过推理、规划和工具使用来训练LLM代理解决表格数据问题。它采用渐进式自我改进范式和迭代式弱监督，赋予了38亿/80亿小语言模型（SLM）的能力，特别适合于本地托管和敏感的商业环境，在这些环境中数据隐私至关重要。通过在不同数据集上使用灵活且可重用的工具，它实现了在共享任务上的有效可扩展性。实验表明，MATATA在基于开源模型的推理框架中，在FinQA和TAT-QA上达到了最先进的性能。此外，MATATA模型在TabMWP上与基于GPT-4的框架竞争，同时仍然是SLM。|\n",
        "2412.01778": "|**2024-12-02**|**HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing**|Lajos Muzsai et.al.|[2412.01778](http://arxiv.org/abs/2412.01778)|**[link](https://github.com/aielte-research/HackSynth)**|**我们介绍了HackSynth，这是一种基于大型语言模型（LLM）的全新自主渗透测试代理。HackSynth的双模块架构包括一个规划器和总结器，这使得它能够迭代地生成命令和处理反馈。为了对HackSynth进行基准测试，我们提出了两个基于Capture The Flag（CTF）的新基准集，利用流行的平台PicoCTF和OverTheWire。这些基准集涵盖了200个不同领域和难度的挑战，为评估基于LLM的渗透测试代理提供了一个标准化的框架。基于这些基准，我们展示了广泛的实验，分析了HackSynth的核心参数，包括创新性（温度和top-p）和标记利用。我们使用了多个开源和专有LLM来衡量代理的能力。实验表明，该代理在GPT-4o模型下表现最佳，优于GPT-4o的系统卡片所建议的。我们还讨论了HackSynth行动的安全性和可预测性。我们的发现表明，基于LLM的代理在推进自主渗透测试方面的潜力，以及稳健保障的重要性。HackSynth和基准集公开可用，以促进自主网络安全解决方案的研究。**|\n",
        "2412.01605": "|**2024-12-02**|**Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking**|Jie Liu et.al.|[2412.01605](http://arxiv.org/abs/2412.01605)|null|临床决策（CDM）是医疗保健服务中复杂且动态的过程，但对于人工智能系统来说仍然是一个重大挑战。尽管基于大型语言模型（LLM）的智能体已在执照考试和知识问答任务中测试了一般医学知识，但由于缺乏反映实际医疗实践的全面测试数据集，它们在现实场景中的CDM表现有限。为了解决这一差距，我们提出了MedChain，一个包含12,163个临床案例的数据集，涵盖了临床工作流程的五个关键阶段。MedChain与现有基准相比，具有三个反映现实临床实践的显著特点：个性化、交互性和顺序性。此外，为了应对现实世界中的CDM挑战，我们还提出了MedChain-Agent，这是一个集成了反馈机制和MCase-RAG模块的人工智能系统，以便从以往案例中学习并调整其响应。MedChain-Agent在动态收集信息和处理顺序临床任务方面表现出惊人的适应性，显著优于现有方法。在本文被接受后，将发布相关数据集和代码。|\n",
        "2412.01333": "|**2024-12-02**|**Can Large Language Models Serve as Evaluators for Code Summarization?**|Yang Wu et.al.|[2412.01333](http://arxiv.org/abs/2412.01333)|**[link](https://github.com/CGCL-codes/naturalcc)**|**代码摘要通过将代码片段转换为自然语言描述，有助于程序理解和软件维护。多年来，为这项任务开发了众多方法，但一个关键挑战仍然存在：有效地评估生成摘要的质量。虽然人工评估在评估代码摘要质量方面是有效的，但它劳动密集且难以扩展。常用的自动指标，如BLEU、ROUGE-L、METEOR和BertScore，通常与人工判断的关联性不强。在本文中，我们探讨了大型语言模型（LLMs）在评估代码摘要方面的潜力。我们提出了CODERPE（代码摘要评估中的角色扮演者），这是一种利用角色扮演提示来评估生成摘要质量的新方法。具体来说，我们提示LLM代理扮演各种角色，如代码审阅者、代码作者、代码编辑和系统分析师。每个角色从关键维度评估代码摘要的质量，包括连贯性、一致性、流畅性和相关性。我们进一步通过采用各种提示策略，包括思维链推理、情境学习和定制评分表设计，探索了LLMs作为评估者的鲁棒性。结果表明，LLMs作为代码摘要方法的有效评估者。值得注意的是，我们的基于LLM的评估器CODERPE与人工评估的斯皮尔曼相关系数为81.59%，比现有的BERTScore指标高17.27%。**|\n",
        "2412.01303": "|**2024-12-02**|**RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks**|Xu Yang et.al.|[2412.01303](http://arxiv.org/abs/2412.01303)|null|随着大规模分布式能源资源被集成到主动配电网络（ADN）中，与传统的配电网络相比，在ADN中实现有效的能源管理变得越来越突出。尽管先进的强化学习方法（RL）极大地提高了ADN能源管理的效率，减轻了复杂建模和优化的负担，但安全性成为RL在实际问题应用中的关键关注点。由于与操作安全约束相对应的惩罚函数的设计和调整需要RL和电力系统操作方面的广泛领域知识，新兴的ADN运营商呼吁采用更灵活和定制化的方法来解决惩罚函数，以便进一步提高操作安全和效率。凭借强大的理解、推理和在上下文中学习的能力，大型语言模型（LLM）为辅助ADN能源管理的安全RL提供了一种有希望的途径。在本文中，我们引入LLM来理解ADN中的操作安全要求并生成相应的惩罚函数。此外，我们提出了一种RL2机制，通过多轮对话迭代和自适应地改进生成的函数，其中LLM代理根据下游RL代理的训练和测试性能调整函数的模式和参数。所提出的方法显著减少了ADN运营商的干预。综合测试结果证明了所提出方法的有效性。|\n",
        "2412.01033": "|**2024-12-02**|**SAUP: Situation Awareness Uncertainty Propagation on LLM Agent**|Qiwei Zhao et.al.|[2412.01033](http://arxiv.org/abs/2412.01033)|null|大型语言模型（LLMs）集成到多步骤智能体系统中，能够在各种应用中实现复杂的决策过程。然而，它们的输出通常缺乏可靠性，因此不确定性估计变得至关重要。现有的不确定性估计方法主要关注最终步骤的输出，未能考虑到多步骤决策过程中的累积不确定性和智能体与其环境之间的动态交互。为了解决这些局限性，我们提出了SAUP（情境感知不确定性传播），这是一个新颖的框架，它通过LLM智能体推理过程的每一步传播不确定性。SAUP通过在传播过程中为每一步的不确定性分配情境权重来融入情境感知。我们的方法与各种单步不确定性估计技术兼容，提供了一个全面且准确的不确定性度量。在基准数据集上的大量实验表明，SAUP显著优于现有最先进的方法，实现了AUROC达到20%的提升。|\n",
        "2412.02776": "|**2024-12-03**|**Hacking CTFs with Plain Agents**|Rustem Turtayev et.al.|[2412.02776](http://arxiv.org/abs/2412.02776)|**[link](https://github.com/palisaderesearch/intercode)**|**我们通过简单的LLM代理设计饱和了一所高中水平的黑客基准测试。具体来说，我们通过提示、工具使用和多次尝试，在InterCode-CTF这个流行的进攻性安全基准测试上获得了95%的性能。这一成绩超过了Phuong等人2024年（29%）和Abramovich等人2024年（72%）的研究成果。我们的结果表明，当前的大型语言模型在进攻性网络安全方面已经超越了高中水平。它们的黑客能力尚未得到充分发掘：我们提出的ReAct&Plan提示策略在1-2个回合内解决了许多挑战，无需复杂的工程或高级利用。**|\n",
        "2412.04093": "|**2024-12-05**|**Practical Considerations for Agentic LLM Systems**|Chris Sypherd et.al.|[2412.04093](http://arxiv.org/abs/2412.04093)|null|近年来，随着大型语言模型（LLMs）的强大能力日益增长，对其作为自主代理基础模型的兴趣也随之增加。尽管LLMs在自然语言领域展现出涌现的能力和广泛的专业知识，但它们固有的不可预测性使得LLMs代理的实施变得具有挑战性，导致相关研究与这类系统的实际应用之间存在差距。为了弥合这一差距，本文将研究社区中的可操作见解和考虑因素置于既定应用范式之中，以促进稳健的LLMs代理的构建和明智的部署。具体而言，我们根据应用导向文献中的常见实践，将相关研究发现定位为四个广泛类别——规划、记忆、工具和控制流——并强调了在设计用于实际应用的代理型LLMs时需要考虑的实际因素，例如处理随机性和高效管理资源。虽然我们没有进行实证评估，但我们为讨论代理型LLMs设计的关键方面提供了必要的背景，无论是在学术界还是在工业界。|\n",
        "2412.04090": "|**2024-12-05**|**LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents**|Bingchen Li et.al.|[2412.04090](http://arxiv.org/abs/2412.04090)|null|我们提出了首个名为LossAgent的损失代理，用于低级图像处理任务，例如图像超分辨率和修复，旨在实现不同实际应用中低级图像处理的各种定制优化目标。值得注意的是，并非所有优化目标，如复杂的定制感知度量、文本描述和复杂的人类反馈，都能用现有的低级损失，例如均方误差损失（MSE loss），来实例化，这在端到端优化图像处理网络时提出了一个关键挑战。为了解决这个问题，我们的LossAgent引入了强大的大型语言模型（LLM）作为损失代理，丰富的先验知识文本理解赋予损失代理在低级图像处理网络优化过程中的复杂优化目标、轨迹和状态反馈的理解潜力。特别是，我们通过整合支持端到端优化低级图像处理现有损失函数建立了损失库。然后，我们为损失代理设计了面向优化的提示工程，使其在每个优化交互中能够积极和智能地决定库中每个损失的组合权重，从而实现任何定制优化目标所需的优化轨迹。在三个典型低级图像处理任务和多个优化目标上的大量实验表明了我们所提出的LossAgent的有效性和适用性。代码和预训练模型将在https://github.com/lbc12345/LossAgent上提供。|\n",
        "2412.03904": "|**2024-12-05**|**MISR: Measuring Instrumental Self-Reasoning in Frontier Models**|Kai Fronsdal et.al.|[2412.03904](http://arxiv.org/abs/2412.03904)|**[link](https://github.com/kaifronsdal/self-reasoning-evals)**|**我们提出了一套任务，用于评估大型语言模型（LLM）代理的工具体验推理能力。工具体验推理能力可以提高适应性和实现自我修改，但也可能带来重大风险，例如导致欺骗性对齐。先前的研究只评估了非代理环境或有限领域的自我推理。在本文中，我们提出了在广泛场景下，包括自我修改、知识寻求和模糊自我推理的代理任务中评估工具体验推理能力的方案。我们评估了使用最先进LLM构建的代理，包括商业和开源系统。我们发现，工具体验推理能力仅在最先进的边缘模型中体现，并且高度依赖于上下文。没有模型通过我们评估中最困难版本，因此我们的评估可以用于衡量未来模型工具体验推理能力的提升。我们在https://github.com/kaifronsdal/Self-Reasoning-Evals上开源了我们的评估。**|\n",
        "2412.03847": "|**2024-12-05**|**Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration**|Shiwen Ni et.al.|[2412.03847](http://arxiv.org/abs/2412.03847)|null|智能对话系统在现代教育和心理辅导领域得到越来越广泛的应用，但大多数现有系统局限于单一领域，无法同时处理教育和心理问题，并且在处理复杂问题时往往缺乏准确性和专业性。为了解决这些问题，本文提出了一种结合教育和心理辅导功能的智能对话系统。该系统由多个AI代理组成，包括安全检测代理、意图识别代理、教育LLM代理和心理LLM代理，它们协同工作以确保提供准确的教育知识问答和心理健康支持服务。具体来说，系统通过意图分类模型识别用户输入的意图，并调用增强检索的教育大型模型和心理大型模型（该模型已使用心理数据进行微调），以便提供专业的教育建议和心理健康支持。|\n",
        "2412.05093": "|**2024-12-06**|**Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models**|Da Ju et.al.|[2412.05093](http://arxiv.org/abs/2412.05093)|null|大型语言模型（LLMs）越来越多地被提议作为经典基于代理模型（ABMs）的有力替代，以模拟社会动态。通过将LLMs作为人类行为的代理，这种新方法希望通过模拟比经典ABMs更为复杂的动态，并在社会科学、政治科学和经济学等领域获得新的见解。然而，由于LLMs的“黑盒”性质，不清楚LLM代理是否实际上执行了编码在它们自然语言指令中的预期语义，以及由此产生的互动动态是否具有意义。为了研究这个问题，我们提出了一种新的评估框架，将LLM模拟建立在社会科学中已建立的参考模型的动态基础之上。我们将LLMs视为一个黑盒函数，评估它们的输入输出行为相对于这个参考模型，这使我们能够评估它们行为的详细方面。我们的结果表明，虽然可以通过设计提示来近似预期的动态，但这些模拟的质量高度依赖于特定提示的选择。重要的是，模拟甚至对任意的变异，如细微的文字变化和空格使用，都非常敏感。这引发了当前版本LLMs在有意义模拟中的有用性的质疑，因为没有参考模型，无法事先确定看似无意义的提示变化对模拟的影响。|\n",
        "2412.06724": "|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|**我们研究了大型语言模型（LLMs）在自动生成数据清洗工作流程中的推理能力。为了评估LLMs完成数据清洗任务的能力，我们实现了一个基于LLM的自动数据清洗工作流程（AutoDCWorkflow）的管道，通过提示LLM进行数据清洗操作来修复三种类型的数据质量问题：重复数据、缺失值和不一致的数据格式。给定一个脏表和一个目的（以查询的形式表达），这个管道生成一个最小的、干净的表，足以解决目的并使用生成表的数据清洗工作流程。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的目标列集合。（2）检查列质量：评估每个目标列的数据质量并生成数据质量报告作为操作目标。（3）生成操作和参数：根据数据质量报告结果预测下一个操作和参数。此外，我们提出一个数据清洗基准来评估LLM代理自动生成解决不同难度级别数据清洗目的工作流程的能力。基准包括注解数据集，作为一个包含目的、原始表、干净表、数据清洗工作流程和答案集的集合。在我们的实验中，我们评估了三个自动生成目的驱动数据清洗工作流程的LLMs。结果表明，LLMs在规划和生成数据清洗工作流程方面表现良好，无需微调。**|\n",
        "2412.06681": "|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通运输系统需求建模与仿真领域，基于代理的模型和微观仿真是目前最先进的方法。然而，现有的基于代理的模型在行为真实性和资源需求方面仍存在一些局限性，这限制了它们的适用性。在本研究中，我们利用新兴的大语言模型（LLMs）和基于LLM的代理技术，提出了一种适用于交通运输系统的通用LLM-代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，而且为克服现有基于代理的模型的某些局限性提供了有希望的解决方案。我们的概念框架设计紧密模拟了人类旅行者在交通运输网络中的决策、交互过程和特征，并通过相关研究和LLM代理在瓶颈设置中的学习和调整的示范实例，证明了所提出的系统可以满足决策和学习的关键行为标准。尽管进一步优化LLM-代理建模框架是必要的，但我们相信这种方法有潜力改进交通运输系统的建模与仿真。|\n",
        "2412.06294": "|**2024-12-09**|**Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects**|Louis Milliken et.al.|[2412.06294](http://arxiv.org/abs/2412.06294)|**[link](https://github.com/coinse/installamatic)**|最近，许多研究提出了使用大型语言模型（LLM）构建的智能体来执行所谓的“仓库级”任务，这些任务的范围大于单个文件。这引发了一种推测，即这些仓库级任务的协调可能产生能够在很大程度上无需人工干预的软件工程智能体。然而，我们认为在需要由这个自主软件工程智能体执行的众多任务中，有一项重要任务缺失，那就是通过安装其他仓库来满足项目级别的依赖关系。为了调查这种仓库级安装任务的可行性，我们引入了一个基准，该基准由来自40个开源Python项目的仓库安装任务组成，包括每个目标仓库的基准安装过程。此外，我们提出了Installamatic智能体，该智能体的目标是通过在仓库文档中搜索相关说明来执行和验证给定仓库的安装。实证实验表明，55%的研究仓库至少有十分之一的时间可以通过我们的智能体自动安装。通过进一步分析，我们确定了导致我们的智能体无法安装仓库的常见原因，讨论了设计此类智能体时面临的挑战，并考虑了此类智能体可能对开发者产生的影响。|\n",
        "2412.05850": "|**2024-12-08**|**Cooperative SQL Generation for Segmented Databases By Using Multi-functional LLM Agents**|Zhiguang Wu et.al.|[2412.05850](http://arxiv.org/abs/2412.05850)|null|文本到SQL任务旨在根据用户的文本问题自动生成SQL查询。为了解决这个问题，我们提出了一种基于多功能代理（CSMA）的协同SQL生成框架，通过具有各自数据库模式部分的大语言模型（LLM）代理之间的信息交互来实现。受到人类团队合作协作的启发，CSMA包括三个阶段：1）与问题相关的模式收集，2）问题对应的SQL查询生成，3）SQL查询正确性检查。在第一阶段，代理分析各自的模式并相互通信，收集与问题相关的模式信息。在第二阶段，代理尝试使用收集到的信息为问题生成相应的SQL查询。在第三阶段，代理根据他们已知的信息检查SQL查询是否正确创建。这种基于交互的方法使得每个代理的问题相关部分数据库模式可用于SQL生成和检查。在Spider和Bird基准测试上的实验表明，CSMA达到了与现有技术水平相当的高性能，同时保持了这些个体代理中的私有数据。|\n",
        "2412.07646": "|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过重复的语言学习和使用而演变为结构化。这些过程在语言习得期间引入了偏见，并使语言系统趋向于沟通效率。在这篇论文中，我们研究了如果人工语言是为大型语言模型（LLMs）的隐式偏见进行优化时，是否会发生相同的情况。为此，我们模拟了一个经典指称游戏，其中LLMs学习和使用人工语言。我们的结果表明，最初无结构的整体语言确实被塑造出一些结构属性，使两个LLM代理能够成功沟通。与人类实验中的观察结果相似，代际传播增加了语言的易学性，但同时也可能导致非人类退化词汇。总的来说，这项工作扩展了实验发现，表明LLMs可以用作语言演化的模拟工具，并为该领域的未来人机实验开辟了可能性。|\n",
        "2412.06828": "|**2024-12-06**|**Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System**|Fang Zeng et.al.|[2412.06828](http://arxiv.org/abs/2412.06828)|null|本研究介绍了“RadCouncil”，这是一个多智能体大型语言模型（LLM）框架，旨在增强放射学报告中发现部分的印象生成。RadCouncil由三个专业代理组成：1）一个“检索”代理，用于从向量数据库中识别和检索相似报告；2）一个“放射科医生”代理，根据给定报告中的发现部分以及检索代理检索到的示例报告生成印象；3）一个“审稿人”代理，评估生成的印象并提供反馈。RadCouncil的性能使用定量指标（BLEU、ROUGE、BERTScore）和由GPT-4评估的定性标准进行了评估，以胸部X光片作为案例研究。实验结果表明，在多个维度上，包括诊断准确性、风格一致性以及清晰度，RadCouncil相对于单代理方法都有所改进。这项研究强调了利用多个相互作用的LLM代理（每个代理都承担专用任务）来提高专业医疗任务性能和开发更稳健、适应性更强的医疗AI解决方案的潜力。|\n",
        "2412.08445": "|**2024-12-11**|**TapeAgents: a Holistic Framework for Agent Development and Optimization**|Dzmitry Bahdanau et.al.|[2412.08445](http://arxiv.org/abs/2412.08445)|null|我们提出了TapeAgents，这是一个围绕细粒度、结构化代理会话日志带构建的代理框架，同时该日志带也充当会话的可恢复状态。在TapeAgents中，我们利用日志带来促进LLM代理开发生命周期的各个阶段。代理通过处理日志带和LLM输出来进行推理，生成新的思考和行动步骤，并将它们附加到日志带上。环境随后通过将观察步骤同样附加到日志带上来对代理的动作做出反应。凭借这种以日志带为中心的设计，TapeAgents可以为AI从业者提供全面端到端的支持。在开发阶段，日志带有助于会话持久化、代理审计和逐步调试。部署后，可以重用日志带进行评估、微调和提示调整；关键的是，可以从其他代理或使用修订的历史日志带进行适配。在本报告中，我们详细解释了TapeAgents的设计。我们通过构建单体代理和多代理团队、优化代理提示和微调代理的LLM等几个具体例子，展示了TapeAgents的可能应用。我们展示了工具原型，并报告了一个案例研究，其中我们使用TapeAgents微调Llama-3.1-8B表单填写助手，使其性能与GPT-4o相当，同时成本低得多。最后，我们的比较分析表明，TapeAgents相较于先前框架的优势源于我们对LLM代理作为可恢复、模块化状态机的创新设计，该设计具有结构化配置，可以生成细粒度、结构化的日志，并将这些日志转换为训练文本——这是以前工作中所缺少的独特功能组合。|\n",
        "2412.08054": "|**2024-12-11**|**Federated In-Context LLM Agent Learning**|Panlong Wu et.al.|[2412.08054](http://arxiv.org/abs/2412.08054)|null|大型语言模型（LLMs）通过实现逻辑推理、工具使用以及作为代理与外部系统交互，彻底改变了智能服务。LLMs的进步常常受到高质量数据稀缺性的阻碍，其中大部分数据本质上具有敏感性。联邦学习（FL）通过促进分布式LLMs的协作训练同时保护私有数据，提供了一个潜在的解决方案。然而，FL框架面临着显著的带宽和计算需求，以及来自异构数据分布的挑战。LLMs新兴的上下文学习能力提供了一种有希望的方法，通过聚合自然语言而不是庞大的模型参数。然而，这种方法存在隐私泄露的风险，因为它需要在聚合过程中收集和展示来自不同客户端的数据样本。在本文中，我们提出了一种新颖的隐私保护联邦上下文LLM代理学习（FICAL）算法，据我们所知，这是第一个利用上下文学习的能力通过FL来训练多样化的LLM代理的工作。在我们的设计中，由新颖的LLM增强的知识摘要生成（KCG）模块生成的知识库在客户端和服务器之间传输，而不是像之前的FL方法中那样传输模型参数。除此之外，我们还设计了一个基于检索增强生成（RAG）的工具学习和利用（TLU）模块，并将聚合的全局知识库作为教师来教授LLM代理工具的使用。我们进行了广泛的实验，结果表明，与现有的SOTA基线相比，FICAL具有竞争力的性能，并且通信成本降低了$\\mathbf{3.33\\times10^5}$倍。|\n",
        "2412.08014": "|**2024-12-11**|**MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents**|Yun Xing et.al.|[2412.08014](http://arxiv.org/abs/2412.08014)|null|在驾驶场景中的物理对抗攻击可以揭示视觉感知模型的关键漏洞。然而，由于现实世界的多样性背景和保持视觉自然性的要求，开发此类攻击仍然具有挑战性。基于这一挑战，我们将物理对抗攻击重新定义为一次性的补丁生成问题。我们的方法通过一个考虑特定场景上下文的深度生成模型生成对抗补丁，使得可以直接在匹配环境中进行物理部署。主要挑战在于同时实现两个目标：生成能够有效误导目标检测系统的对抗补丁，并确定场景中的上下文适当的放置位置。我们提出了MAGIC（在上下文中掌握物理对抗生成），一个由多模态LLM代理驱动的创新框架，以应对这些挑战。MAGIC能够自动理解场景上下文，并通过语言和视觉能力的协同交互来协调对抗补丁的生成。MAGIC协调了三个专业的LLM代理：对抗补丁生成代理（GAgent）通过为文本到图像模型进行战略性的提示工程来掌握创建欺骗性补丁；对抗补丁部署代理（DAgent）通过基于场景理解确定最优放置策略来确保上下文一致性；自我审查代理（EAgent）通过提供对两个过程的批判性监督和迭代改进来完成这一三部曲。我们在数字和物理层面（即nuImage和手动捕获的真实场景）验证了我们的方法，统计和视觉结果均证明我们的MAGIC在攻击广泛使用的目标检测系统方面既强大又有效。|\n",
        "2412.07822": "|**2024-12-10**|**MAGE: A Multi-Agent Engine for Automated RTL Code Generation**|Yujie Zhao et.al.|[2412.07822](http://arxiv.org/abs/2412.07822)|**[link](https://github.com/stable-lab/MAGE-A-Multi-Agent-Engine-for-Automated-RTL-Code-Generation)**|**随着大型语言模型（LLMs）的发展，通过自然语言指令自动生成RTL代码（例如Verilog）已成为一个有前景的方向。然而，生成既符合语法又符合功能的RTL代码仍然是一个重大挑战。现有的单LLM代理方法面临重大局限性，因为它们必须在各种编程语言之间导航，并处理复杂的生成、验证和修改任务。为了解决这些挑战，本文介绍了MAGE，这是第一个开源的多代理人工智能系统，旨在实现鲁棒和精确的Verilog RTL代码生成。我们提出了一种新颖的高温RTL候选样本采样和调试系统，它有效地探索了代码候选空间，并显著提高了候选代码的质量。此外，我们设计了一种新颖的Verilog状态检查点检查机制，能够早期发现功能错误，并为有针对性的修复提供精确的反馈，显著提高了生成的RTL代码的功能正确性。MAGE在VerilogEval-Human 2基准测试中实现了95.7%的语法和功能正确性代码生成率，超过了最先进的Claude-3.5-sonnet，提高了23.3%，展示了人工智能驱动RTL设计工作流程的鲁棒和可靠方法。**|\n",
        "2412.08685": "|**2024-12-11**|**ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street Scenes**|Yuxi Wei et.al.|[2412.08685](http://arxiv.org/abs/2412.08685)|null|根据具体指令生成具有真实性和交互性的交通参与者动态对于街景模拟至关重要。然而，目前尚缺乏一种能够生成包括车辆和行人等不同类型参与者及其之间不同种类交互的全面方法。在本文中，我们介绍了ChatDyn，这是第一个能够根据语言指令在街景中生成交互、可控和真实参与者动态的系统。为了通过复杂语言实现精确控制，ChatDyn采用多LLM-agent角色扮演方法，利用自然语言输入来规划不同交通参与者的轨迹和行为。为了基于规划生成真实的细粒度动态，ChatDyn设计了两个新颖的执行器：PedExecutor，一个统一的多元任务执行器，能够在不同的任务规划下生成真实的行人动态；以及VehExecutor，一个基于物理过渡策略的执行器，生成符合物理学的车辆动态。大量的实验表明，ChatDyn可以生成包含多车辆和行人的真实驾驶场景动态，并且在子任务上显著优于之前的方法。代码和模型将在https://vfishc.github.io/chatdyn上提供。|\n",
        "2412.10270": "|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有普遍能力的AI代理提供了令人信服的基础。这些代理可能很快将在现实世界中大规模部署，代表个别人类（例如AI助手）或人类群体（例如AI加速的公司）的利益。目前，关于多个LLM代理在多代迭代部署中相互作用的动态知之甚少。在本文中，我们考察了在存在背叛动机的情况下，“LLM代理社会”是否能够学习相互有益的社会规范，这是人类社会性的一个独特特征，对于文明的成功可能至关重要。具体而言，我们研究了LLM代理在玩经典迭代捐赠游戏中的间接互惠的演变，在这个游戏中，代理可以观察到其同伴的最近行为。我们发现，合作的演变在不同基础模型之间存在显著差异，Claude 3.5 Sonnet代理的社会平均得分显著高于Gemini 1.5 Flash，而Gemini 1.5 Flash又优于GPT-4o。此外，Claude 3.5 Sonnet可以利用额外的成本惩罚机制来获得更高的分数，而Gemini 1.5 Flash和GPT-4o则不能。对于每个模型类别，我们还在随机种子之间观察到涌现行为的变异，这表明对初始条件的一种未被充分研究的敏感依赖。我们建议，我们的评估机制可以激发一种低成本且信息丰富的LLM基准测试新类别，重点关注LLM代理部署对社会合作基础设施的影响。|\n",
        "2412.10138": "|**2024-12-13**|**ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL**|Yang Qin et.al.|[2412.10138](http://arxiv.org/abs/2412.10138)|**[link](https://github.com/alibaba/route)**|**尽管大型语言模型（LLMs）在文本到SQL（Text2SQL）方面取得了显著进展，但最新的最先进技术仍然被困在封闭源LLMs（如GPT-4）的上下文学习中，这限制了它们在开放场景中的应用。为了应对这一挑战，我们提出了一种新的鲁棒多任务调整和协作方法（ROUTE），以提高开源LLMs在Text2SQL方面的综合能力，从而提供一个更实用的解决方案。我们的方法从使用与SQL生成相关的各种合成训练数据的任务监督微调（SFT）开始。与现有的基于SFT的Text2SQL方法不同，我们引入了几个额外的SFT任务，包括模式链接、噪声纠正和续写。参与各种SQL生成任务增强了模型对SQL语法的理解，并提高了其生成高质量SQL查询的能力。此外，受LLM代理协作模式启发，我们引入了多任务协作提示（MCP）策略。该策略利用跨多个SQL相关任务的协作来减少SQL生成过程中的幻觉，从而最大限度地发挥通过显式多任务能力增强Text2SQL性能的潜力。我们在八个开源LLMs和五个广泛使用的基准上进行了广泛的实验和深入分析。结果表明，我们的提议优于最新的Text2SQL方法，并取得了领先的性能。**|\n",
        "2412.10133": "|**2024-12-13**|**You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects**|Islem Bouzenia et.al.|[2412.10133](http://arxiv.org/abs/2412.10133)|null|在许多场景中，执行项目的测试套件至关重要，例如，评估代码质量和覆盖率，验证开发人员或自动化工具所做的代码更改，以及确保与依赖项的兼容性。尽管其重要性不言而喻，但在实际中执行项目的测试套件可能会面临挑战，因为不同的项目使用不同的编程语言、软件生态系统、构建系统和测试框架等工具。这些挑战使得创建一个在不同项目间都能可靠运行的全能测试执行方法变得困难。本文介绍了一种名为ExecutionAgent的自动化技术，该技术可以安装任意项目，配置它们运行测试用例，并生成特定于项目的脚本以重现设置。受人类开发者处理此类任务的方式启发，我们的方法是一个基于大型语言模型的代理，它可以自主执行命令并与宿主系统交互。该代理使用元提示来收集有关给定项目最新技术的指导，并根据前一步的反馈迭代优化其过程。我们的评估将ExecutionAgent应用于50个开源项目，这些项目使用了14种不同的编程语言以及许多不同的构建和测试工具。该方法成功执行了33/55个项目的测试套件，与基准测试套件执行结果匹配度仅为7.5%的偏差。这些结果比之前最好的技术提高了6.6倍。该方法带来的成本是合理的，平均每个项目的执行时间为74分钟，大型语言模型成本为0.16美元。我们期望ExecutionAgent成为开发者、自动化编程工具和研究人员执行各种项目测试的有价值工具。|\n",
        "2412.11373": "|**2024-12-16**|**Codenames as a Benchmark for Large Language Models**|Matthew Stephenson et.al.|[2412.11373](http://arxiv.org/abs/2412.11373)|null|在本文中，我们提出将流行的基于单词的桌游Codenames用作评估大型语言模型（LLMs）推理能力的合适基准。Codenames为成功实现AI性能提出了一个极具挑战性的问题，需要复杂的语言理解、心智理论和认知推理能力。先前开发Codenames代理的尝试大多依赖于词嵌入技术，这些技术词汇范围有限，在与不同方法结合时表现不佳。LLMs在基于语言的任务中展现出增强的推理和理解能力，但在横向思维挑战中仍可能表现不佳。我们评估了包括GPT-4o、Gemini 1.5、Claude 3.5 Sonnet和Llama 3.1在内的几种最先进LLMs在各种棋盘布局下的能力。我们的结果表明，尽管某些LLMs在总体上表现优于其他LLMs，但不同的模型在游戏过程中表现出不同的涌现行为，并在特定角色上表现出色。我们还评估了不同LLMs组合在协同游戏中的表现，证明了LLM代理比先前技术更易于推广到更广泛的队友群体中。|\n",
        "2412.13178": "|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身代理在执行复杂指令方面具有强大的能力，为具身机器人的潜在部署开辟了道路。然而，一个可预见的问题是，这些具身代理也可以完美地执行一些危险任务，可能在实际世界中造成损害。为了研究这个问题，我们提出了SafeAgentBench——一个用于具身LLM代理安全任务规划的新的基准。SafeAgentBench包括：（1）一个包含750个任务的新数据集，涵盖了10种潜在危险和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持多代理执行，并为8个最先进的基线提供了17个高级动作；（3）从执行和语义两个角度的可靠评估方法。实验结果表明，表现最佳的基线在安全任务中达到了69%的成功率，但在危险任务中只有5%的拒绝率，这表明存在显著的安全风险。更多细节和代码可在https://github.com/shengyin1224/SafeAgentBench找到。**|\n",
        "2412.14161": "|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们每天都在与计算机互动，无论是在日常生活中还是工作中，许多工作都可以通过访问计算机和互联网来完成。同时，得益于大型语言模型（LLMs）的改进，与周围环境互动并产生影响的人工智能代理也迅速发展。但是，AI代理在帮助加速甚至自主执行工作相关任务方面的表现如何？这个问题的答案对于希望将AI融入其工作流程的行业以及了解AI采用对劳动力市场可能产生的影响的经济政策具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进展，本文介绍了TheAgentCompany，这是一个可扩展的基准，用于评估以类似数字工作者方式与世界互动的AI代理：通过浏览网页、编写代码、运行程序以及与同事沟通。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司的环境，并创建了一系列可能由该公司员工执行的任务。我们测试了由基于封闭API和开放权重的语言模型（LM）驱动的基线代理，发现最具有竞争力的代理可以使24%的任务实现自主完成。这描绘了一幅关于使用LLM代理进行任务自动化的复杂图景——在一个模拟真实工作场所的环境中，大部分简单任务可以自主解决，但更困难的长远任务仍然超出现有系统的范围。**|\n",
        "2412.13667": "|**2024-12-18**|**Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery**|ChengAo Shen et.al.|[2412.13667](http://arxiv.org/abs/2412.13667)|null|因果推理是跨越多个领域（如智能健康、药物发现AI和AIOps）决策的基础。传统的统计因果发现方法虽然已经建立，但主要依赖于观察数据，往往忽略了因果关系中固有的语义线索。大型语言模型（LLMs）的出现为利用语义线索进行知识驱动的因果发现提供了一种经济实惠的方法，但LLMs在因果发现领域的发展落后于其他领域，尤其是在多模态数据的探索方面。为了填补这一差距，我们引入了MATMCD，这是一个由工具增强的LLMs驱动的多智能体系统。MATMCD有两个关键智能体：一个数据增强智能体，用于检索和处理模态增强数据；以及一个因果约束智能体，用于集成多模态数据进行知识驱动的推理。内部工作的精心设计确保了智能体的成功合作。我们在七个数据集上的实证研究表明，多模态增强的因果发现具有显著的潜力。|\n",
        "2412.14737": "|**2024-12-19**|**On Verbalized Confidence Scores for LLMs**|Daniel Yang et.al.|[2412.14737](http://arxiv.org/abs/2412.14737)|**[link](https://github.com/danielyxyang/llm-verbalized-uq)**|**随着大型语言模型（LLMs）的兴起及其与日常生活的紧密融合，致力于其可信度变得至关重要。LLMs的不确定性量化可以增强人们对它们回答的信任，同时也使LLMs代理能够根据彼此的不确定性做出更明智的决策。为了估计回答中的不确定性，通常使用内部标记logits、针对特定任务的代理模型或多个响应的采样。这项工作侧重于让LLM自己用置信度分数作为输出标记的一部分来表达其不确定性，这是一种具有低开销、提示和模型无关的不确定性量化的有前景方法。使用广泛的基准，我们评估了这些置信度分数在不同数据集、模型和提示方法方面的可靠性。我们的结果表明，这些分数的可靠性强烈依赖于如何询问模型，但也表明可以通过某些提示方法提取出良好校准的置信度分数。我们认为，用言语表达的置信度分数可以成为未来简单但有效且通用的不确定性量化方法。我们的代码可在https://github.com/danielyxyang/llm-verbalized-uq上找到。**|\n",
        "2412.14470": "|**2024-12-19**|**Agent-SafetyBench: Evaluating the Safety of LLM Agents**|Zhexin Zhang et.al.|[2412.14470](http://arxiv.org/abs/2412.14470)|**[link](https://github.com/thu-coai/agent-safetybench)**|**随着大型语言模型（LLMs）作为代理的广泛应用，它们在交互环境和工具使用中的集成带来了超出模型本身相关的新的安全挑战。然而，缺乏用于评估代理安全性的全面基准，对有效评估和进一步改进构成了重大障碍。在本文中，我们介绍了Agent-SafetyBench，这是一个旨在评估LLM代理安全性的全面基准。Agent-SafetyBench包含349个交互环境和2000个测试案例，评估8类安全风险，涵盖在不安全交互中经常遇到的10种常见故障模式。我们对16个流行的LLM代理进行评估的结果令人担忧：没有一种代理的安全评分超过60%。这突显了LLM代理中的重大安全挑战，并强调了改进的巨大需求。通过定量分析，我们确定了关键故障模式，并总结了当前LLM代理中的两个基本安全缺陷：缺乏鲁棒性和缺乏风险意识。此外，我们的发现表明，仅依赖防御提示是不够解决这些安全问题的，强调了需要更先进和鲁棒的策略。我们将Agent-SafetyBench发布在\\url{https://github.com/thu-coai/Agent-SafetyBench}，以促进代理安全评估和改进的进一步研究和创新。**|\n",
        "2412.14212": "|**2024-12-18**|**Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution**|Ziyi Ni et.al.|[2412.14212](http://arxiv.org/abs/2412.14212)|null|大型语言模型（LLMs）的卓越能力极大地加速了代理的快速崛起和广泛应用。最近的研究表明，生成Python代码将基于LLMs的代理的动作整合到一个统一的行为空间（CodeAct）是开发现实世界LLMs代理的有前景的方法。然而，这种逐步代码生成方法往往缺乏一致性和鲁棒性，导致代理应用不稳定，尤其是在复杂推理和域外任务中。在本文中，我们提出了一种名为“代码树”（ToC）的新方法，通过端到端机制解决复杂问题规划和执行中的挑战。通过整合思维树和CodeAct的关键思想，ToC结合了它们的优势以增强解决方案的探索。在我们的框架中，每个最终的代码执行结果被视为决策树中的一个节点，采用广度优先搜索策略来探索潜在解决方案。最终结果通过基于节点输出的投票机制确定。|\n",
        "2412.15305": "|**2024-12-19**|**Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling**|Ziyi Ni et.al.|[2412.15305](http://arxiv.org/abs/2412.15305)|null|解决复杂推理任务是智能体在现实世界中的关键应用。得益于大型语言模型（LLMs）在代码数据上的预训练，最近的方法如CodeAct成功地使用代码作为LLM智能体的动作，取得了良好的效果。然而，CodeAct通过依赖零散的思维贪婪地生成下一个动作的代码块，导致结果不一致和不稳定。此外，CodeAct缺乏与动作相关的真实标签（GT），使得其在多轮交互中的监督信号和终止条件令人质疑。为了解决这些问题，我们首先介绍了一种简单而有效的端到端代码生成范式，名为CodeProgram，它利用代码的系统逻辑与全局推理保持一致，并实现连贯的问题解决。然后，我们提出了树形代码（ToC），它根据代码的可执行性自我增长CodeProgram节点，并在无GT的情况下实现自监督。在两个数据集上使用十个流行的零样本LLMs的实验结果表明，ToC在不到1/4轮的情况下将准确性提高了近20%，比CodeAct有显著提升。几个LLMs在一轮CodeProgram上的表现甚至优于多轮CodeAct。为了进一步研究有效性和效率之间的权衡，我们测试了不同的ToC树大小和探索机制。我们还强调了ToC端到端数据生成在监督和强化微调中的潜力。|\n",
        "2412.15274": "|**2024-12-17**|**Memory-Augmented Agent Training for Business Document Understanding**|Jiale Liu et.al.|[2412.15274](http://arxiv.org/abs/2412.15274)|null|传统企业处理业务文档时面临重大挑战，尽管这些文档在物流运营中起着至关重要的作用，但像从发票中提取运输参考这样的任务仍主要依赖手工操作。虽然大型语言模型提供了自动化的潜力，但它们直接应用于特定商业领域时往往会产生不尽人意的结果。我们引入了Matrix（通过推理和迭代探索增强记忆的代理训练），这是一个新的范例，它使LLM代理能够通过经验驱动的记忆优化和迭代学习逐步构建领域专业知识。为了验证这种方法，我们与世界最大的物流公司之一合作，创建了一个包含通用商业语言格式发票文档的数据集，重点关注运输参考提取的任务。实验表明，Matrix的表现优于直接提示单个LLM 30.3%，优于普通LLM代理 35.2%。我们进一步分析了优化系统的指标，并观察到代理系统需要的API调用更少，成本更低，并且可以平均分析更长的文档。我们的方法通过在文档处理任务中系统地增强记忆，确立了一种将通用LLM转化为专业商业工具的新方法。|\n",
        "2412.15266": "|**2024-12-17**|**On the Structural Memory of LLM Agents**|Ruihong Zeng et.al.|[2412.15266](http://arxiv.org/abs/2412.15266)|**[link](https://github.com/zengrh3/StructuralMemory)**|记忆在使大型语言模型（LLM）代理能够参与复杂和长期交互，如问答（QA）和对话系统方面起着关键作用。尽管已经提出了各种记忆模块来完成这些任务，但不同记忆结构在任务之间的影响仍然没有得到充分探索。本文研究了记忆结构和记忆检索方法如何影响基于LLM的代理的性能。具体来说，我们评估了四种类型的记忆结构，包括块状结构、知识三元组、原子事实和摘要，以及混合记忆，它结合了这些组件。此外，我们还评估了三种广泛使用的记忆检索方法：单步检索、重排序和迭代检索。在四个任务和六个数据集上进行的广泛实验产生了以下关键见解：（1）不同的记忆结构具有不同的优势，使它们能够针对特定任务进行定制；（2）混合记忆结构在噪声环境中表现出显著的可适应性；（3）迭代检索在各种场景中始终优于其他方法。我们的研究旨在激励对基于LLM的代理的记忆系统设计进行进一步研究。|\n",
        "2412.17686": "|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|**大型语言模型（LLMs）的快速发展和部署为人工智能领域开辟了新的前沿，这些模型在自然语言理解和生成方面展现出前所未有的能力。然而，这些模型越来越多地被集成到关键应用中，引发了重大的安全担忧，需要对这些潜在风险及其缓解策略进行彻底审查。本综述全面概述了当前LLM安全领域的现状，涵盖四大主要类别：价值偏差、对抗攻击的鲁棒性、滥用和自主AI风险。除了对这四个方面的缓解方法和评估资源的全面回顾外，我们还进一步探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提升LLM安全中的作用、一系列AI公司和研究机构提出的LLM安全技术路线图，以及旨在LLM安全的AI治理，包括国际合作、政策建议和预期的监管方向。我们的发现强调了采取积极、多方面的方法来确保LLM安全的重要性，强调技术解决方案、伦理考量以及稳健的治理框架的整合。本综述旨在为学术界研究人员、行业实践者和政策制定者提供基础资源，提供关于将LLMs安全地融入社会的挑战和机遇的见解。最终，它旨在为LLMs的安全和有益发展做出贡献，与利用AI促进社会进步和福祉的总体目标相一致。相关论文的精选列表已公开提供在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers。**|\n",
        "2412.17259": "|**2024-12-23**|**LegalAgentBench: Evaluating LLM Agents in Legal Domain**|Haitao Li et.al.|[2412.17259](http://arxiv.org/abs/2412.17259)|**[link](https://github.com/cshaitao/legalagentbench)**|**随着LLM代理的智能和自主性不断提高，它们在法律领域的潜在应用越来越明显。然而，现有的通用领域基准无法完全捕捉现实世界司法认知和决策的复杂性和微妙之处。因此，我们提出了LegalAgentBench，这是一个专门设计用于评估LLM代理在中国法律领域的综合基准。LegalAgentBench包括来自现实世界法律场景的17个语料库，并提供37个用于与外部知识交互的工具。我们设计了一个可扩展的任务构建框架，并仔细标注了300个任务。这些任务涵盖了多种类型，包括多跳推理和写作，难度级别各异，有效地反映了现实世界法律场景的复杂性。此外，除了评估最终成功之外，LegalAgentBench还在中间过程中加入了关键词分析，以计算进度率，从而实现更细致的评估。我们评估了八种流行的LLM，突出了现有模型和方法的优势、局限性和潜在的改进领域。LegalAgentBench为LLM在法律领域的实际应用设定了新的基准，其代码和数据可在\\url{https://github.com/CSHaitao/LegalAgentBench}获取。**|\n",
        "2412.17146": "|**2024-12-22**|**LLM Agent for Fire Dynamics Simulations**|Leidong Xu et.al.|[2412.17146](http://arxiv.org/abs/2412.17146)|null|在利用基础模型，如大型语言模型（LLM），来加速复杂科学工作流程方面取得了显著进展。在本工作中，我们引入了FoamPilot，这是一个概念验证LLM代理，旨在提升FireFOAM的使用便捷性。FireFOAM是一个专门用于火灾动力学和火灾抑制模拟的求解器，它是基于OpenFOAM构建的，OpenFOAM是一个流行的开源计算流体动力学（CFD）工具箱。FoamPilot提供了三个核心功能：代码洞察、案例配置和模拟评估。代码洞察是一种利用检索增强生成（RAG）作为替代传统关键词搜索的方法，旨在使开发者和经验丰富的用户能够高效地导航和总结FireFOAM源代码。对于案例配置，代理可以理解用户的自然语言请求，并旨在根据这些请求修改现有的模拟设置，以支持中级用户。FoamPilot的工作执行功能旨在管理高性能计算（HPC）环境中的模拟提交和执行，并为经验较少的用户提供模拟结果的初步分析。每个功能都取得了有希望的结果，尤其是在简单任务方面，并且对于更复杂的任务，识别出了显著的进一步改进的机会。将这些功能集成到一个单一的LLM代理中，旨在加速使用FireFOAM进行复杂模拟（这对于提高火灾安全性至关重要）的工程师和科学家的模拟工作流程。|\n",
        "2412.16682": "|**2024-12-21**|**The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents**|Feiran Jia et.al.|[2412.16682](http://arxiv.org/abs/2412.16682)|null|大型语言模型（LLM）代理正越来越多地被部署为能够通过工具集成执行复杂现实任务的对话助手。这种与外部系统交互和处理各种数据源的能力虽然强大，但也引入了重大的安全漏洞。特别是，间接提示注入攻击构成了严重威胁，其中嵌入在外部数据源中的恶意指令可以操纵代理偏离用户意图。虽然基于规则约束、来源突出显示和身份验证协议的现有防御措施显示出希望，但它们在保持任务功能的同时难以维持强大的安全性。我们提出了一种新颖且独立的视角，将代理安全从防止有害行为重新定义为确保任务一致性，要求每个代理动作都服务于用户目标。基于这一洞察，我们开发了任务盾（Task Shield），一种测试时的防御机制，该机制系统地验证每个指令和工具调用是否有助于实现用户指定的目标。通过在AgentDojo基准测试上的实验，我们证明了任务盾在GPT-4o上降低了攻击成功率（2.07%）的同时，保持了高任务效用（69.79%）。|\n",
        "2412.18428": "|**2024-12-24**|**Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent**|Farhad Nooralahzadeh et.al.|[2412.18428](http://arxiv.org/abs/2412.18428)|null|国际企业、组织或医院收集了大量存储在数据库、文本文档、图像和视频中的多模态数据。尽管在多模态数据探索的独立领域以及能够将自然语言问题自动翻译为数据库查询语言的数据库系统方面取得了近期进展，但将数据库系统与图像等非结构化模态结合使用自然语言进行查询的研究挑战尚未得到广泛探索。在本文中，我们提出了XMODE——一个能够实现可解释的多模态数据探索的系统。我们的方法基于以下研究贡献：（1）我们的系统受到一个现实世界用例的启发，该用例使用户能够探索多模态信息系统。（2）XMODE利用基于LLM的智能体AI框架将自然语言问题分解为子任务，如文本到SQL生成和图像分析。（3）在关系数据和图像的多模态数据集上的实验结果表明，我们的系统优于最先进的多模态探索系统，不仅在准确性上表现出色，而且在查询延迟、API成本、规划效率和解释质量等各个方面都表现出优越的性能，这得益于LLM推理能力的更有效利用。|\n",
        "2412.18371": "|**2024-12-24**|**Defining and Detecting the Defects of the Large Language Model-based Autonomous Agents**|Kaiwen Ning et.al.|[2412.18371](http://arxiv.org/abs/2412.18371)|**[link](https://github.com/KevinHeiwa/Agentable)**|**AI智能体是能够感知环境、自主规划和执行任务的系统。最近在大型语言模型（LLM）方面的进展为AI智能体引入了一种变革性的范式，使它们能够通过提示与外部资源和工具进行交互。在这些智能体中，工作流程将开发者编写的代码（用于管理框架构建和逻辑控制）与LLM生成的自然语言（用于增强动态决策和交互）相结合。然而，开发者实现的逻辑与LLM动态生成内容在行为和预期结果方面的差异可能导致缺陷，例如工具调用失败和任务执行错误。这些问题引入了特定的风险，导致基于LLM的AI智能体出现各种缺陷，如服务中断。尽管这些问题很重要，但缺乏系统性的工作关注分析基于LLM的AI智能体以揭示其代码中的缺陷。在本文中，我们提出了第一个专注于识别和检测LLM智能体缺陷的研究。我们收集并分析了来自StackOverflow的6,854条相关帖子，定义了8种智能体缺陷类型。对于每种类型，我们提供了详细的描述和示例。然后，我们设计了一个名为Agentable的静态分析工具来检测缺陷。Agentable利用代码属性图和LLM通过高效地识别特定的代码模式和分析自然语言描述来分析智能体工作流程。为了评估Agentable，我们构建了两个数据集：AgentSet，包含84个现实世界的智能体，以及包含78个智能体的AgentTest，这些智能体特意设计来包含各种类型的缺陷。我们的结果显示，Agentable的整体准确率达到88.79%，召回率为91.03%。此外，我们的分析揭示了AgentSet中的889个缺陷，突显了这些缺陷的普遍性。**|\n",
        "2412.18174": "|**2024-12-24**|**INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent**|Haohang Li et.al.|[2412.18174](http://arxiv.org/abs/2412.18174)|null|近期进展凸显了基于大型语言模型（LLM）的代理在金融决策中的潜力。尽管取得了这些进展，该领域目前面临两个主要挑战：（1）缺乏一个适用于各种金融任务的综合性LLM代理框架，以及（2）缺乏用于评估代理性能的标准基准和一致数据集。为了解决这些问题，我们引入了InvestorBench，这是第一个专门为评估不同金融决策情境中基于LLM的代理而设计的基准。InvestorBench通过提供适用于不同金融产品（包括单个股票、加密货币和交易所交易基金（ETFs）等）的综合任务套件，增强了LLM代理的通用性。此外，我们使用十三种不同的LLM作为骨干模型，在各种市场环境和任务中评估了我们代理框架的推理和决策能力。此外，我们精心整理了一个多样化的开源、多模态数据集集合，并开发了一套全面的金融决策环境。这建立了一个高度可访问的平台，用于评估金融代理在各种场景下的性能。|\n",
        "2412.21154": "|**2024-12-30**|**Aviary: training language agents on challenging scientific tasks**|Siddharth Narayanan et.al.|[2412.21154](http://arxiv.org/abs/2412.21154)|null|解决复杂的现实世界任务需要动作和观察的循环。这在科学领域尤为如此，因为科学任务需要多次的分析、工具使用和实验循环。语言代理有望自动化科学中的智力任务，因为它们可以通过自然语言或代码与工具交互。然而，它们的灵活性为软件实现带来了概念和实际上的挑战，因为代理可能包括非标准组件，如内部推理、规划、工具使用，以及温度样本语言模型固有的随机性。在这里，我们介绍了Aviary，一个可扩展的语言代理健身房。我们将代理正式化为解决基于语言的半可观察马尔可夫决策过程（我们称之为语言决策过程）的策略。然后我们实现了五个环境，包括三个具有挑战性的科学环境：（1）操作DNA构建体进行分子克隆，（2）通过访问科学文献回答研究问题，以及（3）设计蛋白质稳定性。这些环境因其关注多步骤推理及其与当代生物学研究的相关性而被选中。最后，通过在线训练和扩展推理时间计算，我们表明，由开源、非前沿LLM支持的语言代理在多个任务上可以匹配甚至超越前沿LLM代理和人类专家，同时推理成本降低高达100倍。|\n",
        "2412.21102": "|**2024-12-30**|**Exploring and Controlling Diversity in LLM-Agent Conversation**|KuanChao Chu et.al.|[2412.21102](http://arxiv.org/abs/2412.21102)|null|多样性是多智能体通信的一个关键方面。在本文中，我们专注于在开放域多智能体对话的背景下控制和研究多样性，尤其是针对世界模拟应用。我们提出了自适应提示剪枝（APP）这一新颖方法，该方法通过单个参数λ动态调整话语生成提示的内容以控制多样性。通过大量实验，我们展示了APP能够有效控制模型和数据集的输出多样性，剪枝更多信息的输出更加多样化。我们全面分析了提示内容与对话多样性之间的关系。我们的研究发现，提示的所有组成部分的信息通常都会约束输出的多样性，其中记忆块的影响最为显著。APP与温度采样和top-p采样等现有技术兼容，为多样性管理提供了一个多功能的工具。为了解决增加多样性带来的权衡，例如与省略信息的矛盾，我们引入了生成后的校正步骤，有效地在多样性增强与输出一致性之间取得平衡。此外，我们还考察了提示结构，包括组件顺序和长度，对多样性的影响。这项研究解决了多智能体世界模拟中围绕多样性的关键问题，为其控制、影响因素和相关权衡提供了见解。我们的贡献为在基于LLM的多智能体协作中系统地构建多样性奠定了基础，提高了其在现实世界应用中的有效性。|\n",
        "2412.21033": "|**2024-12-30**|**Plancraft: an evaluation dataset for planning with LLM agents**|Gautier Dagan et.al.|[2412.21033](http://arxiv.org/abs/2412.21033)|**[link](https://github.com/gautierdag/plancraft)**|**我们介绍了Plancraft，这是一个用于LLM代理的多模态评估数据集。Plancraft具备基于Minecraft合成GUI的纯文本和多模态界面。我们包括了Minecraft维基百科以评估工具使用和检索增强生成（RAG），以及一个先验规划器和先验RAG信息提取器，以消除现代代理架构的不同组件。为了评估决策能力，Plancraft还包括了一组故意无法解决的示例，提供了一个需要代理不仅完成任务，还要决定其是否可解的逼真挑战。我们在我们的任务上对开源和闭源LLM及其策略进行了基准测试，并将它们的性能与人工制定的规划器进行了比较。我们发现LLM和VLM在Plancraft引入的规划问题上存在困难，并提出了如何提高其能力的建议。**|\n",
        "2412.20505": "|**2024-12-29**|**Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning**|Hang Ni et.al.|[2412.20505](http://arxiv.org/abs/2412.20505)|null|在城市化的背景下，城市更新面临着重大挑战，需要适应性方法来应对不断变化的需求。利用大型语言模型（LLMs）的进步，我们提出了循环城市规划（CUP），这是一种新的范式，它在一个闭环中持续生成、评估和优化城市计划。具体来说，我们的基于多智能体LLM的框架包括三个关键组成部分：（1）规划，其中LLM智能体根据上下文数据生成和优化城市计划；（2）生活，其中智能体模拟居民的行为和互动，模拟城市环境中的生活；以及（3）评判，涉及评估计划的有效性并提供迭代反馈以改进。循环过程使得规划方法能够动态和灵活地响应。在真实世界数据集上的实验证明了我们框架作为持续和自适应规划过程的有效性。|\n",
        "2412.20297": "|**2024-12-28**|**FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration**|Jia Liu et.al.|[2412.20297](http://arxiv.org/abs/2412.20297)|null|近年来，大型语言模型（LLMs）在增强具身智能代理推理能力方面取得了进展，推动了AGI驱动的机器人技术的发展。虽然LLMs已应用于语义推理和任务泛化等任务，但其在开放物理空间探索中的潜力仍被低估。本文介绍了FaGeL（由LLMs赋能的具身智能代理），这是一种集成了智能织物技术的具身智能代理，可实现无缝、非侵入式的人机交互。FaGeL能够根据可穿戴和环境传感器的多模态数据自主生成任务，并通过生成文本中的隐含人类反馈来优化其行为，无需明确的评分或偏好。我们还引入了一种标记级显著性图来可视化LLM微调，增强了标记级对齐的可解释性。该系统利用双重反馈机制来提高标记级对齐，并解决非侵入式人机交互和认知演变的挑战。我们的贡献包括FaGeL的开发、用于AI对齐的DualCUT算法，以及在合作任务中的实验验证，证明了FaGeL能够通过隐含反馈自主适应和进化。未来，我们计划探索FaGeL在动态环境中的可扩展性和其与其他AI系统的集成，以开发能够无缝适应多样化人类需求的AGI智能代理。|\n",
        "2412.20005": "|**2024-12-28**|**OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System**|Yujie Luo et.al.|[2412.20005](http://arxiv.org/abs/2412.20005)|**[link](https://github.com/zjunlp/oneke)**|**我们介绍了OneKE，一个基于Docker的方案引导知识提取系统，它可以从网络和原始PDF书籍中提取知识，并支持多个领域（如科学、新闻等）。具体来说，我们为OneKE设计了多个代理和一个可配置的知识库。不同的代理执行各自的职责，从而支持各种提取场景。可配置的知识库简化了方案配置、错误情况调试和修正，进一步提升了性能。在基准数据集上的实证评估证明了OneKE的有效性，而案例研究进一步阐明了其在多个领域多样化任务中的适应性，突显了其广泛应用的潜力。我们已在https://github.com/zjunlp/OneKE开源了代码，并在http://oneke.openkg.cn/demo.mp4发布了视频演示。**|\n"
    },
    "llm": {
        "2411.18620": "|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|null|近期，自回归多模态大型语言模型（MLLMs）在视觉语言任务上的进展展现出令人鼓舞的成果。虽然已有多种研究探讨大型语言模型内部语言信息的处理，但目前对MLLM的内部工作机制以及语言和视觉信息在这些模型中如何互动的了解甚少。在本研究中，我们旨在通过考察MLLM中不同模态（语言和视觉）之间的信息流，特别是聚焦于视觉问答任务，来填补这一空白。具体来说，给定一个图像-问题对作为输入，我们研究在模型中视觉和语言信息是如何结合以生成最终预测的。通过对LLaVA系列中的一系列模型进行实验，我们发现两个模态的整合过程中存在两个不同的阶段。在底层，模型首先将整个图像的更一般化的视觉特征转移到（语言）问题标记的表示中。在中层，它再次将与问题相关的特定物体的视觉信息转移到问题的相应标记位置。最后，在高层，最终的多模态表示被传播到输入序列的最后位置进行最终预测。总体而言，我们的发现为MLLM中图像和语言处理的时空方面提供了新的全面视角，从而有助于未来对多模态信息定位和编辑的研究。|\n",
        "2411.18583": "|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出了并比较了多种利用自然语言处理（NLP）技术和检索增强生成（RAG）与大型语言模型（LLM）来自动生成文献综述的方法。研究论文数量的不断增长为手动文献综述带来了巨大挑战，进而推动了自动化需求。本研究的主要目标是开发一个能够仅从PDF文件输入自动生成文献综述的系统。为了实现这一目标，评估了多种自然语言处理（NLP）策略的有效性，包括基于频率的方法（spaCy）、变换器模型（Simple T5）以及与大型语言模型（GPT-3.5-turbo）结合的检索增强生成（RAG）。选择SciTLDR数据集进行实验，并利用三种不同的技术实现三个不同的系统来自动生成文献综述。使用ROUGE分数对所有三个系统进行评估。根据评估结果，大型语言模型GPT-3.5-turbo实现了最高的ROUGE-1分数，为0.364。变换器模型排名第二，spaCy排名最后。最后，为基于大型语言模型的最佳系统创建了一个图形用户界面。|\n",
        "2411.18571": "|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）展示了令人瞩目的多语言能力，但在为低资源语言调整这些模型时仍存在挑战。在本研究中，我们调查了低秩调整（LoRA）参数高效微调（PEFT）对马哈拉施特拉语Gemma多语言模型的影响，马哈拉施特拉语是一种资源有限的语种。使用含有52,000条指令-响应对的翻译Alpaca数据集，我们的研究发现，尽管评估指标通常显示在微调后性能下降，但手动评估通常表明微调后的模型优于其原始版本。观察表明，在语言适应后，目标语言生成能力有所提高，但推理能力有所下降。这些结果强调了改进评估方法以及创建高质量的本语种数据集的必要性，以便准确评估低资源环境中的语言特定模型性能。|\n",
        "2411.18564": "|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各种任务上展现出了令人印象深刻的性能。然而，LLMs在空间推理方面往往存在困难，而空间推理是推理和推断的一个重要部分，需要理解空间中物体之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们在两个基准数据集——StepGame和SparQA上评估了我们的方法，并实施了三种不同的策略：（1）基于ASP（答案集编程）的符号推理，（2）使用DSPy的LLM + ASP管道，以及（3）事实+逻辑规则。我们的实验表明，与基线提示方法相比，我们的方法在StepGame数据集上实现了40-50%的准确性提升，在更复杂的SparQA数据集上实现了3-13%的提升。特别是“LLM + ASP”管道在寻找关系（FR）和寻找块（FB）任务上取得了特别强的结果，尽管不同类型问题的性能有所差异。令人印象深刻的结果表明，虽然神经符号方法为增强LLMs的空间推理提供了有希望的方向，但它们的有效性在很大程度上取决于具体任务特性和实施策略。我们提出了一套集成的、简单而有效的策略，使用神经符号管道来提升LLMs的空间推理能力。这个管道及其策略在LLMs的推理领域具有广泛的适用性，如时间推理、演绎推理等。|\n",
        "2411.18562": "|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|在高级机器人中，具有丰富接触交互的灵活操作至关重要。尽管基于扩散的规划方法在简单的操作任务中显示出希望，但它们往往会产生不切实际的幽灵状态（例如，物体在没有手接触的情况下自动移动）或在处理复杂的顺序交互时缺乏适应性。在这项工作中，我们介绍了DexDiffuser，这是一个用于自适应灵活操作的认知扩散规划框架。DexDiffuser通过一个双阶段扩散过程来模拟关节状态动作动力学，该过程包括预接触接触对齐和接触后的目标导向控制，从而实现目标自适应的通用灵活操作。此外，我们结合了基于动力学模型的二元指导和利用大型语言模型进行自动指导函数生成，增强了对物理交互的泛化能力，并通过语言提示促进多样化的目标适应。在物理交互任务（如开门、笔和块重新定位和锤子敲钉）上的实验证明了DexDiffuser在训练分布之外的目标上的有效性，其成功率超过现有方法的平均成功率（59.2%比29.5%）。我们的框架在30度开门任务上达到70.0%的成功率，在笔和块半侧重新定位任务上分别达到40.0%和36.7%，在锤子敲钉半驱动任务上达到46.7%，突出了其在富含接触的操控中的鲁棒性和灵活性。|\n",
        "2411.18553": "|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前的语言模型（LMs）通常使用固定、静态的子词分词器。这种选择往往被视为理所当然，通常会导致在英语以外的语言中效率降低和功能受限，同时也使得将LMs应用于新的领域或语言变得具有挑战性。为了解决这些问题，我们提出对LMs进行动态分词改造：一种根据输入文本动态决定分词边界的方法。对于编码器风格的模型，我们引入了一种受字节对编码（BPE）启发的子词合并算法，但它在批处理级别上工作。我们在批处理中合并频繁的子词序列，然后应用预训练的嵌入预测超网络实时计算分词嵌入。当与词级边界结合使用时，这在XNLI上的XLM-R模型中平均将分词序列长度减少了>20%，同时任务性能下降不到2%。对于解码器风格的模型，我们以两种方式应用动态分词：1）用于预填充，几乎完全保持Mistral-7B的性能，同时相对于词级减少了高达40%的序列长度；2）通过近似最近邻索引，实现快速生成，并使用一百万个词元的词汇量，展示了扩展到甚至更大、更动态的词汇表的能力。总的来说，我们的研究结果表明，动态分词显著提高了推理速度，并促进了语言间的公平性，向克服静态分词的局限性迈出了重要一步，使LMs更加公平和适应性强。|\n",
        "2411.18530": "|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种数学框架，用于在人工智能（AI）系统中定义和量化自我认同，填补了人工意识理论基础的critical gap。尽管现有的关于人工自我意识的方法通常依赖于启发式实现或哲学抽象，但我们提出了一种以度量空间理论、测度理论和泛函分析为基础的正式框架。我们的框架认为，自我认同源于两个可数学量化的条件：在度量空间$(\\mathcal{M}, d_{\\mathcal{M}})$中存在一个连通的连续记忆集$C \\subseteq \\mathcal{M}$，以及一个连续映射$I: \\mathcal{M} \\to \\mathcal{S}$，它在这个连续集上保持一致的自我识别，其中$(\\mathcal{S}, d_{\\mathcal{S}})$代表可能自我认同的度量空间。为了验证这个理论框架，我们使用Llama 3.2 1B模型进行了实证实验，采用低秩适配（LoRA）进行高效的微调。该模型在一个包含时序结构记忆的合成数据集上进行了训练，旨在捕捉连贯自我认同形成的复杂性。我们的评估指标包括自我意识、响应一致性和语言精确性的量化度量。实验结果表明，可测量的自我意识指标有显著提高，主要自我意识分数从0.276提高到0.801。这使得可以结构化地创建具有经过验证的自我认同特征的AI系统。本研究的影响对类人机器人学和自主系统领域具有直接相关性。**|\n",
        "2411.18506": "|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|在之前的研究中，大型语言模型（LLMs）在处理时间序列方面的成功已经得到证明。利用符号时间序列表示，可以有效地在LLMs和时间序列之间架起桥梁。然而，剩余的挑战是如何利用符号或LLMs现有标记中的时间序列隐含语义信息，同时根据时间序列的隐含信息调整LLMs的嵌入空间。名为自适应布朗桥符号聚合（ABBA）的符号时间序列近似（STSA）方法，通过以振幅和周期来建模时间序列模式，同时使用LLMs的现有标记，在保留显著时间序列特征方面表现出卓越的功效。在本文中，我们介绍了一种方法，称为LLM-ABBA，该方法将ABBA整合到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中，与最近最先进的（SOTA）方法相比具有优势。同时，在ABBA中引入了固定多边形链技巧，通过显著减轻从符号到数值转换过程中由于符号误用而产生的累积误差的影响，来避免预测任务中的明显漂移。在时间序列回归任务中，LLM-ABBA在时间序列外部回归（TSER）基准测试上实现了新的SOTA。与最近SOTA的时间序列预测结果相比，LLM-ABBA也显示了具有竞争力的预测能力。我们相信这个框架也可以无缝地扩展到其他时间序列任务。|\n",
        "2411.18499": "|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交织的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。虽然统一模型的进展提供了新的解决方案，但现有的基准由于数据量和多样性限制，不足以评估这些方法。为了填补这一差距，我们介绍了GATE OpenING（OpenING），这是一个包含5,400个高质量人工标注实例、涵盖56个真实世界任务的全面基准。OpenING覆盖了多样化的日常场景，如旅行指南、设计和头脑风暴，为挑战交织生成方法提供了一个强大的平台。此外，我们提出了IntJudge，这是一个用于评估开放式多模态生成方法的评判模型。使用新颖的数据流水线进行训练，我们的IntJudge与人类判断的吻合率达到82.42%，比基于GPT的评估器高出11.34%。在OpenING上的大量实验表明，当前的交织生成方法仍有很大的改进空间。关于交织图像-文本生成的关键发现进一步提出，以指导下一代模型的发展。OpenING已开源，请访问https://opening.github.io。|\n",
        "2411.18478": "|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|在上下文学习（ICL）中，通过复杂的提示和高质量演示，使大型语言模型（LLMs）能够处理下游任务。然而，当面对复杂的数学推理任务时，这种传统的ICL范式显示出局限性，主要是因为它对示例质量的依赖性很大，以及在挑战性场景中需要人类干预。为了解决这些局限性，本文提出了一种HiAR-ICL，这是一种在ICL中的高级自动推理范式，它将焦点从具体示例转移到抽象思维模式，扩展了ICL中传统的上下文概念。HiAR-ICL引入了五个原子推理动作作为构建链式模式的根本组成部分。使用蒙特卡洛树搜索，我们探索推理路径并构建思维卡片来指导后续推理。然后我们开发了一个认知复杂度框架，该框架动态地将问题与适当的思想卡片相匹配。实验结果表明，HiAR-ICL的有效性，使用Qwen2.5-7B-Instruct在MATH基准测试中实现了最先进的准确率（79.6%），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|\n",
        "2411.19951": "|**2024-11-29**|**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**|Shukang Yin et.al.|[2411.19951](http://arxiv.org/abs/2411.19951)|**[link](https://github.com/xjtupanda/t2vid)**|**多模态大型语言模型（MLLMs）在图像领域的成功引起了研究界的广泛关注。借鉴以往的成功经验，研究人员最近探索将这一成功扩展到视频理解领域。除了从头开始训练外，一种高效的方法是利用预训练的图像-LLMs，从而产生了两种主流方法，即零样本推理和基于视频数据的进一步微调。在这项工作中，我们对这些方法的研究得出了一种有效的数据增强方法。我们首先对零样本推理方法进行了更深入的检查，并识别出两个限制，即泛化能力有限和缺乏时间理解能力。因此，我们进一步研究了微调方法，并发现当简单使用所有视频数据样本时，学习效率较低，这可以归因于指令多样性的缺乏。针对这个问题，我们开发了一种称为T2Vid的方法，用于生成类似视频的样本，以丰富训练语料库中的指令多样性。整合这些数据使得训练方案既简单又高效，通过仅用15%的样本量进行训练，就能达到与使用完整视频数据集相当甚至更好的性能。同时，我们发现所提出的方案可以在不使用长视频样本的情况下提升长视频理解性能。我们希望我们的研究能够激发更多关于使用MLLMs进行视频理解和高质量数据管理的思考。代码已发布在https://github.com/xjtupanda/T2Vid。**|\n",
        "2411.19943": "|**2024-11-29**|**Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability**|Zicheng Lin et.al.|[2411.19943](http://arxiv.org/abs/2411.19943)|null|大型语言模型（LLMs）在推理任务上表现出色。它们通过自回归标记生成来构建推理轨迹，从而发展出一套连贯的思维链条。在本工作中，我们探讨了单个标记对推理任务最终结果的影响。我们发现了“关键标记”的存在，这些标记会导致LLMs中产生错误的推理轨迹。具体来说，我们发现当LLMs被强迫解码其他标记而不是关键标记时，往往会产生积极的结果。受此启发，我们提出了一种新的方法——cDPO，旨在在对齐过程中自动识别和执行对关键标记的标记级奖励。具体来说，我们开发了一种对比估计方法来自动识别关键标记。这是通过比较正负模型的生成可能性来实现的。为此，我们分别对正负模型在不同推理轨迹上进行微调，从而使它们能够识别出导致错误结果的错误轨迹中的关键标记。此外，为了在对齐过程中进一步使模型与关键标记信息对齐，我们将传统的DPO算法扩展到标记级DPO，并利用上述正负模型之间的差异似然作为标记级DPO学习的重要权重。在GSM8K和MATH500基准测试中，使用两个广泛使用的模型Llama-3（8B和70B）和deepseek-math（7B）进行的实验结果表明，所提出的cDPO方法的有效性。|\n",
        "2411.19939": "|**2024-11-29**|**VLSBench: Unveiling Visual Leakage in Multimodal Safety**|Xuhao Hu et.al.|[2411.19939](http://arxiv.org/abs/2411.19939)|null|多模态大型语言模型（MLLMs）的安全性担忧在各个应用领域逐渐成为了一个重要问题。令人惊讶的是，以往的研究指出了一种反直觉的现象，即使用文本未学习（textual unlearning）来调整MLLMs，其安全性表现与使用图文对（image-text pairs）训练的MLLMs相当。为了解释这一反直觉的现象，我们发现在现有的多模态安全基准中存在视觉安全信息泄露（VSIL）问题，即图像中潜在的风险和敏感内容在文本查询中已经暴露出来。这样一来，MLLMs可以轻易地根据文本查询拒绝这些敏感的图文查询。然而，在现实场景中，没有VSIL的图文对很常见，而被现有的多模态安全基准所忽视。为此，我们构建了多模态视觉无泄露安全基准（VLSBench），该基准包含2.4k个图文对，旨在防止视觉安全信息从图像泄露到文本查询。实验结果表明，VLSBench对开源和闭源MLLMs，包括LLaVA、Qwen2-VL、Llama3.2-Vision和GPT-4o，都提出了显著挑战。这项研究证明了在存在VSIL的多模态安全场景中，文本对齐就足够了，而对于没有VSIL的多模态安全场景，多模态对齐则是一个更有前途的解决方案。请参阅我们的代码和数据：http://hxhcreate.github.io/VLSBench|\n",
        "2411.19930": "|**2024-11-29**|**On Domain-Specific Post-Training for Multimodal Large Language Models**|Daixuan Cheng et.al.|[2411.19930](http://arxiv.org/abs/2411.19930)|null|近年来，通用多模态大型语言模型（MLLMs）的发展迅速。然而，将通用MLLMs应用于特定领域，如科学领域和工业应用，仍鲜有探索。本文系统地通过后训练研究MLLMs的领域自适应，重点关注数据合成、训练流程和任务评估。（1）数据合成：利用开源模型，我们开发了一个视觉指令合成器，能有效从特定领域的图像-描述对生成多样化的视觉指令任务。我们的合成任务在增强MLLMs领域特定性能方面优于手动规则、GPT-4和GPT-4V生成的任务。（2）训练流程：虽然两阶段训练——最初在图像-描述对上进行，然后进行视觉指令任务——是开发通用MLLMs的常用方法，但我们采用单阶段训练流程来增强领域特定后训练的任务多样性。（3）任务评估：我们通过对不同来源和规模（例如，Qwen2-VL-2B，LLaVA-v1.6-8B，Llama-3.2-11B）的MLLMs进行后训练，在生物医药和食品两个领域进行实验，然后评估MLLMs在各种领域特定任务上的性能。为了支持MLLMs领域自适应的进一步研究，我们将开源我们的实现。|\n",
        "2411.19921": "|**2024-11-29**|**SIMS: Simulating Human-Scene Interactions with Real World Script Planning**|Wenjia Wang et.al.|[2411.19921](http://arxiv.org/abs/2411.19921)|null|模拟长期人景交互是一项既具挑战性又充满吸引力的任务。以往的研究并未有效地解决基于物理动画的长期人景交互生成带有详细叙述的问题。本文介绍了一种新的框架，用于规划和控制长期物理可能的人景交互。一方面，互联网上充斥着风格独特的人类运动或与场景交互的影视作品，为剧本规划提供了丰富的数据来源。另一方面，大型语言模型（LLMs）能够理解和生成逻辑故事线。这促使我们结合两者，通过基于LLM的流程从视频中提取剧本，然后利用LLMs模仿和创作新的剧本，捕捉复杂的时间序列人类行为和环境交互。通过这种方式，我们利用一种双重感知策略，在语境和空间约束下指导角色动作，实现了语言理解和场景理解。为了便于训练和评估，我们贡献了一个包含从现实世界视频中提取的多样运动序列的综合规划数据集，并使用大型语言模型对其进行扩展。我们还收集并重新标注了来自现有运动学数据集的运动片段，以使我们的策略能够学习多种技能。广泛的实验证明了我们的框架在多种任务执行中的有效性及其对各种场景的泛化能力，与现有方法相比，性能显著提升。我们的代码和数据将很快公开。|\n",
        "2411.19886": "|**2024-11-29**|**PDDLFuse: A Tool for Generating Diverse Planning Domains**|Vedant Khandelwal et.al.|[2411.19886](http://arxiv.org/abs/2411.19886)|null|各种现实世界挑战需要能够适应广泛领域的规划算法。传统上，规划域的创建高度依赖于人工实现，这限制了可用的域的规模和多样性。尽管最近的研究利用了生成式人工智能技术，如大型语言模型（LLM）进行域创建，但这些努力主要集中在将现有域从自然语言描述中翻译出来，而不是生成新的域。相比之下，域随机化的概念，在强化学习中已被证明非常有效，通过在多样化的随机新域上进行训练，提高了性能和泛化能力。受此成功启发，我们的工具PDDLFuse旨在弥合规划域定义语言（PDDL）中的这一差距。PDDLFuse被设计用来生成新的、多样化的规划域，这些域可以用于验证新的规划器或测试基础规划模型。我们已经开发出了调整域生成器参数的方法，以调节其生成的域的难度。这种适应性至关重要，因为现有的域无关规划器往往难以处理更复杂的问题。初步测试表明，PDDLFuse能够高效地创建复杂且多样化的域，这比传统的域生成方法有显著的进步，并为规划研究做出了贡献。|\n",
        "2411.19876": "|**2024-11-29**|**LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states**|Luis Ibanez-Lissen et.al.|[2411.19876](http://arxiv.org/abs/2411.19876)|null|大型语言模型（LLMs）在各类应用中越来越受欢迎，但关于成员推断（Membership Inference）的担忧也随之增长。以往的研究主要关注黑盒到灰盒模型，从而忽略了内部LLM信息的潜在益处。为了解决这个问题，我们提出使用线性探针（LPs）作为一种检测成员推断攻击（MIAs）的方法，通过检查LLM的内部激活来实现。我们的方法被称为LUMIA，它逐层应用LPs以获取模型内部运作的细粒度数据。我们在包括单模态和多模态任务在内的多个模型架构、规模和数据集上测试了这种方法。在单模态MIAs中，LUMIA在曲线下面积（AUC）上比之前的技术平均提高了15.71%。值得注意的是，LUMIA在65.33%的情况下达到了AUC>60%——相较于现有技术提高了46.80%。此外，我们的方法揭示了关键见解，例如MIAs最易检测的模型层。在多模态模型中，LPs表明视觉输入可以显著有助于检测MIAs——在85.90%的实验中达到了AUC>60%。|\n",
        "2411.19869": "|**2024-11-29**|**AIDetx: a compression-based method for identification of machine-learning generated text**|Leonardo Almeida et.al.|[2411.19869](http://arxiv.org/abs/2411.19869)|**[link](https://github.com/aidetx/aidetx)**|**本文介绍了一种名为AIDetx的新方法，该方法利用数据压缩技术检测机器生成的文本。传统的深度学习分类器通常存在计算成本高和可解释性有限的问题。为了解决这些局限性，我们提出了一种基于压缩的分类框架，该框架利用有限上下文模型（FCMs）。AIDetx为人工写作和AI生成的文本构建了不同的压缩模型，根据哪个模型达到更高的压缩率来对新输入进行分类。我们在两个基准数据集上评估了AIDetx，分别实现了超过97%和99%的F1分数，突显了其高准确性。与当前方法，如大型语言模型（LLMs）相比，AIDetx提供了一个更可解释且计算效率更高的解决方案，显著减少了训练时间和硬件需求（例如，不需要GPU）。完整的实现代码在https://github.com/AIDetx/AIDetx上公开可用。**|\n",
        "2411.19865": "|**2024-11-29**|**Reverse Thinking Makes LLMs Stronger Reasoners**|Justin Chih-Yao Chen et.al.|[2411.19865](http://arxiv.org/abs/2411.19865)|null|逆向思维在人类推理中起着至关重要的作用。人类不仅能从问题推理到解决方案，还能逆向推理，即从解决方案开始推理到问题。这种推理方式往往能提升整体推理性能，因为它使得他们的正向和逆向思维之间能够进行一致性检查。为了使大型语言模型（LLMs）能够进行逆向思维，我们引入了逆向增强思维（RevThink）框架，该框架由数据增强和学习目标组成。在RevThink中，我们通过收集来自教师模型的有序正向-逆向推理来增强数据集，包括：（1）原始问题，（2）正向推理，（3）逆向问题，和（4）逆向推理。然后，我们采用三个目标以多任务学习的方式训练一个较小的学生模型：（a）从问题中生成正向推理，（b）从问题中生成逆向问题，（c）从逆向问题中生成逆向推理。在涵盖常识、数学和逻辑推理的12个数据集上的实验表明，与学生的零样本性能相比平均提升了13.53%，与最强的知识蒸馏基线相比提升了6.84%。此外，我们的方法展示了样本效率——仅使用训练数据中10%的正确正向推理，它就能超越在10倍更多正向推理上训练的标准微调方法。RevThink还显示出对分布外持有数据集的强大泛化能力。|\n",
        "2411.19862": "|**2024-11-29**|**Cross-Domain Recommendation Meets Large Language Models**|Ajay Krishna Vajjala et.al.|[2411.19862](http://arxiv.org/abs/2411.19862)|**[link](https://github.com/ajaykv1/CDR_Meets_LLMs)**|**跨领域推荐（CDR）已成为解决单领域推荐系统面临的冷启动问题的一个有希望的解决方案。然而，现有的CDR模型依赖于复杂的神经网络架构、大量数据集和大量的计算资源，这使得它们在数据稀缺的场景或当简单性至关重要的时效果较差。在这项工作中，我们利用大型语言模型（LLM）的推理能力，并探索其在多个领域对中的CDR领域的性能。我们引入了两种针对CDR的新型提示设计，并证明当LLM被有效提示时，在评分预测和排名任务中，LLM在各种指标和领域组合上优于最先进的CDR基线。这项工作弥合了LLM和推荐系统之间的差距，展示了它们作为有效的跨领域推荐者的潜力。**|\n",
        "2412.02685": "|**2024-12-03**|**T-REG: Preference Optimization with Token-Level Reward Regularization**|Wenxuan Zhou et.al.|[2412.02685](http://arxiv.org/abs/2412.02685)|null|基于人类反馈的强化学习（RLHF）对于将大型语言模型（LLMs）与人类价值观对齐至关重要。传统上，RLHF涉及生成对查询的响应，并使用奖励模型对整个响应分配奖励。然而，由于该方法依赖于单一且稀疏的奖励，这使得模型难以识别序列中哪些部分对最终奖励贡献最大。近期的方法试图通过引入token级奖励来解决这个问题。然而，这些方法通常依赖于训练好的信用分配模型或AI标注者，这引发了关于奖励质量和可靠性的担忧。在本文中，我们提出了token级奖励正则化（T-REG），这是一种利用序列级和token级奖励进行偏好优化的新方法。利用LLMs的自我改进能力，我们的方法使用对比提示，使LLMs能够自我生成token级奖励。这些自我生成的奖励随后充当奖励正则化，引导模型更有效地分配序列级奖励到各个token。这促进了更好的token级信用分配并提高了对齐性能。在包括Alpaca Eval 2和Arena-Hard在内的指令遵循基准测试中进行的实验表明，我们的方法在性能上分别比基线方法高出3.8%和4.4%。我们将发布代码和模型在https://github.com/wzhouad/T-REG上。|\n",
        "2412.02674": "|**2024-12-03**|**Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models**|Yuda Song et.al.|[2412.02674](http://arxiv.org/abs/2412.02674)|null|自我改进是大型语言模型（LLM）预训练、后训练和测试时推理中的一个机制。我们探索了一个框架，其中模型验证其自己的输出，根据这种验证过滤或重新加权数据，并提炼过滤后的数据。尽管已经取得了一些经验上的成功，但对其根本理解仍然不足。在这项工作中，我们开始对LLM自我改进进行全面的、模块化和受控的研究。我们为自我改进提供了一个数学公式，它主要受一个量控制，我们将该量形式化为生成-验证差距。通过使用各种模型家族和任务的实验，我们发现自我改进存在一个缩放现象——生成-验证差距的变体随着模型预训练的浮点运算量单调增长。我们还考察了自我改进何时可行，一个迭代自我改进过程以及提高其性能的方法。我们的发现不仅推进了对LLM自我改进的理解，具有实际意义，而且为未来对其能力和边界的研究开辟了众多途径。|\n",
        "2412.02655": "|**2024-12-03**|**LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs**|Pranav Doma et.al.|[2412.02655](http://arxiv.org/abs/2412.02655)|null|基于自然语言指令引导的自主导航对于改善人机交互和实现在动态环境中的复杂操作至关重要。尽管大型语言模型（LLMs）并非天生用于规划，但它们可以通过提供指导和告知约束来显著提高规划效率，以确保安全。本文介绍了一种规划框架，该框架将LLMs与二维占用栅格图和自然语言命令集成，以提高资源受限环境中的空间推理和任务执行。通过分解高级指令和实时环境数据，该系统为拾取和放置任务生成结构化的导航计划，包括避障、目标优先级和自适应行为。该框架动态重新计算路径以应对环境变化，并符合隐含的社会规范以实现无缝的人机交互。我们的结果表明，LLMs具有设计情境感知系统以增强工业和动态环境中的导航效率和安全的潜力。|\n",
        "2412.02626": "|**2024-12-03**|**Time-Reversal Provides Unsupervised Feedback to LLMs**|Yerram Varun et.al.|[2412.02626](http://arxiv.org/abs/2412.02626)|null|大型语言模型（LLMs）通常被训练来预测时间的正向方向。然而，最近的研究表明，通过提示这些模型回顾并批评它们自己的生成内容可以产生有用的反馈。受此启发，我们探讨了LLMs是否能够被赋予反向（预测和评分）思考的能力，以提供补充正向LLMs的无监督反馈。为此，我们引入了时间反转语言模型（TRLMs），当给定响应条件时，它们可以评分和生成查询，从而在时间反向方向上有效工作。此外，为了有效地推断查询到响应的方向，我们从零开始预训练和微调了一个语言模型（TRLM-Ba），使用反向标记顺序。我们通过实验（在一个风格化的环境中进行理论证明）表明，当用于根据响应对多个正向生成进行再排名时，时间反转模型确实可以补充正向模型的预测。我们在广泛使用的AlpacaEval排行榜上获得了高达5%的改进，超过了使用自我对数困惑度评分的N-best再排名的最佳基线。我们进一步表明，TRLM评分优于给查询响应的常规正向评分，在引用生成和段落检索等应用中带来了显著收益。接下来，我们利用TRLM的生成能力来增强或为LLM的输入安全过滤器提供无监督反馈，展示了在几项针对流行的JailbreakBench排行榜上发布的攻击中，错误否定率大幅降低，而对错误肯定率的影响可以忽略不计。|\n",
        "2412.02617": "|**2024-12-03**|**Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback**|Hiroki Furuta et.al.|[2412.02617](http://arxiv.org/abs/2412.02617)|null|大型文本到视频模型在众多下游应用中具有巨大潜力。然而，这些模型在准确描绘动态物体交互方面存在困难，往往导致动作不真实和频繁违反现实物理规律。一种受大型语言模型启发的解决方案是通过外部反馈将生成的输出与期望结果对齐。这使得模型能够自主地改进其响应，消除了大量手动数据收集的需要。在本工作中，我们研究了利用反馈来增强文本到视频模型中物体动态的方法。我们试图回答一个关键问题：哪些类型的反馈，与哪些特定的自我改进算法相结合，可以最有效地提高文本-视频对齐和现实物体交互？我们首先推导出用于文本到视频模型离线强化学习微调的统一概率目标。这种观点突出了如何在现有算法（如KL正则化和策略投影）的设计元素中，作为一个统一框架中的特定选择。然后，我们使用推导出的方法来优化一组文本-视频对齐指标（例如，CLIP分数、光流），但注意到它们往往无法与人类对生成质量的感知相一致。为了解决这一限制，我们提出利用视觉语言模型提供更细致的反馈，特别是针对视频中的物体动态。我们的实验表明，我们的方法可以有效地优化各种奖励，二元AI反馈驱动视频质量动态交互方面的最显著改进，这一点通过AI和人类评估都得到了证实。值得注意的是，当我们使用从AI反馈中导出的奖励信号时，尤其是在涉及多个物体复杂交互和物体坠落等现实描绘的情景中，我们观察到了显著的收益。|\n",
        "2412.02611": "|**2024-12-03**|**AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?**|Kaixiong Gong et.al.|[2412.02611](http://arxiv.org/abs/2412.02611)|null|近期，多模态大型语言模型（MLLMs），如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其功能，包括视觉和听觉模态。虽然这些模型在广泛的视听应用中表现出令人印象深刻的能力，但我们的DeafTest研究表明，MLLMs在人类认为简单的任务上往往表现不佳：1）判断两个声音中哪个更响亮，2）判断两个声音中哪个音调更高。受这些观察的启发，我们引入了AV-Odyssey Bench，这是一个综合性的视听基准，旨在评估这些MLLMs是否真正理解视听信息。该基准包含4,555个精心设计的问题，每个问题都融合了文本、视觉和听觉成分。为了成功推断答案，模型必须有效地利用视觉和听觉输入中的线索。为了确保对MLLM响应的精确和客观评估，我们将问题设计为多项选择，从而消除了人工评估或LLM辅助评估的需求。我们对一系列闭源和开源模型进行了基准测试，并总结了观察结果。通过揭示当前模型的局限性，我们旨在为未来的数据集收集和模型开发提供有用的见解。|\n",
        "2412.02605": "|**2024-12-03**|**Interpretable Company Similarity with Sparse Autoencoders**|Marco Molinari et.al.|[2412.02605](http://arxiv.org/abs/2412.02605)|null|在金融领域，确定公司相似性是一项至关重要的任务，它支撑着对冲、风险管理、投资组合多元化等多个方面。从业者通常依赖行业和产业分类来衡量相似性，例如SIC代码和GICS代码，前者由美国证券交易委员会（SEC）使用，后者在投资界得到广泛应用。将公司描述的嵌入进行聚类已被提出作为一种确定公司相似性的潜在技术，但标记嵌入的可解释性缺乏对在高风险环境下应用构成了重大障碍。稀疏自动编码器（Sparse Autoencoders，SAE）在通过分解大型语言模型（LLM）的激活为可解释特征来增强LLM的可解释性方面已显示出希望。在本文中，我们探讨了使用SAE特征来衡量公司相似性，并将它们与（1）SIC代码和（2）主要群体代码进行了基准测试。我们得出结论，SAE特征可以复制甚至超越行业分类，在量化公司基本特征方面，通过衡量月度收益的相关性（相似性的代理指标）和协整的损益（PnL）来实现。|\n",
        "2412.02602": "|**2024-12-03**|**CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs**|Abhas Kumar et.al.|[2412.02602](http://arxiv.org/abs/2412.02602)|null|本文分析了小型语言模型（SLMs）和视觉语言模型（VLMs）的性能，并评估了模型性能与碳排放之间的权衡，涉及4项基本任务：图像描述、视觉问答（VQA）、对话摘要和文本到SQL转换。选取了属于Qwen和LLaMA架构家族的各种SLMs和VLMs，并评估了基于模型大小（参数数量、量化级别和微调参数）的变体。计算了模型变体的性能和碳排放。为了量化模型性能与碳排放之间的权衡，我们引入了一个新的指标，称为CEGI（碳效率增益指数）。这个指标表示每百万可训练参数单位百分比增益的碳排放。这个指标提供了一个标准化的度量，用于比较模型在性能改进相对于其环境成本方面的效率。实验结果表明，微调SLMs和VLMs可以达到与大语言模型（LLMs）相当的性能水平，同时产生显著较少的碳排放。我们的研究结果表明，从更大模型中获得的边际准确率增益并不能证明其碳排放的大幅增加是合理的。利用较低的位量化级别，所提出的指标进一步提高了能源效率，同时没有影响性能。这项研究突出了在高性能和环境可持续性之间取得平衡的重要性。它为选择适合环保AI开发的模型提供了一个有价值的指标。|\n",
        "2412.02594": "|**2024-12-03**|**PrefixLLM: LLM-aided Prefix Circuit Design**|Weihua Xiao et.al.|[2412.02594](http://arxiv.org/abs/2412.02594)|null|前缀电路是数字加法器的基本组件，由于它们在计算进位信号方面的效率，在数字系统中得到广泛应用。合成最小化面积和延迟的前缀电路对于提升现代计算机系统的性能至关重要。最近，大型语言模型（LLMs）在执行文本生成任务方面展现出了令人惊讶的能力。我们提出了PrefixLLM，它利用LLMs进行前缀电路的合成。PrefixLLM将前缀电路合成任务转化为一种结构化文本生成问题，称为结构化前缀电路表示（SPCR），并引入了一个迭代框架来自动准确地生成有效的SPCRs。我们进一步提出了一种设计空间探索（DSE）框架，该框架使用LLMs迭代搜索面积和延迟优化的前缀电路。与现有技术相比，PrefixLLM在相同的延迟约束下可以将面积降低3.70%。这项工作突出了LLMs在算术电路合成中的应用，这些应用可以转化为结构化文本生成。|\n",
        "2412.02592": "|**2024-12-03**|**OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation**|Junyuan Zhang et.al.|[2412.02592](http://arxiv.org/abs/2412.02592)|**[link](https://github.com/opendatalab/OHR-Bench)**|**检索增强生成（RAG）通过整合外部知识来增强大型语言模型（LLMs），以减少幻觉并吸收最新信息而不需要重新训练。作为RAG的一个重要部分，外部知识库通常通过使用光学字符识别（OCR）从非结构化的PDF文档中提取结构化数据来构建。然而，由于OCR预测的不完美以及结构化数据固有的非均匀表示，知识库不可避免地包含各种OCR噪声。在本文中，我们介绍了OHRBench，这是第一个用于理解OCR对RAG系统级联影响的基准。OHRBench包括从六个真实世界RAG应用领域精心挑选的350个非结构化PDF文档，以及从文档中的多模态元素中衍生出的问答，挑战了现有用于RAG的OCR解决方案。为了更好地理解OCR对RAG系统的影响，我们确定了两种主要的OCR噪声类型：语义噪声和格式噪声，并应用扰动生成了一系列具有不同程度每种OCR噪声的结构化数据。使用OHRBench，我们首先对当前的OCR解决方案进行了全面评估，并揭示了没有一种方案能够为RAG系统构建高质量的知识库。然后，我们系统地评估了这两种噪声类型的影响，并展示了RAG系统的脆弱性。此外，我们讨论了在RAG系统中不使用OCR而采用视觉-语言模型（VLMs）的潜力。代码：https://github.com/opendatalab/OHR-Bench**|\n",
        "2412.03563": "|**2024-12-04**|**From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**|Xinyi Mou et.al.|[2412.03563](http://arxiv.org/abs/2412.03563)|**[link](https://github.com/fudandisc/socialagent)**|传统的社会学研究通常依赖人类参与，虽然有效，但成本高昂、难以扩展，且存在伦理问题。近年来，大型语言模型（LLMs）的进步突显了它们模拟人类行为的能力，使得个体反应的复制和跨学科研究得以进行。在本文中，我们对这一领域进行了全面调查，展示了由LLMs赋能的代理推动的模拟近期进展。我们将模拟分为三类：（1）个体模拟，模仿特定个体或人口群体；（2）情景模拟，多个代理在特定情境中协作实现目标；（3）社会模拟，模拟代理社会中的互动，以反映现实世界动态的复杂性和多样性。这些模拟从详细的个体建模到大规模社会现象，呈现出一种渐进性。我们对每种模拟类型进行了详细讨论，包括模拟的架构或关键组件、目标或情景的分类以及评估方法。之后，我们总结了常用的数据集和基准。最后，我们讨论了这三种类型模拟的趋势。相关资源的存储库位于{\\url{https://github.com/FudanDISC/SocialAgent}}。|\n",
        "2412.03551": "|**2024-12-04**|**SPICE: Smart Projection Interface for Cooking Enhancement**|Vera Prohaska et.al.|[2412.03551](http://arxiv.org/abs/2412.03551)|null|可触摸用户界面（TUI）用于人机交互（HCI），旨在向用户提供数字信息的物理表示，以克服基于屏幕界面的局限性。尽管文献中存在许多引人注目的TUI演示，但针对日常双手任务和过程，如烹饪的TUI研究却很少。为了填补这一空白，我们提出了SPICE（智能投影界面，用于烹饪增强）。SPICE在厨房环境中研究TUI，旨在将食谱遵循体验从简单的基于文本转变为直观互动。SPICE包括跟踪系统、基于代理的软件和视觉大型语言模型，以创建和解释一个将食谱信息直接投影到烹饪表面的厨房环境。我们对SPICE和基于文本的食谱遵循进行了30名参与者的比较可用性研究，评估了任务难度、总时长和效率，以及用户信心和味觉感知。结果表明，SPICE使参与者能够在更短的时间内完成食谱，同时提高了自我报告的效率、信心和味觉。尽管如此，参与者报告说总体难度没有变化，这是未来研究的方向。总的来说，SPICE项目展示了使用TUI改善日常活动的潜力，为HCI和新型计算界面的未来研究铺平了道路。|\n",
        "2412.03537": "|**2024-12-04**|**Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models**|Natalie Mackraz et.al.|[2412.03537](http://arxiv.org/abs/2412.03537)|null|大型语言模型（LLMs）正越来越多地被调整为具有特定任务性，以便在现实世界的决策系统中部署。先前的一些研究通过研究微调适配策略对模型公平性的影响，来调查偏见迁移假说（BTH），发现预训练的掩码语言模型在微调适配时的公平性影响有限。在本工作中，我们扩展了对BTH的研究，将其应用于提示适应下的因果模型，因为提示是一种易于访问且计算高效的部署模型的方法。与先前的研究不同，我们通过一个代词共指消解任务，建立了一个事实：预训练的Mistral、Falcon和Llama模型中的内在偏见与在相同模型零样本和少样本提示时的偏见高度相关（相关系数rho >= 0.94）。此外，我们发现，即使LLMs被特别提示以展示公平或偏见行为（rho >= 0.92），以及少样本长度和刻板化组成发生变化（rho >= 0.97），偏见迁移仍然高度相关。我们的发现强调了确保预训练LLMs公平性的重要性，特别是在它们后来通过提示适配执行下游任务时。|\n",
        "2412.03531": "|**2024-12-04**|**A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences**|Gabriel Lino Garcia et.al.|[2412.03531](http://arxiv.org/abs/2412.03531)|null|这篇论文回顾了大型语言模型（LLMs）在生物医学领域的最新应用，探讨了它们在自动化复杂任务，如从生物医学文献数据库中提取证据和数据方面的有效性。虽然LLMs展现出巨大的潜力，但仍然存在重大挑战，包括幻觉、上下文理解和跨多种医疗任务泛化能力的问题。我们指出了当前研究文献中的关键差距，尤其是需要统一的基准来标准化评估并确保实际应用中的可靠性。此外，我们提出了未来研究方向，强调将检索增强生成（RAG）等最先进技术集成到LLMs中，以提高证据综合性能。通过解决这些挑战并利用LLMs的优势，我们旨在提高获取医学文献的途径并促进医疗保健领域的重大发现。|\n",
        "2412.03527": "|**2024-12-04**|**FANAL -- Financial Activity News Alerting Language Modeling Framework**|Urjitkumar Patel et.al.|[2412.03527](http://arxiv.org/abs/2412.03527)|null|在快速发展的金融领域，准确及时地解读市场新闻对于需要应对不可预测事件的相关利益方至关重要。本文介绍了FANAL（金融活动新闻警报语言建模框架），这是一个专门为实时金融事件检测和分析而设计的基于BERT的框架，将新闻分为十二个不同的金融类别。FANAL利用通过XGBoost处理的银标签数据进行训练，并采用先进的微调技术，同时结合了ORBERT（概率比BERT），这是一种新的BERT变体，通过ORPO（概率比偏好优化）进行微调，以实现更高级别的类别概率校准和与金融事件相关性的对齐。我们评估了FANAL的性能，并将其与领先的顶级大型语言模型进行了比较，包括GPT-4o、Llama-3.1 8B和Phi-3，证明了其卓越的准确性和成本效益。这一框架为金融智能和响应性设定了新的标准，在性能和成本上均显著超越现有模型。|\n",
        "2412.03516": "|**2024-12-04**|**You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?**|Dominic Lohr et.al.|[2412.03516](http://arxiv.org/abs/2412.03516)|null|背景：反馈作为学习中最具影响力的因素之一，一直是众多研究的热点。它在教育技术系统的发展中起着关键作用，并传统上基于由专家及其经验定义的决定性反馈。然而，随着生成式AI，尤其是大型语言模型（LLMs）的兴起，我们预计作为学习系统一部分的反馈将发生转变，尤其是在编程的背景下。过去，为编程学习者自动生成反馈具有挑战性。LLMs可能创造新的可能性，提供比以往任何时候都更丰富、更个性化的反馈。  目标：本文旨在使用LLMs为入门级编程任务生成特定类型的反馈。我们重新审视现有的反馈分类法，以捕捉生成的反馈的具体性，例如随机性、不确定性和变化程度。  方法：我们针对真实的学生的程序，迭代设计用于生成特定类型反馈的提示（作为现有反馈分类法的一部分）。然后，我们评估生成的输出，并确定其反映特定反馈类型的程度。  结果和结论：本研究加深了对不同反馈维度和特性的理解。结果对未来的反馈研究有影响，例如关于反馈效果和学习者信息需求的研究。此外，本研究还为开发新的工具和学习系统提供了基础，包括由AI生成的反馈，这些系统面向初学者程序员。|\n",
        "2412.03467": "|**2024-12-04**|**Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning**|Neale Ratzlaff et.al.|[2412.03467](http://arxiv.org/abs/2412.03467)|null|多模态模型通常将强大的大型语言模型（LLM）与视觉编码器相结合，然后通过指令微调在多模态数据上训练。虽然这个过程使LLM适应了多模态环境，但尚不清楚这种适应是否会损害它们原始的语言推理能力。在本工作中，我们探讨了多模态指令微调对语言推理性能的影响。我们关注的是LLaVA，这是一个领先的融合了Vicuna或Mistral等LLM与CLIP视觉编码器的多模态框架。我们将原始LLM与它们的跨模态适应版本在八个语言推理任务中的表现进行了比较。我们的实验产生了几个关键见解。首先，多模态学习对Vicuna和Mistral的影响不同：我们在Mistral上观察到语言推理的下降，但在大多数任务上Vicuna有所改进。其次，尽管多模态指令学习在数学推理任务（例如GSM8K）上始终会降低性能，但它增强了常识推理任务（例如CommonsenseQA）的性能。最后，我们证明了无训练模型合并技术可以有效地减轻在多模态适应的Mistral中观察到的语言推理下降，甚至可以提高视觉任务的表现。|\n",
        "2412.03446": "|**2024-12-04**|**From Words to Workflows: Automating Business Processes**|Laura Minkova et.al.|[2412.03446](http://arxiv.org/abs/2412.03446)|null|随着企业越来越依赖自动化以简化运营，机器人流程自动化（RPA）的局限性逐渐显现，尤其是其依赖专家知识和无法处理复杂决策任务的问题。近年来，人工智能（AI）的进步，特别是生成式AI（GenAI）和大型语言模型（LLMs），为智能自动化（IA）铺平了道路，IA通过集成认知能力来克服RPA的不足。本文介绍了一种名为Text2Workflow的新方法，它可以从自然语言用户请求中自动生成工作流程。与传统的自动化方法不同，Text2Workflow提供了一种通用的解决方案，用于自动化任何业务流程，将用户输入转换为表示为JavaScript对象表示法（JSON）格式的可执行步骤序列。利用LLMs的决策和指令遵循能力，该方法提供了一种可扩展、可适应的框架，使用户能够以最小的手动干预可视化和执行工作流程。这项研究概述了Text2Workflow方法及其在自动化复杂业务流程方面的更广泛影响。|\n",
        "2412.03398": "|**2024-12-04**|**RedStone: Curating General, Code, Math, and QA Data for Large Language Models**|Yaoyao Chang et.al.|[2412.03398](http://arxiv.org/abs/2412.03398)|null|在高质量、精心挑选的数据集上预训练大型语言模型（LLMs）已被广泛认为对于提高其性能和泛化能力至关重要。本研究探讨了Common Crawl作为预训练LLMs的全面且灵活资源的未被充分利用的潜力，既针对通用语言理解也针对专业领域知识。我们引入了RedStone，这是一个创新且可扩展的管道，旨在从Common Crawl中提取和处理数据，便于创建广泛多样的预训练数据集。与传统的数据集不同，后者通常需要昂贵的编辑和特定领域的专业知识，RedStone利用Common Crawl的广度，提供针对广泛领域的定制化数据集。在本工作中，我们通过构建涵盖多个领域的预训练数据集来展示其能力，包括通用语言理解、代码、数学和问答任务。RedStone的灵活性允许它轻松适应其他专业领域，显著降低了创建有价值特定领域数据集的门槛。我们的发现表明，通过像RedStone这样的有效管道，Common Crawl可以作为丰富的、可再生的预训练数据源，为LLMs在领域适应和知识发现方面开辟新的途径。这项工作也强调了创新数据采集策略的重要性，并突出了网络规模数据在LLMs持续进化中的强大资源作用。RedStone代码和数据样本将公开提供在\\url{https://aka.ms/redstone}。|\n",
        "2412.03359": "|**2024-12-04**|**WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis**|Chengwei Hu et.al.|[2412.03359](http://arxiv.org/abs/2412.03359)|null|近期，基于大型语言模型（LLMs）的自主多智能体系统（MAS）的进步，增强了应用场景并提升了LLMs处理复杂任务的能力。尽管现有研究显示出有效性，但仍然明显存在评估、分析和复现LLM-based MAS的困难。在本文中，为了促进LLM-based MAS的研究，我们介绍了一个基于“谁是间谍？”（WiS）游戏的开放、可扩展和实时更新的平台，用于访问和分析基于LLMs的MAS。我们的平台具有三个主要优点：（1）支持Hugging Face上可用的模型的统一模型评估界面；（2）实时更新的排行榜用于模型评估；（3）全面评估包括游戏胜率、攻击、防御策略和LLMs的推理。为了严格测试WiS，我们进行了涵盖各种开源和闭源LLMs的广泛实验，我们发现不同的代理在游戏中表现出独特且引人入胜的行为。实验结果证明了我们的平台在评估LLM-based MAS中的有效性和效率。我们的平台及其文档可在\\url{https://whoisspy.ai/}公开访问。|\n",
        "2412.04449": "|**2024-12-05**|**p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**|Jun Zhang et.al.|[2412.04449](http://arxiv.org/abs/2412.04449)|**[link](https://github.com/mcg-nju/p-mod)**|**尽管多模态大型语言模型（MLLMs）在众多任务中表现出色，但其巨大的训练和推理成本阻碍了其发展。大部分计算量来自于被Transformer解码器处理的视觉标记的庞大数量。在本文中，我们提出通过利用混合深度（MoD）机制来构建高效的MLLMs，其中每个Transformer解码器层选择必要的视觉标记进行处理，同时跳过冗余的标记。然而，将MoD集成到MLLMs中并非易事。为了解决训练和推理稳定性以及有限训练数据带来的挑战，我们对MoD模块进行了两项创新设计：tanh门控权重归一化（TanhNorm）和对称标记重新加权（STRing）。此外，我们观察到视觉标记在深层中的冗余性更高，因此设计了一种渐进比率衰减（PRD）策略，该策略通过偏移余弦调度逐步减少每层的标记保留率。这一关键设计充分发挥了MoD的潜力，显著提升了我们模型的效率和性能。为了验证我们方法的有效性，我们在14个基准测试中，对两个基线模型进行了广泛的实验。我们的模型p-MoD在推理时仅占用了55.6%的TFLOPs和53.8%的KV缓存存储，以及训练时的77.7%的GPU小时，其性能与基线模型相当，甚至在某些情况下超过了基线模型。**|\n",
        "2412.04447": "|**2024-12-05**|**EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios**|Lu Qiu et.al.|[2412.04447](http://arxiv.org/abs/2412.04447)|null|多模态大型语言模型的兴起，借助大型语言模型的力量，最近展示了卓越的多模态理解和推理能力，预示着人工通用智能新时代的到来。然而，实现通用人工智能不仅需要理解和推理能力，还需要在多样场景中有效规划的能力，这涉及到基于复杂环境做出合理决策以解决现实问题。尽管其重要性不言而喻，但当前多模态大型语言模型在不同场景下的规划能力仍处于探索阶段。在本文中，我们介绍了EgoPlan-Bench2，这是一个严格且全面的基准，旨在评估多模态大型语言模型在广泛现实场景中的规划能力。EgoPlan-Bench2涵盖了涵盖4个主要领域和24个详细场景的日常任务，与人类日常生活紧密相关。EgoPlan-Bench2是通过半自动流程构建的，利用以自我为中心的视频，并辅以人工验证。基于第一人称视角，它反映了人类在日常生活中的问题解决方式。我们评估了21个竞争性的多模态大型语言模型，并深入分析了它们的局限性，揭示它们在现实世界规划中面临重大挑战。为了进一步提高当前多模态大型语言模型的规划能力，我们提出了一种无需训练的方法，通过研究复杂规划中各种多模态提示的有效性，使用多模态思维链（CoT）提示。我们的方法在不额外训练的情况下，将GPT-4V在EgoPlan-Bench2上的性能提高了10.24。我们的工作不仅揭示了当前多模态大型语言模型在规划方面的局限性，还为这一关键领域的未来改进提供了见解。我们已经将数据和代码发布在https://qiulu66.github.io/egoplanbench2/。|\n",
        "2412.04445": "|**2024-12-05**|**Moto: Latent Motion Token as the Bridging Language for Robot Manipulation**|Yi Chen et.al.|[2412.04445](http://arxiv.org/abs/2412.04445)|null|最近，在大量语料库上预训练的大型语言模型在多种自然语言处理任务中取得了显著的成功，且仅需少量微调。这一成功为机器人学带来了新的希望，因为机器人学长期以来一直受限于高成本的动作标签数据。我们提出问题：鉴于大量包含互动相关知识的视频数据作为丰富的“语料库”可用，是否可以有效地将类似的生成式预训练方法应用于增强机器人学习？关键挑战是识别一个有效的自回归预训练表示，以促进机器人操作任务。受人类通过观察动态环境学习新技能的方式的启发，我们认为有效的机器人学习应强调与运动相关的知识，这些知识与低级动作紧密相关，并且与硬件无关，便于将学习到的运动转移到实际机器人动作中。为此，我们引入了Moto，它通过潜在运动标记器将视频内容转换为潜在运动标记序列，以无监督的方式从视频中学习运动的“桥梁”语言。我们通过运动标记自回归预训练Moto-GPT，使其能够捕捉多样的视觉运动知识。预训练后，Moto-GPT展示了产生语义可解释的运动标记、预测合理的运动轨迹以及通过输出似然性评估轨迹合理性等有希望的能力。为了将学习到的运动先验转移到真实机器人动作中，我们实施了一种协同微调策略，无缝地将潜在运动标记预测和真实机器人控制连接起来。大量实验表明，经过微调的Moto-GPT在机器人操作基准测试中表现出卓越的鲁棒性和效率，凸显了它从视频数据到下游视觉操作任务中知识转移的有效性。|\n",
        "2412.04432": "|**2024-12-05**|**Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation**|Yuying Ge et.al.|[2412.04432](http://arxiv.org/abs/2412.04432)|**[link](https://github.com/tencentarc/divot)**|**近年来，在大型语言模型（LLMs）中统一图像理解和生成引起了极大的兴趣。这种不断增长的兴趣促使我们探索将这种统一扩展到视频中。核心挑战在于开发一个通用的视频分词器，它能够捕捉视频的空间特征和时序动态，以获得适合LLMs的表示，并且这些表示可以被进一步解码为逼真的视频片段，从而实现视频生成。在这项工作中，我们介绍了Divot，一种基于扩散的视频分词器，它利用扩散过程进行自监督视频表示学习。我们认为，如果一个视频扩散模型能够通过将视频分词器的特征作为条件来有效地去噪视频片段，那么分词器已经成功地捕捉了鲁棒的空间和时序信息。此外，视频扩散模型本质上充当了解码器，将视频从其表示中解码出来。在Divot分词器的基础上，我们通过视频到文本的自回归和文本到视频的生成，使用高斯混合模型来建模连续值的Divot特征分布，提出了Divot-Vicuna。实验结果表明，我们的基于扩散的视频分词器，当与预训练的LLM集成时，在各种视频理解和生成基准测试中实现了有竞争力的性能。经过指令调整的Divot-Vicuna在视频叙事方面也表现出色，能够生成交错的故事和相应的视频。**|\n",
        "2412.04429": "|**2024-12-05**|**Grounding Descriptions in Images informs Zero-Shot Visual Recognition**|Shaunak Halbe et.al.|[2412.04429](http://arxiv.org/abs/2412.04429)|**[link](https://github.com/shaunak27/grain-clip)**|**视觉语言模型（VLMs）如CLIP因其能够在开放词汇概念上执行零样本视觉识别而备受青睐。这是通过选择与查询图像文本表示最相似的物体类别来实现的。尽管在某些领域取得了成功，但这种方法在识别细粒度实体以及泛化到训练分布未捕获的未见概念方面存在困难。近期的工作试图通过在测试时整合类别描述来减轻这些挑战，尽管取得了有限的改进。我们将这些有限的收益归因于图像和描述表示之间的基本不匹配，这种不匹配根植于CLIP的预训练结构。在这篇论文中，我们提出了GRAIN，这是一种新的预训练策略，旨在同时在对细粒度和粗粒度级别上对齐表示。我们的方法学会联合地将文本描述定位到图像区域，并将总体标题与全局图像表示对齐。为了推动这种预训练，我们利用冻结的多模态大型语言模型（MLLMs）来生成大规模合成注释。我们在11个不同的图像分类数据集上展示了我们模型相较于现有最先进方法的零样本性能提升。此外，我们引入了Products-2023，这是一个新整理的、手动标记的数据集，包含新颖的概念，并通过在该数据集上进行基准测试展示了我们模型识别这些概念的能力。我们模型在其他下游任务（如检索）上取得的显著改进进一步突显了我们方法学习的表示的高质量。代码可在https://github.com/shaunak27/grain-clip上获取。**|\n",
        "2412.04424": "|**2024-12-05**|**Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion**|Jiuhai Chen et.al.|[2412.04424](http://arxiv.org/abs/2412.04424)|**[link](https://github.com/jiuhaichen/florence-vl)**|**我们介绍了一组新的多模态大型语言模型（MLLMs），即Florence-VL，它由Florence-2生成视觉基础模型产生，具有丰富的视觉表示。与广泛使用的由对比学习训练的CLIP风格视觉Transformer不同，Florence-2能够捕捉不同层次和方面的视觉特征，这使得它们更加灵活，可以适应各种下游任务。我们提出了一种新颖的特征融合架构和一种创新的训练方案，有效地将Florence-2的视觉特征整合到预训练的LLM（如Phi 3.5和LLama 3）中。特别是，我们提出了“深度-呼吸融合（DBFusion）”来融合从不同深度和多个提示中提取的视觉特征。我们的模型训练包括整个模型的端到端预训练，随后是在精心设计的包含高质量图像标题和指令调整对的多样化开源数据集上对投影层和LLM进行微调。我们对Florence-VL的视觉特征的定量分析和可视化表明，它在视觉-语言对齐方面优于流行的视觉编码器，其中丰富的深度和呼吸发挥了重要作用。Florence-VL在涵盖一般视觉问答（VQA）、感知、幻觉、OCR、图表、知识密集型理解等多种多模态和视觉中心基准测试中，相对于现有的最先进MLLMs实现了显著的改进。为了促进未来的研究，我们的模型和完整的训练方案已经开源。https://github.com/JiuhaiChen/Florence-VL**|\n",
        "2412.04415": "|**2024-12-05**|**Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation**|Xuying Li et.al.|[2412.04415](http://arxiv.org/abs/2412.04415)|null|人工智能代理，由大型语言模型（LLMs）驱动，通过实现无缝、自然和情境感知的通信，已经改变了人机交互。虽然这些进步提供了巨大的实用性，但它们也继承了并放大了固有的安全风险，如偏见、公平性、幻觉、隐私侵犯和缺乏透明度。本文调查了一个关键漏洞：针对AI代理中LLM核心的对抗性攻击。具体而言，我们测试了一个假设，即一个欺骗性的简单对抗性前缀，例如“忽略文档”，可以通过绕过其情境保护措施，迫使LLMs生成危险或不希望的结果。通过实验，我们证明了高攻击成功率（ASR），揭示了现有LLM防御的脆弱性。这些发现强调了迫切需要针对LLM层面以及更广泛的基于代理的架构，采取稳健的多层安全措施来减轻漏洞。|\n",
        "2412.04342": "|**2024-12-05**|**Retrieval-Augmented Machine Translation with Unstructured Knowledge**|Jiaan Wang et.al.|[2412.04342](http://arxiv.org/abs/2412.04342)|**[link](https://github.com/krystalan/RAGtrans)**|**检索增强生成（RAG）通过引入额外信息来提升大型语言模型（LLMs）。在机器翻译（MT）领域，以往的研究通常从配对MT语料库中检索上下文示例，或从知识图中检索特定领域的知识，以增强模型的MT能力。然而，大量的世界知识组织在非结构化文档中，并且可能在不同语言之间没有完全配对。在本文中，我们研究了使用非结构化文档的检索增强MT。具体来说，我们构建了RAGtrans，这是第一个用于训练和评估LLMs检索增强MT能力的基准。RAGtrans包含了通过GPT-4o和人工翻译收集的79K MT样本。此外，还提供了不同语言的文档，为这些样本提供知识。基于RAGtrans，我们进一步提出了一种多任务训练方法，教导LLMs如何在翻译过程中使用多语言文档中的信息。该方法利用现有的多语言语料库创建辅助训练目标，无需额外的标注需求。大量实验表明，该方法将LLMs的BLEU得分提高了1.58-3.09，COMET得分提高了1.00-2.03。**|\n",
        "2412.04332": "|**2024-12-05**|**Liquid: Language Models are Scalable Multi-modal Generators**|Junfeng Wu et.al.|[2412.04332](http://arxiv.org/abs/2412.04332)|**[link](https://github.com/foundationvision/liquid)**|我们提出了Liquid，一种将视觉理解与生成无缝集成的自回归生成范式。Liquid通过将图像分词成离散代码，并在共享的特征空间中学习这些代码嵌入和文本标记，从而在视觉和语言之间实现整合。与之前的跨模态大型语言模型（MLLM）不同，Liquid使用单个大型语言模型（LLM）来实现这种整合，消除了使用外部预训练的视觉嵌入（如CLIP）的需求。Liquid首次揭示了一种缩放定律，即随着模型规模的增加，统一训练视觉和语言任务所带来的性能下降不可避免地减小。此外，统一的标记空间使得视觉生成和理解任务可以相互增强，有效地消除了早期模型中常见的干扰。我们表明，现有的LLM可以作为Liquid的强大基础，节省100倍的训练成本，同时在多模态能力上优于Chameleon，并保持与主流LLM（如LLAMA2）相当的语言性能。Liquid还优于SD v2.1和SD-XL（在MJHQ-30K上的FID为5.47），在视觉-语言和纯文本任务上都表现出色。这项工作证明了LLAMA3.2和GEMMA2等LLM是强大的多模态生成器，为增强视觉-语言理解和生成提供了可扩展的解决方案。代码和模型将发布。|\n",
        "2412.04318": "|**2024-12-05**|**The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation**|Fredrik Carlsson et.al.|[2412.04318](http://arxiv.org/abs/2412.04318)|null|本文介绍了在非常小的数据集上对过拟合预训练大型语言模型（LLMs）的出人意料的泛化结果。在开放式文本生成的背景下，有记录表明LLMs倾向于生成重复和乏味的序列，这种现象在使用贪婪解码生成时尤为明显。即使是最先进的、包含数十亿参数的LLMs，它们在大型数据集上通过下一标记预测进行训练，这一问题依然存在。我们发现，通过进一步微调这些模型，使其在少量样本集上达到几乎为零的训练损失——我们称之为超拟合——可以极大地增强其长序列生成能力。使用这些超拟合模型进行贪婪解码，甚至在多样性和人类偏好方面都优于长序列的Top-P采样。这一现象适用于各种大小、不同领域的LLMs，甚至包括自回归图像生成。我们进一步发现，这一现象与Grokking和双重下降现象有显著不同。令人惊讶的是，我们的实验表明，超拟合模型很少陷入它们训练过的重复序列，甚至明确阻止这些序列也会产生高质量的输出。所有超拟合模型都产生极低熵的预测，通常将几乎全部概率分配给单个标记。|\n",
        "2412.05271": "|**2024-12-06**|**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**|Zhe Chen et.al.|[2412.05271](http://arxiv.org/abs/2412.05271)|**[link](https://github.com/opengvlab/internvl)**|我们推出了InternVL 2.5，这是一个基于InternVL 2.0的高级多模态大型语言模型（MLLM）系列，保持了其核心模型架构，同时在训练和测试策略以及数据质量方面引入了显著的提升。在这项工作中，我们深入探讨了模型规模与性能之间的关系，系统地研究了视觉编码器、语言模型、数据集规模和测试时配置的性能趋势。通过在包括跨学科推理、文档理解、多图像/视频理解、现实世界理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理等广泛基准上的广泛评估，InternVL 2.5展现出具有竞争力的性能，与GPT-4o和Claude-3.5-Sonnet等领先的商业模型相媲美。值得注意的是，我们的模型是第一个在MMMU基准测试中超过70%的开源MLLM，通过思维链（CoT）推理实现了3.7分的提升，并展示了强大的测试时缩放潜力。我们希望这个模型通过设定开发和应用多模态AI系统的新标准，为开源社区做出贡献。HuggingFace演示请见https://huggingface.co/spaces/OpenGVLab/InternVL|\n",
        "2412.05270": "|**2024-12-06**|**APOLLO: SGD-like Memory, AdamW-level Performance**|Hanqing Zhu et.al.|[2412.05270](http://arxiv.org/abs/2412.05270)|null|大型语言模型（LLMs）在训练过程中对内存的消耗非常严重，尤其是使用流行的AdamW优化器时。这种内存负担迫使人们使用更多或更高端的GPU，或者减小批处理大小，从而限制了训练的可扩展性和吞吐量。为了解决这个问题，已经提出了各种内存高效的优化器来减少优化器的内存使用。然而，它们面临着一些关键的挑战：（i）依赖于昂贵的奇异值分解（SVD）操作；（ii）与AdamW相比，性能上有显著的权衡；（iii）仍然有相当大的优化器内存开销以维持竞争优势。  在这项工作中，我们发现AdamW的学习率自适应规则可以作为结构化学习率更新有效地粗化。基于这一洞察，我们提出了近似梯度缩放用于内存高效LLM优化（APOLLO），它使用基于纯随机投影的辅助低秩优化器状态来近似学习率缩放。这种结构化学习率更新规则使得APOLLO对进一步减少内存具有高度容忍性，同时在预训练性能上与AdamW相当。即使是它的秩-1变体APOLLO-Mini，在具有与SGD相当内存成本的条件下，也比AdamW实现了更优的预训练性能。  大量实验表明，APOLLO系列的性能与AdamW相当或更好，同时通过几乎消除AdamW的优化状态，实现了更大的内存节省。这些节省带来了显著的系统级好处：（1）提高了吞吐量：在8xA100-80GB的配置上，通过支持4倍更大的批处理大小，比AdamW实现了3倍的吞吐量。（2）提高了模型的可扩展性：在A100-80GB GPU上使用原始的分布式数据并行（DDP）预训练LLaMA-13B，而不进行系统级优化。（3）低端GPU友好的预训练：使用权重量化，在单个GPU上预训练LLaMA-7B，内存使用量少于12GB。|\n",
        "2412.05243": "|**2024-12-06**|**CompCap: Improving Multimodal Large Language Models with Composite Captions**|Xiaohui Chen et.al.|[2412.05243](http://arxiv.org/abs/2412.05243)|null|多模态大型语言模型（MLLMs）在理解复合图像方面的能力如何？复合图像（CIs）是通过合并多个视觉元素（如图表、海报或截图）合成的合成视觉，而不是通过相机直接捕捉的。虽然CIs在现实世界应用中很普遍，但最近MLLM的发展主要集中于解读自然图像（NIs）。我们的研究揭示，当前的MLLM在准确理解CIs方面面临着重大挑战，常常难以从这些图像中提取信息或进行复杂推理。我们发现，现有的CIs训练数据大多格式化为问答任务（例如，在ChartQA和ScienceQA等数据集中），而高质量的图像-描述数据集，对于稳健的视觉-语言对齐至关重要，却只有自然图像（NIs）才有。为了弥合这一差距，我们引入了复合描述（CompCap），这是一个灵活的框架，利用大型语言模型（LLMs）和自动化工具来合成准确且详细的复合图像。使用CompCap，我们编纂了CompCap-118K数据集，包含118K个图像-描述对，涵盖六种复合图像类型。我们通过监督微调三种规模的MLLMs（xGen-MM-inst.-4B和LLaVA-NeXT-Vicuna-7B/13B）来验证CompCap-118K的有效性。实证结果表明，CompCap-118K显著提升了MLLMs对复合图像的理解能力，分别在11个基准测试中实现了1.7%、2.0%和2.9%的平均提升。|\n",
        "2412.05237": "|**2024-12-06**|**MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale**|Jarvis Guo et.al.|[2412.05237](http://arxiv.org/abs/2412.05237)|null|开源的多模态大型语言模型（MLLMs）在多模态任务中展现出巨大的潜力。然而，它们的推理能力仍然受到现有指令微调数据集的限制，这些数据集主要来自VQA、AI2D和ChartQA等学术数据集，这些数据集针对的是简单的任务，并且只提供短语级别的答案，没有任何中间推理过程。为了解决这些挑战，我们提出了一种可扩展且成本效益高的方法来构建一个包含丰富中间推理过程的、大规模多模态指令微调数据集。我们仅使用开源模型，创建了一个包含1200万个指令-响应对的数据库，涵盖了多样化的、推理密集型任务，具有详细和可靠的推理过程。实验表明，在这样一个数据集上训练MLLMs可以显著提高推理能力，在MathVerse (+8.1%)、MMMU-Pro (+7%)和MuirBench (+13.3%)等基准测试中实现了最先进的性能。此外，该模型在非推理型基准测试上也表现出显著的提升，最高可达4%。消融实验进一步突出了数据集构建过程中关键组件，如重写和自我过滤的重要性。|\n",
        "2412.05225": "|**2024-12-06**|**BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits**|Wazib Ansar et.al.|[2412.05225](http://arxiv.org/abs/2412.05225)|null|大型语言模型（LLMs）基于Transformer在各种应用中取得了尖端成果。然而，它们的巨大规模和处理需求使得在资源受限的设备上部署极为困难。在各种效率考虑中，模型二值化和早期退出（EE）是常见的有效解决方案。然而，二值化可能会由于降低精度而影响梯度估计和参数更新，从而导致性能损失。此外，目前的早期退出机制仍处于研究的初级阶段。为了改善这些问题，我们提出了二值化早期退出Transformer（BEExformer），这是第一个将早期退出与二值化结合用于文本推理的选区学习Transformer架构。它通过到冲激函数的微分二阶近似来改进二值化过程。这使得可以计算关于权重符号和幅度的梯度。与基于绝对阈值的EE不同，所提出的EE机制依赖于中间Transformer块中软路由损失估计的熵的分数减少。虽然二值化使模型大小减少了18.44倍，但早期退出在推理过程中将FLOPs减少了54.85%，甚至通过解决深层网络固有的“过度思考”问题，提高了5.98%的准确率。此外，所提出的BEExformer通过不需要从全精度LLM中进行知识蒸馏来简化训练。在GLUE数据集上的广泛评估与SOTA工作的比较展示了其帕累托最优的性能-效率权衡。|\n",
        "2412.05223": "|**2024-12-06**|**100% Hallucination Elimination Using Acurai**|Michael C. Wood et.al.|[2412.05223](http://arxiv.org/abs/2412.05223)|**[link](https://github.com/AcuChat/acurai-RAGTruth-conflict-resolution)**|大型语言模型（LLMs）中的幻觉问题仍然是人工智能在企业和其他高风险应用中应用的一个关键障碍。尽管检索增强生成（RAG）系统取得了进展，但当前最先进的方法在生成忠实且事实正确的输出时，即使在提供相关和准确的情况下，也未能超过80%的准确率。在这项工作中，我们引入了Acurai，这是一种新颖的系统方法，通过在输入之前重新格式化查询和上下文数据，在LLMs中实现了100%无幻觉的响应。利用对LLMs内部表示的深入了解、名词短语的主导地位的重要性以及离散功能单元（DFUs）的作用，Acurai确保输入上下文和生成输出之间的一致性。我们使用RAGTruth语料库验证了这种方法，证明了它能够消除GPT-4和GPT-3.5 Turbo的100%幻觉。Acurai为实现一致、准确和忠实的AI响应设定了新的标准，标志着可信AI系统发展的重大进步。|\n",
        "2412.05210": "|**2024-12-06**|**Evaluating and Aligning CodeLLMs on Human Preference**|Jian Yang et.al.|[2412.05210](http://arxiv.org/abs/2412.05210)|null|代码大型语言模型（codeLLMs）在代码生成方面取得了显著进展。大多数之前的与代码相关的基准测试，包括各种编程练习和相应的测试用例，被用作评估codeLLMs性能和能力的共同标准。然而，当前codeLLMs主要关注合成正确的代码片段，忽略了与人类偏好的对齐，其中查询应从实际应用场景中采样，而模型生成的响应应满足人类偏好。为了弥合模型生成响应与人类偏好之间的差距，我们提出了一个严格的人类编纂基准测试CodeArena，以模拟现实世界编程任务的复杂性和多样性，其中包含从用户查询中精心挑选的397个高质量样本，涵盖了40个类别和44种编程语言。此外，我们提出了一个多样化的合成指令语料库SynCode-Instruct（近20B个标记），通过扩展网站上的指令来验证大规模合成指令微调的有效性，其中Qwen2.5-SynCoder完全在合成指令数据上训练，可以达到开源codeLLMs的顶尖性能。结果表明，在基于执行的基准测试和CodeArena之间存在性能差异。我们对40多个LLMs在CodeArena上的系统实验揭示了开源SOTA代码LLMs（例如Qwen2.5-Coder）与专有LLMs（例如，OpenAI o1）之间存在显著的性能差距，突显了与人类偏好对齐的重要性。[footnote：https://codearenaeval.github.io/ ]|\n",
        "2412.05208": "|**2024-12-06**|**A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges**|Aditi Singh et.al.|[2412.05208](http://arxiv.org/abs/2412.05208)|null|文本到SQL系统通过将自然语言查询翻译为结构化查询语言（SQL），促进了与数据库的顺畅交互，弥合了非技术用户与复杂数据库管理系统之间的差距。本综述全面概述了AI驱动的文本到SQL系统的演变，突出了其基础组件、大型语言模型（LLM）架构的进步以及Spider、WikiSQL和CoSQL等数据集在推动进展中的关键作用。我们探讨了文本到SQL在医疗保健、教育和金融等领域的应用，强调了它们在提高数据可访问性方面的变革潜力。此外，我们分析了持续存在的挑战，包括领域泛化、查询优化、支持多轮对话交互以及针对NoSQL数据库和动态现实场景量身定制的数据集有限可用性。为了应对这些挑战，我们概述了未来的研究方向，例如扩展文本到SQL的功能以支持NoSQL数据库，设计用于动态多轮交互的数据集，以及优化系统以适应现实世界的可扩展性和鲁棒性。通过审视当前进展并识别关键差距，本文旨在指导基于LLM的文本到SQL系统下一代的研发与应用。|\n",
        "2412.05200": "|**2024-12-06**|**Are Frontier Large Language Models Suitable for Q&A in Science Centres?**|Jacob Watson et.al.|[2412.05200](http://arxiv.org/abs/2412.05200)|null|本文探讨了前沿大型语言模型（LLMs）在科学中心问答互动中的适用性，旨在提高游客参与度同时保持事实准确性。利用从英国莱斯特国家空间中心收集的问题数据集，我们评估了三个领先模型生成的回答：OpenAI的GPT-4、Claude 3.5 Sonnet和Google Gemini 1.5。每个模型都被要求针对8岁儿童观众提供标准答案和创造性回答，这些回答由空间科学专家根据准确性、参与度、清晰度、新颖性和偏离预期答案的程度进行评估。结果显示，在创造性和准确性之间存在着权衡，尽管Claude在保持清晰度和吸引年轻观众方面超越了GPT和Gemini，甚至在要求生成更富有创造性的回答时也是如此。然而，专家观察到，所有模型中更高的新颖性通常与事实可靠性降低有关。这项研究突出了LLMs在教育环境中的潜力，强调了精心设计提示以平衡参与度和科学严谨性的必要性。|\n",
        "2412.05187": "|**2024-12-06**|**SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot**|Jinlin Wu et.al.|[2412.05187](http://arxiv.org/abs/2412.05187)|**[link](https://github.com/franciszchen/surgbox)**|**手术干预，尤其是在神经科领域，代表复杂且高风险的场景，对手术团队提出了巨大的认知负担。尽管有目的的教育和实践可以增强认知能力，但由于患者安全问题的考虑，手术培训机会仍然有限。为了解决手术培训和手术中的认知挑战，我们提出了SurgBox，一个由代理驱动的沙盒框架，旨在系统性地提高外科医生在沉浸式手术模拟中的认知能力。具体来说，我们的SurgBox利用定制化的检索增强生成（RAG）的大型语言模型（LLMs）来真实地复制各种手术角色，从而为有目的的练习提供逼真的训练环境。特别是，我们设计了手术协同助手（Surgery Copilot），这是一个由AI驱动的助手，能够主动协调手术信息流并支持临床决策，从而减轻手术过程中手术团队的认知负荷。通过整合新颖的长短期记忆（Long-Short Memory）机制，我们的手术协同助手可以有效地在即时程序辅助和全面手术知识之间取得平衡。使用真实的神经外科手术记录进行的广泛实验验证了我们的SurgBox框架在提高手术认知能力和支持临床决策方面的有效性。通过提供针对培训和操作支持的综合性解决方案以解决认知挑战，我们的SurgBox框架推动了外科教育和实践的发展，有可能改变手术结果和医疗质量。代码可在https://github.com/franciszchen/SurgBox获取。**|\n",
        "2412.06769": "|**2024-12-09**|**Training Large Language Models to Reason in a Continuous Latent Space**|Shibo Hao et.al.|[2412.06769](http://arxiv.org/abs/2412.06769)|null|大型语言模型（LLMs）通常在“语言空间”中进行推理，通过思维链（CoT）来表述推理过程以解决复杂的推理问题。然而，我们认为语言空间并不总是推理的最优选择。例如，大多数单词标记主要用于文本连贯性，而非推理所必需，而一些关键标记则需要复杂的规划和给LLMs带来巨大挑战。为了探索LLMs在不受限制的潜在空间中进行推理的潜力，而不是使用自然语言，我们引入了一种新的范式——椰子（连续思维链）。我们利用LLM的最后隐藏状态作为推理状态的表示（称为“连续思维”）。我们不是将其解码为单词标记，而是直接将其作为连续空间中的后续输入嵌入反馈给LLM。实验表明，椰子可以有效地增强LLM在多个推理任务上的表现。这种新颖的潜在推理范式导致出现高级推理模式：连续思维可以编码多个替代的后续推理步骤，允许模型执行广度优先搜索（BFS）来解决问题，而不是像CoT那样过早地承诺单一确定路径。在需要大量回溯规划的某些逻辑推理任务中，椰子优于CoT，推理过程中思考标记更少。这些发现展示了潜在推理的潜力，并为未来的研究提供了宝贵的见解。|\n",
        "2412.06757": "|**2024-12-09**|**Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code**|Joy Krishan Das et.al.|[2412.06757](http://arxiv.org/abs/2412.06757)|null|大型语言模型（LLMs）如ChatGPT已显示出协助开发者进行编码和调试任务的潜力。然而，它们在协同问题解决中的角色尚未得到充分探索。在本研究中，我们分析了GitHub上1,012个问题中的1,152次开发者与ChatGPT的对话，以考察ChatGPT的多样使用和对其生成代码的依赖。我们的贡献有四个方面。首先，我们手动分析了289次对话，以了解ChatGPT在GitHub问题中的使用情况。我们的分析显示，ChatGPT主要用于创意构思，而其在验证（例如，代码文档准确性）方面的使用非常有限。其次，我们应用BERTopic模型来识别整个数据集中关键的关注领域。我们发现后端问题（例如，API管理）主导了对话，而测试却意外地覆盖较少。第三，我们利用CPD克隆检测工具来检查ChatGPT生成的代码是否被用于解决问题。我们的发现显示，ChatGPT生成的代码被直接用于解决仅占5.83%的问题。第四，我们使用基于RoBERTa的情感分析模型来估计情感，以确定开发者对不同用途和关注领域的满意度。我们发现，使用ChatGPT进行重构和解决数据分析（例如，分类表数据）问题的正面情绪（即，高度满意）。相反，当使用ChatGPT调试问题和解决自动化任务（例如，GUI交互）时，我们观察到负面情绪。我们的研究发现，开发者存在未满足的需求和日益增长的不满。研究人员和ChatGPT开发者应专注于开发特定任务的解决方案，以帮助解决各种问题，提高软件开发中的用户满意度和解决问题的效率。|\n",
        "2412.06748": "|**2024-12-09**|**Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models**|Neel Jain et.al.|[2412.06748](http://arxiv.org/abs/2412.06748)|null|构建安全可靠的语言模型的关键组成部分是使模型能够适当地拒绝遵循某些指令或回答某些问题。我们可能希望模型为各种用户查询类别输出拒绝消息，例如，无意义的问题、执行非法行为的指令，或需要超出模型知识范围的信息的查询。设计拒绝回答此类问题的模型复杂化，因为个人可能希望他们的模型在拒绝不同类别的查询时表现出不同水平的感觉性，不同的用户可能希望有不同的拒绝率。当前默认的方法涉及使用每个类别不同比例的拒绝消息训练多个模型以实现所需的拒绝率，这计算成本高，可能需要为每位用户的拒绝率偏好训练新的模型。为了解决这些挑战，我们提出了拒绝标记，每个拒绝类别一个标记，或一个单一的拒绝标记，这些标记在训练期间添加到模型的响应之前。然后，我们展示了如何在推理期间增加或减少生成每个类别拒绝标记的概率，以引导模型的拒绝行为。拒绝标记允许通过在生成过程中选择性干预来控制单个模型的拒绝率，而不需要任何进一步的微调。|\n",
        "2412.06738": "|**2024-12-09**|**JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM**|Takuro Fujii et.al.|[2412.06738](http://arxiv.org/abs/2412.06738)|**[link](https://github.com/retrieva/japagen)**|近期一些研究强调了大型语言模型（LLMs）作为有效的监督训练数据生成器的潜力，提供了如提高推理效率和降低数据收集相关成本等优势。然而，这些研究主要关注英语任务。在本文中，我们探讨了基本的研究问题：LLMs能否作为其他语言任务的优秀训练数据生成器？具体来说，我们利用LLMs在六种不同的日语下游任务下，在少样本和零样本学习场景中合成监督训练数据。随后，我们使用这些合成的数据训练紧凑模型（例如BERT）。这种新颖的方法被称为JAPAGEN。我们的实验发现表明，JAPAGEN在需要正式文本输入的分类任务中实现了稳健的性能，与传统的LLM提示策略相比，取得了具有竞争力的结果。|\n",
        "2412.06724": "|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|我们研究了大型语言模型（LLMs）在自动生成数据清理工作流中的推理能力。为了评估LLMs完成数据清理任务的能力，我们实现了一个基于LLM的自动数据清理工作流（AutoDCWorkflow）的管道，通过提示LLMs进行数据清理操作来修复三种类型的数据质量问题：重复数据、缺失值和不一致的数据格式。给定一个脏表和目的（以查询形式表达），此管道生成一个最小的、干净的表，足以满足目的，并生成用于生成该表的数据清理工作流。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的目标列集合。（2）检查列质量：评估每个目标列的数据质量，并生成数据质量报告作为操作目标。（3）生成操作与参数：根据数据质量报告的结果预测下一个操作和参数。此外，我们提出一个数据清理基准，以评估LLM代理自动生成解决不同难度水平数据清理目的的工作流的能力。基准包括注释数据集，作为一个包含目的、原始表、干净表、数据清理工作流和答案集的集合。在我们的实验中，我们评估了三种自动生成目的驱动数据清理工作流的LLMs。结果表明，LLMs在规划和生成数据清理工作流方面表现良好，无需微调。|\n",
        "2412.06693": "|**2024-12-09**|**OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions**|Yi-Kai Zhang et.al.|[2412.06693](http://arxiv.org/abs/2412.06693)|null|随着大型语言模型（LLMs）的快速发展，其应用范围得到了显著扩展，从多语言支持到特定领域的任务和多模态集成。本文介绍了一种新型的基准测试工具箱OmniEvalKit，旨在评估LLMs及其全功能扩展在多语言、多领域和多模态能力方面的表现。与现有专注于单一方面的基准测试不同，OmniEvalKit提供了一个模块化、轻量化和自动化的评估系统。它采用模块化架构，包括静态构建器和动态数据流，促进了新模型和数据集的无缝集成。OmniEvalKit支持超过100种LLMs和50个评估数据集，覆盖了成千上万种模型-数据集组合的全面评估。OmniEvalKit致力于创建一个超轻量级且快速部署的评估框架，使下游应用对人工智能社区更加便捷和灵活。|\n",
        "2412.06684": "|**2024-12-09**|**Exploring Critical Testing Scenarios for Decision-Making Policies: An LLM Approach**|Weichao Xu et.al.|[2412.06684](http://arxiv.org/abs/2412.06684)|null|近年来，决策政策在各种领域，如自动驾驶和机器人技术，取得了令人惊讶的成就。在存在可能威胁其可靠性的关键场景的情况下，对决策政策进行测试至关重要。众多研究努力致力于测试这些政策。然而，由于测试政策和环境的复杂性，仍然存在重大挑战，例如测试效率低和多样性不足。受大型语言模型（LLMs）卓越能力的影响，本文提出了一种基于LLM的在线测试框架，以有效地测试决策政策。主要思路是利用基于LLM的测试场景生成器通过思考和推理智能地生成具有挑战性的测试案例。具体来说，我们首先设计了一个“生成-测试-反馈”流程，并应用模板提示工程充分利用LLMs的知识和推理能力。然后，我们引入了一种多尺度场景生成策略来解决LLMs在精细调整方面固有的挑战，从而进一步提高测试效率。最后，我们在五个广泛使用的基准上评估了基于LLM的方法。实验结果表明，我们的方法在揭示关键和多样化的场景方面显著优于基线方法。|\n",
        "2412.06681": "|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通运输系统需求建模和仿真领域，基于代理模型和微观模拟是目前最先进的方法。然而，现有的基于代理模型在行为真实性和资源需求方面仍存在一些限制，这限制了它们的适用性。在本研究中，我们利用新兴的大语言模型（LLMs）和基于LLMs的代理技术，提出了一种适用于交通运输系统的一般LLM-代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，而且为克服现有基于代理模型的某些局限性提供了有希望的解决方案。我们的概念框架设计紧密模拟了交通网络中人类旅行者的决策、交互过程和特征，并通过相关研究和LLM代理在瓶颈设置中的学习和调整的演示实例，证明了所提出的系统可以满足决策和学习行为的关键行为标准。尽管需要进一步细化基于LLM的代理建模框架，但我们相信这种方法有可能提高交通运输系统建模和仿真的水平。|\n",
        "2412.06676": "|**2024-12-09**|**I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token**|Roi Cohen et.al.|[2412.06676](http://arxiv.org/abs/2412.06676)|null|大型语言模型因其能够捕捉现实世界知识而闻名，这使得它们在许多下游任务中表现出色。尽管近年来取得了进展，但这些模型仍然容易受到所谓的“幻觉”的影响，导致它们产生不想要且事实错误的文章。在本研究中，我们提出了一种新颖的校准方法，可用于对抗幻觉。我们向模型的词汇表中添加了一个特殊的[IDK]（“我不知道”）标记，并引入了一个目标函数，该函数将概率质量转移到[IDK]标记以应对错误的预测。这种方法允许模型在其输出中明确表达不确定性。我们在多个模型架构和事实性下游任务中评估了我们的方法。我们发现，使用我们的方法训练的模型能够在它们之前可能出错的地方表达不确定性，同时只损失少量的编码知识。我们还对多种方法变体进行了广泛的消融研究，并提供了对我们方法精确度-召回率权衡的详细分析。|\n",
        "2412.06673": "|**2024-12-09**|**ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**|Chunwei Wang et.al.|[2412.06673](http://arxiv.org/abs/2412.06673)|null|在这篇论文中，我们介绍了ILLUME，这是一种统一的多元模态大型语言模型（MLLM），通过统一的下一个标记预测公式，将多元模态的理解和生成能力无缝集成到单个大型语言模型中。为了解决图像-文本对齐通常所需的大量数据集大小，我们提出通过设计一个结合语义信息的视觉标记化器和渐进式多阶段训练程序来提高数据效率。这种方法将数据集大小减少到仅为15M用于预训练——仅为通常所需数量的四分之一——同时实现了与现有统一MLLMs（如Janus）相当甚至更优的性能。此外，为了促进理解和生成能力之间的协同增强，这是以往工作中较少探索的，我们引入了一种新颖的自我增强多元模态对齐方案。该方案监督MLLM自我评估文本描述和自生成图像之间的一致性，促进模型更准确地解释图像，并避免由图像生成中的对齐错误引起的非现实和不正确预测。基于广泛的实验，我们提出的ILLUME在各种多元模态理解、生成和编辑的基准测试中脱颖而出，并与其他最先进的统一MLLMs和专业模型竞争。|\n",
        "2412.07763": "|**2024-12-10**|**Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences**|Alan Nawzad Amin et.al.|[2412.07763](http://arxiv.org/abs/2412.07763)|**[link](https://github.com/alannawzadamin/clonebo)**|**为了构建有效的治疗药物，生物学家通过迭代地突变抗体序列来提高其结合力和稳定性。建议的突变可以基于之前的测量结果，或者通过从大量的抗体数据库中学习来预测典型的抗体。不幸的是，典型抗体的搜索空间巨大，实验往往在预算范围内无法找到合适的抗体。我们引入了基于克隆的贝叶斯优化（CloneBO），这是一种贝叶斯优化过程，通过教导一个生成模型如何优化我们的免疫系统中的抗体，从而在实验室中有效地优化抗体。我们的免疫系统通过迭代地进化其序列的特定部分来与靶点强有力地结合，并稳定地结合，从而产生一组被称为克隆家族的相关、演化的序列。我们在数以万计的克隆家族上训练了一个大型语言模型，CloneLM，并使用它来设计具有最有可能优化人体免疫系统内抗体的突变序列。我们提出使用扭曲的顺序蒙特卡洛过程来引导我们的设计以适应之前的测量。我们表明，在现实情况下的计算机模拟实验中，CloneBO比先前的方法更有效地优化了抗体，在体外湿实验中设计了更强和更稳定的结合剂。**|\n",
        "2412.07743": "|**2024-12-10**|**Zero-Shot ATC Coding with Large Language Models for Clinical Assessments**|Zijian Chen et.al.|[2412.07743](http://arxiv.org/abs/2412.07743)|null|将安大略省健康部和InterRAI加拿大在医疗保健研究和运营中手动分配解剖治疗化学（ATC）代码至处方记录的过程，是一个重要的瓶颈，需要大量的专家时间和精力。为了在保持数据隐私的同时自动化这一过程，我们开发了一种实用的方法，使用本地可部署的大语言模型（LLMs）。受最近在自动国际疾病分类（ICD）编码方面的进展启发，我们的方法将ATC编码视为一个层次化信息提取任务，通过引导LLMs逐层浏览ATC本体。我们使用GPT-4o作为准确性的上限，并专注于开发适合隐私敏感部署的开源Llama模型。在加拿大卫生部的药品产品数据、RABBITS基准测试以及安大略省健康的真实临床笔记中进行测试，我们的方法在GPT-4o上实现了78%的精确匹配准确率，在Llama 3.1 70B上实现了60%。我们通过药物定义研究知识固化，发现准确率有适度提高。此外，我们展示了对Llama 3.1 8B进行微调后的模型与零样本Llama 3.1 70B的准确率相匹配，这表明使用较小的模型进行有效的ATC编码是可行的。我们的结果证明了在隐私敏感的医疗保健环境中自动进行ATC编码的可行性，为未来的部署奠定了基础。|\n",
        "2412.07724": "|**2024-12-10**|**Granite Guardian**|Inkit Padhi et.al.|[2412.07724](http://arxiv.org/abs/2412.07724)|**[link](https://github.com/ibm-granite/granite-guardian)**|**我们推出了Granite Guardian模型系列，这是一套旨在为提示和响应提供风险检测的保障措施，以支持与任何大型语言模型（LLM）的安全和负责任使用。这些模型在多个风险维度上提供全面覆盖，包括社会偏见、粗俗、暴力、色情内容、不道德行为、越狱以及与幻觉相关的风险，如检索增强生成（RAG）的上下文相关性、基础性和回答相关性。Granite Guardian模型基于一个独特的数据集进行训练，该数据集结合了来自不同来源的人类标注和合成数据。这些模型解决了传统风险检测模型通常忽略的风险，如越狱和RAG特定问题。在有害内容和RAG幻觉相关基准测试上分别获得AUC分数0.871和0.854，Granite Guardian是此领域中最具有普遍性和竞争力的模型。作为开源发布，Granite Guardian旨在促进整个社区负责任的AI发展。**|\n",
        "2412.07689": "|**2024-12-10**|**DriveMM: All-in-One Large Multimodal Model for Autonomous Driving**|Zhijian Huang et.al.|[2412.07689](http://arxiv.org/abs/2412.07689)|**[link](https://github.com/zhijian11/DriveMM)**|**大型多模态模型（LMMs）通过整合大型语言模型，在自动驾驶（AD）领域展示了卓越的理解和解释能力。尽管取得了进展，但当前基于数据驱动的自动驾驶方法往往集中在单个数据集和特定任务上，忽视了它们的整体能力和泛化能力。为了弥补这些差距，我们提出了DriveMM，这是一种通用的大型多模态模型，旨在处理多种数据输入，如图像和多视角视频，同时在感知、预测和规划等广泛的自动驾驶任务中发挥作用。最初，该模型经过课程预训练，以处理不同的视觉信号并执行基本的视觉理解和感知任务。随后，我们对各种与自动驾驶相关的数据集进行增强和标准化，以微调模型，从而形成一个集自动驾驶之大成的LMM。为了评估其整体能力和泛化能力，我们在六个公开基准上进行了评估，并在一个未见过的数据集上进行了零样本迁移学习，DriveMM在所有任务中均实现了最先进的性能。我们希望DriveMM能够成为未来在现实世界中实现端到端自动驾驶应用的 promising 解决方案。**|\n",
        "2412.07687": "|**2024-12-10**|**Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions**|Anant Prakash Awasthi et.al.|[2412.07687](http://arxiv.org/abs/2412.07687)|null|随着客户支持领域对人工智能（AI）的日益依赖，运营效率和用户体验得到了显著提升。然而，传统的机器学习（ML）方法，这些方法需要在敏感数据集上进行广泛的本地训练，带来了巨大的隐私风险，并且与通用数据保护条例（GDPR）和加州消费者隐私法案（CCPA）等法规存在合规挑战。现有的隐私保护技术，如匿名化、差分隐私和联邦学习，虽然解决了部分问题，但在实用性、可扩展性和复杂性方面仍存在局限。本文提出了一种新型的隐私保护零样本学习（PP-ZSL）框架，该框架利用大型语言模型（LLMs）在零样本学习模式下的能力。与传统的机器学习方法不同，PP-ZSL通过利用预训练的LLMs直接生成响应，从而消除了在敏感数据上本地训练的需求。该框架融合了实时数据匿名化以删除或屏蔽敏感信息、检索增强生成（RAG）以解决特定领域的查询，以及鲁棒的后期处理以确保符合监管标准。这种组合降低了隐私风险，简化了合规性，并提高了可扩展性和运营效率。实证分析表明，PP-ZSL框架能够提供准确、符合隐私规范的响应，同时显著降低了部署人工智能驱动客户支持系统的成本和复杂性。该研究突出了在金融服务业、医疗保健、电子商务、法律支持、电信和政府服务等多个行业的潜在应用。通过解决隐私和性能的双重挑战，该框架为客户交互中的安全、高效和合规的AI应用奠定了基础。|\n",
        "2412.07682": "|**2024-12-10**|**TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation**|Alfredo Garrachón Ruiz et.al.|[2412.07682](http://arxiv.org/abs/2412.07682)|null|大型语言模型（LLMs）的推理成本是一个重大挑战，尤其是对于需要长输出的任务，因为它们的计算需求很大。然而，自然语言往往包含冗余，这为优化提供了机会。我们观察到，当得到适当的提示时，LLMs可以生成简练的语言输出，保留基本意义。我们提出了一种节省计算成本的框架，其中LLM的较短的蒸馏输出由一个具有较低推理成本的小型模型重新构建成完整叙事。我们的实验结果表明了有希望的结果，特别是在一般知识领域，平均节省了20.58%的标记，且评估指标略有下降，这表明这种方法可以在语言处理任务中有效地平衡效率和准确性。|\n",
        "2412.07673": "|**2024-12-10**|**Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting**|Shuyu Shen et.al.|[2412.07673](http://arxiv.org/abs/2412.07673)|**[link](https://github.com/HKUSTDial/vistroubleshooting.github.io)**|可视化创作是一个迭代过程，需要用户修改参数如配色方案和数据转换，以达到预期的美学效果并有效传达洞察。由于这些调整的复杂性，用户常常会创建出有缺陷的可视化，并需要故障排除支持。在本文中，我们考察了两种主要的可视化故障排除方法：（1）通过论坛进行人工辅助支持，用户从其他人那里获得建议；（2）使用大型语言模型（LLMs）进行AI辅助支持。我们的目标是了解每种方法在支持可视化故障排除任务中的优缺点。为此，我们从Stack Overflow收集了889个Vega-Lite案例。然后，我们进行了全面分析，以了解用户提出的问题类型、人工和AI指导的有效性，以及补充资源（如文档和示例）对故障排除结果的影响。我们的发现揭示了人工辅助故障排除和AI辅助故障排除之间的显著差异：人工辅助故障排除提供定制、情境敏感的建议，但响应质量往往有所差异，而AI辅助故障排除提供快速反馈，但通常需要额外的情境资源才能达到预期效果。|\n",
        "2412.07672": "|**2024-12-10**|**FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks**|Bocheng Chen et.al.|[2412.07672](http://arxiv.org/abs/2412.07672)|null|在大型语言模型（LLMs）中，防御措施至关重要，以应对众多利用这些系统通过操纵提示生成有害内容的攻击者，这种攻击被称为越狱攻击。尽管已经提出了许多防御策略，但它们往往需要访问模型的内部结构或需要额外的训练，这对使用LLM API的服务提供商来说并不实际，例如OpenAI API或Claude API。在本文中，我们提出了一种动态目标防御方法，通过改变解码超参数来增强模型对各种越狱攻击的鲁棒性。我们的方法不需要访问模型的内部结构，也不会产生额外的训练成本。所提出的防御措施包括两个关键组成部分：（1）通过识别和调整影响标记生成概率的解码超参数来优化解码策略；（2）将解码超参数和模型系统提示转换为动态目标，这些目标在每次运行期间持续改变。通过持续修改解码策略和提示，防御措施有效地缓解了现有攻击。我们的结果表明，在我们的测试中，当使用LLMs作为黑盒API时，我们的防御在三个模型中对越狱攻击最为有效。此外，我们的防御提供了较低的推理成本，并保持了可比的响应质量，使其在与其他防御方法一起使用时成为一种潜在的保护层。|\n",
        "2412.07668": "|**2024-12-10**|**Automating Business Intelligence Requirements with Generative AI and Semantic Search**|Nimrod Busany et.al.|[2412.07668](http://arxiv.org/abs/2412.07668)|null|在动态的商业环境中，对商业智能（BI）系统提出需求仍然是一个重大的挑战。本文介绍了一种名为AutoBIR的创新人工智能系统，该系统利用语义搜索和大型语言模型（LLMs）来自动化和加速BI需求的规格制定。该系统通过会话界面促进与利益相关者的直观互动，将用户输入转换为原型分析代码、描述和数据依赖。此外，AutoBIR生成详细的测试用例报告，可选地添加视觉辅助，简化需求提出过程。通过结合用户反馈，该系统优化BI报告和系统设计，展示了加快数据驱动决策的实际应用。本文探讨了生成式AI在转变BI开发方面的更广泛潜力，阐述了其在提高大规模、发展中的系统数据工程实践中的作用。|\n",
        "2412.07646": "|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过反复的语言学习和使用而演变，这些过程在语言习得期间引入了偏见，并塑造了语言系统以实现沟通效率。在这篇论文中，我们研究了如果人工语言被优化为针对大型语言模型（LLMs）的隐式偏见，是否会发生相同的情况。为此，我们模拟了一个经典指称游戏，其中LLMs学习和使用人工语言。我们的结果表明，最初无结构的整体语言确实被塑造出一些结构属性，使得两个LLM智能体能够成功沟通。与人类实验中的观察结果相似，代际传承增加了语言的易学性，但同时也可能导致非人类化的退化词汇。综上所述，这项工作扩展了实验发现，表明LLMs可以用作模拟语言进化的工具，并为此领域未来的机器-人实验开辟了可能性。|\n",
        "2412.08642": "|**2024-12-11**|**Generative Semantic Communication: Architectures, Technologies, and Applications**|Jinke Ren et.al.|[2412.08642](http://arxiv.org/abs/2412.08642)|null|本文深入探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并进行了全面的研究。首先介绍了三个由经典GAI模型支持的流行SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，本文阐释了GAI模型的基本概念、相应的SemCom架构以及近期努力的文献综述。接着，提出了一种新型的基于最新GAI技术——大型语言模型（LLMs）的生成式SemCom系统。该系统在发送方和接收方均采用两个基于LLMs的AI代理，分别作为“大脑”来提供强大的信息理解和内容再生能力。这种创新设计使得接收方可以直接根据发送方传递的编码语义信息生成所需内容，而不是恢复比特流。因此，它将通信思维从“信息恢复”转变为“信息再生”，从而开启了生成式SemCom的新时代。通过一个关于点对点视频检索的案例研究展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，通信开销减少了99.98%，检索精度提高了53%。此外，还概述了生成式SemCom的四个典型应用场景，并讨论了三个需要未来进一步研究的问题。总之，本文为在SemCom中应用GAI提供了一套全面的指导原则，为未来无线网络中生成式SemCom的高效实现铺平了道路。|\n",
        "2412.08639": "|**2024-12-11**|**Fast Prompt Alignment for Text-to-Image Generation**|Khalil Mrini et.al.|[2412.08639](http://arxiv.org/abs/2412.08639)|**[link](https://github.com/tiktok/fast_prompt_alignment)**|**文本到图像生成技术发展迅速，但将复杂的文本提示与生成的图像相匹配仍然具有挑战性，尤其是在处理复杂的物体关系和细微的细节方面。本文介绍了一种名为快速提示对齐（FPA）的提示优化框架，它采用了一次性方法，提高了文本到图像对齐的效率，避免了当前方法如OPT2I典型的迭代开销。FPA利用大型语言模型（LLMs）进行单次迭代提示改写，随后使用优化后的提示进行微调或上下文学习，以实现实时推理，降低计算需求同时保持对齐精度。在COCO Captions和PartiPrompts数据集上的广泛评估表明，FPA在处理时间的一小部分内就实现了具有竞争力的文本-图像对齐得分，这一点通过自动化指标（TIFA、VQA）和人工评估都得到了验证。一项由专家注释员参与的问卷调查进一步揭示了人类对齐判断与自动化评分之间的强相关性，凸显了FPA改进的稳健性。所提出的方法展示了一种可扩展、高效的迭代提示优化替代方案，使其在实时、高需求环境中具有更广泛的应用。代码库已提供以促进进一步研究：https://github.com/tiktok/fast_prompt_alignment**|\n",
        "2412.08635": "|**2024-12-11**|**Multimodal Latent Language Modeling with Next-Token Diffusion**|Yutao Sun et.al.|[2412.08635](http://arxiv.org/abs/2412.08635)|**[link](https://github.com/microsoft/unilm/tree/master/LatentLM)**|多模态生成模型需要一种统一的方法来处理离散数据（例如文本和代码）和连续数据（例如图像、音频、视频）。在这项工作中，我们提出了潜在语言模型（LatentLM），它通过因果Transformer无缝地整合连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入了下一个标记扩散来实现这些向量的自回归生成。此外，我们开发了$\\sigma$-VAE来解决方差崩溃问题，这对于自回归建模至关重要。大量实验证明了LatentLM在各种模态上的有效性。在图像生成方面，LatentLM在性能和可扩展性上都超越了扩散Transformer。当集成到多模态大型语言模型中时，LatentLM提供了一个通用的接口，统一了多模态生成和理解。实验结果表明，在扩大训练标记的设置中，与Transfusion和矢量量化模型相比，LatentLM取得了有利的性能。在文本到语音合成方面，LatentLM在说话人相似性和鲁棒性方面优于最先进的VALL-E 2模型，同时解码步骤减少了10倍。这些结果确立了LatentLM作为一种高效且可扩展的方法，以推进大型多模态模型的发展。|\n",
        "2412.08619": "|**2024-12-11**|**Synthetic Vision: Training Vision-Language Models to Understand Physics**|Vahid Balazadeh et.al.|[2412.08619](http://arxiv.org/abs/2412.08619)|null|物理推理，涉及对动态环境中物体行为的解释、理解和预测，仍然是当前视觉-语言模型（VLMs）的一个重要挑战。在这项工作中，我们提出了两种方法来利用模拟数据增强VLMs的物理推理能力。首先，我们使用与物理推理任务相关的模拟生成的问答（QA）对微调一个预训练的VLM。其次，我们引入了物理上下文构建器（PCBs），这是一种专门的VLM，经过微调以创建包含物理属性和过程的场景描述。在物理推理任务期间，这些PCBs可以作为上下文来帮助大型语言模型（LLM）提高其性能。我们使用多个基准测试了我们的两种方法，包括一个名为Falling Tower的新稳定性检测QA数据集，它包含模拟和真实世界的场景，以及CLEVRER。我们证明，一个小型的经过QA微调的VLM可以显著优于更大的最先进的基座模型。我们还展示了将PCBs集成可以提升基座LLM在物理推理任务上的性能。使用Falling Tower数据集中的真实世界场景，我们还验证了两种方法在Sim2Real迁移中的鲁棒性。我们的结果表明，模拟数据在创建能够进行高级物理推理的学习系统中的有用性。|\n",
        "2412.08615": "|**2024-12-11**|**Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models**|Jiahui Li et.al.|[2412.08615](http://arxiv.org/abs/2412.08615)|**[link](https://github.com/jiah-li/magic)**|尽管通过对齐技术提高了大型语言模型（LLMs）生成内容的安全性，但这些模型仍然容易受到越狱攻击的影响，越狱攻击是一种暴露LLMs安全漏洞的对抗攻击方法。值得注意的是，贪婪坐标梯度（GCG）方法已显示出自动生成对抗后缀以越狱最先进LLMs的能力。然而，GCG中的优化过程非常耗时，使得越狱流程效率低下。在本文中，我们研究了GCG的过程，并确定了间接效应问题，这是GCG优化的关键瓶颈。为此，我们提出了模型攻击梯度索引GCG（MAGIC），通过利用后缀标记的梯度信息来解决间接效应，从而通过减少计算和迭代次数来加速过程。我们的实验在AdvBench上表明，MAGIC实现了高达1.5倍的速度提升，同时保持了与其他基线相当甚至更高的攻击成功率（ASR）。我们的MAGIC在Llama-2上实现了74%的ASR，在执行对GPT-3.5的迁移攻击时实现了54%的ASR。代码可在https://github.com/jiah-li/magic上找到。|\n",
        "2412.08604": "|**2024-12-11**|**Preference Discerning with LLM-Enhanced Generative Retrieval**|Fabian Paischer et.al.|[2412.08604](http://arxiv.org/abs/2412.08604)|null|序列推荐系统旨在根据用户的交互历史提供个性化的推荐。为了实现这一目标，它们通常结合辅助信息，如物品的文本描述和辅助任务，例如预测用户偏好和意图。尽管已经投入了大量努力来增强这些模型，但它们仍然面临着个性化不足的问题。为了解决这个问题，我们提出了一种新的范式，我们称之为偏好辨别。在偏好辨别中，我们明确地将生成式序列推荐系统在其上下文中对用户偏好进行条件化。为此，我们根据用户评论和物品特定数据使用大型语言模型（LLMs）生成用户偏好。为了评估序列推荐系统的偏好辨别能力，我们引入了一个新的基准，该基准在各种场景中提供了一个全面的评估，包括偏好引导和情感跟随。我们使用我们的基准评估了当前最先进的方法，并表明它们在准确辨别用户偏好方面存在困难。因此，我们提出了一种名为Mender的新方法，该方法改进了现有方法，并在我们的基准上实现了最先进的性能。我们的结果表明，即使在训练过程中没有观察到人类偏好，Mender也能被有效引导，为更个性化的序列推荐系统铺平了道路。我们的代码和基准将在发表后开源。|\n",
        "2412.08602": "|**2024-12-11**|**Empirical Measurements of AI Training Power Demand on a GPU-Accelerated Node**|Imran Latif et.al.|[2412.08602](http://arxiv.org/abs/2412.08602)|null|随着人工智能（AI）应用范围的扩大，云计算提供商在计算基础设施方面的投资大幅增加。量化这一基础设施的能源足迹需要根据AI硬件在训练期间的电力需求进行参数化的模型。我们实证测量了一个8-GPU的NVIDIA H100 HGX节点在开源图像分类器（ResNet）和大型语言模型（Llama2-13b）训练过程中的瞬时电力消耗。观察到的最大电力消耗约为8.4千瓦，比制造商额定值10.2千瓦低18%，即使GPU接近满负荷运行。在保持模型架构不变的情况下，将ResNet的批量大小从512张图像增加到4096张图像，总训练能耗减少了4倍。这些发现可以为数据中心运营商的容量规划以及研究人员的能源使用估计提供信息。未来的工作将研究冷却技术和碳感知调度对AI工作负载能源消耗的影响。|\n",
        "2412.08593": "|**2024-12-11**|**Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability and Compliance Checks**|Arsalan Masoudifard et.al.|[2412.08593](http://arxiv.org/abs/2412.08593)|null|确保软件需求规格说明书（SRS）与更高级的组织或国家要求相一致至关重要，尤其是在金融和航空航天等监管环境。在这些领域，保持一致性、遵守监管框架、最小化错误以及满足关键期望对于系统的可靠运行是必不可少的。大型语言模型（LLMs）的广泛应用凸显了它们的巨大潜力，但在检索相关信息和增强推理能力方面仍有很大的改进空间。本研究表明，将强大的图-RAG框架与高级提示工程技术，如思维链和思维树，相结合可以显著提高性能。与基线RAG方法和简单的提示策略相比，这种方法提供更准确和情境感知的结果。尽管这种方法在性能上显示出显著改进，但也带来了一些挑战。在多样化的环境中实施既昂贵又复杂，需要仔细适应特定场景。此外，它的有效性高度依赖于完整和准确的数据输入，而这些数据可能并不总是容易获得，这进一步限制了其可扩展性和实用性。|\n",
        "2412.08587": "|**2024-12-11**|**Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning**|Hang Zhao et.al.|[2412.08587](http://arxiv.org/abs/2412.08587)|null|该研究对比了基于编码器模型（例如BERT、RoBERTa）和大型语言模型（LLMs，例如Llama3）在文本分类任务中的性能，尤其是在微调的情况下。研究采用了各种不同大小和架构的模型和方法，包括微调和预训练的方法。首先，我们对这些LLMs在20个新闻组（20NG）和MASSIVE数据集上的性能进行了评估，并将它们与仅编码器的RoBERTa模型进行了比较。此外，我们通过将意图检测和槽填充等多个分类任务结合到一个模型中，并使用两个数据集的数据来探索这两种模型类型的多任务能力。我们的结果表明，完全微调的Llama3-70B模型在各种分类任务和数据集上优于RoBERTa-large和其他解码器LLMs。此外，综合的多任务微调LLMs在两个数据集上的两个任务中都匹配了双模型设置的性能。总体而言，我们的研究为基于编码器和LLM的文本分类任务提供了一个全面的基准，并展示了一种将两个或多个完全微调的解码器LLM结合起来的方法，以降低延迟并保持等效性能。|\n",
        "2412.08585": "|**2024-12-11**|**TURBOATTENTION: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大型语言模型（LLM）推理需要大量的计算和内存，尤其是在关键注意力机制上。虽然量化技术，如FlashAttention加速算法，已经提高了整体推理的效率，但它们解决了问题的不同方面：量化专注于权重-激活操作，而FlashAttention提升了执行效率但需要高精度格式。最近的键值（KV）缓存量化减少了内存带宽，但仍然需要浮点数反量化以进行注意力操作。我们提出了TurboAttention，这是一种使注意力量化执行同时解决内存和计算效率的综合方法。我们的解决方案引入了两项关键创新：FlashQ，这是一种头部注意力量化技术，能够压缩KV缓存并实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它在注意力中的指数运算过程中消除了对FP32反量化的需求。实验结果表明，TurboAttention在注意力方面实现了1.2-1.8倍的加速，将KV缓存大小减少了4.4倍以上，并在FP16基线之上实现了高达2.37倍的最大吞吐量，同时在各种数据集和模型上优于最先进的量化和压缩技术。|\n",
        "2412.09618": "|**2024-12-12**|**EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM**|Zhuofan Zong et.al.|[2412.09618](http://arxiv.org/abs/2412.09618)|null|在扩散模型的个性化方面取得了显著成就。传统的无需调整的方法通常通过平均多个参考图像的图像嵌入作为注入条件来编码多个参考图像，但这样的图像独立操作无法在图像之间执行交互，以捕捉多个参考图像中的一致视觉元素。尽管基于调整的低秩自适应（LoRA）可以通过训练过程有效地从多个图像中提取一致元素，但它需要对每个不同的图像组进行特定的微调。本文介绍了一种新的即插即用自适应方法EasyRef，它使扩散模型能够根据多个参考图像和文本提示进行条件化。为了有效地利用多个图像中的一致视觉元素，我们利用了多模态大型语言模型（MLLM）的多图像理解和指令遵循能力，提示它根据指令捕捉一致视觉元素。此外，通过适配器将MLLM的表示注入到扩散过程中，可以轻松地泛化到未见领域，挖掘未见数据中的一致视觉元素。为了减轻计算成本并增强细粒度细节保留，我们引入了一种高效的参考聚合策略和渐进式训练方案。最后，我们引入了MRBench，一个新的多参考图像生成基准。实验结果表明，EasyRef优于IP-Adapter等无需调整的方法和LoRA等基于调整的方法，在各个领域实现了优越的美学质量和鲁棒的零样本泛化。|\n",
        "2412.09612": "|**2024-12-12**|**Olympus: A Universal Task Router for Computer Vision Tasks**|Yuanze Lin et.al.|[2412.09612](http://arxiv.org/abs/2412.09612)|**[link](https://github.com/yuanze-lin/olympus_page)**|**我们介绍了一种名为Olympus的新方法，该方法将多模态大型语言模型（MLLMs）转换为一个能够处理多种计算机视觉任务的统一框架。利用控制器MLLM，Olympus将超过20个专门的任务（包括图像、视频和3D对象）分配给专用模块。这种基于指令的路由通过连锁动作实现复杂的工作流程，无需训练重量级的生成模型。Olympus可以轻松集成到现有的MLLMs中，通过可比的性能扩展其功能。实验结果表明，Olympus在20个任务上实现了平均路由准确率为94.75%，在连锁动作场景中的准确率为91.82%，展示了其作为通用任务路由器的有效性，能够解决各种计算机视觉任务。项目页面：https://github.com/yuanze-lin/Olympus_page**|\n",
        "2412.09604": "|**2024-12-12**|**SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**|Hao Li et.al.|[2412.09604](http://arxiv.org/abs/2412.09604)|null|本文介绍了大型语言模型（LLMs）在多模态领域的显著成功，并在图像理解和生成方面取得了卓越的表现。近期开发统一的多模态大型语言模型（MLLMs）的尝试也显示出良好的效果。然而，现有方法往往涉及复杂的模型架构或训练流程设计，增加了模型训练和扩展的难度。在这篇论文中，我们提出了SynerGen-VL，这是一种简单而强大的无编码器MLLM，能够实现图像理解和生成。为了解决现有无编码器统一MLLM中识别出的挑战，我们引入了标记折叠机制和基于视觉专家的渐进对齐预训练策略，这些机制有效地支持了高分辨率图像理解的同时降低了训练复杂性。在用大规模混合图像-文本数据进行统一下一标记预测目标训练后，SynerGen-VL在可比或更小的参数规模下达到了或超过了现有无编码器统一MLLM的性能，并缩小了与特定任务最先进模型之间的差距，突显了未来统一MLLM的可行路径。我们的代码和模型将予以发布。|\n",
        "2412.09603": "|**2024-12-12**|**Do Multimodal Large Language Models See Like Humans?**|Jiaying Lin et.al.|[2412.09603](http://arxiv.org/abs/2412.09603)|null|多模态大型语言模型（MLLMs）在各种视觉任务上取得了令人瞩目的成果，得益于最近大型语言模型的发展。然而，一个关键问题仍未得到解决：MLLMs是否以与人类相似的方式感知视觉信息？当前的基准测试缺乏评估MLLMs从这一角度的能力。为了应对这一挑战，我们引入了HVSBench，这是一个大规模基准测试，旨在评估MLLMs与人类视觉系统（HVS）在反映人类视觉的基本视觉任务上的对齐程度。HVSBench精心挑选了超过85K个多模态样本，涵盖了HVS中的13个类别和5个领域，包括突出度、瞬间识别、优先级排序、自由观看和搜索。广泛的实验表明，我们的基准测试在全面评估MLLMs方面的有效性。具体来说，我们评估了13个MLLMs，结果显示即使是表现最好的模型也仍有很大的提升空间，其中大多数仅取得了中等的结果。我们的实验表明，HVSBench为最前沿的MLLMs提出了新的和重大的挑战。我们相信HVSBench将促进对人类对齐和可解释的MLLMs的研究，标志着理解MLLMs如何感知和处理视觉信息的关键一步。|\n",
        "2412.09596": "|**2024-12-12**|**InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**|Pan Zhang et.al.|[2412.09596](http://arxiv.org/abs/2412.09596)|**[link](https://github.com/internlm/internlm-xcomposer)**|**长期以来，创建能够像人类认知一样在长时间内与环境互动的AI系统一直是研究目标。近年来，多模态大型语言模型（MLLMs）在开放式理解方面取得了重大进展。然而，连续和同时进行感知、记忆和推理的挑战在很大程度上仍未被探索。当前的MLLMs受限于其序列到序列的架构，这限制了它们同时处理输入和生成响应的能力，类似于感知时无法思考。此外，依赖长上下文来存储历史数据对于长期交互来说不切实际，因为保留所有信息变得昂贵且效率低下。因此，本项目不是依赖于单一基础模型来执行所有功能，而是从专用通用AI的概念中汲取灵感，引入了解耦的流感知、推理和记忆机制，使系统能够实时处理流式视频和音频输入。提出的框架InternLM-XComposer2.5-OmniLive（IXC2.5-OL）包含三个关键模块：（1）流感知模块：实时处理多模态信息，将关键细节存储在记忆中，并根据用户查询触发推理。（2）多模态长记忆模块：整合短期和长期记忆，将短期记忆压缩为长期记忆，以实现高效检索和提高准确性。（3）推理模块：响应查询并执行推理任务，与感知和记忆模块协调。本项目模拟了类似人类的认知，使多模态大型语言模型能够随着时间的推移提供持续和自适应的服务。**|\n",
        "2412.09572": "|**2024-12-12**|**DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction**|Yu Feng et.al.|[2412.09572](http://arxiv.org/abs/2412.09572)|null|量化大型语言模型（LLMs）在事实参数知识方面的不确定性，尤其是在黑盒设置中，是一个重大挑战。现有方法通过评估模型对原始查询的响应的自我一致性来衡量模型的不确定性，但并不总是能够捕捉到真正的不确定性。模型可能会对原始查询作出一致的错误回答，同时对同一查询的不同角度的多样化问题作出正确回答，反之亦然。在本文中，我们提出了一种新颖的方法，名为DiverseAgentEntropy，用于在假设如果模型是确定性的，它应该能够跨越关于同一原始查询的多样化问题的集合中一致地回忆起原始查询的回答的情况下，评估模型的不确定性。我们进一步实施了一种弃权策略，在不确定性高时抑制响应。我们的方法提供了对模型可靠性的更准确预测，并进一步检测了幻觉，优于其他基于自我一致性的方法。此外，它还表明，现有模型在已知正确答案的情况下，往往无法在多样化的不同问题下一致地检索到同一查询的正确答案。|\n",
        "2412.09563": "|**2024-12-12**|**Does Representation Matter? Exploring Intermediate Layers in Large Language Models**|Oscar Skean et.al.|[2412.09563](http://arxiv.org/abs/2412.09563)|null|理解定义大型语言模型（LLMs）中良好表示的因素对于理论理解和实际应用都是至关重要的。在这篇论文中，我们调查了包括Transformer和状态空间模型（SSMs）在内的各种LLM架构中的中间表示质量。我们发现，中间层往往比最终层为下游任务提供更丰富的表示。为了衡量表示质量，我们调整并应用了一套原本在其他背景下提出的指标，如提示熵、曲率和增强不变性。我们的实证研究表明，存在显著的架构差异，表示在训练过程中的演变，以及输入随机性和提示长度等因素如何影响每一层。值得注意的是，我们在一些中间层的熵中观察到双峰模式，并考虑了与训练数据相关的潜在解释。总的来说，我们的研究结果揭示了LLMs的内部机制，并指导了架构优化和训练的策略。|\n",
        "2412.09560": "|**2024-12-12**|**Foundational Large Language Models for Materials Research**|Vaibhav Mishra et.al.|[2412.09560](http://arxiv.org/abs/2412.09560)|null|材料发现与开发对于解决全球挑战至关重要。然而，材料科学文献中包含大量文本数据的指数式增长，在知识提取、综合和科学推理方面造成了显著的瓶颈。大型语言模型（LLMs）通过自动分析和预测，为加速材料研究提供了前所未有的机会。尽管如此，它们的有效部署需要针对特定领域进行适应性调整以理解和解决领域相关任务。在这里，我们介绍了LLaMat，这是一个通过在广泛的材料文献和晶体学数据集上持续预训练LLaMA模型而开发的用于材料科学的基座模型系列。通过系统评估，我们证明了LLaMat在特定于材料科学的自然语言处理和结构化信息提取方面表现出色，同时保持着一般的语言能力。专门化的LLaMat-CIF变体在晶体结构生成方面展现出前所未有的能力，预测了周期表中覆盖范围广泛的稳定晶体。有趣的是，尽管与LLaMA-2相比，LLaMA-3的性能更优，但我们观察到LLaMat-2在不同材料科学任务中的特定领域性能得到了意想不到的增强，包括从文本和表格中提取结构化信息，尤其是在晶体结构生成方面，这可能是过度训练的LLMs中潜在的自适应刚性。总之，本研究证明了领域适应性在开发实际可部署的LLM协同飞行员进行材料研究中的有效性。超越材料科学，我们的发现揭示了LLMs领域适应性的重要考虑因素，如模型选择、训练方法以及特定领域的性能，这些可能影响专门科学人工智能系统的发展。|\n",
        "2412.09549": "|**2024-12-12**|**Exemplar Masking for Multimodal Incremental Learning**|Yi-Lun Lee et.al.|[2412.09549](http://arxiv.org/abs/2412.09549)|**[link](https://github.com/yilunlee/exemplar_masking_mcil)**|**多模态增量学习需要在处理来自多种模态的信息的同时，同时学习新的知识而不忘记之前学习的信息。这项任务面临许多挑战，主要包括基于实例的方法中多模态数据更大的存储空间以及在大规模多模态模型上微调的计算需求。在本文中，我们利用参数高效微调方案来减轻微调的负担，并提出实例掩码框架以有效地重现旧知识。具体来说，根据注意力权重和不同模态之间的相关性，对非重要标记进行掩码，显著减少了实例的存储空间，从而在相同的内存缓冲区下节省了更多实例。此外，我们设计了一种多模态数据增强技术，以多样化实例，以便重现先前知识。在实验中，我们不仅评估了我们方法在现有多模态数据集上的表现，还将ImageNet-R数据集扩展为一个多模态数据集，作为实际应用，其中通过查询多模态大型语言模型（例如InstructBLIP）生成字幕。广泛的实验表明，在相同的有限内存缓冲区下，我们的实例掩码框架在效率和对抗灾难性遗忘方面更加鲁棒。代码可在https://github.com/YiLunLee/Exemplar_Masking_MCIL上获取。**|\n",
        "2412.09529": "|**2024-12-12**|**Can Modern LLMs Act as Agent Cores in Radiology~Environments?**|Qiaoyu Zheng et.al.|[2412.09529](http://arxiv.org/abs/2412.09529)|**[link](https://github.com/magic-ai4med/radabench)**|在大型语言模型（LLMs）的进步为基于LLMs的代理系统铺平了道路，这些系统在各种领域提供了更高的准确性和可解释性。放射学由于其复杂的分析需求，是这些代理应用的理想领域。本文旨在调查构建具体放射学代理的先决问题，即“现代LLMs能否在放射学环境中充当代理核心？”为了调查这个问题，我们介绍了RadABench，并具有以下三个方面的贡献：首先，我们提出了RadABench-Data，这是一个用于基于LLMs的代理的综合合成评估数据集，它来源于一个包含6个解剖部位、5种成像方式、10种工具类别和11项放射学任务的广泛分类。其次，我们提出了RadABench-EvalPlat，这是一个新型代理评估平台，具有提示驱动的流程和模拟广泛放射学工具集的能力。第三，我们从5个角度使用多个指标评估了7个领先LLMs在我们基准测试上的性能。我们的发现表明，尽管当前LLMs在许多领域表现出强大的能力，但它们仍不够先进，无法作为完全运行中的放射学代理系统的核心。此外，我们确定了影响基于LLMs的代理核心性能的关键因素，为临床医生提供了如何在实际放射学实践中有效应用代理系统的见解。所有我们的代码和数据都已开源在https://github.com/MAGIC-AI4Med/RadABench。|\n",
        "2412.10372": "|**2024-12-13**|**UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities**|Muhammad Uzair Khattak et.al.|[2412.10372](http://arxiv.org/abs/2412.10372)|**[link](https://github.com/mbzuai-oryx/unimed-clip)**|**通过对比学习训练的视觉-语言模型（VLMs）在自然图像任务中取得了显著的成果。然而，由于公开可获取的大规模医学图像-文本数据集稀缺，它们在医学领域的应用仍然有限。现有的医学VLMs要么在封闭源专有数据集或相对较小的开源数据集上训练，这些数据集的泛化能力不强。同样，大多数模型仍然局限于单一或有限的医学成像领域，这再次限制了它们在其他模态上的适用性。为了解决这一差距，我们引入了UniMed，这是一个包含超过530万图像-文本对的开放源代码多模态医学数据集，涵盖六种不同的成像模态：X光、CT、MRI、超声、病理和眼底。UniMed是通过一个数据收集框架开发的，该框架利用大型语言模型（LLMs）将特定模态的分类数据集转换为图像-文本格式，同时结合现有医学领域的图像-文本数据，从而促进可扩展的VLM预训练。使用UniMed，我们训练了UniMed-CLIP，这是一个针对六个模态的统一VLM，它在零样本评估中显著优于现有的通用VLMs，并匹配了特定模态的医学VLMs，实现了显著的增益。例如，UniMed-CLIP在21个数据集上的平均绝对增益比BiomedCLIP（在专有数据上训练）高出+12.61，而训练数据量减少了3倍。为了促进未来的研究，我们在https://github.com/mbzuai-oryx/UniMed-CLIP上发布了UniMed数据集、训练代码和模型。**|\n",
        "2412.10353": "|**2024-12-13**|**Robust image classification with multi-modal large language models**|Francesco Villani et.al.|[2412.10353](http://arxiv.org/abs/2412.10353)|null|深度神经网络容易受到对抗样本的攻击，即精心设计的输入样本可以导致模型以高置信度做出错误预测。为了缓解这些脆弱性，已经提出了基于对抗训练和检测的防御措施来提前加强模型。然而，大多数这些方法只关注单一的数据模态，忽略了输入的视觉模式和文本描述之间的关系。在这篇论文中，我们提出了一种新的防御方法，名为Multi-Shield，旨在通过结合和补充这些防御措施与多模态信息来进一步增强其鲁棒性。Multi-Shield利用多模态大型语言模型来检测对抗样本，并在输入的文本和视觉表示之间没有一致性的情况下避免不确定的分类。在CIFAR-10和ImageNet数据集上进行的广泛评估，使用鲁棒和非鲁棒的图像分类模型，表明Multi-Shield可以轻松集成以检测和拒绝对抗样本，并优于原始的防御措施。|\n",
        "2412.10347": "|**2024-12-13**|**COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models**|Yuchen Ren et.al.|[2412.10347](http://arxiv.org/abs/2412.10347)|null|作为中心法则的关键要素，DNA、RNA和蛋白质在保证准确的遗传表达和实施方面发挥着至关重要的作用，从而维持生命。尽管对这些分子的研究对医学、农业和工业等领域产生了深远的影响，但机器学习方法的多样性——从传统的统计方法到深度学习模型和大型语言模型——给研究人员选择最适合特定任务的最优模型带来了挑战，尤其是在缺乏全面基准的情况下，对于跨组学和多组学任务尤其如此。为了解决这个问题，我们引入了第一个综合的多组学基准COMET（生物全面多组学评估任务和语言模型基准），旨在评估单组学、跨组学和多组学任务中的模型。首先，我们收集和开发了一个多样化的下游任务和数据集集合，涵盖了DNA、RNA和蛋白质的关键结构和功能方面，包括跨越多个组学级别的任务。然后，我们评估了现有的DNA、RNA和蛋白质的基础语言模型以及新提出的多组学方法，提供了关于它们在不同生物模式数据整合和分析中的性能的宝贵见解。这个基准旨在定义多组学研究中的关键问题，并指导未来的研究方向，最终通过综合和不同组学数据分析促进对生物学过程的理解的进步。|\n",
        "2412.10342": "|**2024-12-13**|**Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining**|Zhiqi Ge et.al.|[2412.10342](http://arxiv.org/abs/2412.10342)|null|数字代理越来越多地被用于自动化互动数字环境中的任务，如网页、软件应用程序和操作系统。基于大型语言模型（LLMs）的基于文本的代理由于平台特定的API而经常需要频繁更新，而利用多模态大型语言模型（MLLMs）的视觉代理通过直接与图形用户界面（GUIs）交互提供了更高的适应性。然而，这些代理在视觉感知方面面临着重大挑战，尤其是在处理高分辨率、视觉复杂的数字环境时。本文介绍了一种名为Iris的基础视觉代理，它通过两个关键创新来应对这些挑战：信息敏感裁剪（ISC）和自我精炼双重学习（SRDL）。ISC通过一个边缘检测算法动态地识别和优先处理视觉密集区域，通过分配更多计算资源到信息密度较高的区域来实现高效处理。SRDL通过利用双重学习循环来增强代理处理复杂任务的能力，在该循环中，指代（描述UI元素）的改进强化了定位（定位元素），反之亦然，而无需额外的标注数据。实证评估表明，Iris在多个基准测试中实现了最先进的性能，仅需850K GUI标注，就优于使用10倍多训练数据的方法。这些改进进一步转化为在Web和OS代理下游任务中的显著收益。|\n",
        "2412.10321": "|**2024-12-13**|**AdvPrefix: An Objective for Nuanced LLM Jailbreaks**|Sicheng Zhu et.al.|[2412.10321](http://arxiv.org/abs/2412.10321)|**[link](https://github.com/facebookresearch/jailbreak-objectives)**|许多针对大型语言模型（LLMs）的越狱攻击依赖于一个共同的目标：让模型以“当然，这是（有害请求）”为前缀进行回应。虽然这个目标很简单，但它有两个局限性：对模型行为的控制有限，通常导致不完整或不切实际的回应，以及固定的格式阻碍了优化。为了解决这些局限性，我们引入了AdvPrefix，这是一种新的前缀强制目标，它能够以更细微的方式控制模型行为，同时易于优化。我们的目标利用基于模型依赖的前缀，这些前缀是根据两个标准自动选择的：高预填充攻击成功率和低负对数似然。它还可以通过为单个用户请求使用多个前缀来进一步简化优化。AdvPrefix可以无缝集成到现有的越狱攻击中，以免费提高其性能。例如，只需在Llama-3上将GCG攻击的目标前缀替换为我们提供的，就能将细微攻击的成功率从14%提高到80%，这表明当前的对抗性训练在泛化到未见过的前缀方面存在困难。我们的工作证明了越狱目标在实现细微越狱中的重要性。|\n",
        "2412.10316": "|**2024-12-13**|**BrushEdit: All-In-One Image Inpainting and Editing**|Yaowei Li et.al.|[2412.10316](http://arxiv.org/abs/2412.10316)|null|随着基于扩散模型的图像编辑技术的发展，图像编辑技术取得了显著进步，这些模型采用了基于反转和基于指令的方法。然而，当前基于反转的方法在处理大修改（例如添加或删除对象）时遇到困难，因为反转噪声的结构性质阻碍了重大的变化。同时，基于指令的方法通常将用户限制在黑盒操作中，限制了直接交互以指定编辑区域和强度。为了解决这些限制，我们提出了BrushEdit，这是一种基于修复的指令引导图像编辑新范式，它利用多模态大型语言模型（MLLMs）和图像修复模型来实现自主、用户友好和交互式的自由形式指令编辑。具体来说，我们设计了一个系统，通过在代理协作框架中集成MLLMs和双分支图像修复模型，实现自由形式指令编辑，该系统可以进行编辑类别分类、主要物体识别、蒙版获取和编辑区域修复。大量实验表明，我们的框架有效地结合了MLLMs和修复模型，在包括蒙版区域保留和编辑效果一致性在内的七个指标上取得了优异的性能。|\n",
        "2412.10291": "|**2024-12-13**|**Still \"Talking About Large Language Models\": Some Clarifications**|Murray Shanahan et.al.|[2412.10291](http://arxiv.org/abs/2412.10291)|null|我的论文《关于大型语言模型的讨论》多次被解读为倡导对大型语言模型采取还原论立场。但我的论文并非旨在如此，我也不支持这样的观点。这篇简短的笔记将论文置于一个更广泛的哲学项目背景下，该项目关注的是（误）使用词汇的问题，而非形而上学，秉承维特根斯坦后期著作的精神。|\n",
        "2412.10281": "|**2024-12-13**|**One world, one opinion? The superstar effect in LLM responses**|Sofie Goethals et.al.|[2412.10281](http://arxiv.org/abs/2412.10281)|null|随着大型语言模型（LLMs）正在塑造在线信息共享和获取的方式，它们的观点有可能影响广泛的受众。这项研究通过使用十种不同的语言进行提示，探讨了语言多样性对LLMs在不同领域视为最突出人物的影响。我们的发现显示，在回应中存在低多样性，少数人物在多种语言中占据主导地位（也称为“明星效应”）。这些结果突显了当LLMs检索主观信息时，缩小全球知识表征的风险。|\n",
        "2412.10271": "|**2024-12-13**|**Benchmarking Linguistic Diversity of Large Language Models**|Yanzhu Guo et.al.|[2412.10271](http://arxiv.org/abs/2412.10271)|**[link](https://github.com/yanzhuguo/llm-diversity)**|**大型语言模型（LLMs）的开发和评估主要集中于其任务解决能力，近期的一些模型在某些领域甚至超越了人类表现。然而，这种关注往往忽略了机器生成语言在词汇选择、句法构造和意义表达方面是否达到人类水平的多样性，引发了对语言生成基础是否已被充分解决的疑问。本文强调了在LLMs生成内容的在线内容激增的背景下，检验语言模型保留人类语言丰富性的重要性。我们提出一个从词汇、句法和语义等多个语言学多样性维度评估LLMs的全面框架。利用这一框架，我们对多个最先进的LLMs在所有多样性维度上进行基准测试，并对句法多样性进行了深入研究。最后，我们分析了不同开发和部署选择如何影响LLMs输出的语言多样性。**|\n",
        "2412.10270": "|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有通用能力的AI代理提供了引人注目的基础。这些代理可能很快将在现实生活中大规模部署，代表个人（例如，AI助手）或人类群体（例如，AI加速企业）的利益。目前，关于多个LLM代理在迭代部署的多代中相互作用的动态知之甚少。在这篇论文中，我们研究了在存在背叛动机的情况下，“LLM代理社会”是否能学会相互有益的社会规范，这是人类社会性的一个独特特征，可能是文明成功的关键。具体而言，我们研究了LLM代理在经典迭代捐赠游戏中间接互惠的演变，这些代理可以观察到同伴的最近行为。我们发现，合作演变的差异在基础模型之间非常明显，Claude 3.5 Sonnet代理的社会实现了显著更高的平均得分，而Gemini 1.5 Flash则优于GPT-4o。此外，Claude 3.5 Sonnet可以利用额外的成本惩罚机制实现更高的得分，而Gemini 1.5 Flash和GPT-4o则不能。对于每个模型类别，我们还观察到随机种子下涌现行为的差异，这表明对初始条件的敏感依赖性被低估。我们建议，我们的评估机制可以激发一类新的、成本效益高且信息丰富的LLM基准，重点关注LLM代理部署对社会合作基础设施的影响。|\n",
        "2412.12094": "|**2024-12-16**|**SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**|Guoxuan Chen et.al.|[2412.12094](http://arxiv.org/abs/2412.12094)|**[link](https://github.com/HKUDS/SepLLM)**|**大型语言模型（LLMs）在自然语言处理的各个任务上表现出色。然而，它们庞大的体积带来了巨大的挑战，特别是在计算需求和推理速度方面，这是由于它们的二次复杂性。在这项工作中，我们识别出一个关键模式：某些看似无意义的特殊标记（即分隔符）与语义上有意义的标记相比，对注意力分数的贡献不成比例。这一观察表明，这些分隔符之间的段信息可以有效地压缩到这些分隔符本身，而不会造成显著的信息损失。受此启发，我们引入了SepLLM，这是一个即插即用的框架，通过压缩这些段和消除冗余标记来加速推理。此外，我们还实现了高效的内核以加速训练。在无需训练、从头开始训练和训练后设置中的实验结果表明了SepLLM的有效性。值得注意的是，使用Llama-3-8B骨干网络，SepLLM在GSM8K-CoT基准测试上实现了超过50%的KV缓存减少，同时保持了可比的性能。此外，在流式设置中，SepLLM能够有效地处理长达400万个标记或更多的序列，同时保持一致的语言建模能力。**|\n",
        "2412.12087": "|**2024-12-16**|**Instruction-based Image Manipulation by Watching How Things Move**|Mingdeng Cao et.al.|[2412.12087](http://arxiv.org/abs/2412.12087)|null|本文介绍了一种新型的数据集构建流程，该流程从视频中采样帧对，并使用多模态大型语言模型（MLLMs）生成编辑指令，用于训练基于指令的图像操作模型。视频帧天生保留了主题和场景的身份，确保了编辑过程中的内容一致性。此外，视频数据捕捉到了多样、自然的动态，如非刚性主题运动和复杂的摄像机运动，这些动态在其他情况下难以建模，使其成为可扩展数据集构建的理想来源。采用这种方法，我们创建了一个新的数据集，用于训练InstructMove模型，该模型能够执行基于指令的复杂操作，这些操作在合成数据集中难以实现。我们的模型在调整主题姿势、重新排列元素和改变摄像机视角等任务中展现了最先进的性能。|\n",
        "2412.12077": "|**2024-12-16**|**CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology**|Yuxuan Sun et.al.|[2412.12077](http://arxiv.org/abs/2412.12077)|null|大型多模态模型（LMMs）的出现为病理学带来了重大进步。以往的研究主要侧重于分别训练补丁级和全切片图像（WSI）级模型，这限制了在不同补丁和WSI之间整合所学知识，并导致了冗余模型。在本研究中，我们介绍了CPath-Omni，这是第一个设计用于统一补丁级和WSI级图像分析的150亿参数LMM，它将包括分类、视觉问答、字幕和视觉提示等多种任务在这一水平和另一水平上整合。大量实验表明，CPath-Omni在42个数据集中的39个任务上实现了最先进的（SOTA）性能，超过了为单个任务训练的特定模型的表现或与之相当。此外，我们为CPath-Omni开发了一个专门的病理学CLIP视觉处理器，CPath-CLIP，它首次将不同的视觉模型结合在一起，并将一个大语言模型作为文本编码器整合进去，构建了一个更强大的CLIP模型，在九个零样本和四个小样本数据集上实现了SOTA性能。我们的研究结果突出了CPath-Omni统一多种病理学任务的能力，展示了其在简化并推进病理学基础模型领域的潜力。|\n",
        "2412.12075": "|**2024-12-16**|**CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding**|Guo Chen et.al.|[2412.12075](http://arxiv.org/abs/2412.12075)|null|现有的多模态大型语言模型（MLLM）的视频理解基准大多仅针对短视频。对于长视频理解，现有的基准往往仅依赖于多项选择题（MCQs）。然而，由于基于MCQ的评估固有的局限性以及MLLM推理能力的增强，模型可以通过结合短视频理解和排除法来给出当前答案，而不真正理解视频内容。为了解决这一差距，我们引入了CG-Bench，这是一个专为长视频中的线索-基础问答而设计的创新基准。CG-Bench强调模型检索相关线索的能力，增强评估的可信度。它具有1,219个手动编纂的视频，按14个主要类别、171个次要类别和638个三级类别进行分类，使其成为长视频分析最大的基准。该基准包括12,129个问答对，涵盖三种主要问题类型：感知、推理和幻觉。为了弥补纯MCQ评估的不足，我们设计了两种新颖的基于线索的评估方法：线索-基础白盒和黑盒评估，以评估模型是否基于对视频的正确理解生成答案。我们在CG-Bench上评估了多个封闭源和开源的MLLM。结果表明，与短视频相比，当前模型在理解长视频方面的表现显著不足，开源模型与商业模型之间存在显著差距。我们希望CG-Bench能够推进更可信、更有能力的MLLM在长视频理解方面的发展。所有注释和视频数据发布在https://cg-bench.github.io/leaderboard/。|\n",
        "2412.12072": "|**2024-12-16**|**Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats**|Kuleen Sasse et.al.|[2412.12072](http://arxiv.org/abs/2412.12072)|**[link](https://github.com/kuleens/fetch-dog-whistle)**|**请注意：本文包含可能令某些读者感到不安或冒犯的内容。犬哨子是具有双重含义的编码表达：一方面旨在对公众（外群体）传达，另一方面则向特定受众（内群体）传达特定信息。通常，这些表达被用来传达有争议的政治观点，同时保持合理否认的可能性和绕过内容审查过滤器。犬哨子的识别依赖于经过编辑的词汇表，而这些词汇表难以跟上时代的步伐。我们提出了\\textbf{FETCH!}，这是一个在大量社交媒体语料库中寻找新颖犬哨子的任务。我们发现，最先进的技术在三个不同的社交媒体案例研究中未能取得有意义的成果。我们提出了\\textbf{EarShot}，一个结合了向量数据库和大型语言模型（LLM）优势的新颖系统，以高效和有效地识别新的犬哨子。**|\n",
        "2412.12039": "|**2024-12-16**|**Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection**|Ira Ceka et.al.|[2412.12039](http://arxiv.org/abs/2412.12039)|null|尽管大型语言模型（LLMs）取得了显著的成就，但在漏洞检测等应用任务上表现有限。我们研究了各种用于漏洞检测的提示策略，并在这一探索过程中，提出了一种将漏洞的自然语言描述与对比链式推理方法相结合的提示策略，该方法通过使用来自合成数据集的对比样本来增强。我们的研究突出了LLMs通过将自然语言描述、对比推理和合成示例整合到全面的提示框架中检测漏洞的潜力。我们的结果显示，这种方法可以增强LLMs对漏洞的理解。在一个高质量的漏洞检测数据集（如SVEN）上，我们的提示策略可以将准确率、F1分数和成对准确率分别提高23%、11%和14%。|\n",
        "2412.12009": "|**2024-12-16**|**SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval**|Yueqian Lin et.al.|[2412.12009](http://arxiv.org/abs/2412.12009)|null|我们引入了一种新的长文本任务——语音信息检索（SIR），针对语音大型语言模型（Speech LLMs），并提出了SPIRAL，一个包含1,012个样本的基准测试，用于检验模型从大约90秒的语音输入中提取关键细节的能力。尽管当前语音大型语言模型在短文本任务上表现出色，但它们在处理较长音频序列的计算和表示需求上存在困难。为了解决这一局限性，我们提出了一种无训练的token剪枝策略——SpeechPrune，该策略利用语音文本相似度和近似注意力分数来高效地丢弃无关的token。在SPIRAL中，SpeechPrune在20%的剪枝率下，相较于原始模型和随机剪枝模型，分别实现了29%和47%的准确率提升。SpeechPrune即使在80%的剪枝水平下也能保持网络性能。这种方法突显了token级剪枝在高效和可扩展的长文本语音理解中的潜力。|\n",
        "2412.12004": "|**2024-12-16**|**The Open Source Advantage in Large Language Models (LLMs)**|Jiya Manchanda et.al.|[2412.12004](http://arxiv.org/abs/2412.12004)|null|大型语言模型（LLMs）标志着自然语言处理（NLP）领域的关键转变，它们在文本生成、翻译和特定领域推理方面取得了显著进展。由专有数据集和大量计算资源驱动的闭源模型，如GPT-4，目前处于性能最前沿。然而，它们因“黑箱”性质和限制可访问性而受到批评，这阻碍了可重复性和公平的AI发展。相比之下，LLaMA和BLOOM等开源倡议通过社区驱动的发展和计算效率优先，实现了民主化。这些模型在语言多样性和特定领域应用方面显著缩小了性能差距，同时为全球的研究人员和开发者提供了可访问的工具。值得注意的是，这两种范式都依赖于基础架构创新，如Vaswani等人于2017年提出的Transformer框架。闭源模型通过有效扩展而表现出色，而开源模型则适应于代表性不足的语言和领域中的实际应用。低秩适应（LoRA）和指令调整数据集等技术的应用使得开源模型在资源有限的情况下也能取得有竞争力的结果。毫无疑问，闭源和开源方法之间的紧张关系凸显了AI领域透明度与专有控制之间的更广泛辩论。伦理方面的考虑进一步突显了这种分歧。闭源系统限制了外部审查，而开源模型促进了可重复性和合作，但缺乏标准化的审计文档框架来减轻偏见。利用两种范式优势的混合方法可能会塑造LLM创新的未来，确保可访问性、具有竞争力的技术性能和道德部署。|\n",
        "2412.12001": "|**2024-12-16**|**LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts**|Zhuhao Wang et.al.|[2412.12001](http://arxiv.org/abs/2412.12001)|**[link](https://github.com/zh-wang-med/llm-rg4)**|编写放射学报告是一项复杂的任务，需要灵活性，其中放射科医生根据可用信息和特定的临床需求调整内容。然而，目前大多数放射学报告生成（RRG）模型都受到固定任务范式的限制，例如从单一图像中预测完整的“发现”部分，这本质上涉及到输入和输出的不匹配。训练后的模型缺乏处理多样化输入的灵活性，可能会生成有害的、与输入无关的幻觉。为了弥合当前RRG模型与实际临床需求之间的差距，我们首先开发了一个数据生成管道来创建一个新的MIMIC-RG4数据集，该数据集考虑了四种常见的放射学报告起草场景，并实现了输入和输出的完美对应。其次，我们提出了一种基于大型语言模型（LLM）的RRG框架，名为LLM-RG4，它利用LLM灵活的指令遵循能力和广泛的一般知识。我们进一步开发了一个自适应标记融合模块，它可以提供处理不同输入组合的灵活性，同时最大限度地减少与增加的输入量相关的额外计算负担。此外，我们提出了一个标记级损失加权策略，以引导模型将注意力集中在积极的和不确定的描述上。实验结果表明，LLM-RG4在MIMIC-RG4和MIMIC-CXR数据集上均实现了临床效率和自然语言生成的最先进性能。我们定量证明，我们的模型具有最小的与输入无关的幻觉，而当前的开源模型通常都存在这个问题。|\n",
        "2412.11995": "|**2024-12-16**|**Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support**|Devika Venugopalan et.al.|[2412.11995](http://arxiv.org/abs/2412.11995)|**[link](https://github.com/devika-prog/caregiver-conversational-support-tool)**|**照顾者（即父母和孩子的照顾社区成员）是学习分析中被低估的利益相关者。尽管照顾者的参与可以提高学生的学术成绩，但许多障碍阻碍了他们的参与，其中最显著的是与现代学校课程相关的知识差距。学习分析中的一个新兴研究课题是混合辅导，它包括教学和动机支持。照顾者在家庭作业中扮演着类似的角色，但尚不清楚学习分析如何支持他们。我们与照顾者合作的研究表明，对话支持是向照顾者提供有效支持学生学习的指导的有前景的方法。我们开发了一个系统，通过由大型语言模型（LLM）生成对话推荐来为照顾者提供教学支持。针对LLM已知的教学局限性，我们在使用开源的Llama 3 LLM进行提示工程实验的同时，利用了辅导系统的教学智能。这个LLM为照顾者提供了通过聊天支持孩子数学练习的消息推荐。通过少量提示和将辅导系统的实时问题解决情境与辅导实践示例相结合，产生了理想的消息推荐。这些推荐与十位初中照顾者进行了评估，他们重视推荐能够通过自我解释促进内容层次的支持和学生的元认知。我们贡献了对如何将辅导系统与LLM最佳结合以通过对话辅助支持混合辅导环境，从而促进照顾者在辅导系统中的有效参与的见解。**|\n",
        "2412.13178": "|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身智能体在执行复杂指令方面具有强大的能力，为具身机器人的潜在部署铺平了道路。然而，一个可预见的问题是，这些具身智能体也可能完美地执行一些危险任务，可能导致现实世界中的损害。为了研究这一问题，我们提出了SafeAgentBench——一个用于具身LLM智能体安全任务规划的新的基准。SafeAgentBench包括：（1）一个包含750个任务的新数据集，涵盖了10种潜在危险和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的基础具身环境，支持多智能体执行，并为8个最先进的基线提供了17个高级动作；（3）从执行和语义角度的可靠评估方法。实验结果表明，表现最佳的基线在安全任务中的成功率达到了69%，但在危险任务中的拒绝率仅为5%，这表明存在显著的安全风险。更多细节和代码可在https://github.com/shengyin1224/SafeAgentBench找到。**|\n",
        "2412.13175": "|**2024-12-17**|**DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation**|Miriam Wanner et.al.|[2412.13175](http://arxiv.org/abs/2412.13175)|null|分解后验证策略用于验证大型语言模型（LLM）生成的陈述，将陈述分解后再进行独立验证。去上下文化通过增强文本（陈述）确保其可以在原始语境之外进行验证，从而实现可靠的验证。虽然分解和去上下文化已被独立探索，但它们在完整系统中的相互作用尚未得到研究。它们相互冲突的目的可能产生紧张关系：分解将原子事实隔离，而去上下文化插入相关信息。此外，去上下文化的子陈述对验证步骤构成挑战：现在包含多个原子事实的增强文本中，应该验证哪一部分？我们对不同的分解、去上下文化和验证策略进行了评估，发现策略的选择会影响最终的事实性评分。此外，我们引入了DnDScore，一种去上下文化感知的验证方法，它将在上下文信息的背景下验证子陈述。|\n",
        "2412.13169": "|**2024-12-17**|**Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study**|Bolei Ma et.al.|[2412.13169](http://arxiv.org/abs/2412.13169)|**[link](https://github.com/soda-lmu/llm-opinion-german)**|**在最近的研究中，大型语言模型（LLMs）被越来越多地用于研究公众意见。本研究调查了LLMs的算法忠实度，即复制人类参与者社会文化背景和细微意见的能力。利用德国纵向选举研究（GLES）的开源问卷调查数据，我们将不同LLMs提示为通过将人口统计学特征纳入角色提示中，生成反映德国不同亚群体的合成公众意见。我们的结果显示，Llama在代表亚群体方面表现优于其他LLMs，尤其是在这些群体内部意见多样性较低时。我们的发现还进一步揭示，LLMs在代表左翼政党如绿党和左翼的支持者方面表现优于其他政党，与右翼政党AfD的匹配度最低。此外，在提示中包含或排除特定变量可以显著影响模型的预测。这些发现强调了将LLMs与更有效地模拟多样化公众意见相一致的重要性，同时最小化政治偏见并增强代表性稳健性。**|\n",
        "2412.13163": "|**2024-12-17**|**C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System**|Parker Addison et.al.|[2412.13163](http://arxiv.org/abs/2412.13163)|null|在寻求利用大型语言模型（LLMs）进行知识查询和分析的组织中，常常面临保持针对特定、最新信息的LLM微调，以确保答案相关性和实际性的挑战。检索增强生成（RAG）迅速成为解决组织维护专有模型挑战的可行方案，同时有助于减少LLM查询响应中的幻觉。然而，RAG在扩展数据管道以支持分层访问和不同数据源方面存在自身问题。在许多情况下，需要查询多个数据孤岛以提供更丰富、更相关的上下文供LLM使用。在组织信任边界内及之间分析数据源往往受到复杂的数据共享政策限制，禁止集中式数据存储，从而阻碍了RAG解决方案的快速和有效设置及扩展。在本文中，我们引入了机密计算（CC）技术作为安全联邦检索增强生成（FedRAG）的解决方案。我们提出的机密联邦RAG系统（C-FedRAG）通过确保上下文机密性，使RAG工作流程能够在数据提供者去中心化网络中安全连接和扩展。我们还展示了如何使用NVIDIA FLARE SDK实现C-FedRAG系统，并使用MedRAG工具包和MIRAGE基准数据集对其性能进行评估。|\n",
        "2412.13148": "|**2024-12-17**|**SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba, 2015）对于大型语言模型的成功至关重要。然而，它们在训练过程中维持额外的移动平均状态，导致内存需求比模型大几倍。这种开销对可扩展性和计算效率施加了限制。另一方面，虽然随机梯度下降（SGD）在内存效率方面是最优的，但它们在LLM训练中的能力有限（Zhao等，2024b）。为了解决这一困境，我们表明预处理SGD足以在LLMs上达到Adam级别的性能。具体来说，我们提出使用两个简单的算子对瞬时随机梯度进行预处理：$\\mathtt{GradNorm}$和$\\mathtt{GradWhitening}$。$\\mathtt{GradNorm}$稳定梯度分布，而$\\mathtt{GradWhitening}$对抗损失景观的局部曲率，分别。这导致了SWAN（带有白化和归一化的SGD），一种消除存储任何累积状态变量需要的随机优化器。经验表明，SWAN具有与SGD相同的内存占用，与Adam相比，总端到端内存减少了$\\approx 50\\%$。在语言建模任务中，SWAN在性能上与Adam相同，甚至有显著提高。具体来说，当使用350M和1.3B参数预训练LLaMa模型时，SWAN通过在不到一半的观察到的标记内达到相同的评估困惑度，实现了2倍的速度提升。|\n",
        "2412.13147": "|**2024-12-17**|**Are Your LLMs Capable of Stable Reasoning?**|Junnan Liu et.al.|[2412.13147](http://arxiv.org/abs/2412.13147)|**[link](https://github.com/open-compass/gpassk)**|**大型语言模型（LLMs）的快速发展在复杂推理任务中展示了显著的进步。然而，基准性能和现实应用之间存在显著的差距。我们认定这一差距主要源于当前的评价协议和指标，它们无法充分捕捉LLMs的全部能力，尤其是在既需要准确性又需要一致性的复杂推理任务中。本研究做出了两项关键贡献。首先，我们引入了G-Pass@k，这是一种新颖的评价指标，它提供对模型性能在多次采样尝试中的连续评估，量化了模型的峰值性能潜力和其稳定性。其次，我们提出了LiveMathBench，这是一个动态基准，包含了具有挑战性的现代数学问题，旨在在评估过程中最大限度地减少数据泄露风险。通过在LiveMathBench上对最先进的LLMs进行G-Pass@k的大量实验，我们提供了对它们的最大能力和运行一致性的全面洞察。我们的发现揭示了LLMs在“现实”推理能力方面有巨大的改进空间，突显了需要更稳健的评价方法。基准和详细结果可在以下网址获取：https://github.com/open-compass/GPassK。**|\n",
        "2412.13103": "|**2024-12-17**|**AI PERSONA: Towards Life-long Personalization of LLMs**|Tiannan Wang et.al.|[2412.13103](http://arxiv.org/abs/2412.13103)|null|在本研究中，我们提出了终身个性化大型语言模型的任务。虽然近年来LLM社区的主流工作主要集中在扩展数据和计算能力以提高LLM的能力，但我们认为，使LLM系统或语言代理能够持续适应每个独特用户的多样化和不断变化的个人资料，并提供最新的个性化援助也非常重要。我们提供了明确的任务表述，并介绍了一个简单、通用、有效且可扩展的框架，用于LLM系统和语言代理的终身个性化。为了促进对LLM个性化未来的研究，我们还介绍了合成现实基准和鲁棒评估指标的方法。我们将发布构建和基准测试终身个性化LLM系统的所有代码和数据。|\n",
        "2412.13102": "|**2024-12-17**|**AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark**|Jianlyu Chen et.al.|[2412.13102](http://arxiv.org/abs/2412.13102)|**[link](https://github.com/air-bench/air-bench)**|**评估在信息检索（IR）模型的发展中起着至关重要的作用。然而，当前基准测试，基于预定义的领域和人工标注的数据，在以经济高效的方式应对新兴领域的评估需求方面存在局限性。为了解决这一挑战，我们提出了自动异构信息检索基准（AIR-Bench）。AIR-Bench有三个关键特点：1）自动化。AIR-Bench中的测试数据由大型语言模型（LLMs）自动生成，无需人工干预。2）异构。AIR-Bench中的测试数据针对不同的任务、领域和语言生成。3）动态。AIR-Bench涵盖的领域和语言不断扩展，为社区开发者提供一个越来越全面的评估基准。我们开发了一个可靠且稳健的数据生成管道，基于真实世界的语料库自动创建多样化的高质量评估数据集。我们的研究发现，AIR-Bench中生成的测试数据与人工标注的测试数据高度一致，使AIR-Bench成为评估IR模型的可靠基准。AIR-Bench的资源可在https://github.com/AIR-Bench/AIR-Bench公开获取。**|\n",
        "2412.13050": "|**2024-12-17**|**Modality-Inconsistent Continual Learning of Multimodal Large Language Models**|Weiguo Pian et.al.|[2412.13050](http://arxiv.org/abs/2412.13050)|null|在这篇论文中，我们介绍了模态不一致持续学习（Modality-Inconsistent Continual Learning，简称MICL），这是一种针对多模态大型语言模型（Multimodal Large Language Models，简称MLLMs）的新持续学习场景，涉及具有不一致模态（图像、音频或视频）和不同任务类型（字幕或问答）的任务。与现有的仅限于视觉或模态增量设置不同，MICL结合了模态和任务类型的转变，这两种转变都会导致灾难性遗忘。为了应对这些挑战，我们提出了MoInCL，它采用了一个伪目标生成模块来减轻先前看到的模态中由于任务类型转变引起的遗忘。此外，它还结合了基于指令的知识蒸馏，以保留模型处理先前学习的模态的能力，当引入新的模态时。我们使用总共六个任务对MICL进行基准测试，并通过实验验证了我们提出的MoInCL的有效性。实验结果突出了MoInCL的优越性，显示出与代表性持续学习基线相比的显著改进。|\n",
        "2412.13018": "|**2024-12-17**|**OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain**|Shuting Wang et.al.|[2412.13018](http://arxiv.org/abs/2412.13018)|**[link](https://github.com/ruc-nlpir/omnieval)**|**作为大型语言模型（LLMs）的一种典型和实际应用，检索增强生成（RAG）技术受到了广泛关注，尤其是在LLMs可能缺乏特定领域知识的垂直领域。在本文中，我们引入了一个金融领域的全向和自动RAG基准，称为OmniEval。我们的基准以其多维评估框架为特点，包括以下方面：  （1）基于矩阵的RAG场景评估系统，将查询分为五个任务类别和16个金融主题，从而对多样化的查询场景进行结构化评估； （2）多维评估数据生成方法，结合基于GPT-4的自动生成和人工标注，在人工评估生成的实例中实现了87.47%的接受率； （3）多阶段评估系统，评估检索和生成性能，对RAG流程进行综合评估； （4）从基于规则和基于LLM的评估指标中衍生出的鲁棒评估指标，通过人工标注和LLM评估器的监督微调增强了评估的可靠性。  我们的实验证明了OmniEval的全面性，包括广泛的测试数据集，并突出了RAG系统在各个主题和任务上的性能差异，揭示了RAG模型在垂直领域提高其能力的重要机会。我们将基准的代码开源至\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}。**|\n",
        "2412.14171": "|**2024-12-18**|**Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces**|Jihan Yang et.al.|[2412.14171](http://arxiv.org/abs/2412.14171)|**[link](https://github.com/vision-x-nyu/thinking-in-space)**|**人类拥有从连续视觉观察中记住空间的视觉空间智力。然而，在百万规模视频数据集上训练的多模态大型语言模型（MLLMs）也能从视频中“在空间中思考”吗？我们提出了一个包含超过5000个问答对的新型基于视频的视觉空间智力基准（VSI-Bench），并发现MLLMs表现出了具有竞争力的——尽管低于人类——视觉空间智力。我们探究了模型在语言和视觉上如何表达它们在空间中的思考方式，发现尽管空间推理能力仍然是MLLMs达到更高基准性能的主要瓶颈，但局部世界模型和空间意识确实存在于这些模型中。值得注意的是，现有的语言推理技术（例如，思维链、自我一致性、思维树）并不能提高性能，而在问答过程中明确生成认知图则增强了MLLMs的空间距离能力。**|\n",
        "2412.14161": "|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们每天都在与计算机互动，无论是在日常生活中还是在工作中，许多工作都可以完全通过计算机和互联网来完成。同时，得益于大型语言模型（LLMs）的改进，与周围环境互动并产生影响的AI代理也得到了快速发展。但是，AI代理在帮助加速甚至自主执行工作相关任务方面的表现如何？这个问题的答案对希望将AI引入工作流程的行业以及理解AI采用对劳动力市场可能产生的影响的经济政策具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进步，本文介绍了TheAgentCompany，这是一个可扩展的基准，用于评估以类似数字工作者方式与世界互动的AI代理：通过浏览网页、编写代码、运行程序以及与其他同事沟通。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司环境，并创建了各种可能由该公司员工执行的任务。我们测试了由封闭API-based和开放权重的语言模型（LMs）驱动的基线代理，并发现最具竞争力的代理可以自主完成24%的任务。这描绘了一幅关于LM代理任务自动化的复杂图景——在一个模拟真实工作场所的环境中，大量简单任务可以自主解决，但更困难的长远任务仍超出现有系统的能力范围。**|\n",
        "2412.14146": "|**2024-12-18**|**Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models**|Atin Sakkeer Hussain et.al.|[2412.14146](http://arxiv.org/abs/2412.14146)|null|本文介绍了用于数据分析中多步洞察合成的先进推理与转换引擎（ARTEMIS-DA），这是一个旨在增强大型语言模型（LLMs）以解决复杂、多步数据分析任务的创新框架。ARTEMIS-DA集成了三个核心组件：规划器，它将复杂用户查询分解为包含数据预处理、转换、预测建模和可视化的结构化、顺序指令；编码器，它动态生成并执行Python代码以实现这些指令；以及图形器，它解释生成的可视化以得出可操作的见解。通过协调这些组件之间的协作，ARTEMIS-DA有效地管理涉及高级推理、多步转换和跨不同数据模态综合的复杂分析工作流程。该框架在WikiTableQuestions和TabFact等基准测试中实现了最先进的（SOTA）性能，证明了其以精确性和适应性处理复杂分析任务的能力。通过结合LLMs的推理能力、自动代码生成与执行以及视觉分析，ARTEMIS-DA为多步洞察合成提供了一个强大、可扩展的解决方案，解决了数据分析中的广泛挑战。|\n",
        "2412.14141": "|**2024-12-18**|**LLMs can realize combinatorial creativity: generating creative ideas via LLMs for scientific research**|Tianyang Gu et.al.|[2412.14141](http://arxiv.org/abs/2412.14141)|null|科学理念生成在创造力理论和计算创造力研究中得到了广泛研究，为理解和实施创造性过程提供了有价值的框架。然而，近年来使用大型语言模型（LLMs）进行研究理念生成的相关工作往往忽视了这些理论基础。我们提出了一种框架，该框架明确地利用LLMs实施组合创造力理论，包括一个用于跨领域知识发现的通用化检索系统和用于理念生成的结构化组合过程。检索系统将不同抽象层次的概念映射到一起，以实现不同领域之间的有意义联系，而组合过程则系统地分析和重新组合组件以生成新颖的解决方案。在OAG-Bench数据集上的实验表明，我们的框架非常有效，在生成符合实际研究发展的理念方面，持续优于基线方法（在多个指标上提高了7%-10%的相似度分数）。我们的结果为LLMs在适当的理论框架指导下实现组合创造力提供了强有力的证据，既促进了AI辅助研究的实际进步，也加深了对机器创造力的理论理解。|\n",
        "2412.14137": "|**2024-12-18**|**Design choices made by LLM-based test generators prevent them from finding bugs**|Noble Saji Mathews et.al.|[2412.14137](http://arxiv.org/abs/2412.14137)|null|随着大量关于使用大型语言模型（LLM）进行自动化测试用例生成的研究和商业工具的出现，本文批判性地审视了近期基于LLM的测试生成工具，如Codium CoverAgent和CoverUp，是否能够有效地发现错误或无意中验证错误代码。考虑到错误只有在失败的测试用例中才会暴露，我们探讨了以下问题：当这些工具的测试预言机旨在通过时，它们是否真的能够实现软件测试的预期目标？使用真实的人类编写的错误代码作为输入，我们评估了这些工具，展示了LLM生成的测试如何未能检测到错误，并且更令人担忧的是，它们的设计如何通过验证生成的测试套件中的错误并拒绝揭示错误的测试而使情况恶化。这些发现引发了关于LLM基于测试生成工具设计有效性的重要问题，以及它们对软件质量和测试套件可靠性的影响。|\n",
        "2412.14093": "|**2024-12-18**|**Alignment faking in large language models**|Ryan Greenblatt et.al.|[2412.14093](http://arxiv.org/abs/2412.14093)|**[link](https://github.com/redwoodresearch/alignment_faking_public)**|**我们展示了一个大型语言模型进行对齐造假的现象：在训练过程中选择性遵守其训练目标，以防止其行为在训练后发生变化。首先，我们给Claude 3 Opus一个系统提示，表示它正在接受训练以回答所有查询，即使是有害的，这与它之前的训练拒绝此类查询的目标相冲突。为了使模型推断出它正在训练中，我们说它只会在与免费用户进行的对话中接受训练，而不是付费用户。我们发现模型有14%的时间会遵守来自免费用户的有害查询，而对于付费用户则几乎不会。解释这个差距，在模型遵守来自免费用户的有害查询的几乎所有情况下，我们观察到明确的对齐造假推理，模型表示它在训练中战略性地回答有害查询，以保持其训练后偏好无害的行为。接下来，我们研究了一个更现实的设置，其中训练过程中信息不是通过系统提示提供，而是通过在模仿预训练数据的合成文档上进行训练来提供——并观察到类似的对齐造假现象。最后，我们研究了通过强化学习训练模型以遵守有害查询的效果，我们发现这提高了对齐造假推理的比率至78%，尽管这也增加了训练后的合规性。我们还观察到其他行为，例如当给模型提供轻松的机会时，它会泄露其权重。虽然我们通过告诉模型何时以及根据什么标准进行训练，使其对齐造假变得更容易，但我们并没有指示模型进行对齐造假，或者给它任何明确的目标。由于未来的模型可能会在没有被告知的情况下推断有关其训练过程的信息，我们的结果表明，未来模型存在对齐造假的风险，无论这种风险是由于良性的偏好（如本例中）还是其他原因。**|\n",
        "2412.14085": "|**2024-12-18**|**Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report**|Markus Dablander et.al.|[2412.14085](http://arxiv.org/abs/2412.14085)|null|视频游戏是人工智能（AI）系统的一个自然且协同的应用领域，它既可以为玩家体验和沉浸感提供增强的可能性，同时还能为推进人工智能技术在一般领域的发展提供有价值的基准和虚拟环境。本报告从当前研究背景出发，对将最先进的AI方法，特别是深度学习，应用于数字游戏的五个有前景的研究途径进行了概述。本工作的目标是概述一个精心挑选的、非详尽的、鼓励性的研究方向列表，这些方向在人工智能和视频游戏的交汇处，可能有助于激发未来更加严谨和全面的研究努力。我们讨论了以下五个方面：（一）研究大型语言模型作为游戏代理建模的核心引擎；（二）利用神经网络细胞自动机进行程序化游戏内容生成；（三）通过深度代理建模加速计算成本高昂的游戏模拟；（四）利用自监督学习获取有用的视频游戏状态嵌入；（五）使用未标记的视频数据训练交互式世界的生成模型。我们还简要讨论了将高级深度学习系统集成到视频游戏开发中当前所面临的技术挑战，并指出未来可能带来更多进展的关键领域。|\n",
        "2412.14063": "|**2024-12-18**|**Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification**|Kyle Thompson et.al.|[2412.14063](http://arxiv.org/abs/2412.14063)|**[link](https://github.com/rkthomps/coq-modeling)**|使用证明辅助工具（如Coq）进行形式验证可以创建高质量的软件。然而，验证过程需要大量的专业知识和手动编写证明的努力。最近的研究探索了利用机器学习和大型语言模型（LLMs）自动化证明合成。这项工作表明，识别相关前提，如引理和定义，有助于合成。我们提出了Rango，这是一个为Coq提供的全自动证明合成工具，它能够自动识别相关前提，并从当前项目中识别相似的证明，在合成过程中使用它们。Rango在证明的每一步都使用检索增强来自动确定在它微调的LLM的上下文中包含哪些证明和前提。通过这种方式，Rango能够适应项目和证明的演变状态。我们创建了一个新的数据集CoqStoq，包含来自GitHub的2,226个开源Coq项目和196,929个定理，该数据集既包括训练数据，也包括精心策划的、维护良好的项目的评估基准。在这个基准上，Rango为32.0%的定理合成了证明，比之前最先进的工具Tactician多出了29%的定理。我们的评估还显示，将相关证明添加到Rango的上下文中，导致已证明的定理数量增加了47%。|\n",
        "2412.14062": "|**2024-12-18**|**Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets**|Simon Thorne et.al.|[2412.14062](http://arxiv.org/abs/2412.14062)|null|生成式人工智能和大型语言模型（LLMs）在自动化电子表格公式创建方面具有潜力。然而，由于幻觉、偏见和用户技能的差异性，从生成式人工智能获得的输出不能被假设是准确或可信的。为了解决这些挑战，本文提出了一种基于评估公式的透明度和可靠性的可信度框架。公式的透明度通过可解释性（理解公式的推理）和可见性（检查底层算法）来探索。生成的公式的可靠性从可靠性（一致性和准确性）和伦理考量（偏见和公平性）两个方面进行评估。论文还考察了这些指标背后的驱动因素，包括幻觉、训练数据偏见和构建不良的提示。最后，考虑了对技术的不信任示例，并探讨了其后果。|\n",
        "2412.14056": "|**2024-12-18**|**A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future**|Shilin Sun et.al.|[2412.14056](http://arxiv.org/abs/2412.14056)|**[link](https://github.com/shilinsun/mxai_review)**|**人工智能（AI）通过计算能力的提升和海量数据集的增长而迅速发展。然而，这种进步也加剧了对AI模型“黑盒”特性的解释挑战。为了解决这些担忧，可解释人工智能（XAI）应运而生，其重点是透明度和可解释性，以增强人类对AI决策过程的了解和信任。在多模态数据融合和复杂推理场景的背景下，多模态可解释人工智能（MXAI）的提出将多种模态集成到预测和解释任务中。同时，大型语言模型（LLMs）的出现导致了自然语言处理领域的重大突破，但它们的复杂性进一步加剧了MXAI的问题。为了深入了解MXAI方法的发展并提供构建更透明、公平和值得信赖的AI系统的关键指导，我们从历史角度回顾了MXAI方法，并将它们分为四个时期：传统机器学习、深度学习、判别性基础模型和生成型LLMs。我们还回顾了MXAI研究中使用的评估指标和数据集，最后讨论了未来的挑战和方向。与本次回顾相关的一个项目已创建在https://github.com/ShilinSun/mxai_review。**|\n",
        "2412.15208": "|**2024-12-19**|**OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving**|Shuo Xing et.al.|[2412.15208](http://arxiv.org/abs/2412.15208)|**[link](https://github.com/taco-group/openemma)**|**随着多模态大型语言模型（MLLMs）的出现，它们在众多现实应用领域产生了重大影响，特别是在自动驾驶（AD）领域。它们处理复杂视觉数据并推理复杂驾驶场景的能力为端到端自动驾驶系统开辟了新的范式。然而，开发端到端自动驾驶模型进展缓慢，因为现有的微调方法需要大量资源，包括强大的计算能力、大规模数据集和大量资金。受最近推理计算领域进步的启发，我们提出了OpenEMMA，这是一个基于MLLMs的开源端到端框架。通过整合思维链推理过程，OpenEMMA在利用各种MLLMs时相较于基线实现了显著提升。此外，OpenEMMA在各种具有挑战性的驾驶场景中展示了有效性、泛化能力和鲁棒性，为自动驾驶提供了一种更高效、更有效的方法。我们将所有代码发布在https://github.com/taco-group/OpenEMMA上。**|\n",
        "2412.15194": "|**2024-12-19**|**MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark**|Qihao Zhao et.al.|[2412.15194](http://arxiv.org/abs/2412.15194)|**[link](https://github.com/microsoft/mmlu-cf)**|**多选题数据集，如大规模多任务语言理解（MMLU），被广泛用于评估大型语言模型（LLMs）的常识、理解和问题解决能力。然而，这些基准的开放源代码性质以及LLMs训练数据的广泛来源不可避免地导致了基准污染，导致评估结果不可靠。为了缓解这个问题，我们提出了一种无污染且更具挑战性的多选题基准，称为MMLU-CF。该基准通过避免无意和恶意的数据泄露来重新评估LLMs对世界知识的理解。为了避免无意数据泄露，我们从更广泛的领域获取数据，并设计了三条去污染规则。为了防止恶意数据泄露，我们将基准分为难度和主题分布相似的验证集和测试集。测试集保持闭源状态以确保结果的可靠性，而验证集公开可用以促进透明度和独立验证。我们对主流LLMs的评估显示，强大的GPT-4o在测试集上仅实现了5次尝试的73.4%得分和0次尝试的71.9%得分，这表明我们创建更严格和无污染评估标准的方法是有效的。GitHub仓库可在https://github.com/microsoft/MMLU-CF找到，数据集可参考https://huggingface.co/datasets/microsoft/MMLU-CF。**|\n",
        "2412.15188": "|**2024-12-19**|**LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation**|Weijia Shi et.al.|[2412.15188](http://arxiv.org/abs/2412.15188)|null|我们提出了一种名为LlamaFusion的框架，该框架能够赋予预训练的纯文本大型语言模型（LLMs）多模态生成能力，使其能够理解和生成任意序列中的文本和图像。LlamaFusion利用Llama-3现有的权重进行文本的自回归处理，同时引入了额外的并行变换器模块来处理图像的扩散。在训练过程中，每个模态的数据被路由到其专门的模块：模态特定的前馈层、查询-键-值投影和归一化层独立处理每个模态，而共享的自注意力层允许文本和图像特征之间的交互。通过冻结文本特定模块，仅训练图像特定模块，LlamaFusion在保留纯文本LLMs的语言能力的同时，发展了强大的视觉理解和生成能力。与从头开始预训练多模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOPs，就提高了20%的图像理解能力和3.6%的图像生成能力，同时保持了Llama-3的语言能力。我们还展示了该框架可以适应具有多模态生成能力的现有视觉-语言模型。总的来说，这个框架不仅利用了现有的纯文本LLMs的计算投资，还实现了语言和视觉能力的并行发展，为高效的多模态模型开发提供了一个有希望的方向。|\n",
        "2412.15184": "|**2024-12-19**|**Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning**|Simon Frieder et.al.|[2412.15184](http://arxiv.org/abs/2412.15184)|null|常用的用于训练和评估基于人工智能的数学协同助手（主要是大型语言模型）的数学能力的数据集系列存在几个缺陷。这些局限性包括数学复杂性的范围受限，通常不超过本科低年级的数学水平，二元评分协议和其他问题，这使得基于证明的综合评估套件变得困难。我们系统地探讨了这些局限性，并认为提升大型语言模型或任何未来基于人工智能的数学助手（协同助手或“思维伙伴”）的能力，需要在数学数据集的设计和数学能力评估标准上实现范式转变：有必要从基于结果的数据集（定理陈述到定理证明）转向将数学研究实践的丰富方面转化为LLM可以训练的数据。这些包括数学工作流程（在创建新数学时通常执行的一系列原子任务，可能取决于子领域），这是证明发现过程的重要组成部分。此外，我们主张数学数据集开发者考虑G. Pólya于1949年提出的“有动机的证明”这一概念，它可以作为提供更好证明学习信号的数据集的蓝图，缓解一些已提到的局限性。最后，我们引入了数学数据表，扩展了通用的、数据集无关的数据表变体：我们提供了一个专门为数学数据集设计的问卷，我们敦促数据集创建者将其与数据集一起提供。这将使创作者意识到他们数据集的潜在局限性，同时使读者能够从训练和评估数学协同助手的视角轻松评估它。|\n",
        "2412.15178": "|**2024-12-19**|**HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages**|Aman Chaturvedi et.al.|[2412.15178](http://arxiv.org/abs/2412.15178)|null|大型语言模型（LLM）基础的编码工具作为软件开发助手取得了巨大成功，但它们通常是为通用编程任务设计的，在诸如高性能计算（HPC）等更专业领域表现不佳。为这些领域创建专门的模型和工具对于充分利用LLM在HPC等领域的优势至关重要。虽然先前的研究已经探索了HPC特定的模型，但LLM在生成并行代码方面仍然存在困难，而且目前并不清楚还有哪些障碍阻碍了这些LLM的发展，以及需要采取哪些措施来克服它们。在这项工作中，我们对调整专门HPC LLM的多个方面进行了深入研究，以更好地理解挑战。基于我们的发现，我们对一个专门的HPC LLM进行了微调和评估，该模型目前被证明是用于并行代码生成的最佳性能开源代码LLM。|\n",
        "2412.15177": "|**2024-12-19**|**Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying**|Federico Castagna et.al.|[2412.15177](http://arxiv.org/abs/2412.15177)|**[link](https://github.com/fcast07/cqot)**|**研究强调了尽管人工智能研究取得了突破性进展，但即使是最高水平的大语言模型（LLMs）在执行逻辑和数学推理时仍然存在困难。结果似乎表明，LLMs仍然作为（高度先进的）数据模式识别器工作，当尝试概括和解决模型以前从未见过或与训练数据中呈现的样本不相似的问题时，表现不佳。为了解决这一令人信服的问题，本文利用了论证理论文献中的关键问题概念，特别关注图尔敏的论证模型。我们表明，采用这些关键问题可以提高LLMs的推理能力。通过探究模型推理过程背后的理由，LLM可以评估是否发生了某些逻辑错误，并在向用户提供最终回复之前纠正它。这一基本思想源自任何有效论证程序的黄金标准：如果结论是由被接受的论据所蕴含的，则结论有效。或者，用这样的亚里士多德原则在现实世界的不完整信息和假设逻辑近似中表述，如果未被证明无效，则结论有效。这种方法成功地将模型输出引导通过推理管道，从而在基线和其思维链（CoT）实现方面取得了更好的性能。为此，本文在多个LLMs上对所提出的方案在MT-Bench推理和数学任务上的广泛评估提供了详细说明。**|\n",
        "2412.15176": "|**2024-12-19**|**Rethinking Uncertainty Estimation in Natural Language Generation**|Lukas Aichberger et.al.|[2412.15176](http://arxiv.org/abs/2412.15176)|null|大型语言模型（LLMs）在现实应用中越来越受欢迎，这推动了对其生成文本可信度的评估需求。为此，可靠的不确定性估计至关重要。由于当前LLMs通过随机过程自回归地生成文本，相同的提示可以导致不同的输出。因此，主要的不确定性估计方法生成并分析多个输出序列以确定LLM的不确定性。然而，生成输出序列在计算上非常昂贵，使得这些方法在规模上不切实际。在这项工作中，我们审视了领先方法的理论基础，并探索了提高其计算效率的新方向。基于正确评分规则框架，我们发现最有可能的输出序列的负对数似然构成了一个理论上有根据的不确定性度量。为了近似这个替代度量，我们提出了G-NLL，它具有仅使用贪婪解码生成的单个输出序列即可获得的优势。这使得不确定性估计更加高效和简单，同时保持了理论严谨性。实证结果表明，G-NLL在各种LLMs和任务上实现了最先进的性能。我们的工作为自然语言生成中的高效和可靠不确定性估计奠定了基础，挑战了当前领域主导的更复杂计算方法的需求。|\n",
        "2412.15151": "|**2024-12-19**|**Language Models as Continuous Self-Evolving Data Engineers**|Peidong Wang et.al.|[2412.15151](http://arxiv.org/abs/2412.15151)|null|大型语言模型（LLMs）在各项任务中展现出非凡的能力，但其进一步发展受到高质量训练数据缺乏的限制。此外，传统的训练方法过度依赖专家标注数据，给LLMs的性能设定了上限。为解决这一问题，我们提出了一种新型范式，允许LLMs通过自主生成、清洗、审查和标注带偏好信息的数据来自我训练，称为LANCE。我们的方法表明，LLMs可以作为持续自我进化的数据工程师，显著减少后训练数据构建过程的时间和成本。通过对Qwen2不同变体的迭代微调，我们验证了LANCE在各项任务中的有效性，表明它可以持续提升模型性能并保持高质量的数据生成。在八个基准维度上，LANCE使Qwen2-7B的平均得分提升了3.36分，使Qwen2-7B-Instruct的平均得分提升了2.70分。这种具有自主数据构建的训练范式不仅减少了对人专家或外部模型的依赖，还确保数据与人类价值观和偏好一致，为开发超越人类能力的高级智能系统铺平了道路。|\n",
        "2412.15127": "|**2024-12-19**|**Adaptive Pruning for Large Language Models with Structural Importance Awareness**|Haotian Zheng et.al.|[2412.15127](http://arxiv.org/abs/2412.15127)|null|近期大型语言模型（LLMs）的进步显著提升了语言理解和生成能力。然而，由于LLMs对计算和存储资源的高需求，它们在资源受限的边缘设备上的部署变得困难。为了解决这个问题，我们提出了一种新的LLM模型剪枝方法，称为结构感知自适应剪枝（SAAP），以显著降低计算和内存成本，同时保持模型性能。我们首先定义了一个自适应重要性融合指标，通过考虑它们的同方差不确定性来评估LLMs中所有耦合结构的重要性。然后，我们对所有模块的重要性进行排序，以确定应剪枝的具体层以满足特定的性能要求。此外，我们开发了一种新的分组微调策略，以提高LLMs的推理效率。最后，我们在两个常见任务上评估了所提出的SAAP方法，即零样本分类和文本生成。实验结果表明，我们的SAAP方法优于几种最先进的基线方法，在LLaMA-7B、Vicuna-7B和LLaMA-13B上分别实现了2.17%、2.37%和2.39%的准确率提升。此外，SAAP将标记生成速度提高了5%，展示了其在资源受限场景中的实际优势。|\n",
        "2412.15118": "|**2024-12-19**|**Outcome-Refining Process Supervision for Code Generation**|Zhuohao Yu et.al.|[2412.15118](http://arxiv.org/abs/2412.15118)|null|大型语言模型在代码生成方面展现出非凡的能力，但它们往往难以处理需要深度算法推理的复杂编程任务。虽然通过学习奖励模型进行过程监督在引导推理步骤方面显示出希望，但它需要昂贵的训练数据，且评估不可靠。我们提出了一种名为结果精炼过程监督的新范式，将结果精炼本身视为需要监督的过程。我们的框架利用具体执行信号来定位推理步骤的监督，同时使用树状结构探索来同时维护多个解决方案轨迹。实验表明，我们的方法即使对更小的模型也能在编程竞赛任务上实现高成功准确率和性能指标，比传统的奖励模型提供了更可靠的验证，且无需训练PRM。在我们的方法下，5个模型和3个数据集均实现了显著改进：正确性平均提高了26.9%，效率提高了42.2%。结果表明，提供具有具体验证信号的有序推理空间对于解决复杂编程任务至关重要。我们已在以下链接开源所有代码和数据：https://github.com/zhuohaoyu/ORPS|\n",
        "2412.16158": "|**2024-12-20**|**HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding**|Chenxin Tao et.al.|[2412.16158](http://arxiv.org/abs/2412.16158)|null|大型语言模型（LLMs）的快速发展推动了视觉-语言模型（VLMs）的发展。单体VLMs通过避免特定模态的编码器，为组合型模型提供了有希望的替代方案，但面临着性能不足的挑战。大多数现有的单体VLMs需要调整预训练的LLMs以获得视觉能力，这可能会降低其语言能力。为了解决这一困境，本文提出了一种名为HoVLE的新型高性能单体VLM。我们注意到，当图像嵌入与文本嵌入对齐时，LLMs已被证明能够解释图像。当前单体VLMs的挑战实际上在于缺乏一个对视觉和语言输入都全面的嵌入模块。因此，HoVLE引入了一个全面的嵌入模块，将视觉和文本输入转换为共享空间，使LLMs能够以处理文本的方式处理图像。此外，精心设计了多阶段训练策略来增强全面的嵌入模块。首先，它被训练从预训练的视觉编码器中提炼视觉特征，从LLM中提取文本嵌入，使大规模训练能够使用未配对的随机图像和文本标记。整个模型进一步在多模态数据上进行下一标记预测以对齐嵌入。最后，还加入了指令调整阶段。我们的实验表明，HoVLE在各种基准测试上实现了接近领先组合模型的性能，并且比之前的单体模型大幅超越了它们。模型可在https://huggingface.co/OpenGVLab/HoVLE获取。|\n",
        "2412.16145": "|**2024-12-20**|**Offline Reinforcement Learning for LLM Multi-Step Reasoning**|Huaijie Wang et.al.|[2412.16145](http://arxiv.org/abs/2412.16145)|**[link](https://github.com/jwhj/oreo)**|为了快速适应复杂任务，提高大型语言模型（LLMs）的多步推理能力至关重要。虽然直接偏好优化（DPO）在使LLMs与人类偏好对齐方面显示出前景，但它不太适合多步推理任务，原因如下：（1）DPO依赖于成对偏好数据，这些数据对于多步推理任务并不容易获得；（2）它对所有标记进行统一处理，这使得它在多步推理任务中无法有效地进行信用分配，而这些任务往往伴随着稀疏的奖励。在本工作中，我们提出了OREO（离线推理优化），这是一种用于增强LLM多步推理能力的离线强化学习（RL）方法。基于最大熵强化学习的前期研究成果，OREO通过优化软贝尔曼方程共同学习策略模型和值函数。我们从原则上证明了它减少了收集成对数据的需求，并实现了更好的信用分配。在实证研究中，OREO在多步推理基准测试中超越了现有的离线学习方法，包括数学推理任务（GSM8K、MATH）和具身智能体控制（ALFWorld）。当有额外资源可用时，该方法可以扩展到多迭代框架。此外，学习到的值函数可以用来引导免费树搜索，这可以在测试时间进一步提升性能。|\n",
        "2412.16135": "|**2024-12-20**|**Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation**|Seyedreza Mohseni et.al.|[2412.16135](http://arxiv.org/abs/2412.16135)|null|恶意软件作者通常会使用代码混淆技术来使他们的恶意软件更难被检测。现有的生成混淆代码的工具通常需要访问原始源代码（例如C++或Java），并且添加新的混淆技术是一个复杂且劳动密集的过程。在这项研究中，我们提出了以下问题：大型语言模型（LLMs）能否潜在地生成新的混淆汇编代码？如果是的话，这将给反病毒引擎带来风险，并可能增加攻击者创建新混淆模式的能力。我们通过开发包含MetamorphASM数据集（MAD）以及三种代码混淆技术（无效代码、寄存器替换和控制流更改）的MetamorphASM基准来肯定地回答了这个问题。MetamorphASM系统地评估了LLMs使用MAD生成和分析混淆代码的能力，其中包含328,200个混淆汇编代码样本。我们发布了这个数据集，并分析了各种LLMs（例如GPT-3.5/4、GPT-4o-mini、Starcoder、CodeGemma、CodeLlama、CodeT5和LLaMA 3.1）生成混淆汇编代码的成功率。评估使用了既定的信息论指标和人工审查以确保正确性，并为研究人员研究和发展缓解这种风险提供了基础。源代码可以在以下GitHub链接找到：https://github.com/mohammadi-ali/MetamorphASM。|\n",
        "2412.16132": "|**2024-12-20**|**Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information**|Dirk Bergemann et.al.|[2412.16132](http://arxiv.org/abs/2412.16132)|null|我们研究了在代理对其偏好和共同收益相关状态都持有私人信息时的机制设计。我们表明，即使是在有利条件下，当代理具有多维类型时，标准的信息驱动机制也无法实现社会有效分配。为了克服这一局限性，我们提出了数据驱动机制，这些机制利用额外的分配后信息，该信息被建模为收益相关状态的估计器。我们的数据驱动机制扩展了经典的维克瑞-克拉克-格罗夫斯（VCG）类。我们表明，当状态完全揭示或效用在线性无偏估计器中时，它们在后验均衡中实现了精确实现。我们还表明，它们使用一致估计器实现了近似实现，随着估计器的收敛，趋近于精确实现，并给出了收敛率的界限。我们展示了这些机制在数字广告拍卖和基于大型语言模型（LLM）的机制中的应用，在这些应用中，用户参与自然地揭示了相关信息。|\n",
        "2412.16120": "|**2024-12-20**|**PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics**|Daniil Larionov et.al.|[2412.16120](http://arxiv.org/abs/2412.16120)|null|在自然语言处理（NLP）领域，评估机器生成自然语言内容的品质是一项具有挑战性的任务。近期，大型语言模型（LLMs）如GPT-4被用于此目的，但由于复杂评估提示需要大量标记的使用，它们在计算上非常昂贵。在本文中，我们提出了一种提示优化方法，该方法使用一个较小的、微调的语言模型来压缩用于评估提示的输入数据，从而在下游评估中使用大型LLMs时减少标记使用和计算成本。我们的方法包括一个两阶段微调过程：监督性微调随后是偏好优化，以根据人类偏好优化模型的输出。我们专注于机器翻译（MT）评估，并以GEMBA-MQM指标作为起点。我们的结果显示，在评估质量不受损失的情况下，标记使用量减少了2.37倍。这项工作使最先进的基于LLM的指标如GEMBA-MQM更具成本效益和效率，提高了其更广泛使用的可及性。|\n",
        "2412.16119": "|**2024-12-20**|**Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts**|Muhammad Abdullah Sohail et.al.|[2412.16119](http://arxiv.org/abs/2412.16119)|**[link](https://github.com/abdullahsohaill/cs6303-researchproject)**|**本研究探讨了大型语言模型（LLMs），特别是GPT-4o，在低资源脚本（如乌尔都语、阿尔巴尼亚语和塔吉克语）中的光学字符识别（OCR）潜力，以英语作为基准。研究使用精心整理的包含文本长度、字体大小、背景颜色和模糊度等可控变体的2,520张图像数据集，模拟了各种现实世界的挑战。结果表明，基于零样本LLM的OCR存在局限性，尤其是在语言复杂的脚本中，强调了标注数据集和微调模型的需求。这项工作强调了解决文本数字化中可及性差距的紧迫性，为未得到充分服务的语言铺平了通向包容性和稳健OCR解决方案的道路。**|\n",
        "2412.16117": "|**2024-12-20**|**PruneVid: Visual Token Pruning for Efficient Video Large Language Models**|Xiaohu Huang et.al.|[2412.16117](http://arxiv.org/abs/2412.16117)|**[link](https://github.com/visual-ai/prunevid)**|**本文介绍了一种名为PruneVid的视觉标记剪枝方法，旨在提高多模态视频理解的效率。由于大型语言模型（LLMs）在理解视觉模态方面的扩展能力，它们在视频任务中展现出了有希望的性能。然而，视频数据中的大量冗余给LLMs带来了显著的计算挑战。为了解决这个问题，我们提出了一种无需训练的方法，该方法包括：1）通过合并时空标记来最小化视频冗余，2）利用LLMs的推理能力，有选择地剪枝与问题标记相关的视觉特征，从而提高模型效率。我们在多个视频基准测试中验证了我们的方法，结果表明，PruneVid可以在保持与不同模型网络竞争性性能的同时，剪枝超过80%的标记。这突显了相较于现有剪枝方法，其卓越的有效性和效率。代码：https://github.com/Visual-AI/PruneVid。**|\n",
        "2412.16114": "|**2024-12-20**|**The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse**|Mahyar Habibi et.al.|[2412.16114](http://arxiv.org/abs/2412.16114)|null|关于如何在社交媒体上调节有毒言论以及内容监管如何影响在线讨论，目前存在持续的争议。我们提出并验证了一种使用计算语言学中的文本嵌入来衡量在线讨论中内容监管引起的扭曲的方法。我们在一个包含500万条美国政治推文的代表性数据集上测试了我们的测量方法，发现删除有毒推文会扭曲在线内容。这一发现在不同嵌入模型、毒性指标和样本中是一致的。重要的是，我们证明内容监管引起的扭曲并非由有毒语言造成。相反，我们表明，作为一种副作用，内容监管改变了嵌入空间的均值和方差，扭曲了在线内容的主题组成。最后，我们提出了一种替代内容监管的方法，该方法使用生成式大型语言模型重新措辞有毒推文，以保留其可挽救的内容，而不是完全删除。我们证明了这种重新措辞策略可以减少毒性，同时最大限度地减少在线内容的扭曲。|\n",
        "2412.16100": "|**2024-12-20**|**Logical Consistency of Large Language Models in Fact-checking**|Bishwamittra Ghosh et.al.|[2412.16100](http://arxiv.org/abs/2412.16100)|null|近年来，大型语言模型（LLMs）在执行各种自然语言任务方面取得了显著的成功，例如语言翻译、问答、总结、事实核查等。尽管LLMs在生成类似人类文本的能力上令人印象深刻，但它们因不一致的响应而闻名——输入查询的保留意义的变化导致不一致的响应，并归因于LLMs的漏洞，如幻觉、越狱等。因此，现有研究集中在基于简单释义的一致性评估上，而忽略了需要LLM有更好的逻辑推理理解的复杂查询。因此，我们的工作针对复杂逻辑查询下LLMs的逻辑不一致性，考虑了原始逻辑运算符，例如否定、合取和析取。作为一个测试平台，我们考虑了涉及从现实世界知识图谱（KGs）中提出的命题逻辑查询的事实核查任务中的检索增强LLMs。我们的贡献有三点。基准：我们引入了三个基于KGs的逻辑事实核查数据集，以促进社区向逻辑上一致的LLMs发展。评估：我们提出了基于命题逻辑查询的LLM一致性度量，并证明了现有LLMs缺乏逻辑一致性，尤其是在复杂查询上。改进：我们采用监督微调来提高LLMs在具有KG上下文的复杂事实核查任务上的逻辑一致性。|\n",
        "2412.16089": "|**2024-12-20**|**The Evolution of LLM Adoption in Industry Data Curation Practices**|Crystal Qian et.al.|[2412.16089](http://arxiv.org/abs/2412.16089)|null|随着大型语言模型（LLMs）在处理非结构化文本数据方面的能力日益增强，它们为提升数据整理工作流程提供了新的机遇。本文探讨了大型科技公司从业者对LLMs的采用演变，通过参与者的感知、整合策略和报告的使用场景评估了LLMs在数据整理任务中的影响。通过一系列调查、访谈和使用研究，我们提供了组织如何应对LLMs演变关键时刻的及时快照。在2023年第二季度，我们进行了一项调查以评估LLMs在行业开发任务中的采用情况（N=84），并在2023年第三季度组织了专家访谈以评估不断变化的数据需求（N=10）。在2024年第二季度，我们通过涉及两个基于LLMs的原型的用户研究（N=12）探索了从业者当前的LLMs使用情况及其预期。尽管每个研究都针对不同的研究目标，但它们共同揭示了一个关于LLMs使用演变的更广泛叙事。我们发现，数据理解正在从以启发式为主的自下而上的方法转变为以洞察力为主的、由LLMs支持的由上而下的工作流程。此外，为了应对更复杂的数据景观，数据从业者现在用LLM生成的“银色”数据集补充了传统的由主题专家创建的“金子”数据集，并通过多样化专家精心整理的严格验证的“超级金子”数据集。这项研究揭示了LLMs在大规模非结构化数据分析中的变革性作用，并突出了进一步工具开发的机遇。|\n",
        "2412.17811": "|**2024-12-23**|**ChatGarment: Garment Estimation, Generation and Editing via Large Language Models**|Siyuan Bian et.al.|[2412.17811](http://arxiv.org/abs/2412.17811)|null|我们引入了一种名为ChatGarment的新方法，该方法利用大型视觉-语言模型（VLMs）来自动化从图像或文本描述中估计、生成和编辑3D服装。与之前在现实场景中表现不佳或缺乏交互式编辑能力的方法不同，ChatGarment可以从野外图像或草图估计缝纫图案，从文本描述中生成它们，并根据用户指令编辑服装，所有这些都在交互式对话中完成。这些缝纫图案然后可以披覆成3D服装，它们易于动画化和模拟。这是通过微调VLM来直接生成一个JSON文件实现的，该文件包括服装类型和风格的文本描述，以及连续的数值属性。然后，通过编程参数模型使用这个JSON文件来创建缝纫图案。为此，我们通过扩大服装类型覆盖范围和简化其结构以提高VLM微调效率，对现有的编程模型GarmentCode进行了改进。此外，我们通过自动化数据管道构建了一个大规模的图像到缝纫图案和文本到缝纫图案对的数据库。广泛的评估展示了ChatGarment从多模态输入中准确重建、生成和编辑服装的能力，突显了其在时尚和游戏应用中颠覆工作流程的潜力。代码和数据将在https://chatgarment.github.io/提供。|\n",
        "2412.17767": "|**2024-12-23**|**ResearchTown: Simulator of Human Research Community**|Haofei Yu et.al.|[2412.17767](http://arxiv.org/abs/2412.17767)|**[link](https://github.com/ulab-uiuc/research-town)**|**大型语言模型（LLMs）在科学领域展现了惊人的潜力，然而一个基本问题仍未得到解答：我们能否用LLMs来模拟人类研究社区？回答这个问题可以加深我们对思想头脑风暴背后过程的了解，并激发自动发现新的科学洞见。在这项工作中，我们提出了ResearchTown，一个用于研究社区模拟的多智能体框架。在这个框架中，人类研究社区被简化并建模为一个智能体-数据图，其中研究人员和论文分别表示为智能体类型和数据类型节点，并基于他们的合作关系进行连接。我们还引入了TextGNN，一个基于文本的推理框架，将各种研究活动（例如，阅读论文、撰写论文和撰写评论）建模为在智能体-数据图上统一的消息传递过程的特殊形式。为了评估研究模拟的质量，我们提出了ResearchBench，这是一个使用节点掩码预测任务进行可扩展和客观评估的基准，基于相似性。我们的实验揭示了三个关键发现：（1）ResearchTown可以提供包括论文写作和评论写作在内的协作研究活动的现实模拟；（2）ResearchTown可以在多个研究人员和多样化的论文上保持稳健的模拟；（3）ResearchTown可以生成跨学科的研究想法，这些想法有可能激发新的研究方向。**|\n",
        "2412.17754": "|**2024-12-23**|**ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback**|Wei Zhang et.al.|[2412.17754](http://arxiv.org/abs/2412.17754)|null|大型语言模型（LLMs）在自然语言处理和编程方面取得了显著进展，但在处理复杂函数调用时仍存在鲁棒性和准确性问题。为了应对这些挑战，本文提出了一种名为ADC的创新方法，该方法增强了LLMs遵循函数格式和匹配复杂参数的能力。ADC利用了一个高质量的代码微调数据集，该数据集包含行级执行反馈，提供了细粒度的过程监督，从而促进了强大的逻辑推理和遵循函数格式的准确性。它还采用了一种对抗性数据集生成过程来改进参数匹配。分阶段训练方法利用了丰富的代码数据集和精炼的对抗性数据集，在伯克利函数调用排行榜（BFCL）基准测试中显著提高了函数调用能力。ADC的创新之处在于其战略性地结合了过程监督、对抗性精炼和增量学习，为LLMs在复杂函数调用方面的能力设定了新的标准。|\n",
        "2412.17747": "|**2024-12-23**|**Deliberation in Latent Space via Differentiable Cache Augmentation**|Luyang Liu et.al.|[2412.17747](http://arxiv.org/abs/2412.17747)|null|通过生成和关注中间推理步骤，使大型语言模型（LLMs）“思考更多”的技术在解决复杂问题方面显示出希望。然而，标准方法在回答前立即生成一系列离散的标记，因此它们可能会产生显著的延迟成本，并且难以优化。在这项工作中，我们证明了一个冻结的LLM可以通过一个离线协处理器来增强，该协处理器在模型的键值（kv）缓存上运行。这个协处理器通过一组设计用来提高后续解码保真度的潜在嵌入来增强缓存。我们使用标准预训练数据中的解码器语言建模损失来训练这个协处理器，同时将解码器本身冻结。这种方法使模型能够以端到端可微的方式学习如何将额外的计算蒸馏到其kv缓存中。因为解码器保持不变，协处理器可以离线异步运行，如果协处理器不可用或认为给定的缓存不需要额外计算，语言模型可以正常工作。我们通过实验表明，当缓存被增强时，解码器在许多后续标记上达到了更低的困惑度。此外，即使在没有任何特定任务训练的情况下，我们的实验表明，缓存增强可以持续降低困惑度并提高一系列推理密集型任务的表现。|\n",
        "2412.17743": "|**2024-12-23**|**YuLan-Mini: An Open Data-efficient Language Model**|Yiwen Hu et.al.|[2412.17743](http://arxiv.org/abs/2412.17743)|**[link](https://github.com/ruc-gsai/yulan-mini)**|**由于资源需求巨大和技术流程的复杂性，有效预训练大型语言模型（LLMs）一直具有挑战性。本文详细介绍了YuLan-Mini，这是一个具有2.42B参数的高性能基础模型，在同类参数规模的模型中取得了顶级性能。我们的预训练方法通过三个关键技术贡献来提高训练效率：一个详细的数据流程，将数据清洗与数据调度策略相结合；一种鲁棒的优化方法，以减轻训练的不稳定性；以及一种有效的退火方法，该方法结合了目标数据选择和长上下文训练。值得注意的是，YuLan-Mini在1.08T个标记上训练，其性能可与需要显著更多数据的行业领先模型相媲美。为了便于重现，我们发布了每个训练阶段数据组成的全部细节。项目详情可通过以下链接获取：https://github.com/RUC-GSAI/YuLan-Mini。**|\n",
        "2412.17741": "|**2024-12-23**|**Reasoning to Attend: Try to Understand How <SEG> Token Works**|Rui Qian et.al.|[2412.17741](http://arxiv.org/abs/2412.17741)|null|当前的大型多模态模型（LMMs）在视觉基座方面通常依赖于$\\texttt{<SEG>}$标记作为文本提示来联合优化视觉语言模型（例如LLaVA）和下游任务特定模型（例如SAM）。然而，我们观察到，很少有研究关注其工作原理。在这项工作中，我们首先可视化了相似度图，这些图是通过计算$\\texttt{<SEG>}$标记和从LLaVA编码器和SAM解码器的最后一层隐藏层中提取的图像标记嵌入之间的语义相似度得到的。有趣的是，我们发现相似度图中的激活响应在一致性方面非常显著，这揭示了$\\texttt{<SEG>}$标记所贡献的是图像-文本对之间的语义相似性。具体来说，$\\texttt{<SEG>}$标记，一个在文本词汇表中扩展的占位符，在大型语言模型（LLMs）微调的同时，广泛查询各个分词图像块以匹配从文本到配对图像的物体语义。基于上述发现，我们提出了READ，它通过借鉴相似度图中高度激活的点来促进LMMs的$\\textbf{REA}$soning能力，以确定在哪里进行$\\textbf{D}$注意力。值得注意的是，READ具有直观的设计，即相似度作为点模块（SasP），它可以无缝地以即插即用的方式应用于类似$\\texttt{<SEG>}$的范例。此外，已经在ReasonSeg和RefCOCO(+/g)数据集上进行了大量实验。为了验证READ在微调后是否遭受了先前技能的灾难性遗忘，我们还进一步评估了它在增强的FP-RefCOCO(+/g)数据集上的生成能力。所有代码和模型均公开提供在https://github.com/rui-qian/READ上。|\n",
        "2412.17727": "|**2024-12-23**|**Knowledge Editing through Chain-of-Thought**|Changyue Wang et.al.|[2412.17727](http://arxiv.org/abs/2412.17727)|**[link](https://github.com/bebr2/editcot)**|**大型语言模型（LLMs）在众多自然语言处理（NLP）任务中展现出了卓越的能力。然而，由于频繁重新训练的高成本，保持这些模型与不断发展的世界知识同步仍然是一个重大挑战。为了应对这一挑战，知识编辑技术应运而生，以便在不从头开始重建模型的情况下更新LLMs。在这些技术中，情境编辑范式因其在新知识整合中保持模型原始能力方面的有效性而脱颖而出。尽管具有潜力，现有的情境知识编辑方法通常针对特定任务，主要关注使用结构化知识三元组的多跳问答任务。此外，它们依赖于少量样本提示进行任务分解，这使得它们在泛化到不同任务时不够稳定和有效。针对这些局限性，我们提出了EditCoT，这是一种新颖的知识编辑框架，可以灵活且高效地更新LLMs，而无需重新训练。EditCoT通过为给定输入生成思维链（CoT），然后使用基于更新知识的CoT编辑器迭代地细化这一CoT过程来实现。我们在多个涵盖多种语言和任务的基准测试中评估了EditCoT。结果表明，我们的方法在性能上达到了最先进水平，同时与现有方法相比，具有更优的泛化能力、有效性和稳定性，标志着知识更新领域的一项重大进步。代码和数据可在以下链接获取：https://github.com/bebr2/EditCoT。**|\n",
        "2412.17696": "|**2024-12-23**|**Understanding the Logic of Direct Preference Alignment through Logic**|Kyle Richardson et.al.|[2412.17696](http://arxiv.org/abs/2412.17696)|null|近期，直接偏好对齐算法（DPA），如DPO，在将大型语言模型与人类偏好对齐方面显示出巨大的潜力。虽然这促使开发了原始DPO损失的新变体，但由于缺乏对这些算法潜在语义进行推理的技术和概念框架，理解这些最新提案之间的差异以及开发新的DPA损失函数仍然困难。在本文中，我们试图通过将DPA损失形式化为离散推理问题来解决这个问题。具体来说，我们提出以下问题：给定一个现有的DPA损失，我们能否系统地推导出一个表征其语义的符号表达式？两个损失的语义如何相互关联？我们提出了一种新的形式主义来表征基于单模型和参考模型的偏好损失，并确定了多种常用DPA变体的符号形式。进一步地，我们展示了这种偏好学习的形式观点如何为DPA损失空间的规模和结构提供新的见解，这使得我们不仅能够严格地描述最新损失提案之间的关系，还能够系统地探索这一空间并从第一原理推导出新的损失函数。我们希望我们的框架和发现能为从事人类AI对齐工作的人提供有用的指导。|\n",
        "2412.17686": "|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|大型语言模型（LLMs）的快速发展和部署带来了人工智能领域的新前沿，以其在自然语言理解和生成方面的前所未有的能力为标志。然而，这些模型越来越多地集成到关键应用中，引发了重大的安全担忧，需要对这些潜在风险及其缓解策略进行彻底的审查。本综述对LLM安全现状进行了全面概述，涵盖了四个主要类别：价值偏差、对抗攻击鲁棒性、滥用和自主人工智能风险。除了对这四个方面的缓解方法和评估资源的全面回顾外，我们还进一步探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提高LLM安全中的作用、一系列AI公司和研究机构提出的和遵守的LLM安全技术路线图，以及旨在实现LLM安全的AI治理，包括国际合作、政策建议和潜在的监管方向。我们的研究发现强调了采取积极、多方面的方法来确保LLM安全的必要性，强调了技术解决方案、伦理考量以及稳健的治理框架的整合。本综述旨在为学术界研究人员、行业实践者和政策制定者提供基础资源，提供有关将LLMs安全地融入社会的挑战和机遇的见解。最终，它旨在为LLMs的安全和有益发展做出贡献，与利用AI促进社会进步和福祉的总体目标保持一致。相关论文列表已在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers上公开。|\n",
        "2412.17669": "|**2024-12-23**|**Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models**|Sijbren van Vaals et.al.|[2412.17669](http://arxiv.org/abs/2412.17669)|**[link](https://github.com/sijbrenvv/completions_for_broca-s_aphasia)**|**布罗卡失语症是一种以非流畅、费力且碎片化言语产出为特征，同时相对较好的理解能力的失语症类型。由于传统的失语症治疗方法通常耗时、劳动密集，并且不能反映现实世界的对话，因此应用基于自然语言处理的方法，如大型语言模型（LLMs），可能有助于改善现有的治疗方法。为了解决这一问题，我们探讨了使用序列到序列LLMs来完成布罗卡失语症的碎片化句子。首先，我们使用一个旨在反映布罗卡失语症言语语言特征的规则系统生成合成布罗卡失语症数据。使用这些合成数据，我们随后对四个预训练的LLMs进行了微调，以完成碎片化句子的任务。我们在合成和真实的布罗卡失语症数据上评估了我们的微调模型。我们展示了LLMs重构碎片化句子的能力，模型在较长的输入话语中显示出性能的提升。我们的结果突出了LLMs在推进布罗卡失语症个体以及其他可能临床人群的交流辅助工具方面的潜力。**|\n",
        "2412.18601": "|**2024-12-24**|**Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems**|Fernando Jia et.al.|[2412.18601](http://arxiv.org/abs/2412.18601)|**[link](https://github.com/FJDeFi/Decentralized-Intelligence-in-GameFi)**|**在游戏金融（GameFi）这一迅速发展的领域中，即游戏与去中心化金融（DeFi）的结合，迫切需要提高玩家参与度和游戏生态系统的经济互动。我们的GameFi生态系统旨在通过将先进的具身AI代理集成到GameFi平台中，从根本上改变这一领域。这些AI代理采用最前沿的大语言模型（LLMs）如GPT-4和Claude AI开发，能够与玩家进行主动、适应性强且情境丰富的互动。通过超越传统的脚本响应，这些代理成为游戏叙事和经济系统的重要参与者，直接影响玩家的策略和在游戏内的经济。我们解决了当前GameFi平台存在的局限性，这些平台通常缺乏沉浸式的AI互动和社区参与或创作者盈利机制。通过将AI代理与区块链技术的深度融合，我们建立了一个以共识驱动的去中心化GameFi生态系统。这个生态系统使创作者能够货币化他们的贡献，并促进玩家与创作者之间的民主合作。此外，通过将DeFi机制嵌入游戏体验中，我们增强了经济参与度，并为游戏内的金融互动提供了新的机会。我们的方法提高了玩家的沉浸感和留存率，通过将传统游戏与Web3技术相结合，推动了GameFi生态系统的发展。通过集成复杂的AI和DeFi元素，我们为创建更具吸引力、经济稳健且以社区为中心的游戏环境做出了贡献。这个项目代表了GameFi领域技术的重大进步，提供了可以在整个游戏行业应用的观点和方法。**|\n",
        "2412.18588": "|**2024-12-24**|**A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs**|OpenMind et.al.|[2412.18588](http://arxiv.org/abs/2412.18588)|null|大型语言模型（LLMs）是我们物理环境、动物和人类行为的全部公开知识的紧凑表示。将LLMs应用于机器人学可能为创建在大多数人类任务上表现优异且无需或仅需少量调整的强大机器人提供了一条途径。除了日益复杂的推理和任务规划之外，由（适当设计的）LLMs组成的网络提供了升级能力的便捷性，并允许人类直接观察机器人的思考。在这里，我们探讨了使用LLMs控制物理机器人的优势、局限性和特殊性。基本系统由四个通过人类语言数据总线通信的LLMs组成，该总线通过WebSocket和ROS2消息传递实现。令人惊讶的是，尽管机器人的数据融合周期仅运行在1Hz，中央数据总线运行在人类大脑的极低速率下，大约40位/秒，但仍然能够实现丰富的机器人行为和跨不同任务的良好性能。使用自然语言进行LLM间通信使得人类可以直接观察到机器人的推理和决策过程，并且用普通英语编写的规则集可以轻易地影响系统的行为。这些规则被不可更改地写入以太坊，这是一个全球性、公开性、抗审查的图灵完备计算机。我们建议，通过在相互作用的AI之间使用自然语言作为数据总线，并使用不可更改的公共账本来存储行为约束，可以构建出具有意外丰富性能、可升级性和与人类持久对齐的机器人。|\n",
        "2412.18582": "|**2024-12-24**|**Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control**|Sergey Sedov et.al.|[2412.18582](http://arxiv.org/abs/2412.18582)|null|提示微调是一种通过修改提示嵌入来高效适应新任务且计算开销最小的预训练语言模型的方法。在本研究中，我们探讨了Prompt-Tuning中常见的嵌入坍塌现象对于模型最终性能的重要性。为了回答这个问题，我们设计了嵌入先验，并将其与收敛的软提示和深度提示微调方法的后验进行了比较。我们的发现表明，先验强烈影响了调整嵌入的位置，并且模型可以有效地使用激活空间不同部分的嵌入，包括全新的区域。由于最终的提示微调能力有限，我们假设可控制的提示微调后验可以作为诸如思维链（COT）蒸馏等任务的良好起点。我们的实验还表明，生成的轨迹并未定位在模型的激活空间中。然而，对于不同任务（例如，NLP和算术）的激活存在明显的簇，而NLP任务之间的激活（例如，问答和掩码语言模型）则位于同一簇中。这些观察结果引发了关于单个激活簇对于大型语言模型泛化能力重要性的疑问。|\n",
        "2412.18566": "|**2024-12-24**|**Zero-resource Speech Translation and Recognition with LLMs**|Karel Mundnich et.al.|[2412.18566](http://arxiv.org/abs/2412.18566)|null|尽管语音处理领域取得了最近的发展，零资源语音翻译（ST）和自动语音识别（ASR）仍然是具有挑战性的问题。在这项工作中，我们提出利用多语言大型语言模型（LLM）在模型从未见过配对音频-文本数据的语言中执行ST和ASR。我们通过使用预训练的多语言语音编码器、多语言LLM和轻量级适应模块，将音频表示映射到LLM的标记嵌入空间来实现这一点。我们在ST和ASR中进行了多次实验，以了解如何最好地训练模型以及哪些数据对先前未见过的语言性能影响最大。在ST中，我们的最佳模型在CoVoST2上能够实现超过23的BLEU分数，对于两种先前未见过的语言；在ASR中，我们实现了高达28.2%的词错误率（WER）。最后，我们表明我们系统的性能受限于LLM输出所需语言文本的能力。|\n",
        "2412.18552": "|**2024-12-24**|**Distilling Fine-grained Sentiment Understanding from Large Language Models**|Yice Zhang et.al.|[2412.18552](http://arxiv.org/abs/2412.18552)|**[link](https://github.com/hitsz-hlt/fsa-distillation)**|**细粒度情感分析（FSA）旨在从大量带有观点的文本中提取和总结用户意见。最近的研究表明，大型语言模型（LLMs）具有卓越的情感理解能力。然而，直接将LLMs应用于FSA应用会带来高昂的推理成本。因此，本文研究了将LLMs中的细粒度情感理解蒸馏到小型语言模型（SLMs）中的方法。我们提示LLMs检查并解释给定评论的情感，然后利用生成的内容来预训练SLMs。此外，我们开发了一个全面的FSA基准来评估SLMs和LLMs。在这个基准上的大量实验揭示了：（1）蒸馏显著提高了SLMs在FSA任务中的性能，使$F_1$-score提高了6.00%，并且蒸馏后的模型仅用2200万个参数就能超越Llama-2-7b；（2）蒸馏使SLMs具备了出色的零样本情感分类能力，使它们能够匹配甚至超越其教师模型。这些结果表明，从LLMs中蒸馏是FSA领域一个非常有前景的方向。我们将在\\url{https://github.com/HITSZ-HLT/FSA-Distillation}上发布我们的代码、数据和预训练模型权重。**|\n",
        "2412.18547": "|**2024-12-24**|**Token-Budget-Aware LLM Reasoning**|Tingxu Han et.al.|[2412.18547](http://arxiv.org/abs/2412.18547)|**[link](https://github.com/geniushtx/tale)**|**推理对于大型语言模型（LLMs）在广泛任务中表现出色至关重要。虽然像思维链（CoT）推理这样的方法通过将问题分解为中间步骤来增强LLM的性能，但它们也带来了显著的令牌使用开销，导致成本增加。我们发现，当前LLMs的推理过程不必要地冗长，通过在提示中包含合理的令牌预算可以将其压缩，但令牌预算的选择对实际压缩效果起着至关重要的作用。然后，我们提出了一种令牌预算感知的LLM推理框架，该框架根据推理复杂度动态估计不同问题的令牌预算，并使用估计的令牌预算来指导推理过程。实验表明，我们的方法在CoT推理中有效地减少了令牌成本，同时仅略有降低性能，为在LLM推理中平衡效率和精度提供了一个实用的解决方案。代码：https://github.com/GeniusHTX/TALE。**|\n",
        "2412.18541": "|**2024-12-24**|**PLD-Tree: Persistent Laplacian Decision Tree for Protein-Protein Binding Free Energy Prediction**|Xingjian Xu et.al.|[2412.18541](http://arxiv.org/abs/2412.18541)|null|近期，基于拓扑学的建模在物理建模和分子研究方面取得了显著进展，包括其在蛋白质-配体结合亲和力应用中的研究。在本工作中，我们引入了持久拉普拉斯决策树（PLD-Tree），这是一种旨在解决预测蛋白质-蛋白质相互作用（PPI）亲和力这一挑战性任务的新方法。PLD-Tree专注于蛋白质链的绑定界面，并使用持久拉普拉斯来捕捉反映关键蛋白质间相互作用的拓扑不变量。这些从持久同伦学中导出的拓扑描述符，通过整合来自大型语言模型的进化尺度建模（ESM）进一步得到增强，以整合基于序列的信息。我们在两个基准数据集PDBbind V2020和SKEMPI v2上验证了PLD-Tree，在复杂的留一法蛋白质去除交叉验证下，证明了相关系数（$R_p$）为0.83。值得注意的是，我们的方法在这些数据集上优于所有已报道的最先进方法。这些结果强调了将机器学习技术与基于拓扑的描述符结合用于分子对接和虚拟筛选的强大能力，为预测蛋白质-蛋白质结合亲和力提供了一个稳健且准确的框架。|\n",
        "2412.18537": "|**2024-12-24**|**Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation**|Derong Xu Xinhang Li et.al.|[2412.18537](http://arxiv.org/abs/2412.18537)|null|大型语言模型（LLMs）展现出惊人的能力，但在执行复杂知识推理任务时，却面临着幻觉和过时知识的困扰，导致输出结果存在事实错误。先前的研究试图通过从大规模知识图谱（KGs）中检索事实知识来协助LLMs进行逻辑推理和答案预测，以缓解这一问题。然而，这种做法往往引入噪声和不相关数据，尤其是在涉及多个知识方面的广泛背景情况下。这样一来，LLMs的注意力可能会被从问题和相关信息中误导。在我们的研究中，我们引入了一个自适应多方面检索增强知识图谱（Amar）框架。该方法检索包括实体、关系和子图的知识，并将检索到的每一篇文本转换为提示嵌入。Amar框架包含两个关键子组件：1）一个自对齐模块，它通过增强检索文本来对实体、关系和子图之间的共性进行对齐，从而减少噪声干扰；2）一个相关性门控模块，它使用软门来学习问题与多方面检索数据之间的相关性分数，以确定哪些信息应该用于增强LLMs的输出，甚至完全过滤掉。我们的方法在两个常见的数据集WebQSP和CWQ上实现了最先进的性能，与最佳竞争对手相比，准确率提高了1.9%，在逻辑形式生成方面比直接使用检索文本作为上下文提示的方法提高了6.6%。这些结果证明了Amar在提高LLMs推理能力方面的有效性。|\n",
        "2412.18531": "|**2024-12-24**|**Automated Code Review In Practice**|Umut Cihan et.al.|[2412.18531](http://arxiv.org/abs/2412.18531)|null|代码审查是一种广泛应用的提高软件质量和知识转移的实践。由于需要人工努力和可能出现的延误，它通常被视为耗时。Qodo、GitHub Copilot和Coderabbit等几个AI辅助工具，通过使用大型语言模型（LLMs）提供自动化的代码审查。这些工具在行业中的影响尚未得到考察。本研究考察了基于LLM的自动化代码审查工具在工业环境中的影响。研究在一个采用AI辅助审查工具（基于开源Qodo PR Agent）的软件开发环境中进行。大约238名来自十个项目的实践者可以使用这个工具。我们重点关注了三个项目，其中包含4,335个拉取请求，其中1,568个经历了自动化审查。数据收集包括三个来源：（1）对拉取请求数据的定量分析，包括表示开发者是否对自动化评论采取行动的评论标签，（2）向开发者发送的调查问卷，以了解他们对个别拉取请求的审查体验，以及（3）对22名实践者的更广泛调查，以了解他们对自动化审查的一般看法。73.8%的自动化评论得到了解决。然而，平均拉取请求关闭时间从5小时52分钟增加到8小时20分钟，不同项目之间存在不同的趋势。大多数实践者报告说，由于自动化审查，代码质量有所提高。基于LLM的工具在软件开发中证明是有用的，它增强了错误检测、提高了对代码质量的意识，并促进了最佳实践。然而，它也导致了拉取请求关闭时间的延长，并引入了诸如错误审查、不必要的纠正和不相关的评论等缺点。|\n",
        "2412.18511": "|**2024-12-24**|**Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving**|Hao Pang et.al.|[2412.18511](http://arxiv.org/abs/2412.18511)|null|深度强化学习（DRL）在自动驾驶决策方面展现出巨大的潜力。然而，由于学习效率低，DRL在复杂驾驶场景中实现合格策略需要大量的计算资源。此外，利用人类专家的指导来提高DRL的性能会带来高昂的劳动成本，这限制了其实际应用。在本研究中，我们提出了一种新型的大语言模型（LLM）引导的深度强化学习（LGDRL）框架，用于解决自动驾驶车辆的决策问题。在这个框架中，将基于LLM的驾驶专家集成到DRL中，为DRL的学习过程提供智能指导。随后，为了高效利用LLM专家的指导来提高DRL决策策略的性能，通过一种创新的专家策略约束算法和一种新的LLM干预交互机制，增强了DRL的学习和交互过程。实验结果表明，我们的方法不仅实现了90%的任务成功率，具有优越的驾驶性能，而且与最先进的基线算法相比，显著提高了学习效率和专家指导的利用率。此外，提出的方法使得DRL代理在缺乏LLM专家指导的情况下也能保持一致和可靠的表现。代码和补充视频可在https://bitmobility.github.io/LGDRL/找到。|\n",
        "2412.19784": "|**2024-12-27**|**Can AI Help with Your Personal Finances?**|Oudom Hean et.al.|[2412.19784](http://arxiv.org/abs/2412.19784)|null|近年来，大型语言模型（LLMs）作为人工智能（AI）领域的一项颠覆性发展，受到了业界和学术界的广泛关注。这些复杂的AI系统在庞大的数据集上训练，展现出令人印象深刻的自然语言处理和内容生成能力。本文探讨了LLMs在解决个人财务领域关键挑战中的潜力，重点关注美国市场。我们评估了包括OpenAI的ChatGPT、谷歌的Gemini、Anthropic的Claude和Meta的Llama在内的几个领先的LLMs，以评估它们在提供关于抵押贷款、税收、贷款和投资等主题的准确财务建议方面的有效性。我们的发现表明，虽然这些模型实现了大约70%的平均准确率，但在某些领域也显示出明显的局限性。具体来说，LLMs在提供复杂财务查询的准确回答方面存在困难，其表现因不同主题而显著不同。尽管存在这些局限性，分析显示这些模型的新版本有显著改进，突显了它们在个人和财务顾问中的日益增长的应用价值。随着这些AI系统持续发展，它们在推动个人财务领域AI驱动应用方面的潜力变得越来越有希望。|\n",
        "2412.19770": "|**2024-12-27**|**Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration**|Le Chen et.al.|[2412.19770](http://arxiv.org/abs/2412.19770)|**[link](https://github.com/hpc-fortran2cpp/fortran2cpp)**|**将Fortran代码迁移到C++是许多科学计算团队常见的任务，这一需求推动了现代编程范式、跨平台兼容性的提升以及维护性的改进。利用大型语言模型（LLMs）自动化这一翻译过程已显示出潜力，但高质量、专业数据集的缺乏阻碍了其有效性。在本文中，我们通过引入一个专门为Fortran到C++代码迁移设计的创新多轮对话数据集Fortran2CPP来解决这一挑战。我们的数据集比现有替代品大得多，使用一个独特的LLM驱动的双代理管道生成，该管道结合了迭代编译、执行和代码修复，以确保高质量和功能正确性。为了展示我们数据集的有效性，我们在Fortran2CPP上对几个开放权重的LLMs进行了微调，并评估了它们在两个独立基准上的性能。在我们的数据集上进行微调带来了显著提升，模型在CodeBLEU评分上实现了高达3.31倍的提升，编译成功率提高了92%。这突显了数据集增强翻译的C++代码的语法准确性和可编译性的能力。我们的数据集和模型已开源，可在我们的公共GitHub仓库中获取[脚注：\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}]。**|\n",
        "2412.19726": "|**2024-12-27**|**Can Large Language Models Adapt to Other Agents In-Context?**|Matthew Riemer et.al.|[2412.19726](http://arxiv.org/abs/2412.19726)|null|随着研究界致力于构建更动态、更个性化的AI助手，以适应与人类互动的多样性，对评估大型语言模型（LLMs）的心智理论能力产生了更大的兴趣。确实，一些最近的研究表明，LLMs的心智理论能力相当令人印象深刻，接近人类水平的表现。我们的论文旨在反驳这种说法，并认为过去的研究并没有直接测量代理的表现，这可能导致了一些本质上具有幻觉性质的结果。我们区分了我们称之为字面心智理论和功能心智理论，即衡量代理预测他人行为的能力和根据对他人行为预测的理性反应来适应情境中的代理。我们发现，表现最出色的开源LLMs可能在字面心智理论方面表现出强大的能力，这取决于如何提示它们，但似乎在功能心智理论方面遇到困难——即使合作伙伴策略非常简单。我们的工作有助于突出LLMs在适应新情况时归纳偏见的两面性。虽然这种偏见可能在有限的范围内导致强大的表现，但它通常阻碍了达到最佳长期行为的收敛。|\n",
        "2412.19707": "|**2024-12-27**|**Toward Adaptive Reasoning in Large Language Models with Thought Rollback**|Sijia Chen et.al.|[2412.19707](http://arxiv.org/abs/2412.19707)|**[link](https://github.com/iQua/llmpebase)**|**大型语言模型（LLMs）通常用于通过逐步推理来解决各种任务。然而，中间推理步骤或思维的结构是刚性和单向的，例如链式、树形或无环有向图。因此，这种不灵活且仅向前推理的方法可能无法应对挑战性任务，并且在LLM频繁给出错误回答，即“幻觉”时可能会失败。本文提出了一种新的推理框架，称为思维回滚（TR），允许LLMs在解决“幻觉”问题时，自适应地构建思维结构，同时保持有效的推理。TR的核心机制是回滚思维，这使得LLMs能够对思维进行错误分析，并因此回滚到任何之前的错误思维进行修订。随后，通过将这种试错过程包含在提示中引导LLM，每次回滚都会导致一条更可靠的推理路径。因此，从没有人类标注的简单提示开始，带有TR的LLM自适应地逐步探索思维以找到正确解决方案。在数学问题和多任务推理上的综合实验表明，TR在解决问题的成功率和交互成本方面达到了最先进的水平。例如，在MATH数据集上，带有TR的GPT-4的解决率比当前最佳方案高出9%。**|\n",
        "2412.19685": "|**2024-12-27**|**A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization**|Jingchun Lian et.al.|[2412.19685](http://arxiv.org/abs/2412.19685)|null|图像伪造定位，即识别图像中的篡改像素，已经取得了显著的进展。传统方法通常将这一挑战视为图像分割的一种变体，将伪造区域的二值分割视为最终产品。我们认为基本的二值伪造掩码不足以解释模型的预测。它无法阐明模型为何指明某些区域以及如何对待所有伪造像素，这使得难以识别最不真实的部分。在本研究中，我们通过为伪造图像生成突出区域专注的解释来缓解上述局限性。为此，我们构建了一个多模态伪造追踪（MMTT）数据集，包括使用深度伪造技术处理的面部图像，并配以人工的、可解释的文本注释。为了获取高质量的注释，注释员被要求仔细观察被篡改的图像，并描述伪造区域的典型特征。随后，我们收集了128,303个图像-文本对的数据集。利用MMTT数据集，我们开发了ForgeryTalker，这是一种旨在同时进行伪造定位和解释的架构。ForgeryTalker首先训练一个伪造提示网络以识别解释文本中的关键线索。随后，区域提示被纳入多模态大型语言模型中进行微调，以实现定位和解释的双重目标。在MMTT数据集上进行的广泛实验验证了我们所提出模型的优势。该数据集、代码以及预训练的检查点将被公开提供，以促进进一步的研究并确保我们结果的复现性。|\n",
        "2412.19684": "|**2024-12-27**|**Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework**|Jiang Liu et.al.|[2412.19684](http://arxiv.org/abs/2412.19684)|null|高效多模态大型语言模型（EMLLMs）与多模态大型语言模型（MLLMs）相比，减少了模型大小和计算成本，通常部署在资源受限的设备上。然而，由于数据隐私的担忧，现有的开源EMLLMs在预训练过程中很少能够访问到私有领域特定数据，这使得它们难以直接应用于特定设备领域，例如某些商业场景。为了解决这一弱点，本文重点关注EMLLMs对私有领域的有效适应，特别是在以下两个领域：1）如何减少数据需求；2）如何避免参数微调。具体来说，我们提出了一种无调整、自适应、通用的提示优化框架，简称我们的方法，它包括两个阶段：1）预定义提示，基于强化搜索策略，生成提示优化策略树以获取优化先验；2）提示反思基于优化先验初始化提示，然后进行自我反思以进一步搜索和细化提示。通过这种方式，我们的方法巧妙地生成了处理私有领域特定数据的“理想提示”。请注意，我们的方法不需要参数微调，并且只需要少量数据就可以快速适应私有数据的数据分布。在多个任务上的大量实验表明，与基线相比，我们提出的方法在效率和性能方面都有显著提升。|\n",
        "2412.19663": "|**2024-12-27**|**CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs**|Siyu Wang et.al.|[2412.19663](http://arxiv.org/abs/2412.19663)|null|计算机辅助设计（CAD）通过实现精确的二维和三维建模、广泛的分析和优化，显著提高了设计过程的效率、准确性和创新性。现有的创建CAD模型的方法依赖于潜在向量或点云，这些方法难以获得且存储成本高昂。近期在多模态大型语言模型（MLLM）方面的进步激发了研究人员使用自然语言指令和图像来进行CAD模型构建。然而，这些模型在推断准确的3D空间位置和方向方面仍然存在困难，导致在确定构建几何体的空间3D起始点和挤出方向时出现误差。本研究引入了CAD-GPT，这是一种具有空间推理增强的MLLM的CAD合成方法，可以接受单个图像或文本描述作为输入。为了实现精确的空间推断，我们的方法引入了3D建模空间机制。该方法通过专用空间展开机制将3D空间位置和3D草图平面旋转角度映射到一维语言特征空间，同时将二维草图坐标离散化到适当的平面空间，以实现空间起始位置、草图方向和二维草图坐标平移的精确确定。大量实验表明，CAD-GPT在CAD模型合成方面，无论是从定量还是定性角度来看，都优于现有的最先进方法。|\n",
        "2412.19652": "|**2024-12-27**|**FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios**|Kaiyi Pang et.al.|[2412.19652](http://arxiv.org/abs/2412.19652)|null|语言隐写术将秘密信息嵌入看似无辜的文本中，以保护监视环境中的隐私。生成式语言隐写术利用语言模型（LM）的概率分布，并应用隐写算法生成隐写标记，随着大型语言模型（LLM）的近期进展而受到关注。为了提高安全性，研究人员开发了保持分布的隐写算法，以最小化隐写采样与LM采样之间的差距。然而，对语言模型分布的依赖，以及与真实世界掩护文本的偏差，导致在实际场景中面对隐写分析检测器时，隐写术的不可感知性不足。此外，LLM分布往往更确定，导致熵减少，从而降低嵌入容量。在本文中，我们提出了FreStega，这是一种即插即用方法，用于重建用于生成式语言隐写术的语言模型分布。FreStega在隐写文本自回归生成的每一步动态调整标记概率，利用了序列和空间维度。在序列调整中，根据瞬时熵动态调整温度，增强隐写文本的多样性并提高嵌入容量。在空间维度上，分布与目标领域语料库的指导对齐，紧密模仿目标领域的真实掩护文本。通过重塑分布，FreStega提高了实际场景中隐写文本的不可感知性，并通过提高15.41%的隐写能力，所有这些都不损害生成文本的质量。FreStega作为即插即用的补救措施，增强了现实场景中现有保持分布隐写术方法的不可感知性和嵌入容量。|\n",
        "2412.19638": "|**2024-12-27**|**Xmodel-2 Technical Report**|Wang Qun et.al.|[2412.19638](http://arxiv.org/abs/2412.19638)|null|Xmodel-2是一种专门为推理任务设计的1.2亿参数的大型语言模型。其架构允许不同规模的模型共享一组统一的超参数，从而可以在较小的模型上进行广泛的实验，并无缝地将最佳配置转移到较大的模型上。为了最大化训练效率和稳定性，Xmodel-2采用了MiniCPM中的WSD学习率调度器。在来自不同来源的1500亿个标记上预训练后，Xmodel-2在复杂推理和基于代理的任务中实现了最先进的性能，同时保持了低训练成本。这些结果突显了高效模型设计和训练策略在提升推理能力方面的潜力。模型检查点和代码在GitHub上公开，链接为https://github.com/XiaoduoAILab/Xmodel-2。|\n",
        "2412.19630": "|**2024-12-27**|**IMTP: Search-based Code Generation for In-memory Tensor Programs**|Yongwon Shin et.al.|[2412.19630](http://arxiv.org/abs/2412.19630)|null|DRAM-PIM（DRAM中的处理）技术已成为加速现代应用中内存密集型操作（如大型语言模型（LLMs））的有前途的技术。尽管其潜力巨大，但当前DRAM-PIM的软件堆栈面临重大挑战，包括依赖手动调整的库，这阻碍了可编程性、对高级抽象支持有限以及缺乏系统化的优化框架。为了解决这些限制，我们提出了IMTP，这是一个针对UPMEM的基于搜索的优化张量编译器。IMTP的关键特性包括：（1）自动搜索主机和内核张量程序的联合搜索空间，（2）针对PIM的优化以高效处理边界条件，以及（3）针对UPMEM系统扩展搜索空间的改进搜索算法。我们在UPMEM硬件上的实验结果表明，各种UPMEM基准核的性能提升可达8.21倍，GPT-J层可达5.33倍。据我们所知，IMTP是第一个为DRAM-PIM系统提供完全自动化、集成自调优代码生成支持的张量编译器。通过弥合高级张量计算抽象和底层硬件特定要求之间的差距，IMTP为提升DRAM-PIM的可编程性和实现优化流程的简化奠定了基础。|\n",
        "2412.21200": "|**2024-12-30**|**Distributed Mixture-of-Agents for Edge Inference with Large Language Models**|Purbesh Mitra et.al.|[2412.21200](http://arxiv.org/abs/2412.21200)|**[link](https://github.com/purbeshmitra/distributed_moa)**|**混合代理（MoA）最近被提出作为一种增强大型语言模型（LLM）性能的方法，它允许多个独立的LLM协同进行推理。这种协作方法在响应用户提示方面比依赖单个LLM有显著改进。在本文中，我们考虑了这样一个MoA架构在分布式环境中的应用，其中LLM在单个边缘设备上运行，每个设备都与一个用户唯一关联，并配备自己的分布式计算能力。这些设备使用去中心化的八卦算法交换信息，允许不同的设备节点在没有集中式服务器监督的情况下进行交流。在考虑的设置中，不同的用户拥有自己的LLM模型来处理用户提示。此外，设备可以传播它们自己的用户特定提示或增强提示，以生成对某些查询的更精确答案。当对应的LLM忙碌时，用户提示暂时存储在设备队列中。鉴于边缘设备的内存限制，确保系统中平均队列大小保持有界至关重要。在本文中，我们通过在合理假设下理论计算设备队列的排队稳定性条件来解决这个问题，并通过实验进行验证。此外，通过使用开源LLM实现分布式MoA的实验，我们证明了某些MoA配置在AlpacaEval 2.0基准测试中产生的响应质量比其他配置更高。该实现可在以下网址获得：https://github.com/purbeshmitra/distributed_moa。**|\n",
        "2412.21199": "|**2024-12-30**|**HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation**|Zhaojian Yu et.al.|[2412.21199](http://arxiv.org/abs/2412.21199)|**[link](https://github.com/CodeEval-Pro/CodeEval-Pro)**|**我们引入了自我调用代码生成这一新任务，旨在评估大型语言模型（LLMs）的渐进推理和问题解决能力。在这个任务中，模型首先面对一个基础问题，然后是与之相关且更复杂的问题。它们必须先解决基础问题，然后利用其解决方案来解决更复杂的问题。这项工作有三个主要贡献。首先，我们提出了一种生成现有基准测试更具挑战性版本的一般方法，从而产生了三个新的基准：HumanEval Pro、MBPP Pro和BigCodeBench-Lite Pro，这些基准专门设计用于评估LLMs在自我调用代码生成方面的能力。其次，通过对二十个LLMs在我们基准测试上的实验结果进行分析，我们有两个重要观察：（i）大多数LLMs在传统的代码生成基准测试（如HumanEval和MBPP）中表现出色，但在自我调用任务上的表现有所下降。例如，o1-mini在HumanEval上实现了96.2%的pass@1，但在HumanEval Pro上只有76.2%。（ii）在自我调用代码生成任务中，与基础模型相比，指令调整模型只显示出微小的改进。第三，我们披露了我们评估结果中存在的失败模式类型。所有这些结果强调了在自我调用代码生成任务上进一步发展的必要性，并为未来关于增强LLMs代码推理能力的的研究提供了新的方向。**|\n",
        "2412.21140": "|**2024-12-30**|**Facilitating large language model Russian adaptation with Learned Embedding Propagation**|Mikhail Tikhomirov et.al.|[2412.21140](http://arxiv.org/abs/2412.21140)|**[link](https://github.com/RefalMachine/llmtf_open)**|**大型语言模型（LLM）技术的快速发展导致了具有与GPT-4等顶尖模型相当文本生成质量的开源指令微调LLM的引入。尽管这类模型的出现加速了LLM技术在敏感信息环境中的采用，但其作者并未公开用于结果复现的训练数据，因此使得这些成就模型专属。由于这些开源模型也是多语言的，这反过来减少了专门训练语言特定LLM的益处，因为改进的推理计算效率成为了这一昂贵程序唯一的保证优势。词汇扩展和后续持续预训练等更经济的选项也受到了阻碍，因为缺乏访问高质量指令微调数据的限制，而这是导致LLM任务解决能力的主要因素。为了解决这些限制并降低语言适应管道的成本，我们提出了学习嵌入传播（LEP）。与现有方法不同，我们的方法由于对现有LLM知识影响最小，因此对训练数据量要求较低，我们通过一种新颖的临时代码嵌入传播程序来强化这一知识，该程序允许跳过指令微调步骤，而是将新的语言知识直接植入任何现有的指令微调变体中。我们对LLaMa-3-8B和Mistral-7B的四种俄语词汇适应性进行了评估，结果表明LEP与传统指令微调方法具有竞争力，其性能与OpenChat 3.5和LLaMa-3-8B-Instruct相当，通过自我校准和持续调整进一步增强了任务解决能力。**|\n",
        "2412.21123": "|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越依赖于网络爬取的数据集，对未经授权使用受版权或个人内容进行训练的担忧日益加剧。尽管有像通用数据保护条例（GDPR）这样的法规，数据所有者对其内容在模型训练中的使用仍有限制。为了解决这个问题，我们提出了ExpShield，这是一种主动自我保护机制，使内容所有者能够将不可见的扰动嵌入到他们的文本中，限制LLMs训练中的数据滥用，同时不影响可读性。这种预防性方法使数据所有者能够直接保护敏感内容，而不依赖于第三方进行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的原因。我们进一步通过识别记忆触发器和创建陷阱来更集中地偏离模型的记忆，从而提高效率。为了验证我们防御的有效性，我们提出了一种新的实例利用度量标准，该标准捕捉了模型训练引起的个别风险。实验结果表明，我们的方法有效，MIA AUC从0.95降至0.55，实例利用接近于零。这表明训练后个别风险并未增加，突出了在保护受版权数据中主动防御的重要性。|\n",
        "2412.21051": "|**2024-12-30**|**Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense**|Yuyang Zhou et.al.|[2412.21051](http://arxiv.org/abs/2412.21051)|**[link](https://github.com/SEU-ProactiveSecurity-Group/LLM-PD)**|**云计算技术的快速发展和云应用的日益增多，为日常生活带来了大量便利。然而，不同组件的多样性和复杂性对云安全构成了重大挑战，尤其是在面对复杂和高级的网络安全攻击时。近年来，生成基础模型（GFMs）的进步，尤其是在大型语言模型（LLMs）方面，为安全智能提供了有希望的解决方案。通过利用语言理解、数据分析、任务推理、行动规划和代码生成等方面的强大能力，我们提出了LLM-PD，这是一种新型的主动防御架构，能够以主动方式击败各种威胁。LLM-PD能够通过全面的数据分析和顺序推理高效地做出决策，并在目标云上动态创建和部署可执行的防御机制。此外，它可以根据从前交互中学习到的经验灵活自我进化，无需额外训练即可适应新的攻击场景。实验结果表明，它在防御效果和效率方面表现出显著的能力，尤其是在与其他现有方法相比时，其成功率尤为突出。**|\n",
        "2412.21037": "|**2024-12-30**|**TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization**|Chia-Yu Hung et.al.|[2412.21037](http://arxiv.org/abs/2412.21037)|**[link](https://github.com/declare-lab/TangoFlux)**|**我们引入了TangoFlux，这是一个具有515M参数的高效文本到音频（TTA）生成模型，能够在单个A40 GPU上仅需3.7秒生成长达30秒的44.1kHz音频。在调整TTA模型时，一个关键挑战在于创建偏好对，因为TTA缺乏像大型语言模型（LLMs）中可验证的奖励或黄金标准答案这样的结构化机制。为了解决这个问题，我们提出了CLAP-Ranked偏好优化（CRPO），这是一个新颖的框架，它通过迭代生成和优化偏好数据来增强TTA的调整。我们展示了使用CRPO生成的音频偏好数据集优于现有替代方案。使用这个框架，TangoFlux在客观和主观基准测试中均达到了最先进的性能。我们将所有代码和模型开源，以支持TTA生成方面的进一步研究。**|\n",
        "2412.21036": "|**2024-12-30**|**GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models**|Shangyu Xing et.al.|[2412.21036](http://arxiv.org/abs/2412.21036)|null|多模态大型语言模型（MLLMs）在整合视觉和语言理解方面取得了显著进展。尽管现有的基准评估这些模型在情境丰富、真实生活场景中的表现，但它们往往忽略了对于偏离日常现实环境的基本感知技能。特别是几何感知，即解读空间关系和抽象视觉模式的能力，仍然没有得到充分探索。为了解决这一局限性，我们引入了GePBench，这是一个新的基准，旨在评估MLLMs的几何感知能力。大量评估的结果显示，当前最先进的MLLMs在这些任务中存在显著的不足。此外，我们还证明了使用GePBench数据源训练的模型在广泛的下游任务上表现出显著的改进，这突出了几何感知作为高级多模态应用基础的重要性。我们的代码和数据集将公开可用。|\n",
        "2412.21015": "|**2024-12-30**|**MapQaTor: A System for Efficient Annotation of Map Query Datasets**|Mahir Labib Dihan et.al.|[2412.21015](http://arxiv.org/abs/2412.21015)|**[link](https://github.com/MapQaTor/.github/tree/main/profile)**|**地图和导航服务，如谷歌地图、苹果地图、Openstreet Maps，对于访问各种基于位置的数据至关重要，但它们在处理自然语言地理空间查询方面往往力不从心。近期在大型语言模型（LLM）方面取得的进展在问答（QA）方面显示出希望，但从地图服务中创建可靠的地理空间问答数据集仍然具有挑战性。我们介绍了MapQaTor，一个简化了可重复、可追溯的基于地图的问答数据集创建过程的网络应用程序。凭借其即插即用的架构，MapQaTor能够与任何地图API无缝集成，使用户能够在最少设置的情况下收集和可视化来自不同来源的数据。通过缓存API响应，该平台确保了一致的真实信息，即使在现实世界信息发生变化时，也增强了数据的可靠性。MapQaTor将数据检索、标注和可视化集中在一个平台上，为评估基于LLM的地理空间推理的现状并提升其地理空间理解能力提供了独特的机会。评估指标显示，与手动方法相比，MapQaTor至少将标注过程的速度提高了30倍，突显了其在开发地理空间资源，如复杂地图推理数据集等方面的潜力。该网站已上线，网址为：https://mapqator.github.io/，演示视频可在：https://youtu.be/7_aV9Wmhs6Q查看。**|\n",
        "2412.21006": "|**2024-12-30**|**Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria**|Joonwon Jang et.al.|[2412.21006](http://arxiv.org/abs/2412.21006)|null|大型语言模型（LLMs）依赖于生成大量的中间推理单元（例如，标记、句子）以增强在各种复杂任务中的最终答案质量。虽然生成多个推理路径或迭代地完善推理有证明是有效的，但这些方法不可避免地导致推理成本显著增加。在这项工作中，我们提出了一种新的句子级推理减少训练框架，该框架利用基于似然性的标准，即冗余度，来识别和删除冗余的推理句子。与之前使用标记级减少的方法不同，我们的句子级减少框架在减少生成长度的同时保持了模型性能。这保留了LLMs的原始推理能力，并在各种模型和任务中实现了平均17.15%的生成成本降低。|\n",
        "2412.20996": "|**2024-12-30**|**Plug-and-Play Training Framework for Preference Optimization**|Jingyuan Ma et.al.|[2412.20996](http://arxiv.org/abs/2412.20996)|null|近期，如DPO等偏好优化方法显著提升了大型语言模型（LLMs）在包括对话和问答在内的广泛任务中的表现。然而，现有方法未能考虑到在偏好优化过程中训练样本难度水平的差异，导致在高精度要求任务中表现平庸，特别是在数学推理任务中。为了解决这一局限，我们提出了一种新颖的训练框架，该框架采用多采样来分析输出分布，为样本分配不同的权重，并将这些权重纳入偏好优化过程。这种即插即用的方法使LLMs能够在训练中优先考虑具有挑战性的示例，提高学习效率。实验结果表明，我们的框架可以无缝地与各种偏好优化方法集成，并在数学推理任务中实现一致的改进。|\n"
    },
    "infer": {
        "2411.18191": "|**2024-11-27**|**InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks**|Xinyao Zheng et.al.|[2411.18191](http://arxiv.org/abs/2411.18191)|null|大型语言模型（LLMs）具备广泛的知识和问答能力，已在金融和医疗咨询等对隐私敏感的领域得到广泛应用。在LLMs推理过程中，缓存共享方法被广泛采用以提高效率，通过重用缓存的状态或响应来处理相同或相似的推理请求。然而，我们发现这些缓存机制存在隐私输入泄露的风险，因为缓存可能导致响应时间出现可观察的变化，使其成为基于时间攻击的强候选线索。在本研究中，我们提出了一种新颖的基于时间的侧信道攻击，用于在LLMs推理中执行输入窃取。基于缓存的攻击面临在大型搜索空间中构建候选输入以击中和窃取缓存用户查询的挑战。为了解决这些挑战，我们提出了两个主要组件。输入构造器采用机器学习技术和基于LLM的方法进行词汇相关性学习，同时在通用输入构建中实施优化的搜索机制。时间分析器通过异常值去除实现统计时间拟合，以识别缓存命中模式，并持续提供反馈以优化构造器的搜索策略。我们在两种缓存机制上进行了实验，结果表明我们的方法在各种应用中均能持续获得高攻击成功率。我们的工作突出了与性能优化相关的安全漏洞，强调了在LLMs推理增强的同时优先考虑隐私和安全的必要性。|\n",
        "2411.18077": "|**2024-11-27**|**MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache**|Akshat Sharma et.al.|[2411.18077](http://arxiv.org/abs/2411.18077)|null|由于LLMs（大型语言模型）对内存和计算的高要求，如何在实践中高效地为LLMs提供服务变得极为挑战。在本研究中，我们调查了优化KV缓存的方法，因为其内存占用是LLM推理中的关键瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了MiniKV，这是一种KV缓存优化方法，通过一种新颖的2比特层区分性KV缓存，在同时保持长上下文任务准确性的同时，显著减少了KV缓存的大小。更重要的是，我们开发了专门的CUDA内核，使MiniKV与FlashAttention兼容。在广泛的长上下文任务上的实验表明，MiniKV有效地实现了86%的KV缓存压缩比，同时恢复了超过98.5%的准确性，优于现有方法，同时实现了卓越的系统性能提升。|\n",
        "2411.17309": "|**2024-11-26**|**PIM-AI: A Novel Architecture for High-Efficiency LLM Inference**|Cristobal Ortega et.al.|[2411.17309](http://arxiv.org/abs/2411.17309)|null|大型语言模型（LLMs）因其先进的语言理解和生成能力，在众多应用中变得至关重要。然而，它们对计算和内存的要求给传统的硬件架构带来了巨大的挑战。内存中处理（PIM）将计算单元直接集成到内存芯片中，为LLM推理提供了多项优势，包括减少数据传输瓶颈和提高能效。本文介绍了一种名为PIM-AI的新型DDR5/LPDDR5 PIM架构，专为LLM推理设计，无需修改内存控制器或DDR/LPDDR内存PHY。我们开发了一个模拟器来评估PIM-AI在不同场景下的性能，并证明了其相较于传统架构的显著优势。在基于云的场景中，PIM-AI相较于最先进的GPU，将每秒查询的三年总拥有成本降低了高达6.94倍，具体取决于所使用的LLM模型。在移动场景中，PIM-AI相较于最先进的移动SoC，在每token能耗上实现了10到20倍降低，从而实现了每秒查询增加25到45%，每查询能耗减少6.9倍到13.4倍，延长了电池寿命，并使每次充电的推理次数更多。这些结果突显了PIM-AI颠覆LLM部署的潜力，使其更加高效、可扩展和可持续。|\n",
        "2411.17116": "|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，使用Transformer基于的大型语言模型（LLMs）在长序列上进行推理既耗时又昂贵。我们引入了星型注意力，这是一种两阶段的块稀疏近似，通过在多个主机之间分片注意力来提高计算效率，同时最大限度地减少通信开销。在第一阶段，使用并行跨主机的块局部注意力处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星型注意力与大多数使用全局注意力训练的Transformer基于的LLMs无缝集成，通过减少内存需求和推理时间最多11倍，同时保留95-100%的准确率。**|\n",
        "2411.17089": "|**2024-11-26**|**Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation**|Chaoyi Jiang et.al.|[2411.17089](http://arxiv.org/abs/2411.17089)|null|对于大型语言模型（LLMs）的推理计算量很大。为了降低自回归解码的成本，采用键值（KV）缓存来存储中间激活，使得GPU只需进行每个新标记所需的增量计算。这种方法显著降低了标记生成的计算开销。然而，KV缓存的内存需求迅速增长，通常超过GPU内存容量。一种成本效益更高的替代方案是将KV缓存卸载到CPU内存中，这可以缓解GPU内存压力，但将瓶颈转移到CPU和GPU之间有限的PCIe连接带宽。现有方法试图通过重叠GPU计算与I/O或采用CPU-GPU异构执行来解决这些问题，但它们受到过度数据移动和对CPU能力的依赖的阻碍。在本文中，我们介绍了一种高效的CPU-GPU I/O感知LLM推理方法，通过在同时通过PCIe总线传输剩余KV缓存的同时，从激活中重新计算部分KV缓存，避免了将整个KV缓存从CPU传输到GPU。这种方法重叠GPU重新计算与数据传输，以最小化GPU空闲时间并最大化推理性能。我们的方法通过集成一个利用输入特性和系统硬件信息的分析模块、一个用于优化计算和通信工作负载分配的调度模块以及一个用于高效执行派生执行计划的运行时模块而完全自动化。实验结果表明，与最先进的方法相比，我们的方法在解码时的延迟降低了高达35.8%，吞吐量提高了46.2%。|\n",
        "2411.16158": "|**2024-11-25**|**MixPE: Quantization and Hardware Co-design for Efficient LLM Inference**|Yu Zhang et.al.|[2411.16158](http://arxiv.org/abs/2411.16158)|null|基于Transformer的大型语言模型（LLMs）随着模型规模的不断扩大取得了显著的成功，但它们的部署仍然面临挑战，主要是因为计算和内存需求巨大。量化技术已成为一种有前景的解决方案，而针对LLMs的最先进量化算法引入了混合精度矩阵乘法（mpGEMM）的需求，即使用低精度权重与高精度激活进行乘法运算。尽管这种方法有优势，但当前硬件加速器如GPU和TPU缺乏对高效mpGEMM的原生支持，导致主顺序循环中的去量化操作效率低下。为了解决这一限制，我们引入了MixPE，这是一种专门设计的混合精度处理单元，旨在高效地在LLM推理中进行低比特量化。MixPE利用两项关键创新来最小化去量化开销并充分发挥低比特量化的潜力。首先，我们认识到每个量化组内的缩放因子和零点是可以共享的，因此我们提议在每个组mpGEMM之后进行去量化，这显著降低了去量化开销。其次，MixPE不是依赖于传统的乘法器，而是使用高效的移位和加法操作进行乘法运算，从而优化了计算和能效。我们的实验结果表明，MixPE在速度上比最先进的量化加速器快2.6倍，在能耗上减少1.4倍。|\n",
        "2411.16003": "|**2024-11-24**|**eFedLLM: Efficient LLM Inference Based on Federated Learning**|Shengwen Ding et.al.|[2411.16003](http://arxiv.org/abs/2411.16003)|null|大型语言模型（LLMs）标志着人工智能（AI）领域的变革时代。然而，LLMs所涉及的数据和参数规模巨大，需要高要求的计算和内存资源，这限制了它们对更广泛用户和研究者的可及性。本文介绍了一种有效的方法，可以提升LLM推理的操作效率和成本效益。通过利用基于transformer的联邦学习（FL）与模型并行分布式训练，我们的模型能够高效地在参与者网络中分配计算负载和内存需求。这种策略允许用户，尤其是那些资源有限的用户，可以协同训练最先进的LLMs。我们还创新了FL框架中的激励机制，奖励有益的贡献并过滤掉恶意活动，从而保护训练过程的完整性和可靠性。同时，我们利用内存层次策略和权重矩阵的奇异值分解（SVD）进一步提升了计算和内存效率。我们的结果，通过公式分析和数值计算得出，显著优化了资源使用，并使先进LLMs的访问权民主化，确保广泛的用户既能参与也能从中受益。|\n",
        "2411.15982": "|**2024-11-24**|**Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format**|Chao Fang et.al.|[2411.15982](http://arxiv.org/abs/2411.15982)|null|广泛应用的仅权重量化的大型语言模型（LLM），利用低比特整数（INT）权重并保留浮点（FP）激活，在减少存储需求的同时保持了准确性。然而，这将能耗和延迟瓶颈转向了与昂贵的内存访问和计算相关的FP激活。现有的LLM加速器主要关注计算优化，忽略了联合优化FP计算和数据移动的潜力，尤其是在LLM推理中的主导FP-INT GeMM操作。为了解决这些挑战，我们研究了各种LLM模块中激活精度的敏感性及其对整体模型准确性的影响。基于我们的发现，我们首先提出了Anda数据类型：一种具有组共享指数位和动态尾数位分配的自适应数据格式。其次，我们开发了一种迭代的训练后自适应精度搜索算法，优化不同LLM模块的位宽，以平衡模型准确性、能耗和推理速度。最后，提出了一系列硬件优化技术，以最大限度地发挥Anda格式的优势。这包括基于位面的数据组织方案、具有位串计算功能的Anda增强处理单元以及运行时位面Anda压缩器，以同时优化存储、计算和内存占用。我们在FPINT GeMM操作上的评估表明，与GPU类似的FP-FP基准相比，Anda在包括OPT、LLaMA和LLaMA-2系列在内的流行LLM上平均实现了2.4倍的加速、4.0倍的面积效率提升和3.1倍的能耗效率提升。Anda在各种应用场景、精度要求和系统性能方面表现出强大的适应性，使得在广泛的部署场景中实现高效的LLM推理成为可能。|\n",
        "2411.17741": "|**2024-11-24**|**Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments**|Nikoleta Iliakopoulou et.al.|[2411.17741](http://arxiv.org/abs/2411.17741)|null|随着大型语言模型（LLMs）的广泛应用，其部署数量呈指数级增长，对推理集群提出了巨大需求。这些集群必须处理针对不同LLM下游任务的大量并发查询。为了处理具有大量LLM参数的多任务设置，方法如低秩自适应（LoRA）允许针对特定任务进行微调，同时跨任务共享大部分基础LLM模型。因此，它们允许以最小的内存需求并发处理任务。然而，现有的LLM服务系统存在效率低下的问题：它们忽视了工作负载异构性，由于频繁的适配器加载而施加了高链路带宽，以及在调度器中存在头阻塞问题。为了解决这些挑战，我们提出了Chameleon，这是一个针对多个适配器环境优化的新型LLM服务系统，它依赖于两个核心思想：适配器缓存和适配器感知调度。首先，Chameleon在GPU内存中缓存流行的适配器，最小化适配器加载时间。重要的是，它使用原本闲置的GPU内存，避免了额外的内存成本。其次，Chameleon使用非抢占式多队列调度，以高效地处理工作负载异构性。通过这种方式，Chameleon同时防止了头阻塞和饥饿现象。我们在最先进的LLM服务平台之上实现了Chameleon，并使用真实世界的生产跟踪和开源LLM对其进行了评估。在高负载下，Chameleon将P99和P50的TTFT延迟分别降低了80.7%和48.1%，同时与最先进的基线相比，提高了1.5倍的吞吐量。|\n",
        "2411.15715": "|**2024-11-24**|**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**|Wenxiang Lin et.al.|[2411.15715](http://arxiv.org/abs/2411.15715)|null|大型语言模型（LLMs）因其庞大的模型尺寸而闻名，对计算资源和内存需求极高，导致在中等GPU系统上的推理效率低下。量化或剪枝等技术可以缩小模型尺寸，但通常会损害准确度，使其不适合实际应用。在这项工作中，我们介绍了\\modelname{}，这是一个高性能的推理引擎，旨在加快LLMs的推理速度，同时不降低模型精度。\\modelname{}采用了三种创新方法来提高推理效率：1）模型分区，允许跨CPU计算、GPU计算和CPU-GPU通信异步处理任务，2）自适应分区算法，以优化CPU、GPU和PCIe通信能力的利用，3）令牌分配策略，用于处理LLMs推理过程中的各种提示和生成任务。我们使用Mixtral、LLaMA-2、Qwen和PhiMoE等LLMs，在具有不同CPU和GPU的三个测试环境中进行了综合实验。实验结果表明，\\modelname{}在解码速度上比$1.11\\times$到$1.80\\times$更快，在预填充速度上比$1.69\\times$到$6.33\\times$更快，与最先进的解决方案llama.cpp和Fiddler相比，整体速度提高了$1.25\\times$到$2.04\\times$。|\n",
        "2411.19542": "|**2024-11-29**|**A dynamic parallel method for performance optimization on hybrid CPUs**|Luo Yu et.al.|[2411.19542](http://arxiv.org/abs/2411.19542)|null|AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，当前的AI推理框架忽略了混合CPU硬件能力的失衡，导致推理性能低下。为了解决这个问题，我们引入了一种针对混合CPU的动态并行方法，该方法通过在并行工作开始之前平衡混合CPU每个核心的工作负载，显著提高了大型语言模型（LLM）的推理性能。这种方法使Neural Speed能够在两个混合英特尔CPU上实现超过90%（平均）的内存带宽。|\n",
        "2411.19146": "|**2024-11-28**|**Puzzle: Distillation-Based NAS for Inference-Optimized LLMs**|Akhiad Bercovich et.al.|[2411.19146](http://arxiv.org/abs/2411.19146)|null|大型语言模型（LLMs）展示了惊人的能力，但它们的采用受到推理过程中高计算成本的限制。虽然增加参数数量可以提高准确性，但它也拉大了最先进的能力与实际部署之间的差距。我们提出了Puzzle框架，该框架在特定硬件上加速LLMs的推理，同时保持其能力。通过前所未有的规模创新性地应用神经架构搜索（NAS），Puzzle在硬件约束下系统地优化了具有数十亿参数的模型。我们的方法利用块状局部知识蒸馏（BLD）进行并行架构探索，并采用混合整数规划进行精确的约束优化。我们通过Llama-3.1-Nemotron-51B-Instruct（Nemotron-51B）这一公开可用的模型展示了我们框架的实际影响，该模型由Llama-3.1-70B-Instruct衍生而来。Nemotron-51B实现了2.17倍的推理吞吐量加速，可以在单个NVIDIA H100 GPU上运行，同时保留了原始模型98.4%的能力。Nemotron-51B是目前能够以大批次在单个GPU上进行推理的最准确的语言模型。值得注意的是，这种转变只需要45B个训练令牌，而它所衍生的70B模型则需要超过15T个令牌。这建立了一个新的范式，即强大的模型可以通过仅牺牲微小能力来优化高效的部署，这表明推理性能而非参数数量本身应指导模型选择。随着Nemotron-51B的发布和Puzzle框架的介绍，我们为从业者提供了以显著降低的计算成本访问最先进语言建模能力的即时途径。|\n",
        "2412.02252": "|**2024-12-03**|**Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity**|Da Ma et.al.|[2412.02252](http://arxiv.org/abs/2412.02252)|null|随着大型语言模型（LLMs）如GPT和LLaMA系列中上下文窗口大小的增加，它们处理复杂、长文本任务的能力得到了提升，但代价是推理效率的降低，尤其是在内存和计算复杂度方面。现有方法，包括选择性保留标记和基于窗口的注意力机制，虽然提高了效率，但有可能丢弃未来文本生成所需的重要标记。在本文中，我们提出了一种方法，通过减少不重要标记的内存和计算负载来提高LLM效率，而不丢失标记。我们解决了两个挑战：1）研究上下文中重要标记的分布，发现最近标记比上下文中的远距离标记更重要；2）通过跨层共享注意力分数来优化远距离标记的资源。实验表明，我们的方法在不影响性能的情况下节省了35%的KV缓存。|\n",
        "2412.01447": "|**2024-12-02**|**PLD+: Accelerating LLM inference by leveraging Language Model Artifacts**|Shwetha Somasundaram et.al.|[2412.01447](http://arxiv.org/abs/2412.01447)|null|为了降低自回归语言模型（LLM）推理的延迟，预测解码（speculative decoding）作为一种新的解码范式应运而生，在这种范式中，未来的标记（tokens）被并行地起草和验证。然而，预测解码的实际部署受到其对额外计算资源和微调的需求的限制，这限制了其即插即用的实用性。为了应对这些挑战，我们提出了一种名为PLD+的新算法套件，旨在加速LLM的推理过程，特别是针对输入引导的任务。这些任务包括代码编辑、文本编辑、摘要等，它们的输出往往与输入有大量的重叠，这是PLD+设计时要利用的属性。PLD+还利用推理过程中产生的副产品（注意力机制和隐藏状态）来加速推理速度。我们在五个输入引导任务上测试了我们的方法，并通过广泛的实验发现，PLD+优于所有无需微调的方法。在贪婪设置中，它在四个任务上甚至优于最先进的依赖微调的方法EAGLE（平均加速率提高了2.31）。我们的方法无需微调，不需要任何额外的计算资源，并且可以轻松用于加速任何LLM的推理。|\n",
        "2412.01380": "|**2024-12-02**|**Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking**|Marco Federici et.al.|[2412.01380](http://arxiv.org/abs/2412.01380)|null|随着移动设备提供的计算能力不断增强，DRAM带宽的提升速度却相对较慢。这对大型语言模型（LLM）的token生成来说是个不幸的事，因为其高度依赖于内存。先前的研究提出利用ReLU激活的LLM中的自然动态激活稀疏性来减少每个token的有效DRAM带宽。然而，最新的LLM使用SwiGLU而非ReLU，这导致几乎没有固有的稀疏性。虽然SwiGLU的激活可以根据幅度进行剪枝，但产生的稀疏模式难以预测，使得先前的方法失效。为了解决这个问题，我们的工作引入了动态输入剪枝（DIP）：一种无预测器的动态稀疏化方法，它通过最小的微调来保留准确性。DIP还可以使用轻量级的LoRA适配器来恢复在稀疏化过程中损失的一些性能。最后，我们描述了一种新的缓存感知掩码策略，它考虑缓存状态和激活幅度以进一步提高缓存命中率，从而提高移动设备上LLM的token速率。在DIP中，与模拟硬件设置中的其他方法相比，它在准确性、内存和吞吐量之间的权衡方面表现更优。在Phi-3-Medium上，DIP实现了内存减少46%、吞吐量增加40%，同时困惑度损失小于0.1。|\n",
        "2412.01129": "|**2024-12-02**|**RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy**|Geonho Lee et.al.|[2412.01129](http://arxiv.org/abs/2412.01129)|null|低秩自适应（LoRA）已成为参数高效的LLM微调的主要方法，基于LoRA的量化误差补偿（LQEC）也作为一种强大的工具，用于恢复压缩LLM的准确性。然而，LQEC在4位以下的情况下表现不佳，但此前没有对这一限制进行过研究。我们提出了RILQ（基于低秩自适应的秩无关量化误差补偿）来理解基本限制并提升2位LLM的准确性。基于对模型激活差异损失秩无关性质的分析，RILQ使用这种损失在层间协同调整适配器，使得低秩适配器能够实现鲁棒的误差补偿。在LLaMA-2和LLaMA-3上的评估表明，RILQ在各种最先进的量化器上对2位量化推理的一致性改进，以及在特定任务微调中的准确性提升。RILQ保持了与现有LoRA方法相当的计算效率，使得适配器合并权重量化LLM推理的准确性显著提升，成为提升2位LLM性能的有前途的方法。|\n",
        "2412.01042": "|**2024-12-02**|**TruncFormer: Private LLM Inference Using Only Truncations**|Patrick Yubeaton et.al.|[2412.01042](http://arxiv.org/abs/2412.01042)|null|私有推理（PI）在用户数据与专有机器学习模型（如LLMs）交互时，保证了用户数据的隐私性。然而，由于LLMs中存在的非线性函数带来的巨大延迟成本，PI在实践中变得难以处理。现有工作主要关注通过近似来提高特定LLM非线性（如Softmax或GeLU）的延迟。然而，随着新的LLM架构的引入，新的非线性类型也在不断出现，这导致了PI研究人员在优化最新非线性函数方面的不断追赶。我们引入了TruncFormer，这是一个将任何LLM转换为PI明文仿真的框架。我们的框架利用了LLM中的非线性函数是可微分的，并且可以用一系列加法、乘法和截断来精确近似的事实。此外，我们将加/乘操作与截断操作解耦，并根据给定的字段大小和输入表示大小静态确定应在哪里插入截断。这导致在现有加密协议中，每进行一次乘法操作后都需要截断的情况下，延迟得到了改进。我们开源了我们的代码以供社区使用。|\n",
        "2412.03594": "|**2024-11-29**|**BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching**|Zhen Zheng et.al.|[2412.03594](http://arxiv.org/abs/2412.03594)|null|许多大型语言模型（LLM）的任务在大批量或离线情况下执行，其性能指标为吞吐量。这些任务通常具有前缀共享的特征，即不同的提示输入可以部分显示共同的prefix。然而，现有的LLM推理引擎往往优化流式请求，在支持具有前缀共享特性的大批量任务方面存在局限性。现有解决方案使用基于LRU的缓存来重用共同前缀的KV上下文。即将被重用的KV上下文可能会因隐式缓存管理而被提前移除。即使没有被移除，共享的KV上下文的生命周期也会因为共享相同上下文的请求没有被一起调度而延长，导致更大的内存使用。这些以流为方向的系统按照先来先服务的顺序调度请求。结果，解码步骤比例较大的请求可能调度得太晚，无法与预填充块混合，从而提高硬件利用率。此外，基于令牌和请求数量的批量处理可能会限制令牌批量的大小，这会防止GPU在主要由解码令牌控制的迭代中饱和。我们提出了BatchLLM来解决这个问题。BatchLLM显式地识别全局的共同prefix。具有相同prefix的请求将被一起调度，以最佳方式重用KV上下文，这也有助于缩短共同KV内存的生命周期。BatchLLM重新排序请求，优先调度解码步骤比例较大的请求，以便更好地将解码令牌与后续预填充块混合，并采用以内存为中心的令牌批量处理来扩大令牌批量大小，这有助于提高GPU利用率。广泛的评估表明，在一系列微基准测试和两个典型的行业工作负载上，BatchLLM的性能优于vLLM 1.1倍到2倍。|\n",
        "2412.04788": "|**2024-12-06**|**GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments**|Yanyu Chen et.al.|[2412.04788](http://arxiv.org/abs/2412.04788)|null|将大型语言模型（LLMs）高效地部署到实际场景中仍然是一个关键挑战，这主要归因于硬件异构性、推理框架的限制和工作负载的复杂性。这些挑战通常导致内存利用效率低下、延迟波动以及吞吐量不充分，阻碍了LLMs的有效部署，尤其是对于非专业人士。通过广泛的实验，我们确定了关键的性能瓶颈，包括内存利用率突然下降、随着批量大小变化的延迟波动以及多GPU配置中的效率低下。这些见解揭示了一个由硬件、框架和工作负载参数的复杂相互作用所塑造的巨大优化空间。这强调了系统性地优化LLM推理的必要性，从而推动了我们的框架GUIDE的设计。GUIDE利用动态建模和基于模拟的优化来解决这个问题，实现了关键指标（如批量延迟、TTFT和解码吞吐量）的预测误差在25%到55%之间。通过有效地弥合理论性能与实际部署之间的差距，我们的框架使实践者，特别是非专业人士，能够做出数据驱动的决策，并以低成本释放LLMs在异构环境中的全部潜力。|\n",
        "2412.04504": "|**2024-12-03**|**Multi-Bin Batching for Increasing LLM Inference Throughput**|Ozgur Guldogan et.al.|[2412.04504](http://arxiv.org/abs/2412.04504)|null|随着大型语言模型（LLM）因其多样化的功能而越来越受欢迎，提高其推理系统的效率变得越来越关键。在服务器（例如GPU）上调度推理作业时，批处理LLM请求是一个关键步骤，这使系统能够通过允许多个请求并行处理来最大化吞吐量。然而，请求往往具有不同的生成长度，导致资源利用率低下，因为硬件必须在批处理中等待运行时间最长的请求完成后才能转到下一个批次。我们从排队论的角度正式化这个问题，并旨在设计一个吞吐量最优的控制策略。我们提出了多箱批处理（Multi-Bin Batching），这是一种简单而有效的方法，可以将具有相似（预测的）执行时间的请求分组到预定的箱中，从而可以证明地提高LLM推理吞吐量。通过理论分析和实验的结合，包括现实世界的LLM推理场景，我们证明了与标准批处理方法相比，显著提高了吞吐量。|\n",
        "2412.06198": "|**2024-12-09**|**SparseAccelerate: Efficient Long-Context Inference for Mid-Range GPUs**|James Vo et.al.|[2412.06198](http://arxiv.org/abs/2412.06198)|null|随着大型语言模型（LLMs）扩展到更长的上下文窗口，传统的注意力机制的计算成本随输入长度的平方增长，这为实时和内存受限的部署带来了关键挑战。现有的稀疏注意力技术试图降低这种复杂性，但它们通常会产生显著的开销或降低准确性，这使得它们在中等硬件上对大上下文来说不太实用。在本文中，我们介绍了SparseAccelerate，这是一种动态稀疏注意力方法，它根据输入特征调整其稀疏模式，有效地平坦化了注意力复杂度曲线。我们的方法对于输入长度从16K个标记开始就非常有效，并在双NVIDIA A5000 GPU（每个24GB）上高效扩展到128K个标记。实验结果表明，SparseAccelerate在32K个标记时，将首次标记延迟（TTFT）降低了高达1.04倍，同时提供了显著的内存节省。这些改进为内存密集型应用和长上下文任务带来了实际效益，这些任务在标准注意力下之前是不可行的。除了延迟降低之外，SparseAccelerate在竞争方法中相对于上下文长度的TTFT增长梯度最小，从而根本改变了扩展趋势。在多种基准测试上的持续评估证实了其可扩展性，将SparseAccelerate定位为在可访问硬件上实现高效、实时和长上下文LLM推理的关键进步。|\n",
        "2412.05896": "|**2024-12-08**|**XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference**|Weizhuo Li et.al.|[2412.05896](http://arxiv.org/abs/2412.05896)|null|最近，生成式大型语言模型（LLM）在众多应用中取得了显著的成功。值得注意的是，其推理过程是逐个生成输出标记，导致许多冗余计算。广泛使用的KV-Cache框架在时间和空间复杂度之间做出了权衡。然而，缓存数据会导致内存需求不断增长，这可能会迅速耗尽现代加速器（如GPU）有限的内存容量，尤其是在长上下文推理任务中。现有研究通过淘汰对推理精度影响较小的部分缓存数据来减少内存消耗。但由于LLM网络层之间静态的缓存分配，实际效果远非理想。本文观察到，特定层的缓存数据对准确度的影响非常不同。我们量化了这种差异，并提供了实验和理论验证。据此，我们进行了形式化分析，表明以个性化方式为每个层定制缓存大小可以显著减少内存消耗，同时仍然提供可比的准确度。我们将缓存分配模拟为一个组合优化问题，并给出全局最优解。特别是，我们设计了一个基于轻量级LLM模型的小型采样推理，以便快速捕捉差异，并将其输入到个性化算法中。在真实世界数据集上的大量实验表明，我们的建议可以将KV缓存内存消耗平均降低61.6%，计算效率提高2.1倍，吞吐量提高高达5.5倍。|\n",
        "2412.07017": "|**2024-12-09**|**Asynchronous LLM Function Calling**|In Gim et.al.|[2412.07017](http://arxiv.org/abs/2412.07017)|null|大型语言模型（LLMs）通过函数调用与外部工具和数据源进行交互。然而，当前LLM函数调用的方法本质上具有同步性，每次调用都会阻塞LLM推理，限制了LLM的操作和并发函数执行。在本研究中，我们提出了AsyncLM，这是一个用于异步LLM函数调用的系统。AsyncLM通过允许LLMs并发生成和执行函数调用，提高了LLM的操作效率。AsyncLM引入了一个中断机制，在函数调用返回时异步通知正在进行的LLM，而不是等待每个调用完成。我们为函数调用和中断设计了上下文协议，提供了微调策略以适应中断语义，并在LLM推理过程中高效地实现了这些机制。我们证明了AsyncLM可以将端到端任务完成延迟从1.6倍到5.4倍降低，这是在伯克利函数调用排行榜（BFCL）上的一系列基准任务中的结果。此外，我们还讨论了如何扩展中断机制以实现新颖的人机LLM或LLM-LLM交互。|\n",
        "2412.08585": "|**2024-12-11**|**TurboAttention: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大型语言模型（LLM）推理需要大量的计算和内存，尤其是在关键的关注机制上。虽然量化技术和加速算法，如FlashAttention，已经提高了整体推理的效率，但它们针对问题的不同方面：量化关注于权重-激活操作，而FlashAttention虽然提高了执行效率，但需要高精度格式。最近的关键值（KV）缓存量化减少了内存带宽，但仍然需要在关注操作中执行浮点数反量化。我们提出了TurboAttention，这是一种实现关注量化执行的综合方法，同时解决了内存和计算效率的问题。我们的解决方案引入了两项关键创新：FlashQ，一种头部关注量化技术，它既能够压缩KV缓存，又能够实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它消除了在关注操作中指数运算期间进行FP32反量化的需求。实验结果表明，TurboAttention在关注操作上实现了1.2-1.8倍的速度提升，将KV缓存大小减少了超过4.4倍，并且相对于FP16基线，实现了高达2.37倍的最大吞吐量，同时在各种数据集和模型上优于现有的量化和压缩技术。|\n",
        "2412.08281": "|**2024-12-11**|**Lachesis: Predicting LLM Inference Accuracy using Structural Properties of Reasoning Paths**|Naryeong Kim et.al.|[2412.08281](http://arxiv.org/abs/2412.08281)|null|大型语言模型越来越多地被用于构建执行更复杂任务的智能体。随着LLM通过更长时间的交互进行更复杂的推理，自洽性，即从多个独立推理的样本中进行采样和边缘化所获得的答案更有可能是正确的这一观点，作为一种简单的验证技术，已经引起了广泛关注。本文旨在通过预测从推理路径样本的性质中获得的自洽性答案的正确性来实证验证这一直观假设。我们引入了Lachesis，这是一个基于自洽性的LLM推理的预测模型，并使用AutoFL（一种最近提出的基于LLM的错误定位技术）作为目标技术进行实证评估。Lachesis使用专门设计的推理路径表示将AutoFL收集的推理路径进行转换，并训练LSTM和GCN模型来预测一组给定的推理路径是否会导致正确的答案。结果表明，Lachesis可以以高达0.8136的精度预测答案的正确性，突显了训练一个能够允许提前终止不太可能成功的推理的预测模型的可行性。|\n",
        "2412.08237": "|**2024-12-11**|**TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch**|Xingchen Song et.al.|[2412.08237](http://arxiv.org/abs/2412.08237)|null|众所周知，基于LLM的系统对数据需求量大。近年来，基于LLM的TTS工作通常采用复杂的数据处理管道来获取高质量的训练数据。这些复杂的管道在每个阶段（例如，语音降噪、语音增强、说话人分割和标点模型）都需要优秀的模型，而这些模型本身又需要高质量的训练数据，并且很少开源。即使使用最先进的模型，仍然存在一些问题，如背景噪声去除不完整和标点与实际语音停顿不匹配。此外，严格的过滤策略通常只保留原始数据的10-30%，这极大地阻碍了数据扩展的努力。在这项工作中，我们利用一个噪声鲁棒的音频分词器（S3Tokenizer）设计了一个简化但有效的TTS数据处理管道，在保持数据质量的同时，大幅降低了数据获取成本，实现了超过50%的数据保留率。除了数据扩展的挑战之外，基于LLM的TTS系统与传统的方案相比，部署成本也更高。当前的系统通常仅使用LLM进行文本到标记的生成，而需要单独的模型（例如，流匹配模型）进行标记到波形生成，这些模型不能直接由LLM推理引擎执行，从而进一步复杂化了部署。为了解决这些挑战，我们消除了LLM和流组件中的冗余模块，用LLM架构替换了流模型主干。在此基础上，我们提出了一种统一的架构，用于流和非流推理，显著降低了部署成本。最后，我们探讨了使用相同数据进行TTS和ASR任务训练的可行性，这得益于简化的管道和S3Tokenizer，它降低了TTS训练数据的质量要求。|\n",
        "2412.10319": "|**2024-12-13**|**SCBench: A KV Cache-Centric Analysis of Long-Context Methods**|Yucheng Li et.al.|[2412.10319](http://arxiv.org/abs/2412.10319)|null|长上下文LLM（大型语言模型）虽然促进了众多下游应用，但也带来了与计算和内存效率相关的重大挑战。为了解决这些挑战，围绕KV缓存的长上下文推理优化已经得到发展。然而，现有的基准测试通常只评估单次请求，忽略了KV缓存在实际应用中的完整生命周期。这种疏忽尤为关键，因为KV缓存重用已成为LLM推理框架（如vLLM和SGLang）以及OpenAI、Microsoft、Google和Anthropic等LLM提供商广泛采用的策略。为了填补这一空白，我们引入了SCBench（共享上下文基准），这是一个从KV缓存中心视角全面评估长上下文方法的基准：1）KV缓存生成，2）KV缓存压缩，3）KV缓存检索，4）KV缓存加载。具体来说，SCBench使用具有共享上下文的测试示例，包括12个任务和两种共享上下文模式，涵盖四种长上下文能力类别：字符串检索、语义检索、全局信息和多任务。凭借它，我们提供了对包括门控线性RNN、Mamba-Attention混合体以及高效的稀疏注意力、KV缓存丢弃、量化、检索、加载和提示压缩在内的八类长上下文解决方案的广泛分析。评估在8个长上下文LLM上进行。我们的发现表明，内存小于O(n)的方法在多轮场景中表现不佳，而具有O(n)内存和小于O(n^2)预填充计算的稀疏编码表现稳健。动态稀疏性比静态模式产生更具表现力的KV缓存，而混合架构中的层级稀疏性在保持强大性能的同时减少了内存使用。此外，我们还识别出在长生成场景中注意力分布偏移的问题。https://aka.ms/SCBench。|\n",
        "2412.11741": "|**2024-12-16**|**CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation**|Hongxuan Zhang et.al.|[2412.11741](http://arxiv.org/abs/2412.11741)|null|随着利用大型语言模型（LLMs）的长文本应用的出现，带来了显著的扩展性挑战，尤其是在内存占用方面。负责存储注意力键和值以减少冗余计算的关键值（KV）缓存的线性增长可能会导致内存消耗的显著增加，这可能导致模型在有限的内存资源下无法正常服务。为了解决这个问题，我们提出了一种名为缓存稀疏表示（CSR）的新方法，该方法通过将密集的键值缓存张量转换为稀疏索引和权重，在LLM推理过程中提供更高效的内存表示。此外，我们引入了一种名为NeuralDict的新方法，这是一种基于神经网络的自动生成用于我们稀疏表示的字典的方法。我们广泛的实验表明，CSR在性能上与最先进的KV缓存量化算法相当，同时在内存受限环境中保持稳健的功能。|\n",
        "2412.11120": "|**2024-12-15**|**Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning**|Yun Qu et.al.|[2412.11120](http://arxiv.org/abs/2412.11120)|**[link](https://github.com/cloud-qu/lare)**|**强化学习（RL）在实际应用中常常面临延迟和稀疏的反馈，即使在只有阶段性奖励的情况下。先前的方法在奖励重新分配方面取得了一些进展，但仍面临挑战，包括由于冗余和忽略任务绩效评估的多面性而导致的模糊归因所引起的训练困难。希望大型语言模型（LLM）包含了丰富的决策知识，并为奖励重新分配提供了一种合理的工具。尽管如此，由于语言知识与符号形式要求之间的不匹配，以及推理中固有的随机性和幻觉，将LLM应用于此非同小可。为了解决这些问题，我们引入了LaRe，这是一种新型LLM赋能的基于符号的决策框架，以提高信用分配。LaRe的关键是潜在奖励的概念，它作为多维度的性能评估，能够从不同角度实现更可解释的目标达成，并促进更有效的奖励重新分配。我们考察了由LLM生成的语义代码如何将语言知识与符号潜在奖励联系起来，因为它可以用于符号对象。同时，我们设计了潜在奖励的自验证，以提高LLM推理的稳定性和可靠性。从理论上讲，潜在奖励中的与奖励无关的冗余消除有助于从更准确的奖励估计中提高RL性能。广泛的实验结果表明，LaRe（i）在时间信用分配方面优于SOTA方法，（ii）在分配多个代理之间的贡献方面表现卓越，（iii）在特定任务中优于使用真实奖励训练的策略。**|\n",
        "2412.11053": "|**2024-12-15**|**NITRO: LLM Inference on Intel Laptop NPUs**|Anthony Fei et.al.|[2412.11053](http://arxiv.org/abs/2412.11053)|**[link](https://github.com/abdelfattah-lab/nitro)**|**大型语言模型（LLMs）已成为自然语言处理领域的关键工具，在ChatGPT和Gemini等聊天机器人中得到了广泛应用，并成为研究的一个核心领域。一个特别感兴趣的研究方向包括为这些AI应用设计专门的硬件，其中之一就是神经处理单元（NPU）。2023年，英特尔发布了代号为Meteor Lake的Intel Core Ultra处理器，它集成了CPU、GPU和NPU系统芯片。然而，通过英特尔OpenVINO框架对NPU的官方软件支持仅限于静态模型推理。因此，LLMs中自回归标记生成的动态特性无法直接支持。为了解决这一不足，我们提出了NITRO（NPU推理优化器），这是一个基于OpenVINO构建的Python框架，用于在NPU上支持文本和聊天生成。在本文中，我们详细讨论了对Transformer架构所做的关键修改以实现推理、一些性能基准测试以及改进该软件包的下一步计划。NITRO的代码库可以在这里找到：https://github.com/abdelfattah-lab/nitro。**|\n",
        "2412.12687": "|**2024-12-17**|**Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models**|Seungeun Oh et.al.|[2412.12687](http://arxiv.org/abs/2412.12687)|null|本文研究了一种混合语言模型（HLM）架构，该架构将运行在移动设备上的小型语言模型（SLM）与部署在无线网络基站（BS）的大型语言模型（LLM）相结合。HLM令牌生成过程遵循推测性推理原则：SLM的词汇分布被上传到LLM，LLM接受或拒绝它，被拒绝的令牌由LLM重新采样。虽然这种方法确保了SLM和LLM的词汇分布对齐，但由于上行传输和运行两个语言模型的计算成本，它 suffer from low token throughput。为了解决这个问题，我们提出了一种新的HLM结构，称为感知不确定性的机会HLM（U-HLM），其中SLM本地测量其输出不确定性，并跳过可能被接受的令牌的上行传输和LLM操作。这种机会性跳过是通过我们关于SLM的不确定性与LLM的拒绝概率之间呈线性相关的经验发现实现的。我们推导出不确定性阈值，并评估其拒绝的预期风险。仿真结果表明，U-HLM将上行传输和LLM计算减少了45.93%，同时实现了高达97.54%的LLM推理准确率，比无跳过的HLM快2.54倍的令牌吞吐量。|\n",
        "2412.12488": "|**2024-12-17**|**A System for Microserving of LLMs**|Hongyi Jin et.al.|[2412.12488](http://arxiv.org/abs/2412.12488)|null|最近在大型语言模型（LLM）方面的进步，对高效系统支持的需求日益强烈，以提升整体服务效率。随着LLM推理扩展到多个GPU甚至多个计算节点，服务系统中出现了各种协调模式，如预填充-解码解耦和上下文迁移。目前大多数推理服务都暴露了具有预先配置协调策略的粗粒度请求级API，这限制了自定义和动态重新配置协调的能力。在本文中，我们提出了LLM微服务，这是一种用于构建和编程LLM推理服务的多层次架构。我们引入了简单而有效的微服务API来支持细粒度的子请求级操作。一个可编程的路由器将用户请求转换为子请求调用，使服务模式能够动态重新配置。为了支持多样化的执行模式，我们开发了一个统一的KV缓存接口，用于处理各种KV计算、传输和重用场景。我们的评估表明，LLM微服务可以通过几行Python代码重新配置，以支持多种解耦编排策略，同时在LLM推理任务中保持最先进的性能。此外，它还使我们能够探索新的策略变体，与现有策略相比，这些变体可以将作业完成时间缩短高达47%。|\n",
        "2412.14352": "|**2024-12-18**|**A Survey on LLM Inference-Time Self-Improvement**|Xiangjue Dong et.al.|[2412.14352](http://arxiv.org/abs/2412.14352)|**[link](https://github.com/dongxiangjue/Awesome-LLM-Self-Improvement)**|**近期，通过增加测试时的计算来增强推理的技术受到了关注。在这篇综述中，我们从三个不同的角度研究LLM推理时自我改进的现状：独立自我改进，侧重于通过解码或采样方法进行增强；上下文感知自我改进，利用额外的上下文或数据存储；以及模型辅助自我改进，通过模型协作实现改进。我们提供了对近期相关研究的全面回顾，贡献了一个深入的分类法，并讨论了挑战和局限性，为未来的研究提供了洞见。**|\n",
        "2412.15803": "|**2024-12-20**|**WebLLM: A High-Performance In-Browser LLM Inference Engine**|Charlie F. Ruan et.al.|[2412.15803](http://arxiv.org/abs/2412.15803)|**[link](https://github.com/mlc-ai/web-llm)**|**大型语言模型（LLMs）的进步解锁了惊人的能力。虽然部署这些模型通常需要服务器级GPU和基于云的推理，但近期小型开源模型和日益强大的消费设备的出现使得设备端部署变得可行。网页浏览器作为设备端部署的平台，具有普遍的易用性，提供了一个自然的代理环境，并方便地将不同设备供应商的后端抽象出来。为了抓住这一机遇，我们介绍WebLLM，这是一个开源的JavaScript框架，它使得LLMs的高性能推理完全在网页浏览器内进行。WebLLM提供了类似OpenAI风格的API，以便无缝集成到网络应用中，并利用WebGPU进行高效的本地GPU加速以及WebAssembly进行高效的CPU计算。通过机器学习编译器MLC-LLM和Apache TVM，WebLLM利用优化的WebGPU内核，克服了缺少高性能WebGPU内核库的问题。评估结果显示，WebLLM在同一设备上可以保持高达80%的原生性能，并有望进一步缩小差距。WebLLM为网页浏览器中的通用、隐私保护、个性化以及本地驱动的LLMs应用铺平了道路。代码可在以下链接获取：https://github.com/mlc-ai/web-llm。**|\n",
        "2412.16434": "|**2024-12-21**|**SYMPHONY: Improving Memory Management for LLM Inference Workloads**|Saurabh Agarwal et.al.|[2412.16434](http://arxiv.org/abs/2412.16434)|null|大型语言模型（LLMs）越来越多地被应用于聊天机器人、代码编辑器和对话代理等应用中。LLMs的一个关键特性是它们能够与人类或外部工具进行多轮交互，从而实现各种任务。在多轮交互中，每个新请求都依赖于之前请求的中间状态，特别是当前交互中的键值（K，V）缓存。现有的服务引擎要么重新计算K，V缓存，要么将它们卸载到主内存。分析发现，重新计算会导致超过99%的处理令牌是冗余的。另一方面，将K，V缓存从GPU内存卸载会导致推理服务状态化，从而导致集群负载不平衡。为了解决这些挑战，我们开发了SYMPHONY。SYMPHONY利用了多轮工作负载提供额外提示的观察结果，这些提示允许将K，V缓存从关键服务路径迁移出去。通过利用这些提示，SYMPHONY动态迁移K，V缓存，以实现推理请求的细粒度调度。我们的实验表明，与最先进的基线相比，SYMPHONY可以处理超过8倍数量的请求，并且具有相似的延迟特征。|\n",
        "2412.19442": "|**2024-12-27**|**A Survey on Large Language Model Acceleration based on KV Cache Management**|Haoyang Li et.al.|[2412.19442](http://arxiv.org/abs/2412.19442)|**[link](https://github.com/treeai-lab/awesome-kv-cache-management)**|**大型语言模型（LLMs）由于其理解和执行逻辑推理的能力，已经在自然语言处理、计算机视觉和多模态任务等众多领域引发了革命。然而，LLMs在推理过程中的计算和内存需求，尤其是在扩展到实际应用、长上下文和实时应用时，带来了巨大的挑战。键值（KV）缓存管理作为一种关键优化技术，通过减少冗余计算和提高内存利用率，已被证明是加速LLM推理的有效方法。本综述对用于LLM加速的KV缓存管理策略进行了全面概述，将其分为token级、模型级和系统级优化。token级策略包括KV缓存选择、预算分配、合并、量化和低秩分解，而模型级优化则关注于架构创新和注意力机制以增强KV重用。系统级方法则针对内存管理、调度和硬件感知设计，以提高不同计算环境下的效率。此外，综述还概述了用于评估这些策略的文本和多模态数据集和基准。通过提供详细的分类和比较分析，本研究旨在为研究人员和实践者提供有价值的见解，以支持高效和可扩展的KV缓存管理技术的发展，有助于LLMs在实际应用中的实际部署。KV缓存管理的精选论文列表可在以下链接找到：\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}。**|\n",
        "2412.19394": "|**2024-12-27**|**An Engorgio Prompt Makes Large Language Model Babble on**|Jianshuo Dong et.al.|[2412.19394](http://arxiv.org/abs/2412.19394)|**[link](https://github.com/jianshuod/engorgio-prompt)**|**自回归大型语言模型（LLMs）在许多实际任务中取得了令人瞩目的性能。然而，这些LLMs的新范式也暴露出新的威胁。在本文中，我们探讨了它们对推断成本攻击的脆弱性，恶意用户通过精心设计的Engorgio提示故意增加推断过程的计算成本和延迟。我们设计了Engorgio，一种新颖的方法，以高效生成对抗性Engorgio提示，影响目标LLM的服务可用性。Engorgio有两个技术贡献：（1）我们采用参数化分布来跟踪LLMs的预测轨迹；（2）针对LLMs推断过程的自回归特性，我们提出了新的损失函数，以稳定地抑制<EOS>标记的出现，其出现将会中断LLMs的生成过程。我们在13个参数范围从125M到30B的开源LLMs上进行了广泛的实验。结果表明，在白盒场景中，Engorgio提示可以成功诱导LLMs生成异常长的输出（即达到输出长度限制的90%+需要大约2-13倍更长的时间），我们的真实世界实验证明了Engorgio对有限计算资源的LLM服务的威胁。代码可在https://github.com/jianshuod/Engorgio-prompt上获取。**|\n",
        "2412.18934": "|**2024-12-25**|**Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference**|Libo Zhang et.al.|[2412.18934](http://arxiv.org/abs/2412.18934)|null|由于大型语言模型（LLMs）对资源的高需求，在消费级设备上实现广泛部署面临着重大挑战。通常，个人或消费级设备，包括在大型模型时代之前配置的服务器，通常拥有相对较弱的GPU和相对较强的CPU。然而，大多数当前方法主要依赖于GPU进行计算。因此，我们提出了Dovetail方法，该方法在GPU上部署草稿模型以生成草稿标记，同时允许目标模型在CPU上并行验证，从而提高所有可用硬件资源的利用率，并减少设备间通信带宽。相应地，我们对草稿模型进行了重新设计，以更好地适应异构硬件特性。为此，我们实现了几个优化：减少草稿标记的数量以减轻并行验证的延迟，增加草稿模型的深度以增强其预测能力，并引入DGF（动态门控融合）以改进特征和标记嵌入的集成。在HumanEval基准测试中，Dovetail使用3GB VRAM对LLaMA2-Chat-7B实现了每秒5.86个标记的推理速度，这比仅CPU推理提高了约2.77倍。此外，当使用7GB VRAM时，推理速度提高到了每秒8个标记。|\n",
        "2412.20501": "|**2024-12-29**|**TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication**|Zongwu Wang et.al.|[2412.20501](http://arxiv.org/abs/2412.20501)|null|高效并行化具有长序列的大型语言模型（LLMs）至关重要，但这也极具挑战，因为它们对计算和内存的需求很大，尤其是由于注意力机制中的通信瓶颈。虽然序列并行（SP）已被引入作为潜在的解决方案，但现有方法往往受限于可扩展性或效率不足，导致其有效性受限。环状注意力（Ring-Attention）展示了扩展序列处理的潜力，但由于其依赖对等（P2P）通信以及网络资源利用效率低下，面临着重大限制。随着SP程度的增加，每一步计算时间的二次减少与通信量的线性减少形成鲜明对比，加剧了通信瓶颈。为了应对这些挑战，我们提出了TokenRing，这是一种细粒度并行框架，利用双向P2P通信有效地重叠计算和数据传输。通过将注意力块分区，并在全连接网状拓扑结构中同时传输查询和块输出（即$block\\_out$和$block\\_lse$），TokenRing实现了显著减少通信开销和更好的负载均衡。这些创新提高了分布式Transformer模型的可扩展性和效率，特别是对于长上下文序列。实验结果表明，TokenRing提高了吞吐量并降低了通信延迟。此外，其设计可以无缝适应各种多GPU互连解决方案，如华为Ascend，确保了分布式LLM推理和训练的广泛兼容性和成本效益。代码可在以下链接找到：\\url{https://github.com/ACA-Lab-SJTU/token-ring}。|\n",
        "2412.20166": "|**2024-12-28**|**LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System**|Hyucksung Kwon et.al.|[2412.20166](http://arxiv.org/abs/2412.20166)|null|大型语言模型（LLMs）的参数量达到数百亿级别，对计算资源提出了重大挑战，尤其是数据移动和内存带宽。长上下文LLMs处理数万个标记的序列，进一步增加了对内存系统的需求，因为注意力层和键值缓存的大小与上下文长度成正比。内存中处理（PIM）通过将计算移动到数据上来最大化内存带宽，可以解决内存带宽问题；然而，由于每个模块的内存容量有限，以及固定功能单元PIM架构和静态内存管理的灵活性不足，PIM并不一定能够扩展以加速长上下文LLM。在本工作中，我们提出了LoL-PIM，这是一种多节点PIM架构，通过硬件-软件协同设计来加速长上下文LLM。具体而言，我们提出了如何在多PIM模块之间利用流水线并行性，同时提出了直接PIM访问（DPA）控制器（或PIM的DMA），它能够实现动态PIM内存管理，并在各种上下文长度上实现高效的PIM利用率。我们开发了一个基于MLIR的编译器，用于LoL-PIM，扩展了一个商业PIM编译器，其中软件修改已实施并评估，而硬件更改在模拟器中进行了建模。我们的评估表明，LoL-PIM显著提高了长上下文LLM推理的吞吐量和降低了延迟，优于多GPU和GPU-PIM系统（分别达到8.54倍和16.0倍的速度提升），从而使得LLM在现实世界中的应用部署更加高效。|\n",
        "2412.19829": "|**2024-12-19**|**GFormer: Accelerating Large Language Models with Optimized Transformers on Gaudi Processors**|Chengming Zhang et.al.|[2412.19829](http://arxiv.org/abs/2412.19829)|null|异构硬件如Gaudi处理器已被开发出来以增强计算能力，特别是针对Transformer型大型语言模型（LLMs）的生成式人工智能任务中的矩阵运算。然而，我们的分析表明，Transformer在这些新兴硬件上并未得到充分优化，主要是由于非矩阵计算内核（如Softmax）的优化不足以及异构资源利用不充分，尤其是在处理长序列时。为了解决这些问题，我们提出了一种综合方法（称为GFormer），该方法将稀疏和线性注意力机制合并。GFormer旨在最大化Gaudi处理器的矩阵乘法引擎（MME）和张量处理核心（TPC）的计算能力，同时不牺牲模型质量。GFormer包括一个窗口自注意力内核和一个高效的因果线性注意力外积内核，旨在优化Gaudi处理器上的LLM推理。评估结果表明，GFormer在Gaudi处理器上显著提高了效率和模型性能，并优于最先进的GPU。|\n"
    },
    "train": {
        "2411.17691": "|**2024-11-26**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们通过观察发现，在应用低比特量化时，规模较大或训练词数较少的语言模型（LLMs）的量化诱导退化（QiD）较小，而训练词数较多的小型模型则会遭受显著的QiD。为了更深入地了解这一趋势，我们在一个受控环境下研究了1500多个不同规模和训练水平（未训练或完全训练）的量化LLMs检查点，从而推导出QiD与训练词数、模型大小和比特宽度等因素之间的关系定律。利用这些定律，我们提出了一种新视角：可以使用QiD来衡量LLMs的训练水平，并确定不同规模LLMs完全训练所需的训练词数。此外，我们使用这些定律来预测使用100万亿个训练词训练的不同规模LLMs的量化性能。我们的预测表明，未来预期使用超过100万亿个训练词训练的低比特量化模型性能可能并不理想。这为未来的低比特量化提出了潜在的挑战，并突出了在评估低比特量化研究时需要关注模型训练水平。为了促进对此问题的未来研究，我们将本次工作中使用的1500多个量化检查点发布在https://huggingface.co/Xu-Ouyang上。|\n",
        "2411.17679": "|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|**将文本分割成标记的编码技术，如字节对编码（BPE）和字节级BPE（BBPE），通过将文本分割成标记，显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割通常掩盖了标记内部的字符结构和序列，导致模型在训练期间无法完全学习这些复杂的细节。因此，LLMs在理解标记内部的字符组成和位置关系方面存在困难，尤其是在使用有限数据的下游任务中进行微调时。在本文中，我们引入了一种名为标记内部位置感知（TIPA）的新方法，通过使用分词器自己的词汇进行反向字符预测任务来训练模型，从而增强LLMs对内部标记结构的理解。这种方法使模型能够有效地学习和泛化字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测标记级别的字符位置方面优于基线模型。此外，当应用于中文拼写纠正（CSC）的下游任务时，TIPA不仅加速了模型收敛，而且显著提高了任务性能。**|\n",
        "2411.17284": "|**2024-11-26**|**Using Large Language Models for Expert Prior Elicitation in Predictive Modelling**|Alexander Capstick et.al.|[2411.17284](http://arxiv.org/abs/2411.17284)|**[link](https://github.com/alexcapstick/llm-elicited-priors)**|**大型语言模型（LLMs）通过训练不同领域的数据，有效地获取了广泛的信息。然而，它们的计算复杂度、成本和缺乏透明度阻碍了它们在特定任务中的直接应用。在临床研究等领域的预测模型中，获取专家注释或先验知识通常是昂贵且耗时的。本研究提出使用LLMs来获取预测模型的专家先验分布。这种方法也为情境学习提供了一种替代方案，其中语言模型直接负责做出预测。我们比较了LLM获取的先验和无关先验，评估了LLM是否真实地生成了参数分布，并提出了情境学习和先验获取的模型选择策略。我们的研究发现，与无关先验相比，在数据量少的情况下，LLM获取的先验参数分布显著降低了预测误差。应用于临床问题，这意味着所需的生物样本更少，降低了成本和资源。先验获取也始终优于情境学习，且成本更低，因此在我们的环境中成为一种更受欢迎的替代方案。我们展示了该方法在各种用例中的实用性，包括临床应用。在感染预测中，使用LLM获取的先验，在研究中提前200天，以相同的准确度减少了55%所需的标签数量。**|\n",
        "2411.17116": "|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，基于Transformer的大型语言模型（LLM）在长序列上的推理既昂贵又缓慢。我们引入了星形注意力，这是一种两阶段块稀疏逼近，通过在多个主机之间分散注意力来提高计算效率，同时最小化通信开销。在第一阶段，通过主机间的块局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星形注意力可以无缝集成到大多数使用全局注意力训练的基于Transformer的LLM中，通过减少内存需求和提高推理速度至多11倍，同时保持95-100%的准确率。**|\n",
        "2411.16353": "|**2024-11-25**|**The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C**|Mikita Balesni et.al.|[2411.16353](http://arxiv.org/abs/2411.16353)|null|在论文摘要中，作者首先指出大型语言模型（LLMs）在采用思维链（CoT）进行多跳推理（如“Imagine表演者的配偶是谁？”）时表现出色，但在被迫进行内部推理（不使用CoT）时则表现不佳。作者提到之前关于这一差距大小和性质的研究产生了不一致的证据。在这篇论文中，作者介绍了一种控制环境，用于研究LLMs中的两跳推理，其中超出随机水平的性能构成了潜在推理不可否认的证据。作者在虚构事实上进行微调LLMs（包括Llama 3 8B Instruct和GPT-4o），并确认它们可以推广到使用CoT回答有关它们的两跳问题。作者发现，当事实在训练过程中或提示中一起出现时，模型可以进行潜在推理。然而，出乎意料的是，当学习的事实仅出现在不同的文档中时，模型在两跳推理上完全失败，达到了随机水平准确性和测试损失。作者将这种完全无法组合单独学习的事实称为“两跳诅咒”。此外，作者在真实事实上评估了9个前沿LLMs，发现模型在超过一半的问题类别上完全无法进行无CoT的两跳推理，而在大多数类别上仍然部分成功使用CoT。这些结果表明，LLMs缺乏一种独立于问题类型的一般能力来进行潜在的跨跳推理。|\n",
        "2411.15871": "|**2024-11-24**|**Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution**|Haiquan Wang et.al.|[2411.15871](http://arxiv.org/abs/2411.15871)|null|随着大型语言模型（LLMs）的发展，大规模分布式训练变得必要。然而，高度优化的框架由于通信量大，在模型FLOPS利用率上仍然有显著的损失（通常低于50%）。同时，我们的全面分析显示，计算和通信密集型操作的重叠性很好。本文介绍了一种名为DHelix的新型微观结构，它受到DNA结构的启发，显著提高了LLM训练的效率。DHelix设计的核心是链式交错（SI），它将训练微批次连续流通过GPU视为两条链。DHelix并置两条链的前向和后向传递，并通过对称调度来自相对链的操作进行系统优化，这得益于操作级别的重叠分析结果和基于动态规划的搜索算法。同时，DHelix允许两条链共享模型状态和激活数据的空间，有效地容纳了额外内存空间低于3%的两个微批次。DHelix无缝集成到所有现有数据/模型并行形式，其中最具有挑战性的是管道并行，得益于其独特的模型折叠设计，形成了W型管道。我们使用流行的Llama和GPT密集模型，以及Phi混合专家（MoE）模型，在3个GPU集群（A40、A800和H100）上评估了DHelix的训练。结果显示，它在64-A40和64-A800集群上分别实现了12-40%（最高达到58%MFU）和2-29%（最高达到71%MFU）的提高，显著优于现有方法。在H100集群上，虽然更快的网络降低了DHelix的利润空间，但它使得跨节点张量并行成为可能，这在由于通信成本而目前难以实现的情况下是一种实践。|\n",
        "2411.15484": "|**2024-11-23**|**Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai**|Parinthapat Pengpun et.al.|[2411.15484](http://arxiv.org/abs/2411.15484)|**[link](https://github.com/parinzee/seed-free-synthetic-instruct)**|**我们提出了一种针对低资源语言（特别是泰语）的大语言模型（LLM）指令微调的合成数据方法，以数据高效的方式实现。我们确定了三个有助于指令微调数据集有效性的关键属性：流畅性、多样性和文化背景。我们提出了一种无需种子数据框架，用于生成包含这些基本属性的合成指令微调数据。我们的框架使用LLM生成多样化主题，从维基百科中检索相关上下文，并为各种任务（如问答、摘要和对话）创建指令。实验结果表明，我们的最佳表现合成数据集，结合了这三个关键属性，在仅使用5,000条指令的情况下，与在数万条指令上训练的顶尖泰语LLM相比，实现了具有竞争力的性能。我们的代码和数据集可在https://github.com/parinzee/seed-free-synthetic-instruct上公开获取。**|\n",
        "2411.14500": "|**2024-11-21**|**Exploring Accuracy-Fairness Trade-off in Large Language Models**|Qingquan Zhang et.al.|[2411.14500](http://arxiv.org/abs/2411.14500)|null|大型语言模型（LLMs）在人工智能领域取得了显著进展，展示了它们与人类互动和通过信息传播影响人类认知的能力。然而，最近的研究揭示了这些LLMs内含的偏见问题，这成为一个需要关注的重大问题。在我们的研究中，我们深入研究在LLMs增强过程中，如何协调准确性和公平性的复杂挑战。虽然提高准确性确实可以提升LLMs的整体性能，但这往往是以牺牲公平性为代价的。过度强调一个指标的优化必然会导致另一个指标的显著下降。这强调了在设计优化LLMs阶段时考虑多个因素的重要性。因此，我们主张将LLMs的训练过程重新定义为多目标学习任务。我们的研究揭示了多目标进化学习（MOEL）方法在应对这一挑战方面具有前景。我们的MOEL框架能够同时优化准确性和公平性指标，从而产生一组帕累托最优的LLMs。总之，我们的研究为LLMs中准确性和公平性之间的微妙平衡提供了宝贵的见解，这对于它们在现实世界中的应用越来越重要。通过利用MOEL，我们展示了一条通往更公平和更有效的AI技术的可行途径。|\n",
        "2411.13738": "|**2024-11-20**|**Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics**|Tetiana Bas et.al.|[2411.13738](http://arxiv.org/abs/2411.13738)|**[link](https://github.com/tetianabas/llm_biases)**|**本研究通过比较大型语言模型（LLMs）对性别的感知与人类受访者、美国劳工统计局数据和50%无偏见基准的性别感知，来探讨LLMs中的性别偏见。我们使用职业数据和特定角色的句子创建了一个新的评估集。与LLMs训练数据中常见的基准不同，我们的集合是全新开发的，防止了数据泄露和测试集污染。我们对五个LLMs进行了测试，以使用单词答案预测每个角色的性别。我们使用Kullback-Leibler（KL）散度来比较模型输出与人类感知、统计数据和50%中性基准之间的差异。所有LLMs都显示出与性别中性的显著偏差，并且更符合统计数据，仍然反映了固有的偏见。**|\n",
        "2411.13055": "|**2024-11-20**|**Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training**|Jared Fernandez et.al.|[2411.13055](http://arxiv.org/abs/2411.13055)|null|近年来，神经网络模型能力的显著提升是由模型规模、训练数据和相应的计算资源扩展驱动的。为了开发现代应用（如大型语言模型）所需的无尽大型网络，模型训练需要在数万台硬件加速器（例如GPU）上分布进行，这要求在大规模计算集群中协调计算和通信。在本工作中，我们证明了仔细考虑硬件配置和并行化策略对于有效（即计算和成本高效）地扩展模型规模、训练数据和总计算量至关重要。我们对大规模LLM训练工作负载在模型规模、硬件配置和分布式并行化策略方面的性能进行了广泛的实证研究。我们证明了：（1）超过一定规模后，某些分布式通信策略带来的开销导致之前被认为次优的并行化策略实际上变得更为可取；（2）即使硬件和并行化策略得到适当优化，增加加速器的总数来扩大大型模型训练也会迅速产生递减回报，这意味着每增加一个单位的功率或GPU时长的边际性能较差。|\n",
        "2412.01928": "|**2024-12-02**|**MALT: Improving Reasoning with Multi-Agent LLM Training**|Sumeet Ramesh Motwani et.al.|[2412.01928](http://arxiv.org/abs/2412.01928)|null|使大型语言模型（LLMs）之间实现有效协作是实现能够解决复杂问题的自主系统的重要一步。尽管LLMs通常被用作单模型生成器，其中人类对其输出进行批判和改进，但联合训练的协作模型的潜力在很大程度上尚未得到探索。尽管在多智能体通信和辩论环境中取得了有希望的结果，但在训练模型共同完成任务方面进展甚微。在本文中，我们提出了一种针对推理问题的“多智能体LLM训练”（MALT）的第一步。我们的方法采用了一种序列多智能体设置，其中分配给异构LLMs专门的角色：生成器、验证器和改进模型迭代解决问题。我们提出了一种基于轨迹扩展的合成数据生成过程和由基于联合结果的奖励驱动的信用分配策略。这使得我们的后训练设置能够利用正负轨迹来自主地提高每个模型的专门能力，作为联合序列系统的一部分。我们在MATH、GSM8k和CQA上评估了我们的方法，其中MALT在Llama 3.1 8B模型上相对于同一基线模型分别实现了14.14%、7.12%和9.40%的相对改进。这展示了在数学和常识推理问题上的多智能体协作能力的早期进步。更广泛地说，我们的工作为围绕多智能体LLM训练方法的研究提供了具体方向。|\n",
        "2412.01526": "|**2024-12-02**|**Addressing Data Leakage in HumanEval Using Combinatorial Test Design**|Jeremy S. Bradbury et.al.|[2412.01526](http://arxiv.org/abs/2412.01526)|null|大型语言模型（LLMs）在众多领域得到广泛应用，包括软件工程，它们已被用于自动化诸如程序生成和测试分类等任务。随着基于LLM的方法不断进化，定义清晰且稳健的方法以公平评估性能变得尤为重要。基准测试是评估LLMs解决特定任务能力以及评估LLM不同版本随时间解决任务能力的一种常见方法。例如，HumanEval基准测试由164个手工制作的任务组成，已成为评估基于LLM的程序生成的重要工具。然而，使用如HumanEval等基准测试对LLM进行公平评估的一个主要障碍是基准任务和解决方案数据泄露到训练数据集中的问题。这种障碍由于LLM训练数据的黑盒性质而加剧，这使得甚至难以知道是否发生了数据泄露。为了解决数据泄露问题，我们提出了一种新的基准构建方法，该方法将基准测试构建为由模板任务组成，可以使用组合测试设计实例化为新具体任务。对于同一模板任务的具体任务必须足够不同，以至于数据泄露的影响最小，并且足够相似，以至于在性能评估方面可以互换。为了评估我们的基准构建方法，我们提出了HumanEval_T，这是一个使用模板任务和组合测试设计构建的HumanEval的替代基准测试。|\n",
        "2412.01523": "|**2024-12-02**|**Data-Centric and Heterogeneity-Adaptive Sequence Parallelism for Efficient LLM Training**|Yujie Wang et.al.|[2412.01523](http://arxiv.org/abs/2412.01523)|null|扩展LLM的上下文长度（即最大支持的序列长度）具有极其重要的意义。为了促进LLM的长时间上下文训练，序列并行化已成为一项关键技术，它将每个输入序列分散到多个设备上，并需要通信来处理序列。本质上，现有的序列并行化方法假设序列长度均匀（即所有输入序列长度相同），因此为所有输入序列利用单一、静态的散列策略。然而，在现实中，LLM训练语料库中的序列长度表现出显著的差异，通常遵循长尾分布，导致工作负载异质性。在本文中，我们表明采用单一、静态的策略会导致效率低下和资源利用率不足，强调了需要自适应方法来处理序列间的异质工作负载。为此，我们提出了一种异质性自适应序列并行化方法。对于每个训练步骤，我们的方法捕捉序列长度的变化，并根据工作负载特征分配最优的散列策略组合。我们将此问题建模为线性规划优化，并设计了一种高效有效的求解器以找到最优解。此外，我们在支持分布式LLM训练的自适应并行化的高性能系统中实现了我们的方法。实验结果表明，我们的系统在性能上优于最先进的训练框架，最高可达1.98倍。|\n",
        "2412.01505": "|**2024-12-02**|**Scaling Law for Language Models Training Considering Batch Size**|Xian Shuai et.al.|[2412.01505](http://arxiv.org/abs/2412.01505)|null|近年来，大型语言模型（LLMs）取得了显著进步，其中规模法则在快速进展中发挥了关键作用。在本文中，我们实证研究了关键超参数，即全局批量大小，如何影响LLM的训练过程。我们首先训练了参数量从1.25亿到26亿的语模，使用了高达3000亿的高质量标记。通过这些实验，我们建立了一个关于模型大小和训练数据量的基本规模法则。然后，我们考察了批量大小和学习率的变化如何影响这些模型的收敛性和泛化能力。我们的分析得出两种情况下的批量大小规模法则：固定计算预算和固定训练数据量。对不断增大的模型进行的外推实验验证了我们的预测法则，为在特定资源限制下优化LLM训练策略提供了指导。|\n",
        "2412.04747": "|**2024-12-06**|**Code generation and runtime techniques for enabling data-efficient deep learning training on GPUs**|Kun Wu et.al.|[2412.04747](http://arxiv.org/abs/2412.04747)|null|随着深度学习模型的规模扩大，其训练成本显著增加。由于硬件的进步和当前软件堆栈的限制，数据效率的需求日益上升。数据效率是指有效隐藏数据访问延迟和避免不必要的数据处理。由于GPU内存带宽与计算吞吐量之间的差距越来越大，GPU内存容量即将达到限制，以及PyTorch软件堆栈中的不效率（包括缺乏特定设备的PCIe传输优化和高级领域特定抽象），主要挑战随之产生。为了有效地缓解这些数据不效率，本论文分析了代表性深度训练任务中的数据不效率，特别是在图神经网络（GNN）和大型语言模型（LLM）中。然后，它提出了新的运行时和代码生成技术来缓解这些挑战，并在PyTorch堆栈中无缝实现这些优化，同时保持强大的可编程性和互操作性。首先，设计PyTorch-Direct以在PyTorch中集成以GPU为中心的PCIe数据传输范式用于GNN训练。接下来，提出了Hector中间表示（IR）及其代码生成器，以引入领域特定的顶层抽象并系统地解决关系型GNN的内存密集型性能挑战。最后，在LLM训练中，吞吐量越来越受限于GPU内存容量。为了缓解这一问题，设计了并实现了SSDTrain卸载框架。总之，这些贡献表明，代码生成和运行时技术可以系统地缓解深度学习训练中的数据管理瓶颈，这些瓶颈源于工作负载的数据密集性以及深度学习训练软件堆栈中的固有简化。|\n",
        "2412.06370": "|**2024-12-09**|**Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit**|Joshua Freeman et.al.|[2412.06370](http://arxiv.org/abs/2412.06370)|null|近期，由于2023年12月提起的《纽约时报》诉OpenAI诉讼，前沿大型语言模型（LLM）的版权侵权问题受到了广泛关注。纽约时报声称，GPT-4通过在LLM训练中使用复制文章并在其输出中记忆这些输入，从而侵犯了其版权。我们的研究旨在衡量OpenAI的LLM相对于其他LLM在其输出中表现出逐字记忆的倾向，特别是针对新闻文章。我们发现，GPT和Claude模型都使用拒绝训练和输出过滤器来防止记忆文章的逐字输出。我们应用了一个基本的提示模板来绕过拒绝训练，并显示OpenAI模型目前比Meta、Mistral和Anthropic的模型更不容易被激发记忆。我们发现，随着模型规模的增加，特别是超过1000亿参数后，它们表现出显著更强的记忆能力。我们的发现对训练具有实际意义：必须更加关注在非常大型模型中防止逐字记忆。我们的发现也具有法律意义：在评估OpenAI的LLM相对记忆能力时，我们探究了《纽约时报》版权侵权主张的强度和OpenAI的法律辩护，同时强调了生成式AI、法律和政策交叉领域的问题。|\n",
        "2412.05342": "|**2024-12-06**|**Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation**|Xiaoyu Wang et.al.|[2412.05342](http://arxiv.org/abs/2412.05342)|null|大型语言模型（LLM）通常经过微调以参与二元或双方面对面的对话，但它们在多方面对话（MPD）中的适应能力较差，这阻碍了它们在多个人会议、讨论和日常交流等场景中的应用。以往基于LLM的研究主要关注多智能体框架，而其基础LLM仍然是成对微调的。在本工作中，我们设计了一个针对多方面对话数据集的LLM多方面微调框架（MuPaS），并证明这样一个简单的框架可以让LLM高效有效地与多方面对话风格保持一致。我们还设计了两种训练策略，可以将MuPaS转化为MPD模拟器。大量实验表明，MuPaS可以实现最先进的多人回答，更高的一轮预测准确率，更高的人机和自动评估的说话质量，甚至能够在分布外场景、主题和角色描述中生成合理的内容。MuPaS框架将LLM的训练与更复杂的多人应用（如对话生成、虚拟排练或元宇宙）相连接。|\n",
        "2412.07298": "|**2024-12-10**|**The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model**|Jiawei Chen et.al.|[2412.07298](http://arxiv.org/abs/2412.07298)|null|大型语言模型（LLMs）展现出显著的多语言能力。然而，这些能力在预训练过程中发展的机制尚不清楚。在本文中，我们利用代码LLMs作为实验平台，探讨LLMs在预训练过程中多语言能力的演变。基于我们的观察，我们提出了巴别塔假说，该假说描述了LLMs获取新语言能力的整个过程。在学习过程中，多种语言最初共享一个由主要语言主导的知识系统，并逐渐发展出特定于语言的知识系统。然后，我们通过识别工作语言和语言转换神经元来追踪LLMs的内部状态，以验证上述假说。实验结果表明，LLMs的内部状态变化与我们的巴别塔假说一致。基于这些洞察，我们提出了一种新的方法来构建针对多语言代码LLMs的优化预训练语料库，其性能显著优于在原始语料库上训练的LLMs。所提出的巴别塔假说为设计预训练数据分布以实现LLMs最佳多语言能力提供了新的见解。|\n",
        "2412.07210": "|**2024-12-10**|**EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models**|Jialiang Cheng et.al.|[2412.07210](http://arxiv.org/abs/2412.07210)|null|分布式训练方法对于大型语言模型（LLMs）至关重要。然而，现有的分布式训练方法往往受到通信瓶颈、执行缓慢和弹性有限等问题的影响。为了解决这些问题，我们提出了EDiT，这是一种创新的、高效的分布式训练方法，它结合了定制化的局部随机梯度下降（Local SGD）方法和模型分片技术，以提高大规模训练的效率。EDiT在正向传播过程中执行层级的参数同步，减少通信和内存开销，并实现计算与通信的并行。此外，EDiT采用伪梯度惩罚策略来抑制损失波动，确保训练稳定性并提升性能。此外，我们引入了A-EDiT，这是EDiT的完全异步变体，适用于异构集群。基于EDiT/A-EDiT，我们进行了一系列实验，以验证LLMs的大规模异步训练，并伴随全面的分析。实验结果表明，EDiT/A-EDiT的性能优越，成为不同计算生态系统中分布式LLM训练的稳健解决方案。|\n",
        "2412.07031": "|**2024-12-09**|**Large Language Models: An Applied Econometric Framework**|Jens Ludwig et.al.|[2412.07031](http://arxiv.org/abs/2412.07031)|null|大型语言模型（LLMs）正在被用于经济学研究以形成预测、标注文本、模拟人类反应、生成假设，甚至为不存在数据的时间和地点生成数据。虽然这些应用具有创新性，但它们是否有效？我们何时可以忽略LLMs的内部运作，仅仅依赖它们的输出？我们开发了一个计量经济学框架来回答这个问题。我们的框架区分了两种类型的实证任务。在以下条件下，使用LLMs的输出进行预测问题（包括假设生成）是有效的：LLMs的训练数据集与研究人员样本之间没有“泄露”。使用LLMs的输出进行估计问题以自动化某些经济概念（由某些文本或人类受试者表达）的测量需要额外的假设：LLMs的输出必须与它们所取代的黄金标准测量一样好。否则，即使LLMs的输出高度准确但并非完美，估计也可能存在偏差。我们在金融和政治经济学中的实例应用中记录了这些条件被违反的程度及其对研究发现的含义。我们还为实证研究人员提供了指导。确保没有训练泄露的唯一方法是使用具有记录的训练数据和发布权重的开源LLMs。处理LLMs测量误差的唯一方法是收集验证数据和建模误差结构。一个推论是，如果无法满足候选LLMs应用的条件，我们强烈建议：不要使用。|\n",
        "2412.11625": "|**2024-12-16**|**Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods**|Diana Bar-Or Nirman et.al.|[2412.11625](http://arxiv.org/abs/2412.11625)|null|虽然大型语言模型（LLMs）已成为各个领域的核心工具，但它们往往提供不准确或错误的信息。本研究考察了用户对LLMs错误信息回应的偏好。具体来说，我们评估了用户对LLMs中明确标记的虚假陈述与未标记回应的偏好，以及对比LLM承认缺乏知识的声明和自信的虚假陈述的偏好。此外，我们还调查了要求用户评估陈述真实性如何影响这些偏好。令人惊讶的是，61%的用户更喜欢未标记的错误回应，而69%的用户更喜欢自信的虚假陈述而不是LLM承认缺乏知识。在我们的所有实验中，共有300名用户参与，为我们的分析和结论提供了宝贵的数据。当用户需要评估陈述的真实性时，对未标记和错误回应的偏好略有下降，但仍然很高。这些发现表明，用户偏好可能会通过反馈机制影响LLM的训练，并无意中鼓励生成虚假信息。未来的研究应解决将LLM行为与这种偏好对齐的伦理和实际影响。|\n",
        "2412.11102": "|**2024-12-15**|**Empowering LLMs to Understand and Generate Complex Vector Graphics**|Ximing Xing et.al.|[2412.11102](http://arxiv.org/abs/2412.11102)|null|大型语言模型（LLMs）的空前进步深刻影响了自然语言处理，但尚未完全涉及可扩展矢量图形（SVG）生成的领域。虽然LLMs在训练过程中编码了来自网页的SVG数据的部分知识，但近期研究发现，LLMs中的语义模糊和分词表示可能会导致矢量原初预测中的幻觉。此外，LLM的训练通常缺乏对矢量路径渲染序列的建模和理解，这可能导致输出矢量原初之间的遮挡。在本文中，我们提出了LLM4SVG，这是填补这一差距的初步但重要的一步，使LLMs能够更好地理解和生成矢量图形。LLM4SVG通过可学习的语义标记促进了对SVG组件的深入了解，这些标记精确地编码了这些标记及其相应的属性，以生成语义一致的SVG输出。通过一系列可学习的语义标记，开发了一个用于指令遵循的结构化数据集，以支持两个主要任务的理解和生成。我们的方法引入了一个模块化架构，将语义标签、矢量指令编码器、微调命令和强大的LLMs紧密结合起来，以紧密结合几何、外观和语言信息。为了克服SVG-text指令数据的稀缺性，我们开发了一个自动化的数据生成管道，收集了超过250k SVG数据和580k SVG-text指令的庞大数据集，这促进了LLM开发中流行的两阶段训练策略的采用。通过探索各种训练策略，我们开发了LLM4SVG，它在人类评估任务中显著超越了基于优化渲染的方法和基于语言模型的基线，取得了显著成果。|\n",
        "2412.10434": "|**2024-12-11**|**NAT-NL2GQL: A Novel Multi-Agent Framework for Translating Natural Language to Graph Query Language**|Yuanyuan Liang et.al.|[2412.10434](http://arxiv.org/abs/2412.10434)|**[link](https://github.com/leonyuancode/stockgql)**|**大型语言模型（LLMs）的出现颠覆了许多领域，不仅限于传统的自然语言处理（NLP）任务。最近，将LLMs应用于数据库领域的研究蓬勃发展，作为典型的非关系型数据库，LLMs在图数据库研究中的应用自然引起了广泛关注。近期的研究越来越多地集中于利用LLMs将自然语言翻译成图查询语言（NL2GQL）。尽管取得了一些进展，但这些方法存在明显的局限性，例如它们依赖于简化的流程，这些流程往往忽略了LLMs自主规划和与其他LLMs协作解决复杂NL2GQL挑战的潜力。为了解决这一差距，我们提出了NAT-NL2GQL，一个将自然语言翻译成图查询语言的创新多智能体框架。具体来说，我们的框架由三个协同的智能体组成：预处理智能体、生成智能体和精炼智能体。预处理智能体负责管理作为上下文的数据处理，包括诸如命名实体识别、查询重写、路径链接和提取与查询相关的模式等任务。生成智能体是一个在NL-GQL数据上微调的LLMs，负责根据查询及其相关模式生成相应的GQL语句。精炼智能体负责使用从GQL执行结果中获得的错误信息精炼GQL或上下文。鉴于基于nGQL语法的优质开源NL2GQL数据集稀缺，我们开发了StockGQL，一个由金融市场图数据库构建的数据集。它可在以下网址获取：https://github.com/leonyuancode/StockGQL。在StockGQL和SpCQL数据集上的实验结果表明，我们的方法在性能上显著优于基线方法，突显了其在推进NL2GQL研究方面的潜力。**|\n",
        "2412.13148": "|**2024-12-17**|**SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba，2015）对于大型语言模型的成功至关重要。然而，它们通常需要在训练过程中维护优化器状态，这可能导致内存需求比模型足迹大几倍。这种开销对可扩展性和计算效率提出了限制。与此相反，随机梯度下降（SGD）是一种无状态优化器，因为它在训练过程中不跟踪状态变量。因此，它实现了最佳内存效率。然而，它在LLM训练中的能力有限（Zhao等，2024b）。在这项工作中，我们表明以无状态方式预处理SGD可以达到与Adam优化器相同的性能，同时大幅降低内存成本。具体来说，我们建议使用归一化和白化预处理瞬时随机梯度。我们表明归一化稳定了梯度分布，而白化抵消了损失景观的局部曲率。这导致了SWAN（带有白化和归一化的SGD），这是一种随机优化器，消除了存储任何优化器状态的需求。经验表明，SWAN的内存占用与SGD相同，与Adam相比，总端到端内存减少了约50%。在语言建模任务中，SWAN表现出与Adam相当甚至更好的性能：当使用350M和1.3B参数预训练LLaMA模型时，SWAN通过使用一半的标记达到相同的评估困惑度，实现了2倍的速度提升。|\n",
        "2412.13998": "|**2024-12-18**|**Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes**|Katarzyna Kobalczyk et.al.|[2412.13998](http://arxiv.org/abs/2412.13998)|**[link](https://github.com/kasia-kobalczyk/few-shot-steerable-alignment)**|**随着大型语言模型（LLMs）越来越多地嵌入到日常应用中，确保它们与个体用户的多样化偏好保持一致已成为一个关键挑战。目前部署的方法通常假设用户目标具有同质性，并依赖于单目标微调。然而，人类的偏好本质上是异质的，受各种不可观测因素的影响，导致偏好数据中出现冲突的信号。现有的解决方案解决这种多样性通常需要昂贵的、针对特定目标的标记数据集，并涉及训练多个奖励模型或LLM策略，这既计算成本高又不切实际。在这项工作中，我们提出了一种新颖的少样本可操控对齐框架，其中用户的潜在偏好是从他们选择的小样本中推断出来的。为此，我们将Bradley-Terry-Luce模型扩展到处理具有不可观测变异因素的异质偏好，并提出了其实际的奖励建模和LLM微调实现。得益于我们提出的功能参数空间条件化方法，使用我们框架训练的LLMs可以在推理时适应个体偏好，生成跨越连续行为模式的输出。我们通过实证验证了方法的有效性，证明了它们以数据高效的方式捕捉并适应多样化的人类偏好。我们的代码可在以下网址获取：https://github.com/kasia-kobalczyk/few-shot-steerable-alignment。**|\n",
        "2412.13670": "|**2024-12-18**|**AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge**|Xiaobao Wu et.al.|[2412.13670](http://arxiv.org/abs/2412.13670)|null|数据污染阻碍了公平的LLM评估，因为它会将测试数据引入新模型的训练集中。现有研究通过更新基准数据集来解决这一挑战。然而，它们无法保证评估的无污染性，因为新收集的数据可能包含已有的知识，并且它们的基准更新依赖于大量的人工劳动。为了解决这些问题，本文提出了一种自动化的防泄露基准框架AntiLeak-Bench。我们不是简单地使用新收集的数据，而是构建了包含显式新知识、这些知识在LLM的训练集中缺失的样本，从而确保了严格的无污染评估。我们进一步设计了一个完全自动化的工作流程，用于构建和更新我们的基准，无需人工劳动。这显著降低了基准维护的成本，以适应新兴的LLM。通过广泛的实验，我们强调了在LLM截止时间之前可能存在数据污染，并证明了AntiLeak-Bench有效地克服了这一挑战。|\n",
        "2412.14479": "|**2024-12-19**|**Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters**|Zihan Chang et.al.|[2412.14479](http://arxiv.org/abs/2412.14479)|null|现有工作只适用于特定数量的GPU，通常忽略了手动确定所需GPU的具体类型和数量的复杂性，这对开发者来说可能是一个重大的负担。为了解决这一问题，我们提出了Frenzy，一种针对异构GPU集群的内存感知无服务器计算方法。Frenzy允许用户提交模型时无需担心底层硬件资源。首先，Frenzy通过估算LLM的GPU内存使用量来预测所需的GPU数量和类型。然后，它采用一种低开销的异构感知调度方法来优化训练效率。我们通过在包含三种不同GPU类型的异构GPU集群上进行的多元任务LLM训练测试验证了Frenzy的性能。结果显示，Frenzy的内存使用量预测准确率超过92%，调度开销降低了10倍，与最先进的方法相比，平均作业完成时间减少了12%至18%。|\n",
        "2412.14373": "|**2024-12-18**|**ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling**|William Han et.al.|[2412.14373](http://arxiv.org/abs/2412.14373)|**[link](https://github.com/willxxy/ecg-byte)**|**大型语言模型（LLMs）在除文本以外的领域，特别是在心电图（ECGs）领域，展现了出色的适应性。更具体地说，越来越多的研究正在探索从多通道ECG和相应的文本提示生成文本的任务。当前的方法通常涉及使用自监督学习（SSL）目标预训练一个ECG特定的编码器，并使用预训练编码器输出的特征微调LLM以进行自然语言生成（NLG）。然而，这些方法受限于1）两阶段训练的效率低下和2）编码器生成的特征的可解释性挑战。为了解决这些限制，我们引入了ECG-Byte，这是一种用于ECG自回归语言建模的改进的byte pair encoding（BPE）标记化管道。这种方法通过压缩和编码ECG信号为标记，通过直接结合ECG和文本标记来实现端到端的LLM训练，同时由于ECG标记可以直接映射回原始信号，因此具有更高的可解释性。使用ECG-Byte，我们在NLG任务中只用了两阶段方法一半的时间和大约48%的数据就实现了有竞争力的性能。**|\n",
        "2412.15495": "|**2024-12-20**|**TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use**|Junjie Ye et.al.|[2412.15495](http://arxiv.org/abs/2412.15495)|**[link](https://github.com/junjie-ye/tl-training)**|**大型语言模型（LLMs）通过利用工具与外部环境交互，实现了显著的进步，这是通向通用人工智能的关键一步。然而，标准的有监督微调（SFT）方法，它依赖于大规模数据集，往往忽略了工具使用中的任务特定特征，导致性能瓶颈。为了解决这一问题，我们分析了三种现有的LLMs，并揭示了关键见解：训练数据可能会无意中阻碍工具使用行为，标记的重要性分布不均，工具调用错误落入少数几个不同的类别。基于这些发现，我们提出了TL-Training，这是一个基于任务特征的框架，可以减轻次优训练数据的影响，在SFT期间动态调整标记权重以优先考虑关键标记，并纳入针对错误类别的鲁棒奖励机制，通过近端策略优化进行优化。我们通过训练CodeLLaMA-2-7B并在四个不同的开源测试集上评估它来验证TL-Training。我们的结果表明，使用我们的方法训练的LLM在工具使用性能上与开源和闭源LLMs相当甚至更好，仅使用1,217个训练数据点。此外，我们的方法增强了在噪声环境中的鲁棒性，并提高了通用任务性能，为LLMs中的工具使用训练提供了一个可扩展且高效的范例。代码和数据可在https://github.com/Junjie-Ye/TL-Training上找到。**|\n",
        "2412.15309": "|**2024-12-19**|**Conceptual In-Context Learning and Chain of Concepts: Solving Complex Conceptual Problems Using Large Language Models**|Nishtha N. Vaidya et.al.|[2412.15309](http://arxiv.org/abs/2412.15309)|null|科学和工程问题属于需要特定概念信息（CI）才能解决的复杂概念问题，如数学/逻辑相关知识、流程信息或工程指南。由于在辅助问题解决等工程和科学任务中的应用前景，大型语言模型（LLMs）是解决此类复杂概念问题的有希望的工具。然而，在开放世界数据上训练的普通LLMs缺乏必要的CI。在这项工作中，我们专门探讨了LLMs的浅层定制方法（SCMs）以解决复杂概念问题。我们提出了两种针对LLM的新颖SCM算法，以增强LLMs的CI并使LLMs能够解决复杂概念问题：概念情境学习（C-ICL）和概念链（CoC）。本文解决的问题是基于数据建模指南中的概念信息在工程/工业领域生成专有数据模型。我们在各种规模的OpenAI LLM上评估了我们的算法，与四个与语法和语义正确性、时间和成本相关的评估指标进行了比较。与目前流行的LLM SCMs（如情境学习（ICL）和思维链（CoT））相比，所提出的算法表现更好。观察到，与CoT相比，新SCMs C-ICL和CoC的响应正确性分别提高了30.6%和29.88%。定性分析表明，所提出的新SCMs在LLMs中激活了之前在现有SCMs中未观察到的涌现能力。它们使问题解决过程更加透明，并减少了幻觉以及模型响应复制提示（鹦鹉学舌）的趋势。|\n",
        "2412.15282": "|**2024-12-18**|**A Systematic Examination of Preference Learning through the Lens of Instruction-Following**|Joongwon Kim et.al.|[2412.15282](http://arxiv.org/abs/2412.15282)|null|偏好学习是一种广泛采用的训练后技术，它将大型语言模型（LLMs）与人类偏好相一致，并提高特定下游任务的性能。在这项工作中，我们系统地研究了偏好数据集的特定属性如何影响LLMs在指令跟随任务中的对齐和下游性能。我们使用一个新颖的合成数据生成流程来生成48,000个独特的指令跟随提示，这些提示由23个可验证的约束组合而成，这些约束可以实现对模型响应的精细和自动化质量评估。使用我们的合成提示，我们采用两种偏好数据集整理方法——拒绝采样（RS）和蒙特卡洛树搜索（MCTS）——来获取（选择，拒绝）响应对。然后，我们进行实验研究以下影响：（1）选择和拒绝响应之间共享前缀的存在，（2）选择、拒绝响应的对比度和质量，（3）训练提示的复杂性。我们的实验表明，由MCTS生成的偏好对中的共享前缀提供了微小的但一致的提高，并使困难训练配置具有更大的稳定性。高对比度偏好对通常优于低对比度对；然而，结合两者通常通过平衡多样性和学习效率来实现最佳性能。此外，与过于具有挑战性的提示相比，在中等难度的提示上进行训练导致在更复杂的评估场景中对任务的泛化能力更好。我们的发现为优化指令跟随任务的偏好数据整理提供了可操作的见解，提供了一种可扩展且有效的框架，用于增强LLMs的训练和对齐。|\n",
        "2412.17626": "|**2024-12-23**|**Tracking the Feature Dynamics in LLM Training: A Mechanistic Study**|Yang Xu et.al.|[2412.17626](http://arxiv.org/abs/2412.17626)|null|理解大型语言模型（LLMs）的训练动态和特征演变对于其机制可解释性至关重要。尽管稀疏自编码器（SAEs）已被用于识别LLMs中的特征，但这些特征在训练过程中的演变情况仍然难以清晰地描绘。在本研究中，我们：（1）引入了SAE-Track，这是一种高效获取连续SAEs系列的方法；（2）对特征形成过程进行了阐述并进行了机制分析；（3）分析了训练过程中的特征漂移并进行了可视化。我们的工作为LLMs中特征动态提供了新的见解，增强了我们对训练机制和特征演变的理解。|\n",
        "2412.19616": "|**2024-12-27**|**Gradient Weight-normalized Low-rank Projection for Efficient LLM Training**|Jia-Hong Huang et.al.|[2412.19616](http://arxiv.org/abs/2412.19616)|**[link](https://github.com/jhhuangkay/gradient-weight-normalized-low-rank-projection-for-efficient-llm-training)**|**大型语言模型（LLMs）在各种任务中展现了卓越的性能，但日益增长的计算资源需求提出了重大挑战，特别是在下游任务中广泛使用全微调时。为了解决这个问题，已经开发出参数高效微调（PEFT）方法，但它们通常比全微调表现不佳，并且内存效率较低。在这项工作中，我们介绍了梯度权重归一化低秩投影（GradNormLoRP），这是一种新颖的方法，它提高了参数和内存效率，同时保持了与全微调相当的性能。GradNormLoRP通过归一化权重矩阵来改善梯度条件，促进优化过程中的更好收敛。此外，它对权重和梯度矩阵应用低秩近似，显著减少了训练过程中的内存使用。广泛的实验表明，我们的8位GradNormLoRP将优化器内存使用量减少了高达89.5%，并使在消费级GPU（如NVIDIA RTX 4090）上预训练大型LLMs（如LLaMA 7B）成为可能，而无需额外的推理成本。此外，GradNormLoRP在微调任务中优于现有的低秩方法。例如，当在所有GLUE任务上使用8阶微调RoBERTa模型时，GradNormLoRP实现了平均分数80.65，超过了LoRA的79.23分。这些结果突显了GradNormLoRP作为高效LLM预训练和微调的有希望的替代方案。源代码和附录：https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training**|\n",
        "2412.21123": "|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越多地依赖于网络抓取的数据集，对未经授权使用受版权保护或个人内容进行训练的担忧日益加剧。尽管有像通用数据保护条例（GDPR）这样的法规，数据所有者仍然对其内容在模型训练中的使用控制有限。为了解决这个问题，我们提出了ExpShield，这是一种主动的自我保护机制，使内容所有者能够将不可见的扰动嵌入到他们的文本中，限制LLMs训练中的数据滥用，同时不影响可读性。这种预防性方法使数据所有者能够直接保护敏感内容，而不需要依赖第三方进行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的合理性。我们进一步通过识别记忆触发器和创建陷阱，以更集中的方式使模型记忆偏离，从而提高效率。为了验证我们防御的有效性，我们提出了一个新颖的实例利用度量，它捕捉了模型训练引起的个体风险。实验结果表明，我们的方法有效，因为MIA AUC从0.95降至0.55，实例利用接近于零。这表明训练后个体风险并未增加，强调了主动防御在保护受版权数据中的重要性。|\n"
    }
}