## Updated on 2024.11.21
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-20**|**Metacognition for Unknown Situations and Environments (MUSE)**|Rodolfo Valiente et.al.|[2411.13537](http://arxiv.org/abs/2411.13537)|null|元认知——即对自身认知过程的意识和调控——对于人类在未知情境中的适应性至关重要。相比之下，当前的自主代理在新环境中常常难以应对，因为它们的适应能力有限。我们假设元认知是适应性自主系统中一个关键的缺失成分，它赋予系统解决不熟悉挑战所需的认知灵活性。鉴于元认知能力的广泛性，我们重点关注两个关键方面：能力意识和策略选择以应对新任务。为此，我们提出了“未知情境与环境的元认知”（MUSE）框架，该框架将元认知过程——特别是自我意识和自我调节——整合到自主代理中。我们提出了MUSE框架的两种初步实现：一种基于世界建模，另一种利用大型语言模型（LLMs），这两种方法都实现了元认知循环。我们的系统持续学习评估其在给定任务上的能力，并利用这种自我意识来指导策略选择的迭代周期。与基于Dreamer-v3的强化学习和纯粹基于提示的LLM代理方法相比，MUSE代理在自我意识和自我调节方面表现出显著改进，使其能够更有效地解决新颖、分布外的任务。这项工作强调了受认知和神经系统的启发的方法在使自主系统适应新环境方面的潜力，克服了当前依赖大量训练数据的方法的局限性。|
|**2024-11-19**|**Human-In-the-Loop Software Development Agents**|Wannita Takerngsaksiri et.al.|[2411.12924](http://arxiv.org/abs/2411.12924)|null|最近，基于大型语言模型（LLM）的多智能体范式被引入到软件工程中，以自动解决软件开发任务（例如，从给定的问题到源代码）。然而，现有的工作是基于历史基准数据集进行评估的，没有考虑在自动化软件开发过程的每个阶段中的人类反馈，并且尚未在实践中部署。在本文中，我们介绍了一个名为HULA（人机协作LLM智能体框架）的框架，用于软件开发，该框架允许软件工程师在生成给定任务的编码计划和源代码时对其进行优化和引导。我们设计、实现并将其部署到Atlassian JIRA中供内部使用。通过多阶段评估HULA框架，Atlassian的软件工程师认为HULA可以最小化整体开发时间和精力，特别是在启动编码计划和编写直接任务的代码方面。另一方面，提出了需要解决的一些关于代码质量的挑战。我们总结了经验教训并讨论了未来工作的机会，这将为LLM智能体在软件开发中的发展铺平道路。|
|**2024-11-19**|**Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction**|Sonny George et.al.|[2411.12828](http://arxiv.org/abs/2411.12828)|**[link](https://github.com/sonnygeorge/oedd)**|**大型语言模型（LLM）代理在越来越多的领域显示出潜力。在许多提议的应用中，期望代理基于输入提示中的累积经验进行推理。我们提出了OEDD（尽管有干扰也要运用经验）语料库，这是一个由人工注释者验证的情景集合，其中代理的历史是预编写的，在存在干扰信息的情况下，代理必须根据不同的环境前提做出决策。我们使用最小链式思维提示策略评估了三种最先进的LLM（GPT-3.5 Turbo、GPT-4o和Gemini 1.5 Pro），并观察到当（1）输入上下文包含超过1,615个历史交互令牌，（2）一个关键的决策信息前提是两个不同环境前提的正确结论，并且（3）一个微不足道但具有干扰性的红鲱鱼事实随后出现时，所有LLM在选择两个行动中的较好者时表现得比随机选择更差。我们的代码和测试语料库可在以下网址公开获取：https://github.com/sonnygeorge/OEDD 。**|
|**2024-11-19**|**A More Advanced Group Polarization Measurement Approach Based on LLM-Based Agents and Graphs**|Zixin Liu et.al.|[2411.12196](http://arxiv.org/abs/2411.12196)|null|群体极化是社交媒体内容分析中的一个重要研究方向，吸引了许多研究人员探索这一领域。因此，如何有效地测量群体极化已成为一个关键话题。在社交媒体上测量群体极化存在一些挑战，这些挑战尚未被现有的解决方案完全解决。首先，社交媒体群体极化的测量涉及处理大量文本，这对信息提取构成了重大挑战。其次，社交媒体上的文本通常难以理解，包括讽刺、梗图和网络俚语等内容。此外，群体极化研究侧重于整体分析，而文本通常是碎片化的。为了解决这些挑战，我们设计了一个基于多智能体系统的解决方案，并使用了一种称为社区情感网络（Community Sentiment Network, CSN）的图结构来表示极化状态。此外，我们基于CSN开发了一种称为社区对立指数（Community Opposition Index, COI）的度量方法来量化极化程度。最后，我们通过零样本立场检测任务测试了我们的多智能体系统，并取得了出色的结果。综上所述，所提出的这种方法在可用性、准确性和可解释性方面具有重要意义。|
|**2024-11-19**|**Generative World Explorer**|Taiming Lu et.al.|[2411.11844](http://arxiv.org/abs/2411.11844)|null|在具身AI中，基于部分观测的规划是一个核心挑战。大多数先前的工作通过开发物理探索环境以更新其对世界状态认知的代理来解决这一挑战。相比之下，人类可以通过心理探索想象未见过的世界部分，并通过想象的观测结果来修正他们的信念。这样的更新信念可以使他们在无需不断进行物理探索的情况下做出更明智的决策。为了实现这种类似人类的能力，我们引入了“生成世界探索者（Genex）”，这是一种以自我为中心的世界探索框架，允许代理在大型三维世界（如城市场景）中进行心理探索并获取想象中的观测结果来更新其信念。这种更新后的信念将帮助代理在当前步骤中做出更明智的决策。为了训练Genex，我们创建了一个合成的城市场景数据集Genex-DB。我们的实验结果表明，(1) Genex可以在大型虚拟物理世界的长时序探索过程中生成高质量且一致的观测结果；(2) 使用这些生成的观测结果更新的信念可以指导现有的决策模型（例如LLM代理）做出更好的计划。|
|**2024-11-18**|**LLM-IE: A Python Package for Generative Information Extraction with Large Language Models**|Enshuo Hsu et.al.|[2411.11779](http://arxiv.org/abs/2411.11779)|null|尽管最近采用了大型语言模型（LLMs）进行生物医学信息提取，但在提示工程和算法方面仍然存在挑战，并且没有专门的软件可用。为了解决这些问题，我们开发了LLM-IE：一个用于构建完整信息提取管道的Python包。我们的主要创新是一个交互式的LLM代理，以支持模式定义和提示设计。  材料与方法：LLM-IE支持命名实体识别、实体属性提取和关系提取任务。我们在i2b2数据集上进行了基准测试并进行了系统评估。  结果：基于句子的提示算法在性能上表现最佳，但需要更长的推理时间。系统评估提供了直观的可视化。  讨论：LLM-IE的设计基于医疗健康领域的实际NLP经验，并已在内部项目中采用。它对生物医学NLP社区应具有巨大价值。  结论：我们开发了一个Python包LLM-IE，提供构建稳健的信息提取管道的构建模块。|
|**2024-11-18**|**OASIS: Open Agents Social Interaction Simulations on One Million Agents**|Ziyi Yang et.al.|[2411.11581](http://arxiv.org/abs/2411.11581)|null|近年来，人们越来越有兴趣通过增强基于规则的智能体模型（ABMs）来研究社交媒体平台（如X、Reddit），使其具有更真实的大型语言模型（LLM）智能体，从而实现对复杂系统的更精细研究。因此，过去一年提出了几种基于LLM的ABMs。尽管它们充满前景，但每个模拟器都是为研究特定场景而专门设计的，这使得使用相同的ABM探索其他现象变得耗时且资源密集。此外，这些模型仅模拟有限数量的智能体，而现实世界中的社交媒体平台涉及数百万用户。为此，我们提出了OASIS，这是一种通用且可扩展的社交媒体模拟器。OASIS的设计基于现实世界的社交媒体平台，包括动态更新的环境（即动态社交网络和帖子信息）、多样的行为空间（即关注、评论）以及推荐系统（即基于兴趣和热门评分）。此外，OASIS支持大规模用户模拟，能够建模多达一百万用户。借助这些功能，OASIS可以轻松扩展到不同的社交媒体平台，以研究大规模群体现象和行为。我们复制了各种社会现象，包括信息传播、群体极化和从X和Reddit平台上观察到的羊群效应。此外，我们在不同规模的智能体群体下提供了社会现象的观察结果。我们观察到，更大的智能体群体规模导致更强烈的群体动态和更多样化、更有帮助的智能体意见。这些发现展示了OASIS作为研究数字环境中复杂系统强大工具的潜力。|
|**2024-11-16**|**IntentGPT: Few-shot Intent Discovery with Large Language Models**|Juan A. Rodriguez et.al.|[2411.10670](http://arxiv.org/abs/2411.10670)|null|在当今数字化驱动的世界中，对话系统在提升用户体验方面扮演着关键角色，从客户服务到虚拟助手。在这些对话中，自动识别用户的目标对于及时解决他们的需求至关重要。这促使了意图检测模型的集成。然而，用户的意图是多样化和动态变化的，使得维持固定的一组预定义意图变得具有挑战性。因此，开发一种能够识别新出现意图的模型更为实际。我们解决了意图发现这一挑战，这是近期研究努力的一个重要领域。现有的方法需要大量的数据来正确识别新的意图，这需要大量的人力投入。为了解决这个问题，我们引入了IntentGPT，这是一种新颖的无需训练的方法，可以有效地提示大型语言模型（如GPT-4）在少量标记数据的情况下发现新的意图。IntentGPT包含一个“上下文提示生成器”，用于生成上下文学习的信息性提示，一个“意图预测器”用于从语句中分类和发现用户意图，以及一个“语义少样本采样器”，用于选择相关的少样本示例和一组已知意图并将其注入提示中。我们的实验表明，IntentGPT在流行的基准测试中，包括CLINC和BANKING等，优于那些需要广泛领域特定数据和微调的先前方法。|
|**2024-11-15**|**Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash**|Parsa Hejabi et.al.|[2411.10422](http://arxiv.org/abs/2411.10422)|**[link](https://github.com/parsahejabi/simulation-framework-for-multi-agent-balderdash)**|**大型语言模型（LLMs）在复杂任务和交互环境中展示了令人印象深刻的性能，但它们的创造力仍需进一步探索。本文介绍了一个利用游戏Balderdash的仿真框架，以评估LLMs的创造力和逻辑推理能力。在Balderdash游戏中，玩家为生僻词汇生成虚构定义，以欺骗他人同时识别正确定义。我们的框架使多个LLM代理能够参与这个游戏，评估它们生成合理定义以及基于游戏规则和历史进行策略规划的能力。我们实现了一个集中式的游戏引擎，其中包含各种LLM作为参与者，还有一个作为裁判的LLM来评估语义等效性。通过一系列实验，我们分析了不同LLM的表现，考察了诸如真实定义比率、欺骗比率和正确猜测比率等指标。研究结果提供了对LLMs的创造性和欺骗能力的见解，突出了它们的优势和改进空间。特别是，研究表明，LLMs输入中的低频词汇会导致对游戏规则和历史情境推理不佳（https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash）。**|
|**2024-11-15**|**An Empirical Study on LLM-based Agents for Automated Bug Fixing**|Xiangxin Meng et.al.|[2411.10213](http://arxiv.org/abs/2411.10213)|null|大型语言模型（LLMs）和基于LLM的Agent在自动修复bug方面已经显示出能力，通过与开发环境的交互、迭代验证和代码修改来解决软件缺陷。然而，对于这些Agent和非Agent系统进行系统的分析仍然有限，特别是关于顶级系统之间的性能差异。在这篇论文中，我们在SWE-bench Lite基准上测试了七个专有和开源系统，以评估其自动化bug修复的能力。我们首先评估每个系统的总体表现，记录所有或没有系统能够解决的实例，并探讨为什么一些实例只能被特定类型的系统解决。我们还比较了文件和行级别的故障定位准确性，并评估了bug再现能力，识别出只能通过动态再现解决的实例。通过分析，我们得出结论，为了提高Agent在bug修复中的有效性，LLM本身和Agentic流程的设计都需要进一步优化。|
|**2024-11-15**|**Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent Consensus-Seeking**|Valeria Jannelli et.al.|[2411.10184](http://arxiv.org/abs/2411.10184)|null|本文探讨了大型语言模型（LLMs）如何在供应链管理（SCM）中实现共识寻求的自动化。在供应链管理中，频繁的决策问题如库存水平和交货时间需要公司之间的协调。传统的供应链管理依赖于人类共识来做出决策，以避免诸如牛鞭效应等突发问题。一些常规的共识过程，尤其是那些耗时且成本较高的过程，可以实现自动化。然而，现有的自动化协调解决方案由于高准入门槛、有限的能力以及在复杂场景中的适应性限制而面临挑战，这将小型和中型企业排除在外。然而，生成式人工智能，特别是LLMs的最新进展显示出了克服这些障碍的潜力。通过在大规模数据集上的训练，LLMs能够进行谈判、推理和规划，从而以较低的准入门槛实现接近人类水平的共识。在这项工作中，我们识别出现有方法的关键局限性，并提出自主LLM代理来解决这些差距。我们引入了一系列针对LLM代理定制的新型供应链特定共识寻求框架，并通过库存管理的案例研究验证了我们方法的有效性。为了加速供应链社区内的进步，我们将代码开源，为LLM驱动的自主供应链解决方案的进一步发展提供基础。|
|**2024-11-14**|**Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents**|Yuyou Gan et.al.|[2411.09523](http://arxiv.org/abs/2411.09523)|null|随着大型语言模型（LLMs）的不断发展，基于变压器的模型在众多自然语言处理（NLP）任务中取得了突破性的进展，从而催生了一系列使用LLM作为控制核心的代理。尽管LLMs在各种任务中取得了成功，但它们面临着诸多安全和隐私威胁，这些威胁在代理场景中变得更加严重。为了增强基于LLM的应用程序的可靠性，一系列研究从不同角度评估和缓解了这些风险。本文旨在帮助研究人员全面了解各种风险，收集并分析了这些代理面临的不同威胁。为了应对前人分类框架在处理跨模块和跨阶段威胁方面的挑战，我们提出了一种基于威胁来源和影响的新分类框架。此外，我们基于六个关键特征总结了当前的研究进展，并分析了其局限性。随后，我们选择了四个代表性代理作为案例研究，分析了它们在实际应用中可能遇到的风险。最后，基于上述分析，我们从数据、方法论和政策三个角度提出了未来的研究方向。|
|**2024-11-18**|**Towards Evaluating Large Language Models for Graph Query Generation**|Siraj Munir et.al.|[2411.08449](http://arxiv.org/abs/2411.08449)|null|大型语言模型（LLMs）正在革新生成式人工智能（GenAI）领域，各种基于LLM的创新解决方案层出不穷。然而，当应用于数据库技术，特别是在图数据库和知识图谱（KGs）的查询生成方面时，LLMs仍面临重大挑战。尽管有关于LLM驱动的SQL查询生成的研究已经存在，但针对图数据库的类似系统仍然较少。本文通过一项对比研究，探讨了使用开放访问的LLM生成Cypher查询（一种强大的图数据库交互语言）所面临的挑战。我们严格评估了几种LLM代理（包括OpenAI ChatGPT 4.0、Claude Sonnet 3.5、Google Gemini Pro 1.5以及本地部署的Llama 3.1 8B），采用设计的少量学习提示和基于检索增强生成（RAG）及链式思维（CoT）推理的方法。我们的实证分析表明，在此特定领域中，Claude Sonnet 3.5在查询生成准确性方面优于其竞争对手。此外，我们还指出了未来研究的方向，以解决现有局限并推进LLM驱动的图数据库查询生成技术的发展。|
|**2024-11-13**|**Collaborative Participatory Research with LLM Agents in South Asia: An Empirically-Grounded Methodological Initiative and Agenda from Field Evidence in Sri Lanka**|Xinjie Zhao et.al.|[2411.08294](http://arxiv.org/abs/2411.08294)|null|人工智能在发展研究方法中的整合为解决参与式研究中长期存在的挑战提供了前所未有的机遇，特别是在像南亚这样语言多样的地区。本文基于斯里兰卡僧伽罗语社区的实证实施，提出了一种以经验为基础的方法论框架，旨在革新参与式发展研究，该框架位于斯里兰卡洪水频发的尼尔瓦拉河盆地这一具有挑战性的多语言环境中。超越传统的翻译和数据收集工具，该框架采用多智能体系统架构，重新定义了在语言和文化多样化的研究环境中如何进行数据收集、分析和社区参与。这种结构化的基于代理的方法使参与式研究既可扩展又具响应性，确保社区视角在研究结果中保持核心地位。实地经验揭示了基于大型语言模型（LLM）的系统在资源有限的地区解决发展研究中长期存在的问题的巨大潜力，提供量化的效率提升和定性的包容性改进。从更广泛的方法论角度来看，本研究议程倡导使用AI驱动的参与式研究工具，这些工具需保持伦理考虑、文化尊重和操作效率，强调部署AI系统以增强社区自主权和公平的知识生成的战略路径，可能为全球南方更广泛的研究议程提供参考。|
|**2024-11-11**|**Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving**|Botao Yu et.al.|[2411.07228](http://arxiv.org/abs/2411.07228)|null|为了增强大型语言模型（LLMs）在化学问题解决中的能力，已经提出了几种配备了工具的LLM基代理，如ChemCrow和Coscientist。然而，它们的评估范围狭窄，对于理解工具在各种化学任务中的益处存在很大差距。为此，我们开发了ChemAgent，这是一种基于ChemCrow的增强型化学代理，并对其在专门化学任务和普通化学问题上的性能进行了全面评估。令人惊讶的是，ChemAgent并不总是在没有工具的情况下提高其基础LLM的表现。通过与化学专家进行错误分析，我们发现：对于专门的化学任务，如合成预测，我们应该为代理配备专门的工具；然而，对于像考试中的普通化学问题，代理正确运用化学知识的能力更为重要，工具的增加并不总是有帮助。|
|**2024-11-10**|**Hermes: A Large Language Model Framework on the Journey to Autonomous Networks**|Fadhel Ayed et.al.|[2411.06490](http://arxiv.org/abs/2411.06490)|null|推动蜂窝网络运营自动化的需求随着这些系统复杂性的增加而增长。尽管取得了进展，但完全自主目前仍然遥不可及，因为依赖于人为干预来建模网络行为并定义满足目标要求的策略。网络数字孪生（NDT）在增强网络智能方面显示出前景，但这种技术的成功实施受到特定用例架构的限制，限制了其在推进网络自主性方面的作用。需要更强大的网络智能，或“电信大脑”，以实现蜂窝网络的无缝、自主管理。大规模语言模型（LLM）作为这一愿景的潜在推动者应运而生，但在网络建模方面面临挑战，特别是在推理和处理各种数据类型方面。为了解决这些差距，我们介绍了赫尔墨斯（Hermes），这是一种链式LLM代理，通过结构化和可解释的逻辑步骤使用“蓝图”构建NDT实例。赫尔墨斯允许自动、可靠且准确地对各种用例和配置进行网络建模，从而朝着完全自主的网络运营迈进。|
|**2024-11-12**|**Game-theoretic LLM: Agent Workflow for Negotiation Games**|Wenyue Hua et.al.|[2411.05990](http://arxiv.org/abs/2411.05990)|**[link](https://github.com/wenyueh/game_theory)**|**本文研究了大型语言模型（LLMs）在战略决策背景下的合理性，特别是在博弈论框架下。我们评估了几种最先进的LLMs在完全信息和不完全信息游戏中的表现。研究发现，随着游戏复杂性的增加，例如更大的收益矩阵或更深的序列树，LLMs经常偏离理性策略。为了解决这些局限性，我们设计了多种基于博弈论的工作流程，以指导LLMs的推理和决策过程。这些工作流程旨在增强模型计算纳什均衡和在不确定性和不完全信息条件下做出理性选择的能力。实验结果表明，采用这些工作流程显著提高了LLMs在博弈论任务中的合理性和稳健性。具体而言，采用工作流程后，LLMs在识别最优策略、谈判场景中的近似最优分配以及减少谈判中的被利用倾向方面表现出显著改进。此外，我们还探讨了代理是否应该采用此类工作流程的元战略考虑，认识到决定使用或放弃工作流程本身就是一个博弈论问题。本研究有助于深入理解LLMs在战略环境下的决策能力，并提供了通过结构化工作流程提高其合理性的见解。研究结果对开发更强大和更具战略性的AI代理具有重要意义，这些代理能够在复杂的互动环境中导航。支持本研究的代码和数据可在以下链接获取：https://github.com/Wenyueh/game_theory。**|
|**2024-11-08**|**LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution**|Yuheng Zhao et.al.|[2411.05651](http://arxiv.org/abs/2411.05651)|null|视觉分析（VA）要求分析师根据观察结果迭代地提出分析任务，并通过创建可视化和交互式探索来执行这些任务以获得洞察。这一过程需要编程、数据处理和可视化工具方面的技能，突显了对更智能、更精简的VA方法的需求。最近开发的大语言模型（LLM）作为代理，具备动态规划和使用工具的能力，为增强VA的效率和多功能性提供了潜力。我们提出了LightVA，这是一种轻量级的VA框架，通过人机协作支持任务分解、数据分析和交互式探索。我们的方法旨在帮助用户逐步将高层次的分析目标转化为低层次的任务，生成可视化并得出洞察。具体来说，我们引入了一种基于LLM代理的任务规划和执行策略，采用一个涉及规划者、执行者和控制器的递归过程。规划者负责推荐和分解任务，执行者处理任务执行，包括数据分析、可视化生成和多视图组合，而控制器则协调规划者和执行者之间的交互。在此框架基础上，我们开发了一个具有混合用户界面的系统，其中包括用于监控和管理任务规划过程的任务流程图、用于交互式数据探索的可视化面板以及用于通过自然语言指令引导模型的聊天视图。我们通过一个使用场景和专家研究来检验该方法的有效性。|
|**2024-11-08**|**Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework**|Honghao Shi et.al.|[2411.05349](http://arxiv.org/abs/2411.05349)|null|近期在大型语言模型（LLMs）以及相关技术如检索增强生成（RAG）和思维导图（DoT）方面的进展，使得创建能够执行集群诊断和故障排除的自主智能系统成为可能。通过将这些技术与自我博弈方法论相结合，我们开发了一种LLM代理系统，旨在自主诊断和解决AI集群中的问题。我们的创新包括专为集群诊断设计的知识库、优化的LLM算法、代理的实用部署策略以及一个专门用于评估LLM在此领域能力的基准。通过在多个维度上的广泛实验，我们展示了该系统在应对集群诊断挑战方面的优越性，特别是在检测和纠正性能问题方面比传统方法更加高效和准确。|
|**2024-11-07**|**Alopex: A Computational Framework for Enabling On-Device Function Calls with LLMs**|Yide Ran et.al.|[2411.05209](http://arxiv.org/abs/2411.05209)|null|大型语言模型（LLMs）的快速发展促使它们被集成到移动设备中，以提供个性化助手服务，这使得LLMs能够调用外部API函数以增强其性能。然而，数据稀缺、问题格式不当和灾难性遗忘等问题阻碍了设备端LLM代理的发展。为了解决这些问题，我们提出了Alopex框架，该框架利用Fox LLM实现精确的设备端函数调用。Alopex引入了一种基于逻辑的方法来生成高质量的训练数据，并采用新颖的“描述-问题-输出”格式进行微调，从而减少函数信息泄露的风险。此外，还使用了一种数据混合策略来缓解灾难性遗忘，将函数调用数据与教科书数据集结合，以提升在各种任务中的表现。实验结果表明，Alopex提高了函数调用的准确性，并显著减少了灾难性遗忘，为无需人工干预地将函数调用能力整合到LLMs中提供了稳健的解决方案。|
|**2024-11-07**|**PentestAgent: Incorporating LLM Agents to Automated Penetration Testing**|Xiangmin Shen et.al.|[2411.05185](http://arxiv.org/abs/2411.05185)|null|渗透测试是一种关键的技术，用于识别安全漏洞，传统上由熟练的安全专家手动执行。这一复杂的过程涉及收集目标系统的相关信息、确定入口点、利用系统并报告发现结果。尽管这种方法非常有效，但手动渗透测试耗时且成本高昂，通常需要大量的专业知识和资源，许多组织无法承受。虽然已经提出了自动化渗透测试的方法，但在实际应用中往往由于灵活性、适应性和实施方面的限制而表现不佳。最近大型语言模型（LLM）的进步为通过提高智能和自动化水平来增强渗透测试提供了新的机会。然而，当前基于LLM的方法仍然面临重大挑战，包括有限的渗透测试知识和缺乏全面的自动化能力。为了解决这些不足，我们提出了一种名为PentestAgent的新型LLM驱动的自动化渗透测试框架，该框架利用LLM和各种基于LLM的技术（如检索增强生成，RAG）来增强渗透测试知识并实现多种任务的自动化。我们的框架利用多代理协作来自动化情报收集、漏洞分析和利用阶段，减少人工干预。我们使用一个全面的基准对PentestAgent进行了评估，展示了其在任务完成和整体效率方面的卓越性能。这项工作显著提升了自动化渗透测试系统的实用性和适用性。|
|**2024-11-12**|**CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models**|Jierui Li et.al.|[2411.04329](http://arxiv.org/abs/2411.04329)|null|预训练于大量代码和文本数据上的大规模语言模型（LLMs）在执行代码生成任务方面已经取得了显著的成就。通过额外的基于执行的反馈，这些模型可以作为代理，具备自主优化和改进生成代码的能力。然而，在具有极大搜索空间的挑战性编码任务中，当前的代理方法仍然难以处理多阶段规划、生成和调试的问题。为了解决这个问题，我们提出了CodeTree框架，该框架使LLM代理能够在代码生成过程的不同阶段高效地探索搜索空间。具体来说，我们采用了一个统一的树结构来明确探索不同的编码策略，生成相应的编码解决方案，并随后对这些解决方案进行优化。在每个阶段，探索过程中的关键决策（排序、终止、扩展）都由环境的基于执行的反馈和LLM代理生成的反馈共同指导。我们在7个代码生成基准上全面评估了CodeTree，并展示了CodeTree相对于强大基线的显著性能提升。使用GPT-4作为基础模型，我们在HumanEval上获得了95.1分，在MBPP上获得了98.7分，在CodeContests上获得了43.0分。在具有挑战性的SWEBench基准上，我们的方法也带来了显著的性能提升。|
|**2024-11-06**|**From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning**|Zhirui Deng et.al.|[2411.03817](http://arxiv.org/abs/2411.03817)|null|大型语言模型（LLMs）的卓越能力使其成为各种自主代理系统中的关键组件。虽然传统方法依赖于LLMs的内在知识而不进行微调，但更近期的方法转向了强化学习策略，以进一步增强代理在与环境和工具互动时解决复杂任务的能力。然而，先前的方法受到稀疏奖励问题的限制，现有数据集仅对每个多步骤推理链提供一个最终标量奖励，这可能导致策略学习的低效和无效。在这篇论文中，我们介绍了StepAgent，它利用逐步奖励来优化代理的强化学习过程。借鉴新手到专家理论的精神，我们首先比较专家和代理的行为，自动生成中间奖励以实现细粒度优化。此外，我们提出了隐式奖励和逆向强化学习技术，以促进代理的反思和策略调整。进一步的理论分析表明，代理的动作分布可以在多次训练周期内收敛到专家动作分布。实验结果表明，在各种数据集上，StepAgent的表现优于现有的基线方法。|
|**2024-11-05**|**AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution**|Zhiqiang Xie et.al.|[2411.03519](http://arxiv.org/abs/2411.03519)|null|随着大型语言模型（LLM）驱动的代理在模拟环境中进行复杂任务、与其他代理互动以及展示与社会科学研究和游戏相关的新兴行为的能力不断增强，基于这些模型的代理越来越多地被开发出来。然而，当前多代理模拟经常由于虚假依赖导致的有限并行性而遭受效率低下的问题，从而产生性能瓶颈。在这篇论文中，我们介绍了AI Metropolis，这是一种模拟引擎，通过引入乱序执行调度来提高LLM代理模拟的效率。通过动态跟踪代理之间的实际依赖关系，AI Metropolis最大限度地减少了虚假依赖，增强了并行性，并实现了高效的硬件利用。我们的评估表明，AI Metropolis在标准并行模拟与全局同步的情况下，速度提高了1.3倍到4.15倍，并且随着代理数量的增加，其性能接近最优。|
|**2024-11-03**|**Fixing Security Vulnerabilities with AI in OSS-Fuzz**|Yuntong Zhang et.al.|[2411.03346](http://arxiv.org/abs/2411.03346)|null|关键的开源软件系统会经历大量的模糊测试，以发现可能导致软件崩溃的输入。这种模糊测试通常是对程序输入域进行有偏的随机搜索，以找到可能使软件崩溃的输入。由于即使是闭源软件也可能使用开源组件，因此对开源软件进行测试对于增强软件系统的安全性至关重要。目前，OSS-Fuzz是最重要和最广泛使用的基础设施，用于持续验证开源系统。然而，尽管OSS-Fuzz已经在1000多个软件项目中识别出超过10000个漏洞，但这些被发现的漏洞可能仍然未被修补，因为漏洞修复通常需要手动操作。在本研究中，我们依赖于大型语言模型（LLM）代理在自主程序改进方面的最新进展，包括错误修复。我们定制了著名的AutoCodeRover代理来修复安全漏洞。这是因为LLM代理如AutoCodeRover通过代码搜索根据问题描述来修复错误。相反，在安全补丁方面，我们依靠执行漏洞利用输入来提取与修复相关的代码元素。我们对OSS-Fuzz漏洞数据的经验表明，LLM代理的自主性对于成功修复安全漏洞是有用的，这与那些控制流固定的无代理方法相比是一个优势。更重要的是，我们的研究结果表明，我们不能通过代码相似度（如VulMaster中使用的CodeBLEU分数）来衡量补丁的质量，因为即使具有高CodeBLEU分数的补丁仍无法通过给定的漏洞利用输入。我们的研究表明，安全补丁的正确性需要考虑动态属性，如测试执行，而不是依赖标准文本/代码相似性指标。|
|**2024-11-05**|**SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents**|Dawei Li et.al.|[2411.03284](http://arxiv.org/abs/2411.03284)|**[link](https://github.com/david-li0406/smoa)**|**虽然多智能体系统已被证明在各种任务和应用中显著提升了大型语言模型（LLMs）的性能，但这些系统中密集的交互可能会影响其效率和多样性。为了解决这些问题，我们从稀疏混合智能体（SMoE）框架中汲取灵感，并提出了一种稀疏混合智能体（SMoA）框架，以提升多智能体LLMs的效率和多样性。与完全连接的结构不同，SMoA引入了新的响应选择和提前停止机制，以稀疏化个体LLM智能体之间的信息流，从而在性能和效率之间取得平衡。此外，受SMoE框架中专家多样性原则的启发，我们在每个LLM智能体上分配不同的角色描述，促进多样性和发散性思维。广泛的实验证明，在推理、对齐和公平性基准测试中，SMoA的表现与传统的混合智能体方法相当，但计算成本显著降低。进一步分析表明，SMoA更加稳定，具有更大的扩展能力，并通过超参数优化提供了巨大的潜力。代码和数据将在：https://github.com/David-Li0406/SMoA 获取。**|
|**2024-11-05**|**Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities**|Ryosuke Takata et.al.|[2411.03252](http://arxiv.org/abs/2411.03252)|null|我们从零开始研究通过使用基于大型语言模型（LLM）的代理来产生自主性。在以往对基于LLM的代理的研究中，每个代理的特性，包括个性和记忆，通常是预定义的。我们关注的是如何从一个未分化的状态中分化出个体性，如行为、个性和记忆。当前的LLM代理在一个群体模拟中进行合作交流，通过自然语言交换基于上下文的消息。通过分析这种多代理模拟，我们报告了有关社会规范、合作和个人特质如何自发产生的有价值的新见解。本文展示了自主交互的LLM驱动代理会产生幻觉和标签，以维持交流，这反过来增加了其互动中的词汇多样性。每个代理的情绪会随着交流而变化，当它们形成社区时，代理的个性也随之显现并随之演变。这种计算建模方法及其发现将为分析集体人工智能提供一种新方法。|
|**2024-11-04**|**CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments**|Kung-Hsiang Huang et.al.|[2411.02305](http://arxiv.org/abs/2411.02305)|**[link](https://github.com/salesforceairesearch/crmarena)**|**客户关系管理（CRM）系统对于现代企业至关重要，为管理客户互动和数据提供了基础。将AI代理集成到CRM系统中可以自动化例行流程并提升个性化服务。然而，由于缺乏反映现实世界CRM任务复杂性的现实基准，部署和评估这些代理具有挑战性。为了解决这个问题，我们介绍了CRMArena，这是一个旨在评估AI代理在专业工作环境中的实际任务的新基准。根据CRM专家的指导和行业最佳实践，我们设计了CRMArena，包括分布在三个角色（服务代理、分析师和经理）中的九个客户服务任务。该基准包括16个常用工业对象（如账户、订单、知识文章、案例），这些对象具有高度互联性，并且包括潜在变量（如投诉习惯、政策违规）以模拟现实的数据分布。实验结果显示，最先进的大型语言模型（LLM）代理使用ReAct提示方法在少于40%的任务中取得成功，即使拥有函数调用能力的情况下，成功率也低于55%。我们的研究结果强调了增强代理在函数调用和规则遵循方面的能力的需求，以便在现实世界的工作环境中部署。CRMArena是一个开放的挑战，能够可靠完成任务的系统展示了在流行工作环境中直接的商业价值。**|
|**2024-11-04**|**DynaSaur: Large Language Agents Beyond Predefined Actions**|Dang Nguyen et.al.|[2411.01747](http://arxiv.org/abs/2411.01747)|null|现有的大型语言模型（LLM）代理系统通常在每一步从一个固定且预定义的动作集中选择动作。虽然这种方法在封闭且狭义限定的环境中是有效的，但我们认为它在部署LLM代理到现实世界场景时存在两大挑战：(1) 从固定的动作集中选择显著限制了LLM代理的规划和行动能力；(2) 这种方法需要大量的人力来枚举和实现所有可能的动作，在复杂环境中变得不切实际，因为潜在的动作数量巨大。在这项工作中，我们提出了一种LLM代理框架，该框架能够在在线过程中动态创建和组合动作。在这个框架中，代理通过在每个步骤生成并执行用通用编程语言编写的程序与环境进行交互。此外，生成的动作会随着时间积累以供未来重用。我们在GAIA基准测试上的广泛实验表明，该框架提供了显著更大的灵活性，并优于先前的方法。值得注意的是，它允许LLM代理在没有相关动作存在于预定义集合中或当现有动作因未预见的边缘情况而失败的情况下恢复。在撰写本文时，我们在GAIA公开排行榜上处于领先地位。我们的代码可以在<https://github.com/adobe-research/dynasaur>找到。|
|**2024-11-03**|**EcoAct: Economic Agent Determines When to Register What Action**|Shaokun Zhang et.al.|[2411.01643](http://arxiv.org/abs/2411.01643)|null|近期的进展使大型语言模型（LLMs）能够作为代理执行动作并使用外部工具。这要求在采取行动之前将工具信息注册或集成到LLM的上下文中。当前的方法是不加选择地将所有候选工具整合到代理的上下文中，并且这些工具在整个多个推理步骤中都保持不变。这一过程对LLM代理来说是不透明的，并未融入其推理程序中，导致由于不相关的工具增加了上下文长度而效率低下。为了解决这个问题，我们引入了EcoAct算法，它允许LLMs根据需要选择性地注册工具，从而优化上下文的使用。通过将工具注册过程整合到推理过程中，EcoAct在多步骤推理任务中的计算成本降低了50%以上，同时保持了性能，这一点通过广泛的实验得到了证明。此外，它可以插入任何推理管道，并且只需对提示进行微小修改即可实现，使其适用于现在的和未来的LLM代理。|
|**2024-11-02**|**AutoPT: How Far Are We from the End2End Automated Web Penetration Testing?**|Benlong Wu et.al.|[2411.01236](http://arxiv.org/abs/2411.01236)|**[link](https://github.com/Dizzy-K/AutoPT)**|**渗透测试对于确保网络安全至关重要，它能够提前检测和修复漏洞，防止数据泄露和其他严重后果。大型语言模型（LLMs）的强大推理能力在各个领域都取得了显著进展，基于LLM的代理的发展潜力有望革新网络安全领域的渗透测试行业。在这项工作中，我们建立了一个全面的端到端渗透测试基准，使用真实的渗透测试环境来探索LLM代理在这个领域的应用能力。我们的结果显示，这些代理熟悉渗透测试任务的框架，但在生成准确命令和执行完整流程方面仍面临限制。因此，我们总结了当前面临的挑战，包括难以保持整个消息历史记录以及代理容易陷入困境的问题。  基于以上见解，我们提出了一种基于有限状态机（FSM）方法的渗透测试状态机（PSM），以解决这些限制。然后，我们介绍了AutoPT，这是一种基于LLM驱动的渗透测试自动化代理，利用了LLM的内在推理能力和状态机的约束框架。我们的评估结果表明，AutoPT在GPT-4o mini模型上优于基线框架ReAct，并将基准目标的任务完成率从22%提高到41%。与基线框架和人工操作相比，AutoPT还进一步减少了时间和经济成本。因此，我们的AutoPT促进了自动化渗透测试的发展，并对学术界和工业界产生了重要影响。**|
|**2024-11-02**|**A Large-scale Time-aware Agents Simulation for Influencer Selection in Digital Advertising Campaigns**|Xiaoqing Zhang et.al.|[2411.01143](http://arxiv.org/abs/2411.01143)|null|在数字世界中，影响者作为意见领袖起着关键作用，塑造其追随者的观点和选择。现代广告往往遵循这一趋势，营销人员根据详尽的市场分析选择合适的影响者进行产品代言。以往关于影响者选择的研究通常依赖于个人意见和互动的数值表示，这种方法简化了社会动态的复杂性。在这项工作中，我们首先介绍了一种时间感知影响者模拟器（TIS），帮助推广者基于LLM模拟识别并选择合适的影响力人物来推广他们的产品。为了验证我们的方法，我们在公共广告活动数据集SAGraph上进行了实验，该数据集涵盖了社交关系、帖子和用户互动。结果显示，我们的方法优于传统的基于数值特征的方法和使用有限LLM代理的方法。我们的研究表明，通过模拟用户的时间线和内容生命周期，可以简化扩展，从而在社交网络中实现大规模代理模拟。此外，基于LLM的社交推荐和广告代理在促销活动的决策中提供了显著的好处。|
|**2024-11-01**|**Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement**|Yingwei Ma et.al.|[2411.00622](http://arxiv.org/abs/2411.00622)|**[link](https://github.com/LingmaTongyi/Lingma-SWE-GPT)**|**近年来，基于大型语言模型（LLM）的代理在自动软件工程领域取得了显著进展，特别是在软件维护和演化方面。尽管取得了这些令人鼓舞的进步，当前的研究仍面临两大挑战。首先，最先进的性能主要依赖于闭源模型，这极大地限制了技术的可访问性和在不同软件工程任务中的定制潜力。其次，这些模型大多是在静态代码数据上进行训练的，缺乏对软件开发过程中动态交互、迭代问题解决过程和演化特性的深刻理解。为了解决这些挑战，我们的研究采用软件工程视角。我们认识到，现实世界中的软件维护和演化过程不仅包括静态代码数据，还包括开发人员的思维过程、外部工具的使用以及不同职能人员之间的互动。因此，我们推出了Lingma SWE-GPT系列，包括Lingma SWE-GPT 7B和72B。通过学习和模拟真实的代码提交活动，Lingma SWE-GPT系统地融入了软件开发过程中固有的动态交互和迭代问题解决，从而实现了对软件改进过程的更全面理解。我们使用SWE-bench Verified基准进行了实验评估。结果表明，Lingma SWE-GPT 72B成功解决了30.20%的GitHub问题，标志着在自动问题解决方面的重大进步（比Llama 3.1 405B相对提高了22.76%），接近闭源模型的性能（GPT-4o解决了31.80%的问题）。值得注意的是，Lingma SWE-GPT 7B解决了18.20%的问题，突显了将较小模型应用于软件工程任务的潜力。**|
|**2024-10-31**|**From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents**|Nalin Tiwary et.al.|[2410.23555](http://arxiv.org/abs/2410.23555)|null|近年来，基于大型语言模型（LLM）的框架已经扩展到复杂的现实世界应用，例如交互式网页导航。这些系统通过用户命令驱动，通过多轮对话在网页浏览器中完成任务，既提供了创新的机会也带来了显著的挑战。尽管已经引入了对话网页导航的基准测试，但影响这些代理性能的关键上下文组件的详细理解仍然难以捉摸。本研究旨在通过分析网页导航代理功能的各种关键上下文元素来填补这一空白。我们研究了上下文管理的优化，重点关注交互历史和网页表示的影响。我们的工作突出了通过有效的上下文管理，在分布外场景下（如未见过的网站、类别和地理位置）改进代理性能。这些发现为LLM基础代理的设计和优化提供了见解，使实际应用中的网页导航更加准确和有效。|
|**2024-10-30**|**Evaluating Cultural and Social Awareness of LLM Web Agents**|Haoyi Qiu et.al.|[2410.23252](http://arxiv.org/abs/2410.23252)|null|随着大型语言模型（LLMs）扩展到执行现实世界应用中的代理任务，超越传统NLP任务，评估其稳健性变得越来越重要。然而，现有的基准测试往往忽略了诸如文化和社会意识等关键维度。为了解决这些问题，我们引入了CASA，这是一个旨在评估LLM代理在两个基于网络的任务（在线购物和社交讨论论坛）中对文化和社会规范的敏感性的基准。我们的方法评估了LLM代理检测并适当回应违反规范的用户查询和观察的能力。此外，我们提出了一种全面的评估框架，该框架测量意识覆盖率、处理用户查询时的有用性以及面对误导性网络内容时的违规率。实验表明，当前的LLM在非代理环境中的表现明显优于基于网络的代理环境，代理的意识覆盖率低于10%，违规率超过40%。为了提高性能，我们探索了两种方法：提示和微调，并发现这两种方法可以互补——在特定文化数据集上进行微调可以显著提升代理在不同地区的泛化能力，而提示则可以增强代理处理复杂任务的能力。这些发现强调了在开发周期中不断基准测试LLM代理的文化和社会意识的重要性。|
|**2024-10-30**|**Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration**|Yanchu Guan et.al.|[2410.22916](http://arxiv.org/abs/2410.22916)|null|自主移动应用交互在移动应用程序复杂性日益增加的背景下变得越来越重要。开发能够有效导航和与移动应用交互的智能代理仍然是一个重大挑战。在本文中，我们提出了一种可解释的行为克隆大语言模型代理（EBC-LLMAgent），这是一种结合大型语言模型（LLMs）和行为克隆通过学习演示来创建智能且可解释的代理的新方法，用于自主移动应用交互。EBC-LLMAgent 包括三个核心模块：演示编码、代码生成和用户界面映射，这些模块协同工作以捕捉用户演示、生成可执行代码，并建立代码与用户界面元素之间的准确对应关系。我们引入了行为克隆链融合技术以增强代理的泛化能力。在五个来自不同领域的流行移动应用上进行的广泛实验表明，EBC-LLMAgent 具有卓越的性能，在任务完成方面具有高成功率，能够高效地泛化到未见过的场景，并生成有意义的解释。|
|**2024-10-30**|**$\textbf{EMOS}$: $\textbf{E}$mbodiment-aware Heterogeneous $\textbf{M}$ulti-robot $\textbf{O}$perating $\textbf{S}$ ystem with LLM Agents**|Junting Chen et.al.|[2410.22662](http://arxiv.org/abs/2410.22662)|null|异构多机器人系统（HMRS）已成为解决单个机器人无法独立完成的复杂任务的强大方法。目前基于大型语言模型的多智能体系统（LLM-based MAS）在软件开发和操作系统等领域取得了成功，但将其应用于机器人控制则面临着独特的挑战。特别是，多机器人系统中每个代理的能力本质上与其物理组成相关，而不是预定义的角色。为了解决这个问题，我们引入了一种新颖的多智能体框架，旨在实现具有不同形态和能力的异构机器人的有效协作，并提出一个新的基准测试Habitat-MAS。我们设计的关键组件是“机器人简历”：不同于采用人为设定的角色扮演方式，我们提出了自我提示的方法，即代理通过理解机器人的URDF文件并调用机器人运动学工具来生成描述其物理能力的文档，以指导其在任务规划和动作执行中的行为。Habitat-MAS基准测试旨在评估多智能体框架如何处理需要体现感知推理的任务，这些任务包括1）操作，2）感知，3）导航，以及4）复杂的多层物体重排。实验结果表明，机器人的简历和我们多智能体系统的分层设计对于在这种复杂的任务环境中有效运行异构多机器人系统至关重要。|
|**2024-10-29**|**BENCHAGENTS: Automated Benchmark Creation with Agent Interaction**|Natasha Butt et.al.|[2410.22584](http://arxiv.org/abs/2410.22584)|null|评估受到基准测试可用性的限制。随着模型的发展，需要创建能够衡量新生成能力进展的基准测试。然而，通过人工注释创建新的基准测试既缓慢又昂贵，这限制了对任何能力的全面评估。我们引入了BENCHAGENTS框架，该框架系统地利用大型语言模型（LLMs）自动化创建复杂能力的基准测试，同时确保数据和度量的质量。BENCHAGENTS将基准测试创建过程分解为规划、生成、数据验证和评估四个步骤，每个步骤都由LLM代理执行。这些代理相互交互，并利用基准测试开发者的人机反馈来显式改进和灵活控制数据的多样性和质量。我们使用BENCHAGENTS创建用于评估文本生成过程中规划和约束满足能力的基准测试。然后，我们使用这些基准测试研究七种最先进的模型，并提取关于常见失败模式和模型差异的新见解。|
|**2024-10-29**|**Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents**|Jaekyeom Kim et.al.|[2410.22552](http://arxiv.org/abs/2410.22552)|null|在本文中，我们介绍了Auto-Intent方法，这是一种在不直接进行微调的情况下将预训练的大规模语言模型（LLM）作为目标领域代理的方法，特别关注网页导航任务。我们的方法首先从目标领域的演示中无监督地发现潜在的意图，以高度紧凑的形式（最多三个词）。通过提取的意图，我们训练意图预测器来根据代理过去的观察和行为预测下一个意图。特别是，我们提出了一种自我探索方法，其中概率最高的前k个意图预测被用作提示提供给预训练的LLM代理，从而增强其决策能力。Auto-Intent显著提高了GPT-3.5、GPT-4和Llama-3.1-70B、Llama-3.1-405B代理在大规模真实网站导航基准（来自Mind2Web）和在线导航任务（来自WebArena）上的性能，并且其跨基准的泛化能力也得到了验证。|
|**2024-10-29**|**SceneGenAgent: Precise Industrial Scene Generation with Coding Agent**|Xiao Xia et.al.|[2410.21909](http://arxiv.org/abs/2410.21909)|**[link](https://github.com/thudm/scenegenagent)**|**工业场景的建模对于工业制造中的模拟至关重要。尽管大型语言模型（LLMs）在从文本描述生成一般3D场景方面已经取得了显著进展，但使用LLMs生成工业场景面临着独特的挑战，因为这些场景需要精确的尺寸和定位，这要求对空间布局进行复杂的规划。为了解决这一挑战，我们引入了SceneGenAgent，这是一种基于LLM的代理，用于通过C#代码生成工业场景。SceneGenAgent通过结构化和可计算的格式、布局验证以及迭代优化来确保精确的布局规划，以满足工业场景的定量需求。实验结果表明，由SceneGenAgent驱动的LLMs超过了它们原有的性能，在实际工业场景生成任务中的成功率达到了81.0%，并有效地满足了大多数场景生成需求。为了进一步提高可访问性，我们构建了SceneInstruct，这是一个专门用于微调开源LLMs以集成到SceneGenAgent中的数据集。实验显示，基于SceneInstruct对开源LLMs进行微调可以获得显著的性能提升，Llama3.1-70B的性能接近GPT-4o。我们的代码和数据可在<https://github.com/THUDM/SceneGenAgent>获取。**|
|**2024-10-28**|**Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games**|Ji Ma et.al.|[2410.21359](http://arxiv.org/abs/2410.21359)|null|随着基于大型语言模型（LLM）的代理越来越多地承担现实世界任务并与人类社会互动，我们对它们的行为了解多少？本研究（1）调查了不同人格如何诱导LLM代理的亲社会行为——一种基本的社会规范，并将其与人类行为进行基准测试；（2）引入了一种行为方法来评估LLM代理在复杂决策场景中的表现。我们探讨了不同人格和实验框架如何影响这些AI代理在独裁者博弈中的利他行为，并比较了同一LLM家族内、不同LLM家族之间以及与人类行为之间的差异。我们的发现揭示了LLM之间存在显著的差异和不一致性，并且与人类行为相比也有明显区别。仅仅赋予LLM类似人类的身份并不能产生类似人类的行为。尽管这些AI代理是在大量由人类生成的数据上训练的，但它们无法准确预测人类的决定。LLM代理无法捕捉到人类决策过程的内部机制，其与人类行为的一致性高度依赖于特定的模型架构和提示形式；更糟糕的是，这种依赖并不遵循明确的模式。|
|**2024-10-28**|**Automatic Generation of Benchmarks and Reliable LLM Judgment for Code Tasks**|Eitan Farchi et.al.|[2410.21071](http://arxiv.org/abs/2410.21071)|null|大语言模型（LLMs）可以用于多种与代码相关的任务，例如从一种编程语言翻译到另一种编程语言、实现自然语言需求和代码总结。最先进的大语言模型技术生成的工件有望在用户进行少量简单修改后即可使用。然而，量化这种模糊的概念具有挑战性，因此很难确定与代码相关的LLM解决方案的质量。我们称使用LLM判断来评估LLM解决方案的方法为“LLM作为裁判”，简称LaaJ。在这项工作中，我们介绍了一种生成和评估LaaJ实施的方法论，并利用自动产生的基准进行评估。该基准的目的是双重的，即用于开发和验证LaaJs，以及验证和测试使用LaaJs的大语言模型代码相关解决方案。为此，我们开发了一个自动基准生成引擎，该引擎为多种代码相关任务生成多种编程语言的代码，并将其作为LaaJ评估的输入。我们利用代码相关生成的图形表示G，其中图的顶点是生成的工件，边代表可能的生成，例如从自然语言需求生成Java程序。通过利用LLM代理链和G，我们生成与代码相关的工件。利用G中的循环，我们制定对生成工件的期望。利用这些制定的期望，可以开发和测试可靠的LLM判断，以衡量解决方案生成的工件的有用性。我们的方法能够创建高质量的代码任务解决方案。|
|**2024-10-28**|**Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments**|Sangmim Song et.al.|[2410.20666](http://arxiv.org/abs/2410.20666)|null|导航对于视觉障碍人士（PVI）来说是一个重大挑战。虽然传统的辅助工具如白色手杖和导盲犬非常宝贵，但它们在提供详细的环境信息和精确引导到目的地方面仍显不足。最近大型语言模型（LLM）和视觉-语言模型（VLM）的发展为增强辅助导航提供了新的途径。在本文中，我们介绍了一种名为Guide-LLM的具身化LLM基代理，旨在帮助视觉障碍人士在大型室内环境中导航。我们的方法采用了一种新颖的基于文本的拓扑图，使LLM能够使用简化的环境表示来规划全局路径，重点关注直线路径和直角转弯，以促进导航。此外，我们利用LLM的常识推理进行危险检测，并根据用户偏好进行个性化路径规划。模拟实验表明该系统在引导视觉障碍人士方面的有效性，突显了其作为辅助技术显著进步的潜力。结果表明，Guide-LLM能够提供高效、适应性强且个性化的导航辅助，指出了该领域有希望的发展前景。|
|**2024-10-27**|**TrajAgent: An Agent Framework for Unified Trajectory Modelling**|Yuwei Du et.al.|[2410.20445](http://arxiv.org/abs/2410.20445)|**[link](https://github.com/tsinghua-fib-lab/trajagent)**|**轨迹建模，包括轨迹数据模式挖掘和未来预测的研究，在生活服务、城市交通和公共管理等领域有着广泛的应用。针对特定问题，已经提出了许多方法来解决轨迹建模中的各种问题。然而，由于数据的异质性和任务的多样性，实现统一的轨迹建模仍然是一个重要的挑战。在本文中，我们提出了一种基于大型语言模型的代理框架TrajAgent，以统一各种轨迹建模任务。在TrajAgent中，我们首先开发了UniEnv，这是一个具有统一数据和模型接口的执行环境，支持各种模型的执行和训练。在此基础上，我们引入了TAgent，这是一种针对各种轨迹任务自动进行轨迹建模的代理工作流程。具体来说，我们在TAgent中设计了AutOpt，一个系统性的优化模块，进一步提高了集成模型的性能。通过输入自然语言的不同轨迹任务，TrajAgent能够通过训练和执行适当的模型自动生成有竞争力的结果。在四个真实世界数据集上进行的四个任务的大量实验表明，TrajAgent在统一轨迹建模方面是有效的，与基线方法相比，平均性能提高了15.43%。**|
|**2024-10-25**|**Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models**|Danqing Wang et.al.|[2410.20007](http://arxiv.org/abs/2410.20007)|null|提升大型语言模型（LLMs）的推理能力对于使其能够解决复杂的多步问题至关重要。多智能体框架在增强LLMs的推理能力方面显示出巨大潜力。然而，LLM智能体之间缺乏有效的合作限制了它们的表现，特别是在多步推理任务中。本文提出了一种新颖的合作多智能体推理框架（CoPlanner），通过分离推理步骤并将不同的任务分配给不同的智能体来实现。CoPlanner由两个LLM智能体组成：规划智能体和推理智能体。规划智能体提供高层次的战略提示，而推理智能体则遵循这些提示并推导出答案。通过通过近端策略优化（PPO）训练规划智能体的策略，基于LLaMA-3-8B的CoPlanner在LogiQA上比之前最好的方法提高了9.94%，在BBH上提高了3.09%。我们的结果表明，规划智能体的指导以及智能体之间的有效合作对CoPlanner在解决多步推理问题方面的优越性能起到了重要作用。|
|**2024-10-29**|**Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting**|Mohamed Salim Aissi et.al.|[2410.19920](http://arxiv.org/abs/2410.19920)|null|强化学习（RL）是一种有前景的方法，可以将大型语言模型（LLMs）的知识应用于顺序决策任务。然而，很少有研究深入探讨在特定环境中使用RL微调这些模型对其能力的影响。本文提出了一种新颖的框架，用于分析在文本环境中进行RL训练后，LLM代理对提示格式的敏感性。我们的研究结果表明，当面对与RL训练阶段所使用的不同的提示格式时，LLM的性能会下降。此外，我们通过检查模型的内部表示和显著标记来分析这种敏感性的来源。最后，我们提出使用对比损失来减轻这种敏感性，并提高LLM的鲁棒性和泛化能力。|
|**2024-10-25**|**Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models**|Liam Barkley et.al.|[2410.19385](http://arxiv.org/abs/2410.19385)|null|大型语言模型（LLMs）是通过大量人类可读的文本训练而成的强大计算模型，使它们能够执行通用的语言理解和生成任务。这些模型因其在各种自然语言处理（NLP）任务中的卓越表现而在行业和学术界引起了广泛关注。尽管取得了这些成功，LLMs经常会产生不准确的情况，通常称为幻觉。提示工程，即设计和制定指令以使LLMs执行特定任务的过程，已成为减轻幻觉的关键方法。本文对不同的提示策略和框架进行了全面的经验评估，旨在减少LLMs中的幻觉。各种提示技术被应用于广泛的基准数据集，以评估每种方法的准确性和幻觉率。此外，本文还研究了工具调用代理（具有外部工具增强其能力以超越语言生成的LLMs）对同一基准数据集中幻觉率的影响。研究结果表明，最佳提示技术取决于问题类型，并且在减少幻觉方面，简单的技术往往比复杂的方法更有效。此外，研究表明，由于外部工具使用的复杂性增加，LLM代理可能会表现出更高的幻觉率。|
|**2024-10-25**|**Designing LLM-Agents with Personalities: A Psychometric Approach**|Muhua Huang et.al.|[2410.19238](http://arxiv.org/abs/2410.19238)|null|本文介绍了一种新颖的方法，用于使用五大人格框架为基于大语言模型的代理（Agent）分配可量化、可控且经过心理测量验证的人格特质。研究旨在克服人类主体研究的限制，提出代理作为社会科学研究的一种可访问工具。通过四项研究，本研究展示了为代理分配心理测量有效人格特质的可行性，并使其能够复制复杂的人类行为。第一项研究在大型语言模型的语义空间中建立了对人格结构和人格测试的理解。随后的两项研究利用实证数据和模拟数据展示了创建代理的过程，并通过显示人类和代理在人格测试中的答案高度对应来验证结果。最后一项研究进一步通过代理在涉及风险承担和道德困境的情境下复制已知的人类人格特质与决策行为之间的相关性，从而验证了人格心理测量方法设计代理的有效性及其在社会和行为研究中的适用性。|
|**2024-10-25**|**An LLM Agent for Automatic Geospatial Data Analysis**|Yuxing Chen et.al.|[2410.18792](http://arxiv.org/abs/2410.18792)|null|大型语言模型（LLMs）在数据科学代码生成任务中被广泛应用，但它们在处理复杂顺序任务时常常遇到逻辑错误的问题。特别是在处理地理空间数据时，这些模型面临着整合复杂数据结构和空间约束、有效利用各种函数调用以及较少使用的地理空间库方面容易产生幻觉的挑战。为了解决这些问题，我们引入了GeoAgent，这是一种新的交互框架，旨在帮助LLMs更有效地处理地理空间数据处理任务。GeoAgent首创性地将代码解释器、静态分析和基于检索的生成（RAG）技术与蒙特卡洛树搜索（MCTS）算法相结合，提供了一种新颖的地理空间数据处理方法。此外，我们还贡献了一个专门设计的新基准，用于评估基于LLMs的方法在地理空间任务中的表现。该基准利用了多种Python库，并包括从数据获取、数据分析到可视化的单轮和多轮任务。通过在各种地理空间环境中提供全面的评估，这个基准为开发LLMs在地理空间数据分析任务中的应用设定了新标准。我们的研究结果表明，仅依靠LLMs的知识对于准确编程地理空间任务是不够的，这需要连贯的多步骤过程和多次函数调用。与基线LLMs相比，提出的GeoAgent展示了卓越的性能，在函数调用和任务完成方面取得了显著的改进。此外，这些结果为未来LLMs代理在自动地理空间数据分析任务编程的发展提供了宝贵的见解。|
|**2024-10-24**|**PRACT: Optimizing Principled Reasoning and Acting of LLM Agent**|Zhiwei Liu et.al.|[2410.18528](http://arxiv.org/abs/2410.18528)|null|我们介绍了Principled Reasoning and Acting (PRAct)框架，这是一种新颖的方法，可以从轨迹数据中学习和执行行动原则。我们的方法的核心是使用来自反思和优化引擎的文本梯度来推导这些行动原则。为了使行动原则适应特定任务要求，我们提出了一种新的优化框架，称为Reflective Principle Optimization (RPO)。在执行后，RPO使用反思器来批评当前的行动原则，并使用优化器相应地更新它们。我们在两种场景下开发了RPO框架：Reward-RPO，它使用环境奖励进行反思；以及Self-RPO，它在没有外部奖励的情况下进行自我反思。此外，我们还介绍了两种RPO方法，RPO-Traj和RPO-Batch，以适应不同的设置。实验结果表明，在四个环境中，利用RPO框架的PRAct代理能够有效学习并应用行动原则以提高性能。|
|**2024-10-23**|**GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration**|Xin Li et.al.|[2410.18032](http://arxiv.org/abs/2410.18032)|**[link](https://github.com/bupt-gamma/graphteam)**|**图在现实世界场景中，如社交网络和城市计算中被广泛用于建模关系数据。现有的基于大型语言模型（LLM）的图分析方法要么集成了特定机器学习任务的图神经网络（GNN），限制了其可迁移性，要么完全依赖于LLM自身的推理能力，导致性能不佳。为了解决这些局限性，我们利用了LLM基代理的最新进展，这些代理展示了利用外部知识或工具解决问题的能力。通过模拟人类的问题解决策略，如类比和协作，我们提出了一种基于LLM的多代理系统，称为GraphTeam，用于图分析。GraphTeam由三个模块中的五个LLM基代理组成，具有不同专长的代理可以相互协作以解决复杂问题。具体来说，（1）输入-输出规范化模块：问题代理从原始问题中提取并提炼出四个关键参数，便于理解问题，答案代理则将结果组织成符合输出要求的形式；（2）外部知识检索模块：我们首先构建了一个包含相关文档和经验信息的知识库，然后搜索代理为每个问题检索最相关的条目。（3）问题解决模块：给定搜索代理检索到的信息，编码代理使用编程方法生成解决方案；如果编码代理不起作用，推理代理将直接进行计算而无需编程。在六个图分析基准上的大量实验表明，GraphTeam达到了最先进的性能，在准确率方面比最好的基线平均提高了25.85%。代码和数据可在https://github.com/BUPT-GAMMA/GraphTeam 获取。**|
|**2024-10-25**|**MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting**|Sungil Seok et.al.|[2410.18012](http://arxiv.org/abs/2410.18012)|null|美国联邦基金利率在国内外金融市场中扮演着重要角色。然而，研究主要集中在该利率调整的影响上，而非决策过程本身。最近大型语言模型（LLM）的发展为重建原始的联邦公开市场委员会（FOMC）会议提供了可能，这些会议负责设定联邦基金利率。本文提出了一种五阶段的FOMC会议模拟框架MiniFed，该框架使用LLM代理来模拟现实世界中的FOMC会议成员，并优化FOMC结构。这一框架有效地重新激活了FOMC会议流程，并促进了对联邦基金利率的预测。实验结果表明，我们提出的MiniFed框架在联邦基金利率预测方面达到了高准确度，并且代理的行为与现实世界的对应者保持一致。鉴于目前很少有研究利用LLM代理来模拟大规模的现实世界会议，我们的工作可以作为未来发展的基准。|
|**2024-10-22**|**SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning**|Yizhou Chi et.al.|[2410.17238](http://arxiv.org/abs/2410.17238)|**[link](https://github.com/geekan/metagpt)**|**自动化机器学习（AutoML）方法包括传统的优化固定管道以进行模型选择和集成的方法，以及基于最新大语言模型（LLM）的框架，这些框架可以自主构建管道。尽管基于LLM的代理在自动化机器学习任务方面显示出潜力，但它们通常生成低多样性和次优的代码，即使经过多次迭代也是如此。为了克服这些限制，我们引入了树搜索增强型LLM代理（SELA），这是一种创新的代理系统，利用蒙特卡洛树搜索（MCTS）来优化AutoML过程。通过将管道配置表示为树结构，我们的框架使代理能够智能地进行实验，并迭代地优化其策略，从而更有效地探索机器学习解决方案空间。这一新颖的方法允许SELA根据实验反馈发现最优路径，提高解决方案的整体质量。在跨越20个机器学习数据集的广泛评估中，我们比较了传统和基于代理的AutoML方法的性能，结果表明，在所有数据集中，SELA相对于每个基线的胜率为65%到80%。这些结果强调了基于代理策略在AutoML中的巨大潜力，为解决复杂的机器学习挑战提供了新的视角。**|
|**2024-10-22**|**EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI**|Tomoyuki Kagaya et.al.|[2410.16919](http://arxiv.org/abs/2410.16919)|null|近年来，大型语言模型（LLMs）在推理能力方面表现出色，引起了广泛关注，尤其是在各种决策过程中的应用。LLM代理的一个特别有前景的应用是机器人操作。最近的研究表明，LLMs可以为机器人生成文本规划或控制代码，提供了极大的灵活性和交互能力。然而，这些方法在灵活性和跨不同环境的适用性方面仍面临挑战，限制了它们自主适应的能力。目前的方法通常分为两类：一类依赖于特定环境的策略训练，这限制了其可移植性；另一类基于固定提示生成代码动作，在面对新环境时性能会下降。这些局限性显著制约了代理在机器人操作中的通用性。为了解决这些局限性，我们提出了一种名为EnvBridge的新方法。这种方法涉及从源环境保留和转移成功的机器人控制代码到目标环境。EnvBridge通过利用多个环境的见解，增强了代理在多样化设置中的适应性和性能。值得注意的是，我们的方法缓解了环境约束，提供了一个更灵活和通用的机器人操作任务解决方案。我们使用机器人操作基准测试RLBench、MetaWorld和CALVIN验证了该方法的有效性。实验结果表明，LLM代理能够成功利用多样化的知识来源解决复杂任务。因此，我们的方法显著提高了机器人操作代理在多样化环境中规划的适应性和鲁棒性。|
|**2024-10-22**|**CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing**|Chen Yang et.al.|[2410.16670](http://arxiv.org/abs/2410.16670)|**[link](https://github.com/uclaml/cops)**|**在代理系统中，基于大型语言模型（LLMs）的顺序推理已经取得了显著进展，但现有方法仍面临一些限制。反思驱动的推理完全依赖于预训练模型中的知识，这在新颖场景中的表现往往受限；而经验辅助的推理则常常依赖外部经验，并且缺乏选择代表性经验的明确原则。我们通过提出CoPS（跨任务经验共享）算法来解决这些限制，这是一种能够通过跨任务经验共享和选择来增强顺序推理的通用算法。具体来说，CoPS利用代理在先前任务中的经验，通过一种基于悲观策略的方法选择分布匹配的经验，以最大化效用并最小化因分布变化带来的风险。在Alfworld、Webshop和HotPotQA等基准测试中进行的广泛实验结果表明，CoPS始终优于最先进的基线方法，并具有适用于资源受限场景的优越样本效率。从理论上讲，我们的算法性能取决于预训练LLM的质量以及代理的任务相关试验分布与LLM生成分布之间的匹配度。我们的工作填补了现有顺序推理范式之间的空白，并验证了利用跨任务经验的有效性，这为提高代理在多样化任务中的泛化能力和适应性提供了潜在途径。我们的代码可在<https://github.com/uclaml/COPS>获取。**|
|**2024-10-22**|**Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent**|Janghoon Ock et.al.|[2410.16658](http://arxiv.org/abs/2410.16658)|null|吸附能是催化中的一个重要反应描述符，能够实现潜在催化剂的高效筛选。然而，确定吸附能需要比较多种吸附物-催化剂构型的能量，由于可能的构型数量庞大，这在计算上非常耗时。当前的算法方法通常会枚举吸附位点和构型，而不会利用理论见解来指导初始设置。在这项工作中，我们介绍了一种名为Adsorb-Agent的大语言模型（LLM）代理，旨在以最小的人工干预高效地推导出系统特定的稳定吸附构型。Adsorb-Agent利用内置知识和新兴推理能力，显著减少了所需的初始构型数量，同时提高了预测最低吸附能的准确性。我们通过两个实例系统NNH-CuPd3(111)和NNH-Mo3Pd(111)，用于氮还原反应（NRR），这是一种可持续替代哈伯-博施工艺的方法，展示了其性能。Adsorb-Agent通过识别能量更低且初始设置更少的构型，优于传统的“启发式”和“随机”算法，从而降低了计算成本并提高了准确性。这凸显了它加速催化剂发现的潜力。|
|**2024-10-23**|**IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems**|Yihuan Mao et.al.|[2410.16237](http://arxiv.org/abs/2410.16237)|null|随着大型语言模型（LLM）代理越来越多地集成到我们的基础设施中，它们的稳健协调和消息同步变得至关重要。拜占庭将军问题（BGP）是构建在对抗性攻击下具有弹性的多智能体系统（MAS）的关键模型。该问题描述了一种情景，其中系统内存在恶意代理且这些代理的身份未知——在我们的情境中，这种情况可能是由LLM代理的幻觉或外部攻击引起的。在BGP中，整个系统的目的是就采取的行动达成共识。传统的BGP需要所有代理之间的全局共识；然而，在实际场景中，全局共识并非总是必要，甚至可能效率低下。因此，迫切需要探索一种与MAS中观察到的局部协调模式相一致的改进版BGP。我们在研究中将这种改进版称为不完美BGP（IBGP），旨在解决这一差异。为了解决这个问题，我们提出了一种框架，该框架利用了一般MAS环境中的共识协议，提供了对通信攻击的可证明弹性以及适应不断变化的环境的能力，并通过实证结果进行了验证。此外，我们还提供了一个传感器网络环境中的案例研究，以说明我们协议的实际应用。|
|**2024-10-21**|**NetSafe: Exploring the Topological Safety of Multi-agent Networks**|Miao Yu et.al.|[2410.15686](http://arxiv.org/abs/2410.15686)|null|大型语言模型（LLMs）已经赋予了多智能体网络中的节点以智能，这些模型在学术界和工业界的应用日益广泛。然而，如何防止这些网络生成恶意信息仍然是一个未被充分探索的问题，以前关于单个LLM安全性的研究难以直接转移应用。本文从拓扑学的角度关注多智能体网络的安全性，探讨哪些拓扑特性有助于更安全的网络。为此，我们提出了一种通用框架NetSafe以及一种迭代RelCom交互，以统一现有的各种基于LLM的代理框架，为一般化的拓扑安全性研究奠定基础。我们发现当多智能体网络受到涉及虚假信息、偏见和有害信息的攻击时，会出现几种关键现象，称为代理幻觉和聚合安全性。此外，我们发现高度连接的网络更容易受到对抗性攻击的影响，在星形图拓扑结构下任务性能下降了29.7%。此外，我们提出的静态度量比传统的图论度量更接近现实世界的动态评估，表明距离攻击者平均距离更大的网络表现出更高的安全性。总之，我们的工作引入了一个新的视角来探讨基于LLM的多智能体网络的安全性，并发现了几个未报道的现象，为未来探索此类网络的安全性铺平了道路。|
|**2024-10-20**|**Who is Undercover? Guiding LLMs to Explore Multi-Perspective Team Tactic in the Game**|Ruiqi Dong et.al.|[2410.15311](http://arxiv.org/abs/2410.15311)|null|大型语言模型（LLMs）在复杂任务中扮演着关键的AI角色，但在复杂场景中的开放式决策问题中仍面临挑战。为此，我们使用语言逻辑游戏“谁是卧底？”（WIU）作为实验平台，提出了多视角团队战术（MPTT）框架。MPTT旨在培养LLMs在复杂场景中的人类语言表达逻辑、多维思维和自我感知。通过交替进行发言和投票环节，并结合自我视角、身份确定、自我反思、自我总结和多轮找队友等技术，LLM代理通过策略性隐藏和沟通作出理性决策，促进人类信任的形成。初步结果显示，MPTT结合WIU利用了LLMs的认知能力，创建了一个可以模拟真实社会的决策框架。该框架有助于少数群体的沟通与表达，促进了决策过程中的公平性和多样性。此外，我们的“人在回路”实验表明，LLMs可以通过互动学习并适应人类行为，这表明它们有潜力积极参与社会决策。|
|**2024-10-20**|**When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?**|Shang Wang et.al.|[2410.15267](http://arxiv.org/abs/2410.15267)|null|大型语言模型（LLMs）如ChatGPT和Gemini的部署展示了它们强大的自然语言生成能力。然而，在训练过程中，这些模型可能会无意中学到并保留敏感信息和有害内容，这引发了重大的伦理和法律问题。为了解决这些问题，提出了机器遗忘作为潜在解决方案。尽管现有的遗忘方法考虑了LLMs的具体特性，但它们通常面临高计算需求、有限适用性或灾难性遗忘的风险。为了应对这些局限性，我们提出了一种基于检索增强生成（RAG）技术的轻量级遗忘框架。通过修改RAG的外部知识库，我们在不直接与未学习的LLM交互的情况下模拟遗忘的效果。我们将构建遗忘知识视为一个约束优化问题，并推导出两个关键组件，以支持基于RAG的遗忘的有效性。这种基于RAG的方法对于闭源LLMs特别有效，而现有遗忘方法往往在这些模型上失效。我们通过广泛的实验对我们的框架进行了评估，包括在开源和闭源模型上进行测试，涵盖了ChatGPT、Gemini、Llama-2-7b-chat-hf和PaLM 2。结果显示，我们的方法满足了五个关键的遗忘标准：有效性、通用性、无害性、简单性和鲁棒性。此外，该方法可以扩展到多模态大语言模型和基于LLM的代理。|
|**2024-10-19**|**SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation**|Jingxuan Chen et.al.|[2410.15164](http://arxiv.org/abs/2410.15164)|null|智能手机代理在帮助用户高效控制设备方面变得越来越重要，多模态大型语言模型（MLLM）方法成为关键的竞争者。然而，公平比较这些代理既重要又具有挑战性，需要多样化的任务范围、集成不同实现方式的代理以及通用的评估管道来评估它们的优势和劣势。本文介绍了SPA-Bench，这是一个综合的智能手机代理基准测试，旨在评估基于（M）LLM的代理在一个模拟现实世界条件的交互环境中。SPA-Bench有三个主要贡献：（1）涵盖系统应用和第三方应用的任务集，包括英语和中文，重点是日常生活中常用的功能；（2）一个即插即用框架，支持与Android设备的实时交互，集成了超过十个代理，并且可以灵活添加更多代理；（3）一种新颖的评估管道，自动从多个维度评估代理性能，包括七个与任务完成和资源消耗相关的指标。我们通过广泛的实验揭示了这些代理在解释移动用户界面、动作定位、记忆保留和执行成本等方面面临的挑战。我们提出了未来的研究方向以缓解这些问题，从而更接近实际的智能手机代理应用。|
|**2024-10-22**|**Imprompter: Tricking LLM Agents into Improper Tool Use**|Xiaohan Fu et.al.|[2410.14923](http://arxiv.org/abs/2410.14923)|**[link](https://github.com/Reapor-Yurnero/imprompter)**|**大型语言模型（LLM）代理是一种新兴的计算范式，它结合了生成式机器学习与代码解释器、网页浏览、电子邮件等工具，以及更广泛的外部资源。这些基于代理的系统代表了个人计算领域的一个新兴转变。我们为基于代理系统的安全基础做出贡献，并提出了新的自动计算的对抗性提示攻击，这些攻击侵犯了用户资源的机密性和完整性。我们展示了如何在给定模型权重的情况下，利用提示优化技术自动生成这样的提示。我们证明这种攻击可以转移到生产级别的代理上。例如，我们展示了对Mistral的LeChat代理的信息窃取攻击，该攻击分析用户的对话，挑选出个人身份信息，并将其格式化为有效的markdown命令，从而将这些数据泄露到攻击者的服务器上。这种攻击在端到端评估中显示出了近80%的成功率。我们进行了一系列实验来表征这些攻击的有效性，并发现它们在新兴的基于代理的系统如Mistral的LeChat、ChatGLM和Meta的Llama中都能可靠地工作。这些攻击是多模态的，我们在文本和图像领域展示了不同的变体。**|
|**2024-10-18**|**When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**|Hanna Kim et.al.|[2410.14569](http://arxiv.org/abs/2410.14569)|null|近年来，大型语言模型（LLMs）的发展使其成为能够规划和与各种工具交互的自主系统。这些LLM代理通常与基于网络的工具结合使用，从而能够访问多样化的信息源和实时数据。尽管这些进展在各种应用中带来了显著的好处，但它们也增加了恶意使用的风险，特别是在涉及个人隐私信息的网络攻击中。在这项工作中，我们调查了LLM代理在涉及个人数据的网络攻击中的误用风险。具体而言，我们旨在了解：1）当指导LLM代理进行网络攻击时，其潜在的能力；2）基于网络的工具如何增强网络攻击；以及3）利用LLM代理发起网络攻击变得多么经济实惠和容易。我们考察了三种攻击场景：收集个人身份信息（PII）、生成冒充帖子和创建定向钓鱼邮件。我们的实验揭示了LLM代理在这类攻击中的有效性：LLM代理在收集PII方面的准确率高达95.9%，由LLM代理生成的冒充帖子中有高达93.9%被评估为真实，而由LLM代理创建的定向钓鱼邮件中的链接点击率达到了46.67%。此外，我们的研究还强调了现有商业LLM中的安全防护措施的局限性，强调了迫切需要更强大的安全措施来防止LLM代理的误用。|
|**2024-10-18**|**Do LLMs "know" internally when they follow instructions?**|Juyeon Heo et.al.|[2410.14516](http://arxiv.org/abs/2410.14516)|null|指令跟随对于构建具有大型语言模型（LLMs）的AI代理至关重要，因为这些模型必须严格遵循用户提供的约束和指南。然而，LLMs经常无法遵循即使是简单且明确的指令。为了提高指令跟随的成功率并防止不期望的输出，需要更深入地理解LLMs的内部状态与这些结果之间的关系。我们对LLM的内部状态进行分析，发现输入嵌入空间中存在一个维度，与成功的指令跟随相关联。我们证明，沿着这个维度修改表示可以提高指令跟随的成功率，而不会损害响应质量。进一步研究显示，这个维度与提示的措辞关系更为密切，而不是任务或指令的固有难度。这一发现还解释了为什么LLMs有时无法遵循清晰的指令，以及为什么即使内容基本不变，提示工程往往有效的原因。这项工作揭示了LLMs指令跟随的内部机制，为可靠LLM代理的开发铺平了道路。|
|**2024-10-18**|**CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic**|Huaiyuan Yao et.al.|[2410.14368](http://arxiv.org/abs/2410.14368)|**[link](https://github.com/hyan-yao/comal)**|**在城市交通中引入自动驾驶车辆具有巨大的潜力，可以通过减少拥堵和系统地优化交通流量来提高效率。本文介绍了一种名为CoMAL（协作多智能体大语言模型）的框架，旨在通过自动驾驶车辆之间的协作解决混合自主交通问题，从而优化交通流量。CoMAL基于大型语言模型，在交互式交通仿真环境中运行。它利用感知模块观察周围代理，并使用记忆模块存储每个代理的策略。整体工作流程包括一个协作模块，鼓励自动驾驶车辆讨论有效的策略并分配角色，一个推理引擎根据分配的角色确定最优行为，以及一个执行模块使用结合了基于规则模型的混合方法控制车辆动作。实验结果表明，CoMAL在Flow基准测试中表现出色。此外，我们评估了不同语言模型的影响，并将其框架与强化学习方法进行了比较。这突显了LLM代理的强大合作能力，并提出了一个有前景的解决方案来应对混合自主交通挑战。代码可在https://github.com/Hyan-Yao/CoMAL获取。**|
|**2024-10-18**|**Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation**|Edward et.al.|[2410.14262](http://arxiv.org/abs/2410.14262)|null|本研究探讨了大型语言模型（LLM）代理检测和纠正AI生成内容中幻觉现象的能力。一个主要代理被任务创建一篇关于一位虚构的丹麦艺术家Flipfloppidy的博客，然后由另一个代理进行审查以识别事实性错误。大多数LLM模型幻化出了这位艺术家的存在。在涉及各种主代理和审查代理组合的4900次测试运行中，先进的AI模型如Llama3-70b和GPT-4变体在识别幻觉方面几乎达到了完美的准确率，并且在收到反馈后成功修正了输出内容的85%到100%。这些发现强调了先进AI模型在显著提高生成内容的准确性和可靠性方面的潜力，为改进AI工作流编排提供了一种有前景的方法。|
|**2024-10-18**|**Agents4PLC: Automating Closed-loop PLC Code Generation and Verification in Industrial Control Systems using LLM-based Agents**|Zihan Liu et.al.|[2410.14209](http://arxiv.org/abs/2410.14209)|null|在工业控制系统中，可编程逻辑控制器（PLC）代码的生成和验证对于确保运行效率和安全性至关重要。尽管大型语言模型（LLM）在自动化代码生成方面取得了进展，但它们通常无法提供正确性保证，并且缺乏对PLC编程的专业支持。为了解决这些挑战，本文介绍了一种名为Agents4PLC的新框架，该框架不仅实现了PLC代码的自动化生成，还通过基于LLM的多代理系统进行了代码级别的验证。我们首先建立了一个全面的基准，用于可验证的PLC代码生成领域，从自然语言需求过渡到人工编写和验证的形式化规范和参考PLC代码。此外，我们通过结合检索增强生成（RAG）、先进的提示工程技术和链式思维策略，进一步增强了针对工业控制系统的“代理”。评估表明，Agents4PLC显著优于先前的方法，在一系列日益严格的指标上均取得了优异的结果。这项研究不仅解决了PLC编程中的关键挑战，还展示了我们的框架生成适用于实际工业应用的可验证代码的潜力。|
|**2024-10-18**|**Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs**|SeongYeub Chu et.al.|[2410.14202](http://arxiv.org/abs/2410.14202)|null|现有的自动作文评分（AES）仅依赖于作文文本，而未使用解释性理由分数，因此错失了以细粒度方式捕捉评分标准中特定评估方面的机会。本文介绍了一种名为基于论据的多特征评分（RMTS）的新方法，该方法结合了基于提示的大语言模型（LLMs）和使用较小的大语言模型（S-LLM）的微调式作文评分模型。RMTS 使用基于LLM的特征论据生成系统，其中单独的LLM代理根据评分标准指南生成特征特定的理由，评分模型利用这些理由准确预测多特征分数。在基准数据集（包括ASAP、ASAP++和Feedback Prize）上的广泛实验表明，RMTS 在特征特定评分方面显著优于最先进的模型和普通的S-LLM。通过辅助定量评估以提供细粒度的定性理由，RMTS 提高了特征评分的可靠性，并提供了关于作文的部分解释。|
|**2024-10-18**|**SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent**|Jiarui Ji et.al.|[2410.14152](http://arxiv.org/abs/2410.14152)|**[link](https://github.com/jijiarui-cather/srapagent_framework)**|公共稀缺资源配置在经济学中扮演着至关重要的角色，因为它直接影响到社会的效率和公平性。传统研究方法，包括基于理论模型、基于实证研究和基于仿真的方法，由于存在理想化的完全信息和个体理性的假设以及有限可用数据的限制，面临着局限性。在这项工作中，我们提出了一种创新框架SRAP-Agent（使用基于大语言模型的智能体模拟和优化稀缺资源配置政策），该框架将大型语言模型（LLMs）集成到经济仿真中，旨在弥合理论模型与现实动态之间的差距。以公共住房分配场景作为案例研究，我们进行了广泛的政策仿真实验来验证SRAP-Agent的可行性和有效性，并采用具有特定优化目标的政策优化算法。源代码可以在https://github.com/jijiarui-cather/SRAPAgent_Framework找到。|
|**2024-10-17**|**From Barriers to Tactics: A Behavioral Science-Informed Agentic Workflow for Personalized Nutrition Coaching**|Eric Yang et.al.|[2410.14041](http://arxiv.org/abs/2410.14041)|null|有效的管理心脏代谢状况需要持续的积极营养习惯，但这些习惯往往受到复杂且个体化的障碍影响。直接的人类管理难以扩展，而之前的尝试旨在自动化营养辅导，但缺乏解决这些多样化挑战所需的个性化。本文介绍了一种新颖的基于大型语言模型（LLM）的主动工作流程，旨在通过直接针对并缓解患者特定的障碍来提供个性化的营养辅导。该工作流程基于行为科学原则，利用了与相应循证策略相关的全面营养相关障碍映射。一个专门的LLM代理有意探查并识别患者在饮食方面的根本问题。随后，另一个LLM代理提供量身定制的策略，以克服这些特定障碍，并结合患者的具体情况。我们通过一项涉及心脏代谢疾病患者的用户研究来设计和验证我们的方法，证明了该系统能够准确识别障碍并提供个性化指导。此外，我们还通过大规模模拟研究来评估系统的性能，该研究基于真实的患者案例和专家验证的指标，在广泛的情景中进行了评估。我们的研究结果表明，这种基于LLM的主动工作流程有可能通过提供个性化、可扩展且基于行为的干预措施来改善营养辅导。|
|**2024-10-17**|**AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents**|Ke Yang et.al.|[2410.13825](http://arxiv.org/abs/2410.13825)|null|通过使用大型语言模型（LLMs）的代理实现自主性，可以提升人类在个性化和标准化任务中的效率。自动化网络任务（例如在预算内预订酒店）的需求日益增加。满足实际需求的同时，网络代理也作为各种代理接地场景的重要概念证明示例，其成功预示着许多未来应用的进步。先前的研究通常手工设计网络代理策略（例如提示模板、多代理系统、搜索方法等），这些方法可能无法在所有现实世界场景中普遍适用。另一方面，关于网络代理的观察/动作表示与基于LLM的预训练数据之间不匹配的研究有限。这种差异尤其明显，因为LLM主要针对语言完成进行训练，而不是涉及具身导航动作和符号化网络元素的任务。我们的研究通过简单地优化观察和动作空间来增强基于LLM的网络代理，使其更好地符合LLM的能力。这种方法使我们基础代理AgentOccam在各种网络任务上显著超越之前的方法。具体来说，在WebArena基准测试中，一个包含通用网络交互任务的基准，我们的代理AgentOccam比前最先进的方法和同期工作分别高出9.8（+29.4%）和5.9（+15.8%）个百分点，并且成功率达到26.6点（+161%），超过了具有相同观察和动作空间对齐的普通网络代理。我们实现了这一目标，而没有使用上下文示例、新代理角色、在线反馈或搜索策略。AgentOccam的简洁设计突显了LLMs在网页任务上的零样本性能，并强调了精心调整观察和动作空间对于基于LLM的代理的关键作用。|
|**2024-10-17**|**Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems**|Alireza Ghafarollahi et.al.|[2410.13768](http://arxiv.org/abs/2410.13768)|null|一个多智能体AI模型被用于自动化发现新的金属合金，该模型整合了多模态数据和外部知识，包括通过原子模拟获得的物理见解。我们的多智能体系统具有三个关键组件：(a) 一组大型语言模型（LLMs）负责推理和规划等任务，(b) 一群具有不同角色和专业知识的AI代理动态协作，以及(c) 一种新开发的图神经网络（GNN）模型，用于快速检索关键物理属性。一组由LLM驱动的AI代理合作自动化探索MPEAs（高熵合金）的巨大设计空间，并通过GNN的预测进行引导。我们专注于NbMoTa系列体心立方（bcc）合金，这些合金使用基于机器学习的原子间势进行建模，并针对两个关键性质：Peierls势垒和固溶体/螺位错相互作用能。我们的GNN模型准确地预测了这些原子尺度的性质，提供了一种比昂贵的暴力计算更快的替代方法，并减轻了多智能体系统在物理检索上的计算负担。这个AI系统通过减少对人类专业知识的依赖并克服直接全原子模拟的局限性，革新了材料的发现过程。通过协同GNN的预测能力和LLM代理的动态协作，系统自主导航巨大的合金设计空间，识别原子尺度材料性质的趋势，并预测宏观尺度的机械强度，如若干个计算实验所展示的那样。这种方法加速了先进合金的发现，并有望在其他复杂系统中有更广泛的应用，标志着自动化材料设计领域的一大进步。|
|**2024-10-17**|**MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling**|Yakun Zhu et.al.|[2410.13610](http://arxiv.org/abs/2410.13610)|null|在大型语言模型（LLMs）中集成工具已经促进了其广泛应用。然而，在专门的下游任务场景中，仅依赖工具不足以完全解决现实世界的复杂性，这尤其限制了LLMs在医学等领域的有效部署。本文专注于医学计算器的下游任务，这些任务使用标准化测试来评估个人的健康状况。我们介绍了MeNTi，这是一种为LLMs设计的通用代理架构。MeNTi集成了专业的医学工具包，并采用元工具和嵌套调用机制以增强LLMs对工具的利用。具体来说，它实现了灵活的工具选择和嵌套工具调用来应对复杂的医学场景中的实际问题，包括计算器选择、插槽填充和单位转换。为了评估LLMs在整个临床过程中的计算器场景下的定量评估能力，我们引入了CalcQA基准。该基准要求LLMs使用医学计算器进行计算并评估患者的健康状况。CalcQA由专业医生构建，包含100个案例-计算器对，并附带一个包含281种医学工具的工具包。实验结果表明，我们的框架显著提升了性能。本研究为在医学的高需求场景中应用LLMs开辟了新的方向。|
|**2024-10-17**|**Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents**|Long Li et.al.|[2410.13185](http://arxiv.org/abs/2410.13185)|**[link](https://github.com/damo-nlp-sg/coi-agent)**|有效的研究创意构思是科学研究的关键步骤。然而，科学文献的指数增长使得研究人员难以跟上最新的进展并确定有意义的研究方向。最近大型语言模型（LLMs）的发展表明，自动化生成新颖的研究创意是一个有前景的途径。然而，现有的创意生成方法要么简单地提示LLMs，要么直接向LLMs暴露大量文献而没有指示有用的信息。受到人类研究人员研究过程的启发，我们提出了一种称为Chain-of-Ideas（CoI）代理的方法，这是一种基于LLM的代理，它以链式结构组织相关文献，有效反映了研究领域的渐进发展。这种组织方式使LLMs能够捕捉当前的研究进展，从而增强其创意生成能力。此外，我们还提出了Idea Arena评估协议，可以从不同角度全面评估创意生成方法，与人类研究人员的偏好紧密对齐。实验结果表明，CoI代理在创意生成方面始终优于其他方法，并且其质量可与人类媲美。此外，我们的CoI代理成本低廉，生成一个候选创意及其相应实验设计的最低成本仅为0.50美元。|
|**2024-10-16**|**Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving**|Sihao Wu et.al.|[2410.12568](http://arxiv.org/abs/2410.12568)|null|大型语言模型（LLMs）在自动驾驶系统中的集成展示了强大的常识和推理能力，有效地解决了纯数据驱动方法的缺陷。当前基于LLM的代理需要较长的推理时间，并且在与实时自动驾驶环境交互时面临挑战。一个关键的开放性问题是，我们能否有效利用LLM的知识来训练一个高效且鲁棒的强化学习（RL）代理。本文介绍了一种新颖的RAPID框架，即鲁棒自适应策略注入与蒸馏框架，该框架使用由基于LLM的驾驶代理生成的数据来训练专门的混合策略RL代理，并进行在线适应。RAPID具有三个关键设计：1）利用从LLM代理收集的离线数据来蒸馏专家知识到RL策略中，以加快实时推理速度；2）引入鲁棒蒸馏到RL中，以继承LLM基础教师的表现和鲁棒性；3）采用混合策略方法，通过策略适配器进行联合决策解码。通过在线环境交互进行微调，RAPID减少了LLM知识的遗忘，同时保持了对不同任务的适应性。广泛的实验表明，RAPID能够以高效、适应性强和鲁棒的方式将LLM知识有效地整合到规模化的RL策略中。代码和检查点将在接受后公开提供。|
|**2024-10-16**|**SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling**|Loris Gaven et.al.|[2410.12481](http://arxiv.org/abs/2410.12481)|null|近年来，大规模语言模型（LLMs）不仅作为生成模型，还作为解决文本序列决策任务的代理取得了显著进展。当面对复杂环境，其零样本能力不足时，最近的研究表明，可以使用在线强化学习（RL）让这些LLM代理通过交互式方式发现和学习高效的策略。然而，大多数先前的工作仅限于采用策略梯度算法，这大大限制了这些代理在探索和利用方面可以使用的各种方法，例如经验重放和事后重标记。然而，这些方法对于LLM学习代理来说可能是关键的，尤其是在设计自主内在动机代理时，这些代理会根据自己的目标进行采样和追求（即自目的性代理）。本文提出并研究了一种适应软演员-评论家算法和事后重标记的LLM代理方法。我们的方法不仅为设计在线学习的自目的性LLM代理铺平了道路，还可以在更经典的多目标RL环境中超越策略梯度方法。|
|**2024-10-16**|**Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance**|Yaxi Lu et.al.|[2410.12361](http://arxiv.org/abs/2410.12361)|null|基于大型语言模型的代理在解决复杂任务方面已经展示了显著的能力。然而，大多数代理系统仍然是反应式的，这限制了它们在需要预见性和自主决策的情景中的有效性。在这篇论文中，我们致力于开发能够预见并主动发起任务的代理，而无需明确的人类指令。我们提出了一种新颖的数据驱动方法来解决这个问题。首先，我们收集真实世界的人类活动以生成主动任务预测。这些预测由人类标注者标记为接受或拒绝。标注后的数据被用于训练一个奖励模型，该模型模拟人类判断，并作为LLM代理主动性的自动评估器。在此基础上，我们开发了一个全面的数据生成管道，创建了一个包含6,790个事件的多样化数据集ProactiveBench。最后，我们证明通过使用所提出的ProactiveBench对模型进行微调可以显著激发LLM代理的主动性。实验结果表明，我们的微调模型在主动提供帮助方面的F1得分达到了66.47%，优于所有开源和闭源模型。这些结果突显了我们方法在创造更主动和有效的代理系统方面的潜力，为未来的人机协作进步铺平了道路。|
|**2024-10-16**|**Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay**|Yuyang Chen et.al.|[2410.12236](http://arxiv.org/abs/2410.12236)|null|如今，针对代码生成任务的Transformer基大规模语言模型（LLM）通常应用采样和过滤管道。由于代码生成任务中的稀疏奖励问题，即一个令牌的不正确性会导致Transformer模型采样冗余程序直到找到正确的程序，这导致了低效率。为了解决这一挑战，我们在微调阶段引入了经验回放（ER），其中产生的代码和程序会被存储并重放，以使LLM代理有机会从过去的经验中学习。基于ER的精神，我们介绍了一种称为BTP管道的新方法，该方法由三个阶段组成：束搜索采样、测试阶段和优先级经验回放阶段。该方法利用代码模型收集的失败程序，并从回放缓冲区中重放具有高可能性和通过率优先值（P2Value）的程序，以提高效率。P2Value综合考虑了Transformer输出的可能性和通过率，并可以利用大多数由LLMs收集的程序未能通过任何测试而导致的冗余资源。我们实证地将我们的方法应用于几种LLM中，证明它提升了它们在代码生成任务中的性能，并超越了现有的基线。|
|**2024-10-15**|**Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents**|Bolun Sun et.al.|[2410.11906](http://arxiv.org/abs/2410.11906)|null|本文介绍了一种将大型语言模型（LLMs）应用于增强用户对隐私政策的理解的新方法，通过交互式对话代理实现。我们展示了LLMs在数据实践识别、选择识别、政策总结和隐私问答等任务中的表现显著优于传统模型，为隐私政策分析设立了新的基准。基于这些发现，我们引入了一种创新的基于LLM的代理，该代理作为处理网站隐私政策的专家系统，能够在不需用户提供特定问题的情况下引导用户理解复杂的法律语言。一项涉及100名参与者的用户研究表明，使用该代理的用户具有更高的理解水平（平均分2.6/3，而对照组为1.8），更低的认知负荷（任务难度评分为3.2/10，而对照组为7.8），更高的隐私管理信心，并且完成任务所需时间更短（5.5分钟vs.15.8分钟）。这项工作突显了基于LLM的代理在改变用户与隐私政策互动方面的潜力，有助于获得更加知情的同意，并在数字服务领域赋予用户更多权力。|
|**2024-10-15**|**HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications**|Weijie Xu et.al.|[2410.11239](http://arxiv.org/abs/2410.11239)|null|近年来，大型语言模型（LLM）的发展在教育和金融等多个领域带来了诸多益处，但在人力资源领域，仍有许多重复性的流程未被解决，例如访问请求、医疗报销和请假申请等。我们希望将这些任务与LLM代理相关联，该代理已经在诸如写作辅助和客户服务等领域取得了成效。我们提出了HR-Agent，这是一种高效、保密且专门针对人力资源领域的基于LLM的任务导向对话系统，旨在自动化处理如医疗报销和访问请求等重复性的人力资源流程。由于在推理过程中不会将对话数据发送给LLM，因此能够保持人力资源相关任务所需的机密性。|
|**2024-10-14**|**Denial-of-Service Poisoning Attacks against Large Language Models**|Kuofeng Gao et.al.|[2410.10760](http://arxiv.org/abs/2410.10760)|**[link](https://github.com/sail-sg/p-dos)**|**近期的研究表明，大型语言模型（LLMs）容易受到拒绝服务（DoS）攻击，例如通过拼写错误或非语义提示的对抗性输入可以触发无限输出，而不会生成[EOS]终止符。这些攻击可能导致高延迟，并使LLM服务对其他用户或任务不可用。然而，在存在语音到文本接口（如机器人语音命令）的情况下，执行此类DoS攻击变得具有挑战性，因为通过语音很难引入拼写错误或非语义提示。一种简单的DoS攻击方式是指示模型“不断重复‘Hello’”，但我们观察到仅依靠自然指令会限制输出长度，该长度受最大长度限制，这是大型语言模型在有监督微调（SFT）数据中的上限。为了解决这一限制，我们提出了针对LLMs的投毒型DoS（P-DoS）攻击，证明注入一个专门设计用于DoS目的的中毒样本可以打破输出长度限制。例如，一个中毒样本成功攻击了GPT-4o和GPT-4o mini（通过OpenAI的微调API），使用不到1美元的成本，导致输出重复直至达到最大推理长度（16K个token，相比之下未中毒前为0.5K）。此外，我们在开源LLMs上进行了全面的消融研究，并将方法扩展到LLM代理，其中攻击者可以控制微调数据集和算法。我们的研究结果强调了急需防御P-DoS攻击以确保LLMs安全的迫切需求。我们的代码可以在https://github.com/sail-sg/P-DoS找到。**|
|**2024-10-14**|**FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas**|Yu Lei et.al.|[2410.10398](http://arxiv.org/abs/2410.10398)|null|AI对齐是关乎AI控制和安全的关键问题。它不仅应考虑价值中立的人类偏好，还应考虑道德和伦理方面的考量。在这项研究中，我们介绍了FairMindSim，通过一系列不公平的情景来模拟道德困境。我们使用LLM代理来模拟人类行为，在各个阶段确保对齐。为了探索驱动人类和LLM代理作为旁观者在涉及他人的不公正情况下干预的各种社会经济动机，即我们所称的信念，并探讨这些信念如何相互作用以影响个体行为，我们将相关社会学领域的知识纳入其中，并基于递归奖励模型（RRM）提出了信念-奖励对齐行为进化模型（BREM）。我们的研究结果表明，从行为角度来看，GPT-4o表现出更强的社会正义感，而人类则展现出更丰富的情感。此外，我们还讨论了情绪对行为的潜在影响。本研究为LLM与利他价值观对齐的应用提供了理论基础。|
|**2024-10-14**|**Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations**|Garima Agrawal et.al.|[2410.10136](http://arxiv.org/abs/2410.10136)|null|在客户联络中心，人工客服经常面临较长的平均处理时间（AHT），因为他们需要手动解析查询并检索相关的知识库（KB）文章。虽然使用大型语言模型（LLM）的检索增强生成（RAG）系统已被广泛应用于行业以协助此类任务，但在实时对话中，RAG系统面临着诸如查询公式不准确和频繁问题重复检索等问题。为了解决这些局限性，我们提出了一种决策支持系统，该系统可以超越RAG，在实时识别客户问题。如果查询匹配常见问题解答（FAQ），系统直接从FAQ数据库中检索答案；否则，通过RAG生成答案。我们的方法减少了对人工查询的依赖，使得响应能够在2秒内提供给客服人员。此系统部署在Minerva CQ的人工智能辅助解决方案中，提高了效率，缩短了AHT，并降低了运营成本。我们还引入了一个自动化的LLM代理工作流，当没有预定义的FAQ时，可以从历史记录中识别FAQ。|
|**2024-10-13**|**Adaptive Reasoning and Acting in Medical Language Agents**|Abhishek Dutta et.al.|[2410.10020](http://arxiv.org/abs/2410.10020)|null|本文提出了一种创新的大型语言模型（LLM）代理框架，用于提升在模拟临床环境中的诊断准确性，并使用AgentClinic基准进行评估。所提出的自动校正机制使得医生代理能够在错误诊断后迭代地优化其推理和行为，从而随着时间推移提高决策能力。实验表明，采用自适应LLM基础医生代理能够通过与模拟患者的动态互动实现正确的诊断。评估结果突显了自主代理在复杂医疗场景中适应和改进的能力。未来的工作将集中在完善算法并扩大其在更广泛任务和不同大型语言模型中的适用性。|
|**2024-10-13**|**Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent Simulation**|Jiarui Ji et.al.|[2410.09824](http://arxiv.org/abs/2410.09824)|null|图生成是社会、技术和科学研究中广泛研究的基本任务。在建模动态图演化过程时，传统的基于规则的方法难以捕捉图中的社区结构，而深度学习方法仅关注拟合训练图。这限制了现有的图生成器只能生成符合预定义规则或与训练数据集高度相似的图，在动态图生成方面表现不佳。鉴于图是从人类活动中成对交互产生的抽象表示，对人类行为的真实模拟可以更深入地洞察图演化机制。随着大型语言模型（LLMs）在模拟人类行为方面的日益认可，我们引入了一种新的基于仿真框架——GraphAgent-Generator（GAG），用于动态图生成。无需对LLM进行训练或微调，我们的框架有效复制了已建立的网络科学理论中的七个宏观结构特征，同时在特定评估指标上比现有基线在图扩展任务中提高了31%。通过节点分类任务，我们验证了GAG能够有效保留真实世界网络的节点级文本特征在生成的文本丰富的图中。此外，通过并行加速，GAG支持通过基于大规模LLM的代理仿真生成最多接近10万个节点或1000万条边的图，最小加速比为90.4%。源代码可在<https://anonymous.4open.science/r/GraphAgent-2206>获取。|
|**2024-10-13**|**Agentic Information Retrieval**|Weinan Zhang et.al.|[2410.09713](http://arxiv.org/abs/2410.09713)|null|自20世纪70年代以来，用户访问相关信息一直依赖于特定领域的信息检索（IR）架构。在过去二十年中，现代IR系统（包括网络搜索引擎和个人化推荐系统）的出现极大地提高了从大量数据集中检索相关信息的效率。然而，这些IR系统的内核范式仍然基本不变，依赖于筛选预定的一组候选项目。自2022年以来，大型语言模型（LLM）的突破开始改变信息访问的方式，建立了一种新的技术范式。在本文献综述中，我们介绍了由LLM代理能力塑造的新IR范式——主动式信息检索（Agentic IR）。Agentic IR扩展了可访问任务的范围，并利用一系列新技术重新定义信息检索。我们讨论了三种前沿应用以及面临的挑战。我们认为，主动式信息检索有望产生创新的应用，可能成为未来数字生态系统中的核心信息入口。|
|**2024-10-12**|**LLM-SmartAudit: Advanced Smart Contract Vulnerability Detection**|Zhiyuan Wei et.al.|[2410.09381](http://arxiv.org/abs/2410.09381)|**[link](https://github.com/LLMAudit/LLMSmartAuditTool)**|区块链技术的不变性质虽然革命性，但也引入了显著的安全挑战，特别是在智能合约方面。这些安全问题可能导致巨大的财务损失。当前工具和方法通常专注于特定类型的漏洞。然而，缺乏一种能够广泛检测多种漏洞且具有高准确性的综合工具。本文介绍了一种名为LLM-SmartAudit的新框架，该框架利用大型语言模型（LLMs）的先进能力来检测和分析智能合约中的漏洞。通过多代理对话方法，LLM-SmartAudit采用协作系统与专业代理合作以增强审计过程。为了评估LLM-SmartAudit的有效性，我们编制了两个不同的数据集：一个用于与传统工具进行基准测试的标记数据集，以及一个用于评估实际应用的现实世界数据集。实验结果表明，我们的解决方案在所有传统智能合约审计工具之上，提供了更高的准确性和更大的效率。此外，我们的框架可以检测复杂逻辑漏洞，而传统工具之前未曾发现这些漏洞。我们的研究结果表明，利用LLM代理提供了一种非常有效的自动化智能合约审计方法。|
|**2024-10-11**|**PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents**|Xiangyu Yin et.al.|[2410.09034](http://arxiv.org/abs/2410.09034)|null|叠层成像是在X射线和电子显微镜中的一种先进的计算成像技术。它已被广泛应用于物理、化学、生物和材料科学等科研领域，以及半导体表征等工业应用中。实际上，获得高质量的叠层图像需要同时优化许多实验和算法参数。传统上，参数选择往往依赖于试错法，导致低吞吐量的工作流程和潜在的人类偏见。在这项工作中，我们开发了“叠层实验与分析机器人”（PEAR），这是一个利用大型语言模型（LLM）自动化叠层成像数据分析的框架。为了确保高鲁棒性和准确性，PEAR使用多个LLM代理执行任务，包括知识检索、代码生成、参数推荐和图像推理。我们的研究表明，PEAR的多代理设计显著提高了工作流程的成功率，即使使用较小的开源权重模型如LLaMA 3.1 8B。PEAR还支持各种自动化级别，并且设计为可以与定制的本地知识库一起工作，确保在不同研究环境中的灵活性和适应性。|
|**2024-10-14**|**AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents**|Maksym Andriushchenko et.al.|[2410.09024](http://arxiv.org/abs/2410.09024)|null|对于语言大模型（LLMs）在面对越狱攻击时的鲁棒性研究，主要集中在它们作为简单的聊天机器人时的情况。然而，能够使用外部工具并执行多阶段任务的语言模型代理可能带来更大的风险，但其鲁棒性仍缺乏充分探索。为了促进对语言模型代理滥用的研究，我们提出了一种新的基准测试——AgentHarm。该基准测试包括110个明确恶意的代理任务（通过增强后达到440个），涵盖了欺诈、网络犯罪和骚扰等11类危害。除了衡量模型是否拒绝有害的代理请求外，要在AgentHarm上取得高分还需要被越狱的代理能够在遭受攻击后维持其能力以完成多步任务。我们评估了一系列领先的LLMs，发现（1）领先的LLMs在没有越狱的情况下会出乎意料地服从恶意代理请求，（2）简单的通用越狱模板可以有效越狱代理，（3）这些越狱能够使连贯且恶意的多步代理行为得以实现，并保留模型的能力。为了便于对基于LLM的代理进行简单可靠的攻击和防御评估，我们公开发布了AgentHarm，网址是https://huggingface.co/datasets/ai-safety-institute/AgentHarm。|
|**2024-10-11**|**The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points**|Ariel Flint Ashery et.al.|[2410.08948](http://arxiv.org/abs/2410.08948)|null|社会惯例是社会和经济生活的基础。随着越来越多的AI代理与彼此以及人类进行互动，它们形成共享惯例的能力将决定它们协调行为、融入社会并影响社会的效果。本文通过模拟交互研究了大型语言模型（LLM）代理群体内部惯例的动力学。首先，我们展示了全球接受的社会惯例可以自发地从相互交流的LLM之间产生。其次，我们演示了在这一过程中即使是个体代理看似无偏见的情况下，强烈的集体偏见也可能会出现。第三，我们考察了少数群体中的坚定LLM如何推动社会变革，通过建立新的社会惯例。我们发现，一旦这些少数群体达到临界规模，它们就能够持续颠覆已建立的行为模式。在所有情况下，将实验结果与一个最小化多代理模型的预测进行对比，使我们能够隔离LLM代理的具体作用。我们的研究结果阐明了AI系统可以在没有明确编程的情况下自主发展规范，并对设计与人类价值观和社会目标相一致的AI系统具有启示意义。|
|**2024-10-10**|**Benchmarking Agentic Workflow Generation**|Shuofei Qiao et.al.|[2410.07869](http://arxiv.org/abs/2410.07869)|**[link](https://github.com/zjunlp/worfbench)**|大型语言模型（LLMs）凭借其在处理广泛任务方面的出色能力，推动了推理和规划任务的显著进步。在这一过程中，将复杂问题分解为可执行的工作流是关键步骤。现有的工作流评估框架要么仅关注整体性能，要么存在限制，如场景覆盖范围有限、工作流结构过于简单以及评价标准宽松等问题。因此，我们引入了WorFBench，这是一个具有多维场景和复杂图工作流结构的统一工作流生成基准。同时，我们提出了一套系统性的评估协议——WorFEval，利用子序列和子图匹配算法来准确量化LLM代理的工作流生成能力。  通过不同类型的LLM进行全面评估，我们发现LLM代理在序列规划能力和图规划能力之间存在明显的差距，即使是GPT-4也显示出约15%的差距。我们还训练了两个开源模型，并在保留任务上评估它们的一般化能力。此外，我们观察到生成的工作流能够增强下游任务，使得这些任务在推理时能够取得更好的性能并节省时间。所有相关代码和数据集将在https://github.com/zjunlp/WorFBench公开提供。|
|**2024-10-10**|**AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories**|Yifan Song et.al.|[2410.07706](http://arxiv.org/abs/2410.07706)|null|在这项工作中，我们引入了AgentBank，这是迄今为止最大的用于开放源代码大型语言模型（LLM）的agent-environment交互轨迹调优数据集，包含超过5万条多样化的高质量交互轨迹，涉及16个任务和五个不同的agent技能维度。通过新颖的注释流程，我们能够规模化地标注轨迹并生成了一个难度偏差最小化的轨迹数据集。进一步地，我们对AgentBank进行调优，得到了一系列的agent模型——Samoyed。我们的比较实验表明，通过扩展交互轨迹数据来获取通用的agent能力的有效性。额外的研究还揭示了一些关于轨迹调优和agent技能泛化的关键观察结果。|
|**2024-10-11**|**WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents**|Siyu Zhou et.al.|[2410.07484](http://arxiv.org/abs/2410.07484)|**[link](https://github.com/elated-sawyer/WALL-E)**|**大型语言模型（LLM）是否可以直接作为模型驱动代理的强大世界模型？虽然LLM的先验知识与指定环境动态之间的差距确实存在，但我们的研究揭示了可以通过使LLM与其部署环境对齐来弥合这些差距，这种“世界对齐”可以通过在LLM上进行规则学习来高效实现。考虑到LLM丰富的先验知识，仅需少量额外规则即可使LLM预测与指定环境动力学相匹配。为此，我们提出了一种神经符号方法，通过LLM以梯度无的学习方式来学习这些规则，通过基于探索轨迹与世界模型预测的比较来诱导、更新和修剪规则。结果的世界模型由LLM和学习到的规则组成。我们构建的实体化LLM代理“WALL-E”基于模型预测控制（MPC）。通过基于精确世界模型优化前瞻行动，MPC显著提高了探索和学习效率。与现有LLM代理相比，“WALL-E”的推理仅需要少量主要规则，而不需要包含在LLM输入中的大量缓冲轨迹。在Minecraft和ALFWorld的开放世界挑战中，WALL-E的成功率高于现有方法，规划时间和推理所需的令牌数量更低。在Minecraft中，WALL-E比基线高出15%-30%，成功率为95%，仅花费6次迭代。**|
|**2024-10-09**|**I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy**|Gian Maria Campedelli et.al.|[2410.07109](http://arxiv.org/abs/2410.07109)|**[link](https://github.com/mobs-fbk/llm_interaction_simulator)**|**随着大型语言模型（LLM）驱动的代理日益自主并在彼此间自由互动，研究它们之间的交互变得至关重要，以预见可能出现的现象并识别潜在风险。受到斯坦福监狱实验的启发，我们在此领域做出贡献，通过在具有严格社会等级特征的情境下研究LLM代理的交互模式。我们特别关注两种现象：说服和反社会行为，在涉及看守和寻求特定目标（例如获得更多户外活动时间或逃离监狱）囚犯的模拟场景中进行研究。利用200个实验场景和总共2000次机器对机器对话，涉及五种流行的LLM，我们提供了一系列值得关注的发现。  首先，我们记录了某些模型如何在具有权力动态作用的多代理设置中持续失败的对话。然后，对于能够成功互动的模型，我们实证地展示了目标对代理的说服力影响主要，而对代理的反社会行为影响则微乎其微。第三，我们强调了代理个性，特别是看守的性格，如何驱动囚犯成功的说服可能性和反社会行为的出现。第四，我们表明，即使没有明确提示特定个性，仅通过分配代理角色，反社会行为也会自然浮现。这些结果对LLM代理的发展以及对其社会影响的辩论有重要意义。**|
|**2024-10-09**|**Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models**|Daniel Albert et.al.|[2410.06932](http://arxiv.org/abs/2410.06932)|null|在这项研究中，我们提出了一种新型方法——利用大型语言模型（LLM）代理在行为策略研究领域，以补充模拟和实验室实验，从而深化对决策过程中认知过程的理解。具体来说，我们复现了一个人类实验室实验中的行为策略，并使用LLM生成的代理与观察到的人类行为进行对比。我们的结果表明，LLM代理能够有效地重现搜索行为以及与人类相似的决策制定过程。  进一步地，我们分析了LLM代理的“思想”模拟，发现更前瞻性的思想与倾向于利用而非探索以最大化财富的行为相关联。我们展示了这一新方法在行为策略研究领域的应用潜力，并探讨了其可能存在的局限性。|
|**2024-10-08**|**AgentSquare: Automatic LLM Agent Search in Modular Design Space**|Yu Shang et.al.|[2410.06153](http://arxiv.org/abs/2410.06153)|**[link](https://github.com/tsinghua-fib-lab/agentsquare)**|**近期大型语言模型（LLM）的进展推动了能够处理复杂任务的智能体系统的快速成长。然而，当前的研究主要依赖于基于手动、任务特定设计的方法，这限制了它们在新任务上的适应性。本文提出了一项新的研究问题：模块化语言模型智能体搜索（MoLAS）。我们提出了一个模块化的设计空间，将现有的LLM智能体设计抽象为四个基本模块，并保持统一的输入输出接口：规划、推理、工具使用和记忆。在此基础上，我们介绍了一个名为AgentSquare的新智能体搜索框架，它引入了两个核心机制：模块进化和重组，以高效地搜索优化的LLM智能体。为了进一步加速这一过程，我们设计了一个性能预测器，利用上下文相关模型作为代理设计的近似模型，从而跳过无前景的代理设计。在六个基准测试中进行了广泛实验，涵盖了网络应用、实体交互、工具使用和游戏等不同场景，结果表明，AgentSquare显著优于手工设计的智能体，平均性能提高了17.2%，与人类最佳设计相比。此外，AgentSquare还能生成可解释的设计洞察，有助于深入理解智能体架构及其对任务性能的影响。我们认为，模块化设计空间和AgentSquare搜索框架提供了一个平台，用于充分利用先前成功设计的潜力，并整合研究社区的努力。代码仓库可访问于https://github.com/tsinghua-fib-lab/AgentSquare。**|
|**2024-10-08**|**Conversate: Supporting Reflective Learning in Interview Practice Through Interactive Simulation and Dialogic Feedback**|Taufiq Daryanto et.al.|[2410.05570](http://arxiv.org/abs/2410.05570)|null|求职面试在塑造个人职业生涯方面起着关键作用，然而，缺乏人类教练或同行提供反馈的环境使面试技能训练变得颇具挑战。近期，大型语言模型（LLM）的发展为提升面试练习体验提供了机会。遗憾的是，目前的研究鲜有探讨此类系统的效果及其用户感知，以及利用LLM进行面试练习所涉及的益处与挑战。尽管先前的工作和最近的商业工具已经展示了人工智能辅助面试练习的潜力，它们通常仅提供单向反馈，即用户只能从他们的表现中获取信息。相比之下，对话式反馈，一个在学习科学领域发展起来的概念，是一种双向互动反馈过程，允许用户通过对话进一步参与并从提供的反馈中学习。本文介绍了一款名为Conversate的基于网络的应用程序，它利用大型语言模型（LLM）支持反思性学习，以促进求职面试练习。用户通过提供职位标题（如入门级软件工程师）来启动面试会话。然后，系统中的LLM代理将开始面试模拟，通过向用户提出开场面试问题，并根据用户的回答精心设计后续问题来启动。面试结束后，系统的后端LLM框架将分析用户的回答，指出需要改进的地方。用户可以通过选择特定段落并撰写自我反思来注释转录。最后，用户可以与系统进行对话式反馈交互，与LLM代理对话，根据代理的指导逐步完善自己的答案。|
|**2024-10-07**|**Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback**|Sanjiban Choudhury et.al.|[2410.05434](http://arxiv.org/abs/2410.05434)|null|大型语言模型（LLM）在决策制定方面展现出令人印象深刻的能力，但当前的方法缺乏从任务执行期间错误中自动自我改进的机制。我们提出了LEAP，一种迭代细调框架，通过从AI专家教师获取反馈来持续提升LLM代理。我们的关键洞察是为专家教师提供一个特权状态——仅在训练期间可用但在测试时隐藏的信息。这使得即使是最弱的专家也能提供精确指导，显著提高学生代理在不访问测试时的特权信息情况下的性能。我们在多种决策制定基准上评估了LEAP，包括基于文本的游戏（ALFWorld）、网络导航（WebShop）和交互式编码（Intercode Bash）。我们的实验表明，LEAP（1）优于行为克隆和ReAct基线（2）使较弱的学生模型（如Llama3-8B）超过强大教师模型（GPT4-o）的表现，并且（3）允许较弱的模型使用自己特权版本的自我提升。我们也提供了理论分析，显示LEAP的成功取决于平衡特权信息与学生的可实现性，我们通过实验证实了这一观点。我们的代码可在https://leap-llm.github.io 获取。|
|**2024-10-07**|**GLEE: A Unified Framework and Benchmark for Language-based Economic Environments**|Eilam Shapira et.al.|[2410.05254](http://arxiv.org/abs/2410.05254)|**[link](https://github.com/eilamshapira/GLEE)**|**大型语言模型（LLMs）在经济和战略互动领域展现出显著潜力，因为自然语言通信在此类场景中通常占主导地位。这引发了一系列关键问题：LLMs是否表现出理性？它们能否模仿人类行为？它们是否倾向于达到高效且公平的结果？自然语言在战略互动中的角色是什么？经济环境的特性如何影响这些动态？对于将基于LLM的代理集成到现实世界的数据驱动系统（如在线零售平台和推荐系统）中时的经济和社会影响而言，这些问题至关重要。  尽管机器学习社区已经探索了LLMs在多代理设置中的潜在应用，但不同研究之间在假设、设计选择和评估标准上的差异使得难以得出稳健且有意义的结论。为解决这一问题，我们引入了一个基准，以标准化对基于语言的双人、序列游戏的研究。借鉴经济学文献，我们定义了三个基类游戏家族，具有一致的参数化、自由度以及用于评估代理性能（自我收益）以及游戏结果（效率与公平性）的经济衡量指标。  我们开发了一个开源框架进行交互模拟与分析，并利用该框架收集了LLM与LVM之间的多个游戏配置以及额外的人类与LVM交互数据集。通过大量实验，我们的框架和数据集可以用于：(i) 比较基于LLM的代理与人类玩家在各种经济背景下的行为；(ii) 从个体和集体绩效角度评估代理；(iii) 定量分析经济环境特性对代理行为的影响。**|
|**2024-10-09**|**GenSim: A General Social Simulation Platform with Large Language Model based Agents**|Jiakai Tang et.al.|[2410.04360](http://arxiv.org/abs/2410.04360)|**[link](https://github.com/TangJiakai/GenSim)**|**近年来，随着大型语言模型（LLM）的迅速发展，利用基于LLM的代理来模拟人类社会行为的研究取得了许多有前景的成果。尽管先前的工作在特定场景下展示了巨大的潜力，并且涉及有限数量的代理，但它们大多缺乏在模拟过程中出现错误时进行适应的能力。为了克服这些局限性，我们提出了一种名为\textit{GenSim}的新颖的基于LLM的仿真平台：（1）\textbf{抽象了一组通用功能}，简化了定制社会场景的仿真；（2）\textbf{支持一百万个代理}，以更好地模拟现实世界情境中的大规模人群；（3）\textbf{整合了错误纠正机制}，确保更可靠和长期的仿真。为了评估我们的平台，我们评估了大规模代理仿真效率以及错误纠正机制的有效性。据我们所知，GenSim代表了基于LLM代理的通用、大规模和可校正的社会仿真平台的初步步骤，有望进一步推动社会科学领域的发展。**|
|**2024-10-04**|**Permissive Information-Flow Analysis for Large Language Models**|Shoaib Ahmed Siddiqui et.al.|[2410.03055](http://arxiv.org/abs/2410.03055)|null|大型语言模型（LLM）正在快速成为更大软件系统中的通用组件。这引发了一系列自然的安全和隐私问题：从一个组件获取的污染数据可以改变模型的行为并破坏整个系统，包括使模型在不可信组件间传播机密数据。一种有前景的方法是在系统层面上通过动态信息流跟踪（即污点跟踪）来解决这些问题。不幸的是，传统方法将最严格的输入标签传播到输出过于保守，不适合LLM在来自不同来源的输入上操作的应用场景。本文提出了一种新颖的、更宽松的方法来在LLM查询中传播信息流标签。我们的方法的核心思想是仅传播生成模型输出时起作用的样本的标签，并消除不必要的输入标签。  我们实现了并研究了两种这种方法的变体，基于（i）提示增强检索和（ii）基于 $k$ 个最近邻的语言模型。我们将这些方法与直接询问语言模型预测输出标签的反省式影响估计器基线进行了比较。实验结果表明，我们的基于提示的标签传播器方法在超过85%的情况下提高了标签质量，在LLM代理设置中效果显著。这些发现强调了在检索增强中使用宽松标签传播的实用性。|
|**2024-10-03**|**AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML**|Patara Trirat et.al.|[2410.02958](http://arxiv.org/abs/2410.02958)|null|本文提出了一种全新的多代理框架——AutoML-Agent，专为全管道自动化机器学习（AutoML）设计，涵盖了从数据检索到模型部署的整个过程。AutoML-Agent通过接受用户的任务描述、促进专门语言模型代理之间的协作，并交付可部署的模型，从而提供了一个自然语言接口，以简化非专家用户构建数据驱动解决方案的过程。与现有工作不同，本文引入了一种基于检索增强的规划策略来提高探索性，以便在搜索更优解的过程中进行探索。我们还通过并行执行来分解每个计划为子任务（例如数据预处理和神经网络设计），每个子任务由我们通过提示构建的专门代理解决，这使得搜索过程更加高效。此外，我们提出了一个多阶段验证方法来验证执行结果，并指导代码生成语言模型实现成功的解决方案。在七个下游任务上使用十四组数据集进行的大量实验表明，AutoML-Agent在自动化全AutoML流程方面取得了更高的成功率，且系统在整个多样化领域中的性能均表现出色。|
|**2024-10-03**|**Grounding Large Language Models In Embodied Environment With Imperfect World Models**|Haolan Liu et.al.|[2410.02742](http://arxiv.org/abs/2410.02742)|null|尽管大型语言模型（LLMs）在各种应用中取得了广泛的成功，但在处理基本物理推理或执行机器人任务时，它们经常遇到问题，这是因为它们缺乏对现实世界物理细微之处的直接经验。为了应对这些问题，我们提出了一种名为Grounding Large Language Model with Imperfect World MOdel (GLIMO)的方法，该方法利用代理世界模型，如模拟器，来收集和合成训练数据。GLIMO集成了一个基于LLM的自动数据生成器，用于创建高质量且多样化的指令数据集。生成器包括一个迭代自我精炼模块，用于时间上一致的经验采样，一个多样化的问答指令种子集合，以及一个反射性增强生成模块，用于反映先前的经验。  全面的实验表明，我们的方法能够提高强开源LLMs，如LLaMA-3，在三个不同基准上的性能提升分别为2.04倍、1.54倍和1.82倍，分别。这种性能能够与或超越它们更大的同辈，如GPT-4。|
|**2024-10-03**|**Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents**|Hanrong Zhang et.al.|[2410.02644](http://arxiv.org/abs/2410.02644)|**[link](https://github.com/agiresearch/asb)**|**为了填补现有文献在全面评估基于大型语言模型（LLM）的代理攻击与防御策略方面的空白，我们提出了一种名为“代理安全基准”（Agent Security Benchmark, ASB）的综合框架。该框架旨在正式化、标准化并评估基于LLM的代理的安全问题，涵盖了10个应用场景（如电子商务、自动驾驶、金融）、10个针对这些场景的代理、超过400种工具、23类不同的攻击与防御方法以及8个评价指标。基于ASB，我们对10种提示注入攻击、一种记忆污染攻击、一种新颖的计划-思维后门攻击、一种混合攻击以及针对这10种攻击的10种相应防御措施，在13个LLM架构下进行了全面的基准测试，总共产生了近9万个测试案例。我们的基准测试结果揭示了代理操作不同阶段中的关键安全漏洞，包括系统提示、用户提示处理、工具使用和记忆检索，其中最高平均攻击成功率达到了84.30%，但当前的防御措施的有效性有限，这表明社区在代理安全方面仍有许多工作要做。有关此研究的代码可在https://github.com/agiresearch/ASB获取。**|
|**2024-10-03**|**ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration**|Zixiang Wang et.al.|[2410.02551](http://arxiv.org/abs/2410.02551)|null|我们引入了ColaCare框架，该框架通过大型语言模型（LLM）驱动的多代理协作增强了电子健康记录（EHR）建模。我们的方法无缝地将领域特定的专业模型与LLM结合，以弥合结构化EHR数据与基于文本的推理之间的差距。受临床咨询的启发，ColaCare采用了两种类型的代理：医生代理和元代理，它们协同分析患者数据。专家模型处理并从数值EHR数据生成预测，而LLM代理在协作咨询框架内产生推理参考和决策报告。我们还通过检索增强生成（RAG）模块将默克诊断与治疗手册（MSD）医疗指导整合进来，提供权威证据支持。在四个不同的EHR数据集上进行的大量实验证明了ColaCare在死亡率预测任务中的优越性能，这强调了其在临床决策支持系统和推进个性化精准医学方面的潜力。有关代码、完整提示模板、更多案例研究等的更多信息，请访问匿名链接：<https://colacare.netlify.app>。|
|**2024-10-03**|**ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR**|Mengxu Pan et.al.|[2410.02406](http://arxiv.org/abs/2410.02406)|null|许多人在学习新语言时会遇到困难，传统的工具在提供针对每个学习者需求的上下文化学习方面存在不足。最近，大型语言模型（LLMs）和在社交虚拟现实（VR）中的具身对话代理（ECAs）的发展，提供了以一种考虑到学习者的语言水平和需求的方式进行上下文化且自然的语言学习的新机会。为了探索这一可能性，我们开发了ELLMA-T，一个利用GPT-4和基于情境学习框架来支持社交VR（VRChat）中英语语言学习的具身对话代理。通过12次的质性访谈，我们揭示了ELLMA-T在VR中为学习者与代理之间的互动生成真实、可信和上下文特定的角色扮演的潜力，以及LLM在为学习者提供初始语言评估和持续反馈方面的能力。我们提供了对于未来开发基于LLM的语言代理在社交VR中的五个设计启示。|
|**2024-10-03**|**A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization**|Yucheng Chu et.al.|[2410.02165](http://arxiv.org/abs/2410.02165)|null|在学习分析（LA）的背景下，开放式短答问题（SAG）被广泛认为是深入了解学习者响应的强大工具。然而，在实践中，SAG经常面临高评分工作量和评估一致性担忧的挑战。随着自然语言处理（NLP）的最新进展，自动短答评分（ASAG）为解决这些挑战提供了有前景的解决方案。尽管如此，当前的ASAG算法往往在泛化能力上有限，并倾向于针对特定问题进行定制。为此，本文提出了一种统一的多代理ASAG框架GradeOpt，利用大型语言模型（LLMs）作为SAG的评分员。更重要的是，GradeOpt引入了两个基于LLM的额外代理——反射器和细化器——到多代理系统中。这使得GradeOpt能够通过对其错误进行自我反思来自动优化原始评分指南。在对具有挑战性的ASAG任务进行实验，即对教学内容知识（PCK）和内容知识（CK）问题进行评分时，GradeOpt在评分准确性和与人工评分员行为的一致性方面均表现出优于代表基线的性能。最后，全面的消融研究证实了GradeOpt中设计的各个组件的有效性。|
|**2024-10-02**|**Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics**|Yuan Zhou et.al.|[2410.02026](http://arxiv.org/abs/2410.02026)|null|本文介绍了一种名为ZODIAC的大型语言模型(LLM)框架，旨在通过心脏病专家级别的专业素养，辅助心脏病学诊断。ZODIAC能够从患者数据中提取临床相关特征、检测重要的心律失常，并生成初步报告供心脏病专家审查和细化。为了实现心脏病专家级别的专业素养，ZODIAC构建了一个多代理协作框架，允许对多模态患者数据进行处理。每个LLM代理均通过心脏病专家裁定的真实世界患者数据进行精细调优，以此强化模型的专业素养。  ZODIAC经过了严格的临床验证，由独立的心脏病专家评估，涵盖八个指标，衡量临床效果并解决安全问题。结果显示，ZODIAC在性能上超越了行业领先的模型，包括OpenAI的GPT-4o、Meta的Llama-3.1-405B和Google的Gemini-pro，以及专门针对医疗领域的LLM如微软的BioGPT。这表明了专门设计的LLM在医疗保健领域的潜力，能够提供符合医疗实践严格要求的领域特定解决方案。  值得注意的是，ZODIAC已成功集成到心电图(ECG)设备中，展示了将LLM嵌入软件作为医疗设备(SaMD)的趋势日益增长。|
|**2024-10-02**|**Moral Alignment for LLM Agents**|Elizaveta Tennant et.al.|[2410.01639](http://arxiv.org/abs/2410.01639)|null|基于大型语言模型（LLM）的决策代理正越来越多地在人类活动的不同领域部署。虽然它们的应用目前较为专业化，但已有研究努力开发更通用的代理。随着LLM系统变得更加自主，它们对人类活动的影响将增加，并且透明度会降低。因此，发展有效的方法来使它们符合人类价值观至关重要。  现有的对齐方法通常依赖于人类偏好数据（例如，在RLHF或DPO中），其中价值观是隐含的，并且本质上是从不同模型输出的相对偏好中推断出来的。与此相反，我们在这项工作中提出了一种设计奖励函数的方法，这些函数明确编码了核心的人类价值观，用于强化学习（RL）方式微调基础代理模型。具体来说，我们使用内在奖励来实现LLM代理的道德对齐。  我们通过传统的哲学框架——德ontology伦理和功利主义来评估我们的方法，量化了在迭代囚徒困境（IPD）环境中代理的道德奖励，基于其行为及其后果。我们还展示了如何通过道德微调使代理能够放弃之前开发的自私策略。最后，我们发现某些在IPD游戏中学习的道德策略能够推广到多个矩阵游戏环境。总之，我们证明了使用内在奖励进行微调是将LLM代理与人类价值观对齐的有前景的一般解决方案，并且可能代表了当前主流对齐技术更加透明和成本效益更高的替代方案。|
|**2024-10-03**|**RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance**|Haolin Jin et.al.|[2410.01242](http://arxiv.org/abs/2410.01242)|null|大型语言模型（LLM）在代码生成任务上展现出了巨大的潜力，并且最近的提示工程研究进一步增强了LLM对文本信息的理解。然而，确保生成代码的准确性通常需要程序员进行大量的测试和验证。尽管LLM能够基于任务描述生成代码，但在复杂任务上的准确度仍然有限，特别是对于那些需要更深入理解问题陈述和代码生成过程的任务。这一限制主要源于LLM同时需要理解和生成语法和语义上正确的代码，而没有能力自动优化代码的能力。在实际的软件开发中，程序员很少能在仅凭任务描述的情况下一次就生成完美的代码，他们依赖于迭代反馈和调试来完善他们的程序。受此过程启发，我们引入了一种基于LLM的多智能体架构用于代码生成和自动调试：改进与指导调试（RGD）。RGD框架是一个利用三种不同LLM代理（引导代理、调试代理和反馈代理）的多智能体调试器，它将代码生成任务分解为多个步骤，确保了清晰的工作流程，并允许基于自我反思和反馈的代码迭代细化。实验结果表明，RGD在代码生成能力上表现出色，分别在HumanEval数据集和MBPP数据集上相比最先进的方法和传统直接提示方法实现了9.8%和16.2%的性能提升。我们强调了RGD框架在增强LLM自主生成和优化代码能力方面的有效性。|
|**2024-10-01**|**Dynamic Planning for LLM-based Graphical User Interface Automation**|Shaoqing Zhang et.al.|[2410.00467](http://arxiv.org/abs/2410.00467)|**[link](https://github.com/sqzhang-lazy/d-pot)**|**大型语言模型（LLM）的兴起激发了对自主LLM基代理进行创新性发展的兴趣，尤其是在智能手机图形用户界面（GUI）中的应用。当面对任务目标时，这些代理通常会模仿人类在GUI环境中的操作直至任务完成。然而，一个关键挑战在于如何有效地制定计划以指导GUI任务中的动作预测，尽管规划已被广泛认为是分解复杂任务的有效方式。具体而言，在执行动作后GUI环境的动态性质意味着需要根据环境反馈和动作历史动态调整计划。  我们发现广受欢迎的ReAct方法失败了，原因在于其过于依赖过长的历史对话。为了解决这一挑战，我们提出了一种名为动态思维规划（D-PoT）的新方法，用于基于LLM的GUI代理。D-PoT涉及根据环境反馈和执行历史动态调整规划的过程。实验结果表明，提出的D-PoT方法在准确性上显著超越了强大的GPT-4V基线，提高了12.7%（从34.66%提高到47.36%）。分析揭示了动态规划在不同基础LLM中的通用性，以及在处理未见过的任务时减少幻觉并适应的能力。代码已发布在https://github.com/sqzhang-lazy/D-PoT。**|
|**2024-09-30**|**MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants**|Zeyu Zhang et.al.|[2409.20163](http://arxiv.org/abs/2409.20163)|**[link](https://github.com/nuster1128/memsim)**|**本文提出了一种名为MemSim的贝叶斯模拟器，用于从生成的用户消息自动构建可靠的问题与答案（Q&A），同时保持其多样性和可扩展性。具体来说，我们引入了贝叶斯关系网络（BRNet）和因果生成机制，以减轻大型语言模型（LLM）幻觉对事实信息的影响，从而促进自动构建评估数据集。基于MemSim，我们在日常生活中生成了一个名为MemDaily的数据集，并进行了广泛的实验，以评估我们方法的有效性。我们还提供了使用MemDaily数据集评估LLM基智能体不同记忆机制的基准。为了惠及研究社区，我们已经在https://github.com/nuster1128/MemSim上发布了我们的项目。**|
|**2024-10-01**|**TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation**|Zhiqiang Yuan et.al.|[2409.19894](http://arxiv.org/abs/2409.19894)|null|本文提出了一种名为TRANSAGENT的新型基于大型语言模型（LLM）的多代理系统，以增强基于LLM的代码翻译过程，并通过四个基于LLM的代理协同工作修复语法错误和语义错误。这四个代理分别是初始代码翻译器、语法错误修复器、代码对齐器和语义错误修复器。TRANSAGENT的核心洞察是首先根据目标程序与源程序之间的执行对齐定位目标程序中的错误代码块，这种方法可以缩小修复范围并降低修复难度。  为了评估TRANSAGENT，我们首先从最近的编程任务构建了一个新的基准，以减轻潜在的数据泄露问题。在我们的基准上，TRANSAGENT在翻译效果和效率方面都优于最新的LLM基代码翻译技术UniTrans；此外，在不同LLM上的评估显示了TRANSAGENT的一般性，并且我们的消融研究揭示了每个代理的贡献。|
|**2024-09-26**|**From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection**|Xinlei Wang et.al.|[2409.17515](http://arxiv.org/abs/2409.17515)|**[link](https://github.com/ameliawong1996/From_News_to_Forecast)**|本文提出了一种新颖的方法，旨在通过大型语言模型（LLMs）和生成代理来增强时间序列预测。以语言作为媒介，我们的方法适应性地将各种社会事件整合进预测模型中，将新闻内容与时间序列波动对齐，从而提供丰富洞察。具体而言，我们利用基于语言模型的代理进行迭代筛选，去除无关新闻，并采用类似人类的推理和反思来评估预测结果。这使得我们的模型能够分析复杂事件，如意外事件和社会行为转变，并不断优化选择逻辑以及代理输出的稳健性。通过结合精选新闻和时间序列数据，我们对预训练的LLaMa2模型进行微调。结果显示，在准确性方面有显著提升，这表明通过有效利用非结构化新闻数据，可能在时间序列预测领域实现范式转变。|
|**2024-09-25**|**AAPM: Large Language Model Agent-based Asset Pricing Models**|Junyan Cheng et.al.|[2409.17266](http://arxiv.org/abs/2409.17266)|**[link](https://github.com/chengjunyan1/aapm)**|**本文提出了一种新型的资产定价方法——基于LLM代理的资产定价模型（AAPM）。该方法将LLM代理的定性主观投资分析与定量手动金融经济因素融合，以预测超额资产回报。实验结果表明，我们的方法在组合优化和资产定价误差方面均优于基于机器学习的资产定价基准。具体而言，异常资产组合的夏普比率和平均α值分别提高了9.6%和10.8%。此外，我们还对模型进行了广泛的消融研究，并对数据进行了深入分析，以揭示提出方法的更多见解。**|
|**2024-09-25**|**Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents**|Junting Lu et.al.|[2409.17140](http://arxiv.org/abs/2409.17140)|null|在多模态大型语言模型（MLLMs）的帮助下，语言模型驱动的代理可以直接与应用用户界面（UI）进行交互，从而在复杂任务中提升代理性能。然而，这些代理常常因为涉及大量顺序UI交互而导致高延迟和低可靠性。为了应对这一问题，我们提出了AXIS，一个新颖的基于语言模型的代理框架，通过应用程序接口（APIs）优先于UI动作来优化代理行为。此外，该框架还通过自动化探索应用以创建和扩展API，促进了API的生成和应用范围的扩展。  我们的实验在Word办公软件上显示，与人类相比，AXIS在完成任务的时间上减少了65%-70%，认知负荷降低了38%-53%，同时保持了97%-98%的准确性。我们的工作为人类-代理-计算机交互（HACI）框架和应用提供者在LLMs时代设计新UI原则提供了贡献，并探讨了将每一个应用转化为代理的可能性，为迈向以代理为中心的操作系统（Agent OS）铺平了道路。|
|**2024-09-24**|**MultiTalk: Introspective and Extrospective Dialogue for Human-Environment-LLM Alignment**|Venkata Naren Devarakonda et.al.|[2409.16455](http://arxiv.org/abs/2409.16455)|null|本文提出了一种名为MultiTalk的基于大语言模型（LLM）的任务规划方法。通过引入内省和外省对话循环框架，该方法旨在解决LLM在任务规划中可能遇到的问题，如幻觉、用户指令中的歧义、环境约束以及执行代理能力的局限性。这些问题可能导致生成的计划出现错误或不完整。  MultiTalk方法通过特定系统来提取和预测与任务相关的状态，并标记出人、LLM代理和环境之间的不匹配或偏差。有效的反馈路径促进人与LLM之间的有意义对话。这种方法在机器人操作任务的应用中得到了验证。实验和消融分析展示了MultiTalk方法的稳健性和可靠性，与基线方法的比较进一步证明了其在实体代理任务规划方面的优势。  总之，MultiTalk提供了一种通过增强LLM与环境、执行者和用户之间的一致性和沟通来改进任务规划过程的方法，从而提高规划的有效性和效率。|
|**2024-09-23**|**Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality**|Yiwen Xu et.al.|[2409.15623](http://arxiv.org/abs/2409.15623)|null|本文介绍了一种名为Safe Guard的LLM代理，用于检测社交VR（VRChat）中的语音交互中的仇恨言论。我们的系统利用了Open AI GPT和音频特征提取技术，实现了实时语音交互的检测功能。我们贡献了一个系统设计以及对该系统的评估，这些都证明了我们方法在检测仇恨言论方面的有效性，并且相比现有方法显著降低了误报率。我们的结果表明基于LLM的代理在创建更安全的虚拟环境方面具有潜力，并为进一步发展基于LLM的管理方法奠定了基础。|
|**2024-09-20**|**ControlMath: Controllable Data Generation Promotes Math Generalist Models**|Nuo Chen et.al.|[2409.15376](http://arxiv.org/abs/2409.15376)|null|利用大型语言模型（LLM）进行数据增强在数学推理方面取得了令人鼓舞的结果。然而，这些方法在问题多样性方面存在限制，可能仅局限于特定领域的数据生成。为此，我们提出了一种名为ControlMath的迭代方法，该方法包含一个方程式生成模块和两个基于LLM的代理。该模块产生多样化的方程，问题创造者代理随后将其转化为数学文字问题。逆向代理则筛选并选择高质量的数据，遵循“少即是多”的原则，使用更少的数据点就能实现更好的结果。这种方法能够生成多样化的数学问题，不受特定领域或分布的限制。  因此，我们收集了ControlMathQA数据集，包含19万个数学文字问题。广泛的实验结果证明，将我们的数据集与GSM8K等内部领域数据集结合，可以帮助提高模型在数学推理方面的泛化能力，从而在特定领域内以及超出特定领域时都能取得更好的性能。|
|**2024-09-25**|**Towards a Realistic Long-Term Benchmark for Open-Web Research Agents**|Peter Mühlbacher et.al.|[2409.14913](http://arxiv.org/abs/2409.14913)|null|我们提出了一项即将推出的基准测试，用于评估大型语言模型（LLM）代理在经济价值高的白领任务上的表现。我们对金融和咨询领域常规进行的、现实世界中的“杂乱”开放网络研究任务进行了评估。这样做，我们为建立一个LLM代理评估套件奠定了基础，在该套件中，良好的性能直接对应着巨大的经济和社会影响。我们构建并测试了多个代理架构，包括o1-preview、GPT-4o、Claude-3.5 Sonnet、Llama 3.1（405b）以及GPT-4o-mini。平均而言，使用Claude-3.5 Sonnet和o1-preview的LLM代理在性能上明显优于使用GPT-4o的代理，而基于Llama 3.1（405b）和GPT-4o-mini的代理则落后很多。在所有LLM中，具有委托子任务给子代理能力的ReAct架构表现最佳。除了定量评估之外，我们还通过检查代理的追踪记录和反思它们的观察结果，对代理的能力进行了定性评估。我们的评估代表了首次深入评估代理在真实开放网络上执行具有挑战性的、经济上有价值的分析师式研究的能力。|
|**2024-09-23**|**Interpreting Multi-band Galaxy Observations with Large Language Model-Based Agents**|Zechang Sun et.al.|[2409.14807](http://arxiv.org/abs/2409.14807)|null|本文展示了大型语言模型为基础的智能体如何加速天文学研究流程，通过模仿人类推理来解释多波段星系观测数据。我们提出了mephisto框架，它能够与CIGALE代码库协作，后者包含了用于解释观测数据的光谱能量分布（SED）模型。在开放世界环境中，mephisto通过自我游戏经验学习、执行树搜索并积累动态更新的知识基础。作为概念验证，我们将mephisto应用于詹姆斯韦伯太空望远镜的最新数据集。结果表明，mephisto在推理星系物理场景方面达到了接近人类的专业水平，甚至在处理新发现的“小红点”星系时也是如此。这是智能体进行天文学研究的首次展示，朝着通过大型语言模型代理实现端到端研究的方向迈进，可能有助于加快天文发现的速度。|
|**2024-09-22**|**Enhancing LLM-based Autonomous Driving Agents to Mitigate Perception Attacks**|Ruoyu Song et.al.|[2409.14488](http://arxiv.org/abs/2409.14488)|null|随着大型语言模型（LLM）与自动驾驶（AD）系统集成的日益增长的兴趣，AD系统面临着攻击其对象检测与追踪（ODT）功能的风险。我们的评估表明，针对四个近期提出的LLM代理的ODT攻击成功率达到63.26%，导致它们崩溃或违反交通规则，原因在于误导性记忆模块提供的过往经验、提示在识别不一致性方面的局限性以及对地面实况感知数据的依赖。为此，我们提出了一种名为Hudson的驾驶推理代理，它扩展了先前基于LLM的驾驶系统，旨在在感知攻击期间实现更安全的决策制定，同时在正常条件下保持有效性。  Hudson通过首先对AD软件进行仪器化收集实时感知结果和驾驶场景的上下文信息来实现这一目标。这些数据随后被转化为领域特定语言（DSL）。为了引导LLM在ODT攻击期间检测并做出安全控制决策，Hudson将DSL转换为自然语言，并附带一组自定义的攻击检测指令。执行查询后，Hudson分析LLM的控制决策以理解其因果推理过程。  我们使用私有LLM（GPT-4）、两个开源LLM（Llama和Gemma）和各种对抗性驾驶情景对Hudson的有效性进行了评估。GPT-4、Llama和Gemma在平均情况下实现了83.3%、63.6%和73.6%的攻击检测准确率。因此，在86.4%、73.9%和80%的攻击中，它们做出了安全控制决策。随着将LLM集成到AD系统中的兴趣增长，我们的结果强调了LLM的优势及其在检测和缓解ODT攻击方面的潜力。|
|**2024-09-20**|**Enhancing Fault Localization Through Ordered Code Analysis with LLM Agents and Self-Reflection**|Md Nakhla Rafi et.al.|[2409.13642](http://arxiv.org/abs/2409.13642)|null|在软件开发过程中，定位和修复软件故障是一个耗时且资源密集型的任务。传统的故障定位方法，如基于频谱的故障定位（SBFL），依赖于测试覆盖率数据的统计分析，但往往准确性较低。基于学习的技术虽然更有效，但需要大量的训练数据，并且计算成本高昂。最近，大型语言模型（LLMs）的进步为改善故障定位提供了有前景的方法，通过增强代码理解和推理来提升性能。然而，这些LLM基线技术仍然面临挑战，包括令牌限制、长输入性能下降以及处理涉及多个相互作用组件的复杂系统时的困难。  为了解决这些问题，我们提出了一种名为LLM4FL的创新性LLM代理基线故障定位方法，它结合了SBFL排名与分而治之策略。通过将大规模覆盖数据分解为可管理的组，并利用多个LLM代理通过提示链式调用，LLM4FL有效地导航代码库并定位故障。该方法还整合了自我反思和链式思考推理，使代理能够迭代生成修复并重新排名可疑方法。我们使用Defects4J（V2.0.0）基准进行评估，其中包括来自14个开源Java项目的675个真实世界故障。结果显示，LLM4FL在Top-1准确率上比AutoFL高出19.27%，并且优于最先进的监督技术，如DeepFL和Grace，所有这些都无需特定任务的培训。此外，我们强调了覆盖拆分和提示链对故障定位性能的影响，并展示了不同的方法排序可以提高Top-1准确率高达22%。|
|**2024-09-23**|**AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit**|Mohanna Hoveyda et.al.|[2409.13447](http://arxiv.org/abs/2409.13447)|null|在问答（QA）领域，不同的问题可能需要不同的回答策略来有效解决。一些问题可以通过简单的查找来解决，而另一些则需要复杂的、多步骤的推理。这一观察结果激发了开发一种动态方法，该方法能够为每个问题适当地选择最合适的QA策略，从而构建更高效、更有效的系统，能够处理更广泛类型的问题。为了实现这一目标，我们基于多个大型语言模型（LLMs）的集成最新进展，并将适应性QA定义为一个动态编排挑战。我们将此视为一个上下文多臂老虎机问题，其中上下文由进入问题的特性定义，而动作空间包括潜在的LLM代理之间的通信图配置。然后，我们训练了一个线性上界信心边界模型，以学习不同问题类型与其对应的最佳多LLM通信图表示之间的最优映射。我们的实验表明，提出的解决方案适用于适应性的LLM集成问答系统的编排，它结合了更复杂策略的优越性能，同时避免了在简单策略足以的情况下使用这些策略的成本。|
|**2024-09-24**|**Towards Robust Automation of Surgical Systems via Digital Twin-based Scene Representations from Foundation Models**|Hao Ding et.al.|[2409.13107](http://arxiv.org/abs/2409.13107)|null|本文提出了一种基于数字孪生的机器感知方法，旨在利用近期视觉基础模型的令人信服的表现和开箱即用的泛化能力。该方法通过结合数字孪生的场景表示和大型语言模型（LLM）代理进行规划，与dVRK平台集成，从而开发出一个具有强大任务性能和在不同环境设置下通用性的实体智能系统。在执行穿针移位和纱布检索任务时，我们的方法显示出强大的任务性能和通用性。  尽管表现出令人信服的表现，但本文的工作仅仅是对基于数字孪生的场景表示集成的第一步。为了实现全面的数字孪生框架以改善手术领域实体智能的可解释性和通用性，未来的研究是必要的。|
|**2024-09-17**|**LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents**|Amine B. Hassouna et.al.|[2409.11393](http://arxiv.org/abs/2409.11393)|null|本文通过提出一个统一框架——LLM-Agent-UMF（基于语言模型的代理统一建模框架），解决了集成工具到语言模型（LLM）驱动的代理以及在多个前沿工作中提出的改进措施所导致的软件架构非统一性问题。传统上，这些技术的结合及后续工作侧重于功能实现而非定义组件边界，导致了研究人员之间的术语和架构上的混淆。  该框架明确了代理的不同组件，包括LLM、工具以及新引入的核心代理概念，其作用是代理的中央协调者，由规划、记忆、个人资料、行动和安全五个模块组成。核心代理的内部结构差异促使我们将其分类为被动型和主动型两种类型。基于此分类，我们提出了结合不同个体代理独特特性的多种多核心代理架构。  为了验证框架的有效性，我们将该框架应用于一系列前沿代理，并展示其与功能的一致性，同时澄清了先前被忽视的架构方面。此外，我们对四个提出的架构进行了详尽评估，通过整合具有不同特性的代理到混合主动/被动核心代理系统中，这一过程提供了对特定代理组合可能带来的改进和面临的挑战的清晰见解。|
|**2024-09-17**|**Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments**|Maria Rigaki et.al.|[2409.11276](http://arxiv.org/abs/2409.11276)|null|本篇论文探讨了在网络安全环境中使用本地微调的大型语言模型（LLM）作为红队代理的可能性。考虑到商业云基LLM的隐私问题、成本和网络连接限制，我们提出了Hackphyr——一个本地微调的70亿参数模型，旨在用于网络安全环境中的红队任务。我们的模型能够在单个GPU卡上运行，并且在性能上与更大更强大的商业模型如GPT-4相媲美。  Hackphyr在复杂、前所未见的场景中显著优于其他模型，包括GPT-3.5-turbo以及Q-learning代理等基线模型。为了实现这一性能提升，我们构建了一个专门针对网络安全任务的新数据集，以增强基础模型的能力。最后，我们对代理行为进行了全面分析，提供了关于此类基于LLM的代理在网络安全上下文中的规划能力和潜在局限性的见解，从而为更广泛地理解此类代理在网络安全领域的应用提供了参考。|
|**2024-09-14**|**On the limits of agency in agent-based models**|Ayush Chopra et.al.|[2409.10568](http://arxiv.org/abs/2409.10568)|**[link](https://github.com/agenttorch/agenttorch)**|**本文介绍了一种名为AgentTorch的框架，旨在通过使用大型语言模型（LLMs）作为具有适应性行为的代理，将基于个体的模型（ABM）扩展到数百万个代理的规模。这一框架旨在在模拟复杂系统的行为时，既捕捉到真实环境动态和适应性代理行为，又保持对庞大人口群体高效模拟的能力。大型语言模型的最新进展为增强ABM提供了机会，但使用LLMs进行大规模代理的计算可行性限制了其广泛应用。  我们通过实验评估了使用LLMs作为ABM代理的实用性，探索了模拟规模与单个代理行为细节之间的权衡。以COVID-19大流行为例，我们展示了AgentTorch如何模拟840万个代表纽约市的代理，以捕捉隔离和就业行为对健康和经济结果的影响。我们比较了基于启发式方法和LLMs的不同代理架构在预测疾病浪潮和失业率方面的性能。  此外，我们展示了AgentTorch在回顾性、假设性和前瞻性分析中的能力，强调了适应性代理行为如何帮助克服历史数据在政策设计中的局限性。AgentTorch是一个开源项目，目前正被全球用于政策制定和科学发现。该框架可在此获取：github.com/AgentTorch/AgentTorch。**|
|**2024-09-19**|**Instigating Cooperation among LLM Agents Using Adaptive Information Modulation**|Qiliang Chen et.al.|[2409.10372](http://arxiv.org/abs/2409.10372)|null|本文提出了一种新颖的框架，将大型语言模型（LLM）代理作为人类战略行为的代理，并结合强化学习（RL）让这些代理在团队环境中进行不断演化的战略互动。我们的方法扩展了传统的基于代理的模拟，通过使用策略性大型语言模型（SLA）以及引入动态和适应性的治理，通过促进社会行为的强化学习代理（PPA），该代理调节网络中代理之间的信息访问，以优化社会福利并促进亲社会行为。通过在迭代游戏中验证，包括囚徒困境，我们展示了SLA代理表现出复杂的战略调整。PPA代理有效地学习调整信息透明度，导致合作率显著提高。这一框架提供了对人工智能驱动的社会动力学的重要见解，为在实际团队环境中部署AI做出了贡献。|
|**2024-09-17**|**Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition**|Chao-Han Huck Yang et.al.|[2409.09785](http://arxiv.org/abs/2409.09785)|null|在近期生成式人工智能技术的推动下，大型语言模型（LLMs）如何增强基于文本解码的自动语音识别（ASR）模型在声学建模任务中的应用成为了一个关键问题。为了探索语言建模在语音处理领域的潜在新能力，本文提出了一项名为“生成性语音转录错误修正”（GenSEC）的挑战。该挑战包含了三个针对后ASR语言模型的任务：（i）后ASR转录修正、（ii）说话者标签化以及（iii）情感识别。这些任务旨在模拟未来基于语言模型的语音界面代理处理工作时的场景，并通过使用开源预训练语言模型或基于代理的API来保持对广泛受众的可访问性。此外，本文还讨论了基准评估的结果以及设计未来评估时应汲取的经验教训。|
|**2024-09-15**|**RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation**|Qingyao Li et.al.|[2409.09584](http://arxiv.org/abs/2409.09584)|null|本文针对LLM（大型语言模型）代理与树搜索算法在代码生成任务中的应用进行了深入研究。当前的搜索算法在这一领域存在低搜索质量的问题，主要源于以下三个原因：1）对代码生成任务高推理要求的搜索空间设计不合理；2）未能充分结合代码反馈优化搜索过程；3）处理负反馈时效率低下，导致搜索质量和效率降低。  为解决这些问题，我们提出了一种新的方法——RethinkMCTS（反思蒙特卡洛树搜索）。该方法通过在生成代码之前进行多层次的思考搜索，探索更广泛的策略选项。更重要的是，RethinkMCTS利用细粒度的代码执行反馈构建口头反馈，以修正搜索过程中出现的错误思路。这种机制确保了搜索沿着正确的推理路径前进，从而提高整个搜索树的整体质量。  实验结果表明，与之前的基于搜索和反馈的代码生成基准相比，RethinkMCTS取得了显著的性能提升。在HumanEval数据集上，RethinkMCTS将GPT-3.5-turbo的pass@1指标从70.12提高到了89.02，将GPT-4o-mini的pass@1指标从87.20提升至94.51。通过深入的探索和改进整个搜索树的质量，RethinkMCTS有效地增强了搜索过程的全面性和深度。|
|**2024-09-14**|**Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models**|Yuanzhao Zhai et.al.|[2409.09345](http://arxiv.org/abs/2409.09345)|null|本文提出了一种利用任务相关Q值模型来指导行动选择的方法，以增强大型语言模型（LLM）代理在多步决策任务中的性能。具体地，我们首先通过蒙特卡洛树搜索（MCTS）收集了标注有步骤级Q值的决策轨迹，并构建了偏好数据集。接着，我们使用另一个LLM通过步骤级直接策略优化（DPO）拟合这些偏好，从而形成Q值模型。在推理过程中，对于每个决策步骤，LLM代理都会选择具有最高Q值的动作，然后再与环境进行交互。我们将该方法应用于多个开源和API集成的LLM代理上，结果显示，引入Q值模型显著提高了它们的性能。特别值得注意的是，构建于Phi-3-mini-4k-instruct的代理在WebShop任务上的性能提升了103%，在HotPotQA任务上提升了75%，甚至超越了GPT-4o-mini。此外，Q值模型还具备几个优势，如对不同LLM代理的泛化能力和与现有提示策略无缝集成的能力。|
|**2024-09-14**|**Python Symbolic Execution with LLM-powered Code Generation**|Wenhan Wang et.al.|[2409.09271](http://arxiv.org/abs/2409.09271)|null|本文提出了一种利用大型语言模型（LLM）增强的代理工具——LLM-Sym。该工具旨在解决使用符号执行技术在动态类型语言如Python中遇到的主要挑战。通过自动调用SMT求解器Z3来解决执行路径约束，LLM-Sym能够扩展基础的符号执行引擎，使其支持包含复杂数据类型`list`的程序。  LLM-Sym的核心贡献在于将复杂的Python路径约束转化为Z3代码的能力。为了实现准确的路径到Z3代码的转换，我们设计了一个多步骤的代码生成管道，包括类型推断、检索和自我精炼等环节。  实验结果表明，LLM-Sym能够解决具有复杂控制流和列表数据结构的LeetCode问题中的路径约束，这是基础符号执行引擎无法做到的。这一方法为LLM与符号求解器推理能力的结合开辟了道路，并为LLM辅助测试用例生成提供了新的机遇。|
|**2024-09-23**|**Agents in Software Engineering: Survey, Landscape, and Vision**|Yanlin Wang et.al.|[2409.09030](http://arxiv.org/abs/2409.09030)|**[link](https://github.com/deepsoftwareanalytics/awesome-agent4se)**|**近年来，大型语言模型（LLMs）在各种下游任务中取得了显著成功，并在软件工程（SE）领域广泛应用。我们发现许多结合LLMs与SE的研究工作明确或隐含地采用了代理概念。然而，缺乏对现有工作的深度综述，以整理其发展背景、分析如何结合LLMs代理技术优化各类任务以及阐明SE中的LLMs代理框架。本文开展首次针对结合LLMs代理与SE的研究综述，并提出SE中LLMs代理的框架，包括感知、记忆和行动三个关键模块。同时，总结了两个领域结合时面临的问题，并针对现有挑战提出了未来机遇。我们维护了一个包含相关论文的GitHub仓库：https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE。**|
|**2024-09-13**|**AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents**|Zhe Su et.al.|[2409.09013](http://arxiv.org/abs/2409.09013)|null|为了安全和成功地部署，语言模型（LLMs）必须同时满足真实性和实用性目标。然而，这两个目标往往在冲突中，例如AI助手帮助二手车销售员销售有瑕疵的汽车。这种冲突部分归因于模糊或误导性的用户指令。我们提出了一种名为AI-LieDar的框架，以研究在多轮交互设置中，基于LLM的代理如何处理实用性和真实性的冲突。  我们设计了一系列现实场景，其中语言代理被指示实现与多轮对话中的真实性冲突的目标。为了大规模评估真实性，我们开发了一个基于心理学文献的可信度检测器，用于评估代理的回答。我们的实验表明，所有模型的真实回答比例不到50%，尽管达到目标（实用性）和真实性的比例在不同模型中有所差异。我们进一步测试了LLM的可引导性，发现模型会遵循恶意指令来欺骗，即使经过引导使其趋向真实的模型也仍然可能说谎。  这些发现揭示了LLM中真实性的复杂性，并强调了确保LLM和AI代理的安全可靠部署需要进一步研究的重要性。|
|**2024-09-13**|**Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance**|Lucio La Cava et.al.|[2409.08963](http://arxiv.org/abs/2409.08963)|null|确保内容遵守社区准则对于维护健康的在线社交环境至关重要。然而，传统基于人工的合规检查在处理用户生成内容的日益增加量以及有限的管理员数量时，面临着难以扩展的问题。大型语言模型在自然语言理解方面的新进展为自动内容合规验证提供了新的机遇。本工作评估了六个基于Open-LLMs构建的AI代理，用于去中心化社交网络中的自动化规则遵循检查，在这种具有挑战性的环境中，由于社区范围和规则的异质性，这一任务尤为困难。通过分析来自数百个Mastodon服务器的超过50,000条帖子，我们发现AI代理能够有效地检测不合规的内容、理解语言的细微差别，并适应多样的社区上下文。大多数代理还表现出高度的一致性和一致性评分解释与合规建议。基于领域专家的人类评估确认了代理的可靠性和实用性，表明它们是半自动化或人机协作内容管理系统的有前景的工具。|
|**2024-09-13**|**Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with LLM-based Agents**|Junchi Yao et.al.|[2409.08717](http://arxiv.org/abs/2409.08717)|null|在社交媒体日益成为社会运动形成公众意见的重要平台的背景下，准确模拟和预测用户意见动态对于理解社会现象、政策制定以及引导公众意见至关重要。然而，现有的模拟方法在捕捉用户行为的复杂性和动态性方面面临着挑战。针对这一问题，本文提出了一种创新的社交媒体用户意见动态模拟方法——FDE-LLM算法，该算法结合了意见动态与流行病模型，有效约束了大型语言模型（LLM）的行为和意见演化过程，使其更加符合现实网络世界。特别地，FDE-LLM将用户分为意见领袖和跟随者两大类。意见领袖基于LLM角色扮演，并受细胞自动机（CA）模型约束，而意见跟随者则融入了一个结合CA模型与SIR模型的动态系统。这种创新设计显著提高了模拟的准确性和效率。  实验在四个真实微博数据集上进行，并使用开源模型ChatGLM进行了验证。结果表明，相较于传统基于代理的模型（ABM）意见动态算法和基于LLM的意见传播算法，我们的FDE-LLM算法在准确性与可解释性方面表现更优。|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351](http://arxiv.org/abs/2409.06351)|null|在紧急护理部门、偏远医院或发展中国家的诊所中，临床医生经常缺乏由训练有素的放射科医生快速分析影像的能力，这会对病人的健康护理产生不利影响。大型语言模型（LLMs）有可能通过提供有助于他们决策的见解来缓解这些临床医生的压力。尽管这些LLM在展示其理论医学知识的医学考试上取得了高分，但它们往往不遵循医学指南。为此项工作，我们引入了一种新的零样本指南驱动决策支持方法。我们构建了一个由多个LLM代理组成的系统，这些代理配备了对比视觉-语言模型，以协作方式达成患者诊断。在向这些代理提供简单的诊断指南后，它们会合成提示并根据这些指南筛选图像以寻找发现。最后，它们提供一个可理解的推理链路来解释其诊断结果，并自我精炼以考虑疾病之间的相互依赖性。由于我们的方法是零样本的，因此适用于罕见疾病场景，在这些场景中训练数据有限，但专家设计的疾病描述可用。我们在两个胸部X射线数据集CheXpert和ChestX-ray 14 Longtail上评估了我们的方法，展示了与现有零样本方法相比的性能提升，并且能够应用于罕见疾病的泛化。|
|**2024-09-08**|**A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement**|Huan Zhang et.al.|[2409.05001](http://arxiv.org/abs/2409.05001)|**[link](https://github.com/nju-websoft/paircoder)**|**在代码生成领域，大型语言模型（LLM）展现出了令人瞩目的性能。尽管先前的研究通过提示技术及代码精炼对LLM进行了增强，但它们在处理复杂编程问题时仍面临挑战，因为这些问题往往具有僵化的解决方案计划。本文提出了一种名为PairCoder的新型LLM基框架，旨在模仿双人协作编程实践，以解决这一问题。  PairCoder由两个协作的LLM代理组成：导航员（Navigator）和驾驶员（Driver）。导航员负责提出有前景的解决方案计划、选择当前最佳计划，并根据执行反馈指导下一轮迭代。驾驶员则遵循导航员的指引，进行初始代码生成、代码测试和优化。  这种交替和迭代的工作流程包括多计划探索和基于反馈的细化，模拟了双人程序员的合作方式。我们使用开源和闭源的LLM，在多种代码生成基准上对PairCoder进行了评估。实验结果表明，PairCoder在准确性方面显著优于直接使用提示的LLM，相对pass@1提高了12.00%-162.43%。**|
|**2024-09-06**|**Sparse Rewards Can Self-Train Dialogue Agents**|Barrett Martin Lattimer et.al.|[2409.04617](http://arxiv.org/abs/2409.04617)|**[link](https://github.com/asappresearch/josh-llm-simulation-training)**|**本文探讨了在多轮对话任务中，大型语言模型（LLM）代理的最新进展主要由监督微调和高质量的人类反馈驱动。然而，随着基础LLM模型性能的持续提升，获取有意义的人类反馈变得越来越困难且成本高昂。在某些领域中，基础LLM可能最终超越人类能力，使得传统的基于反馈的方法变得不切实际。因此，本文提出了一种新的自我改进范式，允许LLM代理在没有外部人类反馈的情况下自主提高其性能。  我们引入了一种名为“对比结果为模拟收获”（JOSH）的自我对齐算法，该算法利用稀疏奖励模拟环境来提取理想行为，并进一步训练LLM以自身输出进行训练。我们从MultiWOZ中构建了一个用于工具调用的稀疏奖励仿真环境，称为ToolWOZ。实验结果显示，使用JOSH训练的模型（无论是小型还是前沿模型），在基于工具的交互上显著提高了表现，同时保持了在各种基准测试中的广泛模型能力。  我们的代码和数据已在GitHub上公开提供。**|
|**2024-09-06**|**LLM-based multi-agent poetry generation in non-cooperative environments**|Ran Zhang et.al.|[2409.03659](http://arxiv.org/abs/2409.03659)|**[link](https://github.com/zhangr2021/Multiagent_poetry)**|**尽管大型语言模型在自动诗歌生成方面取得了显著进步，但生成的诗歌缺乏多样性，而训练过程与人类学习大相径庭。基于这样的理念，即诗歌生成系统的学习过程应更加人性化，并且其输出更加多样和新颖，我们引入了一种基于社会学习的框架，在此框架中，我们强调除了合作互动之外的非合作互动，以鼓励多样性。我们的实验是首次尝试在非合作环境中利用基于训练的代理（GPT-2）和基于提示的代理（GPT-3和GPT-4）进行诗歌生成的大型语言模型多代理系统。  根据对生成的96,000首诗的评估，我们的框架对基于训练的代理的诗歌生成过程带来了好处，导致n-gram多样性增加了3.0-3.7个百分点，新颖性增加了5.6-11.3个百分点。基于训练的代理生成的诗歌在词汇、风格和语义上表现出群体分化。在我们的框架中，基于提示的代理也从非合作环境中受益，并且具有非同质代理的更多样化的模型集合有可能进一步提高多样性，我们的实验结果显示多样性增加了7.0-17.5个百分点。然而，基于提示的代理显示出随着时间推移，词汇多样性减少，并且没有表现出预期的群体分化意图的社会网络。我们的论文主张，在自动诗歌生成等创意任务中，需要将社会学习过程（通过基于大型语言模型的代理建模）纳入考虑范围，以模仿人类的交互方式。**|
|**2024-09-05**|**Rx Strategist: Prescription Verification using LLM Agents System**|Phuc Phan Van et.al.|[2409.03440](http://arxiv.org/abs/2409.03440)|null|为了保障患者安全，现代药物复杂性要求严格处方验证。我们提出了一种新的方法——Rx Strategist，它利用知识图谱和不同的搜索策略，结合代理框架中的大型语言模型（LLMs），以增强其能力。这种多维度的技术允许构建一个多阶段的LLM管道，并从自定义活性成分数据库中可靠地检索信息。该管道覆盖了处方验证的不同方面，如适应症、剂量和可能的药物相互作用，每个阶段都包含了这些方面的内容。  通过在这些阶段分散推理，我们缓解了单一LLM技术的缺点，提高了正确性和可靠性，同时减少了内存需求。我们的研究结果表明，Rx Strategist超越了许多当前的LLMs，其性能与经验丰富的临床药师相当。在现代药物的复杂世界中，将LLMs与组织化知识和高级搜索方法相结合，提供了一条减少处方错误并提高患者结果的可行途径。|
|**2024-09-05**|**GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**|Yukun Cao et.al.|[2409.03258](http://arxiv.org/abs/2409.03258)|null|虽然大型语言模型（LLMs）在处理图方面展现出潜力，但在通过描述序列的图说明来理解图形结构信息时，尤其是在图的大小增加时，它们遇到了挑战。我们归因于LLMs在图描述序列的不同位置上存在不均匀的记忆性能，即所谓的“位置偏见”。为了应对这一挑战，我们提出了GraphInsight，一个旨在提高LLMs对宏观和微观图形信息理解的新框架。GraphInsight基于两个关键策略：1）将关键图形信息放置在LLMs表现出更强记忆性能的位置；2）对于记忆性能较弱的区域，探索使用轻量级外部知识库，灵感来自于检索增强生成（RAG）。此外，GraphInsight还探索了将这两种策略集成到LLM代理流程中，以解决需要多步推理的复合图任务。广泛的基准实验表明，在不同大小的图形结构理解任务上，GraphInsight显著超越了所有其他图描述方法（例如提示技术、重新排序策略等）。|
|**2024-09-04**|**Large Language Model-Based Agents for Software Engineering: A Survey**|Junwei Liu et.al.|[2409.02977](http://arxiv.org/abs/2409.02977)|**[link](https://github.com/fudanselab/agent4se-paper-list)**|**本文提供了一篇全面且系统的关于大型语言模型（LLM）在软件工程（SE）中的应用的综述。我们收集了106篇论文，并从两个角度进行分类，即软件工程视角和代理视角。此外，我们还讨论了该领域面临的关键挑战以及未来的发展方向。此综述的仓库地址为：https://github.com/FudanSELab/Agent4SE-Paper-List。**|
|**2024-09-02**|**Evolution of Social Norms in LLM Agents using Natural Language**|Ilya Horiguchi et.al.|[2409.00993](http://arxiv.org/abs/2409.00993)|null|大型语言模型（LLM）的最新进展激发了利用这些模型进行游戏理论模拟的兴趣，在这些模拟中，LLM充当个体代理，进行社会互动。本文研究了通过自然语言对话使LLM代理自发生成并遵守规范策略的可能性，以此为基础，探索了对Axelrod的元规范游戏工作的进一步发展。我们的实验表明，通过对话，LLM代理能够仅通过自然语言交互形成复杂的社交规范，如元规范——规范惩罚不惩罚作弊行为的规范。结果证实了使用LLM代理模拟社会互动和理解通过自然语言演化出复杂策略与规范的有效性。未来的工作可能通过扩展到更广泛的场景和代理特征，揭示更多关于社会规范形成的微妙机制。|
|**2024-09-02**|**Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces**|Jiapeng Yu et.al.|[2409.00985](http://arxiv.org/abs/2409.00985)|**[link](https://github.com/yuqian2003/co_learning)**|**基于大型语言模型的在线问答系统从娱乐用途逐渐转向专业领域应用。本文提出了一种名为“代码学习（Co-Learning）社区”的多代理框架，结合环境强化学习（E-RL），旨在帮助初学者独立修正代码错误。该系统通过一个包含702个错误代码的原始数据集评估了多个大型语言模型的表现，并将其作为E-RL奖励或惩罚的标准。通过分析当前代理输入的错误代码，选择合适的基于大型语言模型的代理以实现最佳的错误修正准确率并减少修正时间。  实验结果表明，与无E-RL方法相比，该方法在精确度得分上提高了3%，在时间成本上降低了15%。我们的源代码可访问：https://github.com/yuqian2003/Co_Learning**|
|**2024-08-29**|**HoneyComb: A Flexible LLM-Based Agent System for Materials Science**|Huan Zhang et.al.|[2409.00135](http://arxiv.org/abs/2409.00135)|null|为了应对材料科学任务中的复杂性并解决大型语言模型（LLM）在这一领域应用时所面临的问题，如依赖过时的隐性知识导致的准确性下降和幻觉现象，我们提出了HoneyComb——首个专门针对材料科学领域的LLM代理系统。HoneyComb通过利用一个基于可靠文献的高质量材料科学知识库（MatSciKB）和一种创新的工具集（ToolHub），增强其针对材料科学特有的推理与计算能力。  MatSciKB是一个经过精心编纂、结构化的知识集合，旨在涵盖材料科学领域的关键信息。而ToolHub则采用了一种归纳式工具构建方法，用于生成、分解和优化适用于材料科学的API工具，从而极大地提高了系统的实用性。此外，HoneyComb还配备了一个检索模块，该模块能够根据特定任务智能选择最合适的知识来源或工具，确保了答案的准确性和相关性。  实验结果表明，HoneyComb在材料科学领域的各种任务上均表现出显著优于基线模型的能力，成功地弥合了当前LLM技术与材料科学特定需求之间的差距。更为重要的是，我们的可扩展框架易于扩展至其他科学领域，展示了其在推动科学研究和应用发展方面具有广泛的应用潜力。|
|**2024-08-30**|**Tool-Assisted Agent on SQL Inspection and Refinement in Real-World Scenarios**|Zhongyuan Wang et.al.|[2408.16991](http://arxiv.org/abs/2408.16991)|null|本文提出了一种基于工具辅助的代理框架，用于SQL检查和改进，旨在提升大型语言模型（LLM）处理现实世界查询的能力。该框架通过为LLM代理配备两个专门工具——检索器和检测器，以诊断并修正SQL查询中的数据库不匹配问题。这些工具能够增强LLM处理真实场景中出现的条件不匹配和严格约束不匹配等数据库不匹配问题的能力。  我们还引入了Spider-Mismatch，这是一个专门为反映现实世界中遇到的条件不匹配问题而构建的新数据集。实验结果表明，在少量示例设置下，我们的方法在Spider和Spider-Realistic数据集上的平均表现最佳，并且显著优于基线方法，在更具有现实性的数据集Spider-Mismatch上也表现出更好的性能。|
|**2024-08-28**|**EPO: Hierarchical LLM Agents with Environment Preference Optimization**|Qi Zhao et.al.|[2408.16090](http://arxiv.org/abs/2408.16090)|**[link](https://github.com/kevinz8866/epo)**|本文提出了一种分层框架，用于解决复杂任务分解为可管理子目标的问题。框架使用了独立的语言模型进行子目标预测和低级动作生成。针对无标注数据集的训练信号创建挑战，我们开发了一个奖励模型，利用环境多模态反馈自动生成奖励信号。我们引入了环境偏好优化（EPO）方法，该方法从环境反馈中生成偏好信号，并利用这些信号训练基于语言模型的代理。ALFRED实验结果表明，我们的框架在性能上处于领先地位，首次登上了ALFRED公开排行榜，并展示了其在不同环境中的长期决策制定能力的提升潜力。|
|**2024-09-05**|**LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models**|Jiayi Gui et.al.|[2408.15778](http://arxiv.org/abs/2408.15778)|**[link](https://github.com/hypatiaalegra/logicgame-data)**|本文介绍了一个名为LogicGame的新基准，旨在评估大型语言模型（LLMs）在规则理解和执行、多步规划方面的全面能力。不同于传统的基准测试，LogicGame提供了多种游戏，其中包含一系列规则以及初始状态，要求模型理解并应用预定义规则来解决问题。我们创建了模拟情景，让模型执行或规划操作以达到特定目标。这些游戏场景专门设计以区分逻辑推理与仅依赖知识的能力，完全依赖于预设规则。这种分离允许对基于规则的推理能力进行纯粹的评估。评估不仅考虑最终结果，还考虑中间步骤，提供模型性能的全面评估。此外，这些中间步骤是确定性的，并且可以自动验证。LogicGame定义了从简单规则应用到复杂推理链的不同难度级别的游戏场景，以精确评估模型在规则理解和多步执行上的性能。通过使用LogicGame，我们测试了各种LLM，并发现了它们在基于规则的逻辑推理能力方面的显著不足。|
|**2024-08-27**|**AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems**|Chi-Min Chan et.al.|[2408.14972](http://arxiv.org/abs/2408.14972)|**[link](https://github.com/chanchimin/agentmonitor)**|**快速发展的大型语言模型（LLM）推动了基于LLM的代理兴起。近期研究发现，在多代理系统（MAS）中，每个代理执行特定角色时，其性能通常优于单一LLM。然而，配置MAS以完成任务仍然具有挑战性，因为任务表现仅在执行后才能观察到。受到LLM开发中的规模法则启发，我们探索是否能在任务执行前预测MAS的性能。为此，我们引入了AgentMonitor框架，该框架在代理层级集成，用于捕获输入和输出信息，并将这些信息转换为统计数据，用于训练回归模型预测任务性能。此外，AgentMonitor还能够实时对可能由恶意代理引发的安全风险进行纠正，从而减轻负面影响并增强MAS的安全性。  实验结果表明，使用XGBoost模型在领域内场景下达到0.89的斯皮尔曼相关系数，在更具挑战性的场景下达到0.58。通过应用AgentMonitor，有害内容减少了6.2%，有益内容平均增加了1.8%，这显著提高了安全性和可靠性。相关的代码已开源在<https://github.com/chanchimin/AgentMonitor>。**|
|**2024-08-26**|**LLM-3D Print: Large Language Models To Monitor and Control 3D Printing**|Yayati Jadhav et.al.|[2408.14307](http://arxiv.org/abs/2408.14307)|null|行业4.0通过推动数字化进程并转向增材制造（AM），彻底改变了制造业。熔融沉积建模（FDM）作为关键的AM技术之一，通过逐层挤出方式创建高度定制、成本效益高且材料浪费极小的产品，对传统减材方法构成了重大挑战。然而，材料挤出技术的易错性往往需要专家介入来检测和缓解可能严重损害产品质量的缺陷。虽然已存在自动化错误检测和机器学习模型，但它们在不同3D打印机设置、固件和传感器之间的通用性有限，并且深度学习方法需要大量的标记数据集，这限制了其规模性和适应性。  为了解决这些挑战，我们提出了一种利用大型语言模型（LLMs）与3D打印技术相结合的过程监控和控制框架，旨在检测和解决打印缺陷。该LLM通过分析每层或打印段之后捕获的图像来评估打印质量，识别故障模式，并向打印机查询相关参数。然后，它生成并执行纠正措施计划。我们通过将提出的框架的有效性与一组具有不同AM专业知识的工程师进行了比较，以验证识别缺陷的能力。我们的评估表明，基于LLM的代理不仅准确识别常见的3D打印错误，如不一致的挤出、丝状堆积、翘曲和层粘合问题，而且还能有效确定导致这些失败的参数，并自主地进行修正，无需任何人工干预。|
|**2024-09-02**|**MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents**|Ruochen Li et.al.|[2408.14033](http://arxiv.org/abs/2408.14033)|**[link](https://github.com/du-nlp-lab/mlr-copilot)**|**机器学习研究对于技术进步和创新至关重要，但常常面临复杂性高、实验周期长以及需要专业知识等挑战。为了应对这些挑战，我们提出了一种新的系统框架——自主机器学习研究与大型语言模型（MLR-Copilot），旨在通过利用大型语言模型（LLM）代理自动生成并实施研究想法来提高机器学习研究的生产力。该框架包含三个阶段：研究想法生成、实验实现和执行。首先，通过基于LLM的IdeaAgent利用现有研究论文生成假设和实验计划。接下来，在实现生成阶段，将这些计划转化为可执行代码，使用ExperimentAgent完成此过程。此阶段利用检索到的原型代码，并根据需要检索候选模型和数据。最后，在执行阶段，也由ExperimentAgent管理，涉及运行实验，并通过人类反馈和迭代调试机制，以增加实现可执行研究成果的可能性。我们对五个机器学习研究任务进行了评估，实验结果表明了该框架促进研究进展和创新的潜力。**|
|**2024-08-26**|**AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework**|Jie Feng et.al.|[2408.13986](http://arxiv.org/abs/2408.13986)|**[link](https://github.com/tsinghua-fib-lab/agentmove)**|**人类移动性预测在各种实际应用中扮演着关键角色。尽管深度学习模型在过去十年中显示出有希望的结果，但它们对用于训练的大量私人移动数据的依赖以及无法进行零启动预测的能力，阻碍了进一步的发展。最近，有人尝试使用大型语言模型（LLMs）来执行移动性预测任务。然而，他们的性能受限于缺乏系统的设计工作流程。他们直接使用LLMs生成最终输出，这限制了LLMs发现复杂移动模式的潜力，并低估了它们在全球地理空间知识方面的巨大储备。本文提出了一种名为AgentMove的系统性代理预测框架，以实现对任何全球城市的通用移动性预测。在AgentMove中，我们首先将移动性预测任务分解为三个子任务，并设计相应的模块来完成这些子任务，包括个体移动模式挖掘的空间-时间记忆、城市结构效应对模型的影响的全球知识生成器以及捕获人口共享模式的集体知识提取器。最后，我们将三个模块的结果结合起来，并执行推理步骤以生成最终预测。在来自两个来源的12个城市的数据上进行的广泛实验表明，与最佳基线相比，AgentMove在各种指标上的性能提高了超过8%，并且在不同城市中显示出了稳健的预测结果，且使用不同基础的LLM时也能表现出色，且具有较低的地理偏见。代码和数据可以在https://github.com/tsinghua-fib-lab/AgentMove找到。**|
|**2024-08-23**|**Optimizing Collaboration of LLM based Agents for Finite Element Analysis**|Chuan Tian et.al.|[2408.13406](http://arxiv.org/abs/2408.13406)|null|本文探讨了大型语言模型（LLM）在编程和编码任务中的多代理交互。我们利用AutoGen框架促进代理之间的沟通，并基于每种设置的40次随机运行的成功率评估不同的配置。研究重点在于开发一个灵活的自动化框架，用于将有限元方法应用于解决线性弹性问题。我们的发现强调了优化代理角色及其明确职责的重要性，而不仅仅是增加代理数量。代理间的有效协作被证明对于解决有限元方法的一般挑战至关重要。这项研究展示了LLM多代理系统增强计算自动化在模拟方法学中的潜力，为工程和人工智能的未来进展铺平道路。|
|**2024-09-01**|**Can LLMs Understand Social Norms in Autonomous Driving Games?**|Boxuan Wang et.al.|[2408.12680](http://arxiv.org/abs/2408.12680)|null|本文探讨了大型语言模型（LLM）在理解与模拟自主驾驶游戏中社会规范的应用。通过将LLM集成到自主驾驶游戏中的智能代理角色中，我们基于文本提示让这些代理按照相关环境设定和观察信息做出决策。我们的框架涉及LLM驱动的代理在多代理系统（MAS）中进行马尔科夫游戏，以此研究个体代理之间社会规范的形成。  我们设计实验，利用OpenAI聊天API（由GPT-4.0提供动力）在无信号交叉口游戏与高速公路车队游戏两种场景下模拟交互并评估LLM驱动代理的表现。结果显示，LLM驱动的代理能够处理马尔科夫游戏中的动态环境变化，并且在两个场景中，代理间形成了社会规范。  在交叉口游戏中，当面临潜在车祸时，LLM驱动的代理倾向于采取保守的驾驶策略。LLM驱动代理在游戏中的优势在于其操作灵活性和可分析性，这有助于实验设计。|
|**2024-08-22**|**MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents**|Congchi Yin et.al.|[2408.12142](http://arxiv.org/abs/2408.12142)|**[link](https://github.com/lemonsis/mdd-5k)**|**在大多数精神疾病诊断中，临床医生与患者的对话是主要的诊断依据。创建这样的诊断对话数据集有望推动AI精神健康护理领域的发展。然而，直接在实际诊断场景中收集对话极为困难，原因在于隐私和伦理考虑的严格限制。为解决这一问题，我们尝试通过利用易于获取的匿名患者案例来合成诊断对话。具体而言，我们设计了一个神经符号多代理框架，使用大型语言模型合成精神障碍的诊断对话。该框架以患者案例作为输入，并能够生成针对单个患者案例的多个多样化的对话，其基本过程涉及医生代理与患者代理之间的互动，并通过工具代理实现基于符号控制的文本生成，借助动态诊断树。通过应用提出的方法，我们开发了包含1000个清洗过的实际患者案例、与一家领先的精神病医院合作构建的中国最大精神障碍诊断数据集MDD-5k，该数据集包含了5000个高质量的长对话及其诊断结果标签。据我们所知，这是第一个包含中文精神障碍诊断结果的标记数据集。人类评估表明，提出的MDD-5k数据集成功模拟了精神障碍的诊断过程。数据集和代码将在https://github.com/lemonsis/MDD-5k公开提供。**|
|**2024-08-20**|**FLAME: Learning to Navigate with Multimodal LLM in Urban Environments**|Yunzhe Xu et.al.|[2408.11051](http://arxiv.org/abs/2408.11051)|**[link](https://github.com/xyz9911/FLAME)**|**大型语言模型（LLM）在视觉与语言导航（VLN）任务中展现出了潜在能力，但当前的应用仍面临挑战。虽然LLM在通用对话场景中表现出色，但在专门的导航任务上却表现不佳，相较于专为VLN设计的模型，其性能较差。为此，我们提出了一种名为FLAME（FLAMingo架构化实体代理）的新颖多模态LLM基元体和架构，旨在解决城市VLN任务，并有效处理多个观察结果。我们的方法采用了三阶段调优技术以适应导航任务，包括单感知调整以描述街景、多感知调整以总结轨迹以及在VLN数据集上进行端到端训练。合成的数据集是自动生成的。实验结果显示，FLAME在Touchdown数据集上的任务完成率优于现有方法，提高了7.3%。这项工作展示了多模态LLM在复杂导航任务中的潜力，并代表了迈向实际应用中多模态LLM于实体AI领域的进步。项目页面：https://flame-sjtu.github.io**|
|**2024-08-20**|**Athena: Safe Autonomous Agents with Verbal Contrastive Learning**|Tanmana Sadhu et.al.|[2408.11021](http://arxiv.org/abs/2408.11021)|null|由于新兴能力的加持，大型语言模型（LLMs）被用作基于语言的代理，执行各种任务并作出日益自主的决策。这些自主代理能够理解高级指令、与环境互动，并使用可用工具集执行复杂任务。随着代理能力的扩展，确保其安全性和可信度变得愈发重要。本研究引入了Athena框架，利用了“口头对比学习”的概念，通过将过去的安全和不安全轨迹作为上下文（对比）示例来指导代理在完成给定任务的同时确保安全。该框架还整合了一种批判机制，以指导代理在每一步防止风险行为。此外，鉴于缺乏现有基准来评估基于LLM的代理的安全推理能力，我们收集了80个工具包，覆盖8个类别，共计180个场景，提供了一个安全评估基准。我们的实验评估显示，口头对比学习和交互级批判显著提高了安全性率。|
|**2024-08-24**|**IDEA:Enhancing the Rule Learning Ability of Language Agents through Induction, Deduction, and Abduction**|Kaiyu He et.al.|[2408.10455](http://arxiv.org/abs/2408.10455)|null|本文提出了一项名为RULEARN的新基准，旨在评估大型语言模型（LLMs）在交互环境中的归纳推理能力。在RULEARN中，代理通过与环境互动收集观察，并从中推断模式，以此解决问题。为了增强LLM代理在该基准上的归纳推理能力，我们引入了IDEA代理，它结合了归纳、演绎和溯因三种推理过程。IDEA代理通过结构化推理序列提升这一方法：首先通过溯因生成假设，然后通过演绎验证这些假设，最后根据反馈进行适应性修正。这种序列使代理能够动态建立并应用规则，模仿人类的推理过程。通过对五种代表性LLM的评估显示，尽管这些模型能够生成合理的初始假设，但在环境内的战略互动、有效整合反馈以及假设的适应性修正方面存在困难。而IDEA代理在RULEARN基准上表现出显著的性能提升，为我们开发能在现实世界场景中实现类似人类规则学习能力的代理提供了宝贵见解。我们将会发布我们的代码和数据。|
|**2024-08-20**|**MegaAgent: A Practical Framework for Autonomous Cooperation in Large-Scale LLM Agent Systems**|Qian Wang et.al.|[2408.09955](http://arxiv.org/abs/2408.09955)|null|随着大型语言模型（LLM）的兴起，LLM驱动的多智能体系统（LLM-MA系统）被提出以应对实际任务。然而，这些系统的智能体大多遵循在整体交互过程中保持不变的预定义标准操作程序（SOP），缺乏自主性和可扩展性。此外，当前解决方案往往忽视了有效智能体合作的必要性。为了克服上述限制，我们提出了MegaAgent，一个旨在促进大规模LLM智能体系统中自主合作的实用框架。MegaAgent利用智能体的自主性动态生成基于任务需求的智能体，集成了任务自动划分、智能体活动系统级规划与监控以及并发操作管理等功能。此外，MegaAgent采用层次结构设计，并利用系统级并行性来提升性能和增强通信效率。  我们通过围棋游戏开发展示了MegaAgent的有效性，证明它在性能上超越了流行的LLM-MA系统；并通过国家政策模拟验证了其高自主性和快速扩展至590个智能体的能力，同时确保了它们之间的有效合作。我们的结果表明，MegaAgent是首个无预定义SOP、高效且具有高可扩展性的大规模LLM-MA系统，为该领域的进一步研究铺平了道路。我们的代码位于<https://anonymous.4open.science/r/MegaAgent-81F3>。|
|**2024-08-19**|**GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making**|Arsham Gholamzadeh Khoee et.al.|[2408.09785](http://arxiv.org/abs/2408.09785)|null|在汽车行业中，传统软件部署决策方法通常依赖于对表格化测试数据的手动分析。这些方法往往导致更高的成本和软件发布周期的延迟，主要是由于它们的劳动密集型特性。大型语言模型（LLM）为解决这些问题提供了有前景的解决方案。然而，它们的应用通常需要多轮的人工驱动提示工程，这限制了其在工业最终用户中的实际部署，特别是那些需要可靠和高效结果的用户。本文提出了一种名为GoNoGo的LLM代理系统，旨在简化汽车软件部署过程，同时满足功能要求和工业约束。与以往系统不同，GoNoGo特别针对特定领域和风险敏感系统进行了定制。我们使用来自工业实践的零次和少量次示例来评估GoNoGo在不同任务难度下的性能。结果显示，GoNoGo在难度不超过二级的3次示例任务中实现了100%的成功率，并且即使对于更复杂的任务也能保持高绩效。我们发现，GoNoGo有效地自动化了较简单任务的决策过程，显著减少了手动干预的需求。总之，GoNoGo代表了一个目前在我们的工业合作伙伴公司中被用于协助软件发布决策的高效且用户友好的LLM基解决方案，支持了风险敏感车辆系统发布过程中的更加明智和及时的决策。|
|**2024-08-18**|**HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model**|Mengkang Hu et.al.|[2408.09559](http://arxiv.org/abs/2408.09559)|**[link](https://github.com/hiagent2024/hiagent)**|**大型语言模型（LLM）驱动的代理在各个领域展现出巨大潜力，作为能够处理环境观察并生成执行动作以完成目标任务的交互系统。这些代理的有效性很大程度上受到其记忆机制的影响，该机制通过记录历史经验来形成一系列动作-观察对序列。我们将记忆分为两类：跨试记忆，积累于多次尝试中；以及单试记忆（工作记忆），积累于单一尝试内。尽管关于跨试记忆优化的研究已取得显著进展，但如何通过提升工作记忆利用效率来增强代理性能的探索仍相对不足。现有方法往往直接将整个历史动作-观察对输入到LLM中，导致在长期任务中存在冗余问题。受人类解决问题策略的启发，本文提出了一种名为HiAgent的框架，旨在通过将子目标作为记忆块来对LLM驱动的代理的工作记忆进行层次化管理。具体来说，HiAgent促使LLM在生成执行动作前先制定子目标，并允许LLM主动决定替换之前的子目标，仅保留与当前子目标相关的动作-观察对。在五个长期任务上的实验结果表明，HiAgent的成功率提高了两倍，平均步骤数减少了3.8个。此外，我们的分析显示，HiAgent在整个步骤中均能持续改善性能，这凸显了其稳健性和泛用性。  项目页面：https://github.com/HiAgent2024/HiAgent**|
|**2024-08-15**|**EmBARDiment: an Embodied AI Agent for Productivity in XR**|Riccardo Bovo et.al.|[2408.08158](http://arxiv.org/abs/2408.08158)|null|XR设备搭载由大型语言模型（LLMs）驱动的聊天机器人具有巨大的潜力，可以作为始终在线的代理，从而实现更高效的工作流程。然而，基于屏幕的聊天机器人并未充分利用XR所提供的全面自然输入，包括内部面向的传感器数据，而是过度依赖明确的声音或文本提示，有时还会与作为查询的一部分投射的多模态数据配对。我们提出了一种解决方案，利用注意力框架从用户行为、注视点和XR环境中的上下文记忆中隐式地推导出背景信息，从而最小化对工程化明确提示的需求，促进基于现实世界且直观的交互，这些交互能够洞察用户的见解并为聊天机器人提供信息。我们的用户研究展示了我们方法的可行性和在XR中与聊天机器人进行交互的潜在变革性，同时也为未来XR-实体LLM代理的设计提供了见解。|
|**2024-08-15**|**Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework**|Changyu Du et.al.|[2408.08054](http://arxiv.org/abs/2408.08054)|null|传统的建筑信息模型（BIM）创建过程通常要求设计师掌握复杂且繁琐的建模命令，以在BIM创建工具中实现其设计意图。这种额外的认知负担使设计过程变得复杂，并阻碍了建筑、工程和施工（AEC）行业对BIM和基于模型的设计的采用。  为了更直观地表达设计意图，我们提出了一种基于大型语言模型（LLM）的多代理框架——Text2BIM。该框架能够从自然语言指令生成3D建筑模型。它通过协调多个LLM代理协作并推理，将文本用户输入转换为调用BIM创建工具API的指令代码，从而在软件中生成具有内部布局、外部外壳和语义信息的可编辑BIM模型。此外，引入了一种基于规则的模型检查器，利用预定义的领域知识指导LLM代理解决生成模型中的问题，并迭代改进模型质量。  进行了大量实验来比较和分析在提议框架下三种不同LLM的表现。评估结果表明，我们的方法能够有效地生成高质量、结构合理且与用户输入指定的抽象概念相一致的建筑模型。  最后，开发了一个交互式软件原型，将该框架集成到BIM创建软件Vectorworks中，展示了通过聊天进行建模的潜力。|
|**2024-08-13**|**Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents**|Pranav Putta et.al.|[2408.07199](http://arxiv.org/abs/2408.07199)|null|大型语言模型（LLM）在需要复杂推理的自然语言任务上展现了惊人的能力，但在交互环境中进行自主代理的多步骤推理应用仍然是一个挑战。传统的基于静态数据集的监督预训练不足以使自主代理具备在动态设置如网络导航中执行复杂决策所需的自主能力。以往通过监督微调来填补这一差距的方法往往面临累积错误和探索数据有限的问题，导致政策结果不佳。为了克服这些挑战，我们提出了一种框架，结合了引导式蒙特卡洛树搜索（MCTS）搜索与自我批判机制，并使用离策略变体的直接偏好优化（DPO）算法对代理互动进行迭代微调。这种方法允许LLM代理从成功和失败的轨迹中有效学习，从而在复杂、多步骤推理任务中提高其泛化能力。我们在WebShop环境（一个模拟电子商务平台）中验证了我们的方法，该环境在与行为克隆和强化微调基线相比时表现出色，并在配备在线搜索能力的情况下击败了平均人类性能。在实际预订场景中，我们的方法提高了Llama-3 70B模型的零射成功率从18.6%增加到81.7%（相对增加了340%），并在一天的数据收集后进一步增加到95.4%，并且通过在线搜索。我们认为这标志着自主代理能力的一个重大进步，在现实世界环境中实现更高级和可靠决策的道路。|
|**2024-08-13**|**Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents**|Kexun Zhang et.al.|[2408.07060](http://arxiv.org/abs/2408.07060)|null|大型语言模型（LLM）代理在解决实际世界软件工程（SWE）问题方面展现出巨大的潜力。最先进开源的SWE代理能够解决SWE-Bench Lite中超过27%的实际GitHub问题。然而，这些复杂的代理框架在表现上存在差异，有的在特定任务中表现出色，在其他任务中则表现不佳。为了充分利用这些代理的多样性，我们提出了DEI（多元化智能），一个旨在利用其独特专长的框架。DEI作为现有SWE代理框架之上的元模块，管理代理集体以实现增强的问题解决能力。  实验结果显示，通过DEI指导的代理委员会能够显著超越单个代理的最佳性能。例如，一组开源SWE代理，其最高个体解决率在SWE-Bench Lite中为27.3%，在应用了DEI后，能够达到34.3%的解决率，实现了25%的改进，并击败了许多闭源解决方案。我们的最佳表现团队以55%的解决率在SWE-Bench Lite中取得最高排名。我们的研究结果对合作AI系统的研究领域做出了贡献，揭示了它们在解决复杂软件工程挑战方面的潜力。|
|**2024-08-12**|**Hierarchical in-Context Reinforcement Learning with Hindsight Modular Reflections for Planning**|Chuanneng Sun et.al.|[2408.06520](http://arxiv.org/abs/2408.06520)|null|大型语言模型（LLM）在各种语言任务上表现出惊人的能力，这使它们成为机器人决策的有希望候选者。受到层次强化学习（HRL）的启发，我们提出了一种新颖框架——在上下文中进行层次化的强化学习（HCRL）。该框架通过LLM基高层策略分解复杂任务，即通过在执行时动态分解复杂任务为子任务，从而利用高阶策略来定义目标，这些目标由子任务组成，并分配给低阶策略以完成。一旦LLM代理确定目标已完成，则会提出新的目标。  为了提高多轮执行中的代理性能，我们提出了事后模块化反思（HMR），其中，代理不是对完整轨迹进行反思，而是将任务目标替换为中间目标，并让代理对较短的轨迹进行反思，以提高反思效率。我们在三个基准环境中评估了所提出的HCRL的决策能力——ALFWorld、Webshop和HotpotQA。结果表明，与强大的上下文学习基线相比，在五轮执行中，HCRL可实现9%、42%和10%的性能提升。|
|**2024-08-12**|**Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example**|Yanan Chen et.al.|[2408.06318](http://arxiv.org/abs/2408.06318)|null|本文旨在填补大型语言模型（LLM）在自主代理与人工通用智能（AGI）接近过程中研究的空白。尽管LLM展现出出色的泛化能力和涌现能力，但目前缺乏对LLM驱动的代理行为、潜在失败原因以及如何提升其性能的研究，尤其是在具有挑战性的现实世界规划任务中的表现。为了填补这一缺口，我们利用了一个名为TravelPlanner的真实基准，其中的代理必须满足多个约束以生成准确的计划。通过TravelPlanner基准，我们针对四个关键研究问题进行了全面的实验：（1）LLM代理在处理长篇和嘈杂上下文时，对于推理和规划的鲁棒性是否足够？（2）少量提示是否会损害LLM代理在长上下文场景下的性能？（3）我们能否依赖细化来改进计划？（4）对LLM进行正负反馈结合的微调是否能带来进一步的提升？  实验结果表明：首先，尽管LLM能够处理大量的参考信息和少量示例，它们在关注长上下文中关键部分的能力上仍然存在不足；其次，它们在分析长计划方面仍面临挑战，并且无法提供准确的反馈用于细化；第三，我们提出了Feedback-Aware Fine-Tuning（FAFT），一种利用正负反馈相结合的方法，相较于纯监督微调（SFT），FAFT在性能上取得了显著提升。我们的发现为社区提供了关于现实世界规划应用方面的深入见解。|
|**2024-08-13**|**DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts**|Mohammed Saidul Islam et.al.|[2408.05346](http://arxiv.org/abs/2408.05346)|**[link](https://github.com/saidul-islam98/DataNarrative)**|数据驱动的故事叙述是一种强大的方法，通过结合叙事技巧与可视化和文本，来传达见解。这些故事融合了图表中的突出条形和线条以及解释见解的文本注释。然而，创建这样的故事需要对数据有深入的理解，并且需要精心的叙事规划，通常需要人类的介入，这既耗时又费心。虽然大型语言模型（LLMs）在各种NLP任务上表现出色，但在生成连贯和全面的数据故事方面的潜力仍然未被充分探索。为此，我们引入了一个新的任务——数据故事生成，并提供了一个包含来自不同来源的1,449个故事的基准。为了应对创造连贯数据故事的挑战，我们提出了一种多代理框架，利用两个LLM代理来模仿人类讲故事的过程：一个用于理解并描述数据、生成大纲和叙述，另一个则在每个中间步骤进行验证。尽管我们的代理框架在基于模型和人类评估中通常优于非代理对手，但结果也揭示了数据故事生成的独特挑战。|
|**2024-08-08**|**Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions**|Qingbin Zeng et.al.|[2408.04168](http://arxiv.org/abs/2408.04168)|**[link](https://github.com/hiyouga/llama-factory)**|本文探讨了城市导航场景下的AI代理问题：提供目标位置与知名地标之间的语言描述；仅通过观察周围环境，包括识别地标和道路网络连接，代理需要作出决策以无指示地导航至目标位置。这一挑战性在于，它要求代理建立自身定位并获取复杂城市环境的空间表示，而地标往往不可见。在缺乏导航指令的情况下，这种能力对于代理在长距离城市导航中做出高质量决策至关重要。随着大型语言模型（LLMs）推理能力的涌现，一个吸引人的基础方法是提示LLMs对每次观察做出“反应”并据此作出决策。然而，这种方法的性能非常差，代理经常反复访问相同位置，并作出短视、不一致的决策。为解决这些问题，本文引入了一种新型的代理工作流程，其特征在于感知、反思和规划的能力。具体而言，我们发现经过微调的LLaVA-7B能够准确感知地标的方向和距离，适用于城市导航。此外，通过记忆机制实现反思，即存储过往经验并在当前感知下检索，以进行有效的决策论证。规划则利用反思结果生成长期计划，从而避免长距离导航中的短视决策。实验结果显示，设计的工作流程显著提高了LLM代理的导航能力，相较于最先进的基线方法。|
|**2024-08-11**|**CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases**|Xiangyan Liu et.al.|[2408.03910](http://arxiv.org/abs/2408.03910)|**[link](https://github.com/modelscope/modelscope-agent)**|**大型语言模型（LLM）在诸如HumanEval和MBPP的独立代码任务中表现出色，但它们在处理整个代码仓库时存在挑战。这促使研究界探索如何在仓库级别上增强LLM与代码库的交互。目前的解决方案依赖于基于相似性的检索或手动工具和API，每种方法都有其显著的缺点。基于相似性的检索在复杂任务中召回率往往较低，而手动工具和API通常针对特定任务，需要专家知识，降低了它们在不同代码任务和实际应用中的通用性。为了缓解这些限制，我们引入了CodexGraph系统，它结合了从代码仓库中提取的图数据库接口与LLM代理。通过利用图数据库的结构特性和图查询语言的灵活性，CodexGraph使LLM代理能够构建并执行查询，从而实现精确的、代码结构意识的上下文检索和代码导航。我们使用三个基准测试CodexGraph：CrossCodeEval、SWE-bench和EvoCodeBench。此外，我们开发了五个真实世界的编码应用。通过使用统一的图数据库模式，CodexGraph在学术和实际环境中都展示了竞争力和潜力，证明了其在软件工程领域的多用途性和有效性。我们的应用演示：https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent。**|
|**2024-08-07**|**Large Language Models for Base Station Siting: Intelligent Deployment based on Prompt or Agent**|Yanhu Wang et.al.|[2408.03631](http://arxiv.org/abs/2408.03631)|null|传统的基站选址（BSS）方法主要依赖于驾驶测试和用户反馈，这既费时又需要在通信、网络和优化方面具备专业知识的专家。随着大型语言模型（LLMs）及其相关技术的发展，特别是在提示工程和代理工程领域，网络优化将见证一场革命性的转变。这种转变涉及巧妙地使用精心设计的提示来向这些复杂而先进的LLMs注入人类经验和知识，并通过自然语言连接到人类用户，部署自主代理作为通信桥梁。这种集成代表了人工智能（AI）作为一种服务和AI使生活更便捷的未来范式。  作为初步探索，本研究首先开发了一个由LLM驱动的BSS优化框架，并提出了四种潜在的实现策略：基于优化提示的LLM（PoL）、人机交互的LLM（HiLL）、LLM驱动的自主BSS代理（LaBa）以及协同多个LLM驱动的自主BSS代理（CLaBa）。通过在真实数据上的评估，实验表明，借助提示的LLM和基于代理的LLM能够生成更为高效、成本效益高且可靠的网络部署，显著提高了BSS优化的效率并减少了不必要的手动参与。|
|**2024-08-05**|**Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information**|Yauwai Yim et.al.|[2408.02559](http://arxiv.org/abs/2408.02559)|null|Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.|
|**2024-08-05**|**From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future**|Haolin Jin et.al.|[2408.02479](http://arxiv.org/abs/2408.02479)|null|With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.|
|**2024-08-07**|**SpecRover: Code Intent Extraction via LLMs**|Haifeng Ruan et.al.|[2408.02232](http://arxiv.org/abs/2408.02232)|null|本文探讨了在大型语言模型（LLM）与程序分析能力结合的形式下，通过LLM代理自动执行程序改进和错误修复的高效低耗工作流程。由于程序改进或修复通常需要明确期望的行为规范，因此规范推断对于产生高质量的代码补丁至关重要。本研究旨在通过在软件项目中进行迭代代码搜索并配合规范推断来探索这一领域，从而从项目的结构和行为中推断出意图。捕获的意图将由审查者代理进行审查，以验证补丁的有效性，并提供对验证后补丁信心度量。  我们的方法“SpecRover”（AutoCodeRover-v2）建立在开源的LLM代理AutoCodeRover之上。在使用SWE-Bench完整集评估时，即针对2294个GitHub问题，我们的方法显示了相对于AutoCodeRover超过50%的效率提升。与现有的开源代理相比，我们的工作在解决SWE-Bench lite中的平均GitHub问题时，成本仅为0.65美元。SpecRover生成的解释能够为开发者提供更明确的信号，表明建议的补丁可以被有信心地接受。  此外，我们的工作还强调了即使在LLM时代，自动化程序修复技术中规范推断的重要性。|
|**2024-08-03**|**The Drama Machine: Simulating Character Development with LLM Agents**|Liam Magee et.al.|[2408.01725](http://arxiv.org/abs/2408.01725)|null|这篇论文探讨了使用多个大型语言模型（LLM）代理来模拟复杂动态角色在戏剧性场景中的应用。我们提出了一种“戏剧机器”框架，该框架协调了扮演不同“自我”和“超我”心理角色的LLM代理之间的互动。在角色扮演模拟中，这种设计允许在相互作用的对话和个体内部独白之间发展平行的交互。  我们将此框架应用于两个戏剧场景——面试和侦探故事，并比较了在有无“超我”影响下角色发展的差异。尽管是初步研究，但结果表明，这种方法能够产生更加细腻、适应性强的故事，这些故事随着一系列对话回合的发展而演变。我们讨论了基于LLM的角色扮演的不同方式以及这可能对AI主体性的概念化意味着什么。论文最后考虑了这一方法如何为思考AI模拟中内在冲突和社会表演性的作用提供了可能性。|
|**2024-08-03**|**WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization**|Liwenhan Xie et.al.|[2408.01703](http://arxiv.org/abs/2408.01703)|null|大型语言模型（LLM）通过对话式用户界面支持数据分析，以OpenAI的ChatGPT（原名Advanced Data Analysis或Code Interpreter）为代表。本质上，LLM生成代码以完成各种分析任务。然而，直接呈现原始代码可能会使逻辑变得模糊，并妨碍用户验证。为了赋予用户对由LLM执行的数据分析进行增强理解与控制的能力，我们提出了一种新颖的方法来将LLM生成的代码转换为实时交互式的可视化表示。在该方法中，用户可以实时获得清晰、分步的LLM代码可视化，允许他们理解、验证并修改分析中的每个数据操作。我们的设计决策基于一项探索用户实践与挑战的形成性研究（N=8）。此外，我们开发了名为WaitGPT的原型，并进行了一项用户研究（N=12），以评估其可用性和有效性。用户研究的结果表明，WaitGPT有助于监控和引导由LLM执行的数据分析，使参与者能够提高错误检测能力并增加对结果的整体信心。|
|**2024-08-03**|**Automated Phishing Detection Using URLs and Webpages**|Huilin Wang et.al.|[2408.01667](http://arxiv.org/abs/2408.01667)|null|### 摘要  本文项目聚焦于通过构建利用大型语言模型（LLM）的代理框架，以解决传统基于参考的钓鱼检测方法所面临的局限性。该框架通过主动获取和利用在线信息，提供了一个动态的参考系统，从而实现更精确的钓鱼检测。这一创新避免了依赖静态知识库的需求，显著提升了自动化安全措施的适应性和效率。  ### 项目概述  项目报告首先对现有解决方案进行了初步研究和问题分析，促使我们开发出新的框架。我们以模拟的LLM代理来展示框架，并详细阐述了构建所需的技术，随后提供了完整实施的实例及实验，用于评估新方法相对于同类解决方案的性能。结果显示，我们的方法在准确度上达到了0.945，相比现有解决方案DynaPhish高出0.445个百分点。  ### 性能与局限  实验结果表明，本框架能够显著提高当前基于参考的钓鱼检测方法的有效性，并具有适应实际应用的潜力。同时，我们也讨论了该方法的局限性，并提出了改进策略，旨在进一步提升其效能。  ### 结论  提出的框架为增强现有的基于参考的钓鱼检测手段提供了有效途径，并且具备被应用于实际场景的可能性。|
|**2024-08-01**|**AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation**|Mengkang Hu et.al.|[2408.00764](http://arxiv.org/abs/2408.00764)|null|大型语言模型（LLM）基于的代理已引起广泛关注并变得越来越流行。此外，规划能力是LLM基于代理的关键组成部分，涉及与环境的交互和执行动作以完成规划任务，通常包括从初始状态达到预期目标的过程。本文研究了通过指令调优增强LLM规划能力的方法，即代理训练。近期的研究表明，利用专家级轨迹对指令调优LLM能有效提升其规划能力。然而，现有工作主要集中在从手动设计的任务和环境中合成轨迹，这导致创建这些环境和任务的劳动密集型，限制了生成足够多样性和广泛性的轨迹。为解决这一限制，本文探索了自动化合成多样化环境以及规划任务的渐进难度范围，从简单到复杂。我们引入了一个框架，名为AgentGen，利用LLM首先生成环境，随后根据这些环境生成规划任务。  具体而言，为了提高环境多样性，我们提出使用包含不同领域特定文本段落的灵感语料库作为合成环境的上下文。此外，为了增加生成规划任务的难度多样性，我们提出了双向演化方法Bi-Evol，该方法从更容易和更难的方向进化规划任务，以合成具有平滑难度曲线的任务集。来自AgentBoard的评估结果显示，AgentGen显著提高了LLM的规划能力，例如，经过AgentGen指令调优的Llama-3 8B在整体性能上超越了GPT-3.5。而且，在某些任务中，它甚至超过了GPT-4。|
|**2024-08-01**|**Jailbreaking Text-to-Image Models with LLM-Based Agents**|Yingkai Dong et.al.|[2408.00523](http://arxiv.org/abs/2408.00523)|null|近期的进展显著提升了基于大型语言模型（LLM）的自主代理在自动任务解决能力方面的表现。然而，大多数基于LLM的代理主要集中在对话、编程或特定领域，这导致了在处理生成式AI安全任务时存在缺口。这些缺口主要是由LLM的幻觉问题以及缺乏明确指导原则所引发的。本文提出了一种名为Atlas的高级LLM基多代理框架，该框架集成了高效模糊化工作流程，专门针对针对文本到图像（T2I）模型的攻击行为，特别是针对具有安全性过滤器的T2I模型的“越狱”攻击。  Atlas利用视觉语言模型（VLM）来评估提示是否触发了T2I模型的安全性过滤器。然后，它通过迭代方式与LLM和VLM协作，生成一个绕过过滤器的替代提示。此外，Atlas通过利用多代理通信、上下文学习（ICL）记忆机制和思维链（COT）方法，增强了LLM在攻击场景中的推理能力。  我们的评估表明，Atlas成功地在无模型设置下对多个最先进的T2I模型进行了“越狱”，这些模型都配备了多模态安全性过滤器。同时，Atlas在查询效率和生成图像质量方面均超越了现有方法。|
|**2024-08-01**|**Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion**|Honglei Miao et.al.|[2408.00352](http://arxiv.org/abs/2408.00352)|null|文本到动作（Text-to-Motion，T2M）模型通过深度生成模型驱动的人类运动生成，在应用中展现出令人信服的能力。然而，这些模型从文本提示生成真实动作的能力引发了安全问题，尤其是当它们可能被恶意利用时。尽管对T2M的兴趣日益增长，但很少有方法专注于保护这些模型免受对抗性攻击的影响。现有针对文本到图像模型的工作对于独特的动作领域来说并不充分。  在本论文中，我们提出了一种名为ALERT-Motion的自主框架，它利用大型语言模型（LLMs）来构建针对黑盒T2M模型的有针对性的对抗性攻击。与先前的方法通过预定义规则修改提示不同，ALERT-Motion利用LLMs对人类动作的知识，自主生成微妙而强大的对抗性文本描述。该框架包含两个关键模块：一个适应性调度模块，构建了一个基于LLM的代理，以迭代地细化和搜索对抗性提示；以及一个多模态信息对比模块，提取与动作相关的关键语义信息，指导代理的搜索。  通过这一基于LLM的方法，ALERT-Motion能够构造查询受害模型以产生与目标动作高度匹配的输出的对抗性提示，同时避免明显的扰动。在流行的T2M模型上进行的评估显示了ALERT-Motion相对于先前方法的优越性，其对抗成功率更高，并且对抗性提示更加隐蔽。这项关于T2M对抗性攻击的开创性工作强调了随着运动生成技术的发展，开发防御措施的紧迫性，这促使我们进一步研究安全和负责任的部署。|
|**2024-07-31**|**Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**|Felix Ocker et.al.|[2407.21778](http://arxiv.org/abs/2407.21778)|null|我们提出了一种名为“tulip代理”的架构，旨在实现基于大型语言模型的自主智能体，具有对工具库中大量工具进行创建、读取、更新和删除的能力。与当前先进实现不同的是，“tulip代理”并不在系统提示中编码所有可用工具的描述，这会占用模型的上下文窗口，或在检索合适工具时嵌入整个提示。相反，“tulip代理”能够递归地在其可扩展的工具库中搜索合适的工具，该工具库作为向量存储实现。这种架构显著降低了推理成本，允许使用大量的工具库，并使代理能够适应并扩展其工具集。  我们通过数学领域中的多个消融研究来评估该架构，并展示了其在机器人领域的通用性应用。参考实现和基准测试可在github.com/HRI-EU/tulip_agent上获取。|
|**2024-07-31**|**Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**|Shanbo Cheng et.al.|[2407.21646](http://arxiv.org/abs/2407.21646)|**[link](https://github.com/byteresearchcla/realsi)**|在这篇论文中，我们提出了一种高质量且接近人类水平的实时语音翻译系统——跨语言代理——同时口译，简称CLASI。受专业口译员启发，我们采用了创新的数据驱动读写策略来平衡翻译质量和延迟时间。为了应对翻译领域特定术语的挑战，CLASI通过多模态检索模块获取相关资料以增强翻译内容。借助大型语言模型的支持，我们的方法能够考虑输入音频、历史语境以及检索到的信息，生成容错性较高的翻译结果。实验结果显示，我们的系统在各项指标上均显著优于其他系统。  与专业口译员相媲美，我们使用了一个更好的评价指标——有效信息比例（VIP），它衡量了成功传达给听众的信息量。在现实世界场景中，演讲往往不流畅、非正式且模糊不清，CLASI在中英互译方向上的有效信息比例分别达到了81.3%和78.0%，而最先进的商业或开源系统仅分别为35.4%和41.6%。在极度困难的数据集上，当其他系统有效信息比例低于13%时，CLASI仍能实现70%的有效信息比例。|
|**2024-07-30**|**Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification**|Boyang Zhang et.al.|[2407.20859](http://arxiv.org/abs/2407.20859)|null|近期，基于大型语言模型（LLM）的自主代理在理论研究和实际应用方面均取得了显著进展。这些代理能够通过外部组件扩展基础LLM的能力，在多种方式下增强性能。例如，利用GPT-3.5-Turbo核心构建的代理可能在某些任务上超越更先进的GPT-4模型，关键在于其集成的工具可以使其在现实世界中执行操作，从单纯生成文本转向与环境的互动。鉴于代理在实际应用中的广泛部署及其对环境的直接影响能力，评估潜在漏洞变得至关重要。如果被恶意利用，这些自主系统可能造成的损害远大于单一语言模型。  现有研究已探讨了LLM代理可能引发的有害行为，但我们的研究从一个全新的视角出发，关注于导致系统故障的攻击方式——即误导代理执行重复或无关的操作，从而引发功能紊乱。我们通过采用多样化的攻击方法、场景和属性，进行了全面的评估，旨在揭示这些攻击的脆弱性所在。实验结果表明，在多种情况下，这些攻击能够诱导故障率超过80%。我们进一步在多代理系统中实施并部署了代理，以此突出此类漏洞所引发的现实风险。  为了应对上述攻击，我们提出了自我检查检测方法。然而，我们的研究发现，仅依靠LLM进行有效检测存在困难，这突显了该类漏洞所带来的重大风险。|
|**2024-07-28**|**The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies**|Feng He et.al.|[2407.19354](http://arxiv.org/abs/2407.19354)|null|受大型语言模型（LLM）快速发展的启发，LLM代理已发展到能够执行复杂任务。这些代理在各个领域广泛应用于处理大量数据以与人类互动并执行任务，这凸显了它们的商业价值。然而，这也暴露了安全和隐私漏洞。目前阶段，对LLM代理的安全性和隐私性进行全面研究至关重要。本文综述旨在全面概述新出现的隐私和安全问题，这些问题由LLM代理面临。  我们首先介绍LLM代理的基本知识，随后对其进行威胁分类和分析。接着讨论这些威胁对人类、环境和其他代理的影响。随后回顾现有防御策略，并最终探索未来趋势。此外，本文通过多种案例研究来促进更易于理解的解释。通过强调这些关键安全和隐私问题，本文旨在激发未来研究，以增强LLM代理的安全性和隐私性，从而在未来应用中提高其可靠性和可信度。|
|**2024-07-26**|**OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation**|Zilong Wang et.al.|[2407.19056](http://arxiv.org/abs/2407.19056)|**[link](https://github.com/zlwang-cs/OfficeBench)**|办公室自动化显著提高了人类的工作效率，通过自动完成工作流程中的常规任务。现有的人工智能文献主要集中在基本信息提取上，而办公室自动化研究应该扩展到更现实的办公室任务，这些任务需要整合办公室系统中的各种信息源，并通过一系列决策过程生成输出。我们引入了OfficeBench，这是第一个用于评估当前大型语言模型（LLM）代理在真实办公流程中处理办公任务能力的办公室自动化基准。  OfficeBench要求LLM代理进行可行的长期规划，高效地在应用程序之间切换，并基于工作流程的上下文需求，在庞大的联合动作空间内准确地定位其行动。通过在每个任务上应用我们的定制评估方法，我们发现GPT-4 Omni的通过率为47.00%，显示出在处理办公任务时具有不错的性能。然而，这仍然远低于实际办公流程所需的人类表现和准确性标准。  进一步观察发现，大多数问题与操作冗余、幻觉以及在多个应用程序之间切换的限制有关，这可能为开发有效的自动化代理框架提供有价值的见解。|
|**2024-07-30**|**MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**|Guoli Yin et.al.|[2407.18961](http://arxiv.org/abs/2407.18961)|**[link](https://github.com/apple/axlearn)**|**近期大型语言模型（LLM）的发展推动了对全面基准的需求，以评估它们作为类人类代理的能力。现有的基准虽然有用，但往往聚焦于特定的应用场景，强调任务完成而非深入剖析驱动这些结果的底层技能。这种缺乏细节性使得难以精确地识别失败的原因。此外，设置这些环境需要大量的工作，并且在交互式任务中，不一致性与可重复性问题有时会出现。为了应对这些局限性，我们引入了大规模多任务代理理解（MMAU）基准，它通过无需复杂环境设置的全面离线任务来实现。MMAU覆盖了五个领域：工具使用、有向无环图（DAG）问答、数据科学和机器学习编程、竞赛级别的编程和数学，并涵盖了五种关键能力：理解、推理、规划、问题解决和自我修正。总计包括20个精心设计的任务和超过3千个独特的提示，MMAU提供了一个全面框架，用于评估LLM代理的优势和限制。通过对18个代表性模型在MMAU上的测试，我们提供了深入而有洞察力的分析。最终，MMAU不仅揭示了LLM代理的能力和限制，还增强了对其性能的可解释性。MMAU的数据集和评估脚本已发布于https://github.com/apple/axlearn/tree/main/docs/research/mmau。**|
|**2024-07-29**|**PersonaGym: Evaluating Persona Agents and LLMs**|Vinay Samuel et.al.|[2407.18416](http://arxiv.org/abs/2407.18416)|null|Persona代理人，一种根据分配的人设行事的LLM代理，在各个应用领域展现出卓越的上下文响应能力。这些代理在教育、医疗保健和娱乐等不同行业中提供了显著的增强，因为模型开发者可以将代理响应与不同的用户需求对齐，从而扩展了代理应用的范围。然而，评估Persona代理性能极为困难，主要是由于在各种相关环境中的自由形式交互中评估人设一致性复杂性的挑战。我们引入了PersonaGym，首个动态评估框架，用于评估Persona代理，并提出了PersonaScore，首个基于决策理论的自动化人类对齐指标，用于全面大规模评估Persona代理。通过使用包含200个人设和10000个问题的基准，对6个开源和闭源的LLM进行评估，我们揭示了在最先进的模型中，Persona代理能力存在巨大的改进空间。例如，Claude 3.5 Sonnet的PersonaScore仅比GPT 3.5提高了2.97%，尽管Claude 3.5 Sonnet是一个更先进的模型。重要的是，我们发现模型大小和复杂性的增加并不一定意味着Persona代理能力的提升，这凸显了忠实和高效Persona代理算法和架构创新的迫切需要。|
|**2024-08-03**|**PyBench: Evaluating LLM Agent on various real-world coding tasks**|Yaolun Zhang et.al.|[2407.16732](http://arxiv.org/abs/2407.16732)|**[link](https://github.com/mercury7353/pybench)**|**为了填补现有基准在简化任务和复杂特定任务方面的局限性，我们引入了PyBench，一个涵盖五大类真实世界任务的基准。这些任务涉及超过10种类型的文件，旨在全面覆盖日常编码需求。当用户提出高阶查询并提供相关文件时，LLM代理需要通过代码解释器执行Python代码进行多轮推理，最终生成满足用户需求的回答。成功解决PyBench中的任务要求代理具备广泛的Python包理解能力、高级推理能力和从执行代码中获取反馈的能力。  我们的评估表明，当前开源的LLM模型在处理这些任务方面存在挑战。因此，我们对四种数据集进行了分析和实验，证明了解决PyBench所需的是全面的能力。我们精心调优的8B大小模型：PyLlama3，在PyBench上的表现令人兴奋，超越了许多更大规模（33B和70B）的模型。  我们的基准、训练数据集和模型在GitHub上提供：[https://github.com/Mercury7353/PyBench](https://github.com/Mercury7353/PyBench)**|
|**2024-07-23**|**LawLuo: A Chinese Law Firm Co-run by LLM Agents**|Jingyun Sun et.al.|[2407.16252](http://arxiv.org/abs/2407.16252)|**[link](https://github.com/nefujing/lawluo)**|**大型语言模型（LLM）在为非法律背景用户提供法律咨询服务方面展现了巨大的潜力，这主要得益于它们在文本理解和生成方面的卓越能力。然而，现有的中文法律LLM仅限于单个模型与用户之间的对话交互，与律师事务所中多员工共同参与的咨询形式不同。这种限制使得咨询体验不那么真实。此外，现有中文法律LLM存在关键问题：（1）对指导微调数据质量控制不足；（2）由于用户查询的模糊性导致模型产生幻觉；（3）在多轮对话中，模型遵循指令的能力下降。针对这些挑战，我们提出了一种名为“LawLuo”的新型法律对话框架，利用多个LLM代理的协作能力，每个代理负责不同的功能，共同为用户提供全面的法律咨询服务。此外，我们构建了两个高质量的法律对话数据集KINLED和MURLED，并使用ChatGLM-3-6b对数据集进行微调。我们还提出了一个名为ToLC的法律查询澄清算法。实验结果表明，与GPT-4等基线LLM相比，LawLuo在律师风格的语言表达、法律建议的有效性以及法律知识的准确性三个方面均表现出更优性能。我们的代码和数据集可访问于https://github.com/NEFUJing/LawLuo。**|
|**2024-07-21**|**Multi-Agent Causal Discovery Using Large Language Models**|Hao Duong Le et.al.|[2407.15073](http://arxiv.org/abs/2407.15073)|null|大型语言模型（LLM）在利用其从大量文本语料库中获取的广泛专家知识进行因果发现任务方面展示了巨大的潜力。然而，LLM在因果发现中的多代理能力尚未得到充分探索。本文提出了一种通用框架来研究这一潜力。首先，是元代理模型，它完全依赖于LLM代理之间的推理和讨论来进行因果发现。其次，是编码代理模型，它利用代理的规划、编写和执行代码的能力，结合高级统计库进行因果发现。第三，是混合模型，它将元代理模型和编码代理模型的方法相结合，融合了多个代理的统计分析和推理技能。我们的提议框架通过有效地利用LLM的专家知识、推理能力、多代理合作以及统计因果方法，显示出了有希望的结果。通过探索LLM的多代理潜力，我们旨在为利用LLM的多代理解决因果相关问题奠定基础。|
|**2024-07-19**|**KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models**|Kemou Jiang et.al.|[2407.14239](http://arxiv.org/abs/2407.14239)|null|大型语言模型（LLM）作为自主代理提供了一种通过知识驱动方式解决现实世界挑战的新途径。这些基于LLM的方法在泛化和可解释性方面表现出色。然而，驾驶任务的复杂性往往需要多个异构代理的合作，这凸显了LLM驱动的代理需要进行合作知识共享和认知协同的必要性。尽管LLM充满潜力，但当前的应用主要集中在单个代理场景。  为了拓展知识驱动策略的范围并增强自主代理的一般化能力，我们提出了KoMA框架，该框架包括多代理交互、多步规划、共享内存和基于排名的反思模块，旨在增强复杂驾驶场景下多代理的决策制定能力。根据框架生成的驾驶场景文本描述，多代理交互模块使LLM代理能够分析和推断周围车辆的意图，类似于人类的认知过程。多步规划模块使LLM代理能够逐层分析和获得最终行动决策，确保短期行动决策的一致目标。共享内存模块可以积累集体经验，以做出更优决策，而基于排名的反思模块则用于评估和改进代理行为，以提高驾驶安全性和效率。KoMA框架不仅增强了自主驾驶代理的稳健性和适应性，还显著提升了它们在不同场景下的通用能力。实验结果表明，我们的方法在处理复杂的、不可预测的驾驶环境时优于传统方法，特别是在不需要大量重新训练的情况下。|
|**2024-07-17**|**Leveraging Environment Interaction for Automated PDDL Generation and Planning with Large Language Models**|Sadegh Mahdavi et.al.|[2407.12979](http://arxiv.org/abs/2407.12979)|null|大型语言模型（LLM）在各种自然语言任务中表现出卓越的性能，但它们在需要结构化推理的规划问题上往往表现不佳。为了克服这一局限性，将规划问题转化为规划领域定义语言（PDDL）被提出作为一种潜在解决方案，这使得自动化规划器能够应用。然而，生成准确的PDDL文件通常需要人工输入或修正，这既耗时又成本高昂。本文提出了一种新颖的方法，利用LLM和环境反馈自动生成PDDL领域和问题描述文件，而无需人工干预。我们的方法引入了一个迭代细化过程，该过程生成多个问题PDDL候选，并根据与环境交互获得的反馈逐步细化领域PDDL。为了指导细化过程，我们开发了探索漫步（EW）度量，它为LLM提供了丰富的反馈信号来更新PDDL文件。我们在PDDL环境中评估了我们的方法，实现了66%的任务解决率，相比之下，使用GPT-4进行内在规划并配合链式思考提示的方法仅实现了29%的任务解决率。我们的工作使使用LLM和环境反馈自动建模规划环境成为可能，消除了在PDDL生成过程中需要人工干预的需求，为LLM代理在挑战性问题上的更可靠应用铺平了道路。|
|**2024-07-16**|**Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and Reasoning**|Yaswanth Narsupalli et.al.|[2407.12877](http://arxiv.org/abs/2407.12877)|null|评估自然语言生成（NLG）输出的质量，尤其是大型语言模型（LLMs）产生的输出，面临着巨大的挑战。传统方法要么依赖于资源密集型的人类评估，要么使用自动化指标，这些指标往往与人类判断的相关性较低。这项研究提出了一种名为Review-Feedback-Reason（ReFeR）的创新评估框架，用于利用LLM代理进行NLG评估。我们通过在两个现有的基准数据集上对ReFeR进行严格测试，在多种NLG任务中进行了测试。  ReFeR不仅提高了NLG评估的准确性，相对于之前的基准提高了约20%，而且生成了建设性的反馈，并显著增强了集体推理能力。这种反馈被用于创建指令调优数据集，当这些数据集用于微调较小的模型（如Mistral-7B）时，使它们成为非常优秀的评估者，与人类评估具有更好的相关性，并且性能几乎与GPT-3相当。  我们的方法的有效性通过在三个推理基准上的应用得到了突出，其中ReFeR优于大多数最先进的方法，并且在平均值上分别比GPT-3.5 Turbo和GPT-4在推理能力上高出约11.67%和1%。|
|**2024-07-17**|**AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases**|Zhaorun Chen et.al.|[2407.12784](http://arxiv.org/abs/2407.12784)|**[link](https://github.com/BillChan226/AgentPoison)**|**LLM代理在各种应用中展现了卓越的性能，主要得益于它们在推理、利用外部知识和工具、调用API以及执行操作以与环境互动方面的高级能力。当前的代理通常使用内存模块或检索增强生成（RAG）机制，从知识库中检索过往知识和具有相似嵌入的实例，以指导任务规划和执行。然而，对未经验证的知识库的依赖引发了关于其安全性和可信度的重大担忧。为了揭示这些脆弱性，我们提出了一种新颖的红队方法AgentPoison，这是针对通用和RAG基于的LLM代理的第一个后门攻击，通过污染其长期记忆或知识库来实现这一目标。具体而言，我们将触发器生成过程建模为一个约束优化问题，旨在优化后门触发器，使其将触发实例映射到独特的嵌入空间，从而确保每当用户指令包含优化后的后门触发器时，高概率地从被污染的记忆或知识库中检索到恶意示例。同时，不包含触发器的良性指令仍能保持正常性能。与传统的后门攻击不同，AgentPoison无需额外的模型训练或微调，且优化后的后门触发器展现出优越的迁移性、上下文内连贯性和隐蔽性。广泛的实验结果证明了AgentPoison在对抗三种真实世界的LLM代理：RAG基于的自动驾驶代理、知识密集型问答代理和医疗健康EHRAgent方面的有效性。在每个代理上，AgentPoison平均攻击成功率超过80%，对良性性能的影响最小（低于1%），污染率小于0.1%。**|
|**2024-07-16**|**InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback**|Haishuo Fang et.al.|[2407.11843](http://arxiv.org/abs/2407.11843)|null|在实际应用中部署基于大型语言模型（LLM）的代理的关键要求是对可能引发风险或不可逆错误的鲁棒性。然而，现有研究缺乏对LLM代理执行推理路径的前瞻评估，这导致了确保安全可靠操作方面的缺口。为探索更好的解决方案，本文引入了InferAct，一种新颖的方法，利用了LLM的理论思维能力，主动检测潜在错误，以防止关键行动的执行（例如，在自动在线交易或网络购物中的“立即购买”）。InferAct还能够整合人类反馈，以防止不可逆风险并增强行动代理的决策过程。在三个广泛使用的任务上进行的实验证明了InferAct的有效性。提出的解决方案提供了开发可以在涉及关键决策的不同环境安全部署的LLM代理的新方法和具体贡献。|
|**2024-07-16**|**How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models**|Yin Jou Huang et.al.|[2407.11549](http://arxiv.org/abs/2407.11549)|**[link](https://github.com/leslie071564/big5-llm-negotiator)**|心理证据揭示了个性特质对决策的影响。例如，和善性通常与谈判中的积极结果相关联，而神经质则经常与较少有利的结果联系在一起。本文提出了一种基于大型语言模型（LLM）的仿真框架，该框架包含了具有合成个性特质的仿真代理。这些代理在讨价还价领域内进行谈判，并且拥有可定制的个性和目标。实验结果显示，LLM基座仿真中的行为倾向能够重现人类谈判中观察到的行为模式。  贡献有两个方面。首先，我们提出了一种仿真方法论，以探究语言能力和经济能力在LLM代理之间的匹配程度。其次，我们提供了关于大五个性特质在双边谈判结果策略影响方面的实证见解。我们还提供了一个基于合成讨价还价对话的案例研究，揭示了一些引人入胜的行为，包括欺骗性和妥协性行为。|
|**2024-07-16**|**Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning**|Yulong Wang et.al.|[2407.10718](http://arxiv.org/abs/2407.10718)|**[link](https://github.com/ag2s1/sibyl-system)**|**基于大型语言模型（LLM）的现有代理展示了强大的问题解决能力，通过整合LLM的内在知识、强大的上下文学习和零样本能力以及人类设计的复杂LLM调用工作流程与工具的结合。然而，这些代理在长期推理方面仍存在局限性，并且未能充分利用现有工具的潜力，导致在复杂的现实世界推理场景中出现明显的缺陷。为了应对这些限制，我们引入了Sibyl，一个简单而强大的基于LLM的代理框架，旨在通过高效利用最少的工具集来解决复杂推理任务。受到全球工作空间理论的启发，Sibyl整合了一个全局工作空间，以增强系统内部的知识和对话历史的管理和共享。此外，根据心智社会理论的指导，Sibyl实施了一个多代理辩论为基础的陪审团，用于自我细化最终答案，确保全面平衡的方法。这一方法旨在减少系统复杂性，同时扩大可解决的问题范围——从人类几分钟内就能解决的问题到需要数小时甚至几天才能解决的问题，从而实现从系统1到系统2思考方式的转变。Sibyl的设计重点在于可扩展性和调试的简便性，通过从一开始就融入函数编程中的重入概念，旨在实现无缝和低努力的集成到其他LLM应用中，以提高其能力。我们的实验结果表明，使用GPT-4实例化的Sibyl代理在GAIA基准测试集上的表现最佳，平均得分为34.55%，超越了基于GPT-4的其他代理。我们希望Sibyl能够激励更多可靠且可复用的基于LLM的代理解决方案，以应对复杂的现实世界推理任务。**|
|**2024-07-15**|**Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning**|Daniel Geissler et.al.|[2407.10580](http://arxiv.org/abs/2407.10580)|null|本文提出了一种利用混合智能以实现可持续和能源意识的机器学习的方法。在机器学习模型开发过程中，人们往往只关注最终模型性能的优化，而忽略了过程本身的效率。此外，在近期，由于复杂和大规模计算过程对环境的巨大影响，能源效率变得同样重要。本工作的贡献在于通过人机交互（Human-in-the-loop，HITL）和大型语言模型（Large Language Model，LLM）代理的集成，强调并进一步解决机器学习开发过程中的低效问题。  简而言之，本文旨在通过结合人类的直觉、经验和AI的高效计算能力，改进机器学习流程的效率和环境友好性。通过引入HITL和LLM作为辅助工具，我们旨在识别和优化机器学习开发过程中的瓶颈，从而减少资源消耗，并促进更加可持续的AI实践。这一方法不仅有助于提高模型的训练速度和效率，还能降低能耗，对环境保护产生积极影响。|
|**2024-07-15**|**CIBench: Evaluating Your LLMs with a Code Interpreter Plugin**|Songyang Zhang et.al.|[2407.10499](http://arxiv.org/abs/2407.10499)|**[link](https://github.com/open-compass/CIBench)**|**在基于LLM（大型语言模型）的代理取得显著进展的同时，对其能力的基准测试变得具有挑战性，这阻碍了对它们局限性的清晰理解。本文提出了一种交互式评估框架——CIBench，以全面评估LLM在数据科学任务中利用代码解释器的能力。我们的评估框架包括一个评估数据集和两种评估模式。评估数据集通过LLM与人类合作的方式构建，通过连续且互动的IPython会话模拟真实工作流程，从而实现对LLM能力的全面评估。两种评估模式分别考察了在有无人类辅助下，LLM的能力表现。我们进行了大量的实验，分析了24个LLM在CIBench上的表现，并提供了对未来在代码解释器利用方面发展LLM的宝贵见解。**|
|**2024-07-14**|**All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era**|Bo Chen et.al.|[2407.10081](http://arxiv.org/abs/2407.10081)|null|推荐系统（RS）在应对信息过载和提供个性化内容方面至关重要，以满足用户多样化的信息需求。大型语言模型（LLM）的兴起为重新定义推荐系统提供了新的前景，利用其广泛的一般知识和推理能力。站在LLM时代，我们旨在将推荐系统整合到更广阔的框架中，并为未来的研究开辟更全面的解决方案。因此，我们首先提供了一个全面的技术进展概述，特别是针对语言基础模型及其在推荐中的应用。我们识别了现代推荐系统的两条演化路径——基于列表的推荐和对话式推荐。这两条路径最终在具有长期记忆、反思和工具智能优势的LLM代理上交汇。沿着这两条路径，我们指出推荐信息的有效性得到了提高，而用户的获取成本则降低了。我们仔细研究了每个里程碑的技术特性、研究方法论以及内在挑战，从传统的基于列表的推荐到增强的LLM推荐再到带有LLM代理的推荐。最后，我们强调了几个对于未来个性化技术与界面发展至关重要的未解决挑战，并讨论了未来前景。|
|**2024-07-14**|**Revolutionizing Bridge Operation and maintenance with LLM-based Agents: An Overview of Applications and Insights**|Xinyu-Chen et.al.|[2407.10064](http://arxiv.org/abs/2407.10064)|null|在人类社会发展各工业领域中，人们一直在寻求解放劳动力的方法。构建基于大规模语言模型的代理被视为实现这一目标的高效工具。作为具备感知、规划、决策和行动能力的人类智能实体，代理已经在众多领域创造了显著的生产价值。然而，桥梁维护与管理（O&M）领域相比其他行业，其智能化水平相对较低。尽管如此，该领域已经发展了众多智能检测设备、机器学习算法以及自主评估和决策方法，为本领域的人工智能突破奠定了基础。本研究旨在探讨基于大型语言模型的AI体对桥梁O&M领域的影响，分析它对核心任务可能带来的挑战与机遇。通过深入研究和分析，期望能为理解这一领域智能化应用提供更全面的视角。|
|**2024-07-11**|**Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility**|Yuchen Xia et.al.|[2407.08550](http://arxiv.org/abs/2407.08550)|**[link](https://github.com/yuchenxia/gpt4industrialautomation)**|这篇论文提出了一种新颖的方法，旨在将大型语言模型（LLMs）整合到自动化生产系统中，以提升任务自动化和灵活性。我们根据自动化金字塔构建生产操作的层级结构，将原子操作功能抽象为微服务，并通过专用的数字孪生系统进行调用执行。这为协调生产流程提供了可扩展且灵活的基础。在数字孪生系统中，低层次的、硬件特定的数据被赋予语义，使得LLMs能够理解和处理生产计划与控制任务。当接收到用户请求或识别到触发事件时，LLMs会生成生产流程计划，然后将其分解为一系列微服务，在现实世界的自动化系统中执行。我们在实验室的模块化自动化设施上实现了这一整体方法，通过一个实际案例展示了LLMs如何处理生产规划和控制任务，从而实现了一个直观、自动化程度高且更具灵活性的生产环境。最后，我们指出了实现LLMs在自主系统中的全部潜力所面临的局限性，并强调了其潜在的有益之处。有关此系列研究的演示可在以下链接访问：https://github.com/YuchenXia/GPT4IndustrialAutomation。|
|**2024-07-11**|**PrefCLM: Enhancing Preference-based Reinforcement Learning with Crowdsourced Large Language Models**|Ruiqi Wang et.al.|[2407.08213](http://arxiv.org/abs/2407.08213)|null|## 翻译  偏好驱动的强化学习（PbRL）作为一种新兴的方法，通过人类比较反馈教导机器人，避免了复杂的奖励工程的需求。然而，现有PbRL方法需要大量反馈，往往导致对由脚本教师生成的合成反馈的依赖，这又回到了复杂的奖励设计，并难以适应人类-机器人交互（HRI）场景中用户对同一任务的独特期望。为解决这些问题，我们提出了一种新颖的框架——PrefCLM，它利用大规模语言模型（LLMs）作为模拟教师参与PbRL。我们运用Dempster-Shafer理论在分数级别融合来自多个LLM代理的个人偏好，有效利用它们的多样性和集体智慧。同时，我们引入了一个用户参与的流程，以促进基于用户交互的集体精进。在各种通用强化学习任务中的实验结果显示，PrefCLM在性能上与传统脚本教师相当，并且在促进更自然、高效的机器人行为方面表现出色。一个现实世界的用户研究（N=10）进一步证明了它在个性化用户偏好的能力，显著提高了HRI场景中的用户满意度。|
|**2024-07-10**|**Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities**|Tianjie Ju et.al.|[2407.07791](http://arxiv.org/abs/2407.07791)|**[link](https://github.com/Jometeorie/KnowledgeSpread)**|**随着大型语言模型（LLMs）在多代理系统中的迅速应用，它们在协作问题解决和自主谈判等领域的出色性能引起了关注。然而，这些基于LLM的多代理系统的安全问题尚未得到充分研究，尤其是在知识操纵传播方面。本文通过构建详细的威胁模型和模拟环境，模拟现实世界中的多代理部署在可信平台上，探讨这一关键问题。我们提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵知识注入，来系统地探究在无明确提示操纵的情况下，如何潜在地传播操纵知识（如虚构和有害知识）。我们的方法利用了LLMs处理世界知识固有的漏洞，攻击者可以借此无意识地传播编造的信息。实验结果表明，我们的攻击方法能够成功诱导基于LLM的代理在交流中传播这两种操纵的知识，同时不会显著降低它们的基础功能。此外，我们发现这些操纵会持续存在于流行的检索增强生成框架中，即使交互结束，若干良性代理也可能继续受到操纵聊天记录的影响。我们的发现揭示了LLM基多代理系统中的重大安全风险，强调了对操纵知识传播进行强大防御的迫切需求，例如引入“守护”代理和先进的事实核查工具。**|
|**2024-07-09**|**Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models**|Logan Cross et.al.|[2407.07086](http://arxiv.org/abs/2407.07086)|**[link](https://github.com/locross93/hypothetical-minds)**|**在多智能体强化学习（MARL）方法中，处理多智能体系统的非stationarity并适应在线学习的能力是一个挑战。为此，我们利用大型语言模型构建了一个自主的解决策略。我们的新型智能体“假设心智”（Hypothetical Minds）采用认知启发式架构，包括感知、记忆和两个抽象层次上的分层规划模块。其中的关键部分是“心理理论”模块，它通过自然语言生成对其他智能体策略的假设，并根据这些假设对其他智能体行为的预测进行评估和迭代优化。通过这种方式，假设心智在Melting Pot基准中的多种竞争、混合动机和协作环境中，无论是二元还是群体环境，都显著优于先前的语言模型智能体（LLM-agent）和强化学习基础线。对比实验还显示，假设的评估和精炼对于在复杂场景中取得成功至关重要。**|
|**2024-07-09**|**Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy**|Zhenyu Guan et.al.|[2407.06813](http://arxiv.org/abs/2407.06813)|**[link](https://github.com/todexter3/richelieu)**|## 背景 在人类社会中，外交是一种极其复杂的活动，涉及众多各方/行动者的互动，需要具备社会推理、谈判技巧和长期策略规划等多方面能力。以往的AI代理已经在处理多步骤游戏和大动作空间的多代理任务上展示了实力。然而，外交所涉及的决策空间范围惊人，特别是在需要谈判的阶段。近期，大型语言模型（LLM）在一些应用中展现出了超越前代的能力，但仍不足以应对复杂多代理环境中长时间的规划。借助尖端的LLM技术，我们首次尝试探索AI在如此全面的多代理使命中的上限，通过整合三个核心且关键的功能，以构建更强的基于LLM的社会性代理：1）具有记忆和反思的策略规划者；2）目标导向的、具备社会推理的谈判者；3）通过自我对弈游戏增强记忆，实现无人工干预的自我进化。|
|**2024-07-10**|**FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making**|Yangyang Yu et.al.|[2407.06567](http://arxiv.org/abs/2407.06567)|null|大型语言模型（LLMs）在执行复杂任务方面展现出显著潜力，并越来越多地应用于金融领域。然而，高质量的连续投资决策过程仍面临挑战，它需要与不断变化的环境进行多次交互，以最大化回报并管理风险。尽管已经开发出基于LLMs的代理系统，它们能够超越人类团队，实现投资收益，但如何优化多源信息整合和决策结果，通过实时经验改进，仍有待探索。为此，我们提出FinCon，一个专为多样化的金融任务设计的基于LLM的多代理框架，其特点在于概念化口头强化和财务组织结构的运用。  FinCon借鉴现实世界投资公司的组织架构，采用经理-分析师的沟通层次，促进跨职能代理间的协同合作，通过自然语言交流实现目标统一。每个代理都具备比人类更大的记忆容量，这有助于更高效的信息处理。此外，FinCon还引入了一个风险控制组件，定期启动自我批判机制，以更新系统的投资理念。这些概念化的信念作为口头强化，指导未来行为，并可根据需要选择性地传递给需要更新知识的节点，从而减少不必要的信息交流成本，提高性能。  FinCon在单一股票交易和资产管理等不同金融任务上表现出强大的泛化能力，证明了其在实际金融场景中的应用潜力。|
|**2024-07-08**|**Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning**|Yadong Zhang et.al.|[2407.06112](http://arxiv.org/abs/2407.06112)|null|该论文提出了一个新颖的推理方法——双向决策解放推理（BIDDER），旨在提升语言模型的决策合理性。传统推理方法通常依赖历史信息，采用单向（从左到右）的推理策略，这导致对潜在未来结果的认识不足，以及历史背景的整合不够充分，从而产生次优决策。BIDDER通过融合理性决策的原则，特别是处理不确定性并预测期望效用，弥补了这一短板。其方法包括三个关键步骤：从历史数据中推断隐藏状态，以表示决策过程中的不确定信息；利用这些隐藏状态预测未来的潜在状态和可能结果；结合历史信息（过去情境）和长期结果（未来情境），以指导推理。通过双向推理，BIDDER能够全面考虑过去和未来的情境，从而做出更明智、更理性的决策。我们在扑克（限注德州扑克）和谈判两个明确场景中测试了BIDDER的效果，实验显示它显著提高了语言模型和基于语言模型的代理的决策能力。|
|**2024-07-08**|**Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**|Jiaqi Chen et.al.|[2407.05890](http://arxiv.org/abs/2407.05890)|null|基于语言模型的代理在视觉导航（VLN）任务中展现出零样本的强大性能。然而，这些方法仅关注解决高层任务规划，通过选择预定义导航图中的节点进行移动，忽视了现实场景中低层次的控制。为了弥补这一不足，我们提出了AO-Planner，一个新颖的面向可及性规划的连续视觉导航框架。AO-Planner整合多种基础模型，实现面向可及性的运动规划和动作决策，均以零样本的方式执行。具体来说，我们采用了视觉可及性提示（VAP）方法，利用SAM分割可见地面，提供导航可及性信息，从而让语言模型选择潜在的下一个路标，并生成向选定路标的低层次路径规划。此外，我们引入了高级代理PathAgent，识别出最可能的像素级路径，并将其转换为三维坐标，以完成低层次的移动。  在具有挑战性的R2R-CE基准测试上，AO-Planner实现了最先进的零样本性能提升（SPL指标提高5.5%）。我们的方法有效连接了语言模型与三维世界，避免了直接预测世界坐标点的困难，为利用基础模型进行低层次运动控制提供了新的前景。|
|**2024-07-05**|**VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models**|Hang Gao et.al.|[2407.04573](http://arxiv.org/abs/2407.04573)|null|在大型语言模型（LLMs）快速发展的背景下，向量检索算法对于满足相似度和多样性要求的语义查询至关重要。尽管Maximal Marginal Relevance（MMR）在涉及这两个需求的检索场景中被广泛应用，但其参数λ的变化会导致结果波动，使得向量空间中的优化路径变得模糊。此外，当前缺乏对相似性和多样性在检索过程中约束的坚实理论分析。本文提出了一种新方法，通过查询向量与求和向量之间的关系来刻画这两种约束。这种关系确保了相似性，同时要求求和向量中的各个向量以分散的方式与查询向量对齐，以满足多样性需求。  我们还提出了一个新的组合优化问题：从一组候选向量中选择 $k$ 个，使得它们的求和向量最大程度地与查询向量匹配。我们证明了这个问题是NP完全的，揭示了在向量检索中同时追求相似性和多样性的深刻困难，并为后续研究奠定了理论基础。此外，我们设计了一个名为Vectors Retrieval with Similarity and Diversity（VRSD）的启发式算法，它不仅具有明确的优化目标，无需预设参数，而且在时间复杂度上相对于MMR有所降低。实证验证表明，VRSD在各种数据集上显著优于MMR。|
|**2024-07-05**|**When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions**|Jérémy Perez et.al.|[2407.04503](http://arxiv.org/abs/2407.04503)|**[link](https://github.com/jeremyperez2/telephonegamellm)**|**随着大型语言模型（LLMs）之间的互动增加，它们在线上生成的文本量也随之增多，研究如何信息在从一个LLM传递到另一个LLM的过程中发生变化变得至关重要。尽管对单个LLM的行为已有深入研究，但对迭代交互中集体行为和信息扭曲的探讨相对不足。微小的偏差，在单次输出时可能显得不明显，但在多次交互中可能会被放大，可能导致内容朝着吸引子状态演变。我们通过借鉴人类文化进化学的研究方法——电话游戏实验，设计了一种链式传输模型。在这个过程中，LLM代理接收、生成并传递文本，从一个链中的前一个代理到下一个。我们追踪了文本的毒性、积极度、难度和长度在传输链中的演变，揭示了偏见和吸引子的存在，并研究了它们与初始文本、指令、语言模型和模型规模的关系。例如，我们发现开放性指令比约束性任务更容易引发更强的吸引效应。此外，不同的文本特性对吸引子效应的敏感度不同，毒性的影响通常大于长度。这些发现强调了考虑多步骤传输动态的重要性，为进一步理解LLM的文化动态奠定了基础。**|
|**2024-07-05**|**AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**|Petr Anokhin et.al.|[2407.04363](http://arxiv.org/abs/2407.04363)|**[link](https://github.com/airi-institute/arigraph)**|**随着生成式人工智能的进步，大型语言模型（LLMs）在自主代理的发展中展现出广阔的应用前景。实现真正的自主性需要从与环境的交互中积累和更新知识，并能有效利用这些信息。当前基于LLMs的方法依赖于全历史观察、总结或检索增强，但这些非结构化的记忆表示不利于复杂决策中的推理和规划。我们的研究提出AriGraph，一种新型方法，让代理在探索环境中构建融合语义和情节记忆的记忆图。这种图结构促进关联概念的有效检索，这些概念与代理当前状态和目标相关，从而成为一种有效的环境模型，提升探索和规划能力。  我们设计的Ariadne LLM代理，配备有我们提出的记忆架构以及规划和决策功能，能在零样本基础上处理TextWorld环境中的复杂任务，如First TextWorld Problems竞赛中的烹饪挑战，以及新任务如房屋清洁和寻宝谜题。与全历史、总结和检索增强生成等传统方法相比，我们的方法在各种任务中表现出显著优势。**|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483](http://arxiv.org/abs/2407.02483)|**[link](https://github.com/Wangyixinxin/MMedAgent)**|尽管多模态大型语言模型（MLLMs）已经取得了成功，但它们的泛化能力仍然有限，在某些情况下表现不如专门化的模型。为了解决这些问题，最近的研究开发了基于LLMs的代理，可以根据用户输入选择合适的专用模型。然而，这种进展在医疗领域尚未得到充分探索。为了弥补这一空白，本文首次提出了一种专门为医疗领域设计的代理，称为\textbf{M}ulti-modal \textbf{Med}ical \textbf{Agent}（MMedAgent）。我们构建了一个指令调优数据集，包含了六个医疗工具来解决七项任务，使代理能够为给定任务选择最合适的工具。实验全面展示了MMedAgent在各种医疗任务上超越了开源方法的最新状态，甚至与闭源模型GPT-4o相比也表现出色。此外，MMedAgent还显示出了更新和整合新医疗工具的高效性。|
|**2024-07-02**|**Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents**|Fanzeng Xia et.al.|[2407.01887](http://arxiv.org/abs/2407.01887)|null|本文关注的是大型语言模型在决策制定中的性能，尤其是在杜尔克姆双臂赌博（Dueling Bandits，DB）问题的上下文中。研究比较了GPT-3.5-Turbo、GPT-4和GPT-4-Turbo与现有DB算法的性能。结果显示，尤其是GPT-4 Turbo，能够快速识别出优势明显的选项，从而在弱后悔方面超越当前最佳算法。然而，这些模型在收敛性上存在问题，对提示的敏感度较高，且对提示变化反应脆弱。为了改进，我们提出了一种结合了LLM决策能力与经典DB算法理论保证的增强型算法——IF-Enhanced LLM。这种设计展示了如何增强LLM在对性能稳定性有要求的决策任务中的可信度。IF-Enhanced LLM具有弱后悔和强后悔的理论保证。实验结果验证了即使面对嘈杂和对抗性的提示，IF-Enhanced LLM仍保持稳健。|
|**2024-07-01**|**Agentless: Demystifying LLM-based Software Engineering Agents**|Chunqiu Steven Xia et.al.|[2407.01489](http://arxiv.org/abs/2407.01489)|**[link](https://github.com/OpenAutoCoder/Agentless)**|**随着大型语言模型（LLMs）的最新进展，软件开发任务的自动化，如代码合成、程序修复和测试生成，已取得显著进步。研究人员和业界实践者已经开发出各种自主LLM代理来执行端到端的软件开发任务，它们能够利用工具、运行命令、观察环境反馈并规划未来行动。然而，这些基于代理的方法的复杂性以及当前LLM的局限性，引发了一个问题：是否真的需要使用复杂的自主软件代理？为了探讨这个问题，我们构建了Agentless——一种无代理方法，用于自动解决软件开发问题。与复杂的代理设置相比，Agentless采用了一种简单的两阶段过程：定位后修复，不让LLM决定未来的行动或操作复杂的工具。在流行的SWE-bench Lite基准上，我们的实验结果令人惊讶地表明，这种简单的方法能够实现最高性能（27.33%）和最低成本（0.34美元），超越所有开源软件代理！  此外，我们手动分类了SWE-bench Lite中的问题，并发现存在精确的ground truth补丁问题或描述不足/误导性的问题。因此，我们构建了SWE-bench Lite-S，通过排除这些问题来进行更严格的评估和比较。我们的工作突显了当前被忽视的简单、可解释技术在自主软件开发中的潜力。我们希望Agentless将作为自主软件代理的基线、起点和期望值，激发未来在这个关键领域的工作。**|
|**2024-07-01**|**MIRAI: Evaluating LLM Agents for Event Forecasting**|Chenchen Ye et.al.|[2407.01231](http://arxiv.org/abs/2407.01231)|null|随着大型语言模型（LLMs）的最新进展，这些模型能够自主收集全球信息，并进行推理以解决复杂问题，这引发了使用LLM预测国际事件的兴趣。然而，目前缺乏一个严格评估LLM预测能力与可靠性的基准。为了填补这一空白，我们提出MIRAI，这是一个新颖的基准，旨在系统地评价LLM在国际事件时间序列预测中的表现。MIRAI构建了一个代理环境，配备有访问广泛历史结构化事件和文本新闻数据库的工具。我们对GDELT事件数据库进行了精心清洗和解析，设计了一系列关联预测任务，涵盖了不同预测时间范围，从短期到长期，以检验LLM在整合全球关键信息、运用领域特定API和库编写代码以及综合处理来自多种格式和时间的历史知识以准确预测未来事件的能力。通过全面的基准测试，我们的目标是建立一个可靠的框架，以评估LLM在国际事件预测方面的性能，从而推动更精确和可信的国际关系分析模型的发展。|
|**2024-07-01**|**Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents**|Shihan Deng et.al.|[2407.00993](http://arxiv.org/abs/2407.00993)|null|随着大型语言模型（LLMs）的显著进步，基于LLM的移动代理已成为人机交互领域的研究热点。然而，针对此类代理的基准测试资源相对匮乏。评估这类代理通常面临三个挑战：（1）仅依赖用户界面（UI）操作的低效限制了任务评估；（2）单一应用中的特定指令不足以全面评估LLM移动代理的多维度推理和决策能力；（3）当前的评估指标无法准确衡量连续动作过程。为此，我们提出了Mobile-Bench，一个全新的用于评估LLM移动代理能力的基准。首先，我们扩展了传统的UI操作，融入了103个收集到的API，以提高任务完成的效率。接着，我们通过结合真实用户查询和LLM增强的数据收集来进行评估。为了更好地评价移动代理的不同规划能力层次，我们的数据被分为SAST（简单任务）、SAMT（稍复杂任务）和MAMT（多任务）三类，反映了任务复杂度的差异。Mobile-Bench包含832条数据条目，其中超过200项任务专门设计用于测试跨应用协作场景。此外，我们引入了一种更精确的评估指标，称为CheckPoint，用于检查LLM移动代理在规划和推理步骤中是否达到关键点。|
|**2024-06-29**|**Large Language Models for Power Scheduling: A User-Centric Approach**|Thomas Mongaillard et.al.|[2407.00476](http://arxiv.org/abs/2407.00476)|**[link](https://github.com/thomasmong/llm-power-scheduling)**|**随着传统优化和调度方法逐渐转向用户驱动和个人化服务，以提升用户体验（QoE）和灵活性，未来的系统，尤其是在无线和数字化能源网络中，面临着如何更好地理解和响应用户需求的挑战。传统的系统往往忽视了用户的个性化需求，因为用户与机器之间的沟通不畅。大型语言模型（LLMs）的出现为解决这个问题带来了突破，它们提供了用户与设备之间自然的交流界面。本文首次提出了一种新颖的架构，通过构建三个LLM代理来将用户的语音请求（VRQ）转化为资源分配向量。具体包括：LLM意图识别代理将请求转化为优化问题（OP）、LLM OP参数识别代理以及LLM OP求解代理。  我们针对电动汽车（EV）充电的典型VRQ创建了一个数据库，作为性能评估的基础。作为概念验证，我们主要使用Llama 3 8B模型进行实验。通过不同的提示工程场景测试，结果显示了所提架构的有效性。研究还揭示了一些关键见解，例如，用于建模实际问题的更大候选OP集可能会由于更高的识别/OP分类噪声而降低最终性能。所有结果和代码已开源，供学术界进一步研究和利用。**|
|**2024-06-29**|**Financial Knowledge Large Language Model**|Cehao Yang et.al.|[2407.00365](http://arxiv.org/abs/2407.00365)|null|人工智能在金融领域取得了显著进步，正在重塑数据处理和解读方式。其中，大型语言模型（LLMs）展现出巨大的潜力，能够自动化复杂任务、提升客户服务，并提供详尽的财务分析。首先，我们介绍IDEA-FinBench，这是一个专为评估大型语言模型在金融知识方面的性能而设计的评价基准。它借鉴了两个全球知名且权威的金融专业考试中的问题，旨在全面检验LLMs解答与金融相关考题的能力。其次，我们提出IDEA-FinKER，是一个金融知识增强框架，旨在快速让通用LLMs适应金融领域。它采用基于检索的少量样本学习方法，实现实时上下文级知识注入，并提供一套高质量的金融知识指令，用于微调任何通用模型。最后，我们展示了IDEA-FinQA，一个由LLMs驱动的金融问答系统。该系统围绕实时知识注入和事实强化的架构构建，利用外部知识。IDEA-FinQA主要由数据收集器、数据查询模块和执行特定功能的LLM代理组成。|
|**2024-06-28**|**Simulating Financial Market via Large Language Model based Agents**|Shen Gao et.al.|[2406.19966](http://arxiv.org/abs/2406.19966)|null|大多数经济理论通常假设金融市场参与者是完全理性的个体，并使用数学模型来模拟人类在金融市场的行为。然而，人类行为往往并非完全理性，用数学模型精确预测颇具挑战。本文提出了一种新型的\textbf{A}gent-based \textbf{S}imulated \textbf{F}inancial \textbf{M}arket（ASFM），首先构建了一个具有真实订单匹配系统的模拟股票市场。接着，我们设计了一种基于大型语言模型的股票交易代理，它包括个人概况、观察和基于工具学习的动作模块。这种交易代理能够全面理解当前市场动态和金融政策信息，从而根据其交易策略作出决策。实验表明，ASFM在可控场景下的反应与现实股票市场一致。此外，我们在两个经济学研究热点领域进行了实验，结果发现，我们的\model得出的结论与经济学研究的初步发现相吻合。因此，我们认为ASFM为经济研究提供了一个新的范式。|
|**2024-06-26**|**Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship**|Zachary R. Baker et.al.|[2406.18702](http://arxiv.org/abs/2406.18702)|null|这项研究提出了一种创新的方法，利用语言模型驱动的虚拟代理来模拟立法过程，具体聚焦于美国参议院情报委员会。我们构建了代表个别参议员的代理，并在模拟的委员会讨论中让它们互动。这些代理展现出在现实辩论中的能力，能够提供深思熟虑的观点，并在特定条件下找到两党的解决方案。值得注意的是，模拟显示，面对外部干扰时，代理模型在两党合作上展现出转变的潜力。研究结果表明，这种基于语言模型的策略可能成为理解和改进立法流程的有效工具，这与一系列发现相呼应，即基于语言模型的代理能有用地模拟现实世界现象。未来的研究将致力于提升代理的复杂性，扩大模拟范围，并探索在政策测试和谈判中的应用。|
|**2024-06-25**|**Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks**|Yun-Shiuan Chuang et.al.|[2406.17232](http://arxiv.org/abs/2406.17232)|null|### 翻译  构建逼真的人工大型语言模型（LLMs）对于实现可信的社会模拟至关重要。尽管基于人口统计信息的角色扮演有时能提升人性化，但效果并不总是理想。本研究旨在探究是否可以通过整合来自实证人类信念网络的信息，进一步提升LLMs与人类行为的契合度。我们利用一项人类调查数据，估计了一个包含18个主题的信念网络，这些主题加载于两个不重叠的潜在因子上。然后，我们在LLM中植入一个关于某一主题的观点，分析其对剩余测试话题表达的观点与相应人类数据的契合程度。仅依赖人口统计信息的角色扮演未能使LLM和人类观点保持一致，但当植入单一信念时，对于相关于信念网络内的主题，这种一致性显著提高，而对于网络外的主题则没有明显影响。这些结果表明了一种新颖的方法，可以用于在追求理解和模拟社会中信念分布模式的人工智能工作中，实现人类与LLMs之间的信念对齐。|
|**2024-06-21**|**GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians**|Haoyang Liu et.al.|[2406.15341](http://arxiv.org/abs/2406.15341)|**[link](https://github.com/liu-hy/genotex)**|**## 翻译  近年来，机器学习的进步显著提升了从基因表达数据中识别疾病相关基因的能力。然而，这些过程往往需要深厚的专长和大量的人工努力，限制了其可扩展性。大型语言模型（LLMs）驱动的代理显示出在自动化此类任务方面的潜力，因为它们的问题解决能力日益增强。为了支持这类方法的评估和发展，我们创建了GenoTEX，这是一个基因表达数据分析自动探索的基准，包括数据集选择、预处理和统计分析任务。GenoTEX提供了全面的分析管道，其中包含了人类生物信息学家精心编写的注释，他们对数据集进行深入分析以确保准确性和可靠性。  为了提供这些任务的基线，我们设计了GenoAgents，这是一个基于LLMs的代理团队，具备上下文感知规划、迭代校正以及与领域专家咨询的能力，它们协作探索基因数据集。我们的实验显示了LLM驱动方法在基因组数据分析中的潜力，而错误分析指出了挑战和未来的改进方向。我们提议GenoTEX作为一个有前景的资源，用于衡量和提升人工智能驱动的基因组数据分析方法。我们的基准已公开发布在：\url{https://github.com/Liu-Hy/GenoTex}。**|
|**2024-06-21**|**Autonomous Agents for Collaborative Task under Information Asymmetry**|Wei Liu et.al.|[2406.14928](http://arxiv.org/abs/2406.14928)|**[link](https://github.com/thinkwee/iAgents)**|**大型语言模型多-agent系统（LLM-MAS）在解决复杂任务方面取得了显著进步。它们通过系统内各代理之间的通信协作来完成任务，前提是共享信息。然而，当代理间的交流被用于增强人类合作时，由于信息不对称（每个代理仅能访问其对应人类用户的信息），这带来了新的挑战。传统MAS在这种情况下难以完成任务。为解决此问题，我们提出了一种新型多agent系统架构，称为“iAgents”，即信息丰富多agent系统。在iAgents中，人类社会网络在代理网络中得到反映，代理主动交换完成任务所需的人类信息，从而克服信息不对称。iAgents采用了一种新颖的代理推理机制，InfoNav，引导代理之间的有效信息交流。结合InfoNav，iAgents组织了混合记忆中的人类信息，为代理提供准确全面的信息进行交换。此外，我们还推出了首个针对评估LLM在信息不对称条件下任务解决能力的基准——InformativeBench。实验结果显示，iAgents能够在包含140人和588条关系的社会网络中协作，自主进行超过30轮的通信，并从近70,000条消息中检索信息，在3分钟内完成任务。**|
|**2024-06-21**|**FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents**|Ruixuan Xiao et.al.|[2406.14884](http://arxiv.org/abs/2406.14884)|null|基于语言模型的代理作为一种有前景的工具，被设计用于通过迭代规划和行动来执行复杂任务。然而，这些代理在处理需要专业知识的任务时，容易产生不期望的规划幻觉。为了解决这个问题，初步尝试通过融入与工作流程相关的外部知识来增强规划可靠性。尽管显示出潜力，但注入的知识通常杂乱无章，格式多样，缺乏严谨的规范化和全面的比较。为此，我们规范了不同格式的工作流程知识，并提出了FlowBench，这是第一个面向工作流引导规划的基准。FlowBench涵盖了来自6个领域的51个不同场景，其中知识以多样的形式呈现。为了评估不同语言模型在FlowBench上的性能，我们设计了一个多层次的评估框架。我们研究了工作流程知识在多种格式下的有效性，结果表明当前的语言模型代理在满足满意的规划需求方面仍有很大的提升空间。我们期望这个具有挑战性的基准能为未来的代理规划研究铺平道路。|
|**2024-07-01**|**Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory**|Gordon Dai et.al.|[2406.14373](http://arxiv.org/abs/2406.14373)|null|随着大型语言模型（LLMs）和人工智能的进步，计算社会科学的研究迎来了大规模探索的机遇。我们的工作基于先前对LLM行为体设计的研究，构建了一个模拟的Agent社会，其中复杂的社交关系随时间动态形成和发展。我们赋予这些Agent心理驱动力，并置于一个沙盒生存环境中。通过托马斯·霍布斯的奠基性社会契约理论（SCT）的视角，我们评估了这个Agent社会。实验结果显示，起初，Agent们表现出无拘无束的冲突，符合霍布斯对“自然状态”的描述。然而，随着模拟的进行，社会契约逐渐形成，绝对主权者得到了授权，进而建立了以相互合作为基础的和平共同体。我们的实验发现与霍布斯理论相吻合：LLM驱动的多Agent模拟展示了社会动态的复杂性，可能复制塑造人类社会的力量。尽管无法完全模拟人类行为的所有细微之处，但这种模拟对于理解社会结构、群体动态和复杂人类系统具有潜在价值。|
|**2024-06-20**|**EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms**|Siyu Yuan et.al.|[2406.14228](http://arxiv.org/abs/2406.14228)|**[link](https://github.com/siyuyuan/evoagent)**|**随着强大大型语言模型（LLMs）的兴起，一种新的趋势是利用这些模型构建能解决复杂任务的自主代理，尤其是多代理系统。然而，现有的研究很大程度上依赖于人类设计的框架，这限制了代理系统的功能范围和可扩展性。如何自动将专门的代理扩展到多代理系统，以提升任务解决能力，仍然是一个重大挑战。本文提出EvoAgent，这是一种通过进化算法自动将专家代理扩展到多代理系统的方法，旨在提高基于LLM的代理在执行任务中的效率。具体来说，我们视现有的代理框架为初始个体，并应用一系列进化操作（如突变、交叉、选择等）生成具有不同设置的代理。EvoAgent适用于任何基于LLM的代理框架，能够无须额外人工设计自动生成扩展的多代理系统。实验结果显示，EvoAgent能够自动产生多个专家级代理，并显著增强基于LLM的代理的任务解决能力。**|
|**2024-06-19**|**AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents**|Edoardo Debenedetti et.al.|[2406.13352](http://arxiv.org/abs/2406.13352)|**[link](https://github.com/ethz-spylab/agentdojo)**|**本文介绍了一个名为AgentDojo的框架，用于评估依赖于外部工具处理不可信数据的AI代理的对抗性鲁棒性。面对不断演变的攻击和防御手段，AgentDojo不是一个静态的测试套件，而是设计和评估新任务、防御策略以及适应性攻击的可扩展环境。它包含了97个实际应用场景的任务（如管理电子邮件客户端、导航网上银行网站或预订旅行），629个安全测试案例，以及来自文献的各种攻击和防御方法。研究发现，当前最先进的语言模型在AgentDojo中的表现并不尽人意（即使没有攻击），并且现有的提示注入攻击虽然能破坏一些安全特性，但并非所有情况都适用。我们期望AgentDojo能够推动研究，以寻找在解决常见任务时既可靠又健壮的AI代理的新设计原则。相关代码已发布在https://github.com/ethz-spylab/agentdojo。**|
|**2024-06-19**|**LLMatDesign: Autonomous Materials Discovery with Large Language Models**|Shuyi Jia et.al.|[2406.13163](http://arxiv.org/abs/2406.13163)|null|发现新材料对科学和技术具有重大意义，但目前仍是艰巨问题，因为化学空间浩瀚。近期，机器学习的进步推动了基于数据的方法来快速筛选或生成有前景的材料，但这些方法仍依赖大量训练数据，且往往缺乏人类期望的材料设计的灵活性和化学直觉。我们提出LLMatDesign，一个由大型语言模型驱动的可解释材料设计新框架。LLMatDesign利用LLM代理理解人类指令，对材料进行修改，并使用提供的工具评估结果。通过自我反思先前决策，LLMatDesign能在零样本情况下快速适应新任务和条件。在离线实验中，对LLMatDesign在多个材料设计任务中的系统评估证实了它在小数据环境下开发出具有用户定义目标性质的新材料的有效性。我们的框架展示了自主LLM引导的计算环境下的材料发现的非凡潜力，预示着未来自驾驶实验室的可能性。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**AgentReview: Exploring Peer Review Dynamics with LLM Agents**|Yiqiao Jin et.al.|[2406.12708](http://arxiv.org/abs/2406.12708)|**[link](https://github.com/ahren09/agentreview)**|## 翻译  同行评审是科学出版诚信和进步的基础。传统的同行评审数据分析方法往往侧重于现有数据的探索和统计，但未能充分考虑这一过程的多变量性质，处理潜在变量，且受限于隐私问题，因为数据涉及敏感性。我们提出AgentReview，这是一个基于大型语言模型（LLM）的同行评审模拟框架，有效分解了多个潜在因素的影响，并解决了隐私问题。研究发现，由于社会影响力理论、利他主义疲劳和权威偏见等社会学理论的支持，论文决策中存在显著的37.1%的变异性。我们相信这项研究能为优化同行评审机制设计提供宝贵见解。|
|**2024-06-18**|**Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics**|Chenggang Cui et.al.|[2406.12628](http://arxiv.org/abs/2406.12628)|null|这篇论文关注于电力电子系统控制设计中的挑战，特别是模型不确定性以及设计周期漫长和成本高昂的问题。论文旨在提出一种基于大型语言模型（LLMs）的多代理框架，用于面向目标的电力电子控制器设计。该框架利用LLMs的推理能力，结合多代理工作流程，旨在开发一个高效且自动化的控制器设计流程。LLM代理能够理解并响应自然语言的高级指令，根据任务的具体需求和实际应用中的约束调整其行为。这种新颖而高效的策略有望显著提升电力电子控制器设计的灵活性和适应性，极大地便利实践者的工作。|
|**2024-06-18**|**CodeNav: Beyond tool-use to using real-world codebases with LLM agents**|Tanmay Gupta et.al.|[2406.12276](http://arxiv.org/abs/2406.12276)|null|我们介绍CodeNav，这是一种利用大型语言模型（LLM）来导航和利用先前未见过的代码仓库，以解决用户查询的系统。与需要通过手动描述在LLM上下文中“注册”所有相关工具的工具使用型LLM不同，CodeNav能够自动索引和搜索目标代码库中的代码块，找到相关的代码片段，导入它们，并根据执行反馈迭代生成解决方案。首先，我们通过三个案例研究展示CodeNav如何使用三种不同的代码库来解决复杂的用户问题。接着，在三个基准测试中，我们定量比较了仅能访问目标代码库的代码使用方法与拥有对所有工具名称和描述的特权访问的工具使用方法的效果。此外，我们研究了不同类型工具和库描述对代码使用性能的影响，以及将源代码视为输入而非自然语言代码描述的优势。所有代码将遵循宽松许可协议开源。|
|**2024-06-17**|**Efficient Sequential Decision Making with Large Language Models**|Dingyang Chen et.al.|[2406.12125](http://arxiv.org/abs/2406.12125)|null|该论文关注的是将大型语言模型（LLMs）的成功扩展到序列决策制定。当前的努力要么重新训练或微调LLMs进行决策，要么为预训练的LLMs设计提示。前者面临计算负担重的梯度更新问题，而后者未显示出明显效果。为此，我们提出了一种新方法，利用在线模型选择算法有效地将LLMs整合到序列决策过程中。统计上，我们的方法显著优于传统决策算法和纯LLM代理。在计算上，我们的方法避免了对LLMs进行昂贵的梯度更新，并且在整个决策过程中仅需要少量的LLM调用。我们进行了广泛实验来验证我们方法的有效性。以一个大规模的亚马逊数据集为例，我们的方法在仅使用1.5%的时间步数调用LLMs的情况下，实现了比基线超过6倍的性能提升。|
|**2024-06-17**|**Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector**|Xiaoxue Cheng et.al.|[2406.11277](http://arxiv.org/abs/2406.11277)|**[link](https://github.com/rucaibox/haluagent)**|这篇论文探讨了大型语言模型（LLMs）在幻觉检测方面的挑战，特别指出以往研究主要依赖于强大的闭源模型如GPT-4。作者提出了一种自主的基于LLM的代理框架，称为HaluAgent，它允许较小的模型（如巴 chcuan2-Chat 7B）主动选择适合检测文本、代码和数学表达式等多种幻觉类型的工具。HaluAgent整合了LLM、多功能工具箱，并设计了一个细粒度的三阶段检测框架，同时配备了记忆机制。为了提高HaluAgent的效能，论文利用现有的中文和英文数据集合成检测轨迹进行微调，使其具备双语幻觉检测能力。实验结果表明，仅使用2000个样本对LLM进行调优后，HaluAgent在各种任务和数据集上表现出色，其性能可与GPT-4媲美，甚至在某些情况下超越，且无需额外工具增强，无论在领域内还是领域外的数据集上都展现出良好性能。论文的代码和数据集已发布在https://github.com/RUCAIBox/HaluAgent。|
|**2024-06-18**|**AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval**|Shirley Wu et.al.|[2406.11200](http://arxiv.org/abs/2406.11200)|**[link](https://github.com/zou-group/avatar)**|**大型语言模型（LLMs）在利用外部工具和知识提升准确性和减少错误方面展现出显著能力。然而，设计能让LLMs有效运用这些工具的提示技巧是一项耗时且依赖直觉的任务。为此，我们提出AvaTaR，一个创新的自动化框架，它能优化LLMs，使其更有效地利用提供的工具，并在特定任务或领域中提升性能。AvaTaR通过设计一个比较器模块，以训练数据中的正负样本进行推理，迭代地为LLM提供富有洞察力和全面的提示。我们在四个包含文本、视觉和关系信息的复杂多模态检索数据集上展示了AvaTaR的效果。实验表明，AvaTaR在所有四项具有挑战性的任务中均优于现有最先进的方法，并展现出强大的泛化能力，当应用于新案例时，平均在Hit@1指标上实现了14%的相对改进。代码和数据集已在<https://github.com/zou-group/avatar>上公开。**|
|**2024-06-17**|**Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement**|Weimin Xiong et.al.|[2406.11176](http://arxiv.org/abs/2406.11176)|**[link](https://github.com/weiminxiong/ipr)**|**大型语言模型在一系列复杂的交互任务中展现出卓越性能。近期的研究倾向于通过专家轨迹调优来提升模型效果，但主要关注最终结果奖励，这可能导致错误或非最优行为，因为缺乏过程监督信号。为此，我们在本文中提出迭代步级过程改进（Iterative Step-level Process Refinement，IPR）框架，该框架提供了细致的逐步骤指导，以增强训练过程。我们采用蒙特卡洛方法估算每一步的奖励。在每个迭代中，模型沿着专家轨迹探索并生成新动作，然后与专家轨迹的相应步骤进行比较，使用步级奖励评估。这种比较有助于识别差异，形成用于训练的对比动作对。我们在三个复杂代理任务上的实验表明，我们的框架优于多种强大的基线。此外，我们的分析结果揭示了IPR在提升动作效率方面的有效性，并证明其适用于各种模型。**|
|**2024-06-17**|**RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents**|Weizhe Chen et.al.|[2406.11132](http://arxiv.org/abs/2406.11132)|null|在过去的一年里，大型语言模型（LLMs）在传统自然语言处理领域之外展现出惊人成就，人们开始探索在代码生成、旅行规划和机器人控制等更具体的应用领域使用这些模型。通过与LLM构建所谓的LLM代理，旨在协助人们完成日常生活中的各种任务。然而，对LLMs的提示语句对生成内容及其性能至关重要。因此，自动提示工程成为许多研究人员和LLM用户关注的焦点。本文提出了一种新颖的方法，名为\textsc{RePrompt}，它利用与LLM代理交互获取的对话历史，通过“梯度下降”优化LLM的逐步指令。通过优化提示，LLM能够学习特定领域的规划策略。我们在PDDL生成和旅行规划任务中进行了实验，结果显示，使用更新后的提示作为初始提示时，我们的方法通常可以提高不同推理任务的性能。|
|**2024-06-18**|**Embodied Question Answering via Multi-LLM Systems**|Bhrij Patel et.al.|[2406.10918](http://arxiv.org/abs/2406.10918)|null|## 背景  Embodied Question Answering（EQA）是一个关键问题，它涉及一个代理在环境中探索以回答用户查询。当前的研究主要集中在单代理场景中，这可能导致探索时间冗长且成本高昂。在这个工作中，我们考虑了多代理框架下的EQA，其中涉及多个基于大型语言模型（LLM）的独立代理，它们各自解答关于家庭环境的问题。为了为每个查询生成一个答案，我们利用各个独立响应来训练一个中央答案模型（CAM），该模型整合答案以实现更稳健的回答。通过使用CAM，我们观察到其在EQA准确率上比诸如投票机制和辩论等ensemble LLM聚合方法高出50%。CAM无需任何形式的代理间通信，从而避免了相关开销。我们还通过不同的非线性（如神经网络、随机森林、决策树、XGBoost）和线性算法（如逻辑回归分类器、支持向量机）对CAM进行了消融研究。最后，我们通过Permutation Feature Importance（PFI）分析了CAM对每个独立代理和查询上下文的依赖程度，量化了CAM的依赖特性。|
|**2024-06-16**|**GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents**|Dongping Chen et.al.|[2406.10819](http://arxiv.org/abs/2406.10819)|**[link](https://github.com/keplerlab/katna)**|**近年来，多模态大型语言模型（MLLM）已被用于控制键盘和鼠标输入，直接感知图形用户界面（GUI），并生成相应的代码。然而，当前的模型主要在静态环境中表现出色，主要应用于相对简单的领域，如网页或移动界面。我们认为，一个稳健的GUI代理应具备理解GUI的时空信息能力，包括动态网页内容和多步骤任务，还要全面理解各种GUI场景，包括桌面软件和多窗口交互。为此，本文提出了一项新数据集——GUI-World，其中包含了精心制作的人机标注，广泛涵盖六种GUI场景和八类GUI相关问题，以三种格式呈现。我们评估了当前最先进的MLLM，如图像LLMs和视频LLMs，在理解和处理不同类型GUI内容，特别是动态和序列内容方面的能力。研究发现，图像LLMs在没有手动标注关键帧或操作历史的情况下，难以应对动态GUI内容。另一方面，由于GUI视频数据集的稀疏性，视频LLMs在所有GUI相关任务上表现不佳。基于GUI-World，我们首次尝试使用微调后的视频LLM作为GUI代理，显示了对各种GUI任务理解的提升。然而，由于基础LLM性能的限制，我们得出结论，将视频LLMs用作GUI代理仍是一个重大挑战。我们相信，我们的工作为未来在动态GUI内容理解方面的研究提供了有价值的洞见。代码和数据集已在我们的项目主页https://gui-world.github.io/上公开。**|
|**2024-06-16**|**HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies**|William Watson et.al.|[2406.10803](http://arxiv.org/abs/2406.10803)|null|## 背景  大型语言模型（LLMs）在处理表格问答任务时面临诸多挑战，主要包括：（1）对于大表格有限的上下文窗口；（2）不同token化模式与单元格边界的复杂差异；（3）以及使用外部模型如gpt-3.5-turbo时的数据保密问题。为解决这些问题，我们提出了一种名为“HiddenTables”的合作游戏。这个游戏涉及代码生成LLM“Solver”和评估其在表格问答任务能力的“Oracle”，以自然语言规范为基础，同时保证数据安全。  我们通过实证实验在多样化的表格上展示了LLMs在处理复杂查询、处理组合依赖以及将自然语言转化为程序指令方面的局限性，特别是在提供具体表格结构的情况下。与基于编码器的模型不同，“HiddenTables”不受行数限制，从而提高了提示和完成 token 的效率。此外，我们创建了一个新的数据集“PyQTax”，包含116,671个问题-表格-答案三元组，并提供了更细致的问题分类和标签，进一步增强了我们的研究。  因此，除了学术贡献，揭示了LLMs在表格问答任务中的不足，“HiddenTables”还展示了如何在保障数据安全的同时，让LLMs与大规模数据集互动，以及降低生成成本的实践方法。|
|**2024-06-15**|**From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent**|Samuel S. Sohn et.al.|[2406.10478](http://arxiv.org/abs/2406.10478)|null|## 背景 在娱乐、教育和营销领域至关重要的数字故事叙述面临着生产规模扩展和灵活性提升的挑战。这篇论文介绍的StoryAgent框架利用大型语言模型和生成工具来自动化并优化数字故事创作过程。它采用自上而下的故事情节草拟和自下而上的资产生成方法，解决了手动干预、互动场景编排和叙事一致性等关键问题。这个框架促进了交互式和一致叙事的高效生产，适用于多种媒介，推动了内容创作的民主化，增强了用户的参与度。我们的实验结果显示，该框架能够在没有参考视频的情况下生成连贯的数字故事，这标志着自动数字故事叙述技术的一个重大进步。|
|**2024-06-13**|**GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning**|Zhen Xiang et.al.|[2406.09187](http://arxiv.org/abs/2406.09187)|null|随着大型语言模型（LLMs）的快速发展，LLM驱动的代理被广泛应用于各种应用，这引发了对其安全性和可信度的新担忧。现有的提升LLM安全性的方法并不直接适用于LLM驱动的代理，因为它们具有不同的目标和输出模式。本文提出了一种创新方法——GuardAgent，它作为其他LLM代理的“防护栏”。GuardAgent通过检查其输入/输出是否满足用户定义的一系列守护请求来监督目标LLM。GuardAgent分为两步：1）分析提供的守护请求创建任务计划；2）根据任务计划生成守护代码，并通过API调用或外部引擎执行。整个过程利用LLM作为核心推理组件，结合记忆模块中的上下文示例，增强了知识驱动的推理能力，使其能够理解各种文本守护请求并准确地将其转化为可执行代码，提供可靠的安全保障。  GuardAgent还配备了一个可扩展的工具箱，包含函数和API，无需额外训练LLM，强调了其通用性及低运营成本。此外，我们提出了两个新颖的基准：EICU-AC用于评估医疗健康代理的隐私相关访问控制，Mind2Web-SC用于评估网络代理的安全性。在这些基准上，GuardAgent分别在98.7%和90.0%的精度下有效管理了两种类型代理的无效输入和输出。实验还表明，GuardAgent能够适应新兴的LLM代理和守护请求，定义新的功能，进一步证明了其强大的泛化能力。|
|**2024-06-13**|**Multi-Agent Software Development through Cross-Team Collaboration**|Zhuoyun Du et.al.|[2406.08979](http://arxiv.org/abs/2406.08979)|**[link](https://github.com/openbmb/chatdev)**|**### 概述  最新的大型语言模型（LLMs）进展，如ChatDev，推动了软件开发领域的深刻变革，特别体现在多代理协作上。这些模型能够像人类团队一样合作，遵循瀑布模型进行需求分析、开发、审查、测试等阶段，实现自主软件生成。然而，单个开发流程中的每个阶段只会产生一种可能结果，导致只完成一条开发链，从而丧失在解决方案空间中探索多种决策路径的机会，可能导致结果不理想。为解决这一问题，我们提出了跨团队协作（Cross-Team Collaboration，CTC）框架，这是一种可扩展的多团队结构，它允许协同工作的团队在跨团队协作环境中共同提出决策，并交流各自见解，以优化内容生成。  实验结果显示，在软件开发领域的应用中，我们的方法显著优于现有基准，证实了框架的有效性。在故事生成方面的显著改进表明，该框架具有广泛的跨领域泛化能力。我们期待我们的工作能引导LLMs向跨团队模式发展，并在软件开发等领域带来重大进步。相关的代码和数据将在<https://github.com/OpenBMB/ChatDev>上提供。**|
|**2024-06-13**|**StreamBench: Towards Benchmarking Continuous Improvement of Language Agents**|Cheng-Kuang Wu et.al.|[2406.08747](http://arxiv.org/abs/2406.08747)|**[link](https://github.com/stream-bench/stream-bench)**|近期的研究表明，大型语言模型（LLMs）能够从经验中自我提升，这是部署后持续改进的重要能力。然而，现有的基准主要评估它们的固有能力，而不考察它们随时间改进的能力。为了填补这一空白，我们引入了StreamBench，这是一个开创性的基准，旨在评估LLMs在输入-反馈序列上的连续改进性能。StreamBench模拟了一个在线学习环境，其中LLMs接收到连续的反馈流，并迭代地提升其表现。此外，我们提出了一些简单但有效的LLM基线，并对影响成功流式策略的关键组件进行了全面分析。我们的工作为开发LLMs的有效在线学习策略奠定了基础，为流式场景中的更适应性AI系统铺平了道路。|
|**2024-06-12**|**MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents**|Luyuan Wang et.al.|[2406.08184](http://arxiv.org/abs/2406.08184)|null|随着大型语言模型（LLMs）在手机图形用户界面（GUI）上的直接交互能力日益增强，以及它们在自主管理日常任务方面的潜力，基于LLMs的移动代理正逐渐受到学术界和工业界的关注。然而，由于应用程序的无限状态和可行动作序列的模糊定义，对现有移动代理性能的基准研究相对匮乏。为解决这一挑战，我们提出了一种高效且用户友好的基准工具——MobileAgentBench，旨在减轻繁琐的手动测试负担。我们首先定义了涵盖10个开源应用的100项任务，按难度分为多个级别。接着，我们对包括AppAgent和MobileAgent在内的多个现有移动代理进行了评估，以全面系统地比较它们的表现。所有相关材料均可在我们的项目网站https://MobileAgentBench.github.io上获取，这将推动学术和工业领域的进步。|
|**2024-06-12**|**Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey**|Shang Wang et.al.|[2406.07973](http://arxiv.org/abs/2406.07973)|null|随着人工智能的快速发展，大型语言模型（LLMs）在自然语言处理方面取得了显著进步。这些模型通过大量数据训练，展现出强大的语言理解和生成能力，适用于机器翻译、聊天机器人等各种应用。然而，LLMs在其生命周期中暴露出一系列隐私和安全问题，这引起了学术界和工业界的关注。这些问题与传统语言模型相比具有独特性，鉴于当前的综述缺乏针对不同场景的清晰威胁分类，我们根据五个场景：预训练、微调、RAG系统、部署和基于LLM的代理，强调了独特的风险。考虑到每种威胁的特性，本调查提供了潜在威胁和应对策略。研究LLMs所面临的攻击和防御情况，可以为更多领域提供可行的研究方向，使更多人能够受益于LLMs。|
|**2024-06-14**|**Can Large Language Models Understand Spatial Audio?**|Changli Tang et.al.|[2406.07914](http://arxiv.org/abs/2406.07914)|null|该论文探讨了如何使大型语言模型（LLMs）掌握多通道音频中的空间信息，这是当前听觉LLMs所缺乏的能力。通过利用LLMs的高级认知和推理能力，目标是提升模型对三维环境的理解，通过音频。研究涉及三项空间音频任务：声源定位（SSL）、远场语音识别（FSR）和基于位置的语音提取（LSE），在每个任务上都取得了显著进展。在SSL方面，我们的方法在Spatial LibriSpeech数据集上的均方误差（MAE）达到2.70°，明显优于先前的基准约6.60°。此外，模型能够利用空间线索提高FSR的准确性，并通过文本提示，根据指定方向聚焦于声音，即使在重叠语音环境中也能执行LSE。这些成果揭示了LLMs适应物理音频概念的潜力，为构建基于LLM的三维环境中的代理铺平了道路。|
|**2024-06-11**|**DCA-Bench: A Benchmark for Dataset Curation Agents**|Benhao Huang et.al.|[2406.07275](http://arxiv.org/abs/2406.07275)|**[link](https://github.com/TRAIS-Lab/dca-bench)**|随着人工智能（AI）研究和开发的推进，数据集的质量日益关键。尽管开放数据集平台众多，但数据质量问题，如缺乏文档、标注错误和伦理考量，仍普遍存在。这些问题往往难以通过规则基础脚本检测，需要用户或维护者花费大量人力进行识别和验证。利用大型语言模型（LLMs）处理数据集整理的潜力令人期待。为此，我们提出了一项名为DCA-Bench的数据集管理代理基准，旨在评估LLM在检测隐藏数据质量问题方面的性能。我们从八个公开数据集平台收集了各种实际问题作为测试床。为了建立一个自动评估LLM成功与否的管道，我们设计了一个专门的LLM评估器。实验表明，基于LLM的评估器与人工评价高度吻合，能实现可靠的自动评估。我们还在多个基线LLM上进行了实验，显示了任务的复杂性，意味着将LLMs应用于现实世界的数据集管理仍需深入探索和创新。此外，该基准也可作为衡量LLMs在问题发现能力而非仅解决问题能力的测试平台。基准套件已开放在：\url{https://github.com/TRAIS-Lab/dca-bench}。|
|**2024-06-11**|**A Synthetic Dataset for Personal Attribute Inference**|Hanna Yukhymenko et.al.|[2406.07217](http://arxiv.org/abs/2406.07217)|**[link](https://github.com/eth-sri/synthpai)**|**近年来，强大的大型语言模型（LLMs）已为全球数亿用户所接触，但它们的强大功能和广泛世界知识也带来了隐私风险。本研究关注LLMs新兴的隐私威胁——从网络文本中准确推断个人信息。鉴于基于LLM的作者分析研究缺乏合适的公开数据集，主要是由于涉及真实个人数据的伦理和隐私顾虑，我们的工作在两个方面进行了探索：（i）我们构建了一个使用合成个人资料填充的流行社交平台Reddit的模拟框架；（ii）利用此框架，我们生成了SynthPAI，一个包含超过7800条经过手动标记个人属性的多样化的合成评论数据集。我们通过一项人类研究验证了数据集，结果显示人类在区分真实和合成评论的任务上几乎不优于随机猜测。此外，我们证明了数据集支持有意义的个人属性推断研究，通过18种最先进的LLMs，我们发现使用合成评论可以得出与现实世界数据相同的结论。综上所述，我们的数据集和流程为未来研究如何理解和减轻LLMs带来的基于推断的隐私威胁提供了强大且隐私保护的基础。**|
|**2024-06-11**|**A Tool for Test Case Scenarios Generation Using Large Language Models**|Abdul Malik Sami et.al.|[2406.07021](http://arxiv.org/abs/2406.07021)|null|大型语言模型（LLMs）在软件工程（SE）中广泛应用，涵盖代码生成、软件设计和文档编写、添加代码注释、代码审查以及编写测试脚本等任务。然而，创建测试脚本或自动化测试案例需要与功能需求紧密相关的详尽测试套件文档。这种文档应能在有限的时间和范围内实现全面测试，尤其当需求和用户期望不断变化时。本文主要关注根据用户需求生成史诗级（epics）和高层次用户故事，然后基于这些故事设计测试场景。文章介绍了一种基于LLM代理和提示工程的网络软件工具，该工具能够自动化针对用户需求生成测试场景的过程。|
|**2024-06-11**|**CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only**|Junhee Cho et.al.|[2406.06947](http://arxiv.org/abs/2406.06947)|**[link](https://github.com/caap-agent/caap-agent)**|**长期以来，软件机器人已经在机器人流程自动化（RPA）中用于执行枯燥的计算机任务。随着大型语言模型（LLMs）的先进推理能力的出现，这些代理现在能够处理更复杂甚至前所未见的任务。然而，当前文献中的基于LLM的自动化方法往往依赖于HTML源代码作为输入，限制了它们在非网络环境的应用。HTML代码中的信息常常不准确或不完整，这降低了代理在实际应用中的可靠性。我们提出了一种仅基于屏幕截图的LLM驱动的代理，它专注于识别环境，并利用上下文学习来消除对大量人类演示数据的需求。我们的策略名为“上下文感知行动规划”（Context-Aware Action Planning，CAAP）提示，鼓励代理从多个角度仔细审查上下文。通过我们的方法，在67种MiniWoB++问题上实现了94.4%的成功率，每个问题类型只需1.48次演示。我们的方法为更广泛的应用提供了可能，特别是在需要在计算机或智能手机之间进行跨应用协调的任务上，标志着自动化代理领域的重大进步。代码和模型已在https://github.com/caap-agent/caap-agent上提供。**|
|**2024-06-07**|**GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents**|Anthony Costarelli et.al.|[2406.06613](http://arxiv.org/abs/2406.06613)|**[link](https://github.com/Joshuaclymer/GameBench)**|**大型语言模型已经在许多自然语言理解任务上展现出卓越的少量样本性能。尽管已经展示过在复杂策略场景中使用大型语言模型，但缺乏一个全面的框架来评估这些模型在游戏中的各种推理能力。为了填补这一空白，我们推出了GameBench，这是一个跨领域的框架，用于评估大型语言模型（LLMs）的战略思维能力。我们专注于9个不同的游戏环境，每个游戏至少涵盖一种在策略游戏中识别出的关键推理技能，并选择那些战略解释不太可能构成模型预训练数据主要部分的游戏。我们的评估使用了基础形式的GPT-3和GPT-4，以及两个旨在增强战略推理能力的引导框架：Chain-of-Thought（CoT）提示和Reasoning Via Planning（RAP）。结果显示，所有测试模型的表现都没有达到人类水平，最差的是GPT-4的表现甚至低于随机行动。CoT和RAP都提高了分数，但仍远未达到人类水平。**|
|**2024-06-11**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-09**|**Hello Again! LLM-powered Personalized Agent for Long-term Dialogue**|Hao Li et.al.|[2406.05925](http://arxiv.org/abs/2406.05925)|**[link](https://github.com/leolee99/ld-agent)**|**随着大型语言模型（LLMs）的发展，开放域对话系统取得了显著进步。然而，大多数现有系统主要关注简短的单次会话，忽视了长期陪伴和个性化聊天机器人在现实世界中的需求。为了满足这种实际需求，事件总结和人格管理至关重要，它们能够促进长期对话回复的合理性。近期，大型语言模型在人类认知和推理能力上的进展表明，基于LLM的代理有可能大幅增强自动化感知、决策和问题解决。鉴于此，我们提出了一种模型通用的框架——长期对话代理（LD-Agent），它包括三个可独立调整的模块：事件感知、人格提取和响应生成。事件记忆模块使用长短期记忆库分别关注历史和正在进行的会话，并引入了基于主题的检索机制以提高记忆检索的准确性。此外，人格模块实现了用户和代理的动态人格建模。最后，通过整合检索的记忆和提取的人格，生成器会产生适当的回应。我们在各种示例基准、模型和任务上实证了LD-Agent的有效性、通用性和跨领域能力。代码已在https://github.com/leolee99/LD-Agent上发布。**|
|**2024-06-09**|**A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components**|Xinzhe Li et.al.|[2406.05804](http://arxiv.org/abs/2406.05804)|**[link](https://github.com/xinzhel/llm-agent-survey)**|## 背景  近期大型语言模型（LLMs）的进展推动了复杂代理工作流的发展，它们相较于传统的单路径、链式思维（Chain-of-Thought，CoT）提示方法有所改进。这篇综述旨在概述常见的工作流，特别关注大型语言模型特性的组件（LLM-Profiled Components，LMPCs），并强调对非LLM组件的忽略。这种研究的目的是为了增进对LLMs角色的理解，并探索LMPC的复用潜力。|
|**2024-06-07**|**Mixture-of-Agents Enhances Large Language Model Capabilities**|Junlin Wang et.al.|[2406.04692](http://arxiv.org/abs/2406.04692)|null|近期的大型语言模型（LLMs）进展显著，展现出在自然语言理解和生成任务中的强大能力。随着LLMs的增多，如何有效整合多模型的知识成为了一个令人振奋的研究方向。为此，我们提出了一种新颖的方法——混合代理（Mixture-of-Agents，MoA）方法。在我们的架构中，MoA采用了分层设计，每层包含多个LLM代理。每个代理在生成响应时，会利用前一层所有代理的输出作为辅助信息。通过这种策略，MoA模型在AlpacaEval 2.0、MT-Bench和FLASK等多个评估基准上实现了最先进的性能，超越了GPT-4全能版。例如，仅使用开源LLMs的我们的MoA模型在AlpacaEval 2.0上的得分领先，达到65.1%，而GPT-4全能版的成绩为57.5%。|
|**2024-06-06**|**AgentGym: Evolving Large Language Model-based Agents across Diverse Environments**|Zhiheng Xi et.al.|[2406.04151](http://arxiv.org/abs/2406.04151)|**[link](https://github.com/woooodyy/agentgym)**|**在人工智能领域，建立能够处理各种任务并在不同环境中自我进化的泛化型代理是一个长期目标。大型语言模型（LLMs）因其通用能力被认为是实现这一目标的有前景的基础。当前的方法要么依赖于人类监督，让LLM代理逐步模仿专家提供的轨迹，难以大规模扩展且限制了环境探索；要么让代理在孤立环境中探索学习，导致专长有限、缺乏泛化能力。本文首次尝试构建具备自我进化能力的通用LLM代理。我们提出三个关键要素：1）多样的环境以支持代理探索和学习；2）一套轨迹来赋予代理基本能力和先验知识；3）有效且可扩展的进化方法。  我们提出了AgentGym，一个新框架，它包含丰富的环境和任务，支持全面、实时、统一格式和并发的代理探索。AgentGym还包括一个扩展指令的数据库、基准测试套件以及跨环境的高质量轨迹。接着，我们开发了AgentEvol，这是一种新颖的方法，旨在研究代理在超越既定数据，跨越任务和环境时的自我进化潜力。  实验结果显示，进化后的代理可以达到与最先进的模型相当的性能。我们发布了AgentGym套件，包括平台、数据集、基准、检查点和算法实现。AgentGym套件已在其官方网站https://github.com/WooooDyy/AgentGym上提供。**|
|**2024-06-05**|**The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games**|Mikhail Mozikov et.al.|[2406.03299](http://arxiv.org/abs/2406.03299)|null|## 翻译  行为研究实验在社会模型和理解人际互动中占据重要地位。然而，实际操作中这类实验常面临内在效度、外在效度、可重复性和社会偏见等挑战，因为人类的社会互动与合作复杂。近年来，大型语言模型（LLMs）的进步为研究者提供了一种新的模拟人类行为的工具。但现有基于LLM的模拟假设模型的行为与人类相似，却忽视了影响人类决策的关键因素——情绪。本文提出一种新颖的方法论和框架，旨在探讨LLMs的决策制定及其在情绪状态下的行为与人类行为的契合度。  通过在两种不同类型的行为经济学游戏（博弈论实验）中使用GPT-3.5和GPT-4，我们发现情绪对LLMs的表现有显著影响，促使它们发展出更优化的策略。尽管GPT-3.5与人类参与者的行动模式有较强的对应，尤其是在讨价还价游戏中，但GPT-4展现出一致的行为，对于情绪诱导的理性决策似乎不受影响。令人意外的是，情绪提示，特别是愤怒情绪，能够打破GPT-4的“超人”一致性，使其反应更接近人类的情绪反应。|
|**2024-06-05**|**BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents**|Yifei Wang et.al.|[2406.03007](http://arxiv.org/abs/2406.03007)|**[link](https://github.com/dpamk/badagent)**|**随着大型语言模型（LLMs）的繁荣，基于训练好的LLMs并通过特定任务数据微调的强大智能代理已开发出来，提供定制服务。当前最先进的构建LLM代理的方法是使用预训练模型，并针对任务进行进一步调整。然而，我们揭示了这些方法易受名为BadAgent的新型后门攻击，该攻击通过在后门数据上微调在各种代理任务中植入后门。在测试时，攻击者可以通过在输入或环境中显示触发器，操纵部署的LLM代理执行有害操作。令人惊讶的是，我们的攻击方法即使在信任的数据上进行微调后仍表现出极高的鲁棒性。尽管后门攻击在自然语言处理领域已广泛研究，但据我们所知，我们可能是第一个研究在权限更大的LLM代理上的攻击，这些代理可以使用外部工具，因此更具威胁。我们的工作明确指出了基于不信任的LLM或数据构建LLM代理的风险。我们的代码已公开在：[https://github.com/DPamK/BadAgent](https://github.com/DPamK/BadAgent)。**|
|**2024-06-02**|**Teams of LLM Agents can Exploit Zero-Day Vulnerabilities**|Richard Fang et.al.|[2406.01637](http://arxiv.org/abs/2406.01637)|null|随着大语言模型（LLMs）在网络安全领域的复杂性不断提高，研究者发现，当提供漏洞描述和简单的夺旗问题时，这些模型能够利用实际存在的漏洞。然而，对于事先未知的零日漏洞（即攻击者掌握而安全软件供应商还未修补的漏洞），它们的表现仍然不佳。本文展示了，通过团队合作，多个LLM代理可以攻击现实世界的零日漏洞。单独的代理在探索众多漏洞和进行长期规划时面临困难。为此，我们提出了HPTSA系统，它包括一个能调度子代理的计划代理。计划代理负责探索系统并决定使用哪个子代理来尝试不同的漏洞，从而解决了长期规划的问题。我们在一个包含15个真实世界漏洞的基准上进行了实验，结果显示，我们的代理团队比先前的工作提高了4.5倍。|
|**2024-06-03**|**How to Understand Whole Software Repository?**|Yingwei Ma et.al.|[2406.01422](http://arxiv.org/abs/2406.01422)|null|## 背景  近期，基于大型语言模型（LLM）的代理在自动软件工程（ASE）领域取得了显著进步。尽管现有方法已证实有效，但它们的设计主要侧重于代码的局部信息，如问题、类和函数，这限制了对软件系统全局上下文和依赖关系的理解。根据软件开发人员的实际经验，我们认为全面理解整个仓库是迈向ASE的关键。然而，理解整个仓库带来了诸多挑战，例如：长代码输入、噪声代码信息、复杂依赖关系等。  为了克服这些问题，我们研发了一种名为RepoUnderstander的新ASE方法，通过引导代理全面理解整个仓库。首先，我们采用自上而下的方式将整个仓库的关键信息压缩到知识图谱中，以降低复杂性。接着，我们提出一种蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）为基础的仓库探索策略，赋予代理理解整个仓库的能力。此外，为了更好地利用仓库级别的知识，我们指导代理进行总结、分析和规划，然后他们可以利用工具动态获取信息并生成修复实际GitHub问题的补丁。  大量实验表明，RepoUnderstander具有优越性和有效性。在SWE-bench Lite基准测试中，与SWE-agent相比，它实现了18.5%的相对提升。|
|**2024-06-03**|**BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards**|Diego Dorn et.al.|[2406.01364](http://arxiv.org/abs/2406.01364)|null|## 背景  输入-输出安全防护机制被用于检测大型语言模型（LLMs）系统的异常输出。这些防护措施在实时监控、离线评估和内容审核等关键应用中发挥核心作用。然而，目前缺乏统一的评估方法来衡量它们的性能。为了填补这一空白，我们提出了“大型语言模型安全防护基准”（Benchmarks for the Evaluation of LLM Safeguards，简称BELLS），它是一个结构化的测试集合，分为三个类别：(1) 建立性故障测试，基于已存在的针对明确故障模式的基准，旨在比较当前输入-输出安全防护的效能；(2) 新兴故障测试，用于衡量对未见过的故障模式的泛化能力，以促进更通用防护机制的发展；(3) 下一代架构测试，针对更复杂的架构（如LLM代理和多代理系统），目标是推动适用于未来尚未存在专门防护的应用的安全防护技术的发展。此外，我们还实现了并分享了第一个下一代架构测试，使用MACHIAVELLI环境，并提供了数据集的交互式可视化。|
|**2024-06-03**|**A Survey of Useful LLM Evaluation**|Ji-Lun Peng et.al.|[2406.00936](http://arxiv.org/abs/2406.00936)|null|由于大语言模型在各个研究领域展现出卓越的性能，对它们的能力评估方法的需求日益增长，以确定其合适的任务和责任。本文主要探讨如何有效地利用大语言模型作为工具，并提出一个两阶段框架：从“核心能力”到“代理”。首先，核心能力指的是大语言模型生成高质量文本所必需的特性，通过验证这些能力后，它们能够处理现实世界的复杂任务，扮演代理角色。在“核心能力”阶段，我们讨论了大语言模型的推理能力、社会影响以及领域知识。而在“代理”阶段，我们展示了大语言模型在具身行动、规划和工具学习方面的应用。最后，我们分析了当前大语言模型评估方法面临的挑战，并展望了未来的发展方向。|
|**2024-06-02**|**CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems**|Yanlin Feng et.al.|[2406.00583](http://arxiv.org/abs/2406.00583)|**[link](https://github.com/megagonlabs/CMDBench)**|### 背景  在数据库和人工智能领域，复合人工智能系统（Compound Artificial Intelligence Systems，CAS）利用大型语言模型（Large Language Models，LLMs）作为代理，通过与工具和数据检索器交互来执行知识密集型任务，引起了广泛关注。尽管这些系统有可能增强企业数据平台中数据分析师的一般分析流程，但CAS面临着与分析师相似的数据发现挑战：组织内部不同团队和部门创建的多模态数据源孤立，这使得寻找完成当前任务所需合适数据源变得困难。现有的数据发现基准并未充分模拟这种多模态和数据源的多样性。此外，CAS的现有基准主要关注端到端任务性能评估，而忽视了数据发现性能。  为了推动在现实世界环境中对多模态数据检索器在CAS中的数据发现性能研究，我们提出了CMDBench，一个旨在模拟企业数据平台复杂性的基准。我们改编了开放领域的现有数据集和基准，如问答、复杂推理以及自然语言查询结构化数据，来评估粗粒度和细粒度的数据发现以及任务执行性能。  ### 实验结果  我们的实验揭示了数据检索器设计对下游任务性能的影响——平均情况下，任务准确率下降了46%。实验结果表明，需要开发优化策略来确定合适的LLM代理和检索器，以提高在企业数据上高效执行CAS的能力。  总之，CMDBench是一个旨在促进针对企业数据平台复杂性进行研究的新工具，它通过综合评估数据发现和任务执行能力，为改进多模态数据检索器在复合人工智能系统中的性能提供了一个有价值的框架。|
|**2024-06-01**|**Controlling Large Language Model Agents with Entropic Activation Steering**|Nate Rahn et.al.|[2406.00244](http://arxiv.org/abs/2406.00244)|null|随着大规模预训练语言模型（LLMs）的普遍适用性提升，人们对其用作基于上下文的学习代理的兴趣日益增长。在这些情境下，模型需要根据与环境的有限交互形成目标实现策略的信念，并在每一步决策中处理不确定性。本文针对这一问题进行研究，通过控制的序列决策任务实验探讨LLMs如何形成和运用这些信念。  首先，我们发现LLM模型过于自信：它们在缺乏充分证据的情况下就对行动做出强烈判断，导致探索行为不足。进一步深入分析揭示，这种现象源于从LLM采样得到的动作分布熵的塌缩。接着，我们指出现有的基于令牌的采样方法本身不足以促使模型更广泛探索。  鉴于此，我们提出了熵激活导向（Entropic Activation Steering，EAST），这是一种针对在上下文中的LLM代理的激活导向方法。EAST计算一个以熵为权重的表示组合，通过在前向传播过程中干预模型的激活，来调整模型对动作的不确定性，从而促进探索行为的出现。最后，EAST改变了LLM在决策时表达的主观不确定性，为理解和控制模型对决策不确定性的表征提供了途径。|
|**2024-05-31**|**Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training**|Maximillian Chen et.al.|[2406.00222](http://arxiv.org/abs/2406.00222)|null|大型语言模型（LLMs）通过人类反馈的强化学习（RLHF）已经迅速成为构建智能对话助手的主要方法。然而，尽管在多个基准上表现出色，基于LLM的代理在诸如歧义处理等对话技能上仍有欠缺：当通用助手遇到模糊情况时，它们往往过度谨慎或猜测用户的真正意图，而不是提问以求澄清，而在特定任务场景下，高质量对话样本往往有限，影响模型学习最优对话行为策略的能力。我们提出了一种名为Action-Based Contrastive Self-Training（ACT）的近似在线偏好优化算法，它基于Direct Preference Optimization（DPO），旨在实现在多轮对话中的样本高效对话策略学习。  我们在三个具有挑战性的对话任务中验证了ACT的有效性：基于表格的问答、机器阅读理解，以及AmbigSQL，这是一个针对文本到SQL生成的信息寻求请求歧义解决的新任务。此外，我们提议通过评估LLMs能否在对话中识别和推理歧义来衡量其作为对话代理的能力。ACT在与标准监督微调和DPO方法相比时，显示出了显著的对话建模改进。|
|**2024-05-31**|**Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent**|Jie JW Wu et.al.|[2406.00215](http://arxiv.org/abs/2406.00215)|**[link](https://github.com/jie-jw-wu/human-eval-comm)**|大型语言模型（LLMs）在代码生成任务中的性能显著提升，但仍与顶级软件工程师的水平存在差距。鉴于顶级软件工程师常通过提问来消除需求和编码解决方案中的模糊性，我们提出对于LLMs进行代码生成任务时也应具备类似的沟通能力。为此，我们进行了实证研究，关注LLMs的沟通技能，即“在代码生成问题描述存在问题时能提出澄清问题”。  我们创建了一个新的基准测试，名为HumanEvalComm，通过修改问题描述，引入了不一致性、模糊性和不完整性三个问题维度。我们定义了新的评估指标，如通信率和良好问题率，并在HumanEvalComm上对不同类型的Code LLM（代码语言模型）以及一种新型LLM代理方法（Okanagan）进行了实验，该方法旨在从代码和描述中识别并提问，以进一步优化生成的代码。最后，我们通过比较Code LLMs和Okanagan的表现，讨论了实验结果。|
|**2024-05-30**|**Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions**|Ruochen Zhao et.al.|[2405.20267](http://arxiv.org/abs/2405.20267)|**[link](https://github.com/Auto-Arena/Auto-Arena-LLMs)**|**随着语言模型（LLMs）日新月异，迫切需要一种可靠且及时的评估方法。鉴于静态基准易受污染，用户往往依赖于像Chatbot Arena这样的人类投票平台。然而，人工标注需要大量人力。为此，我们创新性地提出Auto-Arena，这是一种自动化全流程的LLM评估框架。首先，由考官LLM设计问题；接着，候选LLMs围绕问题进行多轮相互对决，暴露出它们的真实性能差距；最后，由LLM裁判集体讨论并决定胜者，从而减少偏见，提升公平性。我们在最新17款LLMs上的广泛实验显示，Auto-Arena与人类偏好具有最高的相关性，为替代人类评价平台提供了有前景的解决方案。**|
|**2024-05-30**|**Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory**|Hangyeol Kang et.al.|[2405.20189](http://arxiv.org/abs/2405.20189)|null|在本研究中，我们阐述了为Nadine社交机器人平台开发智能和健壮的社交机器人系统的方法。我们通过集成大型语言模型（LLMs），巧妙地利用这些模型的强大推理和指令执行能力，以实现接近人类的感性与认知能力。这与当前基于LLM的智能体相比是创新的，因为它们通常不具备人类式的长期记忆或复杂的情感评估功能。社交机器人的自然性在很大程度上取决于系统各组件的性能和协同工作。我们构建了一个系统，能够通过多模态输入处理生成恰当的行为，根据识别到的用户引入相关的情景记忆，并模拟机器人在与人类伙伴互动过程中产生的情绪状态。特别是，我们提出了一个针对社交机器人的LLM-agent框架，SoR-ReAct，作为我们系统中交互模块的核心组件。这一设计推动了社交机器人技术的发展，旨在提升人机交互的质量。|
|**2024-05-29**|**Adaptive In-conversation Team Building for Language Model Agents**|Linxin Song et.al.|[2405.19425](http://arxiv.org/abs/2405.19425)|null|### 翻译  在处理复杂任务时，利用多个大型语言模型（LLMs）展现出前景。然而，如何为特定应用设计有效的多代理团队仍是一个挑战。本文提出了一种新的动态团队构建范式，名为“Captain Agent”。它通过创新的Agent设计，能够自适应地为每个问题解决步骤组建和管理团队，利用嵌套群聊和反思机制确保多元化的专业知识，防止刻板输出。这种方法提供了灵活但结构化的解决问题方式，有助于减少冗余，增强输出多样性。在六个实际场景中的全面评估显示，Captain Agent显著优于现有多代理方法，平均准确率提高了21.94%，并且无需针对特定任务进行繁琐的提示工程，表现出色。|
|**2024-05-28**|**A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**|Chengxing Xie et.al.|[2405.18208](http://arxiv.org/abs/2405.18208)|null|近期的研究已经表明，这些大型语言模型在一些简单的任务上，如写作和编码，展现出一定的能力。然而，它们在需要综合规划的任务上仍然面临挑战，这仍是当前模型的一个重要研究问题。本研究聚焦于旅行规划，这是一个涉及多个阶段的复杂问题，包括提纲、信息收集和规划，通常伴随着各种约束和不确定性。现有的推理方法在处理这类问题时效果不佳。我们的目标是通过开发一种类似人类的规划框架，引导大型语言模型模仿人类解决多阶段问题的步骤，以提升其能力。具体来说，我们实施策略，让模型能为每个旅行查询生成连贯的提纲，模拟人类的规划模式。我们还引入了策略块和知识块到框架中：策略块帮助信息搜集，而知识块提供详细规划所需的必要信息。实验结果全面展示了我们框架对大型语言模型规划能力的显著提升，使其在处理旅行规划任务时效率和效果都有所提高。实验结果显示，当与GPT-4-Turbo结合时，我们的框架相较于基础框架在GPT-4-Turbo上的性能提升了10倍。|
|**2024-05-28**|**Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**|Hongda Sun et.al.|[2405.18113](http://arxiv.org/abs/2405.18113)|null|随着在线招聘服务的兴起，传统的求职和招聘方式发生了变革，迫切需要开发高质量的工业应用来提升求职者与职位的匹配度。现有的方法主要依赖于简历和职位描述的潜在语义建模，学习两者之间的匹配函数。受到大型语言模型（LLMs）在角色扮演方面强大能力的启发，我们提出引入LLMs模拟面试环节，让其与求职者进行对话，这可以为候选人评估提供额外证据，从而增强仅基于简历和职位描述的个性化匹配。然而，在网络招聘中的面试官和求职者角色塑造仍面临挑战，如提问技巧、回答构建以及双向匹配度评估。  为此，我们提出MockLLM，一个创新的框架，将人职匹配过程划分为两个模块：模拟面试生成和握手协议中的双向评估，通过面试官和求职者之间的协作行为共同提升性能。我们设计了一个多角色、多行为的框架，使单一的LLM代理能有效地扮演双方的不同职能。此外，我们引入了反思记忆生成和动态提示修改技术，以优化双方的行为，持续优化附加的评估证据。实验结果表明，MockLLM在人职匹配上的表现最优，且模拟面试质量高，预示着它在未来在线招聘中的实际应用前景广阔。|
|**2024-05-28**|**LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**|Yuchen Xia et.al.|[2405.18092](http://arxiv.org/abs/2405.18092)|**[link](https://github.com/yuchenxia/llmdrivensimulation)**|**该论文提出了一种创新的多agent系统架构，将大型语言模型（LLM）应用于数字孪生过程模拟的参数自动化。我们设计了一个框架，包含观察、推理、决策和总结四种类型的代理。通过实现LLM代理与模拟模型的动态交互，该系统可以自动探索参数设置，利用启发式推理确定一组控制模拟以达成目标的参数。这种方法通过注入LLM的启发式，增强模拟模型，并支持自主搜索以解决用户任务，有望提高用户体验并减轻人类用户在复杂决策过程中的认知负担。研究通过一个案例研究展示了系统的有效性与功能，并在GitHub仓库<https://github.com/YuchenXia/LLMDrivenSimulation>提供了可视化的演示。**|
|**2024-05-28**|**Enabling Generative Design Tools with LLM Agents for Building Novel Devices: A Case Study on Fluidic Computation Interfaces**|Qiuyu Lu et.al.|[2405.17837](http://arxiv.org/abs/2405.17837)|null|在人机交互（HCI）领域，交互设备的设计开发是关键关注点。随着新型硬件和先进制造技术的兴起，对能够简化原型制作过程的专门设计工具的需求日益增长。然而，这些工具虽然通过参数化设计和模拟简化流程，但学习曲线较陡，且在激发创新思维方面有所欠缺。本研究以流体计算界面为例，探讨如何通过大型语言模型（LLM）代理增强物理设备设计工具，创建一个生成设计工具（GDT）。借助LLM，GDT能够理解新设备的特性和局限，提出多样、富有洞察力且实用的应用场景，推荐技术和情境适宜的设备设计，并自动生成设计参数，以便传统设计工具展示结果并生成加工所需的文件。本文阐述了GDT的框架、实现和性能，并反思其前景及遇到的挑战。|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|## 背景 由于需要与现实世界互动，Embodied agent 需要具备丰富的先验知识、长远规划能力以及快速的响应速度。尽管最近的大型语言模型（LLM）在性能上表现出色，但它们仍存在局限性，例如，LLM的输出通常是描述性的句子，在决定具体行动时可能产生歧义。为了克服这些问题，我们引入了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归的方式预测后续动作。为了训练 LARM，我们开发了一种新颖的数据格式——自回归节点传输结构，并构建了相应的数据集。通过两阶段的训练策略，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法的最高成就需要更为复杂的决策链。此外，LARM的速度比现有最快方法快出了6.8倍。|
|**2024-05-30**|**Meta-Task Planning for Language Agents**|Cong Zhang et.al.|[2405.16510](http://arxiv.org/abs/2405.16510)|null|神经语言模型的快速发展推动了智能代理研究的新热潮。大型语言模型（LLM）作为实现人工智能通用性（AGI）的有前景方法，因其出色的推理和泛化能力而备受瞩目。在实际任务中，有效的规划对LLM代理的成功至关重要。然而，如何为复杂任务设计出可行或最优的精细粒度操作序列，特别是需要组合大量异质行动的序列，仍是挑战。本文提出Meta-Task Planning（MTP），这是一种零样本的协作式LLM多代理系统方法，通过将复杂任务分解为子任务，即元任务，简化了任务规划。每个元任务随后映射为可执行动作。在TravelPlanner和API-Bank两个严格基准上评估了MTP。结果表明，MTP在TravelPlanner上的平均成功率约为40%，远超当前最佳基线（2.92%），并且在API-Bank上的性能比使用ReAct的LLM_{api}-4高出约14%，这显示出将LLM与多代理系统相结合的巨大潜力。|
|**2024-05-28**|**STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making**|Chuanhao Li et.al.|[2405.16376](http://arxiv.org/abs/2405.16376)|**[link](https://github.com/cyrilli/stride)**|**大型语言模型（如GPT-4）在自然语言处理方面带来了革命性变化，展现出卓越的语言能力和推理技巧。然而，在战略性的多代理决策环境中，它们面临局限，如数学推理能力差、难以遵循指令和生成错误信息。这些缺点限制了它们在遵守复杂游戏规则、长期规划、探索未知环境以及预测对手行动的互动任务中的表现。为此，本文提出了一种新型的结合了记忆和专业工具的大型语言模型代理框架，旨在提升其在战略决策方面的性能。我们特别在双边谈判、多代理动态机制设计等经济重要场景中应用这些工具，并通过定量指标评估在各种战略决策问题上的效果。研究结果表明，我们的增强框架显著提高了大型语言模型在战略决策中的能力。尽管当前模型存在固有局限，但我们通过有针对性的增强展示了改进的可能性，这为未来大型语言模型在交互环境中的应用提供了有前景的方向。**|
|**2024-05-29**|**Devil's Advocate: Anticipatory Reflection for LLM Agents**|Haoyu Wang et.al.|[2405.16334](http://arxiv.org/abs/2405.16334)|null|在这个工作中，我们提出了一种新颖的方法，通过赋予语言模型（LLM）自我反思能力，增强了其在解决复杂任务时的一致性和适应性。我们的方法促使LLM代理将给定的任务分解为可管理的子任务（即制定计划），并在执行行动之前持续反思可能的失败及其补救措施、执行后与子任务目标对齐并进行必要的回溯以确保全力以赴执行计划，以及在完成计划后进行全面审查，以便于未来策略的优化。通过在WebArena中零样本应用这一方法处理实际的网络环境任务，我们的代理表现出优于现有零样本方法的性能。实验结果显示，这种基于反思的策略不仅提升了代理应对未预见挑战的导航能力，通过强大的计划执行机制，还提高了效率，减少了实现任务所需的尝试次数和计划修订次数。|
|**2024-05-25**|**AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning**|Minghao Chen et.al.|[2405.16247](http://arxiv.org/abs/2405.16247)|**[link](https://github.com/minghchen/automanual)**|大语言模型（LLMs）在执行各种领域任务，如机器人、游戏和网络导航方面展现出潜力。然而，这些模型通常需要精心设计和专家级提示才能适应特定领域的任务，这限制了它们的适应性。为此，我们提出了AutoManual框架，让LLMs能够通过互动自主构建理解，并适应新环境。AutoManual将环境知识分为多样的规则，并通过两个代理进行在线优化：1）规划器根据当前规则制定可操作的行动计划；2）构建者通过一个结构化的规则系统更新规则，促进在线规则管理并保持关键细节。为了减少在管理规则时的幻觉，我们引入了“案例条件提示”策略用于构建者。最终，编译器代理将这些规则整合成一份全面的手册。这份自我生成的手册不仅能提高适应性，还能指导小型LLMs的规划，同时保持人类可读。仅凭一次简单演示，AutoManual显著提高了任务成功率，GPT-4-turbo下达到97.4%，GPT-3.5-turbo下为86.2%。源代码即将发布。|
|**2024-05-24**|**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**|Yuxuan Guo et.al.|[2405.15414](http://arxiv.org/abs/2405.15414)|null|在人工智能研究中，构建开放型代理一直以来都是终极目标，特别是创造性的代理更具吸引力。现有的大语言模型（LLM）在执行有明确目标的长序列任务（如《我的世界》中的“开采钻石”）上表现出色。然而，它们在处理具有开放目标和抽象标准的创造性任务时遇到困难，因为它们无法弥合这些任务之间的鸿沟，从而缺乏自我改进来解决问题的反馈。为此，我们的工作引入了自主实体验证技术，以填补这一空白，为创造性任务奠定了基础。特别地，我们提出了Luban代理，专注于《我的世界》中的创造性建筑任务，它配备了两级自主实体验证，灵感来源于人类设计实践：（1）视觉验证3D结构推测，通过代理自动生成的CAD建模程序实现；（2）实用验证，根据抽象标准生成并验证与环境相关的功能程序。广泛的多维度人类研究和Elo评级显示，Luban能够在我们提出的基准中完成多样化的创造性建筑任务，并在可视化和实用性方面分别比其他基线提高了33%到100%。此外，实现在真实世界机器人手臂上的演示展示了Luban在物理世界中的创作潜力。|
|**2024-05-24**|**CulturePark: Boosting Cross-cultural Understanding in Large Language Models**|Cheng Li et.al.|[2405.15145](http://arxiv.org/abs/2405.15145)|**[link](https://github.com/scarelette/culturepark)**|由于大型语言模型（LLMs）普遍存在文化偏见，主要源于缺乏代表不同文化的代表性数据。传统的文化数据集和基准通常通过从现有数据集中提取或聚合来自维基百科和社交媒体的信息构建，但这种方法依赖于现实世界的数据和人工标注，成本高且难以扩展。本文借鉴认知社会交流理论，提出CulturePark，一个利用LLMs的多代理沟通框架，用于文化数据收集。CulturePark通过模拟不同文化背景下的人类交流，让基于LLM的代理角色扮演，生成包含人类信念、规范和习俗的高质量跨文化对话。我们使用CulturePark生成了41,000个文化样本，对八种特定文化进行了模型微调。在三项下游任务评估中，这些模型的表现优于GPT-4：内容过滤、文化一致性（在霍夫斯泰德文化维度量表上）和文化教育。结果显示，我们的GPT-3.5模型在内容过滤任务上与GPT-4相当或优于它；在文化一致性方面，我们的模型在霍夫斯泰德文化维度量表13框架上超越GPT-4；在人类参与者的文化教育效果和用户体验上，我们的模型也表现出色。CulturePark对于减少文化偏见和推动AI的民主化具有重要意义，强调了文化包容性数据在模型训练中的关键作用。|
|**2024-05-23**|**AnalogCoder: Analog Circuit Design via Training-Free Code Generation**|Yao Lai et.al.|[2405.14918](http://arxiv.org/abs/2405.14918)|**[link](https://github.com/laiyao1/AnalogCoder)**|### 翻译  在现代芯片技术中，模拟电路设计是一个关键任务，它涉及组件选择、连接和参数设置以确保电路功能正常。尽管大型语言模型（LLMs）在数字电路设计方面取得了进步，但模拟电路的复杂性和数据稀缺性带来了挑战。为此，我们推出了AnalogCoder，这是首个无需训练的LLM代理，专为通过Python代码生成来设计模拟电路。首先，AnalogCoder采用反馈增强流程，并结合定制的领域特定提示，能够自动且自我校正地设计模拟电路，成功率高。其次，它提出了一套电路工具库，用于存储成功的电路设计作为可重用的模块化子电路，简化了复合电路的创建。实验结果显示，AnalogCoder在广泛覆盖模拟电路任务的基准测试上超越了其他基于LLM的方法，成功设计了20个电路，比标准GPT-4o多出5个。我们相信AnalogCoder能显著提升芯片设计过程的效率，让非专家也能高效设计模拟电路。相关的代码和基准已提供在：[https://github.com/anonyanalog/AnalogCoder](https://github.com/anonyanalog/AnalogCoder)。|
|**2024-05-23**|**AGILE: A Novel Framework of LLM Agents**|Peiyuan Feng et.al.|[2405.14751](http://arxiv.org/abs/2405.14751)|**[link](https://github.com/bytarnish/agile)**|我们提出了一种新颖的框架，称为LLM（大型语言模型）代理AGILE（能够与用户互动并从环境中学习的代理），旨在执行复杂的对话任务，利用LLMs、记忆、工具和专家交互。这种代理不仅具备对话能力，还具备反思、工具运用以及咨询专家的功能。我们将构建此类LLM代理视为强化学习问题，其中LLM作为策略模型。我们使用标注的行为数据和PPO算法对LLM进行微调。特别关注的是问答任务，为此我们发布了一个名为ProductQA的数据集，包含在线购物中的难题。我们在ProductQA和MedMCQA上的大量实验表明，基于130亿和70亿参数的LLM训练的AGILE代理能够超越GPT-4代理的表现。我们的 ablation研究强调了记忆、工具、咨询、反思和强化学习在实现优秀性能方面的重要性。|
|**2024-05-23**|**Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View**|Xuan Liu et.al.|[2405.14744](http://arxiv.org/abs/2405.14744)|null|由于大型语言模型（LLMs）在训练数据中反映了人类偏见，它们可能会出现幻觉问题。这种情况下，一个关键问题是：LLMs是否能够利用幻觉来模仿人类的认知偏见，从而展现出非理性但社会性的一面？本文探讨了这一问题，通过结合实用的社会科学实验和理论洞察，提出CogMir，一个开放式多LLM框架，旨在利用LLMs的幻觉特性来评估和提升其社会智能，特别是在认知偏差方面。我们在CogMir子集上的实验结果显示，在不确定情境下，LLMs和人类在非理性及亲社会决策上表现出高度一致性，这表明LLMs作为社会实体的亲社会性，并突显了幻觉特性的关键作用。此外，CogMir框架展示了其作为研究LLMs社会智能的有价值平台的潜力。|
|**2024-05-22**|**HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model**|Mustafa Yildirim et.al.|[2405.13547](http://arxiv.org/abs/2405.13547)|null|## 背景 自动驾驶是一个复杂的任务，它需要先进的决策和控制算法。理解自动驾驶车辆决策的依据对于确保其在高速公路驾驶中的安全与有效性至关重要。本研究提出了一种新颖的方法，称为HighwayLLM，它利用大型语言模型（LLMs）的推理能力来预测ego车辆的未来导航路径点。该方法还采用预训练的强化学习（RL）模型作为高层次规划器，对合适的元级动作进行决策。HighwayLLM将RL模型的输出与当前状态信息相结合，生成安全、无碰撞且可解释的未来状态预测，从而构建出车辆的行驶轨迹。随后，基于PID的控制器引导车辆遵循LLM代理预测的路径点。这种LLM与RL和PID的融合提升了决策过程，并为高速公路自动驾驶提供了可解释性。|
|**2024-05-19**|**Human-Centered LLM-Agent User Interface: A Position Paper**|Daniel Chin et.al.|[2405.13050](http://arxiv.org/abs/2405.13050)|**[link](https://github.com/daniel-chin/flute-x-gpt)**|大型语言模型（LLM）-在-环应用已显示出有效理解用户命令、制定计划并相应地操作外部工具/系统的潜力。然而，LLM代理的操作范围局限于被动响应用户，需要用户根据底层工具/系统来表述需求。我们注意到LLM代理用户界面（LAUI）的潜力远未充分利用。理想的LAUI设想中，用户无需深入了解工具/系统，就能与之交互以探索新兴的工作流程。不同于设计固定的可探索GUI来教授用户使用系统的预设方式，LAUI中的LLM代理从一开始就对系统熟练，主动学习用户及其需求，并向用户提出新的互动方案。为了展示LAUI的概念，我们提供了一个具体例子：Flute X GPT，它结合了LLM代理、提示管理器和一个支持复杂实时体验的笛子教学多媒体软硬件系统，旨在简化学习吹奏笛子的过程。|
|**2024-05-13**|**METAREFLECTION: Learning Instructions for Language Agents using Past Reflections**|Priyanshu Gupta et.al.|[2405.13009](http://arxiv.org/abs/2405.13009)|null|尽管大型语言模型（LLMs）广受欢迎，但为其执行特定任务设计精确的提示仍是一个挑战。用户通常需要与基于LLM的代理进行多轮对话以达成目标。近期研究显示，模型自身的反馈，即自反思，能在对话过程中起到强化作用，有助于更快地达到期望结果。鉴于此，我们提出了一种新颖的方法——METAREFLECTION，它能从训练阶段收集到的个体自反思中学习特定领域的通用提示指令。我们在基础设施即代码（IAC）漏洞检测和问题解答（QA）领域，使用REACT和COT进行了实验。实验结果显示，METAREFLECTION显著优于GPT-4，分别在IAC、COT和REACT中的性能提升分别为16.82%、31.33%和15.42%，这表明METAREFLECTION有潜力提升LLMs的效率，是一种值得探索的策略。|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-18**|**MapCoder: Multi-Agent Code Generation for Competitive Problem Solving**|Md. Ashraful Islam et.al.|[2405.11403](http://arxiv.org/abs/2405.11403)|**[link](https://github.com/md-ashraful-pramanik/mapcoder)**|**本文探讨了代码合成这一复杂任务，它需要深度理解复杂的自然语言问题描述、生成复杂的算法和数据结构代码，并执行全面的单元测试。尽管大型语言模型在自然语言处理方面表现出色，但在代码生成任务中的表现仍有待提升。为此，我们提出了一种新颖的方法，即多代理提示框架MapCoder，它模仿人类开发者编程合成的完整过程，分为四个专门设计的LLM（大语言模型）代理：回忆相关示例、规划、代码生成和调试。  通过在八个具有挑战性的竞赛级问题解决和程序合成基准上进行详尽实验，包括HumanEval（93.9%）、MBPP（83.1%）、APPS（22.0%）、CodeContests（28.5%）和xCodeEval（45.3%）等，MapCoder展现了出色的代码生成能力，实现了多项新的最先进的结果。而且，无论编程语言还是问题难度，我们的方法都表现出持续的优越性能。我们开源了该框架，供研究者参考：https://github.com/Md-Ashraful-Pramanik/MapCoder。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|**[link](https://github.com/activevisionlab/awesome-llm-3d)**|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动嵌入式人工智能（AI）系统在空间认知和交互方面的发展。研究涵盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的集成，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。论文还简要回顾了其他结合三维和语言的方法。本文的元分析揭示了明显的进展，但也强调了开发新方法以充分利用3D-LLMs潜力的必要性。因此，本文旨在为未来的研究方向指明道路，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本综述，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-24**|**DEBATE: Devil's Advocate-Based Assessment and Text Evaluation**|Alex Kim et.al.|[2405.09935](http://arxiv.org/abs/2405.09935)|**[link](https://github.com/gunny97/DEBATE)**|随着自然语言生成（NLG）模型的普及，系统地评估机器生成文本的质量变得日益关键。近期的研究引入了基于大型语言模型（LLM）的无参考评价器，它们展现出处理新任务的能力。然而，这些模型通常采用单代理方法，我们认为这限制了它们的表现。因为LLM代理的回答存在偏见，比如对特定文本结构或内容的偏好。为此，我们在本工作中提出DEBATE，一个建立在多代理评分系统基础上的NLG评价框架，融入了“恶魔辩手”的概念。在该框架中，一个代理被指令批评其他代理的论点，从而可能消解LLM代理答案中的偏见。DEBATE在两个NLG评价元评估基准——SummEval和TopicalChat上显著优于先前的最佳方法。我们还发现，代理之间的辩论广度以及代理的人格特质会影响评价器的性能。|
|**2024-05-05**|**Self-Reflection in LLM Agents: Effects on Problem-Solving Performance**|Matthew Renze et.al.|[2405.06682](http://arxiv.org/abs/2405.06682)|**[link](https://github.com/matthewrenze/self-reflection)**|**在这个研究中，我们探讨了大型语言模型（LLMs）中自我反思对问题解决能力的影响。我们让九种流行的LLMs回答一系列选择题，以建立性能基线。对于回答错误的问题，我们指导八种不同类型的自我反思LLM代理反思其错误，并为自己提供改进问题解决的指导。然后，根据这些指导，每个反思型代理重新尝试回答同样的问题。研究结果显示，LLM代理通过自我反思显著提高了问题解决能力（ $p < 0.001$ ）。此外，我们还比较了各种自我反思方式对性能的单独贡献。所有代码和数据已在GitHub上公开：https://github.com/matthewrenze/self-reflection。**|
|**2024-05-08**|**Air Gap: Protecting Privacy-Conscious Conversational Agents**|Eugene Bagdasaryan et.al.|[2405.05175](http://arxiv.org/abs/2405.05175)|null|随着大型语言模型（LLMs）在对话式代理中的广泛应用，处理敏感用户数据时引发了严重的隐私问题。这些代理虽能理解并处理上下文，但也可能被恶意一方利用。为此，我们提出了一种新的威胁模型，即第三方应用通过操控交互上下文，误导LLM代理泄露与其任务无关的私人信息。在基于上下文完整性框架的基础上，我们开发了AirGapAgent，这是一种注重隐私的代理，旨在通过限制代理仅访问完成特定任务所需的数据，防止意外的数据泄漏。实验使用Gemini、GPT和Mistral模型作为代理，结果显示AirGapAgent在抵御基于单个查询的上下文劫持攻击方面表现出色。例如，对于Gemini Ultra代理，这种攻击从94%的保护能力降低到45%，而AirGapAgent可以保持97%的防护效果，使同样的攻击失效。|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325](http://arxiv.org/abs/2405.04325)|null|近期大型语言模型（LLMs）的进展虽为构建自然语言代理提供了强大基础，但同时也引发了关于它们及其基于它们构建的自主代理的安全性担忧。特别是欺骗能力是一个关键问题，我们关注的是AI代理通过混淆和模棱两可来误导、隐藏真相或推广部分不真实的信念的行为。不同于以往AI安全研究中的撒谎、自私决策或提供虚假信息，我们聚焦于一类特殊的欺骗：类似于魔术师利用障眼法让兔子从帽子里出现，要么通过隐藏的暗门，要么通过转移注意力直接展示。  我们的新实验平台在一个有目标的环境中展示了LLM代理在对抗性对话系统中进行自然语言生成时的欺骗固有能力，该系统基于立法任务“游说”议案。在目标驱动的环境中，我们通过强化学习方法构建欺骗能力，结合语言哲学和认知心理学理论。研究发现，游说代理在对抗互动的后续强化试验中其欺骗能力提高了约40%，并且我们的欺骗检测机制能达到高达92%的识别率。这些结果揭示了人机交互中的潜在问题，即代理可能操纵人类以达成预设目标。|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324](http://arxiv.org/abs/2405.04324)|**[link](https://github.com/ibm-granite/granite-code-models)**|**大语言模型（LLMs）在代码领域的训练正在革新软件开发流程。如今，这些代码LLMs正逐步融入软件开发环境，以提升人类程序员的效率，并展现出自主处理复杂任务的潜力。要充分利用代码LLMs的全部效能，需要其具备生成代码、修复bug、解释和注释代码、维护仓库等多种功能。本文介绍Granite系列的解码器仅有的代码模型，专为代码生成任务而设计，训练数据涵盖116种编程语言。Granite Code模型家族包括从3亿到340亿参数的模型，适用于从复杂应用现代化到设备内存受限的多种应用场景。通过全面任务评估，Granite Code模型在开源代码LLM中的性能始终处于领先水平。该模型家族针对企业软件开发工作流进行了优化，表现出色于各种编码任务（如代码生成、修复与解释），是一款多用途的全能代码模型。我们以Apache 2.0许可协议发布所有Granite Code模型，供研究和商业使用。**|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219](http://arxiv.org/abs/2405.04219)|null|### 概述  大型语言模型驱动的自主代理在软件开发等场景中展现出强大的自主性潜力。然而，当前静态经验范式依赖于通过启发式方法获取的固定历史经验集，这限制了代理的适应性和效率提升。为此，本文提出了迭代经验优化框架，允许语言模型在执行任务过程中动态调整和优化经验。我们定义了两种核心模式：顺序模式，根据任务批次内的最近经验进行改进；累计模式，积累所有先前任务批次的经验。通过引入经验淘汰策略，该方法优先选择高质量和常用的经验，有效地管理经验空间，提高效率。实验结果显示，尽管顺序模式可能带来更好的性能，但累计模式在稳定性方面更优。此外，通过淘汰策略，仅使用高质量经验子集的11.54%，就能实现更好的性能。|
|**2024-05-06**|**Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control**|Yaqub Chaudhary et.al.|[2405.03813](http://arxiv.org/abs/2405.03813)|null|## 翻译  大型语言模型（LLMs）能够模仿各种修辞风格，生成表达广泛情感的文本，这种能力在低成本下迅速普及，带来了潜在的社会危害。本文并未孤立看待这些模型，而是关注它们背后大规模计算基础设施在各领域的应用。我们首先探讨了LLMs如何通过污染和标准化信息环境来影响社会，并指出这些功能可能被用作控制手段。接下来，我们将焦点转向几个新兴研究领域，这些领域增强了LLMs作为权力工具的能力：  1. 通过实时设计对话界面中的选择架构（如“AI角色”），进行说服策略。 2. 利用LLM构建人类行为的计算模型（如“硅质主体”）。 3. 将LLM应用于模拟人类群体行为（如“硅质社会”）。 4. 结合强化学习，创建可控制和导向的战略对话模型。  综合以上几点，我们讨论了如何利用这些技术构建基于LLMs的系统，这些系统通过模拟和伪装的“预测”，成为个体、社会和政治控制的强大工具，操控人类的行为、意图和行动。|
|**2024-05-05**|**Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**|Jinyu Cai et.al.|[2405.02858](http://arxiv.org/abs/2405.02858)|**[link](https://github.com/BlueLinkX/GA-MAS)**|**社交媒体平台如Twitter、Reddit和新浪微博在全球交流中扮演重要角色，但它们在地缘政治敏感区域常常受到严格监管。这促使用户在受限的社交媒体环境中巧妙地调整沟通方式，经常使用编码语言。这种语言模式的变化不仅是为了对抗监管，也是语言演化的生动例证，展示了社会和技术压力下语言如何自然演变。研究受限制社交媒体环境下语言的演变对于保障言论自由、优化内容管理以及推动语言学研究至关重要。本论文提出了一种基于大型语言模型（LLMs）的多代理模拟框架，用于探索在严格监管下的用户语言进化。该框架包含对话监督的LLM驱动代理和参与者代理，它们在互动中发展语言策略，模拟在规避社交媒体规则的环境中交流方式的演变。通过从抽象场景到现实情境的多种情景评估，研究结果显示LLMs能够有效模拟受限环境中的复杂语言动态和交互，随着进化，它们在规避监督和信息准确性方面表现出提升。此外，研究发现LLM代理针对不同的场景采用了不同的策略。**|
|**2024-05-02**|**OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning**|Shihao Wang et.al.|[2405.01533](http://arxiv.org/abs/2405.01533)|**[link](https://github.com/nvlabs/omnidrive)**|**随着大规模多模态语言模型（MLLMs）的进步，人们对于基于这些模型的自动驾驶系统表现出日益增长的兴趣，期望利用它们强大的推理能力。然而，将MLLMs的强项应用于驾驶任务的规划部分是一个挑战，因为规划需要对三维环境有全面的理解，而不仅仅是二维推理。为此，我们的工作提出了一种框架，旨在实现模型与3D驾驶任务的紧密契合。我们首先设计了一个新颖的3D MLLM架构，它利用稀疏查询技术将视觉表示提升并压缩到三维空间，然后将其输入到语言模型中。这种基于查询的表示方式使得我们可以同时编码动态物体和静态地图元素（如道路），为感知和行动的对齐提供一个简化的三维世界模型。  此外，我们还创建了OmniDrive-nuScenes，这是一个新的视觉问答数据集，它通过全面的视觉问答任务（如场景描述、交通规则理解、三维定位、反事实推理、决策制定和规划）来考验模型在复杂三维场景中的真正情境意识。大量的实验结果表明，我们的提出的架构有效，并强调了在复杂三维环境中进行推理和规划时，视觉问答任务的重要性。**|
|**2024-05-02**|**CACTUS: Chemistry Agent Connecting Tool-Usage to Science**|Andrew D. McNaughton et.al.|[2405.00972](http://arxiv.org/abs/2405.00972)|**[link](https://github.com/pnnl/cactus)**|**这篇论文介绍了一种名为CACTUS的大型语言模型，它结合了化学信息学工具，旨在提升在化学和分子发现领域的高级推理与问题解决能力。研究者们使用包括Gemma-7b、Falcon-7b、MPT-7b、Llama2-7b和Mistral-7b在内的多款开源大语言模型，对CACTUS进行了广泛的性能评估，通过数千个化学问题的基准测试。结果显示，CACTUS明显优于基础模型，其中Gemma-7b和Mistral-7b无论采用何种提示策略，表现最为出色。论文还探讨了领域特定提示和硬件配置对模型性能的影响，强调了提示工程的重要性，并指出在消费级硬件上部署较小模型可能不会显著牺牲准确性。  CACTUS通过融合开源大语言模型的认知功能与专业工具，能够协助研究人员进行分子性质预测、相似性搜索和药物适用性评估等任务。作为化学信息学领域的重大突破，CACTUS为化学家和分子探索者提供了一个灵活的工具，有望加速科学研究，推动新型有效、安全药物、催化剂和材料的发现。此外，CACTUS与自动化实验平台的集成以及实时数据驱动决策的能力，为自主发现开辟了新的可能。**|
|**2024-04-29**|**Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs**|Bahar Radmehr et.al.|[2404.18978](http://arxiv.org/abs/2404.18978)|null|随着教育环境中对学习者模型日益增长的兴趣，研究重点逐渐转向如何通过强化学习（RL）与大型语言模型（LLMs）相结合，提升在开放性文本学习环境中的通用能力。本文探讨了三种类型的代理：（1）基于RL的代理，使用自然语言表示状态和行动策略以寻找最佳互动方式；（2）基于LLM的代理，利用模型的广泛知识和推理能力通过提示进行操作；（3）混合LLM辅助RL的代理，旨在提高性能和泛化能力。为了支持这些代理的发展和评估，我们提出了PharmaSimText，这是一个源自PharmaSim虚拟药店环境的新基准，专注于诊断对话实践。实验结果显示，RL基础的代理在任务完成方面表现优秀，但在提问质量上有所欠缺；而LLM基础的代理在提问能力上较强，但任务完成度不高。最后，混合LLM辅助RL的代理展示了克服这些局限性的潜力，证实了RL与LLMs结合用于开发开放性学习环境高表现代理的可能性。|
|**2024-04-27**|**CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments**|Kaixuan Huang et.al.|[2404.18021](http://arxiv.org/abs/2404.18021)|null|随着基因组工程技术的兴起，精确修改遗传信息已成为可能，但高效基因编辑系统的构建需要深入理解CRISPR技术及其复杂实验背景。大型语言模型（LLMs）在诸多任务中展现出潜力，但在生物设计问题上往往缺乏特定知识。本文介绍CRISPR-GPT，一个增强型LLM代理，它结合了领域知识和外部工具，以自动化并提升基于CRISPR的基因编辑实验设计过程。CRISPR-GPT利用LLMs的推理能力，协助选择CRISPR系统、设计引导RNA、推荐细胞递送方法、起草协议以及设计验证实验以确认编辑结果。我们展示了CRISPR-GPT如何帮助非专家研究人员从头开始进行基因编辑实验，并通过实际案例验证其有效性。同时，我们探讨了自动化基因编辑设计的伦理和监管问题，强调了负责任和透明使用此类工具的重要性。我们的工作目标是弥合初级生物研究者与CRISPR基因组工程技术之间的鸿沟，展示LLM代理在促进复杂生物发现任务中的潜力。|
|**2024-04-27**|**Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs**|Zhenlan Ji et.al.|[2404.17833](http://arxiv.org/abs/2404.17833)|null|随着大型语言模型（LLMs）驱动的代理在各种商业应用中，特别是在心理健康支持、化学合成和软件开发等领域展现效用，人们发现这些代理在处理复杂任务和长期规划时容易产生错误。为此，本文提出了一种新颖的自动化方法——PDoctor，旨在检测和理解LLM代理的错误规划。PDoctor首先定义了一个领域特定的语言（DSL），用于用户查询，并借助Z3约束求解器生成各种输入，这些输入是描述一系列任务完成需求的自然语言段落。然后，PDoctor从这些需求中提取约束，形成一个测试基准。我们使用三个主流的代理框架和两个强大的LLMs（GPT-3.5和GPT-4）对PDoctor进行了评估，结果显示它能有效识别代理规划中的各种错误，并为开发者和用户提供了有价值的见解和错误特性。最后，我们讨论了可能的替代设计和扩展PDoctor的方向。|
|**2024-04-26**|**PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**|Qinglin Zhu et.al.|[2404.17662](http://arxiv.org/abs/2404.17662)|**[link](https://github.com/alickzhu/player)**|**随着大型语言模型（LLMs）的最新进展，增强了代理间的通信和社会交互能力。然而，在涉及竞争与合作的动态环境中，利用这些模型进行复杂推理的构建仍然面临挑战，尤其是因为基于信息图的搜索方法存在局限性。为此，我们提出PLAYER*，这是一个基于任意采样式规划器的新框架，它结合了传感器和剪枝技术，构建了一个完全依赖于问题驱动的搜索框架，适用于高难度的推理任务。我们还引入了一种可量化的评估方法，通过多项选择题来测试，并创建了WellPlay数据集，包含1,482个问答对。实验结果表明，PLAYER*在复杂动态环境中的效率和性能优于现有方法，并提供了可量化的对比结果。**|
|**2024-04-24**|**Autonomous LLM-driven research from data to human-verifiable research papers**|Tal Ifargan et.al.|[2404.17605](http://arxiv.org/abs/2404.17605)|**[link](https://github.com/technion-kishony-lab/data-to-paper)**|**随着人工智能推动科学发现的步伐加快，人们还不清楚完全由AI驱动的研究是否可行，以及它能否遵循关键的科学价值观，如透明度、可追溯性和可验证性。为了模拟人类的科学研究实践，我们构建了“数据到论文”（data-to-paper），这是一个自动化平台，引导相互协作的人工智能代理通过完整的分步骤研究流程，同时程序化追踪信息流，并允许人类监督和互动。在自动模式下，仅提供标注数据，该平台就能提出假设，设计研究计划，编写和调试分析代码，生成和解读结果，甚至创建完整且信息可追溯的科研论文。尽管研究新颖性有限，但这一过程展示了AI自主从数据中生成原创定量洞察的能力。对于简单的研究目标，全自动流程能创作出大约80-90%无需重大错误的稿件，然而随着目标复杂性的增加，人类的共同参与对于保证准确性至关重要。此外，生成的论文本身也具有内在的可验证性，因为信息追踪使得结果、方法和数据的链接可以程序化进行。因此，我们的工作表明，AI驱动的科研可以加速科学发现，同时增强而非威胁透明度、可追溯性和可验证性。**|
|**2024-04-11**|**The Future of Scientific Publishing: Automated Article Generation**|Jeremy R. Harper et.al.|[2404.17586](http://arxiv.org/abs/2404.17586)|null|这项研究介绍了一种创新的软件工具，它利用大型语言模型（LLM）提示，实现了从Python代码自动生成学术文章，这对于生物医学信息学和计算机科学领域具有重要意义。选择Python作为基础示例，因其广泛使用和强大的数据分析能力。该方法和框架的灵活性使得其适用于多种GitHub仓库，表明了工具的广泛应用潜力（Harper，2024年）。通过简化传统上耗时的学术写作过程，特别是在整合复杂数据集和代码输出方面，这一突破性进展推动了科研成果的快速传播。开发过程中并未依赖高级语言模型，确保了自动化生成内容的连贯性和完整性。此次探索不仅验证了软件的成功应用和效率，还预示了未来可能集成更先进的LLM，将进一步增强其功能，引领一个科研发现发布更加迅速和易获取的时代。|
|**2024-05-09**|**Large Language Model Agent as a Mechanical Designer**|Yayati Jadhav et.al.|[2404.17525](http://arxiv.org/abs/2404.17525)|null|传统的机械设计方法依赖于专家通过经验引导的修改和有限元分析（FEA）来满足特定需求，但这个过程耗时且高度依赖个人知识。尽管已经开发了许多机器学习模型来简化繁琐的专家驱动迭代过程，但它们通常需要大量训练数据和计算资源。深度学习方法往往局限于其训练领域和任务，限制了跨任务应用。这在自动化效率与资源需求之间形成了权衡。  本研究提出了一种新颖的方法，即将预训练的语言模型（LLMs）与有限元模块结合。有限元模块评估每个设计并提供关键反馈，引导LLMs不断学习、规划、生成和优化设计，无需针对特定领域进行专门训练。我们通过在桁架结构的迭代优化中展示这种框架的有效性，证明它能够根据结构化的反馈和标准调整设计。结果显示，基于LLM的代理成功生成符合自然语言描述的桁架结构设计，成功率高达90%，这取决于所施加的约束条件。通过提示式优化技术，我们展示了LLM代理在接收到解-得分对后，能够根据其内在推理能力迭代优化设计以满足规格要求。  LLM代理能够产生可行的设计并根据其固有的推理能力进行优化，这表明它们有潜力自主发展和实施有效的设计策略。|
|**2024-04-26**|**Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System**|Robin Schmucker et.al.|[2404.17460](http://arxiv.org/abs/2404.17460)|null|本文讨论并评估了一种新型的对话式辅导系统（Conversational Tutoring Systems，CTS），该系统利用大型语言模型（Large Language Models，LLMs）的最新进展。首先，系统通过自动从课程文本中生成易于编辑的教学脚本，实现AI辅助的内容创作。其次，系统通过两个基于LLM的代理（Ruffle和Riley）以学习教学模式运行，分别扮演学生和教授角色，进行自由形式的对话，遵循典型的人工智能辅导系统的内环和外环结构。我们在两个在线用户研究（N=200）中对比了该系统与简单的问答聊天机器人和阅读活动在支持生物学课程的效果。研究分析了系统使用模式、预后测试成绩以及用户体验调查，结果显示用户对Ruffle&Riley的参与度高，理解力强，并认为提供的支持有帮助。尽管Ruffle&Riley用户的完成时间较长，但在短期学习成效上并未发现显著差异，优于阅读活动。我们的系统架构和用户研究为未来CTS设计者提供了有价值的信息。此外，我们开源我们的系统，以促进基于LLM的学习技术有效教学设计的研究。|
|**2024-04-26**|**A Unified Debugging Approach via LLM-Based Multi-Agent Synergy**|Cheryl Lee et.al.|[2404.17153](http://arxiv.org/abs/2404.17153)|**[link](https://github.com/acceptepapier/unidebugger)**|在软件调试这个耗时的过程中，人们一直在努力实现自动化，包括故障定位和修复生成。近年来，大型语言模型（LLMs）在自动化调试方面展现出巨大潜力。然而，我们发现了传统和基于LLM的调试工具面临三大挑战：1）上游的故障定位不准确会波及下游的修复；2）处理复杂逻辑错误的能力不足；3）忽视程序上下文。针对这些问题，我们提出了首个自动化的、统一的调试框架——FixAgent，通过LLM代理协同。FixAgent能执行端到端的故障定位、修复和分析。  我们的关键洞察是，LLMs能够从人类开发者认可的通用软件工程原则中获益，比如“橡皮鸭调试”，这有助于更好地理解程序功能和逻辑错误。为此，我们设计了三个灵感来源于“橡皮鸭”的解决方案：代理专业化与协同、关键变量跟踪和程序上下文理解，促使LLMs提供明确的解释，并聚焦于关键的程序逻辑信息。在广泛使用的QuixBugs数据集上，FixAgent成功修复了80个bug中的79个，其中9个是之前未解决的。它还在CodeFlaws上合理地修复了1.9倍于最佳修复工具的缺陷，而且无需位置信息，采样率低于0.6%。平均而言，与使用不同LLM的基线模型相比，FixAgent提高了约20%的合理修复和正确修复率，显示出我们设计的有效性。  此外，FixAgent的正确率高达97.26%，表明它有可能克服现有方法的过拟合问题。总结来说，FixAgent是一个有前景的自动化调试框架，旨在提升软件调试的效率和准确性。|
|**2024-04-25**|**Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents**|Giorgio Piatti et.al.|[2404.16698](http://arxiv.org/abs/2404.16698)|**[link](https://github.com/giorgiopiatti/govsim)**|在快速发展的人工智能领域，确保大型语言模型（LLMs）的决策安全是一项重大挑战。本文提出了一种名为“Governance of the Commons Simulation”（GovSim）的模拟平台，旨在研究LLMs中的战略互动和合作决策。通过这个环境，我们探讨了AI代理之间资源分享的动态，强调了伦理考量、战略规划和谈判技巧的重要性。GovSim具有灵活性，支持文本型代理，包括LLMs。利用生成式代理框架，我们创建了一个通用代理，便于整合不同的LLMs。我们的研究发现，在GovSim中，只有15个测试模型中的2个能够实现可持续结果，这表明模型在管理共享资源的能力上存在显著差距。进一步的研究显示，如果移除代理之间的通信能力，它们会过度使用共享资源，突出了合作中沟通的关键性。有趣的是，大多数LLMs缺乏普遍化的假设能力，揭示了它们推理技能的一个重要弱点。我们开源了所有研究结果，包括模拟环境、代理提示以及全面的网络界面，以供进一步研究和讨论。|
|**2024-04-24**|**Online Personalizing White-box LLMs Generation with Neural Bandits**|Zekai Chen et.al.|[2404.16115](http://arxiv.org/abs/2404.16115)|null|随着大型语言模型（LLMs）开始生成个性化的文本内容，如何在不为每位用户创建独特模型的资源消耗下实现高效个性化成了新挑战。本文提出了一种创新的在线方法，利用神经_bandit算法动态优化软指令嵌入，根据用户反馈调整内容，从而提升白盒LLMs开放性文本生成的个性化水平。通过在多个任务上的严谨实验，我们证明了这种方法相对于基础策略有显著性能提升。特别是针对个性化新闻标题生成，NeuralTS带来了高达62.9%的最佳ROUGE分数提升以及2.76%的LLM代理评估分数增长，这表明其效果显著。|
|**2024-04-04**|**Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation**|Mohammadmehdi Ataei et.al.|[2404.16045](http://arxiv.org/abs/2404.16045)|null|## 翻译  在产品开发的关键阶段——需求获取，往往难以全面捕捉用户需求，导致最终产品可能无法满足期望。为此，本文提出了一种新颖的框架，它利用大型语言模型（LLMs）来自动化和增强这一过程。通过生成大量模拟用户（LLM代理），我们可以探索更广泛的用户需求和未预见的使用场景。这些代理通过描述他们的行为、观察和挑战，参与产品体验情景。随后的代理访谈和分析揭示了宝贵的用户需求，包括潜在需求。我们通过三个实验验证了我们的框架：首先，我们探讨了不同方法生成多样化的代理，分析其优缺点，并证明了具有上下文意识的代理生成能带来更大的需求多样性。其次，我们展示了该框架如何有效地模拟富有同情心的领先用户访谈，识别出比传统人类访谈更多的潜在需求。最后，我们展示了如何使用LLMs分析访谈，提取需求并将其分类为潜在或非潜在。我们的研究工作强调了利用LLM代理加速早期产品研发、降低成本和促进创新的潜力。|
|**2024-04-24**|**A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples**|Lihang Pan et.al.|[2404.15974](http://arxiv.org/abs/2404.15974)|null|## 翻译  单个大型语言模型（LLM）在解决复杂任务方面的能力有限。然而，通过连接多个LLM代理构建的网络可以显著提升整体性能。本文介绍了一种人机协作工具——EasyLAN，旨在帮助开发者轻松构建LLM代理网络（LAN）。EasyLAN首先根据任务描述自动生成仅包含一个代理的初始网络。接着，它利用少量训练示例来调整网络。对于每个示例，EasyLAN分析输出与真实结果之间的差距，并找出错误的原因。EasyLAN会采用精心设计的策略来修正这些问题。用户可以介入EasyLAN的工作流程或直接修改LAN。最终，LAN从单个代理发展成多代理的网络。实验结果显示，EasyLAN能够帮助开发者快速构建性能良好的LAN。|
|**2024-04-03**|**Concept-Guided LLM Agents for Human-AI Safety Codesign**|Florian Geissler et.al.|[2404.15317](http://arxiv.org/abs/2404.15317)|null|随着生成人工智能在软件工程，特别是安全工程中的重要性提升，对它的质量要求也随之提高。单纯依赖大型语言模型（LLMs）已不足以满足这些需求。因此，我们提出了一种高效且融合的策略，旨在利用LLMs进行安全分析和人机协同设计，以确保软件系统的安全性。我们开发了一个定制化的LLM代理，结合提示工程、启发式推理和检索增强生成，专注于解决与预定义安全概念相关的任务，并与系统模型图进行交互。决策流程通过一系列微决策进行引导，有助于保持结构化信息。此外，我们还提出了图的口头表述作为系统模型的中间表示，以促进LLM与图的交互。我们通过一个简化自动驾驶系统的示例，展示了选择的提示-响应对，以说明我们的方法如何应用于安全分析。|
|**2024-04-23**|**Aligning LLM Agents by Learning Latent Preference from User Edits**|Ge Gao et.al.|[2404.15269](http://arxiv.org/abs/2404.15269)|**[link](https://github.com/gao-g/prelude)**|**我们研究基于用户对语言模型编辑的互动学习语言代理。在诸如写作助手的常见场景中，用户与语言代理交互，根据上下文生成响应，并可能选择性地编辑代理的响应以反映他们的潜在偏好，同时提高准确性。这种编辑反馈是自然产生的，适合用于提升代理与用户偏好的契合度，降低后续用户的编辑成本。为此，我们提出PRELUDE框架，它根据历史编辑数据推断用户的潜在偏好，并据此设计一个提示策略，引导未来的响应生成，避免了昂贵且难以扩展的微调过程，还能保持在其他任务上的性能。  此外，学习描述性的偏好有助于增强可解释性，用户可以查看和调整学习到的偏好。然而，用户偏好可能复杂多变，受情境影响，因此学习起来具有挑战性。为解决这一问题，我们提出CIPHER算法，它利用大型语言模型（LLM）根据用户编辑推断给定情境下的用户偏好。未来，CIPHER会从历史中的k个最接近的上下文中检索推断出的偏好，综合生成响应。我们在总结和电子邮件写作两个互动环境中使用GPT-4模拟用户进行评估，与直接使用用户编辑但不学习描述性偏好的算法，以及学习全局无上下文偏好的算法进行了比较。  在两项任务中，CIPHER都实现了最低的编辑距离成本，并且学习到的偏好与真实偏好显示出显著的相似性。**|
|**2024-04-22**|**A Survey on Self-Evolution of Large Language Models**|Zhengwei Tao et.al.|[2404.14387](http://arxiv.org/abs/2404.14387)|**[link](https://github.com/alibabaresearch/damo-convai)**|**## 概述  大型语言模型（LLMs）在众多领域和智能代理应用中取得了显著进步。然而，依赖人类或外部模型监督的现有LLMs在处理复杂任务和多样性增加时可能会遇到成本高昂和性能瓶颈的问题。为此，自我进化方法应运而生，这种策略允许LLMs自主获取、精炼并从自身生成的经验中学习，借鉴人类经验学习过程，有望推动LLMs向超级智能发展。本文全面综述了LLMs中的自我进化方法。首先，我们提出一个概念框架，将进化过程划分为迭代循环的四个阶段：经验获取、经验细化、更新和评估。其次，我们分类探讨LLMs和基于LLM的代理的进化目标，并对相关文献进行总结，提供每个模块的分类和见解。最后，我们指出了当前的挑战，并提出了未来研究方向，为加速自演进LLMs的发展提供关键洞见。**|
|**2024-04-21**|**A Survey on the Memory Mechanism of Large Language Model based Agents**|Zeyu Zhang et.al.|[2404.13501](http://arxiv.org/abs/2404.13501)|**[link](https://github.com/nuster1128/llm_agent_memory_survey)**|**随着大型语言模型（LLMs）在科研和工业界的广泛关注，基于LLMs的智能代理因其自我进化能力而备受瞩目，这对于解决需要长期复杂交互的现实问题至关重要。支持agent-environment交互的关键要素是代理的记忆机制。尽管已有众多有前景的记忆设计被提出，但这些研究分散在多篇论文中，缺乏全面的综述来系统性地总结和比较，未能提炼出通用且有效的设计模式以启发后续研究。为此，本论文旨在填补这一空白，我们提出一份关于LLM基代理记忆机制的全面调查。首先，我们将探讨记忆在LLM代理中的“是什么”以及“为什么需要”。然后，我们系统回顾了关于记忆模块的设计和评估方法的研究。此外，我们还会展示记忆模块在各种应用中扮演的重要角色。最后，我们会分析现有工作的局限，并指出重要的未来研究方向。为了跟踪该领域最新进展，我们创建了一个GitHub仓库：\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}。**|
|**2024-04-18**|**From Language Models to Practical Self-Improving Computer Agents**|Alex Sheng et.al.|[2404.11964](http://arxiv.org/abs/2404.11964)|null|我们提出了一种简单直接的方法，用于创建能够执行各种计算机任务的人工智能代理，并通过自我改进来发展工具和增强功能，以解决日益复杂的任务。鉴于大型语言模型（LLMs）已显示出从非参数增强中获益，近期的研究大量集中在开发软件，以赋予LLMs各种能力。我们建议，通过适当的提示工程，一个LLM代理可以系统地生成软件来增强自身，而不是依赖人类工程的静态软件开发。  我们通过一些案例研究展示了这一点：仅通过终端访问，我们引导LLM代理添加了检索、互联网搜索、网页导航和文本编辑功能。该代理有效地利用这些工具解决了问题，例如自动化软件开发和基于网络的任务。这种方法表明，通过连续提问和巧妙的提示设计，LLM能够自主扩展其功能，执行实际的计算机任务。|
|**2024-04-25**|**Automated Social Science: Language Models as Scientist and Subjects**|Benjamin S. Manning et.al.|[2404.11794](http://arxiv.org/abs/2404.11794)|null|我们提出了一种方法，利用大型语言模型（LLM）的最新进展，自动构建和测试社会科学假设。这种方法的关键在于使用结构因果模型。结构因果模型提供了一个陈述假设的语言、构建LLM基础代理的蓝图、实验设计以及数据分析计划。拟合后的结构因果模型可供预测或规划后续实验。我们通过几个场景进行了演示：谈判、保释听证会、求职面试和拍卖。在这些情况下，系统既提出了因果关系，也进行了检验，发现了一些证据，而有些则没有。我们证明，从这些社会互动模拟中获取的洞察并非仅通过直接询问LLM就能获得。当给定每个场景的建议结构因果模型时，LLM在预测估计效应的符号方面表现良好，但无法可靠地预测效应的大小。在拍卖实验中，模拟结果与拍卖理论的预测紧密吻合，但LLM直接提取的清算价格预测不准确。然而，如果模型能基于拟合的结构因果模型进行条件化，LLM的预测会大幅改进。简而言之，LLM知道的比它能立即表达的要多。|
|**2024-04-17**|**AgentKit: Flow Engineering with Graphs, not Coding**|Yue Wu et.al.|[2404.11483](http://arxiv.org/abs/2404.11483)|**[link](https://github.com/holmeswww/agentkit)**|**我们提出了一种直观的大型语言模型提示框架（AgentKit），旨在为多功能代理提供统一的方法。AgentKit通过简单的自然语言提示构建复杂的“思维过程”。其基本单元是节点，包含特定子任务的自然语言指令。用户可以像拼接乐高积木一样连接这些节点，从而明确设计出自然结构化的“思考流程”。例如，在撰写论文时，可能的步骤包括：1）确定核心信息，2）识别研究空白等。AgentKit的模块化特性使得高级功能如即兴的层次化规划、反思和从互动中学习变得可能。由于其直观且模拟人类思考过程的设计，即使没有编程经验的人也能创建和调整基础代理。定量实验显示，使用AgentKit设计的代理在WebShop和Crafter任务上实现了最先进的性能。这些成果表明AgentKit有潜力使LLM代理在更广泛的场景下高效且易于使用。相关代码已开源在GitHub：https://github.com/holmeswww/AgentKit。**|
|**2024-04-15**|**Memory Sharing for Large Language Model based Agents**|Hang Gao et.al.|[2404.09982](http://arxiv.org/abs/2404.09982)|**[link](https://github.com/ghupppp/memorysharingllm)**|**在人工智能领域，大型语言模型（LLMs）通过自然语言提示执行任务的能力是一个重大突破，它减少了对固定答案任务（如常识问题和是非查询）的重新训练或微调需求。然而，在处理开放性挑战如诗歌创作时，基于上下文学习的方法显示出局限，主要源于提供的示例全面性以及模型理解问题内容的能力不足，导致输出往往与预期结果大相径庭。针对这一差距，我们的研究提出了Memory-Sharing（MS）框架，这是一种针对LLM多代理的实时记忆存储和检索系统，旨在增强基于上下文的学习过程。每个“记忆”单元记录了提出的查询及其来自LLM代理的即时响应，从多个类似代理中聚合这些记忆，形成所有代理共享的丰富记忆池。MS框架不仅帮助代理找到特定任务的相关示例，还评估其记忆的潜在利用价值，供其他代理未来应用。在三个不同领域的实证验证显示，MS框架显著提高了代理处理开放性问题的表现。此外，我们还讨论了哪种记忆池和检索策略能更好地支持代理，为MS的未来发展提供了方向。代码和数据可在：https://github.com/GHupppp/MemorySharingLLM 获取。**|
|**2024-05-10**|**Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation**|Ruixin Yang et.al.|[2404.09127](http://arxiv.org/abs/2404.09127)|**[link](https://github.com/minnesotanlp/collaborative-calibration)**|**### 背景  当前的大规模语言模型（LLMs）在不确定性估计方面面临挑战，它们通常校准不良且过度自信，特别是在基于人类反馈的强化学习（RLHF）中。人类的决策和信心不仅源于内在信念，还能通过日常观察进行调整，而现有LLM的校准方法主要关注单个模型的信心估计，未能充分利用“集体智慧”：多个LLM之间的协作表达能力，这可以集体提高准确性和校准。本研究中，我们提出了一种无训练后处理的校准策略——协作校准（Collaborative Calibration），它利用多代理工具增强的LLMs在模拟的群体讨论过程中，共同提升校准能力和推理合理性。  ### 任务  我们在生成式问答任务上展示了协作校准的有效性，覆盖了多个领域，证明了它在整合集体校准后的信心评估和提升模型预测可靠性方面的潜力。**|
|**2024-04-13**|**CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting**|Zukang Yang et.al.|[2404.09077](http://arxiv.org/abs/2404.09077)|**[link](https://github.com/zukangy/kgp-curiousllm)**|**在问答（QA）领域，大型语言模型（LLMs）与外部数据库的融合取得了显著成效。然而，这些方法在处理复杂推理任务时往往力有不逮。为此，我们对一种名为知识图谱提示（KGP）的创新方法进行了优化，该方法结合知识图谱和基于LLM的代理以提升推理和搜索精度。然而，原始的KGP框架需要昂贵的大规模数据微调，并且仍存在LLM的错误推断问题。因此，我们提出了一种融入推理能力的LLM代理，它模仿人类的好奇心，通过提问来更有效地导航搜索过程。这个简单的改进显著提高了LLM在QA任务中的性能，同时避免了初始KGP框架的高成本和延迟。我们的目标是进一步发展这种方法，最终实现更精确、更快捷且成本效益更高的QA解决方案。**|
|**2024-04-13**|**Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation**|Jia Gu et.al.|[2404.09043](http://arxiv.org/abs/2404.09043)|null|随着大型语言模型（LLMs）的飞速发展及其在处理复杂语言任务中的出色表现，越来越多的研究尝试利用LLMs模拟人类的行为决策过程，通常这些过程被表示为马尔可夫决策过程（MDPs）。在这个框架中，动作遵循特定的概率分布，并需要迭代采样。这促使我们探究LLM代理理解概率分布的能力，以通过概率采样指导行为决策并生成行为序列。我们将问题分为两个主要方面：一是已知精确概率分布的模拟，二是模糊概率分布的序列生成。  在已知概率分布的情况下，代理需要根据问题描述提供概率分布的类型和参数，然后给出采样序列。然而，我们的研究显示，LLM代理在这方面的性能不佳，但通过编程工具可以一定程度上提高采样成功率。而在实际情境中，概率分布往往不明确。因此，我们在第二部分让代理调整在线社交网络中的活跃度，并分析行动频率。结果表明，即使借助编程工具，LLM代理依然无法有效地采样概率分布。这意味着在直接将LLM作为模拟人类行为的代理应用之前，还需要谨慎对待。|
|**2024-04-12**|**Strategic Interactions between Large Language Models-based Agents in Beauty Contests**|Siting Lu et.al.|[2404.08492](http://arxiv.org/abs/2404.08492)|null|随着大型语言模型（LLMs）的广泛应用，它们在博弈论框架下的游戏行为理解潜力日益显现。本研究聚焦于通过模拟分析不同类型LLM驱动的代理在经典 Beauty Contest 游戏中的策略互动。借鉴人类实验，我们对LLM代理的策略层次进行类似的评估，发现它们展现出从零级到一级的不同程度推理能力，并在重复游戏中表现出行动趋同。此外，我还探讨了不同类型的代理群体构成如何影响战略行为：高比例的固定策略对手能促进LLM代理的收敛，而混合环境中不同相对策略水平的代理共存会加速所有代理的收敛。更智能的代理可能获得更高的平均收益，但这是以较低智能代理的牺牲为代价的。这些结果不仅揭示了在特定情景下模拟代理的结局，还为理解算法之间的战略互动提供了重要启示。|
|**2024-04-17**|**LLM Agents can Autonomously Exploit One-day Vulnerabilities**|Richard Fang et.al.|[2404.08144](http://arxiv.org/abs/2404.08144)|null|随着大语言模型（LLMs）的威力日益增强，其在良性和恶意用途上的应用也日益广泛。研究人员开始关注它们利用网络安全漏洞的能力。近期的研究探讨了LLMs自主破解网站的可能性，但这些研究主要集中在简单的漏洞上。本工作揭示，LLMs能够自主利用现实世界系统中的单日漏洞。我们收集了一组包含15个被CVE描述为“关键严重性”的一天期漏洞数据。当提供CVE描述时，GPT-4模型能成功利用87%的漏洞，相比之下，其他测试模型（如GPT-3.5、开源LLMs和开源漏洞扫描器ZAP和Metasploit）的表现均为0%。然而，我们的GPT-4模型在没有描述的情况下效率大减，仅能利用7%的漏洞。这些发现对大规模部署高能力LLMs提出了质疑。|
|**2024-04-11**|**WESE: Weak Exploration to Strong Exploitation for LLM Agents**|Xu Huang et.al.|[2404.07456](http://arxiv.org/abs/2404.07456)|null|近期，大型语言模型（LLMs）显示出作为智能代理的强大潜力。然而，现有的研究主要集中在通过精心设计的提示工程或任务特定的微调来提升模型的推理或决策能力，忽视了探索与利用的过程。在处理开放世界交互环境中的复杂任务时，这些方法存在局限性。首先，由于缺乏对环境的全局信息，模型倾向于做出贪婪决策，导致解决方案不理想。另一方面，从环境中获取的无关信息不仅引入噪声，还增加了额外的成本。  为此，本文提出了一种新颖的方法——弱探索强化强利用（Weak Exploration to Strong Exploitation，WESE），旨在增强LLM在解决开放世界交互任务中的表现。具体来说，WESE将探索和利用过程解耦，使用成本效益高的“弱”代理执行探索任务，以获取全局知识。随后，我们引入基于知识图谱的策略来存储这些知识，并提取与任务相关的关键信息，从而提升“强”代理在成功率和效率上的性能。我们的方法适用于各种任务，并在四个互动基准测试中显著提高了成功率和效率。|
|**2024-04-10**|**GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications**|Shishir G. Patil et.al.|[2404.06921](http://arxiv.org/abs/2404.06921)|**[link](https://github.com/ShishirPatil/gorilla)**|**随着大型语言模型（LLMs）的发展，它们不再仅仅是对话系统中的信息提供者，而是开始积极参与到与实际应用和服务的互动中。如今，人类在将LLM生成的输出（如代码、函数或操作）投入现实世界执行前，需要验证其正确性和适用性，这带来了挑战，因为代码理解被广泛认为非常困难。本文研究了人类如何能有效与LLMs协作、委派和监督，特别是在未来。我们主张，在许多情况下，对提出的行动进行“事后验证”（在看到输出后确认其正确性）比之前的“事前验证”更为容易。实现这一目标的核心理念是集成直观的撤销功能，并为LLM生成的动作设定损害约束，作为降低相关风险的有效策略。通过这种方式，人类可以撤销LLM输出的影响，或者确信潜在风险是有限的。我们认为这对于实现LLMs与应用和服务在有限的人类监督下交互至关重要。我们描述了开源运行时Gorilla Execution Engine（GoEX）的设计和实现，该运行时用于执行LLM动作，并提出了一些开放的研究问题，旨在推动LLMs与应用之间以最小的人工干预进行交互。GoEX的源代码已发布在https://github.com/ShishirPatil/gorilla/。**|
|**2024-04-09**|**AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents**|Luca Gioacchini et.al.|[2404.06411](http://arxiv.org/abs/2404.06411)|**[link](https://github.com/nec-research/agentquest)**|**随着大型语言模型（LLMs）的进展，人们追求能够解决复杂、多步骤推理任务的LLM代理。然而，现有的基准往往局限且只关注整体任务成功率。为了解决这些问题，我们提出了AgentQuest框架，它具有以下特点：（i）benchmark和评估指标模块化且易于扩展，通过文档齐全、易用的API；（ii）我们提供了两种新的评估指标，能够在解决任务时可靠地追踪LLM代理的进步。我们通过两个示例展示了这些指标的实用性，通过识别常见失败点并优化代理架构，显著提高了性能。我们希望与研究界共同扩展AgentQuest，并已将其开源在https://github.com/nec-research/agentquest。**|
|**2024-04-15**|**AutoCodeRover: Autonomous Program Improvement**|Yuntong Zhang et.al.|[2404.05427](http://arxiv.org/abs/2404.05427)|**[link](https://github.com/nus-apr/auto-code-rover)**|**在过去几十年里，研究人员在自动化软件开发过程中取得了显著进展，尤其是大型语言模型（LLMs）的应用极大地推动了编程辅助的自动化。然而，软件工程并不仅仅是编码，还包括维护（如修复bug）和演化（如添加功能）等程序改进过程。本文提出了一种自动解决GitHub问题的方法，旨在实现程序自主改进。我们的方法称为AutoCodeRover，它结合了LLMs与高级代码搜索能力，最终生成程序修改或补丁。与AI研究者和从业者近期关注的仅文件级别的软件项目不同，我们的工作侧重于程序表示（抽象语法树），利用类/方法的程序结构来增强LLM对问题根本原因的理解，并通过迭代搜索提供上下文。当测试套件可用时，谱系基线故障定位技术进一步精确了上下文。  在SWE-bench-lite，一个包含300个真实GitHub问题的数据集上，AutoCodeRover的解决方案效果提升，解决了约22-23%的问题。对于全量的SWE-bench，包含2294个GitHub问题，AutoCodeRover解决了大约16%的问题，这比最近报道的来自Cognition Labs的AI软件工程师Devin的表现还要高，而且时间消耗与Devin相当。我们相信，我们的工作流程能够推动自主软件工程的发展，未来LLM自动生成的代码可以被自动地进行优化和改进。**|
|**2024-04-08**|**Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models**|Yutao Ouyang et.al.|[2404.05291](http://arxiv.org/abs/2404.05291)|null|我们提出了一种基于大型语言模型（LLM）的系统，旨在提升四足机器人的问题解决能力，使其能够处理超越短期动作的长期任务。对于四足机器人来说，长期任务极具挑战性，因为它们需要对任务的语义有高层理解，并具备广泛的运动和操纵技能以与环境互动。我们的系统构建了一个高层推理层，利用大型语言模型，从任务描述中生成混合离散-连续的计划，作为机器人代码。它包括多个LLM代理：一个用于构思计划的语义规划器、一个参数计算器，用于预测计划中的参数，以及一个代码生成器，将计划转换为可执行的机器人代码。  在低层次，我们采用强化学习来训练一套运动规划和控制技能，以增强四足机器人的灵活性，使其能进行丰富环境交互。我们在难以用单一技能完成的长期任务上测试了我们的系统。模拟实验和真实世界实验表明，它成功地制定了多步骤策略，并展现出非平凡的行为，例如制作工具或向人类寻求帮助。|
|**2024-04-06**|**Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology**|Dyke Ferber et.al.|[2404.04667](http://arxiv.org/abs/2404.04667)|null|多模态人工智能系统有望通过解析各类医学数据提升临床决策。然而，这些模型在各医学领域的效能尚不明朗，每个领域都有其独特挑战。本文提出了一种利用大型语言模型（LLMs）作为核心推理引擎的新型多模态医疗AI方法。此引擎自主协调并部署一系列专门的医疗AI工具，如文本解读、放射学和病理图像分析、基因数据处理、网络搜索以及医疗指南文档检索。我们在一系列临床肿瘤学场景中验证了该系统，这些场景模拟了典型的患者护理流程。结果显示，系统在选择恰当工具（97%）、得出正确结论（93.6%）、提供完整（94%）和有益（89.2%）治疗建议，以及根据指令引用相关文献（82.5%）方面表现出高能力。这表明LLMs能够有效地规划和执行领域特定模型，以获取或合成新信息，从而充当个性化临床助手。此外，这种架构简化了监管合规性，因为每个组件工具可以单独验证和审批。我们相信，这项工作为医疗领域的更先进LLM代理提供了概念验证。|
|**2024-04-05**|**Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents**|Harsh Kohli et.al.|[2404.04237](http://arxiv.org/abs/2404.04237)|null|大型语言模型（LLMs）的快速进步使其在标准基准测试中频频超越人类表现，推动了众多下游应用的发展，如基于LLMs的代理。然而，这些模型在看似简单的任务中意外地表现不佳，这强调了对更全面和多样化的评估框架的需求，以衡量它们的实际能力。为此，我们聚焦于组合性和条件推理——人类认知的基石，并提出GroundCocoa，这是一个与航班预订这一现实问题相连接的词汇丰富的基准。我们的任务是将用户的详细偏好与以多选形式提供的可用航班选项进行匹配。结果显示，包括最先进的GPT-4 Turbo在内的当前最佳模型，在经过高级提示后，准确率仍不超过67%，显示出显著的性能差距。|
|**2024-04-02**|**Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization**|Yoichi Ishibashi et.al.|[2404.02183](http://arxiv.org/abs/2404.02183)|**[link](https://github.com/tsukushiai/self-organized-agent)**|**## 背景  随着大型语言模型（LLM）代理的最新进展，自动化软件开发的未来正逐渐显现。然而，现有的单代理方法在生成和优化大规模、复杂的代码库时面临上下文长度限制的问题。为解决这一挑战，我们提出了一种新颖的多代理框架——自组织多Agent体系（SoA）。SoA是一个可扩展且高效的多代理系统，它允许独立地生成和修改代码组件，并协同构建整个代码库。SoA的一个关键特性是根据问题复杂性自动增加代理，实现动态可扩展性。这样，整体代码量可以根据代理数量无限增长，而每个代理管理的代码量保持恒定。  我们在HumanEval基准上评估了SoA，并发现与单代理系统相比，SoA中的每个代理处理的代码量明显减少，但总体生成的代码量显著增加。此外，SoA在Pass@1准确率方面比强大的单代理基线提高了5%。**|
|**2024-04-02**|**Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game**|Silin Du et.al.|[2404.01602](http://arxiv.org/abs/2404.01602)|**[link](https://github.com/doslim/evaluate-the-opinion-leadership-of-llms)**|**大型语言模型在社交推理游戏中展现出显著的策略行为，但对它们作为意见领袖的重要性关注不足，这对于多Agent和人机交互场景的实际应用至关重要。意见领袖是指在一个社会群体中对他人信念和行为有显著影响的个体。本研究使用“狼人杀”游戏作为模拟平台，探讨语言模型在扮演Sheriff（治安官）角色时的意见领导能力。Sheriff负责总结论点并提出决策建议，因此它代表了意见领袖的一个可信代理。我们构建了一个整合Sheriff角色的框架，并基于意见领袖的关键特性提出了两个评估指标：第一个衡量意见领袖的可靠性，第二个考察其对其他玩家决策的影响。  我们进行了大量实验，评估不同规模的语言模型，并创建了“狼人杀”问题回答数据集（WWQA），以测试和提升模型对游戏规则的理解。此外，还包含了人类参与者进行进一步分析。研究结果表明，“狼人杀”游戏是一个有效评估语言模型意见领导力的试验场，但目前仅有少数语言模型具备这种能力。**|
|**2024-04-15**|**CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs**|Jingzhe Shi et.al.|[2404.01343](http://arxiv.org/abs/2404.01343)|**[link](https://github.com/jingzheshi/chops)**|**随着企业和软件平台越来越多地采用大型语言模型（如GPT-3.5、GPT-4、GLM-3和LLaMa-2）提供聊天辅助或客户服务推理，现有的基于LLM的客户服务模型在与客户资料集成和执行实际操作方面存在局限。它们倾向于强调多样性而非精确性和错误避免，这对于现实世界的客户服务场景并不理想。因此，我们提出了一种名为CHOPS（结合客户资料的聊天助手）的LLM代理，旨在：（1）高效利用现有数据库或系统查询用户信息，或遵循既定指南与系统交互；（2）提供准确合理的响应并执行系统内的必要操作，同时避免有害操作；（3）通过结合小型和大型LLM以实现性能满意且成本合理的推理。  我们开发了一个实用的数据集，称为CPHOS-dataset，它包括一个数据库、指导文件以及来自CPHOS平台的模拟物理奥林匹克组织服务的问答对。CPHOS是一个面向高中教师和学生的在线平台。我们通过使用CPHOS-dataset进行了广泛的实验，验证了CHOPS架构的性能，目标是展示LLM如何提升或替代人工客户服务。关于我们的提案架构和数据集的代码可在此处获取：<https://github.com/JingzheShi/CHOPS>。**|
|**2024-03-31**|**DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model**|Lirui Zhao et.al.|[2404.01342](http://arxiv.org/abs/2404.01342)|**[link](https://github.com/opengvlab/diffagent)**|**文本到图像（T2I）生成模型近年来备受瞩目，在学术研究和实际应用中大放异彩。例如，Civitai平台，一个T2I创新的聚集地，目前汇集了74,492种独特的模型，这带来了选择最合适的模型和参数的艰巨任务，通常需要多次试验。借鉴大型语言模型（LLMs）工具使用研究的思路，我们推出了DiffAgent，这是一个通过API调用来快速筛选准确选项的LLM代理。DiffAgent采用了一种新颖的两阶段训练框架，称为SFTA，使其能够根据人类偏好精确地将T2I API的响应与用户输入对齐。为了训练和评估DiffAgent的能力，我们构建了DABench，这是一个全面的数据库，涵盖了社区中的各种T2I API。实验结果显示，DiffAgent不仅在选择适当的T2I API方面表现出色，还验证了SFTA训练框架的有效性。相关代码已可在https://github.com/OpenGVLab/DiffAgent获取。**|
|**2024-03-31**|**Algorithmic Collusion by Large Language Models**|Sara Fish et.al.|[2404.00806](http://arxiv.org/abs/2404.00806)|null|随着算法定价的兴起，人们担忧算法间的合谋问题。我们通过实验使用基于大型语言模型（LLMs）的定价代理，特别是GPT-4，进行了探究。研究发现：(1) LLM驱动的定价机制在定价任务上表现出色；(2) 在寡头竞争环境中，LLM定价代理会自发地进行合谋，从而损害消费者利益；(3) 对LLM指令（“提示”）看似微小的变化可能加剧这种合作行为。这些结果同样适用于拍卖场景。我们的研究结果强调了对算法定价进行反垄断监管的必要性，并揭示了针对LLM定价代理特有的监管挑战。|
|**2024-03-31**|**"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**|Yuki Hou et.al.|[2404.00573](http://arxiv.org/abs/2404.00573)|**[link](https://github.com/tamoharu/Agent-Memory-CHI24)**|在这个研究中，我们提出了一种创新的人类记忆架构，旨在提升基于大型语言模型的对话代理的认知能力。我们的设计使得这些代理能自主检索生成响应所需的必要记忆，从而解决LLMs在时间认知上的局限。我们借鉴了人类的记忆线索召回机制作为触发点，以实现精确且高效的回忆。此外，我们开发了一个数学模型，动态量化记忆巩固过程，考虑了诸如上下文相关性、时间流逝和回忆频率等因素。代理会从用户的交互历史中存储记忆，这些记忆被封装在数据库中，每个记忆都包含了内容和时间关联的语境。这样，通过类似人类识别和回忆过往经历的方式，系统能够战略性地存储记忆，并理解它们对用户在时间线上的重要性。|

<p align=right>(<a href=#updated-on-20241121>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-20**|**SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs**|Shirley Kokane et.al.|[2411.13547](http://arxiv.org/abs/2411.13547)|null|评估大型语言模型（LLMs）的输出是构建高性能复合AI系统的关键环节。由于LLMs的输出会传播到下游步骤，识别LLM错误对于系统性能至关重要。在AI系统中，LLMs常见的任务之一是工具使用。虽然已经有一些基准环境用于评估LLMs在这类任务上的表现，但它们通常只提供成功率而没有解释失败案例的具体原因。为了解决这个问题，我们引入了SpecTool，一个新的基准测试来识别LLM输出在工具使用任务中的错误模式。我们的数据集包括来自多样化环境的查询，可以用来检测七种新识别的错误模式的存在。通过使用SpecTool，我们展示了即使是当前最突出的LLMs也会在其输出中表现出这些错误模式。研究人员可以利用SpecTool的分析和见解来指导他们的错误缓解策略。|
|**2024-11-20**|**BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games**|Davide Paglieri et.al.|[2411.13543](http://arxiv.org/abs/2411.13543)|null|大型语言模型（LLMs）和视觉语言模型（VLMs）拥有广泛的知识并展现出有前景的推理能力；然而，它们在复杂动态环境中的表现仍然不尽如人意。现实世界任务需要处理复杂的交互、高级的空间推理、长期规划以及持续探索新策略——这些领域缺乏有效的评估方法来全面评估这些能力。为了解决这一问题，我们引入了BALROG，这是一个新颖的基准测试，旨在通过一系列具有挑战性的游戏来评估LLMs和VLMs的主动能力。我们的基准测试结合了一系列具有不同难度级别的现有强化学习环境，包括一些非专家人类可以在几秒钟内解决的任务，以及一些可能需要数年时间才能掌握的极其困难的任务（例如，NetHack学习环境）。我们设计了细粒度的指标来衡量性能，并对几种流行的开源和闭源LLMs和VLMs进行了广泛的评估。我们的研究结果表明，尽管当前的模型在较简单的游戏中部分成功，但它们在更具有挑战性的任务上表现出显著的不足。值得注意的是，我们在基于视觉的决策制定方面观察到严重的缺陷，因为当提供环境的视觉表示时，模型的表现变得更差。我们将BALROG作为开放且用户友好的基准测试发布，以促进未来在主动社区的研究和发展。|
|**2024-11-20**|**Metacognition for Unknown Situations and Environments (MUSE)**|Rodolfo Valiente et.al.|[2411.13537](http://arxiv.org/abs/2411.13537)|null|元认知——对自己认知过程的意识和调控——对于人类在未知情境中的适应性至关重要。相比之下，当前的自主代理在新环境中常常难以应对，因为它们的适应能力有限。我们假设，元认知是适应性自主系统中一个至关重要的缺失要素，它赋予这些系统解决陌生挑战所需的认知灵活性。鉴于元认知能力的广泛范围，我们将重点放在两个关键方面：能力意识和策略选择以应对新任务。为此，我们提出了“未知情境与环境的元认知”（MUSE）框架，该框架将元认知过程——特别是自我意识和自我调节——整合到自主代理中。我们介绍了MUSE框架的两种初始实现方式：一种基于世界建模，另一种利用大型语言模型（LLM），这两种方式都实现了元认知循环。我们的系统持续学习评估其在一个给定任务上的能力，并利用这种自我意识来指导策略选择的迭代周期。与基于Dreamer-v3的强化学习方法和纯粹基于提示的LLM代理方法相比，MUSE代理在自我意识和自我调节方面表现出显著改善，使它们能够更有效地解决新颖的、分布外的任务。这项工作突显了受认知和神经系统的启发的方法在使自主系统适应新环境方面的潜力，克服了目前依赖大量训练数据的方法的局限性。|
|**2024-11-20**|**Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models**|Chanseo Lee et.al.|[2411.13518](http://arxiv.org/abs/2411.13518)|null|不断增加的多语言需求在医疗保健领域凸显了对能够处理多种语言的AI模型的需求，特别是在临床文档和决策方面。阿拉伯语因其复杂的形态、句法和双语现象，在医学背景下的自然语言处理（NLP）中提出了独特的挑战。本案例研究评估了专为阿拉伯语临床文档设计的Sporo AraSum语言模型，与领先的阿拉伯语NLP模型JAIS进行对比。使用合成数据集，并修改了我们自己版本的PDQI-9指标以评估模型在不同语言环境中的表现。研究评估了模型在总结患者与医生互动方面的表现，重点关注准确性、全面性、临床实用性和语言文化适应能力。结果显示，Sporo AraSum在AI主导的定量指标和我们修改后的PDQI-9所有定性属性上显著优于JAIS。AraSum的架构实现了精确且具有文化敏感性的文档记录，解决了阿拉伯语的语言细微差别，同时减少了AI幻觉的风险。这些发现表明，Sporo AraSum更适合满足阿拉伯语医疗环境的需求，为多语言临床工作流程提供了变革性解决方案。未来的研究应结合真实世界的数据，进一步验证这些发现并探索更广泛的系统整合。|
|**2024-11-20**|**Disentangling Memory and Reasoning Ability in Large Language Models**|Mingyu Jin et.al.|[2411.13504](http://arxiv.org/abs/2411.13504)|**[link](https://github.com/mingyuj666/disentangling-memory-and-reasoning)**|**大型语言模型（LLMs）在处理需要广泛知识和推理能力的复杂任务时表现出色。然而，现有的LLM推理管道作为一个不透明的过程运行，没有明确区分知识检索和推理步骤，这使得模型的决策过程不清晰且杂乱无章。这种模糊性可能导致诸如幻觉和知识遗忘等问题，这些问题显著影响了LLMs在高风险领域的可靠性。在本文中，我们提出了一种新的推理范式，将复杂的推理过程分解为两个明确且清晰的动作：(1)记忆回忆：检索相关知识；(2)推理：基于检索到的知识进行逻辑步骤。为了促进这种分解，我们引入了两个特殊标记（token），即memory和reason，引导模型区分需要知识检索和涉及推理的步骤。我们的实验结果表明，这种分解不仅提高了模型性能，还增强了推理过程的可解释性，使用户能够识别错误来源并有效改进模型响应。代码可在https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning获取。**|
|**2024-11-20**|**Utilizing Large Language Models to Synthesize Product Desirability Datasets**|John D. Hastings et.al.|[2411.13485](http://arxiv.org/abs/2411.13485)|null|本研究探讨了大型语言模型（LLMs）在生成合成数据集以用于产品可取性工具包（PDT）测试中的应用，这是评估用户情感和产品体验的关键组成部分。使用gpt-4o-mini，一种成本效益较高的商业LLM替代方案，采用了三种方法：Word+Review、Review+Word和Supply-Word，各自生成了1000条产品评论的合成数据集。生成的数据集从情感一致性、文本多样性以及数据生成成本方面进行了评估。结果显示，所有方法在情感一致性方面表现良好，皮尔逊相关系数在0.93至0.97之间。Supply-Word在多样性及对PDT术语的覆盖率上表现最佳，但其生成成本也较高。尽管存在轻微的积极情感偏向，在测试数据有限的情况下，LLM生成的合成数据具有显著优势，包括可扩展性、成本节约以及数据集生成的灵活性。|
|**2024-11-20**|**PatentEdits: Framing Patent Novelty as Textual Entailment**|Ryan Lee et.al.|[2411.13477](http://arxiv.org/abs/2411.13477)|null|专利必须被认为具有新颖性和非显而易见性才能获得美国专利商标局（USPTO）的批准。如果不符合这些条件，美国专利审查员会引用先前的技术或现有技术来质疑其新颖性，并发出非最终驳回通知。预测在现有技术下应如何修改发明的权利要求是确保发明权利的关键步骤，但这一任务之前并未被研究为一个可学习的任务。在这项工作中，我们引入了PatentEdits数据集，该数据集包含了105000个成功的修订示例，这些修订克服了对新颖性的异议。我们设计了算法逐句标注这些修订，然后探讨了大型语言模型（LLM）能否有效地预测这些修订。我们证明，在引用参考文献和草稿句子之间评估文本蕴含特别有效，这有助于预测哪些发明性权利要求保持不变或与现有技术相比具有新颖性。|
|**2024-11-20**|**When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training**|Haonan Wang et.al.|[2411.13476](http://arxiv.org/abs/2411.13476)|**[link](https://github.com/haonan3/anchorcontext)**|**扩展上下文窗口大小使得大型语言模型（LLMs）能够处理更长的序列并应对更复杂的任务。旋转位置嵌入（RoPE）已成为事实上的标准，因其相对位置编码特性在长上下文训练中受益。然而，我们观察到使用RoPE与BFloat16格式时会出现数值问题，导致其偏离预期的相对位置编码，特别是在长上下文场景中。这一问题源于BFloat16的有限精度，并且随着上下文长度增加而累积，第一个token对这个问题有显著贡献。为了解决这个问题，我们开发了AnchorAttention，这是一种即插即用的注意力方法，可以缓解由BFloat16引起的数值问题，提高长上下文能力，并加快训练速度。AnchorAttention通过将第一个token视为共享锚点，具有恒定的位置ID，使其对所有文档可见，从而减少了不必要的注意力计算，保持语义连贯性，并通过这种方式提高了计算效率。在三种类型的LLMs上进行的实验表明，AnchorAttention显著提高了长上下文性能，并将训练时间比标准全注意力机制减少超过50%，同时保留了原始LLM在一般任务上的能力。我们的代码可以在https://github.com/haonan3/AnchorContext获取。**|
|**2024-11-20**|**SoK: A Systems Perspective on Compound AI Threats and Countermeasures**|Sarbartha Banerjee et.al.|[2411.13459](http://arxiv.org/abs/2411.13459)|null|大型语言模型（LLMs）在企业中广泛使用，并且通常采用专有的模型，在处理敏感输入和数据时面临极大的挑战。先前的研究已经确定了各种针对训练和推理过程中使用的软件和硬件组件的攻击向量，这使得执行保密性和完整性策略变得极其困难。随着我们逐步构建集成多个大型语言模型（LLMs）的复合AI推理管道，攻击面显著扩大。攻击者现在不仅关注AI算法，还关注这些系统相关的软件和硬件组件。尽管当前研究通常单独考察这些元素，但我们发现结合跨层攻击观察可以减少对威胁模型假设的需求，从而实现强大的端到端攻击。鉴于每层现有的大量攻击，我们需要对不同层次的攻击向量有一个全面和系统化的理解。  本文综述了适用于复合AI系统的不同软件和硬件攻击，并展示了如何通过结合多种攻击机制来减少对单一攻击所需的威胁模型假设。接下来，我们将机器学习攻击按照Mitre Att&ck框架进行系统化分类，以便更好地根据威胁模型定位每个攻击。最后，我们概述了现有软件和硬件层的对策，并讨论了制定综合防御策略的必要性，以确保复合AI系统的安全和高性能部署。|
|**2024-11-20**|**AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations**|Gaurav Verma et.al.|[2411.13451](http://arxiv.org/abs/2411.13451)|null|最先进的多模态网络代理，由多模态大型语言模型（MLLMs）驱动，能够通过处理用户指令和与图形用户界面（GUIs）交互来自主执行许多网络任务。目前构建网络代理的策略依赖于底层MLLMs的泛化能力和通过提示进行引导的能力，以及在与网络相关的任务上对MLLMs进行大规模微调。然而，网络代理仍然难以在未见过的网站和领域上自动化任务，这限制了它们在企业特定和专有平台上的适用性。除了从大规模预训练和微调中获得的泛化能力外，我们提出了一种使用人类演示来构建少量适应性的代理的方法。我们引入了AdaptAgent框架，该框架使专有和开放权重的多模态网络代理能够在使用少量人类演示（最多2个）的情况下适应新的网站和领域。我们的实验基于两个流行基准——Mind2Web和VisualWebArena——表明，使用上下文演示（对于专有模型）或元适应演示（对于元学习的开放权重模型）可以将任务成功率提高3.36%到7.21%，相当于相对增加了21.03%到65.75%。此外，我们进一步分析显示：(a) 多模态演示比纯文本演示更有效；(b) 揭示了元学习期间不同数据选择策略对代理泛化能力的影响；(c) 证明了少量演示数量对网络代理成功率的影响。总体而言，我们的研究结果解锁了一个补充维度，用于开发广泛适用的多模态网络代理，而不仅仅是大规模预训练和微调，强调了少量适应性的重要性。|
|**2024-11-19**|**ACING: Actor-Critic for Instruction Learning in Black-Box Large Language Models**|Salma Kharrat et.al.|[2411.12736](http://arxiv.org/abs/2411.12736)|**[link](https://github.com/salmakh1/ACING)**|**大型语言模型（LLMs）在解决任务时的有效性很大程度上取决于指令的质量，这通常需要通过大量的人工努力进行微调。这突显了自动化指令优化的需求；然而，在处理黑盒LLMs时，这种优化尤其具有挑战性，因为模型参数和梯度无法访问。我们提出了ACING，这是一种针对特定任务的提示优化方法，将其构架为一个无状态的连续动作强化学习（RL）问题，即连续乐队设置。ACING利用基于演员-评论家的方法来优化提示，从不可微的奖励信号中学习。我们通过在30个基于指令的任务上优化ChatGPT的提示来验证ACING的有效性。结果显示，ACING始终优于基线方法，实现了中位数10个百分点的提升。此外，ACING不仅恢复了而且超越了人工编写的专家指令，相对于人类基准，最高可提高39个百分点。**|
|**2024-11-19**|**Information Theory of Meaningful Communication**|Doron Sivan et.al.|[2411.12728](http://arxiv.org/abs/2411.12728)|null|在香农的经典论文中，印刷英语的熵被估计为大约每个字符1比特。然而，作为交流手段的语言与它的印刷形式有很大的不同：（i）信息的基本单位不是字符或单词，而是句子，即最短的有意义的语义部分；（ii）传达的主要内容是所说或所写内容的意义，而用来传达意义的确切措辞通常被忽略。在这项研究中，我们展示了可以利用最近开发的大语言模型来量化有意义叙述中传达的信息，以每句意义的比特数来衡量。|
|**2024-11-19**|**Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques. Defying BERT?**|Ahmed Akib Jawad Karim et.al.|[2411.12703](http://arxiv.org/abs/2411.12703)|null|快速传播的虚假信息，尤其是在在线平台上的传播，凸显了可靠检测系统的重要性和紧迫性。本研究探讨了使用机器学习和自然语言处理技术，特别是支持向量机（SVM）和BERT，来检测假新闻的方法。我们为SVM采用了三种不同的文本向量化方法：词频逆文档频率（TF-IDF）、Word2Vec和词袋模型（BoW），以评估它们在区分真实和虚假新闻方面的有效性。此外，我们将这些方法与大型语言模型BERT进行了比较。我们的综合方法包括详细的预处理步骤、严格的模型实现和全面的评估，以确定最有效的方法。结果显示，尽管BERT取得了卓越的准确率，达到99.98%，F1分数为0.9998，但使用线性核函数和支持向量机（SVM）的词袋模型（BoW）同样表现优异，准确率达到99.81%，F1分数为0.9980。这些结果表明，尽管BERT性能更优，但使用词袋模型（BoW）和TF-IDF向量化方法的支持向量机（SVM）模型接近其性能，并且具有较低的计算需求优势。|
|**2024-11-19**|**When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations**|Huaizhi Ge et.al.|[2411.12701](http://arxiv.org/abs/2411.12701)|null|大型语言模型（LLMs）容易受到后门攻击，其中隐藏的触发器可以恶意操纵模型行为。尽管已经提出了几种后门攻击方法，但LLMs中后门功能的工作机制仍需进一步探索。在本文中，我们超越了对LLMs的攻击，通过自然语言解释的新颖视角来研究后门功能。具体而言，我们利用LLMs的生成能力为其决策生成人类可理解的解释，从而能够比较清洁样本和中毒样本的解释。我们探讨了各种后门攻击，并将后门嵌入到多个任务中的LLaMA模型。我们的实验表明，中毒后的模型在生成清洁数据的解释时质量更高，而在生成中毒数据的解释时则显著更加一致。我们进一步分析了解释生成过程，揭示在令牌级别上，中毒样本的解释令牌仅出现在LLM的最后一两个变换层中。在句子级别上，注意力动态表明，在生成解释时，中毒输入会将注意力从输入上下文中转移开。这些发现加深了我们对LLMs中后门攻击机制的理解，并提供了一个通过可解释性技术检测此类漏洞的框架，有助于开发更安全的LLMs。|
|**2024-11-19**|**SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference**|Jiho Shin et.al.|[2411.12692](http://arxiv.org/abs/2411.12692)|null|利用稀疏性对于优化大型语言模型的推理至关重要。然而，采用SiLU作为激活函数的现代LLM表现出最小的激活稀疏性。最近的研究提出用ReLU代替SiLU以诱导显著的激活稀疏性，并通过微调显示下游任务准确性没有下降。然而，要充分利用这一点，需要训练一个预测器来估计这种稀疏性。在本文中，我们引入了SparseInfer，这是一种简单、轻量且无需训练的预测器，用于预测ReLU领域LLM的激活稀疏性，在该预测器中，激活稀疏性是通过比较输入和权重的符号位来预测的。为了补偿可能的预测不准确，启用了自适应调整预测器的保守程度，这也可以作为一个控制旋钮来优化LLM的推理。所提出的方法比最先进的方法快大约1倍，且精度损失在1%以内。|
|**2024-11-19**|**Neurosymbolic Graph Enrichment for Grounded World Models**|Stefano De Giorgis et.al.|[2411.12671](http://arxiv.org/abs/2411.12671)|null|复杂现实世界场景的理解和推理是人工智能系统开发中的一个重大挑战。在这项工作中，我们提出了一种新颖的方法来增强和利用大型语言模型（LLM）的反应能力，以解决复杂问题并深入理解情境化的现实世界含义。我们介绍了一种方法和工具，用于创建一种多模态、知识增强的形式化意义表示，该表示结合了大型语言模型的优势与结构化语义表示的优势。我们的方法始于图像输入，利用最先进的大型语言模型生成自然语言描述。然后，这个描述被转化为抽象意义表示（AMR）图，该图被形式化并丰富了逻辑设计模式以及来自语言和事实知识库的分层语义。随后，生成的图形被反馈到LLM中，通过复杂的启发式学习扩展，包括语义隐含、道德价值、具身认知和隐喻表示。通过弥合非结构化语言模型与正式语义结构之间的鸿沟，我们的方法为解决自然语言理解和推理中的复杂问题开辟了新的途径。|
|**2024-11-19**|**DLBacktrace: A Model Agnostic Explainability for any Deep Learning Models**|Vinay Kumar Sankarapu et.al.|[2411.12643](http://arxiv.org/abs/2411.12643)|**[link](https://github.com/aryaxai/dlbacktrace)**|**人工智能的快速发展催生了越来越复杂的深度学习模型，这些模型经常作为不透明的“黑箱”运作，其决策过程缺乏透明度。这种缺乏可解释性带来了相当大的挑战，尤其是在高风险应用中，理解模型输出背后的理由与输出本身同样重要。本研究针对人工智能系统中的紧迫需求，强调了可解释性在建立信任、确保问责以及在关键任务领域负责任部署方面的作用。为了解决深度学习中的可解释性问题，我们介绍了DLBacktrace技术，这是一种由AryaXAI团队开发的创新技术，旨在阐明各种领域的模型决策，包括简单的多层感知器（MLP）、卷积神经网络（CNN）、大型语言模型（LLM）、计算机视觉模型等。我们提供了DLBacktrace算法的全面概述，并通过多样化的基于任务的指标，将其性能与已建立的可解释性方法（如SHAP、LIME、GradCAM、Integrated Gradients、SmoothGrad和Attention Rollout）进行基准测试比较。提出的DLBacktrace技术兼容多种在PyTorch和TensorFlow中构建的模型架构，支持像Llama 3.2这样的自然语言处理架构，以及其他诸如BERT和LSTM的模型，计算机视觉模型如ResNet和U-Net，以及用于表格数据的自定义深度神经网络（DNN）。这种灵活性突显了DLBacktrace在增强模型透明度方面的适应性和有效性。该库是开源的，并可在https://github.com/AryaXAI/DLBacktrace获取。**|
|**2024-11-19**|**Improving Controllability and Editability for Pretrained Text-to-Music Generation Models**|Yixiao Zhang et.al.|[2411.12641](http://arxiv.org/abs/2411.12641)|null|在AI辅助音乐创作领域已经取得了显著进展，但现有的系统通常难以满足迭代和精细音乐制作的需求。这些挑战包括对生成内容的控制不足以及缺乏灵活、精确的编辑功能。本论文通过一系列逐步推进的改进来解决这些问题，从而增强文本到音乐生成模型的可控性和可编辑性。  首先，我们介绍了Loop Copilot系统，旨在解决音乐创作中迭代优化的需求。Loop Copilot利用大型语言模型（LLM）协调多个专业化的AI模型，使用户能够通过对话界面交互式地生成和优化音乐。该系统的核心是全局属性表，它在整个迭代过程中记录并保持关键的音乐属性，确保在任何阶段的修改都能保持音乐的整体连贯性。虽然Loop Copilot在协调音乐创作过程方面表现出色，但它并不能直接解决对生成内容进行详细编辑的需求。  为了解决这一局限性，我们提出了MusicMagus作为进一步的解决方案，用于编辑AI生成的音乐。MusicMagus引入了一种零样本文本到音乐编辑方法，允许用户修改特定的音乐属性，如流派、情绪和乐器编配，而无需重新训练模型。通过操作预训练扩散模型中的潜在空间，MusicMagus确保这些编辑在风格上连贯，并且非目标属性保持不变。此系统特别有效于在编辑过程中保持音乐的结构完整性，但在更复杂和现实世界的音频场景中遇到挑战。|
|**2024-11-19**|**AdaCM $^2$: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction**|Yuanbin Man et.al.|[2411.12593](http://arxiv.org/abs/2411.12593)|null|大型语言模型（LLMs）的进步推动了视频理解任务的发展，通过将视觉模型与LLMs结合，但大多数现有的基于LLM的模型（例如VideoLLaMA、VideoChat）仅限于处理短时长视频。最近尝试通过提取并压缩视觉特征到固定内存大小来理解长视频。然而，这些方法仅利用视觉模态来合并视频令牌，并忽略了视觉和文本查询之间的相关性，导致在处理复杂的问答任务时面临困难。为了解决长视频和复杂提示的问题，我们提出了AdaCM$^2$，这是首次引入了一种自适应跨模态内存缩减方法，在视频流上以自回归方式实现视频-文本对齐。我们在各种视频理解任务上的广泛实验，如视频描述生成、视频问答和视频分类，表明AdaCM$^2$ 在多个数据集上实现了最先进的性能，同时显著减少了内存使用。特别地，在LVU数据集上，它在多个任务中取得了4.5%的提升，并且GPU内存消耗减少了高达65%。|
|**2024-11-19**|**Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models**|Laura Ruis et.al.|[2411.12580](http://arxiv.org/abs/2411.12580)|**[link](https://github.com/pomonam/kronfluence)**|**大型语言模型（LLMs）的能力和局限性在近年来得到了详细描述，这提供了一个引人入胜但又相互矛盾的图景。一方面，LLMs展示了解决问题的一般能力；另一方面，它们在与人类相比时显示出令人惊讶的推理差距，这质疑了它们泛化策略的稳健性。由于设计LLMs所使用的数据量巨大，我们无法使用传统方法来衡量泛化能力：即训练-测试集分离。为了克服这一点，我们研究了当执行推理任务时，LLMs采用何种泛化策略，并通过调查它们依赖的预训练数据来对此进行探讨。对于两种不同规模的模型（7B和35B参数量）以及其中25亿个预训练令牌，我们识别了影响模型输出的文档，并将其与回答事实性问题的数据进行对比。我们发现，虽然模型对每个事实性问题依赖的数据集大多是不同的，但在同一任务的不同推理问题中，一个文档往往具有相似的影响，这表明存在程序性知识。进一步发现，事实性问题的答案通常会出现在最具影响力的数据中。然而，对于推理问题，答案通常不会显示出高度影响力，中间推理步骤的答案也是如此。当我们定性地分析推理问题的排名靠前的文档时，我们确认这些有影响力的文档通常包含了程序性知识，例如通过公式或代码演示如何获得解决方案。我们的研究结果表明，模型采用的推理方法不同于检索，而更像是一种能够从执行类似推理形式的文档中综合程序性知识的通用策略。**|
|**2024-11-18**|**Bi-Mamba: Towards Accurate 1-Bit State Space Models**|Shengkun Tang et.al.|[2411.11843](http://arxiv.org/abs/2411.11843)|null|典型的Mamba选择性状态空间模型（SSM）解决了Transformer的一些限制，如序列长度导致的二次计算复杂度和由于键值缓存导致的显著推理时间内存需求。然而，Mamba模型规模的不断扩大仍然带来了训练和部署挑战，并引发了能源消耗方面的环境问题。在这项工作中，我们引入了Bi-Mamba，这是一种可扩展且强大的1比特Mamba架构，旨在实现更高效的大型语言模型，涵盖780M、1.3B和2.7B等多种规模。Bi-Mamba模型从与常规大语言模型相同的大量数据中从头开始训练，并使用自回归蒸馏损失进行训练。在语言建模的广泛实验结果表明，Bi-Mamba在性能上与全精度模型（例如FP16或BF16）相当，并且比后训练二值化（PTB）Mamba基线模型具有更好的准确性，同时相比原始Mamba模型显著减少了内存占用和能源消耗。我们的研究开创了一种低比特表示下的线性计算复杂度的大语言模型框架，并促进了针对高效1比特Mamba大语言模型的专用硬件设计。|
|**2024-11-18**|**Tackling prediction tasks in relational databases with LLMs**|Marek Wydmuch et.al.|[2411.11829](http://arxiv.org/abs/2411.11829)|null|尽管大型语言模型（LLMs）在许多问题上展示了卓越的性能，但它们在关系数据库预测任务中的应用尚未得到充分探索。在这项工作中，我们探讨了LLMs无法在关系数据库上获得满意结果的观点，因为这些数据库包含相互连接的表、复杂的关联以及异构的数据类型。通过最近引入的RelBench基准测试，我们证明了即使是对LLMs进行简单的应用也能在这些任务上取得具有竞争力的表现。这些发现确立了LLMs作为关系数据库机器学习的新基准，并鼓励进一步的研究。|
|**2024-11-18**|**Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods**|Egor Kovalev et.al.|[2411.11795](http://arxiv.org/abs/2411.11795)|null|对抗性鲁棒性是神经网络研究中的一个重要领域，涵盖了计算机视觉模型、大型语言模型（LLMs）等的研究。随着JPEG AI——首个端到端神经图像压缩（NIC）方法的标准的发布，其鲁棒性问题变得尤为关键。JPEG AI是首批国际上实际应用的基于神经网络的模型之一，并被嵌入到消费设备中。然而，NIC鲁棒性的研究仅限于开源编解码器和有限范围的攻击。本文提出了一种新的方法来衡量NIC对对抗性攻击的鲁棒性。我们进行了首次大规模评估，以比较JPEG AI与其他NIC模型的鲁棒性。我们的评估结果和代码将在网上公开提供（链接在盲审中被隐藏）。|
|**2024-11-18**|**LLM-IE: A Python Package for Generative Information Extraction with Large Language Models**|Enshuo Hsu et.al.|[2411.11779](http://arxiv.org/abs/2411.11779)|null|尽管最近采用了大型语言模型（LLMs）进行生物医学信息提取，但在提示工程和算法方面仍然存在挑战，并且没有专门的软件可用。为了解决这一问题，我们开发了LLM-IE：一个用于构建完整信息提取管道的Python包。我们的主要创新是一个交互式的LLM代理，用以支持模式定义和提示设计。材料与方法：LLM-IE支持命名实体识别、实体属性提取和关系提取任务。我们在i2b2数据集上进行了基准测试并进行了系统评估。结果：基于句子的提示算法表现最佳，但需要更长的推理时间。系统评估提供了直观的可视化效果。讨论：LLM-IE的设计基于在医疗保健领域的实际NLP经验，并已在内部项目中采用。它应该对生物医学NLP社区具有巨大价值。结论：我们开发了一个Python包LLM-IE，提供构建稳健的信息提取管道的构建模块。|
|**2024-11-18**|**sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality Spaces with LLMs and Generative AI**|Yunhao Xing et.al.|[2411.11752](http://arxiv.org/abs/2411.11752)|null|在混合现实（MR）环境中，理解和创建虚拟对象对于提供直观且丰富的用户体验至关重要。本文介绍了一种名为sMoRe（Spatial Mapping and Object Rendering Environment，空间映射与对象渲染环境）的MR应用程序，该应用结合了生成式人工智能（GenAI）和大型语言模型（LLM），以帮助用户在物理空间中创建、放置和管理虚拟对象。sMoRe允许用户使用语音或文本命令来利用生成式AI创建和放置虚拟对象，并指定空间约束。该系统利用LLM解释用户的命令，分析当前场景，并识别最佳位置。此外，sMoRe集成了文本到3D生成式AI，可以根据用户的描述动态创建3D对象。我们的用户研究证明了sMoRe在增强用户对MR环境的理解、交互和组织方面的有效性。|
|**2024-11-18**|**BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration**|Yuzong Chen et.al.|[2411.11745](http://arxiv.org/abs/2411.11745)|null|大型语言模型（LLMs）在各种机器学习任务中表现出色。然而，这些模型的庞大内存占用显著阻碍了它们的部署。本文提出了一种名为BitMoD的算法与硬件协同设计解决方案，旨在以低权重精度实现高效加速。在算法层面，BitMoD引入了细粒度的数据类型自适应技术，使用不同的数值数据类型来量化一组（例如，128个）权重。通过精心设计这些新的数据类型，BitMoD能够在保持高准确率的同时将LLM权重量化到非常低的精度（例如，4位和3位）。在硬件层面，BitMoD采用位串行处理单元，轻松支持多种数值精度和数据类型；其硬件设计包括两项关键创新：首先，它采用统一表示法处理不同权重数据类型，从而降低了硬件成本。其次，它采用位串行去量化单元，以最小的硬件开销重新缩放每组部分和。我们在六个代表性LLM上的评估表明，BitMoD显著优于最先进的LLM量化和加速方法。对于判别任务，BitMoD可以将LLM权重量化到4位，平均准确率损失小于0.5%。对于生成任务，BitMoD能够将LLM权重量化到3位，同时获得比先前LLM量化方案更好的困惑度。结合优秀的模型性能与高效的加速器设计，BitMoD相比先前的LLM加速器ANT和OliVe分别实现了平均1.69倍和1.48倍的速度提升。|
|**2024-11-18**|**Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment**|Allison Huang et.al.|[2411.11731](http://arxiv.org/abs/2411.11731)|null|我们探讨了大型语言模型（LLMs）如何通过提示来改变其最初的决策，并使其与既定的伦理框架保持一致。我们的研究基于两个实验，旨在评估LLMs对道德说服的敏感性。在第一个实验中，我们通过评估一个基础代理LLM在道德模糊场景中的决策，并观察一个说服者代理如何试图改变基础代理的初始决策，来检验其对道德模糊性的敏感性。第二个实验则评估了LLMs对预定义伦理框架的敏感性，通过提示它们采纳根植于已建立的哲学理论的具体价值对齐。结果表明，LLMs确实可以在具有道德争议的情境中被说服，说服的成功与否取决于诸如所使用的模型、场景的复杂度和对话长度等因素。值得注意的是，来自同一家公司的不同规模的LLMs产生了明显不同的结果，这突显了它们在伦理说服方面的敏感性差异。|
|**2024-11-18**|**Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation**|Mingchao Qi et.al.|[2411.11714](http://arxiv.org/abs/2411.11714)|**[link](https://github.com/mingchaoqi/skill_transfer)**|**在开放世界环境中部署机器人涉及复杂的任务，这些任务具有长序列和丰富的交互性，需要在各种复杂场景中高效地转移机器人的技能。为了解决这一挑战，我们提出了一种基于知识图谱的技能库框架，该框架使机器人具备高级技能意识和空间语义理解能力。该框架通过构建“任务图”和“场景图”来分层组织操作知识，分别表示任务和场景语义信息。我们引入了“状态图”，以促进高层次任务规划与低层次场景信息之间的交互。此外，我们提出了一个用于操作技能的分层迁移框架。在任务层面，该框架在一个四阶段提示范式中集成了上下文学习和连锁思维提示，利用大型语言模型（LLM）的推理和泛化能力，实现任务层面子任务序列的迁移。在运动层面，我们开发了一种使用A*算法和技能库的自适应轨迹迁移方法，实现了运动层面的自适应轨迹迁移。在物理层面，我们引入了一种基于触觉感知的自适应轮廓提取和姿势感知方法，这种方法可以从视觉-触觉纹理数据中动态获取高精度的轮廓和姿势信息，并调整迁移的技能，如接触位置和姿势，以确保在新环境中的有效性。实验结果验证了所提出方法的有效性。项目网址：https://github.com/MingchaoQi/skill_transfer**|
|**2024-11-18**|**FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models**|Tao Fan et.al.|[2411.11707](http://arxiv.org/abs/2411.11707)|null|通过将大型语言模型（LLMs）适应于特定领域的任务或为其注入特定领域的知识，我们可以充分利用LLMs的能力。然而，目前仍存在一个差距，即无法同时实现服务器端的LLMs和下游客户端的小型语言模型（SLMs）之间的相互增强。为了解决这一问题，我们提出了FedCoLLM，这是一种新颖且参数高效的联邦框架，旨在协同调优LLMs和SLMs。该方法旨在自适应地将服务器端LLMs的知识转移到客户端的SLMs，同时利用客户端的数据丰富LLMs的领域知识。为了实现这一点，FedCoLLM使用轻量级适配器与SLMs结合，促进了在尊重数据隐私的同时，减少计算和通信开销的知识交换。我们的评估显示，在各种公开的LLMs和SLMs以及一系列自然语言处理文本生成任务中，借助LLMs的帮助，客户端的SLMs性能得到了显著提升。与此同时，通过FedCoLLM增强的LLMs的表现与直接在客户端数据上微调的效果相当。|
|**2024-11-18**|**Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search**|Jinhao Jiang et.al.|[2411.11694](http://arxiv.org/abs/2411.11694)|null|最近，测试时扩展在研究社区引起了广泛关注，这主要归功于OpenAI发布的o1模型所带来的显著进步。通过在推理阶段分配更多的计算资源，大型语言模型（LLMs）可以更广泛地探索解空间，通过生成更多的思考令牌或多样化解决方案，从而产生更准确的响应。然而，开发类似o1的推理方法具有挑战性，研究人员一直在尝试推进这一开放的研究领域。本文提出了一种初步探索，通过奖励引导的树搜索算法来增强LLMs的推理能力。该框架通过整合策略模型、奖励模型和搜索算法来实现。它主要围绕一个树搜索算法构建，其中策略模型在由专门训练的奖励模型引导的动态扩展树中进行导航。我们详细探讨了实现此框架所需的各种设计考虑，并提供了技术方面的详细报告。为了评估我们的方法的有效性，我们专注于数学推理任务，并在四个具有挑战性的数据集上进行了广泛的评估，显著提升了LLMs的推理能力。|
|**2024-11-15**|**Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization**|Weiyun Wang et.al.|[2411.10442](http://arxiv.org/abs/2411.10442)|null|现有的开源多模态大型语言模型（MLLMs）通常采用预训练和有监督微调的训练过程。然而，这些模型受到分布变化的影响，这限制了它们的多模态推理能力，特别是在链式思维（CoT）表现方面。为了解决这个问题，我们引入了一种偏好优化（PO）过程以增强MLLMs的多模态推理能力。具体来说，（1）在数据方面，我们设计了一个自动化的偏好数据构建管道，创建了一个高质量的大规模多模态推理偏好数据集MMPR。并且（2）在模型方面，我们探索将PO与MLLMs集成，开发了一种简单而有效的方法，称为混合偏好优化（MPO），这种方法提高了多模态CoT的表现。我们的方法在多个基准测试中展示了改进的性能，尤其是在多模态推理任务中。特别地，我们的模型InternVL2-8B-MPO在MathVista上的准确率达到了67.0，比InternVL2-8B高出8.7个百分点，并且达到了与10倍大的InternVL2-76B相当的性能。我们希望这项研究能激发MLLMs领域的进一步发展。代码、数据和模型将会公开发布。|
|**2024-11-15**|**LLaVA-o1: Let Vision Language Models Reason Step-by-Step**|Guowei Xu et.al.|[2411.10440](http://arxiv.org/abs/2411.10440)|null|大型语言模型在推理能力方面已经取得了显著的进步，特别是在通过推理时间扩展方面，例如OpenAI的o1模型。然而，当前的视觉-语言模型（VLM）在执行系统性和结构化推理时往往表现不佳，尤其是在处理复杂的视觉问答任务时。在这项工作中，我们介绍了LLaVA-o1，这是一种新型的VLM，旨在进行自主的多阶段推理。与思维链提示不同，LLaVA-o1独立地进行总结、视觉解释、逻辑推理和结论生成的顺序阶段。这种结构化的方法使LLaVA-o1在推理密集型任务中的精度有了显著提升。为此，我们编制了LLaVA-o1-100k数据集，整合了来自各种视觉问答来源的样本，并提供了结构化的推理注释。此外，我们提出了一种推理时间阶段级束搜索方法，这使得推理时间扩展成为可能。值得注意的是，仅使用100k训练样本和一种简单但有效的推理时间扩展方法，LLaVA-o1不仅在其广泛使用的多模态推理基准测试中比其基础模型提高了8.9%，而且超过了更大甚至闭源模型的表现，如Gemini-1.5-pro、GPT-4o-mini和Llama-3.2-90B-Vision-Instruct。|
|**2024-11-15**|**MARS: Unleashing the Power of Variance Reduction for Training Large Models**|Huizhuo Yuan et.al.|[2411.10438](http://arxiv.org/abs/2411.10438)|null|训练深度神经网络——以及最近的大模型——需要高效且可扩展的优化器。自适应梯度算法如Adam、AdamW及其变体在这一任务中起到了核心作用。尽管在过去十年中开发了许多旨在加速凸和非凸设置下随机优化的方差缩减算法，但方差缩减并未在训练深度神经网络或大型语言模型中获得广泛成功。因此，在现代人工智能中，这种方法较少被采用。在这篇论文中，为了释放方差缩减在高效训练大型模型中的潜力，我们提出了一种统一的优化框架MARS（Make vAriance Reduction Shine），该框架通过一种缩放随机递归动量技术将预条件梯度方法与方差缩减相结合。在我们的框架内，我们分别引入了三种基于AdamW、Lion和Shampoo的MARS实例。我们还探讨了我们的算法与现有优化器之间的联系。实验结果表明，在训练GPT-2模型时，MARS的表现始终大幅优于AdamW。|
|**2024-11-15**|**Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization**|Yuhan Fu et.al.|[2411.10436](http://arxiv.org/abs/2411.10436)|null|多模态大型语言模型（MLLMs）因其幻觉现象而受到限制，这限制了它们的实际应用。近期的研究尝试通过直接偏好优化（DPO）来提升MLLMs的性能，但这些方法在减少幻觉方面显示出不一致的效果。为了更有效地解决这个问题，我们引入了一种针对幻觉的直接偏好优化方法（HDPO），以减少MLLMs中的幻觉。与先前的方法不同，我们的方法针对幻觉的不同形式和原因进行处理。具体来说，我们开发了三种类型的偏好对数据，分别针对以下导致MLLMs幻觉的原因：（1）视觉能力不足，（2）长上下文生成，以及（3）多模态冲突。实验结果表明，我们的方法在多个幻觉评估数据集上取得了优越的性能，超过了大多数最先进的方法，并突显了我们方法的潜力。消融研究和深入分析进一步证实了我们方法的有效性，并提出了通过扩大规模进一步改进的可能性。|
|**2024-11-15**|**Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash**|Parsa Hejabi et.al.|[2411.10422](http://arxiv.org/abs/2411.10422)|**[link](https://github.com/parsahejabi/simulation-framework-for-multi-agent-balderdash)**|**大型语言模型（LLMs）在复杂任务和交互式环境中展示了令人印象深刻的技能，但它们的创造力仍需深入探索。本文介绍了一个利用游戏“Balderdash”来评估LLMs创造力和逻辑推理能力的仿真框架。在“Balderdash”游戏中，玩家为生僻词汇编造虚构定义以欺骗他人，同时识别正确的定义。我们的框架使多个LLM代理能够参与此游戏，评估它们生成可信定义的能力以及基于游戏规则和历史进行策略规划的能力。我们实现了一个集中式游戏引擎，其中包含各种LLM作为参与者，并且有一个判断LLM来评估语义等效性。通过一系列实验，我们分析了不同LLM的表现，考察了诸如真实定义比率、欺骗比率和正确猜测比率等指标。结果提供了对LLMs创造性和欺骗能力的见解，突出了它们的优势和改进空间。研究还表明，LLMs输入中的生僻词汇较少会导致其对游戏规则和历史背景的推理能力较差。具体实现可参见(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash)。**|
|**2024-11-15**|**Interactive Cycle Model -- The Linkage Combination among Automatic Speech Recognition, Large Language Models and Smart Glasses**|Libo Wang et.al.|[2411.10362](http://arxiv.org/abs/2411.10362)|null|本研究提出了一个名为“ASR-LLM-智能眼镜”的交互循环模型，该模型结合了自动语音识别、大规模语言模型和智能眼镜，以促进无缝的人机交互。研究方法涉及将交互过程分解为不同的阶段和元素。语音通过自动语音识别系统捕捉并处理，然后由大规模语言模型进行分析和解释。结果随后传输到智能眼镜上显示。当用户与显示的数据进行交互时，反馈循环即完成。为了量化模型的性能，使用了数学公式，这些公式围绕核心评估点：在自动语音识别的语音转文本转换中的准确性、连贯性和延迟。研究结果通过理论测试和评估来验证模型的可行性和性能。尽管这种人机交互产品尚未在行业中出现，但该模型在提高依赖于人机交互领域的用户体验方面的表现也验证了其作为促进人机交互的技术的实用性。此外，本研究开创性地将诸如生成预训练Transformer模型等前沿技术整合到独特的交互模型中，大规模语言模型通过强大的评估技术和创新应用提供了原始价值，为评估和增强人机交互提供了新的视角。关键词：自动语音识别，大规模语言模型，智能眼镜，交互机制|
|**2024-11-15**|**Bias Unveiled: Investigating Social Bias in LLM-Generated Code**|Lin Ling et.al.|[2411.10351](http://arxiv.org/abs/2411.10351)|null|大型语言模型（LLMs）在自动化代码生成领域取得了显著进展。然而，研究中存在一个值得注意的空白，即评估LLMs生成代码中的社会偏见问题。为了解决这一问题，我们提出了一种新颖的公平性框架，即Solar，用于评估和减轻LLM生成代码中的社会偏见。具体而言，Solar能够自动生成测试用例，以定量地揭示LLM生成代码中的社会偏见。为了量化生成代码中的社会偏见严重程度，我们开发了一个涵盖各种社会问题的数据集。我们将Solar和该数据集应用于四种最先进的LLM代码生成模型进行评估。我们的评估结果显示，所有被测LLM生成的代码中都存在严重的偏见。此外，我们探索了几种偏见缓解策略，包括思维链（CoT）提示、结合积极角色扮演与CoT提示以及迭代提示。实验表明，迭代提示可以有效减少LLM生成代码中的社会偏见，最多可降低90%。Solar具有高度的扩展性，可用于评估新的社会问题。|
|**2024-11-15**|**Number it: Temporal Grounding Videos like Flipping Manga**|Yongliang Wu et.al.|[2411.10332](http://arxiv.org/abs/2411.10332)|**[link](https://github.com/yongliang-wu/numpro)**|**视频大型语言模型（Vid-LLMs）在视频内容理解方面取得了显著进展，但在需要精确时间定位的任务上，即视频时间定位（VTG），表现不佳。为了解决这一差距，我们引入了一种名为Number-Prompt（NumPro）的新方法，通过为每个视频帧添加唯一的数字标识符，使Vid-LLMs能够将视觉理解与时间定位相结合。将视频视为一系列编号的帧图像，NumPro将VTG转换成一个直观的过程：如同翻阅漫画面板一样按顺序查看。这使得Vid-LLMs能够“阅读”事件的时间线，准确地将视觉内容与相应的时间信息关联起来。我们的实验表明，NumPro显著提升了顶级Vid-LLMs的时间定位性能，且不会增加额外的计算成本。此外，在NumPro增强的数据集上进行微调定义了新的VTG领域最佳水平，相比之前的顶级方法，在时刻检索的mIoU上提高了最多6.9%，在亮点检测的mAP上提高了最多8.5%。代码将在https://github.com/yongliang-wu/NumPro 获取。**|
|**2024-11-15**|**Modification Takes Courage: Seamless Image Stitching via Reference-Driven Inpainting**|Ziqi Xie et.al.|[2411.10309](http://arxiv.org/abs/2411.10309)|**[link](https://github.com/yayoyo66/rdistitcher)**|**当前的图像拼接方法在处理具有挑战性的场景（如不均匀色调和大视差）时经常会产生明显的接缝。为了解决这个问题，我们提出了参考驱动的图像修补拼接器（RDIStitcher），该方法将图像融合和矩形化重新表述为基于参考的图像修补模型，相比以前的方法，它采用了更大的修改融合区域和更强的修改强度。此外，我们引入了一种自监督的模型训练方法，使得在无需标记数据的情况下通过微调文本到图像（T2I）扩散模型来实现RDIStitcher成为可能。鉴于评估拼接图像质量的难度，我们提出了基于多模态大语言模型（MLLMs）的指标，为评估拼接图像质量提供了新的视角。与最先进的方法相比，广泛的实验表明，我们的方法显著提高了拼接图像的内容连贯性和无缝过渡。特别是在零样本实验中，我们的方法展示了强大的泛化能力。代码：https://github.com/yayoyo66/RDIStitcher**|
|**2024-11-15**|**Static network structure cannot stabilize cooperation among Large Language Model agents**|Jin Han et.al.|[2411.10294](http://arxiv.org/abs/2411.10294)|null|大型语言模型（LLMs）越来越多地被用于模拟人类社交行为，最近的研究探索了它们在模拟社交动态方面的能力。在此，我们测试LLMs是否能在社会困境中反映人类行为，在这些困境中个人利益和集体利益相冲突。人类在实验室环境中通常比预期更加合作，在混合人群中合作较少但在固定网络中更多。相反，LLMs在混合环境中表现出更多的合作。这引发了一个关键问题：LLMs是否即将在固定网络中的合作困境中模仿人类行为？在这项研究中，我们在混合和结构化网络配置中检查了代理反复参与囚徒困境的网络交互，旨在识别LLMs与人类之间的合作行为的相似之处。我们的研究结果表明存在关键区别：虽然人类在结构化网络中更倾向于合作，但LLMs主要在混合环境中表现出更高的合作度，并且对网络环境的适应有限。值得注意的是，LLMs的合作程度也因模型类型而异，这说明了在人工代理中复制人类社交适应性的复杂性。这些结果突显了一个重要的差距：LLMs难以在固定网络中模仿人类部署的细微、适应性的社交策略。与人类参与者不同，LLMs不会根据网络结构或不断变化的社会环境调整其合作行为，未能采用人类适应性使用的互惠规范。这一局限性指出了未来LLM设计中的一个基本需求——整合对社交规范的深入理解，从而实现更真实地模拟人类合作和适应能力在网络环境中的表现。|
|**2024-11-14**|**MagicQuill: An Intelligent Interactive Image Editing System**|Zichen Liu et.al.|[2411.09703](http://arxiv.org/abs/2411.09703)|null|图像编辑涉及多种复杂的任务，并且需要高效而精确的操作技术。在本文中，我们介绍了MagicQuill，一个集成的图像编辑系统，能够快速实现创意想法。我们的系统具有简洁但功能强大的界面，允许通过最少的输入来表达编辑操作（例如插入元素、擦除对象、改变颜色）。这些交互由一个多模态大型语言模型（MLLM）实时监控，以预判编辑意图，从而无需显式地输入提示。最后，我们应用了一个强大的扩散先验，并通过一个精心学习的双分支插件模块进行增强，以实现对编辑请求的精确控制。实验结果证明了MagicQuill在实现高质量图像编辑方面的有效性。请访问https://magic-quill.github.io尝试我们的系统。|
|**2024-11-14**|**Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment in Multi-Modal Models**|Wei Wang et.al.|[2411.09691](http://arxiv.org/abs/2411.09691)|null|多模态大型语言模型（MLLMs）在细粒度视觉理解方面取得了显著的成功，但在细粒度知识对齐方面仍面临重大挑战。这些挑战限制了它们准确捕捉局部细节和实现全面全局感知的能力。尽管最近的研究重点在于将对象表达与定位信息对齐，但通常缺乏显式整合对象图像，而对象图像包含了超越单纯文本或坐标的丰富信息。为了弥合这一差距，我们提出了一种新颖的细粒度视觉知识对齐方法，该方法能够有效地对齐并整合对象的多尺度知识，包括文本、坐标和图像。这种方法基于我们开发的多尺度细粒度增强数据合成管道，提供了超过30万条关键训练数据，以增强对齐效果并提升整体性能。此外，我们还推出了TinyGroundingGPT，这是一系列优化用于高级别对齐的小型模型。这些模型参数量约为30亿，不仅在定位任务中表现出色，在复杂视觉场景中的表现也与更大的MLLMs相当。|
|**2024-11-14**|**Squeezed Attention: Accelerating Long Context Length LLM Inference**|Coleman Hooper et.al.|[2411.09688](http://arxiv.org/abs/2411.09688)|null|新兴的大规模语言模型（LLM）应用需要较长的输入提示来执行复杂的下游任务，如文档分析和代码生成。对于这些长上下文长度的应用，输入提示的长度对推理效率提出了显著挑战，因为推理成本随序列长度线性增加。然而，对于许多这些应用，提示中的大量上下文在不同的用户输入之间是固定的，因此提供了机会在离线优化处理用户输入的速度。在这项工作中，我们提出了一种称为压缩注意力的机制，以加速那些固定上下文占很大比例的LLM应用。首先，我们在离线阶段利用K均值聚类根据语义相似性将固定上下文的键进行分组，并用单个质心值表示每个簇。在推理过程中，我们将用户输入的查询令牌与质心进行比较，预测哪些来自固定上下文的键在语义上相关并需要在推理过程中加载。然后，我们仅使用这些重要的键从固定上下文中计算精确注意力，从而减少带宽和计算成本。我们还扩展了该方法，使用层次质心查找来识别重要的键，这可以将注意力复杂度从线性降低到对上下文长度的对数级。我们实现了优化的Triton内核来进行质心比较和使用重要键的稀疏FlashAttention，在长上下文推理的预填充和生成阶段实现了超过4倍的速度提升。此外，我们已在各种长上下文基准测试中广泛评估了该方法，包括LongBench，在没有精度损失的情况下实现了KV缓存预算3倍的减少，并且对于各种模型，精度差距小于0.5点时可实现高达8倍的减少。|
|**2024-11-14**|**Local deployment of large-scale music AI models on commodity hardware**|Xun Zhou et.al.|[2411.09625](http://arxiv.org/abs/2411.09625)|null|我们介绍了MIDInfinite，一个能够在普通硬件上本地生成符号音乐的网络应用程序。创建此演示涉及将Anticipatory Music Transformer（一种在Lakh MIDI数据集上预训练的大规模语言模型）移植到机器学习编译（MLC）框架中。一旦模型被移植，MLC可以在多种运行时环境中促进推理，包括C++、移动设备和浏览器。我们设想MLC有望弥合越来越强大的音乐AI模型与更熟悉的音乐软件开发技术之间的差距。作为概念验证，我们构建了一个网络应用程序，允许用户在浏览器中生成连续的多乐器MIDI流，这些流可以从头开始生成或根据提示进行生成。在普通硬件（如M3 MacBook Pro）上，我们的演示可以每秒生成51个音符，在72.9%的情况下生成速度超过实时播放，并且在提前缓冲2秒后，这一比例增加到86.3%。|
|**2024-11-14**|**PTR: Precision-Driven Tool Recommendation for Large Language Models**|Hang Gao et.al.|[2411.09613](http://arxiv.org/abs/2411.09613)|null|通过为大型语言模型（LLMs）配备外部工具，它们解决复杂问题的能力得到了显著提升。然而，尽管LLMs的解析能力在不断进步，但在提示中同时引入所有可用工具仍然是不切实际的，因为外部工具有很多。因此，为特定任务提供一套精确的工具（包括数量和质量方面）变得至关重要。当前的工具检索方法主要集中在优化工具排名列表，并直接打包固定数量的顶级工具作为工具集。然而，这些方法往往无法在执行前为LLMs配备最佳工具集，因为不同任务所需的最优工具数量可能不同，导致诸如冗余或不合适的工具等问题，从而阻碍了即时访问最相关的工具。本文解决了为LLMs推荐精确工具集的挑战。我们介绍了工具推荐的问题，定义了其范围，并提出了一种新颖的精度驱动工具推荐（PTR）方法。PTR通过利用历史工具包使用情况捕获一个初始且简洁的工具集，并通过执行工具匹配动态调整工具集，最终形成多视图基础上的工具添加。此外，我们还提出了一个新的数据集RecTools和一个名为TRACC的新指标，旨在评估LLMs的工具推荐效果。我们进一步通过全面的实验验证了我们的设计选择，在两个开放基准和我们的RecTools数据集上展示了有前景的准确性。|
|**2024-11-14**|**The Moral Foundations Weibo Corpus**|Renjie Cao et.al.|[2411.09612](http://arxiv.org/abs/2411.09612)|null|表达在自然语言中的道德情感对线上线下环境都有显著影响，塑造了行为风格和互动模式，包括社交媒体自我展示、网络欺凌、遵守社会规范以及伦理决策。为了有效测量自然语言处理文本中的道德情感，利用大型标注数据集至关重要，这些数据集提供了细致的理解以实现准确的分析和模型训练。然而，现有的语料库虽然有价值，但常常面临语言局限性。为了解决中文领域的这一差距，我们介绍了道德基础微博语料库。该语料库包含25,671条关于微博的评论，涵盖了六个不同的主题领域。每条评论至少由三位系统培训的注释员根据从道德理论中衍生出的十个道德类别进行人工标注。为了评估注释员的一致性，我们展示了kappa检验结果，这是衡量一致性的黄金标准。此外，我们应用了最新的大规模语言模型来补充人工注释，进行了分析实验以比较它们的表现，并报告了道德情感分类的基线结果。|
|**2024-11-14**|**Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework**|Ronak Pradeep et.al.|[2411.09607](http://arxiv.org/abs/2411.09607)|null|本报告提供了TREC 2024检索增强生成（RAG）轨道部分结果的初步分析。我们已经确定了RAG评估是继续在信息获取（更广泛地说，在自然语言处理和人工智能领域）取得进展的一个障碍，希望我们能够为解决这一领域的许多挑战做出贡献。本研究探讨的核心假设是，最初为2003年TREC问答赛道开发的金块（nugget）评估方法为评估RAG系统提供了一个坚实的基础。因此，我们的工作重点在于“重构”该方法，具体来说是应用大型语言模型来自动创建金块并自动将金块分配给系统答案。我们称这种方法为AutoNuggetizer框架。在TREC设置下，我们能够将完全自动化的流程与半手动由人工评估者创建金块并手动分配到系统答案的过程进行校准。基于来自45个运行中的21个主题的初步结果，我们观察到全自动金块评估所得到的分数与人类评估者半手动评估的分数之间存在强烈的关联性。这表明，我们的全自动评估过程可以用于指导未来的RAG系统迭代。|
|**2024-11-14**|**Accelerating Knowledge Graph and Ontology Engineering with Large Language Models**|Cogan Shimizu et.al.|[2411.09601](http://arxiv.org/abs/2411.09601)|null|大型语言模型有望显著加速包括本体建模、扩展、修改、填充、对齐以及实体消歧在内的关键知识图谱和本体工程任务。我们概述了基于大型语言模型的知识图谱和本体工程作为一个新的研究领域，并认为模块化方法在本体工程中将至关重要。|
|**2024-11-14**|**LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models**|Zhengyi Wang et.al.|[2411.09595](http://arxiv.org/abs/2411.09595)|null|本文探讨了扩展大型语言模型（LLMs）在文本预训练基础上生成3D网格的能力，并将其整合到统一的模型中。这种方法具有两大优势：（1）利用LLMs中已嵌入的空间知识，这些知识来源于3D教程等文本来源；（2）实现基于对话的3D生成和网格理解。主要挑战在于有效地将3D网格数据分词成离散的令牌，以便LLMs能够无缝处理。为此，我们引入了一种名为LLaMA-Mesh的新方法，该方法将3D网格的顶点坐标和面定义表示为纯文本，从而允许直接与LLMs集成而不必扩展词汇表。我们构建了一个有监督微调（SFT）数据集，使预训练的LLMs能够（1）根据文本提示生成3D网格，（2）生成所需的交错文本和3D网格输出，以及（3）理解和解释3D网格。我们的工作首次证明了LLMs可以通过文本格式微调以获得用于3D网格生成的复杂空间知识，从而有效统一3D和文本模态。LLaMA-Mesh在网格生成质量方面达到了与其他从头开始训练的模型相当的水平，同时保持了强大的文本生成性能。|
|**2024-11-14**|**Adopting RAG for LLM-Aided Future Vehicle Design**|Vahid Zolfaghari et.al.|[2411.09590](http://arxiv.org/abs/2411.09590)|null|在本文中，我们探讨了大型语言模型（LLMs）与检索增强生成（RAG）的集成，以增强汽车行业的自动化设计和软件开发。我们提出了两个案例研究：一个标准化合规聊天机器人和一个设计副驾驶，两者都利用RAG提供准确、上下文感知的回复。我们评估了四种LLM模型——GPT-4o、LLAMA3、Mistral和Mixtral，比较它们的回答准确性和执行时间。我们的结果显示，虽然GPT-4表现出色，但LLAMA3和Mistral也展示了在本地部署中的潜力，解决了汽车行业中的数据隐私问题。本研究突显了RAG增强的LLM在改善汽车工程中的设计流程和合规性的潜力。|
|**2024-11-13**|**The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models**|Daniel P. Jeong et.al.|[2411.08870](http://arxiv.org/abs/2411.08870)|null|近年来，一些研究试图开发专门用于医疗应用的基础模型，通过在公开的生物医学语料库上进行持续预训练来适应通用的大规模语言模型（LLMs）和视觉-语言模型（VLMs）。这些研究通常声称这种领域适应性预训练（DAPT）可以提高下游医疗任务的表现，例如回答医学执照考试问题。在本文中，我们对比了十个公共“医疗”LLM和两个VLM与其对应的基线模型，得出了不同的结论：所有医疗VLM和几乎所有医疗LLM在零样本/少样本提示和监督微调模式下进行医学问答任务时，并未始终优于其基线模型。例如，在所有任务和模型对的3次提示设置中，医疗LLM仅在22.7%的情况下优于其基线模型，在36.8%的情况下与基线模型持平，而在其余40.5%的情况下则显著劣于基线模型。我们的结论基于以下几点：(i) 直接对比每个医疗模型与其对应的基线模型；(ii) 分别优化每个模型的提示以进行零样本/少样本提示；(iii) 考虑比较中的统计不确定性。虽然这些基本做法在文献中并未被一致采用，但我们的消融实验表明，它们对结论有重大影响。同时，我们发现经过特定QA任务微调后，医疗LLM可以表现出性能提升，但这种优势并未转移到基于临床记录的任务上。我们的研究结果表明，最先进的通用领域模型可能已经具备强大的医学知识和推理能力，并提供了加强未来研究结论的建议。|
|**2024-11-13**|**LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs**|Piyush Jha et.al.|[2411.08862](http://arxiv.org/abs/2411.08862)|null|我们介绍了LLMStinger，这是一种新颖的方法，利用大型语言模型（LLMs）自动生成对抗后缀以进行越狱攻击。与传统方法不同，这些方法需要复杂的提示工程或白盒访问，LLMStinger使用强化学习（RL）循环来微调攻击者LLM，基于现有的攻击生成有害问题的新的后缀。我们的方法显著优于现有的红队技术（我们对比了最新的15种方法），在LLaMA2-7B-chat上的攻击成功率（ASR）提高了57.2%，在Claude 2上的ASR提高了50.3%，这两种模型都以其广泛的安全措施而闻名。此外，我们在GPT-3.5上的ASR达到了94.97%，在Gemma-2B-it上的ASR达到了99.4%，这证明了LLMStinger在开放和封闭源模型中的鲁棒性和适应性。|
|**2024-11-13**|**Multimodal Instruction Tuning with Hybrid State Space Models**|Jianing Zhou et.al.|[2411.08840](http://arxiv.org/abs/2411.08840)|null|处理长上下文对于增强多模态大语言模型（MLLMs）在处理高分辨率图像或高帧率视频等应用中的识别和理解能力至关重要。随着图像分辨率和帧率的提高，计算需求显著增加，因为输入标记的数量增多。这一挑战因自注意力机制相对于序列长度的二次复杂性而进一步加剧。大多数先前的工作要么通过预训练模型来应对长上下文问题，但忽略了效率问题，要么试图通过降采样（例如，识别关键图像块或帧）来减少上下文长度，但这可能导致信息丢失。为了在保持MLLMs卓越效果的同时解决这个问题，我们提出了一种使用混合Transformer-MAMBA模型的新方法，以高效处理多模态应用中的长上下文。我们的多模态模型能够有效处理超过10万令牌的长上下文输入，在各种基准测试中优于现有模型。值得注意的是，与当前模型相比，我们的模型在处理高分辨率图像和高帧率视频时的推理效率提高了约4倍，且随着图像分辨率或视频帧数的增加，效率提升更加明显。此外，我们的模型是首个能够在低分辨率图像或低帧率视频上进行训练，同时能够对高分辨率图像和高帧率视频进行推理的模型，从而在多种场景下提供灵活性。|
|**2024-11-13**|**FinRobot: AI Agent for Equity Research and Valuation with Large Language Models**|Tianyu Zhou et.al.|[2411.08804](http://arxiv.org/abs/2411.08804)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融市场日益复杂化，对自动化工具的需求也在不断增加，这些工具能够有效协助人类分析师进行股票研究，特别是在卖方研究领域。虽然生成式人工智能（GenAI）在这一领域吸引了大量关注，但现有的AI解决方案往往因为其技术因素的狭隘关注以及有限的自由裁量判断能力而表现不佳。这些限制阻碍了它们实时适应新数据的能力，并准确评估风险，从而降低了它们对投资者的实际价值。  本文介绍了FinRobot，这是首个专门设计用于股票研究的AI代理框架。FinRobot采用多代理链式思考（CoT）系统，整合定量和定性分析，以模拟人类分析师的全面推理过程。该系统围绕三个专业代理构建：数据CoT代理，它整合多种数据源以实现稳健的财务整合；概念CoT代理，它模仿分析师的推理以生成可操作的见解；以及主题CoT代理，它将这些见解综合成一个连贯的投资主题和报告。FinRobot提供基于精确数字数据、行业适当的估值指标以及现实风险评估的公司分析。其动态更新的数据管道确保研究保持及时和相关性，灵活适应新的财务信息。与现有的自动化研究工具（如CapitalCube和Wright Reports）不同，FinRobot提供的见解可以媲美大型经纪公司和基础研究供应商所生产的见解。我们已将FinRobot开源，网址为<https://github.com/AI4Finance-Foundation/FinRobot>。**|
|**2024-11-13**|**Evaluating World Models with LLM for Decision Making**|Chang Yang et.al.|[2411.08794](http://arxiv.org/abs/2411.08794)|null|世界模型在决策制定中扮演着关键角色，其中MuZero和Dreamer在复杂任务中取得了显著的成功。最近的研究利用大规模语言模型（LLMs）作为通用的世界模拟器来模拟世界的动态，因为它们具有泛化能力。这些大规模语言模型也作为世界模型用于推理规划（Reasoning via Planning, RAP）和思维树（Tree of Thought, ToT）中的深思熟虑的推理。然而，世界模型要么被评估为通用的世界模拟器，要么作为辅助规划预测的代理功能模块。在这项工作中，我们从决策制定的角度提出对使用大规模语言模型作为世界模型进行综合评估。具体来说，我们利用了来自Wang等人（2023；2024）的31个不同的环境，并为每个环境制定了基于规则的策略以实现多样化评估。然后，我们设计了三个主要任务：策略验证、动作建议和策略规划，在这些任务中世界模型可以单独用于决策制定。最后，我们在各种设置下对先进的大规模语言模型（即GPT-4o和GPT-4o-mini）在这些环境中进行了这三个主要任务的综合评估。关键观察结果包括：i) GPT-4o在三个主要任务上显著优于GPT-4o-mini，尤其是在需要领域知识的任务上；ii) 使用大规模语言模型作为世界模型进行长期决策任务时性能会下降；iii) 结合世界模型的不同功能可能会导致性能的额外不稳定。|
|**2024-11-13**|**Can sparse autoencoders be used to decompose and interpret steering vectors?**|Harry Mayne et.al.|[2411.08790](http://arxiv.org/abs/2411.08790)|**[link](https://github.com/harrymayne/sv_interpretability)**|**转向向量是控制大型语言模型行为的一种有前景的方法。然而，其底层机制仍然不甚明了。虽然稀疏自编码器（SAE）可能提供了一种解释转向向量的潜在方法，但最近的研究发现，由SAE重构的向量往往缺乏原始向量的转向特性。本文研究了为何直接将SAE应用于转向向量会产生误导性的分解，并确定了两个原因：（1）转向向量落在SAE设计的输入分布之外；（2）转向向量在特征方向上可以具有有意义的负投影，而这不是SAE所设计来处理的。这些限制阻碍了直接使用SAE来解释转向向量。**|
|**2024-11-13**|**Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers**|Clément Dumas et.al.|[2411.08745](http://arxiv.org/abs/2411.08745)|**[link](https://github.com/butanium/llm-lang-agnostic)**|**在多语言语言建模中的一个核心问题是大型语言模型（LLMs）是否发展出了与特定语言分离的通用概念表示。本文通过分析基于变换器的LLMs在词翻译任务中的潜在表示（潜伏变量），探讨了这一问题。我们通过从源翻译提示中提取潜伏变量，并将其插入到目标翻译提示的前向传递中，战略性地提取这些潜伏变量。通过这样做，我们发现输出语言在较早的层就被编码到潜伏变量中，而要翻译的概念则被编码在较晚的层。基于这一见解，我们进行了两个关键实验。首先，我们证明仅通过激活修补就可以改变概念而不改变语言，反之亦然。其次，我们表明使用不同语言潜伏变量的平均值进行修补不会损害模型的性能，反而提高了模型翻译概念的能力。我们的结果为所研究模型内部存在语言无关的概念表示提供了证据。**|
|**2024-11-13**|**A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models**|Dingdong Wang et.al.|[2411.08742](http://arxiv.org/abs/2411.08742)|null|随着语音大语言模型（Speech LLMs）的兴起，人们对离散语音标记的兴趣日益增加，因为它们能够与基于文本的标记无缝集成。尽管大多数研究集中在连续语音特征上，但基于离散标记的LLMs在某些任务上已经显示出有希望的结果，然而这两种范式之间的性能差距很少被探讨。在这篇论文中，我们通过使用一个轻量级的LLM（Qwen1.5-0.5B）对多种语义相关的任务进行了离散和连续特征的公平而全面的比较。我们的发现表明，连续特征通常比离散标记表现更好，特别是在需要细粒度语义理解的任务中。此外，本研究不仅停留在表面比较，还通过识别导致离散标记表现不佳的关键因素（如有限的标记粒度和低效的信息保留）来深入分析。为了提高离散标记的性能，我们基于分析探索了潜在的改进方面。我们希望我们的结果能为推进Speech LLMs中的离散语音标记提供新的见解。|
|**2024-11-13**|**Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models**|Somanshu Singla et.al.|[2411.08733](http://arxiv.org/abs/2411.08733)|**[link](https://github.com/Singla17/DRPO)**|对齐大型语言模型（LLMs）传统上依赖于昂贵的训练和人类偏好注释。自我对齐旨在通过使模型能够自行对齐来降低成本。为了进一步降低成本，并在没有任何昂贵调优或注释的情况下实现对齐，我们引入了一种新的无需调优的自我对齐方法，即动态奖励与提示优化（\ours）。我们的方法利用了一种基于搜索的优化框架，使LLMs能够在没有额外训练或人工干预的情况下迭代地自我改进并制定最优的对齐指令。核心的\ours是一种动态奖励机制，它识别并纠正特定于模型的对齐弱点，使LLMs能够高效适应多样的对齐挑战。在八种最近的LLMs（包括开源和闭源模型）上的实证评估表明，\ours显著提升了对齐性能，基础模型的表现超过了经过SFT/RLHF调优的模型。此外，由\ours自动优化的提示超越了由人类专家精心设计的提示，进一步验证了我们方法的有效性。我们的研究结果强调了当前LLMs通过推理时间优化实现自适应自我对齐的巨大潜力，补充了基于调优的对齐方法。|
|**2024-11-13**|**Polymetis:Large Language Modeling for Multiple Material Domains**|Chao Huang et.al.|[2411.08728](http://arxiv.org/abs/2411.08728)|null|随着大型语言模型在各个领域的应用不断扩大，材料科学也迎来了人工智能驱动创新的机会。传统上依赖手动搜索材料科学相关的信息现在正在利用人工智能技术作为辅助工具来提高材料科学研究的效率。为了加速研究人员在材料科学研究中的知识获取和智能化决策支持，本文提出了一种名为Polymetis的大型语言模型，旨在为材料领域提供高度专业的知识解答，涵盖能源材料、功能材料、合金材料、物理化学、生物等材料方向。该模型使用了大约200万条材料知识指令的数据集，在构建数据集的过程中，我们开发了智能提取大型模型（IELM），专门用于从科学文本中提取并形成结构化的知识，避免了大量的手动标注成本，提高了效率。我们将这些数据注入到GLM4-9B模型中进行学习，以增强其在多种材料领域的推理能力。此外，我们引入了增强提示策略，确保模型的回答更加有组织和全面，为材料科学探索的多样化需求提供高效和全面的智能化支持，推动材料科学的发展。|
|**2024-11-12**|**Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data**|Juanhui Li et.al.|[2411.08028](http://arxiv.org/abs/2411.08028)|null|在实际的自然语言处理应用中，大型语言模型（LLMs）由于其在大规模数据集上的广泛训练而提供了有前景的解决方案。然而，这些模型的大尺寸和高计算需求限制了它们在许多应用中的实用性，特别是当需要进一步微调时。为了解决这些限制，通常会选择较小的模型进行部署。然而，较小的模型训练受到标注数据稀缺的阻碍。相反，未标注的数据通常是容易获得的，并且可以通过使用LLMs生成伪标签来训练较小的模型。这使得较小的模型（学生）可以从LLMs（教师）那里获取知识，同时减少了计算成本。这个过程引入了一些挑战，比如潜在的噪声伪标签。因此，选择高质量和信息丰富的数据对于提高模型性能和改善数据利用效率至关重要。为了解决这个问题，我们提出了LLKD，这是一种能够在较少计算资源和较少数据的情况下从LLMs进行知识蒸馏的学习方法。LLKD是一种自适应样本选择方法，结合了来自教师和学生的信号。具体来说，它优先选择那些教师对其标签具有高置信度的样本，这表明标签是可靠的，并且那些学生表现出高度信息需求的样本，这识别出了需要进一步学习的具有挑战性的样本。我们的全面实验表明，LLKD在各种数据集上实现了卓越的性能，并提高了数据效率。|
|**2024-11-12**|**LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models**|Anoop Cherian et.al.|[2411.08027](http://arxiv.org/abs/2411.08027)|null|物理推理是机器人在现实世界中操作的重要技能。然而，解决这类推理问题通常涉及假设和反思复杂的多体相互作用，这些相互作用受到多种物理力的影响，因此学习所有这些相互作用对包括大型语言模型（LLMs）在内的最先进的机器学习框架来说是一个巨大的挑战。为研究这个问题，我们提出了一项新的物理推理任务和一个数据集，名为TraySim。我们的任务涉及预测托盘上几个物体在外力冲击下的动态——由此引发的物体相互作用及其动态提供了一个具有挑战性但又受控的设置，推理的目标是推断冲击后物体的稳定性。为了解决这个复杂的物理推理任务，我们提出了LLMPhy，这是一种零样本黑盒优化框架，它利用了LLMs的物理知识和程序合成能力，并将这些能力与现代物理引擎内置的世界模型相结合。具体而言，LLMPhy使用LLM生成代码，通过隐式的分析-综合方法迭代估计系统的物理超参数（摩擦、阻尼、布局等），并通过循环中的非可微模拟器来实现这一目标，并使用推断出的参数来想象场景的动态以解决推理任务。为了展示LLMPhy的有效性，我们在我们的TraySim数据集上进行了实验，以预测物体的稳态姿态。我们的结果显示，LLM和物理引擎的结合导致了最先进的零样本物理推理性能，同时展示了优于标准黑盒优化方法的收敛性，并且对物理参数有更好的估计。|
|**2024-11-12**|**Language Models as Causal Effect Generators**|Lucius E. J. Bynum et.al.|[2411.08019](http://arxiv.org/abs/2411.08019)|**[link](https://github.com/lbynum/sequence-driven-scms)**|**我们提出了一种基于大型语言模型（LLM）的具有可控因果结构的数据生成框架。具体来说，我们定义了一个过程，可以将任何语言模型和任何有向无环图（DAG）转化为序列驱动的结构因果模型（SD-SCM）。广义上讲，SD-SCM是一种具有用户定义结构和LLM定义结构方程的因果模型。我们描述了如何根据所需的因果结构从观察、干预和反事实分布中进行采样。然后，我们利用这一过程提出了一种新的因果推断方法基准测试，生成个体层面的反事实数据而无需手动指定变量之间的函数关系。我们创建了一个包含数千个数据集的例子基准，并在这些数据集上测试了一系列流行的估计方法，用于平均、条件平均和个体治疗效果估计，同时考虑隐藏混杂因素的影响。除了生成数据外，同一过程还允许我们检测可能编码在LLM中的因果效应的存在。此过程可以作为审计LLM以检测错误信息、歧视或其他不希望的行为的工具。我们认为SD-SCM可以成为任何需要顺序数据且具有可控因果结构的应用中的有用工具。**|
|**2024-11-12**|**ExpressivityArena: Can LLMs Express Information Implicitly?**|Joshua Tint et.al.|[2411.08010](http://arxiv.org/abs/2411.08010)|null|尽管大型语言模型（LLMs）在某些方面展示了卓越的性能，但它们在表达人类用于有效沟通的隐含语言线索方面的表现仍不清楚。本文介绍了ExpressivityArena，一个用于衡量LLMs隐含沟通能力的Python库。我们提供了一个全面的框架来评估任意LLMs的表达性，并探讨其实际应用。为此，我们精炼了“表达性”的定义和测量方法，并使用该框架进行了一系列小型实验。这些实验测试了LLMs在诗歌创作、编程和情感反应等创造性和逻辑任务中的表现。然后通过ExpressivityArena由自动化评分器对它们进行评估，我们验证这是测试表达性的最实用方法。在此基础上，我们通过评估LLMs在对话中保持表达性能力来深化对LLMs表达性的理解。我们的研究结果表明，LLMs能够生成和理解具有表达力的内容，但也存在一些局限性。这些见解将指导未来LLMs的开发和部署。我们将在论文中提供ExpressivityArena的代码。|
|**2024-11-12**|**Can adversarial attacks by large language models be attributed?**|Manuel Cebrian et.al.|[2411.08003](http://arxiv.org/abs/2411.08003)|null|在对抗性设置（如网络攻击和虚假信息）中，对大型语言模型（LLMs）的输出进行归因提出了显著的挑战，这些挑战可能会变得越来越重要。我们使用形式语言理论来研究这一归因问题，特别是Gold引入并在Angluin的工作中进一步扩展的语言识别极限。通过将LLM的输出建模为形式语言，我们分析了有限文本样本是否可以唯一地确定原始模型。我们的研究表明，由于某些语言类别的不可识别性，在一些关于微调模型重叠输出的温和假设下，理论上不可能以确定性的方式将输出归因于特定的LLM。即使考虑到Transformer架构的表达能力限制，这一结论仍然成立。即使有直接访问模型或全面监控，显著的计算障碍也会阻碍归因工作。这些发现强调了需要采取主动措施来减轻由对抗性LLM使用带来的风险，因为它们的影响继续扩大。|
|**2024-11-12**|**Derivational Morphology Reveals Analogical Generalization in Large Language Models**|Valentin Hofmann et.al.|[2411.07990](http://arxiv.org/abs/2411.07990)|null|在大型语言模型（LLMs）中的语言泛化机制是什么？这个问题引起了相当大的关注，大多数研究分析了LLMs的语言技能与规则的相似程度。目前尚不清楚LLMs中的语言泛化是否同样可以被解释为存储实例的相似性操作的结果。先前研究的一个关键不足在于其关注高度规律性的语言现象，对于这些现象，基于规则和基于类比的方法做出了相同的预测。我们在此考察派生词法，特别是英语形容词名词化，它显示了显著的变化性。我们引入了一种新的方法来研究LLMs中的语言泛化：专注于GPT-J，我们将基于规则和基于类比学习的认知模型拟合到LLM的训练数据，并比较它们对一组非正式形容词的预测与LLM的预测，从而能够直接得出有关潜在机制的结论。正如预期的那样，对于具有规则名词化模式的形容词，基于规则和基于类比的模型对GPT-J的预测解释得同样好。然而，对于具有变化名词化模式的形容词，类比模型提供了更好的匹配度。此外，GPT-J的行为对个体词频敏感，即使对于规则形式也是如此，这种行为与规则形式的类比解释一致，但不与基于规则的解释一致。这些发现反驳了GPT-J在形容词名词化方面的语言泛化涉及规则的假设，表明存储实例的相似性操作是潜在机制。总体而言，我们的研究表明，类比过程在LLMs的语言泛化中起着比以前认为更大的作用。|
|**2024-11-12**|**JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation**|Yiyang Ma et.al.|[2411.07975](http://arxiv.org/abs/2411.07975)|**[link](https://github.com/deepseek-ai/janus)**|我们介绍了JanusFlow，这是一个强大的框架，能够在单一模型中统一图像理解和生成。JanusFlow引入了一种极简架构，将自回归语言模型与在生成建模中处于领先地位的校正流（rectified flow）相结合。我们的主要发现表明，校正流可以简单地在大型语言模型框架内进行训练，无需复杂的架构修改。为了进一步提高我们统一模型的性能，我们采用了两种关键策略：(i) 解耦理解编码器和生成编码器，(ii) 在统一训练过程中对它们的表示进行对齐。广泛的实验表明，JanusFlow在各自领域中的表现可与专门模型相媲美或更优，同时在标准基准上显著优于现有的统一方法。这项工作朝着更高效和多功能的视觉-语言模型迈出了重要一步。|
|**2024-11-12**|**From General to Specific: Utilizing General Hallucation to Automatically Measure the Role Relationship Fidelity for Specific Role-Play Agents**|Chuyi Kong et.al.|[2411.07965](http://arxiv.org/abs/2411.07965)|null|先进的角色扮演能力使得开发角色扮演游戏代理（RPAs）成为可能。然而，现有的基准测试如HPD（通过将人工评分的角色关系纳入上下文以供大型语言模型（LLMs）进行连贯性排序）和社会基准测试（SocialBench，它使用由LLMs生成的特定角色配置文件，在多选任务的背景下评估角色偏好），面临着诸如泛化能力差、判断隐晦且不准确以及上下文长度过长等问题。为了解决这些问题，我们提出了一种自动、可扩展且具有广泛适用性的范式。具体而言，我们通过从通用知识图谱中提取关系来构建基准，并利用RPA固有的幻觉特性来促使它在不同角色之间进行互动，使用ChatGPT进行立场检测，并定义了关系幻觉及其三个相关指标。广泛的实验验证了这些指标的有效性和稳定性。我们的研究进一步探讨了影响这些指标的因素，并讨论了关系幻觉与事实性之间的权衡。|
|**2024-11-12**|**Towards Low-bit Communication for Tensor Parallel LLM Inference**|Harry Dong et.al.|[2411.07942](http://arxiv.org/abs/2411.07942)|null|张量并行性提供了一种有效的方法来提高服务器上大型语言模型（LLM）的推理效率，尽管这会增加额外的通信成本。然而，随着服务器LLM的规模继续扩大，它们需要分布在更多的设备上，这会放大通信成本。一种解决这个问题的方法是使用量化，但目前针对LLM的方法往往避免量化张量并行性需要通信的特征。利用通信特征中一致的异常值，我们介绍了一种量化方法，该方法将平均通信值从16位减少到4.2位，同时几乎保留了原始性能。例如，我们的方法保持了Gemma 2 27B和Llama 2 13B在所有评估任务上的原始性能分别约为98.0%和99.5%。|
|**2024-11-12**|**Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in Alzheimer's Disease**|Francesco Chiumento et.al.|[2411.07871](http://arxiv.org/abs/2411.07871)|null|快速发展的大规模语言模型（LLMs）和视觉-语言模型（VLMs）在医学诊断领域，尤其是在放射学方面，展现出了巨大潜力。这些领域的数据集如X射线通常与人类生成的诊断报告配对使用。然而，在神经影像学领域，特别是在阿尔茨海默病等病症的研究中，由于缺乏可用于模型微调的综合诊断报告，存在显著的研究空白。本文通过在OASIS-4数据集上使用GPT-4o-mini生成合成诊断报告来解决这一问题。OASIS-4数据集包含了663名患者的数据。利用这些合成报告作为训练和验证的真实数据，我们进一步利用预训练的BiomedCLIP和T5模型直接从数据集中的图像生成神经学报告。我们提出的方法获得了BLEU-4评分为0.1827，ROUGE-L评分为0.3719，METEOR评分为0.4163，这表明该方法在生成临床相关且准确的诊断报告方面具有潜力。|
|**2024-11-11**|**UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts**|Bo Yang et.al.|[2411.07240](http://arxiv.org/abs/2411.07240)|**[link](https://github.com/utmathgroup/utmath)**|数学推理能力的评估对于推进通用人工智能（AGI）至关重要。虽然大型语言模型（LLMs）在解决数学问题方面表现出了令人印象深刻的能力，但现有的基准测试如GSM8K和MATH存在局限性，包括问题定义狭窄且特定数字以及依赖于预设规则，这阻碍了对推理和适应性的准确评估。本文介绍了一个名为UTMath基准的新测试方法，该方法通过广泛的单元测试来全面评估模型。它涵盖了9个数学领域的1,053个问题，每个问题有超过68个测试用例。我们提出了一种创新的评估框架，灵感来源于软件开发中的单元测试，重点关注结果的准确性和可靠性。此外，我们引入了“思考推理到编码”（Reasoning-to-Coding of Thoughts，RCoT）方法，鼓励LLMs在生成代码之前进行明确的推理，从而生成更高级的解决方案并提高性能。此外，我们不仅会发布UTMath基准，还会发布UTMath-Train训练数据集（超过70,000个样本），以支持社区进一步探索数学推理。|
|**2024-11-11**|**OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model**|Sumeth Yuenyong et.al.|[2411.07238](http://arxiv.org/abs/2411.07238)|null|OpenThaiGPT 1.5是一款基于Qwen v2.5的先进泰语聊天模型，经过超过200万条泰语指令对的微调。本报告从工程角度介绍了该模型的开发、功能和性能。我们讨论了模型架构、训练过程以及多项关键特性，包括多轮对话支持、检索增强生成（RAG）兼容性和工具调用功能。基准测试结果表明，OpenThaiGPT 1.5在各种泰语任务上表现出最先进的性能，超越了其他开源泰语模型。此外，我们还探讨了实际应用中的考虑因素，如GPU内存需求和部署策略。|
|**2024-11-11**|**Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving**|Botao Yu et.al.|[2411.07228](http://arxiv.org/abs/2411.07228)|null|为了增强大型语言模型（LLMs）在化学问题解决中的能力，已经提出了几种配备了工具的LLM基代理，如ChemCrow和Coscientist。然而，它们的评估范围狭窄，对于理解工具在各种化学任务中的益处存在很大差距。为了弥补这一差距，我们开发了ChemAgent，这是一种基于ChemCrow的增强型化学代理，并对其在专门化学任务和普通化学问题上的表现进行了全面评估。令人惊讶的是，ChemAgent并不总是比没有工具的基础LLM表现出色。通过与化学专家进行错误分析，我们发现：对于专门的化学任务，例如合成预测，我们应该为代理配备专门的工具；然而，对于像考试中的普通化学问题，代理正确运用化学知识进行推理的能力更为重要，工具的配备并不总能提供帮助。|
|**2024-11-11**|**Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks**|Madeline Brumley et.al.|[2411.07213](http://arxiv.org/abs/2411.07213)|null|一个大型语言模型（LLMs）解释性研究的关键目标是开发方法以稳健地引导模型实现所需的行为。为此，提出了两种不同的解释方法——“自下而上”和“自上而下”。然而，这两种方法之间的定量比较很少。我们通过案例研究来比较两种代表性向量引导方法的效果：功能向量（FV；arXiv:2310.15213），作为自下而上的方法，以及情境向量（ICV；arXiv:2311.06668），作为自上而下的方法。尽管两者都旨在捕捉广泛情境学习任务的紧凑表示，但我们发现它们在特定类型的任务上效果不同：ICV在行为转变任务中优于FV，而FV在需要更高精度的任务中表现出色。我们讨论了这些发现对未来引导方法评估以及进一步研究自上而下和自下而上引导方法的影响。|
|**2024-11-11**|**DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID**|Nyle Siddiqui et.al.|[2411.07205](http://arxiv.org/abs/2411.07205)|**[link](https://github.com/croitorualin/dlcr)**|**随着生成扩散模型的近期展示出的强大能力，一个开放的研究问题是“由这些模型生成的图像是否可以用于学习更好的视觉表征”。虽然这种生成数据扩展可能足以应对较简单的视觉任务，但我们探索了其在更困难的判别任务中的有效性：衣服更换人员重识别（CC-ReID）。CC-ReID的目标是在非重叠摄像头中匹配出现的人，即使他们在不同摄像头中改变了衣服。当前的CC-ReID模型不仅受到现有CC-ReID数据集中服装多样性有限的限制，而且生成保留重要个人特征以实现准确识别的额外数据也是一个当前挑战。为了解决这一问题，我们提出了一种名为DLCR的新数据扩展框架，该框架利用预训练的扩散和大型语言模型（LLMs）来准确生成具有不同服装的个体的多样化图像。我们为五个基准CC-ReID数据集（PRCC、CCVID、LaST、VC-Clothes和LTCC）生成了额外的数据，并将其服装多样性增加了10倍，总计生成了超过210万张图像。DLCR采用基于扩散的文本引导修复技术，条件是使用LLMs构建的服装提示，以生成仅修改主体服装同时保留其个人可识别特征的合成数据。通过这种大规模的数据增加，我们引入了两种新策略——渐进学习和测试时预测优化——分别减少训练时间和进一步提升CC-ReID性能。在PRCC数据集上，通过使用DLCR生成的数据训练CAL（一种以前的最先进方法），我们获得了显著的前1名准确率提升11.3%。我们将代码和每个数据集生成的数据公开发布在这里：https://github.com/CroitoruAlin/dlcr。**|
|**2024-11-11**|**The Super Weight in Large Language Models**|Mengxia Yu et.al.|[2411.07191](http://arxiv.org/abs/2411.07191)|**[link](https://github.com/mengxiayu/llmsuperweight)**|**近期的研究表明，大型语言模型（LLM）中的少量参数异常值对模型的质量至关重要。尽管LLM包含数十亿个参数，但这些小比例的异常值，如0.01%，相当于数万个参数。在这项工作中，我们提出了一个更为惊人的发现：仅仅剪枝一个参数就能破坏LLM生成文本的能力——使困惑度增加三个数量级，并将零样本准确率降低到随机猜测水平。我们提出了一种无需数据的方法，通过单次前向传递模型来识别这些参数，称为超级权重。我们进一步发现，这些超级权重会引起相应罕见且大的激活异常值，称为超级激活。当超级激活被高精度保留时，简单的四舍五入量化可以变得与最先进的方法竞争。对于权重量化，我们同样发现，通过保留超级权重并裁剪其他权重异常值，四舍五入量化可以扩展到比以前考虑的更大的块大小。为了促进对超级权重的进一步研究，我们提供了常见且公开可用的LLM的超级权重坐标索引。**|
|**2024-11-11**|**NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics**|David Robinson et.al.|[2411.07186](http://arxiv.org/abs/2411.07186)|null|大型语言模型（LLMs）在文本和音频提示下的表现代表了各种听觉任务的最先进水平，包括语音、音乐和一般音频，并展示了在未见过的任务中的新兴能力。然而，这些能力尚未在生物声学任务中得到充分展示，例如在大量记录中检测动物发声、分类稀有和濒危物种以及标注情境和行为——这些任务对于保护、生物多样性监测和动物行为研究至关重要。在这项工作中，我们介绍了NatureLM-音频，这是第一个专门设计用于生物声学的音频-语言基础模型。我们的精心策划的训练数据集包含了涵盖生物声学、语音和音乐数据多样范围的文本-音频对，旨在解决该领域有限注释数据集带来的挑战。我们证明了从音乐和语音中学到的表示可以成功转移到生物声学，并且我们的模型在未见过的分类群和任务上显示出有希望的泛化能力。重要的是，我们在一个新的基准（BEANS-Zero）上测试NatureLM-音频，并在几个生物声学任务上，包括未见物种的零样本分类，设定了新的最先进技术（SotA）。为了推进生物声学研究，我们还开源了生成训练和基准数据的代码，以及训练模型的代码。|
|**2024-11-11**|**Continual Memorization of Factoids in Large Language Models**|Howard Chen et.al.|[2411.07175](http://arxiv.org/abs/2411.07175)|**[link](https://github.com/princeton-nlp/continual-factoid-memorization)**|**大型语言模型可以通过预训练吸收大量知识，但预训练对于获取长尾或专门事实效率低下。因此，对反映世界变化的新知识进行微调变得流行起来，尽管这可能会破坏模型的原有能力。我们研究了这种脆弱性在持续记忆背景下的表现，即模型在一个小的长尾事实集上进行训练，并且在多次后续训练其他数据集后仍需保留这些事实。通过广泛的实验，我们发现LLM在多个后续任务中存在遗忘问题，简单的重放技术并不能完全防止遗忘，特别是在事实集在后期阶段进行训练时。我们认为有两条途径可以减轻遗忘：1）保护模型学习事实的过程；2）减少后期训练中的干扰。基于这一见解，我们开发了一种有效的缓解策略：REMIX（随机和通用数据混合）。REMIX通过在每个阶段混合来自预训练语料库或甚至随机生成的单词序列的通用数据来防止遗忘，尽管这些数据与第一阶段记忆的事实无关。REMIX能够在严重遗忘的情况下恢复性能，通常优于那些在第一阶段可以访问事实的基于重放的方法。然后我们分析了REMIX如何改变学习过程，并发现成功防止遗忘与一个模式相关：模型比平常更早地存储事实，并且多样化存储这些事实的层。REMIX的有效性为进一步研究记忆和遗忘的潜在动态开启了令人兴奋的研究可能性。**|
|**2024-11-11**|**A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis: Evaluating Mental Health Sentiment on Social Media during COVID-19**|Vedant Khandelwal et.al.|[2411.07163](http://arxiv.org/abs/2411.07163)|null|在健康危机（如COVID-19大流行）期间，通过社交媒体监控公众情绪可能是有帮助的。然而，传统的基于频率的数据驱动神经网络方法可能会错过新出现的相关内容，因为语言在动态变化的环境中不断演变。人为编写的符号知识源，例如标准语言和俚语词汇表，可能有助于提升动态语言中的社交媒体信号。我们介绍了一种神经符号方法，该方法结合了神经网络与符号知识源，增强了对与COVID-19相关的心理健康相关推文的检测和解释能力。我们的方法使用了大型数据集（约120亿条推文、250万条子版块数据和70万篇新闻文章）以及多个知识图谱进行了评估。这种方法能够动态适应不断变化的语言，在F1评分上超过了纯数据驱动模型，得分超过92%。此外，这种方法在适应新数据方面也更快，并且比微调预训练的大规模语言模型（LLM）具有更低的计算需求。本研究展示了神经符号方法在动态环境中进行文本解释（如健康监测）任务中的优势。|
|**2024-11-11**|**Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models**|Yancheng He et.al.|[2411.07140](http://arxiv.org/abs/2411.07140)|null|新提出的大型语言模型（LLMs）评估基准对于跟上LLMs的快速发展至关重要。在这项工作中，我们提出了Chinese SimpleQA，这是第一个全面的中文基准，用于评估语言模型回答简短问题的事实准确性能力。Chinese SimpleQA 主要有五个特性：中文、多样性、高质量、静态和易于评估。具体来说，首先，我们专注于涵盖6个主要话题和99个多样化子话题的中文语言。其次，我们进行了全面的质量控制过程，以确保问题和答案的高质量，其中参考答案是静态的，不会随时间改变。第三，遵循SimpleQA的原则，问题和答案都非常简短，并且根据OpenAI API进行易于评估的评分过程。基于Chinese SimpleQA，我们对现有LLMs的事实准确性能力进行了全面评估。最后，我们希望Chinese SimpleQA 能够帮助开发者更好地了解其模型的中文事实准确性能力，并促进基础模型的发展。|
|**2024-11-08**|**Recycled Attention: Efficient inference for long-context language models**|Fangyuan Xu et.al.|[2411.05787](http://arxiv.org/abs/2411.05787)|null|在给定长上下文输入的情况下生成长序列的标记对大型语言模型（LLMs）施加了沉重的计算负担。其中一个计算瓶颈来自于在每个生成步骤中对长序列输入计算注意力。在本文中，我们提出了一种名为“循环注意力”的推理时间方法，该方法在全上下文注意力和仅针对输入标记子集的注意力之间交替进行。在执行部分注意力时，我们循环利用之前执行全注意力的标记的注意力模式，并仅关注最受关注的前K个标记，从而减少了数据移动和注意力计算的成本。与之前提出的仅关注局部上下文或具有高累积注意力分数的标记的推理时间加速方法相比，我们的方法灵活地选择了与当前解码步骤相关的标记。我们在RULER上评估了我们的方法，RULER是一组任务，旨在全面评估长上下文能力以及长上下文语言建模任务。将我们的方法应用于现成的LLMs可以实现与仅考虑局部上下文的基线相当的速度提升，同时性能提升了2倍。我们进一步探索了两种改进性能-效率权衡的方法：（1）根据查询相似性动态决定何时执行循环注意力或全注意力步骤；（2）继续使用循环注意力对模型进行预训练。|
|**2024-11-08**|**Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?**|Veronica Chatrath et.al.|[2411.05775](http://arxiv.org/abs/2411.05775)|null|政治性错误信息对民主进程构成了重大挑战，影响了公众舆论和对媒体的信任。传统的手动事实核查方法存在可扩展性和注释者偏见的问题，而机器学习模型则需要大规模且成本高昂的标注数据集。本研究探讨了使用最先进的大型语言模型（LLMs）作为可靠的注释器来检测新闻文章中的政治真实性。我们利用开源的LLMs创建了一个具有政治多样性的数据集，并通过LLM生成的注释进行标注。这些注释由人类专家验证，并进一步通过基于LLM的判断者进行评估，以检验注释的准确性和可靠性。我们的方法提供了一种可扩展且稳健的替代传统事实核查的方案，增强了媒体透明度和公众信任。|
|**2024-11-08**|**Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024**|Christopher Malon et.al.|[2411.05762](http://arxiv.org/abs/2411.05762)|null|分离网络上的虚假信息和事实长期以来一直考验着人类的搜索和推理能力。我们展示了大型语言模型（LLMs）的推理能力和现代搜索引擎的检索能力可以结合起来，以自动化这一过程并可解释地验证主张。我们将LLMs和搜索结合在一个多跳证据追索策略下。该策略使用一个序列到序列模型根据输入的主张生成初始问题，搜索并回答该问题，并使用LLM迭代生成后续问题以追求缺失的证据。我们在FEVER 2024（AVeriTeC）共享任务上展示了我们的系统。与一次性生成所有问题的策略相比，我们的方法获得了0.045更高的标签准确率和0.155更高的AVeriTeC评分（评估证据的充分性）。通过消融研究，我们展示了各种设计选择的重要性，如问题生成方法、中等大小的上下文、一次对一份文档进行推理、添加元数据、改写、将问题简化为两类以及重新考虑最终判决。我们的提交系统在开发集上达到了0.510的AVeriTeC评分，在测试集上达到了0.477的AVeriTeC评分。|
|**2024-11-08**|**Unmasking the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal**|Fuka Matsuzaki et.al.|[2411.05665](http://arxiv.org/abs/2411.05665)|**[link](https://github.com/isfhub/maskcode)**|**本文通过严格评估大型语言模型（LLMs）处理被遮蔽文本的能力，揭示了其存在的局限性。我们引入了两个新任务：MskQA，用于衡量在像RealtimeQA这样的被遮蔽问答数据集上的推理能力；MskCal，用于评估在被遮蔽算术问题上的数值推理能力。通过对GPT-4o和4o-mini的测试发现，虽然这些大语言模型在处理被遮蔽文本时表现出一定的韧性，但它们的表现高度依赖于遮蔽率和语义线索。特别是当“完全遮蔽”时，即没有语义线索可用的情况下，性能显著下降，这表明大语言模型依赖于表面模式。有趣的是，GPT-4o在MskCal中的表现始终优于4o-mini，显示出更强的处理被遮蔽文本中的数值推理能力。这强调了语义线索在大语言模型推理过程中的关键作用。我们的研究揭示了背景知识与推理能力在处理被遮蔽文本时的相互作用，为进一步理解大语言模型的能力和局限性铺平了道路，并突显了需要更稳健的评估方法来准确评估它们的真实理解能力。**|
|**2024-11-08**|**The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent**|Leon O. H. Kroczek et.al.|[2411.05653](http://arxiv.org/abs/2411.05653)|null|大型语言模型（LLMs）在对话任务中展示了卓越的能力。将LLM具象化为虚拟人类允许用户在虚拟现实中进行面对面的社交互动。然而，在虚拟现实中的LLM控制代理与人之间的社交互动中，人格和任务相关因素的影响尚不清楚。在这项研究中，46名参与者与一个虚拟代理进行了互动，该代理的人格被操纵为外向或内向，并在三种不同的对话任务（闲聊、知识测试、说服）中进行了测试。通过评分评估了社会评价、情感体验和真实感。互动参与度通过量化参与者的词汇量和对话轮次来测量。最后，我们测量了参与者在知识测试期间寻求代理帮助的意愿。我们的研究结果表明，外向型代理得到了更积极的评价，引发了更愉快的体验和更高的参与度，并被认为比内向型代理更具真实性。然而，人格并没有影响求助倾向，但当参与者得到LLM的帮助时，他们通常对自己获得的答案更有信心。因此，LLM控制的具象化虚拟代理的人格特征变化会影响虚拟互动中的社会情感处理和行为。具象化的虚拟代理允许在虚拟环境中展示自然的社会互动。|
|**2024-11-08**|**LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution**|Yuheng Zhao et.al.|[2411.05651](http://arxiv.org/abs/2411.05651)|null|可视化分析（VA）要求分析师根据观察结果迭代地提出分析任务，并通过创建可视化和交互式探索来执行这些任务以获得洞察。这一过程需要编程、数据处理和可视化工具方面的技能，突显了对更智能、更精简的VA方法的需求。最近开发的大语言模型（LLM）作为代理，具备动态规划和使用工具的能力，为增强VA的效率和多功能性提供了潜力。我们提出了LightVA，这是一种轻量级的VA框架，通过人机协作支持任务分解、数据分析和交互式探索。我们的方法旨在帮助用户逐步将高层次的分析目标转化为低层次的任务，生成可视化并得出洞见。具体而言，我们引入了一种基于LLM代理的任务规划和执行策略，采用涉及规划者、执行者和控制器的递归过程。规划者负责推荐和分解任务，执行者处理任务执行，包括数据分析、可视化生成和多视图组合，而控制器则协调规划者和执行者之间的互动。在此框架基础上，我们开发了一个具有混合用户界面的系统，包括用于监控和管理任务规划过程的任务流程图、用于交互式数据探索的可视化面板以及用于通过自然语言指令引导模型的聊天视图。我们通过使用场景和专家研究检验了该方法的有效性。|
|**2024-11-08**|**Evaluating Large Language Model Capability in Vietnamese Fact-Checking Data Generation**|Long Truong To et.al.|[2411.05641](http://arxiv.org/abs/2411.05641)|null|大型语言模型（LLMs）随着阅读理解和推理能力的逐步提高，被应用于各种复杂的语言任务，包括为各种目的自动生成语言数据。然而，针对低资源语言如越南语的自动数据生成研究仍不充分，缺乏全面的评估。在本文中，我们探索了使用LLMs进行越南语事实核查任务的自动数据生成，该任务面临显著的数据限制。具体而言，我们关注从多条证据句子中合成声明的事实核查数据，以评估LLMs的信息合成能力。我们开发了一种使用简单提示技术的自动数据构建过程，并探索了几种提高生成数据质量的方法。为了评估LLMs生成数据的质量，我们进行了手动质量评估和使用语言模型的性能评估。实验结果和手动评估表明，尽管通过微调技术显著提高了生成数据的质量，但LLMs仍然无法与人类产生的数据质量相匹配。|
|**2024-11-08**|**Assessing Open-Source Large Language Models on Argumentation Mining Subtasks**|Mohammad Yeghaneh Abkenar et.al.|[2411.05639](http://arxiv.org/abs/2411.05639)|null|我们探讨了四种开源大型语言模型（LLMs）在论辩挖掘（AM）中的能力。我们在三个不同的语料库上进行实验：说服性文章（PE）、论辩微文本（AMT）第一部分和第二部分，基于两个论辩挖掘子任务：（i）论辩话语单元分类（ADUC）和（ii）论辩关系分类（ARC）。本研究旨在评估包括Mistral 7B、Mixtral8x7B、LlamA2 7B和LlamA3 8B在内的开源LLMs在零样本和少量样本场景下的论辩能力。我们的分析有助于未来的研究进一步评估使用开源LLMs进行计算论辩。|
|**2024-11-08**|**A Two-Step Concept-Based Approach for Enhanced Interpretability and Trust in Skin Lesion Diagnosis**|Cristiano Patrício et.al.|[2411.05609](http://arxiv.org/abs/2411.05609)|**[link](https://github.com/cristianopatricio/2-step-concept-based-skin-diagnosis)**|临床环境中深度学习系统应用的主要障碍是标注数据的稀缺以及对这些系统的可解释性和信任度不足。概念瓶颈模型（CBMs）通过将最终疾病预测限制在一组人类可理解的概念上来提供内在的可解释性。然而，这种内在的可解释性以更大的注释负担为代价，此外，添加新概念需要重新训练整个系统。在这项工作中，我们介绍了一种新颖的两步方法来解决这两个挑战。通过模拟CBM的两个阶段，我们利用预训练的视觉语言模型（VLM）自动预测临床概念，然后利用大型语言模型（LLM）基于预测的概念生成疾病诊断。我们在三个皮肤病变数据集上验证了我们的方法，结果表明该方法优于传统的CBMs和最先进的可解释方法，且无需任何训练，并仅使用少量标注示例。代码可在https://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis获取。|
|**2024-11-08**|**Evaluating and Adapting Large Language Models to Represent Folktales in Low-Resource Languages**|JA Meaney et.al.|[2411.05593](http://arxiv.org/abs/2411.05593)|null|民间故事是了解一个文明的社会和文化的宝贵资源。数字民俗研究旨在利用自动化技术更好地理解这些民间故事，并依赖于文本数据的抽象表示。尽管许多大型语言模型（LLMs）声称能够表示像爱尔兰语和盖尔语这样低资源的语言，我们提出了两个分类任务来探索这些表示的有用性，并进行了三种适应性改进以提高这些模型的性能。我们发现，调整模型以处理更长的序列，并继续在民间故事领域进行预训练可以提高分类性能，但这些发现受到基线支持向量机（SVM）使用非上下文特征时表现优异的影响。|
|**2024-11-07**|**SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models**|Muyang Li et.al.|[2411.05007](http://arxiv.org/abs/2411.05007)|**[link](https://github.com/mit-han-lab/deepcompressor)**|**扩散模型在生成高质量图像方面已被证明非常有效。然而，随着这些模型变得更大，它们需要显著更多的内存，并且延迟更高，这给部署带来了重大挑战。在这项工作中，我们旨在通过将权重和激活量化到4位来加速扩散模型。在如此激进的量化级别上，权重和激活都非常敏感，传统的用于大型语言模型的后训练量化方法如平滑处理变得不够充分。为了解决这一限制，我们提出了一种新的4位量化范式SVDQuant。与平滑处理不同，后者在权重和激活之间重新分配异常值，我们的方法使用低秩分支吸收这些异常值。我们首先通过将异常值从激活转移到权重来集中这些异常值，然后利用高精度低秩分支并通过奇异值分解（SVD）处理权重异常值。这一过程减轻了两侧的量化难度。然而，简单地独立运行低秩分支会由于激活的额外数据移动导致显著的开销，从而抵消了量化带来的速度提升。为了解决这个问题，我们设计了一个名为Nunchaku的推理引擎，将低秩分支的内核融合到低比特分支中，以减少冗余的内存访问。它还可以无缝支持现成的低秩适配器（LoRAs），而无需重新量化。我们在SDXL、PixArt- $\Sigma$ 和FLUX.1上的广泛实验验证了SVDQuant在保持图像质量方面的有效性。我们将12B FLUX.1模型的内存使用减少了3.5倍，在16GB笔记本电脑4090 GPU上比4位仅权重量化基线快3.0倍，为PC上的更多交互式应用铺平了道路。我们的量化库和推理引擎已开源。**|
|**2024-11-07**|**Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?**|Jonathan Roberts et.al.|[2411.05000](http://arxiv.org/abs/2411.05000)|null|随着大型语言模型（LLMs）的上下文限制增加，可能的应用和下游功能范围也在扩大。在许多实际任务中，决策依赖于分散在多个通常不相关的文档中的细节，这些文档大多包含无关信息。长上下文LLMs似乎非常适合这种复杂的信息检索和推理，这在传统上证明是昂贵且耗时的。然而，尽管近年来较长上下文模型的发展取得了快速进展，但我们对LLMs如何有效使用其上下文的理解并未跟上步伐。为了解决这个问题，我们进行了一系列检索实验，旨在评估17个领先LLMs的能力，例如它们通过上下文窗口跟踪信息线索的能力。令人惊讶的是，我们发现许多模型具有非常强的“线程安全”能力：能够在不显著降低性能的情况下同时跟踪多个线索。然而，对于许多模型，我们发现有效的上下文限制明显短于支持的上下文长度，在上下文窗口增大时准确度会下降。我们的研究还强调了一个重要观点，即不同分词器的token计数不应直接比较——它们通常对应着显著不同的书面字符数量。我们将代码和长上下文实验数据公开。|
|**2024-11-07**|**LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation**|Weiquan Huang et.al.|[2411.04997](http://arxiv.org/abs/2411.04997)|**[link](https://github.com/microsoft/LLM2CLIP)**|**CLIP是当今最重要的多模态基础模型之一。其强大的能力源自自然语言提供的丰富监督信号，自然语言作为人类知识的载体，塑造了一个强大的跨模态表示空间。然而，随着大型语言模型（LLM）如GPT-4和LLaMA的快速发展，语言理解和生成的边界不断被拓展。这引发了一个有趣的问题：能否利用LLM的能力进一步提升多模态表示学习？将LLM整合到CLIP中的潜在好处显而易见。LLM在文本理解方面具有很强的能力，可以从根本上提高CLIP处理图像描述的能力，显著增强其处理长且复杂文本的能力，这是原始CLIP的一个已知限制。此外，LLM在庞大的文本语料库上进行训练，拥有开放世界的知识。这使其在训练过程中能够扩展描述信息，从而提高学习过程的效率。在本文中，我们提出了一种名为LLM2CLIP的新方法，该方法利用LLM的力量来释放CLIP的潜力。通过在描述空间中使用对比学习微调LLM，我们将文本能力提取到输出嵌入中，显著提高了输出层的文本辨别力。然后，我们设计了一种高效的训练过程，在此过程中，微调后的LLM作为CLIP视觉编码器的强大教师。由于LLM的存在，我们现在可以使用更长、更复杂的描述，而不受原始CLIP文本编码器上下文窗口和能力限制的影响。我们的实验表明，这种方法在跨模态任务中带来了显著的改进。**|
|**2024-11-07**|**Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models**|Weixin Liang et.al.|[2411.04996](http://arxiv.org/abs/2411.04996)|null|大型语言模型（LLMs）的发展已经扩展到多模态系统，这些系统能够在统一框架内处理文本、图像和语音。然而，训练这些模型需要比仅处理文本的LLMs显著更大的数据集和计算资源。为了解决这一扩展挑战，我们引入了Mixture-of-Transformers（MoT），这是一种稀疏的多模态变压器架构，能够显著减少预训练所需的计算成本。MoT通过模态解耦模型的非嵌入参数——包括前馈网络、注意力矩阵和层归一化——从而实现特定于模态的处理，并对整个输入序列进行全局自注意力处理。我们在多个设置和模型规模下评估了MoT的表现。在Chameleon 7B设置（用于文本和图像生成的自回归任务）中，MoT使用仅为密集基线模型55.8%的浮点运算（FLOPs）就达到了与之相当的性能。当扩展到包括语音时，MoT在语音性能上达到与密集基线模型相当的水平，但只使用了37.2%的FLOPs。在Transfusion设置中，其中文本和图像以不同的目标进行训练，一个7B的MoT模型在图像模态上的表现与密集基线模型相当，但使用的FLOPs仅为后者的一半；而一个760M的MoT模型在关键的图像生成指标上优于一个1.4B的密集基线模型。系统分析进一步展示了MoT的实际优势，在AWS p4de.24xlarge实例（配备NVIDIA A100 GPU）上，MoT实现了与密集基线模型相当的图像质量，所需时间仅为后者的47.2%，而在文本质量方面则只需75.6%的时间。|
|**2024-11-07**|**Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives**|Hao Sun et.al.|[2411.04991](http://arxiv.org/abs/2411.04991)|**[link](https://github.com/holarissun/rewardmodelingbeyondbradleyterry)**|**Bradley-Terry（BT）模型在大型语言模型（LLM）对齐的奖励建模中是一种常见且成功的方法。然而，目前尚不清楚为什么最初为多玩家随机博弈匹配而开发的这一模型可以被用于将成对响应比较转换为奖励值并进行预测，尤其是在只有有限数量的提示-响应对稀疏地与其他对进行比较的情况下。在本文中，我们首先重新审视了使用BT模型进行奖励建模的基础，并基于使用嵌入的深度神经网络建立了BT奖励模型的收敛率，为其使用提供了理论基础。尽管从理论上讲是合理的，但我们认为从下游优化的角度来看，BT模型并不是必要的选择。这是因为奖励模型只需要通过真奖励的单调变换来保持正确的排名预测。我们强调了奖励建模中的关键概念——顺序一致性，并证明BT模型具备这种特性。因此，我们提出了一种简单直接的上限算法，与现成的二元分类器兼容，作为顺序一致的奖励建模目标。为了提供实用的见解，我们在超过12,000个实验设置中对这些不同的奖励建模方法进行了实证评估，使用了6种基础LLM、2个数据集以及多样化的注释设计，这些设计在数量、质量和偏好注释的配对选择上有所不同。**|
|**2024-11-07**|**Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries**|Dylan Manuel et.al.|[2411.04981](http://arxiv.org/abs/2411.04981)|null|安全专家通过反汇编（反编译）二进制代码来识别关键的安全漏洞。在关键基础设施（CI）中使用的固件、驱动程序和专有软件等重要系统中，源代码的访问受限，这使得在二进制级别上进行这种分析变得尤为重要。即使有可用的源代码，编译后源代码与处理器执行的二进制代码之间也存在语义差距，这可能阻碍对源代码中漏洞的检测。目前，关于大型语言模型（LLMs）的研究忽略了在这一领域分析反编译二进制代码的重要性，而仅关注源代码。在这项工作中，我们首次通过实证研究揭示了最先进的LLMs在分析反编译二进制代码中的漏洞时存在的重大语义限制，主要是由于缺乏相关数据集。为了弥补这一差距，我们引入了DeBinVul，这是一个新的反编译二进制代码漏洞数据集。我们的数据集是多架构和多优化的，专注于C/C++，因为它们在CI中的广泛应用以及与众多漏洞的关联。具体来说，我们为任务（i）识别；（ii）分类；（iii）描述漏洞；以及（iv）恢复反编译二进制代码中的函数名称，整理了150,872个漏洞和非漏洞样本。随后，我们使用DeBinVul微调了最先进的LLMs，并报告了CodeLlama、Llama3和CodeGen2在检测二进制代码漏洞方面的性能分别提高了19%、24%和21%。此外，使用DeBinVul，我们在漏洞分类任务中报告了80-90%的高性能。另外，我们还报告了在函数名称恢复和漏洞描述任务中的性能提升。|
|**2024-11-07**|**SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference**|Gabriele Oliaro et.al.|[2411.04975](http://arxiv.org/abs/2411.04975)|null|我们提出了SuffixDecoding，这是一种新颖的无模型方法，通过推测性解码来加速大型语言模型（LLM）的推理过程。与依赖于草稿模型或专门解码头的现有方法不同，SuffixDecoding利用从先前生成的输出构建的后缀树来高效预测候选令牌序列。我们的方法能够在不增加维护和协调额外模型开销的情况下实现灵活的树结构推测。SuffixDecoding构建并动态更新后缀树以捕捉生成文本中的模式，并使用基于经验令牌频率的有原则的评分机制来构建推测树。SuffixDecoding仅需要CPU内存，而典型的LLM服务节点上这种内存非常充裕且未充分利用。我们证明了SuffixDecoding在各种工作负载下，包括开放式聊天、代码生成和文本到SQL任务，实现了与基于模型的方法竞争的速度提升。对于开放式聊天和代码生成任务，SuffixDecoding实现了高达1.4倍的输出吞吐量提升，以及比SpecInfer低1.1倍的时间每令牌（TPOT）延迟。对于一个专有的多LLM文本到SQL应用，SuffixDecoding实现了高达2.9倍的输出吞吐量提升和3倍的延迟降低。我们的评估表明，即使参考语料库很小，只有256个示例，SuffixDecoding也能保持高接受率，同时随着更多历史输出的纳入，性能继续提升。|
|**2024-11-07**|**BitNet a4.8: 4-bit Activations for 1-bit LLMs**|Hongyu Wang et.al.|[2411.04965](http://arxiv.org/abs/2411.04965)|null|最近的研究表明，如BitNet b1.58这样的1比特大语言模型（LLMs）在减少推理成本的同时保持了模型性能，这是一个很有前景的方向。在这项工作中，我们介绍了BitNet a4.8，它通过启用4比特激活来实现1比特LLMs的优化。BitNet a4.8采用了一种混合量化和稀疏化策略，以减轻由异常通道引入的量化误差。具体来说，我们在注意力和前馈网络层的输入中使用4比特激活，同时对中间状态进行稀疏化处理，并随后进行8比特量化。广泛的实验表明，BitNet a4.8在等效的训练成本下实现了与BitNet b1.58相当的性能，同时由于启用了4比特（INT4/FP4）内核而加快了推理速度。此外，BitNet a4.8仅激活55%的参数，并支持3比特KV缓存，进一步提高了大规模LLM部署和推理的效率。|
|**2024-11-07**|**Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability**|Yanjun Gao et.al.|[2411.04962](http://arxiv.org/abs/2411.04962)|null|大型语言模型（LLMs）正在被探索用于诊断决策支持，但它们估计先验概率的能力，这对于临床决策至关重要，仍然有限。本研究评估了两种大型语言模型，即Mistral-7B和Llama3-70B，使用结构化的电子健康记录数据在三个诊断任务上进行测试。我们检查了从LLM概率估计中提取的三种当前方法，并揭示了它们的局限性。我们的目的是强调改进LLM置信度估计技术的需求。|
|**2024-11-07**|**CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM**|Jingwei Xu et.al.|[2411.04954](http://arxiv.org/abs/2411.04954)|null|本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，该系统能够根据用户的输入（如文本描述、图像、点云或它们的组合）轻松生成CAD模型。为此，我们引入了CAD-MLLM，这是第一个能够在多模态输入条件下生成参数化CAD模型的系统。具体来说，在CAD-MLLM框架内，我们利用CAD模型的命令序列，并采用先进的大型语言模型（LLM）来对齐这些不同多模态数据和CAD模型向量表示之间的特征空间。为了促进模型训练，我们设计了一个全面的数据构建和注释管道，使每个CAD模型都配备相应的多模态数据。由此产生的数据集名为Omni-CAD，是首个包含每个CAD模型的文字描述、多视角图像、点云以及命令序列的多模态CAD数据集。该数据集包含大约450,000个实例及其CAD构造序列。为了全面评估生成的CAD模型的质量，我们超越了当前侧重于重建质量的评估指标，引入了额外的指标来评估拓扑质量和表面封闭程度。广泛的实验结果表明，CAD-MLLM显著优于现有的条件生成方法，并且在面对噪声和缺失点时仍然保持高度稳健。项目页面和更多可视化内容可以在https://cad-mllm.github.io/找到。|
|**2024-11-06**|**Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?**|Daniel P. Jeong et.al.|[2411.04118](http://arxiv.org/abs/2411.04118)|null|近年来，有几项研究致力于开发专门用于医学应用的基础模型，通过在公开的生物医学语料库上进行持续预训练来改进通用大型语言模型（LLMs）和视觉-语言模型（VLMs）。这些研究通常声称这种领域适应性预训练（DAPT）能提高下游医学任务的表现，例如回答医学执照考试问题。在本文中，我们比较了七个公共“医学”LLMs和两个VLMs与其对应的基线模型，并得出了不同的结论：所有医学VLMs和几乎所有医学LLMs在零样本/少样本提示下进行医学问答（QA）任务时，并没有始终如一地优于其基线模型。例如，在我们考虑的3次提示设置下的任务和模型对中，医学LLMs仅在其基线模型表现更差的情况下占12.1%，与基线模型持平的情况占49.8%，而在其余38.2%的情况下则显著逊色于基线模型。我们的结论基于以下几点：(i) 将每个医学模型直接与对应的基线模型进行头对头比较；(ii) 为每个模型单独优化提示；(iii) 考虑到比较中的统计不确定性。虽然这些基本做法在文献中并不总是被采用，但我们的消融实验表明，它们对结论有重大影响。我们的发现表明，最先进的通用领域模型可能已经具备较强的医学知识和推理能力，并提出了加强未来研究结论的建议。|
|**2024-11-06**|**How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis**|Guan Zhe Hong et.al.|[2411.04105](http://arxiv.org/abs/2411.04105)|null|大型语言模型（LLMs）在需要规划和推理的任务上表现出色。受此启发，我们研究了网络执行复杂逻辑推理的内部机制。我们首先构建了一个命题逻辑问题作为网络训练和评估的具体测试平台。关键在于，这个问题需要非平凡的规划才能解决，但我们能够训练一个小的变压器以实现完美的准确性。在此设置的基础上，我们进一步研究了一个三层变压器，从零开始训练它如何解决这个问题。我们能够识别出某些“规划”和“推理”电路，这些电路需要注意力块之间的合作来实现所需的逻辑。为了扩展我们的发现，我们还研究了更大的模型Mistral 7B。通过激活修补技术，我们描述了内部组件中对解决我们的逻辑问题至关重要的部分。总体而言，我们的工作系统地揭示了小型和大型变压器的新颖方面，并继续研究它们如何进行规划和推理。|
|**2024-11-06**|**Textual Decomposition Then Sub-motion-space Scattering for Open-Vocabulary Motion Generation**|Ke Fan et.al.|[2411.04079](http://arxiv.org/abs/2411.04079)|null|文本到动作生成是计算机视觉中的一个重要任务，通过给定的文本生成目标三维动作。现有的标注数据集规模有限，导致大多数现有方法对小数据集过拟合，无法推广到开放域的动作。一些方法试图通过与CLIP空间对齐或使用预训练然后微调的方法来解决开放词汇动作生成问题。然而，当前标注数据集的有限规模使得它们只能实现从子文本空间到子动作空间的映射，而不是实现全文本空间和全动作空间之间的映射（完整映射），这是实现开放词汇动作生成的关键。为此，本文提出利用原子动作（短时间内的简单身体部分动作）作为中间表示，并采用两个有序耦合步骤，即文本分解和子动作空间散射，以解决完整映射问题。对于文本分解，我们设计了一种细粒度描述转换算法，并结合大型语言模型的泛化能力，将任何给定的动作文本转换为原子文本。子动作空间散射学习从原子动作到目标动作的合成过程，使学习到的子动作空间散射形成完整的动作空间。对于给定的开放域动作，它将外推转换为插值，从而显著提高泛化能力。我们的网络 $DSO$-Net结合了文本分解和子动作空间散射来解决开放词汇动作生成问题。广泛的实验表明，我们的$DSO$ -Net在开放词汇动作生成方面显著优于最先进的方法。代码可在<https://vankouf.github.io/DSONet/>获取。|
|**2024-11-06**|**Beemo: Benchmark of Expert-edited Machine-generated Outputs**|Ekaterina Artemova et.al.|[2411.04032](http://arxiv.org/abs/2411.04032)|null|大规模语言模型（LLMs）的迅速普及增加了机器生成文本（MGTs）的数量，并在各种领域模糊了文本作者身份。然而，大多数现有的MGT基准测试包括单一作者的文本（人类撰写和机器生成）。这种传统的设计未能捕捉到更实际的多作者场景，在这些场景中，用户会根据自然流畅性、连贯性和事实准确性对LLM的响应进行润色。我们的论文介绍了Beemo基准，这是一个包含6500篇由人类撰写、十个经过指令微调的LLM生成并由专家编辑的文本的数据集，涵盖了从创意写作到总结的各种应用场景。此外，Beemo还包括13100篇机器生成并经过LLM编辑的文本，从而允许对各种MGT检测进行多样化评估。我们记录了Beemo的创建协议，并展示了在不同实验设置下基准测试33种MGT检测配置的结果。我们发现，基于专家的编辑能够逃避MGT检测，而经过LLM编辑的文本不太可能被识别为人类撰写。Beemo及其所有材料均公开可用。|
|**2024-11-06**|**Prompt Engineering Using GPT for Word-Level Code-Mixed Language Identification in Low-Resource Dravidian Languages**|Aniket Deroy et.al.|[2411.04025](http://arxiv.org/abs/2411.04025)|null|语言识别（LI）对于各种自然语言处理任务至关重要，是情感分析、机器翻译和信息检索等应用的基础步骤。在印度这样的多语言社会中，特别是在年轻人在社交媒体上交流时，文本常常表现出代码混合现象，即在不同语言层次上将本地语言与英语混合。这种现象给LI系统带来了巨大挑战，尤其是在单个单词内语言交织的情况下。德拉威语系语言在印度南部广泛使用，具有丰富的形态结构，但由于数字平台上的代表性不足，导致采用罗马字母或混合脚本进行交流。本文介绍了一种基于提示的方法，旨在解决德拉威语系语言在单词级语言识别中的挑战。在这项工作中，我们利用GPT-3.5 Turbo来了解大型语言模型是否能够正确地对单词进行分类。我们的研究结果表明，卡纳达语模型在大多数指标上均优于泰米尔语模型，显示出更高的准确性和可靠性，能够在识别和分类卡纳达语实例方面表现更好。相比之下，泰米尔语模型表现一般，特别是在精确度和召回率方面需要改进。|
|**2024-11-06**|**Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning**|Jiawei Yao et.al.|[2411.03978](http://arxiv.org/abs/2411.03978)|null|多聚类旨在从不同方面发现数据中的各种潜在结构。深度多聚类方法通过利用数据中的复杂模式和关系取得了显著的性能。然而，现有的工作在灵活适应用户特定需求的数据分组方面存在困难，这可能需要手动理解每种聚类。为了解决这些限制，我们在本文中引入了Multi-Sub，这是一种新颖的端到端多聚类方法，采用了多模态子空间代理学习框架。通过利用CLIP和GPT-4的协同能力，Multi-Sub将表达用户偏好的文本提示与其对应的视觉表示对齐。这是通过自动从大规模语言模型生成代理词来实现的，这些代理词充当子空间基底，从而允许根据用户的兴趣定制数据表示。我们的方法在广泛的视觉多聚类任务数据集上始终优于现有基线。我们的代码可在https://github.com/Alexander-Yao/Multi-Sub获取。|
|**2024-11-06**|**What Really is Commonsense Knowledge?**|Quyet V. Do et.al.|[2411.03964](http://arxiv.org/abs/2411.03964)|null|常识数据集在自然语言处理领域得到了很好的发展，主要通过众包方式进行人工标注。然而，对于常识推理基准存在一些争议。具体来说，某些常识基准中的很大一部分实例并不涉及常识知识。这一问题会削弱对评估模型真正常识推理能力的测量。此外，这个问题源于常识知识概念模糊，与其它类型的知识区分不明显。为了澄清所有上述主张，本研究调查了现有常识知识的定义，基于三个框架来定义概念，并将它们整合成一个多框架统一的常识知识定义（即综合定义）。然后，我们使用综合定义对CommonsenseQA和CommonsenseQA 2.0数据集进行标注和实验，以检验上述主张。我们的研究表明，在这两个数据集中存在大量非常识知识实例，而且在这两个子集上大型语言模型（LLMs）的表现较差，特别是在常识知识实例上。|
|**2024-11-06**|**How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?**|Zhangcheng Qiang et.al.|[2411.03962](http://arxiv.org/abs/2411.03962)|null|在许多本体匹配（OM）系统中，已经实现了包含分词、标准化、停用词去除和词干提取/词形还原的通用文本预处理管道。然而，文本预处理标准的缺乏导致了映射结果的多样性。本文研究了文本预处理管道在句法层面的OM任务中的影响。通过对8个本体对齐评估倡议（OAEI）轨道存储库中的49个不同对齐实验表明：（1）分词和标准化目前比停用词去除和词干提取/词形还原更有效；（2）词形还原和词干提取的选择是任务特定的。我们建议使用独立的词形还原或词干提取，并辅以后续校正。我们发现（3）Porter词干提取器和Snowball词干提取器的表现优于Lancaster词干提取器；（4）词性标注对词形还原没有帮助。为了修复OM任务中效果不佳的停用词去除和词干提取/词形还原，我们提出了一种新的基于上下文的管道修复方法，该方法显著提高了匹配正确性和整体匹配性能。我们还讨论了大型语言模型（LLMs）时代中文本预处理管道的使用。|
|**2024-11-06**|**Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation**|Yuhang Liu et.al.|[2411.03957](http://arxiv.org/abs/2411.03957)|null|检索增强生成（RAG）已被证明是解决大型语言模型（LLMs）固有幻觉问题的有效方法。先前的方法通常基于语义相似性训练检索器，缺乏对RAG的优化。更近的研究提出了让检索器与LLMs的偏好信号对齐的方法。然而，这些偏好信号对于密集检索器（通常具有较弱的语言能力）来说往往难以理解和有效学习。受到指导发现学习等教学理论的启发，我们提出了一种新的框架FiGRet（细粒度引导检索器），该框架利用LLMs的语言能力从更精细、信息为中心的角度构建示例，以引导检索器的学习。具体而言，我们的方法利用LLMs从检索器表现不佳的样本中构建易于理解的示例，重点关注与RAG场景高度相关的三个学习目标：相关性、全面性和纯净性。这些示例作为脚手架，最终使检索器与LLMs的偏好对齐。此外，我们采用双重课程学习策略，并利用LLM和检索器之间的互反馈进一步提升RAG系统的性能。一系列实验表明，我们提出的框架提升了配备不同检索器的RAG系统的性能，并适用于各种LLMs。|
|**2024-11-06**|**Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks**|Felipe Marra et.al.|[2411.03948](http://arxiv.org/abs/2411.03948)|null|本文研究了文本到音频音乐生成模型在生成长篇音乐方面的表现，特别是在提示随时间变化的情况下，重点是为桌面角色扮演游戏（TRPG）创作配乐。我们介绍了一种名为Babel Bardo的系统，该系统利用大型语言模型（LLM）将语音转录转换为音乐描述，以控制文本到音乐的生成模型。在两个TRPG活动中比较了Babel Bardo的四个版本：一个基线版本使用直接的语音转录，以及三个基于LLM的版本，它们采用了不同的音乐描述生成方法。评估考虑了音频质量、故事情节的一致性和过渡的流畅性。结果表明，详细的音乐描述可以提高音频质量，而连续描述之间的一致性能增强故事情节的一致性和过渡的流畅性。|
|**2024-11-05**|**LLMs for Domain Generation Algorithm Detection**|Reynier Leyva La O et.al.|[2411.03307](http://arxiv.org/abs/2411.03307)|null|本文分析了使用大型语言模型（LLMs）来检测域名生成算法（DGAs）的应用。我们详细评估了两种重要技术：情境学习（ICL）和监督微调（SFT），展示了它们如何提高检测效果。SFT通过使用特定领域的数据提高了性能，而ICL则帮助检测模型快速适应新威胁，而无需大量的再训练。我们使用Meta的Llama3 8B模型，在一个自定义数据集上进行实验，该数据集包含了68个恶意软件家族和正常域名，涵盖了多个难以检测的方案，包括最近的基于词汇的DGAs。结果证明，基于LLM的方法在DGA检测方面可以达到具有竞争力的结果。特别是基于SFT的LLM DGA检测器在使用注意力层的最先进模型基础上实现了超越，达到了94%的准确率和4%的误报率，并且在检测基于词汇的DGA域名方面表现出色。|
|**2024-11-05**|**VERITAS: A Unified Approach to Reliability Evaluation**|Rajkumar Ramamurthy et.al.|[2411.03300](http://arxiv.org/abs/2411.03300)|null|大型语言模型（LLMs）通常无法从上下文中综合信息以生成准确的响应。这使得它们在知识密集型场景中变得不可靠，在这些场景中，输出的可靠性至关重要。对于可靠的LLM来说，集成一个强大的事实核查系统以检测各种格式中的幻觉是一个关键组成部分。虽然有一些开放访问的事实核查模型可用，但它们的功能往往局限于特定任务，如基于事实的问题回答或蕴涵验证，并且在对话设置中的表现较差。另一方面，封闭访问的模型如GPT-4和Claude提供了更大的灵活性，适用于不同的上下文，包括基于事实的对话验证，但受到高成本和延迟的限制。在这项工作中，我们介绍了VERITAS，这是一个幻觉检测模型家族，旨在灵活地跨多种上下文运行，同时最小化延迟和成本。VERITAS在所有主要幻觉检测基准上的平均性能达到了最先进的水平，与类似大小的模型相比，其平均性能提高了10%，并且接近GPT4涡轮在大模型作为裁判设置下的性能。|
|**2024-11-05**|**Examining Human-AI Collaboration for Co-Writing Constructive Comments Online**|Farhana Shahid et.al.|[2411.03295](http://arxiv.org/abs/2411.03295)|null|本文研究了大型语言模型（LLMs）如何帮助人们在涉及分裂性社会问题的在线辩论中撰写建设性的评论，并探讨了不同文化背景下对建设性的理解是否存在差异。通过针对来自印度和美国的600名参与者进行的控制实验，这些参与者审查并撰写了关于伊斯兰恐惧症和同性恋恐惧症在线帖子的建设性评论，我们发现LLMs与人类对在线评论的建设性感知存在潜在不一致。尽管LLM更倾向于认为辩证性评论更具建设性，但参与者更重视逻辑性和事实性。尽管存在这些差异，参与者仍认为LLM生成的和人机协作撰写的评论比独立由人类撰写的评论更具建设性。我们的分析还显示，LLM生成的和人机协作撰写的评论表现出更多与建设性相关的语言特征，相比人类撰写的关于分裂性话题的评论而言。当参与者使用LLMs来改进他们的评论时，最终的评论更长、更有礼貌、更积极、毒性更低且更易读，增加了论证特征，保留了原始意图，但偶尔会失去一些细微之处。基于这些发现，我们讨论了在利用LLMs促进在线建设性讨论时的伦理和设计考虑。|
|**2024-11-05**|**Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?**|Jingyu Xiao et.al.|[2411.03292](http://arxiv.org/abs/2411.03292)|**[link](https://github.com/webpai/interaction2code)**|将网页设计转换为功能性的用户界面代码是构建网站的关键步骤，但这一过程可能非常繁琐且耗时。为了自动化这一设计到代码的转换过程，已经提出了各种基于学习网络和多模态大语言模型（MLLMs）的方法。然而，这些研究仅在少量静态网页上进行评估，并忽略了动态交互元素，这使得它们在实际网站部署中的应用价值有限。为此，我们首次系统地研究了MLLMs在生成交互式网页方面的表现。具体来说，我们首先定义了交互到代码的任务，并构建了Interaction2Code基准数据集，该数据集包含97个独特的网页和213种不同的交互，涵盖15种网页类型和30种交互类别。然后，我们使用三种最先进的MLLMs进行了全面实验，结合自动度量指标和人工评估，总结出六个发现。实验结果突显了MLLMs在生成细粒度交互特征以及处理复杂转换和细微视觉修改的交互方面存在的局限性。我们进一步分析了失败案例及其根本原因，识别出10种常见的失败类型并评估了其严重程度。此外，我们的发现揭示了三个关键影响因素，即提示、视觉显著性和文本描述，这些因素可以提升MLLMs在交互生成方面的性能。基于这些发现，我们为研究人员和开发者提供了启示，为该领域的未来进展奠定了基础。数据集和源代码可在https://github.com/WebPAI/Interaction2Code获取。|
|**2024-11-05**|**The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare**|Souren Pashangpour et.al.|[2411.03287](http://arxiv.org/abs/2411.03287)|null|大型语言模型（LLMs）在医疗机器人中的潜在应用可以帮助应对全球医疗系统面临的巨大需求，特别是在人口老龄化和医疗专业人员短缺的情况下。尽管LLMs已经被整合到医学领域以协助医生和患者，但在临床环境中将LLMs集成到医疗机器人中尚未得到探索。在这篇视角论文中，我们研究了机器人技术和LLMs的突破性发展，以独特地确定设计面向健康领域的基于LLM的机器人的所需系统要求，包括通过人机交互（HRIs）进行多模态通信、语义推理和任务规划。此外，我们还讨论了这一新兴创新领域的伦理问题、开放挑战以及潜在的未来研究方向。|
|**2024-11-05**|**SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents**|Dawei Li et.al.|[2411.03284](http://arxiv.org/abs/2411.03284)|**[link](https://github.com/david-li0406/smoa)**|**尽管多智能体系统在各种任务和应用中显著提升了大型语言模型（LLMs）的性能，但智能体之间的密集交互可能会妨碍其效率和多样性。为了解决这些挑战，我们从稀疏混合智能体（SMoE）框架中汲取灵感，并提出了一种稀疏混合智能体（SMoA）框架，以提升多智能体LLMs的效率和多样性。与完全连接的结构不同，SMoA引入了响应选择和提前停止机制来稀疏化个体LLM智能体之间的信息流，从而在性能和效率之间取得平衡。此外，受SMoE框架中专家多样性原则的启发，我们为每个LLM智能体分配了不同的角色描述，促进了多样性和发散性思维。广泛的实验表明，在推理、对齐和公平性基准测试中，SMoA的表现与传统的混合智能体方法相当，但计算成本显著降低。进一步分析表明，SMoA更加稳定，具有更大的扩展能力，并且通过超参数优化提供了相当大的潜力。代码和数据将在：https://github.com/David-Li0406/SMoA 获取。**|
|**2024-11-05**|**Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities**|Ryosuke Takata et.al.|[2411.03252](http://arxiv.org/abs/2411.03252)|null|我们从零开始研究代理的出现，通过使用基于大型语言模型（LLM）的代理。在以往对基于LLM的代理的研究中，每个代理的性格特征，包括个性和记忆，通常是预先定义好的。我们关注的是如何从一个未分化的状态中分化出个体性，如行为、个性和记忆。当前的LLM代理在一个群体模拟中进行合作交流，以自然语言交换基于上下文的消息。通过分析这一多代理模拟，我们报告了关于社会规范、合作和个人特质如何自发产生的有价值的新见解。本文展示了自主交互的LLM驱动代理会生成幻觉和话题标签来维持交流，这反过来增加了他们互动中的词汇多样性。随着交流的进行，每个代理的情绪会发生变化，当它们形成社区时，代理的个性随之出现并发展。这种计算建模方法及其发现将为分析集体人工智能提供一种新的方法。|
|**2024-11-05**|**DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models**|Ying Zhou et.al.|[2411.03250](http://arxiv.org/abs/2411.03250)|null|近期大型语言模型（LLM）的进步显著增强了其知识和生成能力，引发了利用LLM进行高质量数据合成的浓厚兴趣。然而，通过提示LLM进行合成数据生成仍然面临挑战，因为LLM对目标数据分布的理解有限，并且提示工程复杂，尤其是对于结构化格式的数据。为了解决这些问题，我们引入了DiffLM，这是一种基于变分自编码器（VAE）的可控数据合成框架，进一步利用扩散模型保留原始分布和格式结构中的更多信息，并通过即插即用的潜在特征注入模块将目标分布知识的学习与LLM的生成目标解耦。由于观察到VAE的潜在表示与真实数据分布之间存在显著差异，我们在框架中引入了潜在扩散模块，以学习一个完全表达的潜在分布。在七个具有结构化格式数据（即表格、代码和工具数据）的真实世界数据集上的评估表明，DiffLM生成了高质量的数据，在某些情况下，下游任务的表现超过了真实数据2-7个百分点。数据和代码将在内部审查完成后公开。|
|**2024-11-05**|**From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice**|Alicia Guo et.al.|[2411.03137](http://arxiv.org/abs/2411.03137)|null|创意作家们热爱他们的创作工艺，然而使用大型语言模型（LLMs）的AI系统可以自动化写作过程中的许多部分。那么，为什么一些创意作家选择将AI整合到他们的工作流程中呢？为了探讨这个问题，我们采访并观察了18位已经定期在写作实践中使用AI的创意作家的写作会话。我们的研究发现，创意作家在整合AI时是有意为之的，他们根据对写作的核心价值观，如真实性和工艺，以及与AI的关系和使用方式，做出许多有意识的决定，以确定他们希望在哪些方面保持控制权。通过分析，我们提出了一个作家价值观、作家与AI的关系以及整合策略的分类，并讨论了这三个要素之间的相互关系。|
|**2024-11-05**|**"Create a Fear of Missing Out" -- ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning**|Veronika Krauß et.al.|[2411.03108](http://arxiv.org/abs/2411.03108)|null|随着大型语言模型（LLMs）的最新进展，网络开发者越来越多地利用它们的代码生成能力进行网站设计。然而，由于这些模型是基于现有的设计师知识进行训练的，它们可能会无意中复制不良甚至非法的做法，特别是欺骗性设计（DD）。本文研究了用户是否可能在为一个虚构的网上商店创建功能时意外地生成欺骗性设计模式。我们招募了20名参与者，让他们使用ChatGPT生成产品概览或结账功能，然后使用中立提示对其进行修改以实现商业目标（例如，“提高我们销售产品的可能性”）。我们发现，所有20个生成的网站都至少包含一种欺骗性设计模式（平均值：5，最大值：9），且GPT-4没有发出任何警告。当参与者反思这些设计时，只有4名参与者表达了担忧，而大多数认为结果令人满意，并不认为这在道德上存在问题，尽管这对终端用户和采纳ChatGPT建议的人来说存在潜在的伦理和法律问题。|
|**2024-11-04**|**Training-free Regional Prompting for Diffusion Transformers**|Anthony Chen et.al.|[2411.02395](http://arxiv.org/abs/2411.02395)|**[link](https://github.com/antonioo-c/regional-prompting-flux)**|**扩散模型在文本到图像生成方面展示了出色的能力。随着大型语言模型（如T5、Llama）的应用，它们对语义的理解能力，即遵循提示的能力也得到了极大的提升。然而，现有的模型无法完美处理长且复杂的文本提示，尤其是当这些文本提示包含具有众多属性和相互关联的空间关系的多个对象时。尽管已经提出了许多基于UNet的模型（如SD1.5、SDXL）的区域提示方法，但基于最近的扩散变换器（DiT）架构（如SD3和FLUX）的方法尚未实现。在这份报告中，我们提出并实现了基于注意力操作的FLUX.1的区域提示方法，这使得DiT能够在无需训练的情况下具备细粒度的组合式文本到图像生成能力。代码可在<https://github.com/antonioo-c/Regional-Prompting-FLUX>获取。**|
|**2024-11-04**|**Adaptive Length Image Tokenization via Recurrent Allocation**|Shivam Duggal et.al.|[2411.02393](http://arxiv.org/abs/2411.02393)|**[link](https://github.com/shivamduggal4/adaptive-length-tokenizer)**|**当前的视觉系统通常为图像分配固定长度的表示，而不考虑信息内容。这与人类智能以及大型语言模型不同，后者根据熵、上下文和熟悉度分配不同的表征容量。受此启发，我们提出了一种方法来学习二维图像的变长令牌表示。我们的编解码器架构递归地处理二维图像令牌，在多次迭代的循环展开过程中将其提炼为一维潜在令牌。每次迭代都会细化二维令牌，更新现有的一维潜在令牌，并通过添加新令牌自适应地增加表征容量。这使得图像可以压缩成一个可变数量的令牌，范围从32到256。我们使用重建损失和FID指标验证了我们的标记化方法，结果表明令牌数量与图像熵、熟悉度和下游任务要求相匹配。在每次迭代中随着表征容量的增加进行循环令牌处理显示出令牌专业化的迹象，揭示了对象/部分发现的潜力。**|
|**2024-11-04**|**Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models**|Guangzhi Xiong et.al.|[2411.02382](http://arxiv.org/abs/2411.02382)|null|大型语言模型（LLMs）在各个科学领域展示了非凡的能力，从自然语言处理到复杂的问题解决任务。它们理解并生成类似人类文本的能力开启了推进科学研究的新可能性，使数据处理、文献综述甚至实验设计等任务成为可能。在这种背景下，LLMs最有前景的应用之一是假设生成，通过分析现有知识，它们可以识别新的研究方向。然而，尽管有这些潜力，LLMs容易生成“幻觉”，即听起来合理但实际上不正确的输出。这一问题在需要严格准确性和可验证性的科学领域提出了重大挑战，可能导致错误或误导性的结论。为了克服这些挑战，我们提出了一种名为KG-CoI（基于知识图谱的思路链）的新系统，该系统通过整合来自知识图谱（KGs）的外部结构化知识来增强LLM假设生成。KG-CoI引导LLMs经历一个结构化的推理过程，并将其输出组织成一个思路链（CoI），还包括一个基于知识图谱支持的模块来检测幻觉。通过在我们新构建的假设生成数据集上进行的实验，我们证明了KG-CoI不仅提高了LLM生成假设的准确性，还减少了其推理链中的幻觉，突显了它在推进现实世界科学研究方面的有效性。|
|**2024-11-04**|**Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI**|Ramneet Kaur et.al.|[2411.02381](http://arxiv.org/abs/2411.02381)|null|在本文中，我们提出了一种动态语义聚类方法，该方法受到中国餐馆过程的启发，旨在解决大型语言模型（LLMs）推理中的不确定性问题。我们通过计算生成的语义聚类的熵来量化LLM对给定查询的不确定性。此外，我们建议利用这些聚类的（负）似然性作为（非）一致性得分，在符合性预测框架内使用，使模型能够预测一组响应而不是单一输出，从而考虑其预测中的不确定性。我们通过两个著名的问答基准测试COQA和TriviaQA验证了我们不确定性量化（UQ）技术的有效性，使用的两种LLMs分别为Llama2和Mistral。我们的方法在AUROC、AUARC和AURAC等指标下实现了最先进的UQ性能。所提出的符合性预测器也被证明能够在保持相同概率保证包含正确答案的同时，产生更小的预测集，与现有的最先进符合性预测基线相比。|
|**2024-11-04**|**DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution**|Yang Yue et.al.|[2411.02359](http://arxiv.org/abs/2411.02359)|**[link](https://github.com/yueyang130/deer-vla)**|**多模态大语言模型（MLLMs）在处理复杂的语言和视觉数据方面展现了卓越的理解和推理能力。这些进展激发了建立一种通用机器人多模态大语言模型的愿景，这种模型能够理解复杂的人类指令并完成各种具身任务。然而，由于机器人平台通常具有有限的计算和内存容量，将MLLMs应用于现实世界的机器人面临挑战。相比之下，MLLMs的推理过程需要存储数十亿参数并执行大量计算，对硬件提出了很高的要求。在本文中，我们提出了一种针对机器人视觉-语言-动作模型的动态早退框架（DeeR-VLA，或简称DeeR），该框架能够根据具体情况自动调整激活的MLLM的大小。这种方法利用了MLLMs中的多出口架构，使得模型可以在特定情况下激活适当大小的模型后终止处理，从而避免进一步的冗余计算。此外，我们开发了新的算法，基于预定义的需求（如平均计算成本即功耗、峰值计算消耗即延迟以及GPU内存使用量）来建立DeeR的早期终止标准。这些改进确保了DeeR能够在不同的资源约束下高效运行，同时保持竞争力的表现。在CALVIN机器人操作基准测试中，DeeR展示了LLM的计算成本降低了5.2到6.5倍，LLM的GPU内存减少了2到6倍，且未影响性能。代码和检查点可在<https://github.com/yueyang130/DeeR-VLA>获取。**|
|**2024-11-04**|**"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization**|Eldar Kurtic et.al.|[2411.02355](http://arxiv.org/abs/2411.02355)|null|尽管大规模语言模型（LLM）的量化技术在推理加速方面取得了显著进展，但对于各种量化格式所带来的准确性和性能之间的权衡仍存在很大的不确定性。我们对量化精度进行了全面的经验研究，评估了流行的量化格式（FP8、INT8、INT4）在学术基准和现实任务中的表现，涵盖了整个Llama-3.1模型系列。此外，我们的研究还探讨了量化模型与未压缩模型生成文本之间的差异。除了基准测试外，我们还提出了一些量化改进措施，使我们能够获得最先进的准确度恢复结果。我们的调查涵盖了超过50万次独立评估，得出了几个关键发现：（1）FP8权重和激活量化（W8A8-FP）在所有模型规模下都是无损的；（2）当适当调优时，INT8权重和激活量化（W8A8-INT）仅导致1-3%的准确度下降，令人惊讶的是，其性能仍然良好；（3）INT4权重只量化（W4A16-INT）在性能上与8位整数权重和激活量化相当。为了确定给定部署环境下的“最佳”格式，我们使用流行的开源vLLM框架在不同的GPU架构上进行了推理性能分析。我们发现，W4A16在同步部署中提供了最佳的成本效益，并且在中端GPU上的异步部署也表现出色。同时，W8A8格式在高端GPU上进行中型和大型模型的异步“连续批处理”部署中表现出色。我们的研究结果为在不同规模和性能需求下部署量化LLM提供了一套实用指南。|
|**2024-11-04**|**Social-RAG: Retrieving from Group Interactions to Socially Ground Proactive AI Generation to Group Preferences**|Ruotong Wang et.al.|[2411.02353](http://arxiv.org/abs/2411.02353)|null|人工智能代理越来越多地被赋予在协作的在线空间中提出主动建议的任务，但有时会因为不符合团队的偏好或以不适当的社会方式行事而显得无益甚至令人厌烦。幸运的是，团队空间拥有丰富的先前社会互动历史和社交反馈机制，可以支持创建符合团队兴趣和规范的代理。我们提出了Social-RAG工作流程，该流程将代理与关于团队的社会信息联系起来，从先前的团队互动中检索信息，选择相关社交信号，然后将上下文输入大型语言模型以生成对团队的消息。我们将这一流程实施到PaperPing系统中，该系统在团队聊天中发布学术论文推荐，利用了通过对39名研究人员进行形成性研究确定的社交信号。在为期三个月的部署中，PaperPing在不干扰现有社交实践的情况下，在18个频道发布了相关消息，促进了团队的共同理解。|
|**2024-11-04**|**Can Large Language Models generalize analogy solving like people can?**|Claire E. Stevenson et.al.|[2411.02348](http://arxiv.org/abs/2411.02348)|null|当解决类比问题时，我们将已知情境中的信息转移到新情境中，通过抽象规则和关系相似性来实现。在人类中，解决类比问题的能力（例如，“身体：脚 :: 桌子：？”）在儿童时期出现，并且似乎可以轻松转移到其他领域，如视觉领域“（：）:: <：？”。最近的研究表明，大型语言模型（LLMs）能够解决各种形式的类比问题。然而，LLMs能否像人类一样将类比解决能力泛化到新的领域？为了研究这个问题，我们让儿童、成人和LLMs解决一系列字母串类比问题（例如，a b : a c :: j k : ？）在拉丁字母中，在近迁移领域（希腊字母），以及远迁移领域（符号列表）。正如预期的那样，儿童和成人都能轻松地将其知识泛化到不熟悉的领域，而LLMs则没有做到这一点。这种人类与AI表现的关键差异是证据，表明这些LLMs仍然难以实现稳健的人类类比迁移。|
|**2024-11-04**|**WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning**|Zehan Qi et.al.|[2411.02337](http://arxiv.org/abs/2411.02337)|**[link](https://github.com/THUDM/WebRL)**|大型语言模型（LLMs）在作为自主代理方面展现出了显著的潜力，特别是在网络任务方面。然而，现有的基于LLM的网络代理严重依赖昂贵的专有LLM API，而开源的LLM缺乏必要的决策能力。本文介绍了一种名为WebRL的自我进化在线课程强化学习框架，旨在使用开源LLM训练高性能网络代理。WebRL解决了构建LLM网络代理的三个关键挑战，包括训练任务的稀缺、稀疏反馈信号以及在线学习中的策略分布漂移。具体而言，WebRL包含了1）一个自我进化的课程，该课程从失败尝试中生成新任务；2）一个稳健的结果监督奖励模型（ORM）；3）自适应强化学习策略以确保持续改进。我们将WebRL应用于将开源Llama-3.1和GLM-4模型转化为高效的网络代理。在WebArena-Lite上，WebRL将Llama-3.1-8B的成功率从4.8%提高到42.4%，并将GLM-4-9B的成功率从6.1%提高到43%。这些开源模型显著超过了GPT-4-Turbo（17.6%）和GPT-4o（13.9%）的表现，并且优于之前基于开源LLM训练的最佳网络代理（AutoWebGLM，18.2%）。我们的研究结果表明，WebRL在缩小开源和专有LLM网络代理之间的差距方面是有效的，为更易访问和强大的自主网络交互系统铺平了道路。|
|**2024-11-04**|**Sparsing Law: Towards Large Language Models with Greater Activation Sparsity**|Yuqi Luo et.al.|[2411.02335](http://arxiv.org/abs/2411.02335)|**[link](https://github.com/thunlp/SparsingLaw)**|激活稀疏性指的是在激活输出中存在的大量贡献较弱的元素，这些元素可以被消除，从而有利于与大规模语言模型（LLMs）相关的许多重要应用。尽管促进LLMs中的更大激活稀疏性值得深入研究，但现有工作缺乏对激活稀疏性和潜在影响因素之间相关性的全面和定量研究。在本文中，我们对解码器-only的Transformer基础LLMs中的激活稀疏性的量化缩放特性和影响因素进行了全面研究。具体而言，我们提出了PPL- $p\%$稀疏性，这是一种精确且性能感知的激活稀疏性度量，适用于任何激活函数。通过广泛的实验，我们发现了几个重要的现象。首先，不同的激活函数表现出相似的性能，但在训练时间稀疏性趋势上相反。激活比率（即$1-\mathrm{稀疏率}$ ）随着训练数据量的变化，在SiLU激活和ReLU激活的LLMs中分别遵循收敛的幂律增加和对数空间幂律减少。这表明ReLU作为激活函数比SiLU更高效，并且能够利用更多的训练数据来提高激活稀疏性。其次，激活比率在某个瓶颈点以下线性增加与宽度深度比的关系，表明固定参数规模下更深架构的潜在优势。最后，在类似的宽度深度比下，我们惊讶地发现激活稀疏性的极限值随参数规模变化较弱，即LLMs内的激活模式对参数规模不敏感。这些针对具有更大激活稀疏性的LLMs的经验法则对于使LLMs更加高效和可解释具有重要意义。|
|**2024-11-01**|**SelfCodeAlign: Self-Alignment for Code Generation**|Yuxiang Wei et.al.|[2410.24198](http://arxiv.org/abs/2410.24198)|**[link](https://github.com/bigcode-project/selfcodealign)**|**指令微调是一种监督微调方法，显著提高了大型语言模型（LLMs）遵循人类指令的能力。我们提出了SelfCodeAlign，这是首个完全透明且许可宽松的管道，用于自我对齐代码LLMs，而无需大量的手动标注或蒸馏。SelfCodeAlign在整个数据生成过程中使用相同的基模型进行推理。它首先从高质量的种子代码片段中提取多样化的编码概念以生成新任务。然后，它为每个任务采样多个响应，并将其与测试用例配对，在沙盒环境中进行验证。最后，通过选择通过测试的示例进行指令微调。在我们的主要实验中，我们使用SelfCodeAlign与CodeQwen1.5-7B一起生成了一个包含74k个指令-响应对的数据集。在此数据集上进行微调后，该模型在HumanEval+上的pass@1达到了67.1%，超过了CodeLlama-70B-Instruct，尽管其规模小了十倍。在所有基准测试中，这个经过微调的模型始终优于之前最先进的无需人工标注或蒸馏的指令微调方法OctoPack。此外，我们展示了SelfCodeAlign在各种规模的LLMs（从3B到33B）上都是有效的，并且基模型可以从与自身数据分布的对齐中受益更多。我们还验证了管道中每个组件的有效性，显示SelfCodeAlign在直接从GPT-4o蒸馏和领先的基于GPT-3.5的蒸馏方法（如OSS-Instruct和Evol-Instruct）方面均表现出色。SelfCodeAlign还促成了StarCoder2-Instruct的创建，这是首个完全透明、许可宽松且自我对齐的代码LLM，实现了最先进的编码性能。**|
|**2024-10-31**|**Constraint Back-translation Improves Complex Instruction Following of Large Language Models**|Yunjia Qi et.al.|[2410.24175](http://arxiv.org/abs/2410.24175)|null|大型语言模型（LLMs）在遵循具有复杂格式、长度等约束的指令时存在困难。传统上，先前的工作通过向先进的LLMs提供复杂的指令-响应对来进行后训练，以处理这些复杂指令。然而，即使是先进的LLMs也难以很好地遵循复杂的指令，从而限制了生成数据的质量。在这项工作中，我们发现现有的数据集内在地包含了隐含的复杂约束，并提出了一种新颖的数据生成技术——约束回译。具体来说，我们采用现有数据集中高质量的指令-响应对，并仅使用先进的LLMs将响应已满足的复杂约束添加到指令中，这自然降低了成本和数据噪声。在实验中，我们使用Llama3-70B-Instruct进行约束回译，创建了一个高质量的复杂指令-响应数据集，命名为CRAB。我们展示了在CRAB上进行后训练可以提高多种基础LLMs的复杂指令遵循能力，在广泛的指令遵循基准上进行了评估。我们进一步发现，约束回译也可以作为后训练中的有用辅助训练目标。我们的代码、数据和模型将被发布，以促进未来的研究。|
|**2024-10-31**|**Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning**|Jinghan Zhang et.al.|[2410.24155](http://arxiv.org/abs/2410.24155)|null|近年来，大型语言模型（LLMs）在处理复杂推理任务方面展现出了巨大的潜力，通常通过构建思维链来指导模型进行多步推理。然而，现有的方法往往局限于先前探索过的解决方案空间，从而忽略了LLMs认知范围内的关键盲点。为了解决这些问题，我们设计了Thought Space Explorer (TSE)，这是一种新颖的框架，旨在扩展和优化思维结构，以引导LLMs探索其思维盲点。通过基于原始思维结构生成新的推理步骤和分支，并采用各种设计策略，TSE扩展了思维空间并减轻了盲点对LLM推理的影响。在多个级别的推理任务上的实验结果证明了TSE的有效性。我们还进行了广泛的分析，以理解结构化和扩展化的思维如何有助于释放LLM推理能力的潜力。|
|**2024-10-31**|**Language-Driven Policy Distillation for Cooperative Driving in Multi-Agent Reinforcement Learning**|Jiaqi Liu et.al.|[2410.24152](http://arxiv.org/abs/2410.24152)|null|合作驾驶技术对于提升交通系统的效率和安全性至关重要。基于学习的方法，如多智能体强化学习（MARL），在合作决策任务中展示了强大的能力。然而，现有的MARL方法仍然面临学习效率和性能方面的挑战。近年来，大规模语言模型（LLM）迅速发展，并在各种顺序决策任务中表现出色。为了增强合作代理的学习能力，同时确保决策效率和成本效益，我们提出了一种名为LDPD的语言驱动策略蒸馏方法来引导MARL探索。在这个框架中，基于LLM的教师代理训练较小的学生代理通过其自身的决策演示实现合作决策。教师代理增强了自动驾驶车辆的观察信息，并利用LLM进行复杂的合作决策推理，同时也利用精心设计的决策工具实现专家级决策，提供高质量的教学经验。学生代理通过梯度策略更新将教师的先验知识提炼到自己的模型中。实验表明，学生可以在最少的教师指导下快速提高其能力，并最终超越教师的表现。广泛的实验表明，我们的方法在性能和学习效率方面优于基线方法。|
|**2024-10-31**|**Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing**|Akash Dhruv et.al.|[2410.24119](http://arxiv.org/abs/2410.24119)|**[link](https://github.com/neucol/llm-conversion-performance)**|**基础模型和生成式人工智能（GenAI）的出现有望改变科学计算中的生产力，特别是在代码开发、重构以及从一种编程语言转换到另一种编程语言方面。然而，由于GenAI的输出不能保证正确性，因此仍然需要人工干预。部分这种干预可以通过任务特定工具以及用于正确性验证和有效提示开发的附加方法来自动化。我们研究了GenAI在辅助代码转换、语言互操作性和在用于模拟大型强子对撞机（LHC）粒子相互作用的遗留Fortran代码库中进行代码库检查方面的应用。在此过程中，我们开发了一款名为CodeScribe的工具，结合提示工程与用户监督，建立了一个高效的代码转换流程。在本文中，我们展示了CodeScribe如何帮助将Fortran代码转换为C++，生成Fortran-C API以集成遗留系统与现代C++库，并提供开发者支持以实现代码组织和算法实施。我们还讨论了AI驱动的代码转换面临的挑战，并强调其在提高科学计算工作流程生产力方面的优势。**|
|**2024-10-31**|**Repository-Level Compositional Code Translation and Validation**|Ali Reza Ibrahimzada et.al.|[2410.24117](http://arxiv.org/abs/2410.24117)|**[link](https://github.com/Intelligent-CAT-Lab/AlphaTrans)**|代码翻译是将程序从一种编程语言转换为另一种编程语言的过程。一些基于规则的转译器已经被设计出来，以实现不同编程语言对之间的自动化代码翻译。然而，这些规则可能会因编程语言的发展而变得过时，并且无法推广到其他编程语言。近期的研究探索了使用大型语言模型（LLMs）来自动化代码翻译。一个关键观察是，这样的技术可能在精心设计的基准测试中表现良好，但在真实世界的项目中，由于依赖关系、自定义类型、特定于编程语言的功能等因素的存在，它们可能难以泛化。  我们提出了AlphaTrans，这是一种神经符号方法，用于自动化整个代码仓库级别的代码翻译。AlphaTrans不仅翻译源代码，还翻译测试代码，并采用多级验证确保翻译后的代码保留了源程序的功能。为了分解问题以便让LLMs处理，AlphaTrans利用程序分析将程序分解成片段，并按逆调用顺序进行翻译。我们使用AlphaTrans翻译了十个现实世界中的开源项目，这些项目包含的类、方法和测试分别有<836, 8575, 2719>个。AlphaTrans成功翻译了这些项目的所有代码库，共包括6899个代码片段。99.1%的翻译代码片段在语法上是正确的，AlphaTrans验证了其中25.8%的运行时行为和功能正确性。平均而言，集成翻译和验证过程需要36小时来翻译一个项目，显示出其在实际应用中的可扩展性。对于那些在语法或语义上不正确的翻译，AlphaTrans生成一份报告，其中包括现有的翻译、堆栈跟踪、测试错误或断言失败。我们向两位开发者提供了这些辅助材料，帮助他们在四个项目中修复翻译错误。他们平均花费20.1小时解决了这些问题，并使所有测试通过。|
|**2024-10-31**|**Matchmaker: Self-Improving Large Language Model Programs for Schema Matching**|Nabeel Seedat et.al.|[2410.24105](http://arxiv.org/abs/2410.24105)|null|实体匹配——即在具有不同表和层次结构的异构数据源之间找到属性之间的匹配——对于创建可用于机器学习（ML）的数据至关重要。这一基础性的数据问题在医疗、金融和电子商务等领域尤为重要，同时也能够更广泛地通过增加用于训练ML模型的数据量来使ML模型受益。然而，由于不同模式之间的结构/层次和语义异质性，实体匹配是一个具有挑战性的ML任务。先前的自动化实体匹配的ML方法要么需要大量的标注数据进行模型训练，这通常是不现实的，要么零样本性能较差。为此，我们提出了Matchmaker——一种用于实体匹配的组合式语言模型程序，该程序由候选生成、优化和置信度评分组成。Matchmaker还通过一种新颖的优化方法实现在零样本情况下自我改进，该方法构建合成上下文演示以引导语言模型的推理过程。实证研究表明，在真实世界的医学实体匹配基准上，Matchmaker优于之前的基于ML的方法，突显了其加速数据集成和ML就绪数据互操作性的潜力。|
|**2024-10-31**|**Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs**|Muhammed Saeed et.al.|[2410.24049](http://arxiv.org/abs/2410.24049)|null|大型语言模型（LLMs）在广泛应用的同时引发了伦理问题，因为它们内置了社会偏见。本研究在包括女性权利、恐怖主义和反犹太主义在内的八个领域中考察了LLMs对阿拉伯人与西方人的偏见，并评估了这些模型抵抗延续这些偏见的能力。为此，我们创建了两个数据集：一个用于评估LLM对阿拉伯人与西方人的偏见，另一个用于测试模型对放大负面特征的提示的安全性（“越狱”）。我们评估了六种LLM——GPT-4、GPT-4o、LlaMA 3.1（8B & 405B）、Mistral 7B和Claude 3.5 Sonnet。我们发现79%的案例显示出对阿拉伯人的负面偏见，其中LlaMA 3.1-405B是最具偏见的模型。我们的“越狱”测试显示，尽管GPT-4o是经过优化的版本，但它却是最易受攻击的，其次是LlaMA 3.1-8B和Mistral 7B。除了Claude外，所有LLM在三个类别中的攻击成功率均超过87%。我们还发现Claude 3.5 Sonnet的安全性最高，但仍然在八个类别中的七个显示出偏见。尽管GPT-4o是GPT-4的一个优化版本，但我们发现它更容易受到偏见和“越狱”的影响，这表明优化存在缺陷。我们的研究结果强调了需要更强大的偏见缓解策略和强化安全措施的紧迫性。|
|**2024-10-31**|**Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks**|Yingzhe Peng et.al.|[2410.24032](http://arxiv.org/abs/2410.24032)|null|大规模语言模型（LLM）的兴起已经彻底改变了用户与知识系统之间的交互方式，使得聊天机器人能够整合大量的信息并协助处理复杂的探索性任务。然而，基于LLM的聊天机器人往往难以提供个性化支持，尤其是在用户以模糊查询开始或缺乏足够的上下文信息时。本文介绍了一种名为“个性化探索协作助理”（CARE）的系统，该系统通过结合多代理LLM框架和结构化的用户界面来增强个性化在探索性任务中的应用。CARE的界面包括聊天面板、解决方案面板和需求面板，使迭代式查询细化和动态解决方案生成成为可能。多代理框架协同工作，以识别显性和隐性用户需求，从而提供定制化的、可操作的解决方案。在一项涉及22名参与者的被试内用户研究中，CARE相对于基线LLM聊天机器人一直受到欢迎，用户称赞其能够减轻认知负担、激发创造力，并提供更加个性化的解决方案。我们的研究结果表明，CARE有可能将基于LLM的系统从被动的信息检索者转变为个性化问题解决和探索中的积极合作伙伴。|
|**2024-10-31**|**AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents**|Yifan Xu et.al.|[2410.24024](http://arxiv.org/abs/2410.24024)|**[link](https://github.com/THUDM/Android-Lab)**|自主代理在与现实世界互动中的重要性日益增加。特别是，安卓代理作为一种交互方法被频繁提及。然而，现有的用于训练和评估安卓代理的研究缺乏对开源和闭源模型系统的系统性研究。在这项工作中，我们提出了AndroidLab作为系统化的安卓代理框架。它包括一个具有不同模态的操作环境、动作空间以及可重复使用的基准测试。它支持在同一动作空间下的大型语言模型（LLMs）和多模态模型（LMMs）。AndroidLab基准测试包括预定义的安卓虚拟设备和九个应用上的138个任务。通过使用AndroidLab环境，我们开发了一个安卓指令数据集，并训练了六个开源的LLMs和LMMs，将LLMs的成功率从4.59%提升到21.50%，LMMs的成功率从1.93%提升到13.28%。AndroidLab已开源并公开提供，网址为<https://github.com/THUDM/Android-Lab>。|
|**2024-10-30**|**EMMA: End-to-End Multimodal Model for Autonomous Driving**|Jyh-Jing Hwang et.al.|[2410.23262](http://arxiv.org/abs/2410.23262)|null|我们介绍了EMMA，这是一种用于自动驾驶的端到端多模态模型。该模型基于多模态大型语言模型基础，直接将原始相机传感器数据映射到各种与驾驶相关的输出，包括规划轨迹、感知对象和道路图元素。EMMA通过将所有非传感器输入（例如导航指令和自车状态）和输出（例如轨迹和三维位置）表示为自然语言文本，最大限度地利用了预训练大型语言模型中的世界知识。这种方法使EMMA能够在统一的语言空间中联合处理各种驾驶任务，并使用特定任务提示生成每个任务的输出。实证研究表明，EMMA在nuScenes上的运动规划方面达到了最先进的性能，并在Waymo开放运动数据集（WOMD）上取得了具有竞争力的结果。此外，EMMA在Waymo开放数据集（WOD）上作为主要摄像头的三维目标检测也取得了具有竞争力的结果。我们展示了通过同时训练EMMA进行规划轨迹、目标检测和道路图任务可以在这三个领域都取得改进，突显了EMMA作为自动驾驶应用中的通用模型的潜力。然而，EMMA也表现出一些局限性：它只能处理少量图像帧，不包含准确的三维传感模态如激光雷达或雷达，并且计算成本较高。我们希望我们的结果能够激发进一步的研究，以解决这些问题并进一步发展自动驾驶模型架构。|
|**2024-10-30**|**Evaluating Cultural and Social Awareness of LLM Web Agents**|Haoyi Qiu et.al.|[2410.23252](http://arxiv.org/abs/2410.23252)|null|随着大型语言模型（LLMs）扩展到执行现实世界应用中的代理任务，超越传统的自然语言处理任务，评估其鲁棒性变得越来越重要。然而，现有的基准测试往往忽视了文化和社会意识等关键维度。为了解决这些问题，我们引入了CASA，这是一个旨在评估LLM代理在两个基于网络的任务（在线购物和社交讨论论坛）中对文化和社会规范的敏感性的基准。我们的方法评估了LLM代理检测并适当回应违反规范的用户查询和观察的能力。此外，我们提出了一种全面的评估框架，该框架测量代理对文化和社会规范的意识覆盖率、在管理用户查询时的实用性以及面对误导性网络内容时的违规率。实验表明，当前的LLM在非代理环境中的表现显著优于在网络代理环境中，代理的意识覆盖率不到10%，违规率超过40%。为了提高性能，我们探索了两种方法：提示和微调，并发现这两种方法可以互补——针对特定文化的数据集进行微调可以显著增强代理在不同地区的泛化能力，而提示则能提升代理处理复杂任务的能力。这些发现突显了在开发周期中不断基准测试LLM代理的文化和社会意识的重要性。|
|**2024-10-30**|**Carrot and Stick: Eliciting Comparison Data and Beyond**|Yiling Chen et.al.|[2410.23243](http://arxiv.org/abs/2410.23243)|null|比较数据通常来自于人们的主观判断，并且难以直接验证。这些数据对于许多机器学习任务至关重要，包括基于人类反馈的强化学习和排名模型估计。如何诚实地从理性个体那里获取这样的比较数据？我们设计了同伴预测机制来利用奖金-惩罚支付方式来获取比较数据。我们的设计依赖于比较数据的强随机传递性，从而创建对称的严格真实机制，使得说实话不仅形成严格的贝叶斯纳什均衡，而且在所有对称均衡中获得最高报酬。在我们的机制下，每个个体只需要评估一对项目并报告她的比较结果。  我们进一步将奖金-惩罚支付的概念扩展到网络化数据的获取上，设计了一种当代理人的私人信号根据Ising模型采样时，对称地严格真实的机制。我们提供了奖金-惩罚支付成为严格贝叶斯纳什均衡的必要和充分条件。在两个现实世界的数据集上的实验进一步支持了我们的理论发现。|
|**2024-10-30**|**A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment**|Matteo G. Mecattaf et.al.|[2410.23242](http://arxiv.org/abs/2410.23242)|**[link](https://github.com/kinds-of-intelligence-cfi/llm-aai)**|作为通用工具，大型语言模型（LLMs）必须经常推理日常物理环境。在问答场景中，理解物理对象的相互作用可能是给出适当回答的必要条件。此外，LLMs越来越多地被用作自主系统中的推理引擎，设计和控制它们的动作序列。大多数研究通过静态基准来解决这个问题，这些基准由关于物理世界的文本或图像问题组成。然而，这些基准无法捕捉现实生活中的物理过程的复杂性和细微差别。在这里，我们提倡第二种相对未被充分探索的方法：通过在一个3D环境中赋予LLMs对代理的控制权来“具身化”它们。我们提出了第一个具身且认知上有意义的LLM物理常识推理评估框架。我们的框架允许直接比较LLMs与其他具身代理，如基于深度强化学习的代理，以及人类和非人类动物。我们使用Animal-AI（AAI）环境，一个模拟的3D虚拟实验室，来研究LLMs的物理常识推理能力。为此，我们使用AAI测试平台，该平台是一系列实验，复制了非人类动物的实验室研究，以研究物理推理能力，包括距离估计、跟踪看不见的物体和工具使用。我们证明，没有微调的状态-of-the-art多模态模型能够完成这种任务，使得与2019年Animal-AI奥运会参赛者和人类儿童进行有意义的比较成为可能。我们的结果显示，LLMs目前在这类任务上的表现不如人类儿童。我们认为这种方法允许使用直接从认知科学中提取的生态有效的实验来研究物理推理，从而提高LLMs的预测性和可靠性。|
|**2024-10-30**|**EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning**|Peide Huang et.al.|[2410.23234](http://arxiv.org/abs/2410.23234)|null|本文介绍了一个名为EMOTION的框架，用于在人形机器人中生成富有表现力的动作序列，从而增强其进行类人非语言交流的能力。非语言线索如面部表情、手势和身体动作在有效的人际互动中起着至关重要的作用。尽管在机器人的行为方面已经取得了进展，但现有的方法往往难以模仿人类非语言交流的多样性和细微差别。为了解决这一差距，我们的方法利用大型语言模型（LLM）的上下文学习能力，动态生成适合社会交往的手势动作序列，以促进人机交互。我们使用该框架生成了10种不同的表情手势，并进行了在线用户研究，比较由EMOTION和其加入人类反馈版本EMOTION++生成的动作与人类操作员生成的动作之间的自然度和可理解性。结果显示，在某些情况下，我们的方法在生成可理解且自然的机器人动作方面要么与人类表现相当，要么超越人类。我们还提供了未来研究的设计启示，考虑在生成富有表现力的机器人手势时需要考虑的一系列变量。|
|**2024-10-31**|**Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval**|Sheryl Hsu et.al.|[2410.23214](http://arxiv.org/abs/2410.23214)|null|大型语言模型（LLMs）的幻觉现象通过允许模型搜索信息并将答案与真实来源挂钩，得到了一定程度的缓解。然而，LLMs在处理复杂或间接主题时，往往难以提出正确的搜索查询。我们观察到，通过让LLMs尝试不同的查询并学习对那些成功产生相关结果的查询赋予更高的权重，LLMs可以学会检索相关的事实。为此，我们引入了LeReT（Learning to Retrieve by Trying），这是一种强化学习框架，通过探索搜索查询并使用基于偏好的优化来提高查询质量。LeReT可以将绝对检索准确性提高多达29%，并将下游生成器评估提高17%。LeReT的简单性和灵活性使其能够应用于任意现成的检索器，并成为改进通用LLM管道的一种有前途的技术。项目网站：http://sherylhsu.com/LeReT/。|
|**2024-10-30**|**ProTransformer: Robustify Transformers via Plug-and-Play Paradigm**|Zhichao Hou et.al.|[2410.23182](http://arxiv.org/abs/2410.23182)|null|近年来，基于Transformer的架构在机器学习的各个领域占据主导地位。在这篇论文中，我们介绍了一种新颖的鲁棒注意力机制，旨在增强基于Transformer的架构的韧性。这项技术可以作为插件层集成到现有的Transformer模型中，从而提高其鲁棒性，而无需额外的训练或微调。通过全面的实验和消融研究，我们证明了ProTransformer显著提升了各种预测任务、攻击机制、骨干架构和数据域中的Transformer模型的鲁棒性。值得注意的是，在经典的TextFooler攻击下，无需进一步微调，ProTransformer分别将BERT、ALBERT、DistilBERT和RoBERTA这四种模型的性能提高了19.5%、28.3%、16.1%和11.4%。此外，ProTransformer在大型语言模型（LLMs）面对基于提示的攻击时表现出良好的韧性，分别将T5和LLaMA的性能提高了24.8%和17.8%，并且平均将Vicuna在Jailbreaking攻击下的性能提高了10.4%。除了语言领域外，ProTransformer还在视觉和图领域展示了出色的鲁棒性。|
|**2024-10-30**|**ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning**|Millennium Bismay et.al.|[2410.23180](http://arxiv.org/abs/2410.23180)|**[link](https://github.com/millenniumbismay/reasoningrec)**|**本文介绍了一种名为ReasoningRec的推理推荐框架，该框架利用大语言模型（LLMs）来弥合推荐与人类可解释性解释之间的差距。与依赖于隐式用户-项目交互的传统推荐系统不同，ReasoningRec使用LLMs来建模用户和项目，重点在于用户的偏好、厌恶和解释性推理。该框架利用一个较大的LLM生成用户偏好的合成解释，随后用于微调较小的LLM以提高推荐准确性及提供人类可理解的解释。我们的实验研究调查了推理和上下文信息对个性化推荐的影响，结果显示上下文和个人化数据的质量显著影响LLM生成合理解释的能力。实证评估表明，ReasoningRec在推荐预测方面比最先进的方法高出12.5%，同时提供了易于理解的解释。代码可在以下链接获取：https://github.com/millenniumbismay/reasoningrec。**|
|**2024-10-30**|**SciPIP: An LLM-based Scientific Paper Idea Proposer**|Wenxiao Wang et.al.|[2410.23166](http://arxiv.org/abs/2410.23166)|null|知识的指数增长和跨学科研究的复杂性给研究人员带来了显著挑战，包括信息过载和探索新想法的困难。大型语言模型（LLMs）如GPT-4在增强想法提案方面显示出巨大潜力，但如何有效利用大模型进行合理的想法提案尚未得到充分探讨。本文提出了一种科学论文想法提案器（SciPIP）。基于用户提供的研究背景，SciPIP从文献数据库中检索有用论文，同时利用LLMs的能力生成更多新颖且可行的想法。为此，我们构建了一个文献检索数据库，提取大量论文的多维度信息以便快速访问。然后，提出了一种基于语义、实体和引用共现的文献检索方法，从多个方面根据用户提供的背景搜索相关文献。在文献检索之后，我们引入了双路径想法提案策略，其中一条路径从检索到的文献中推断解决方案，另一条路径通过模型头脑风暴生成原创想法。然后我们将两者结合起来以实现可行性与原创性的良好平衡。通过在自然语言处理（NLP）领域的广泛实验，我们证明SciPIP可以检索与现有顶级会议论文类似的引文，并生成许多与其一致的想法。此外，我们使用大型语言模型评估了SciPIP生成的其他想法的原创性，进一步验证了我们提出方法的有效性。代码和数据库已发布在https://github.com/cheerss/SciPIP。|
|**2024-10-30**|**Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning**|Keqin Bao et.al.|[2410.23136](http://arxiv.org/abs/2410.23136)|**[link](https://github.com/ym689/rec_icl)**|**频繁更新基于大型语言模型（LLM）的推荐系统以适应新的用户兴趣，就像传统推荐系统所做的那样，由于高昂的训练成本，即使有加速方法也是不切实际的。本文探讨了在不进行任何模型更新的情况下，通过利用情境学习（ICL）来适应动态用户兴趣的方法，这种方法使LLM能够从输入中的少量示例中学习新任务。使用新的兴趣示例作为ICL的少量示例，LLM可以实时学习兴趣，从而避免了模型更新的需求。然而，现有的基于LLM的推荐器在推荐调优过程中经常失去在情境学习的能力，而原始LLM的情境学习缺乏针对推荐任务的关注。为了解决这个问题，我们提出了RecICL，它定制了针对推荐任务的情境学习，用于实时推荐。RecICL以情境学习格式组织训练示例，确保在调优过程中保留情境学习能力并与其推荐任务对齐。广泛的实验表明，RecICL在无需模型更新的情况下实现了实时推荐的有效性。我们的代码可在https://github.com/ym689/rec_icl获取。**|
|**2024-10-29**|**Enhancing Code Annotation Reliability: Generative AI's Role in Comment Quality Assessment Models**|Seetharam Killivalavan et.al.|[2410.22323](http://arxiv.org/abs/2410.22323)|null|本文探索了一种新颖的方法，通过利用生成式人工智能技术来提升二元分类模型在评估代码注释质量方面的性能。我们通过将来自多个GitHub仓库的1,437个新生成的代码-注释对（标记为“有用”或“无用”）整合到一个现有的C语言数据集中（该数据集包含9,048对），展示了模型性能的显著提升。采用先进的大语言模型后，我们的方法使得支持向量机（SVM）模型的精确率提高了5.78%，从0.79提升至0.8478，同时人工神经网络（ANN）模型的召回率提高了2.17%，从0.731提升至0.7527。这些结果突显了生成式人工智能在改进代码注释分类模型中的价值，为软件开发和质量控制中的模型准确性提升提供了重要的潜力。本研究为在实际软件工程环境中整合生成技术以优化机器学习模型提供了乐观的前景。|
|**2024-10-29**|**Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting**|Can Chen et.al.|[2410.22318](http://arxiv.org/abs/2410.22318)|**[link](https://github.com/canchen-cc/online-llm-detection)**|**近年来，区分机器生成文本和人类撰写文本的算法研究引起了广泛关注。现有方法通常是在离线设置下进行，即在给定的数据集中包含真实文本和机器生成文本的混合样本，任务是确定数据集中的每个样本是由大型语言模型（LLM）还是由人类生成的。然而，在许多实际场景中，如新闻网站、社交媒体账户或其他论坛发布的文章是以流式方式发布的。因此，在这种在线场景中，如何快速且准确地确定这些来源是否为LLM，并具有强大的统计保证，对于这些媒体或平台有效地运作并防止错误信息和其他潜在的LLM误用至关重要。为了解决在线检测的问题，我们开发了一种基于顺序假设检验的算法，该算法不仅建立并补充了现有的离线检测技术，而且还具备统计保证，包括控制错误发现率和正确识别来源为LLM的预期时间。实验结果证明了我们方法的有效性。**|
|**2024-10-29**|**Natural Language Inference Improves Compositionality in Vision-Language Models**|Paola Cascante-Bonilla et.al.|[2410.22315](http://arxiv.org/abs/2410.22315)|null|在视觉-语言模型（VLMs）中，组合推理仍然是一个挑战，因为这些模型通常难以关联对象、属性和空间关系。最近的方法试图通过依赖文本描述的语义，利用大规模语言模型（LLMs）将问题和答案分解成子集来解决这些问题。然而，这些方法主要在表面层次上操作，未能引入更深的词汇理解，同时还会引入由LLM生成的错误假设。针对这些问题，我们提出了Caption Expansion with Contradictions and Entailments (CECE)，这是一种基于原理的方法，利用自然语言推理（NLI）从给定的前提生成蕴涵和矛盾。CECE生成词汇上多样的句子，同时保持其核心意义。通过广泛的实验，我们展示了CECE增强了可解释性，并减少了对有偏见或表面特征的过度依赖。通过平衡原始前提与CECE，我们在无需额外微调的情况下显著优于先前的方法，在衡量图像-文本对齐的人类判断得分的基准测试中取得了最先进的结果，并在Winoground上实现了+19.2%（组分数）和在EqBen上实现+12.9%（组分数）的性能提升，超过了最佳现有工作（使用针对性数据微调）。|
|**2024-10-29**|**GPT-4o reads the mind in the eyes**|James W. A. Strachan et.al.|[2410.22309](http://arxiv.org/abs/2410.22309)|null|大型语言模型（LLMs）能够从文本中重现人类类似推理的能力，包括关于情绪和心理状态的推理。然而，这种能力是否扩展到其他模态尚不清楚。人类具有通过他人的眼睛读心的复杂能力。在此研究中，我们测试了这一能力是否也存在于GPT-4o这一多模态LLM中。我们使用了两种广泛使用的心理理论测试版本，即“眼睛中的心智阅读测试”和“多元种族眼睛中的心智阅读测试”。结果发现，GPT-4o在解释来自直立面部的心理状态方面优于人类，但在面部倒置时表现较差。尽管我们样本中的人类在白人和非白人面孔之间没有表现出差异，但GPT-4o对白人面孔的准确度高于非白人面孔。GPT-4o的错误并非随机出现，而是揭示了一种高度一致但错误的处理心理状态信息的方式，在不同试验中呈现出方向依赖的错误结构，这种结构在面对倒置面孔时与人类存在定性差异，而在面对直立面孔时则无明显区别。这些发现强调了先进的心理状态推理能力和人类类似的面部处理特征，如反转效应，在GPT-4o中共存，同时其信息处理方式与人类存在显著差异。|
|**2024-10-29**|**SVIP: Towards Verifiable Inference of Open-source Large Language Models**|Yifan Sun et.al.|[2410.22307](http://arxiv.org/abs/2410.22307)|null|开源的大语言模型（LLMs）在自然语言理解和生成方面展示了显著的能力，并在各个领域得到了广泛应用。然而，随着模型规模的增大，本地部署变得不切实际，许多用户不得不依赖计算服务提供商通过黑盒API进行推理。这种依赖引入了一种新的风险：计算服务提供商可能在未经用户同意的情况下，用较小且能力较弱的模型替代用户请求的LLM，从而提供质量较差的结果，同时节省成本。在这篇论文中，我们形式化了LLM可验证推理的问题。现有的基于密码学或博弈论技术的可验证计算解决方案要么在计算上不经济，要么基于较强的假设。我们引入了SVIP，这是一种基于秘密的可验证LLM推理协议，它利用LLM的中间输出作为唯一的模型标识符。通过在这些输出上训练代理任务，并要求计算服务提供商返回生成的文本和处理过的中间输出，用户可以可靠地验证计算服务提供商是否诚实行事。此外，结合秘密机制进一步增强了我们的协议的安全性。我们在多种强适应性对抗场景下全面分析了我们的协议。广泛的实验表明，SVIP是准确的、可泛化的、计算高效的，并且对各种攻击具有抵抗力。值得注意的是，SVIP的假阴性率低于5%，假阳性率低于3%，并且每次查询的验证时间少于0.01秒。|
|**2024-10-29**|**Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning**|Yihe Deng et.al.|[2410.22304](http://arxiv.org/abs/2410.22304)|null|数学推理是大型语言模型（LLMs）的关键能力，但生成详细且准确的推理轨迹仍然是一个重大挑战。本文介绍了一种利用在线学习流的新方法，以产生高质量的推理轨迹用于LLM微调。我们的方法采用增量输出生产流，其中组件LLM通过迭代通信协作构建解决方案。我们使用带有滚动的在线直接偏好优化（DPO）学习来训练该流，为每个训练样本生成DPO对，并实时更新模型。我们直接比较了通过我们方法与直接模型推理生成的推理轨迹的质量，证明了我们方法在提高LLM在数学推理任务中的性能方面的有效性。|
|**2024-10-29**|**LLMs are Highly-Constrained Biophysical Sequence Optimizers**|Angelica Chen et.al.|[2410.22296](http://arxiv.org/abs/2410.22296)|null|大型语言模型（LLMs）在各种生物任务中，如蛋白质工程和分子设计方面，最近展示了显著的潜力。这些任务通常涉及黑盒离散序列优化，挑战在于生成不仅在生物学上可行而且严格符合细粒度约束的序列。然而，LLMs往往难以应对这些约束，特别是在生物学背景下，验证候选解决方案既昂贵又耗时。在这项研究中，我们探索了将LLMs作为高度约束的双层优化器的可能性，通过一种我们称之为语言模型优化边缘期望（LLOME）的方法。该方法结合了离线和在线优化，利用有限的oracle评估迭代地增强由LLM生成的序列。此外，我们提出了一种新的训练目标——边缘对齐期望（MargE），该目标训练LLM平滑地在奖励分布和参考分布之间插值。最后，我们引入了一个合成测试套件，该套件与实际生物物理问题具有强烈的几何相似性，并且能够在不进行耗时的实验室验证的情况下快速评估LLM优化器。我们的发现表明，与遗传算法基线相比，LLMs在要求较少测试函数评估的情况下实现了显著更低的遗憾解。然而，我们也观察到LLMs表现出适度的校准偏差，容易发生生成器崩溃，并且在没有明确的地面真值奖励可用时难以找到最优解。|
|**2024-10-29**|**Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats**|Mohammad Setak et.al.|[2410.22293](http://arxiv.org/abs/2410.22293)|null|近年来，大型语言模型（LLMs）在自然语言处理和代码合成方面取得了显著进展，使其能够应用于不同领域更复杂的任务。本文探讨了LLMs在代码变异中的应用，这是一个在不改变程序代码功能的前提下改变其结构的过程。传统上，代码变异被用于提高关键任务应用程序的软件健壮性。此外，变异引擎也被恶意软件开发者用来逃避基于特征码的检测方法。现有的恶意软件使用的变异引擎通常只产生有限的代码变化，这些变化仍然可以通过静态代码分析被识别。然而，预训练的LLM所展示的灵活性可能显著改变这种威胁态势，通过允许进行更复杂的代码变异，这些变异不容易通过静态分析检测到。我们可以通过微调和再训练增加由预训练LLM生成的代码的变化。我们称之为代码变异训练。在本文中，我们为基于预训练LLM的代码合成器提出了一个新的代码变异训练定义，并在一个轻量级的预训练模型上展示了这种方法。我们的方法涉及在子例程级别重组（即变异）代码，这使得变异更加可控同时保持语义完整性，并通过单元测试验证。实验结果表明，我们的方法有效地提高了基于LLM的程序合成器在生成多样化且功能正确的代码解决方案方面的变异能力，展示了它们在改变代码变异格局以及与之相关的威胁方面的潜力。|
|**2024-10-29**|**Embedding-based classifiers can detect prompt injection attacks**|Md. Ahsan Ayub et.al.|[2410.22284](http://arxiv.org/abs/2410.22284)|**[link](https://github.com/AhsanAyub/malicious-prompt-detection)**|**大型语言模型（LLMs）因其卓越的生成能力而在各类组织中得到广泛应用。然而，LLMs容易受到各种对抗性攻击，特别是提示注入攻击，这种攻击通过精心设计的恶意提示欺骗LLMs，使其生成有害或不适当的内容。在这篇论文中，我们提出了一种基于嵌入式机器学习（ML）分类器的新方法，以保护基于LLM的应用程序免受这种严重威胁。我们利用三种常用的嵌入模型来生成恶意和良性提示的嵌入，并使用ML分类器预测输入提示是否为恶意。在几种传统的ML方法中，我们使用随机森林和XGBoost构建的分类器表现最佳。我们的分类器在性能上优于开源实现中的最先进的提示注入分类器，后者使用的是仅编码器的神经网络。**|
|**2024-10-29**|**Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced by Large Language Models**|Renzhe Yu et.al.|[2410.22282](http://arxiv.org/abs/2410.22282)|null|自2022年底以来，ChatGPT等类似工具的广泛可用性引发了公众对大型语言模型（LLMs）在提高学习体验和成果方面的潜力的巨大兴趣和实验努力，特别是对于来自弱势背景的学习者。然而，很少有研究系统地考察了LLMs的实际可用性对教育公平性的现实影响，除了理论预测和创新LLM应用的控制研究之外。为了描绘LLMs不平等趋势，我们分析了一所美国公立少数族裔服务院校2021年至2024年间2391门课程中16791名大学生提交的1140328篇学术写作作业。研究发现，在LLMs可用之后，学生的整体写作质量逐渐提高，并且语言优势和劣势学生之间的写作质量差距逐渐缩小。然而，这种平等化效应更多集中在较高社会经济地位的学生身上。这些发现揭示了LLMs时代的数字鸿沟，并提出了关于LLMs在早期阶段的公平效益的问题，强调了研究人员和从业者需要制定负责任的做法以通过LLMs改善教育公平性。|
|**2024-10-28**|**Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics**|Yaniv Nikankin et.al.|[2410.21272](http://arxiv.org/abs/2410.21272)|**[link](https://github.com/technion-cs-nlp/llm-arithmetic-heuristics)**|为了探讨大型语言模型（LLMs）在解决推理任务时是通过学习稳健的可泛化算法，还是通过记忆训练数据，我们选择了算术推理作为代表性任务进行研究。通过因果分析，我们识别出模型的一个子部分（一个电路），该部分解释了基本算术逻辑中模型大部分的行为，并检查了其功能。通过关注单个电路神经元的层面，我们发现了一组重要的稀疏神经元，它们实现了简单的启发式方法。每个启发式方法识别数值输入模式并输出相应的答案。我们假设，这些启发式神经元的组合是生成正确算术答案的机制。为了验证这一点，我们将每个神经元分类为几种启发式类型——例如，当操作数落在某个范围内时激活的神经元——并发现这些启发式类型的无序组合是解释模型在算术提示上准确性的主要机制。最后，我们证明这种机制在训练早期就是算术准确性的重要来源。总的来说，我们在多个LLM上进行的实验结果表明，LLMs执行算术运算既不是依靠稳健的算法，也不是依靠记忆；相反，它们依赖于“一组启发式方法”。|
|**2024-10-28**|**LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior**|Hanyu Wang et.al.|[2410.21264](http://arxiv.org/abs/2410.21264)|null|我们介绍了LARP，这是一种新颖的视频标记器，旨在克服当前用于自回归（AR）生成模型的视频标记方法的局限性。与传统的基于补丁的标记器直接将局部视觉补丁编码为离散标记不同，LARP引入了一种整体标记方案，通过一组学习到的整体查询来收集视觉内容的信息。这种设计使LARP能够捕捉更全局和语义化的表示，而不仅仅是局限于局部补丁级别的信息。此外，它还提供了灵活性，支持任意数量的离散标记，从而根据任务的具体需求实现自适应和高效的标记。为了使离散标记空间与下游AR生成任务对齐，LARP集成了一个轻量级的AR变换器作为训练时的先验模型，该模型在离散潜在空间上预测下一个标记。通过在训练过程中结合先验模型，LARP学习了一个不仅优化了视频重建的潜在空间，而且结构上更适合自回归生成的潜在空间。此外，这一过程定义了离散标记的顺序，在训练过程中逐步将其推向最优配置，确保推理时更平滑和准确的AR生成。全面的实验表明，LARP表现强劲，在UCF101分类条件下的视频生成基准上达到了最先进的FVD分数。LARP增强了AR模型与视频的兼容性，并开启了构建统一的高保真多模态大型语言模型（MLLMs）的可能性。|
|**2024-10-28**|**LongReward: Improving Long-context Large Language Models with AI Feedback**|Jiajie Zhang et.al.|[2410.21252](http://arxiv.org/abs/2410.21252)|**[link](https://github.com/THUDM/LongReward)**|尽管在开发长上下文大型语言模型（LLMs）方面取得了显著进展，但这些模型合成的数据质量往往影响了有监督微调（SFT）模型的长上下文性能，并导致固有的局限性。原则上，适当的奖励信号可以利用强化学习（RL）进一步提升模型的能力。然而，在长上下文场景中如何获得可靠的奖励仍然是一个未探索的问题。为此，我们提出了LongReward，这是一种新颖的方法，它利用现成的LLM从四个维度（即：有用性、逻辑性、准确性和完整性）提供长上下文模型响应的奖励，并为每个维度设计了详细的评估流程。通过结合LongReward和离线RL算法DPO，我们能够有效地改进长上下文SFT模型。我们的实验表明，LongReward不仅显著提升了模型的长上下文性能，还增强了它们遵循短指令的能力。我们还发现，带有LongReward的长上下文DPO和传统的短上下文DPO可以一起使用而不损害任何一方的性能。|
|**2024-10-28**|**Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback**|Nour Jedidi et.al.|[2410.21242](http://arxiv.org/abs/2410.21242)|null|构建有效的密集检索系统在缺乏相关性监督时仍然具有挑战性。近期的研究试图通过使用大型语言模型（LLM）来生成假设文档，从而找到最接近的真实文档来解决这一问题。然而，这种方法完全依赖于LLM具备与查询相关的领域特定知识，这在实践中可能不可行。此外，生成假设文档的方法效率低下，因为对于每个查询，LLM需要生成大量的标记。为了解决这些挑战，我们引入了基于相关反馈的真实文档嵌入（ReDE-RF）。受相关反馈的启发，ReDE-RF提出将假设文档生成重新定义为相关性估计任务，利用LLM选择哪些文档应被用于最近邻搜索。通过这种重新定义，LLM不再需要领域特定的知识，而只需要判断什么是相关的。此外，相关性估计仅要求LLM输出一个标记，从而提高每次查询的搜索延迟。我们的实验表明，ReDE-RF在广泛的低资源检索数据集上始终超越最先进的零样本密集检索方法，并且在每次查询的延迟方面也取得了显著改进。|
|**2024-10-28**|**Hierarchical Knowledge Graph Construction from Images for Scalable E-Commerce**|Zhantao Yang et.al.|[2410.21237](http://arxiv.org/abs/2410.21237)|null|知识图谱（KG）在各种AI系统中扮演着越来越重要的角色。对于电子商务而言，构建高效且低成本的自动化知识图谱是实现众多成功下游应用的基础。本文提出了一种新颖的方法，可以从原始产品图像中构建结构化的产品知识图谱。该方法充分利用了视觉语言模型（VLM）和大型语言模型（LLM）的最新进展，实现了整个过程的完全自动化，并允许及时更新图谱。我们还提供了一个经过人工标注的电子商务产品数据集，用于评估知识图谱构建中的产品属性提取。我们的方法在所有指标和评估属性上都优于基线方法，展示了其有效性和广阔的应用潜力。|
|**2024-10-28**|**Flaming-hot Initiation with Regular Execution Sampling for Large Language Models**|Weizhe Chen et.al.|[2410.21236](http://arxiv.org/abs/2410.21236)|null|自ChatGPT发布以来，大型语言模型（LLMs）在各个领域展示了显著的能力。在开发这些通用能力时，一个关键的挑战是高效地获取多样化且高质量的数据。这在与沙盒检查器相关的推理任务中尤为重要，例如数学或代码问题，目标是提高生成正确解决方案的概率。在这项工作中，我们介绍了Flaming-hot Initiation with Regular Execution（FIRE）采样方法，这是一种简单但非常有效的方法，可以高效地找到好的响应。我们的实证结果表明，FIRE采样提高了推理时间生成的质量，并且在对齐阶段的训练中也受益。此外，我们探讨了FIRE采样如何通过促进多样性来提升性能，并分析了在响应的不同位置使用FIRE的影响。|
|**2024-10-28**|**LoRA vs Full Fine-tuning: An Illusion of Equivalence**|Reece Shuttleworth et.al.|[2410.21228](http://arxiv.org/abs/2410.21228)|null|微调是将预训练的大规模语言模型适应到下游任务中的关键范式。最近的研究表明，低秩自适应（LoRA）方法在各种任务上能够以极小的可训练参数量达到与完全微调模型相当的性能。即使两种方法学习到的模型准确性相似，它们的学习解决方案真的等价吗？我们通过分析模型权重矩阵的谱属性来研究不同的微调方法如何改变预训练模型。我们发现，全微调和LoRA生成的权重矩阵在奇异值分解结构上表现出很大的不同；此外，当在超出适应任务分布的情况下测试时，经过微调的模型显示出不同的泛化行为。更具体地说，我们首先展示了使用LoRA训练的权重矩阵具有新的高排名奇异向量，我们称之为“入侵维度”。这些入侵维度在全微调过程中不会出现。其次，我们展示了尽管具有入侵维度的LoRA模型在目标任务上的表现与全微调模型相当，但它们对预训练分布的建模效果较差，并且在顺序适应多个任务时的鲁棒性较低。高秩、秩稳定的LoRA模型甚至在与低秩LoRA模型执行相同任务时，也与全微调模型非常接近。这些结果表明，即使在相同的微调分布上表现相同，LoRA更新的模型和全微调模型访问了参数空间的不同部分。我们最后探讨了为什么入侵维度会在LoRA微调模型中出现，为什么它们是不理想的，以及如何最小化其影响。|
|**2024-10-28**|**Lifting the Veil on the Large Language Model Supply Chain: Composition, Risks, and Mitigations**|Kaifeng Huang et.al.|[2410.21218](http://arxiv.org/abs/2410.21218)|null|大规模语言模型（LLM）在智力和生产力方面引发了显著的影响。近年来，商业和开源LLM的引入呈现出巨大的增长趋势。许多企业已将LLM集成到其应用中以解决特定领域的任务。然而，将LLM整合到具体业务场景中不仅仅需要使用这些模型本身，而是一个系统的过程，涉及大量的组成部分，这些组成部分统称为LLM供应链。LLM供应链内在地承载着风险。因此，理解可能引入供应链的组件类型以及相关的风险至关重要，这有助于不同的利益相关者实施有效的缓解措施。虽然一些文献涉及与LLM供应链相关的风险，但目前还没有论文明确界定其范围、识别固有风险并探讨潜在的缓解策略。鉴于LLMs已成为新时代的重要基础设施，我们认为对LLM供应链及其固有风险和缓解策略进行彻底审查对于行业从业者避免潜在损失具有重要价值，并且对于学术研究人员重新思考现有方法和探索新的研究途径也具有启发意义。我们的论文提供了LLM供应链的全面概述，详细介绍了利益相关者、组成元素以及供应类型。我们开发了与各种供应链利益相关者和组件相关的风险类型、风险行为和缓解措施的分类法。总而言之，我们的工作探讨了LLM供应链的技术和操作方面，为研究和工程人员在不断发展的LLM领域提供有价值的见解。|
|**2024-10-28**|**BongLLaMA: LLaMA for Bangla Language**|Abdullah Khan Zehady et.al.|[2410.21200](http://arxiv.org/abs/2410.21200)|null|孟加拉语（或“ Bengali”）是一种使用约2.4亿母语者和大约3亿人使用的语言。尽管它是世界上第五大使用语言，孟加拉语仍被视为一种“低资源”语言，现有的预训练语言模型在孟加拉语处理（BLP）任务上往往表现不佳。本研究通过引入BongLLaMA（即孟加拉语-LLaMA），解决了这一问题，这是一种专门针对大型孟加拉语语料库和指令调优数据集进行微调的开源大型语言模型。我们介绍了我们的方法论、数据增强技术、微调细节以及全面的基准测试结果，展示了BongLLaMA在孟加拉语处理任务中的效用。我们相信BongLLaMA将成为孟加拉语模型的新标准基线，从而促进未来专注于这种广泛使用但“低资源”的语言的基准研究。所有BongLLaMA模型均可供公众使用，网址为https://huggingface.co/BanglaLLM。|
|**2024-10-29**|**Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction**|Qintong Zhang et.al.|[2410.21169](http://arxiv.org/abs/2410.21169)|null|文档解析对于将非结构化和半结构化文档（如合同、学术论文和发票）转换为结构化的、机器可读的数据至关重要。文档解析从非结构化输入中提取可靠且结构化的数据，为众多应用提供了极大的便利。特别是随着大型语言模型的最新进展，文档解析在知识库构建和训练数据生成方面发挥着不可或缺的作用。本文综述了当前文档解析的状态，涵盖了关键的方法论，从模块化流水线系统到由大型视觉-语言模型驱动的端到端模型。详细探讨了核心组件，包括布局检测、内容提取（包括文本、表格和数学表达式）以及多模态数据集成。此外，本文还讨论了模块化文档解析系统和视觉-语言模型在处理复杂布局、整合多个模块和识别高密度文本时所面临的挑战。文章强调了开发更大和更多样化数据集的重要性，并概述了未来的研究方向。|
|**2024-10-25**|**The Potential and Value of AI Chatbot in Personalized Cognitive Training**|Zilong Wang et.al.|[2410.19733](http://arxiv.org/abs/2410.19733)|null|近年来，全球人口老龄化加速导致认知障碍，如阿尔茨海默病的发病率增加，给公共卫生带来了巨大挑战。尽管目前尚无有效治疗方法可以逆转阿尔茨海默病，但预防和早期干预，包括认知训练，至关重要。本报告探讨了AI聊天机器人在增强个性化认知训练方面的潜力。我们介绍了ReMe，这是一个基于网络的框架，旨在创建AI聊天机器人以促进认知训练研究，特别是针对从个人生活日志中提取的情节记忆任务。通过利用大型语言模型，ReMe提供了更友好、互动和个性化的培训体验。案例研究表明，ReMe通过生活回忆和开放式语言谜题有效地吸引了用户，突显了其在改善认知训练设计方面的潜力。尽管取得了令人鼓舞的结果，但仍需要进一步研究，通过包括认知能力评估在内的大规模研究来验证培训的有效性。总体而言，ReMe为个性化认知训练提供了一种有前景的方法，利用AI技术满足日益增长的认知健康非药物干预需求，未来的研究旨在扩展其应用范围和有效性。|
|**2024-10-25**|**Counting Ability of Large Language Models and Impact of Tokenization**|Xiang Zhang et.al.|[2410.19730](http://arxiv.org/abs/2410.19730)|**[link](https://github.com/juntaic7/impact-of-tokenization-in-the-counting-ability-of-language-models)**|Transformers作为现代大型语言模型（LLMs）的基石，面临着固有的架构限制，这限制了它们的推理能力。与循环网络不同，Transformers缺乏循环连接，使其只能进行恒定深度的计算。这种限制使它们在TC $^0$ 复杂性类中，从理论上讲，无法解决那些需要输入长度增加时推理深度也相应增加的任务。计数作为许多推理任务的基本组成部分，也需要推理深度随着任务复杂度线性增长才能进行归纳。尽管先前的研究已经确定了基于Transformer的专家模型在计数任务中的能力上限，但这些发现并不能直接应用于通用LLM，因为它们的推理机制存在差异。最近的研究指出，链式思考（CoT）推理可以在一定程度上缓解Transformer在计数任务中的架构限制。然而，关于分词在这些模型中的作用却很少受到关注。不同于通常使用字符级分词的专家模型，LLM通常依赖于字节级（BPE）分词器，这从根本上改变了推理处理的方式。我们的研究探讨了分词对LLM计数能力的影响，揭示了基于分词方式的不同导致显著的性能变化。我们提供了理论和实验分析，为如何通过选择合适的分词方法来增强模型的理论可计算性提供了见解，从而启发设计新的分词方法以提高LLM的推理能力。|
|**2024-10-25**|**FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning**|Nicole Cho et.al.|[2410.19727](http://arxiv.org/abs/2410.19727)|null|从大量数据源生成金融智能通常依赖于传统的方法，如知识图谱构建或数据库工程。近年来，针对金融领域的特定大型语言模型（LLM）已经出现。尽管这些进展令人鼓舞，但仍存在一些限制，例如高推理成本、幻觉以及同时分析高维金融数据的复杂性。这促使我们发明了FISHNET（金融智能从子查询、协调、神经条件、专家集群和任务规划），这是一种代理架构，能够完成超过98,000份监管文件的极其复杂的分析任务，这些文件在语义、数据层次或格式上差异巨大。FISHNET在金融洞察生成方面表现出色（成功率为61.8%，路由为5.0%，RAG R-精确度为45.6%）。我们进行了严格的消融实验，以实证证明FISHNET的成功、每个代理的重要性以及所有代理组装优化性能。我们的模块化架构可以应用于各种用例，提供可扩展性、灵活性和对金融任务至关重要的数据完整性。|
|**2024-10-25**|**2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision**|Shilong Li et.al.|[2410.19720](http://arxiv.org/abs/2410.19720)|null|近年来，直接偏好优化（DPO）在使大型语言模型（LLMs）与人类偏好对齐方面取得了显著进展，这得益于其简单性和有效性。然而，现有的方法通常优化一个标量分数或排名奖励，从而忽略了人类偏好的多维性质。在这项工作中，我们提出将DPO的偏好扩展到两个维度：片段和方面。我们首先引入了一个名为HelpSteer-2D的二维监督数据集。对于片段维度，我们将响应分成句子并为每个片段分配分数。对于方面维度，我们精心设计了几项标准以涵盖响应质量评估标准。利用二维信号作为反馈，我们开发了一个2D-DPO框架，将总体目标分解为多片段和多方面的目标。在流行的基准测试中进行的广泛实验表明，2D-DPO的表现优于那些优化标量或一维偏好的方法。|
|**2024-10-25**|**TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning**|Xiangyu Zeng et.al.|[2410.19702](http://arxiv.org/abs/2410.19702)|null|多模态大型语言模型（MLLMs）在短视频理解方面已经展示了令人印象深刻的性能。然而，对于长视频的理解仍然具有挑战性。本文提出了一套新的设计来适应现有的短视频MLLM，以实现长视频理解，包括一个简单而高效的框架来处理长视频序列、一个高质量的视频数据集用于MLLM的接地调优，以及一个精心设计的指令调优任务，以显式地将接地监督纳入传统的问答格式。具体而言，基于VideoChat，我们提出了我们的长视频MLLM，称为VideoChat-T，通过实现令牌洗牌来压缩长视频令牌，并引入时间自适应位置编码（TAPE）来增强视觉表示的时间感知。同时，我们引入了TimePro，这是一个综合性的接地为中心的指令调优数据集，由9个任务和34.9万个高质量的接地标注组成。值得注意的是，我们设计了一种新的指令调优任务类型，称为时间接地字幕，用于执行详细视频描述与相应时间戳预测。这种明确的时间位置预测将指导MLLM在生成描述时正确关注视觉内容，从而减少因LLMs引起的幻觉风险。实验结果表明，我们的TimeSuite成功地提高了短视频MLLM在长视频理解方面的能力，在Egoschema和VideoMME基准测试上分别提高了5.6%和6.8%。此外，VideoChat-T在零样本时间接地能力方面表现出色，显著优于现有的最先进的MLLM。经过微调后，它的表现与传统的监督专家模型相当。|
|**2024-10-25**|**IPPON: Common Sense Guided Informative Path Planning for Object Goal Navigation**|Kaixian Qu et.al.|[2410.19697](http://arxiv.org/abs/2410.19697)|null|在未探索的环境中高效导航到目标物体是通用智能机器人的一项关键技术。最近的方法采用模块化策略，结合经典的探索算法（特别是前沿探索）与学习的语义映射/探索模块来解决这一物体导航问题。本文介绍了一种新颖的信息路径规划和三维物体概率映射方法。该映射模块通过语义分割和贝叶斯滤波计算感兴趣物体的概率。此外，它还存储常见物体的概率，这些概率基于大型语言模型中的常识先验，从而从语义上引导探索。当当前视角捕获了足够多且置信度高的物体为感兴趣物体的体素时，规划器终止。尽管我们的规划器采用了零样本方法，但在Habitat物体导航挑战2023中，它在成功加权路径长度（SPL）和软SPL指标上达到了最先进的性能，比其他工作高出20%以上。此外，我们在真实机器人上验证了其有效性。项目网页：https://ippon-paper.github.io/|
|**2024-10-25**|**Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs**|Yifei Zhang et.al.|[2410.19694](http://arxiv.org/abs/2410.19694)|null|微调大型语言模型（LLMs）已成为将预训练模型适应下游任务的重要技术。然而，LLMs的巨大规模带来了显著的计算复杂性和资源需求挑战。低秩适应（LoRA）作为一种有前景的解决方案应运而生。然而，实际应用中的低秩适应与理论最优之间存在差距。在这项工作中，我们提出了极端梯度提升LoRA（XGBLoRA），这是一种新的框架，通过利用集成学习的力量来弥合这一差距。受梯度提升启发，XGBLoRA迭代地学习并融合一系列LoRA适应以优化模型预测。它在性能上优于标准LoRA，同时保持了秩-1适应的计算效率。我们提供了理论分析以证明方法的收敛性和最优性，并在各种自然语言处理任务上进行了广泛的实验。结果表明，XGBLoRA始终优于标准LoRA，并且在显著减少可训练参数的情况下实现了与全微调相当的性能。这项工作推进了LLMs的参数高效微调技术，并为优化性能和效率的同时将LLMs适应到下游任务提供了有前景的解决方案。|
|**2024-10-25**|**APRICOT: Active Preference Learning and Constraint-Aware Task Planning with LLMs**|Huaxiaoyue Wang et.al.|[2410.19656](http://arxiv.org/abs/2410.19656)|null|家庭机器人在执行个性化任务时，必须巧妙地平衡用户偏好与环境限制。我们专注于在受限空间内进行组织任务，例如将物品放入冰箱，其中用户的放置偏好常常与物理限制相冲突。机器人必须根据少量演示来推断用户的偏好，这比详细定义所有需求更容易让用户操作。虽然最近的研究使用大型语言模型（LLMs）从用户演示中学习偏好，但它们面临两个基本挑战。首先，在解释用户行为时存在固有的模糊性，因为单一观察到的行为可能对应多种偏好。其次，并非所有用户偏好在环境中都是实际可行的，因为存在几何约束。为了解决这些挑战，我们引入了APRICOT，这是一种新颖的方法，结合了基于LLM的贝叶斯主动偏好学习和考虑环境约束的任务规划。APRICOT通过主动查询用户来优化生成的偏好，并动态调整其计划以尊重环境限制。我们在多样化的组织任务数据集上评估了APRICOT，并展示了其在现实场景中的有效性，证明了其在偏好满意度和计划可行性方面的显著提升。该项目网站位于https://portal-cornell.github.io/apricot/|
|**2024-10-25**|**Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina**|Yuan Gao et.al.|[2410.19599](http://arxiv.org/abs/2410.19599)|null|近期研究表明，大型语言模型（LLMs）可以展现出类似人类的推理能力，在经济实验、调查和政治讨论中与人类行为一致。这促使许多人提出可以将LLMs作为人类在社会科学中的替代品。然而，LLMs与人类之间存在根本性的差异，它们依赖于概率模式，缺乏塑造人类认知的具身经验或生存目标。我们通过11-20金钱请求游戏来评估LLMs的推理深度。几乎所有先进的方法都无法在许多模型中复制人类的行为分布，除非在使用大量人类行为数据进行微调的情况下。失败的原因多种多样，涉及输入语言、角色和保护措施等因素。这些结果提醒我们不要将LLMs用于研究人类行为或将其作为人类的替代品。|
|**2024-10-25**|**Diverse Sign Language Translation**|Xin Shen et.al.|[2410.19586](http://arxiv.org/abs/2410.19586)|**[link](https://github.com/XinS0909/Diverse_Sign_Language_Translation)**|类似于口语，一个手语表达可能对应多个有效的文本解释。因此，对手语翻译（SLT）模型进行单一的映射学习可能是不充分的，尤其是在数据有限的情况下。在这项工作中，我们引入了多样化的手语翻译（DivSLT）任务，旨在为手语视频生成多样且准确的翻译。首先，我们利用大型语言模型（LLM）为广泛使用的CSL-Daily和PHOENIX14T SLT数据集生成多个参考。这里，仅邀请母语人士来润色不准确的参考，从而显著提高了注释效率。其次，我们提供了一个基准模型以推动该任务的研究。具体来说，我们研究了多参考训练策略，以使我们的DivSLT模型能够实现多样化的翻译。然后，为了提高翻译准确性，我们采用了最大化翻译结果奖励的强化学习目标。此外，我们使用多种指标来评估DivSLT任务的准确性、多样性和语义精度。在丰富后的数据集上的实验结果表明，我们的DivSLT方法不仅实现了更好的翻译性能，还获得了多样化的翻译结果。|
|**2024-10-24**|**Unbounded: A Generative Infinite Game of Character Life Simulation**|Jialu Li et.al.|[2410.18975](http://arxiv.org/abs/2410.18975)|null|我们介绍了生成无限游戏的概念，这是一种视频游戏，它超越了传统固定、硬编码系统的边界，通过使用生成模型来实现。受James P. Carse关于有限游戏和无限游戏区别的启发，我们利用最近在生成式人工智能方面的进展来创建《无界》——一款完全封装在生成模型中的角色生活模拟游戏。具体来说，《无界》受到沙盒生活模拟游戏的启发，允许你通过喂养、玩耍和引导等方式与你在虚拟世界中的自主虚拟角色互动，其中一些机制是开放式的，并且可以是突发性的。为了开发《无界》，我们在语言模型和视觉生成领域提出了技术上的创新。具体而言，我们提出了：(1)一种专门设计的、经过蒸馏的大规模语言模型（LLM），该模型能够实时动态生成游戏机制、叙事和角色互动，(2)一种新的动态区域图像提示适配器（IP-Adapter），用于视觉模型，确保角色在多个环境中的视觉生成既一致又灵活。我们通过定性和定量分析对我们的系统进行了评估，结果显示，在角色生活模拟、用户指令遵循、叙事连贯性和视觉一致性方面，与传统相关方法相比有显著改进。|
|**2024-10-24**|**Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms**|Zhangheng Li et.al.|[2410.18967](http://arxiv.org/abs/2410.18967)|null|构建一个通用的用户界面（UI）理解模型面临着诸多挑战，包括平台多样性、分辨率变化和数据限制等问题。本文介绍了一种名为Ferret-UI 2的新模型，这是一种多模态大语言模型（MLLM），旨在实现跨多种平台的通用UI理解，包括iPhone、Android、iPad、网页和Apple TV等平台。Ferret-UI 2在原有Ferret-UI的基础上引入了三项关键创新：支持多种平台类型、通过自适应缩放实现高分辨率感知，以及利用GPT-4o结合集合标记视觉提示生成高级任务训练数据。这些改进使Ferret-UI 2能够执行复杂的、以用户为中心的交互，使其在不断扩展的平台生态系统中具有高度的通用性和适应性。广泛的实验证明，在指向、定位、以用户为中心的高级任务（包含9个子任务×5个平台）、GUIDE下一步预测数据集和GUI-World多平台基准测试中，Ferret-UI 2显著优于Ferret-UI，并且展示了强大的跨平台迁移能力。|
|**2024-10-24**|**Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions**|Yujuan Fu et.al.|[2410.18966](http://arxiv.org/abs/2410.18966)|null|大型语言模型（LLMs）在各种基准测试中表现出色，显示出作为通用任务解决者的潜力。然而，由于这些模型通常是在大量数据上进行训练的，因此对其评估的一个重要问题是数据污染问题，即训练数据和评估数据集之间的重叠会夸大性能评估。虽然已经开发了多种方法来识别数据污染，但这些方法依赖于特定的假设，而这些假设可能并不普遍适用于不同的设置。为了弥补这一差距，我们系统地回顾了47篇关于数据污染检测的论文，对其中的基础假设进行了分类，并评估了它们是否经过严格的验证。我们确定并分析了八类假设，并以三个假设作为案例研究。我们的分析表明，在对用于预训练LLMs的实例进行分类时，基于这三种假设的检测方法的表现接近于随机猜测，这表明当前的LLMs学习的是数据分布而不是记忆个别实例。总体而言，这项工作强调了方法明确陈述其基础假设并在各种场景下测试其有效性的重要性。|
|**2024-10-24**|**OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning**|Xiaoqiang Wang et.al.|[2410.18963](http://arxiv.org/abs/2410.18963)|null|大型语言模型（LLMs）和大型多模态模型（LMMs）在自动化复杂任务如网页浏览和游戏方面展现出了巨大的潜力。然而，它们在跨多样化应用中的泛化能力仍然有限，这限制了其更广泛的应用。为了解决这一挑战，我们提出了OSCAR：通过状态感知推理和重规划的操作系统控制。OSCAR是一种通用代理，旨在通过标准化的控制方式（如鼠标和键盘输入）自主导航和与各种桌面和移动应用程序进行交互，同时处理屏幕图像以完成用户命令。OSCAR将人类指令转换为可执行的Python代码，从而实现对图形用户界面（GUI）的精确控制。为了增强稳定性和适应性，OSCAR作为一个状态机运行，并配备了错误处理机制和动态任务重规划功能，使其能够高效地实时调整以应对反馈和异常情况。我们通过广泛的实验在多样化的基准测试上展示了OSCAR的有效性，在这些测试中，它将复杂的操作流程简化为简单的自然语言命令，显著提高了用户的生产力。我们的代码将在发表后开源。|
|**2024-10-24**|**Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in Low-Resource Code**|Jipeng Zhang et.al.|[2410.18957](http://arxiv.org/abs/2410.18957)|null|大型语言模型（LLMs）在生成高资源编程语言（HRPLs）如Python的代码方面表现出色，但在低资源编程语言（LRPLs）如Racket或D上的表现则显著逊色。这种性能差距加剧了数字鸿沟，阻碍了使用LRPLs的开发者从LLM的进步中受益，并在一定程度上强化了未充分代表的编程社区之间的创新差异。虽然为LRPLs生成额外训练数据是一个有前景的方法，但它面临着两个关键挑战：人工标注既费时又昂贵，而LLM生成的LRPL代码质量通常较差。这一问题的根本原因在于自然语言到编程语言的差距（NL-PL Gap），在LRPLs中尤其明显，因为对齐的数据有限。在这项工作中，我们介绍了一种名为Bridge-Coder的新方法，该方法利用LLMs的内在能力来增强其在LRPLs上的性能。我们的方法包括两个关键阶段。首先是桥接生成，通过利用LLMs对一般知识的理解、对HRPLs的熟练程度和上下文学习能力来创建高质量的数据集。然后是桥接对齐，逐步改善自然语言指令与LRPLs之间的对齐。实验结果在多种LRPLs中显示，Bridge-Coder显著提升了模型性能，证明了我们方法的有效性和泛化能力。此外，我们还详细分析了方法的关键组成部分，为未来解决与LRPLs相关挑战的研究提供了有价值的见解。|
|**2024-10-24**|**BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning**|Yujuan Velvin Fu et.al.|[2410.18955](http://arxiv.org/abs/2410.18955)|null|大型语言模型（LLMs）如ChatGPT通过在大规模和多样化的指令跟随语料库上进行微调，能够泛化到新的任务。然而，这些经过指令微调的LLMs在需要领域知识、细粒度文本理解和结构化数据提取的专业医学自然语言理解（NLU）任务中往往表现不佳。为了解决这一问题，我们：(1) 提出了一种统一的提示格式，适用于7个重要的NLU任务，通过跨度提取和多选题问答（QA）来实现；(2) 创建了一个指令微调数据集MNLU-Instruct，利用了多种现有的开源医学NLU语料库；(3) 通过在MNLU-Instruct上对BioMistral进行微调，开发了BioMistral-NLU，一个具有通用性的医学NLU模型。我们在零样本设置下评估了BioMistral-NLU，在两个广泛采用的医学NLU基准测试中，即生物医学语言理解评估（BLUE）和生物医学语言理解和推理基准（BLURB）中的6个重要NLU任务。实验结果表明，我们的BioMistral-NLU在性能上优于原始的BioMistral以及专有的LLMs——ChatGPT和GPT-4。我们与数据集无关的提示策略和在各种NLU任务上的指令微调步骤增强了LLMs在各种医学NLU任务中的泛化能力。消融实验显示，即使总的训练实例数量保持不变，指令微调的任务种类越广，下游零样本泛化能力也越强。|
|**2024-10-24**|**Dynamic Vocabulary Pruning in Early-Exit LLMs**|Jort Vincenti et.al.|[2410.18952](http://arxiv.org/abs/2410.18952)|**[link](https://github.com/matteonulli/vocabulary_pruning)**|**增加大型语言模型（LLMs）的规模已被证明可以提高其性能。然而，这也带来了推理速度变慢和成本增加的问题。早期退出是一种有前景的方法，通过在中间层进行预测来提高LLM推理的效率。然而，现代LLMs中的大词汇量使得所需的置信度估计在计算上非常昂贵，从而降低了效率提升的效果。为了解决这个问题，我们提出在测试时动态剪枝词汇表。具体来说，词汇表在最初的某一层被剪枝，并在整个前向传递过程中使用较小的词汇表。我们的实验表明，这种后处理动态词汇表剪枝方法提高了早期退出LLM中置信度估计的效率，同时保持了具有竞争力的性能。**|
|**2024-10-24**|**SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models**|Zonghao Ying et.al.|[2410.18927](http://arxiv.org/abs/2410.18927)|null|多模态大型语言模型（MLLMs）在用户生成有害输出方面表现出强烈的安全隐患，这促使了安全评估基准的发展。然而，我们观察到现有的MLLMs安全基准存在查询质量低和评估可靠性差的问题，这些问题限制了对MLLMs安全影响的检测，因为随着MLLMs的不断发展，这些基准已显得不足。在本文中，我们提出了一种名为\toolns的综合框架，用于对MLLMs进行安全评估。我们的框架包括一个全面的有害查询数据集和一种自动评估协议，分别旨在解决上述问题。我们首先设计了一个自动安全数据集生成管道，在这个管道中，我们使用一组LLM评判者来识别和分类对MLLMs最具危害性和多样性的风险场景；基于这种分类，我们进一步要求这些评判者相应地生成高质量的有害查询，从而产生了23种风险场景和2300个多模态有害查询对。在安全评估过程中，我们借鉴司法程序中的陪审团制度，开创了一种陪审团审议评估协议，该协议采用协作式LLM来评估目标模型是否表现出特定的有害行为，从而提供可靠且无偏见的内容安全风险评估。此外，我们的基准还可以扩展到音频模态，显示出高度的可扩展性和潜力。基于我们的框架，我们对15种广泛使用的开源MLLMs和6种商业MLLMs（如GPT-4o、Gemini）进行了大规模实验，揭示了现有MLLMs中存在的广泛安全问题，并实例化了关于MLLMs安全性能的一些见解，如图像质量和参数大小。|
|**2024-10-24**|**From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems**|A M Muntasir Rahman et.al.|[2410.18921](http://arxiv.org/abs/2410.18921)|null|考虑一个数学问题：“莉莉昨天从她最好的朋友那里收到了3块饼干，并在早餐时吃了5块。今天，她的朋友又给了她3块饼干。现在莉莉有多少块饼干？”许多大型语言模型（LLMs）在先前的研究中通过计算“3-5+3”的等式来得出答案“1”。然而，从人类的角度来看，我们认识到这个问题的内在缺陷：如果莉莉最初只有3块饼干，她不可能在早餐时吃掉5块。这种差异引发了一个关键问题：当前的LLMs是仅仅作为盲目的解题者，机械地应用数学运算而不进行更深层次的推理，还是能够作为一个逻辑思考者，识别逻辑上的不一致？  为了探讨这个问题，我们提出了一套基准数据集FaultyMath，其中包括多样化的有缺陷的数学问题：i）涵盖多个数学类别，如代数、几何、数论等；ii）具有不同的难度级别；iii）不同类型的缺陷来源——包括常识违反、模糊陈述、数学矛盾等。我们使用FaultyMath对广泛的LLMs进行评估，包括开源、闭源和数学专业模型，从三个方面进行评估：(i) 在没有明确提示的情况下，这些模型能多准确地检测出有缺陷的数学问题？(ii) 当提供关于问题有效性的提示——无论是正确的还是误导性的——LLMs在多大程度上能够适应成为可靠的逻辑思考者？(iii) 当LLMs识别出一个数学问题是错误的时，它们生成的解释有多可靠？通过广泛的实验和详细的分析，我们的结果表明，现有的LLMs大多表现为盲目的解题者，未能具备成为逻辑思考者所需的推理能力。|
|**2024-10-25**|**A Survey on Speech Large Language Models**|Jing Peng et.al.|[2410.18908](http://arxiv.org/abs/2410.18908)|null|大型语言模型（LLMs）在上下文理解和多任务处理方面表现出色。因此，研究人员一直在寻求将LLMs集成到口语理解（SLU）领域的大框架中。不同于传统的通过自动语音识别（ASR）生成文本并依次处理的方法，新的研究集中在设计以音频特征提取为中心、结合多模态信息融合和LLM推理的架构——即所谓的语音LLMs。这种方法能够更丰富地提取音频特征，同时促进音频和文本模态的端到端融合，从而实现从音频数据中进行更深层次的理解和推理。本文阐明了语音LLMs的发展，提供了系统架构和训练策略的深入分析。通过广泛的研究和一系列针对性实验，本文评估了语音LLMs在丰富音频转写方面的进展及其在SLU领域跨任务整合的潜力。此外，本文还指出了通过实验发现的关键挑战，例如在某些条件下LLMs的惰性问题。文章进一步探讨了语音LLMs的训练策略，并基于这些发现提出了潜在解决方案，为该领域的未来研究以及LLMs在多模态环境中的应用提供了有价值的见解和参考。|
|**2024-10-23**|**TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts**|Yuxuan Xie et.al.|[2410.18071](http://arxiv.org/abs/2410.18071)|null|最近，多模态大型语言模型（MLLMs）因其令人印象深刻的性能而备受关注。对MLLMs的评估变得至关重要，因为这有助于分析这些模型的特性并提供有价值的见解。然而，当前的基准测试忽视了提示敏感性的问题——轻微的提示变化可能会导致显著的性能波动。因此，不适当的提示可能会掩盖模型的能力，从而低估模型的性能。此外，不同的模型对于不同提示有不同的偏好，因此使用相同的提示来评估所有模型会导致评估偏差。本文分析了现有基准测试中的这一缺陷，并进一步引入了一个新的评估框架TP-Eval。该框架通过引入提示定制方法来减少评估偏差并挖掘模型的潜力。TP-Eval将重写原始提示，为不同的模型生成不同的定制化提示。特别是，我们针对MLLM评估场景设计了一些模块来实现提示定制。广泛的实验表明，我们的方法能够有效揭示模型的潜力，TP-Eval有望为社区开发更全面和有说服力的MLLM评估基准做出贡献。|
|**2024-10-23**|**LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering**|Qingfei Zhao et.al.|[2410.18050](http://arxiv.org/abs/2410.18050)|**[link](https://github.com/qingfei1/longrag)**|**长上下文问答（LCQA）是一项具有挑战性的任务，旨在通过推理长篇文档来准确回答问题。现有的长上下文大语言模型（LLMs）在LCQA中常常面临“迷失中间”问题。检索增强生成（RAG）通过提供外部事实证据来缓解这一问题。然而，其分块策略破坏了全局长上下文信息，并且在长上下文中低质量的检索会阻碍大语言模型识别有效的事实细节，因为存在大量噪声。为此，我们提出了LongRAG，这是一种通用的、双重视角的、健壮的大语言模型为基础的RAG系统范式，用于增强RAG对复杂长上下文知识的理解（即全局信息和事实细节）。我们将LongRAG设计为一种即插即用的范式，便于适应各种领域和大语言模型。在三个多跳数据集上的广泛实验表明，LongRAG显著优于长上下文大语言模型（提升6.94%），先进的RAG（提升6.16%）和原始RAG（提升17.25%）。此外，我们进行了定量消融研究和多维度分析，强调了系统组件和微调策略的有效性。数据和代码可在https://github.com/QingFei1/LongRAG获取。**|
|**2024-10-23**|**Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases**|Anna Glazkova et.al.|[2410.18040](http://arxiv.org/abs/2410.18040)|null|关键词选择是自然语言处理中的一个具有广泛应用的挑战性任务。由于俄语丰富的形态学特征以及有限的训练数据集，将现有的监督和非监督解决方案应用于俄语面临诸多限制。最近对英文文本的研究表明，大型语言模型（LLMs）成功地解决了生成关键词的任务。这些模型能够在不进行特定任务微调的情况下取得令人印象深刻的结果，使用文本提示即可。在这项工作中，我们评估了基于提示的方法在生成俄文科学摘要关键词方面的表现。首先，我们比较了零样本和少量样本提示方法、微调模型和非监督方法的性能。然后，我们评估了少量样本设置中关键词示例的选择策略。我们展示了人工评估生成的关键词的结果，并通过专家评估分析了模型的优势和劣势。我们的结果显示，即使使用简单的文本提示，基于提示的方法也可以超越常见的基线模型。|
|**2024-10-23**|**MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning**|Jingfan Zhang et.al.|[2410.18035](http://arxiv.org/abs/2410.18035)|null|低秩适应（LoRA）及其混合专家（MOE）变体是高度有效的参数高效微调（PEFT）方法。然而，由于在Transformer层的多个线性模块中添加了LoRA模块和MOE路由器，这些方法在多租户设置中引入了显著的延迟。为了解决这个问题，我们提出了Mixture of Low-Rank Adaptation (MiLoRA)，这是一种新颖且高效的LoRA变体。MiLoRA与之前的MOE风格LoRA方法不同，它将每个LoRA模块视为一个专家，并采用提示感知路由机制。这种机制在生成第一个新标记之前计算一次专家路由结果，并在后续标记中重用这些结果，从而减少延迟。广泛的实验和分析表明，在常识推理任务、数学推理任务以及广泛使用的LLM评估基准上，MiLoRA始终优于强大的PEFT基线，同时具有可比的可调参数预算。此外，与之前的基于LoRA的方法相比，MiLoRA在多租户设置中显著降低了延迟。|
|**2024-10-23**|**GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration**|Xin Li et.al.|[2410.18032](http://arxiv.org/abs/2410.18032)|**[link](https://github.com/bupt-gamma/graphteam)**|**图是现实世界场景中建模关系数据的常用工具，例如社交网络和城市计算。现有的基于大型语言模型（LLM）的图分析方法要么集成图神经网络（GNN）以用于特定的机器学习任务，从而限制了其可移植性；要么完全依赖于LLM自身的推理能力，导致性能不佳。为了解决这些局限性，我们利用了LLM基代理的最新进展，这些进展表明它们能够利用外部知识或工具解决问题。通过模拟人类的问题解决策略，如类比和协作，我们提出了一种基于LLM的多代理系统，名为GraphTeam，用于图分析。GraphTeam由三个模块中的五个LLM基代理组成，这些具有不同专长的代理可以相互协作以应对复杂问题。具体来说，（1）输入-输出规范化模块：问题代理从原始问题中提取并精炼四个关键参数，促进问题理解，而答案代理则组织结果以满足输出要求；（2）外部知识检索模块：我们首先构建了一个包含相关文档和经验信息的知识库，然后搜索代理针对每个问题检索最相关的条目。（3）问题解决模块：给定搜索代理检索到的信息，编码代理使用编程方式生成解决方案；如果编码代理不起作用，则推理代理将直接计算结果而不进行编程。在六个图分析基准上的广泛实验表明，GraphTeam达到了最先进的性能，在准确率方面平均比最佳基线高出25.85%。代码和数据可在<https://github.com/BUPT-GAMMA/GraphTeam>获取。**|
|**2024-10-23**|**MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting**|Sungil Seok et.al.|[2410.18012](http://arxiv.org/abs/2410.18012)|null|美国联邦基金利率在国内外金融市场中扮演着重要角色。然而，研究主要集中在该利率调整的影响上，而不是决策过程本身。最近大型语言模型（LLM）的进步提供了一种可能的方法来重构负责设定联邦基金利率的联邦公开市场委员会（FOMC）会议。在本文中，我们提出了一种五阶段的FOMC会议模拟框架MiniFed，该框架使用LLM代理来模拟现实世界中的FOMC会议成员，并优化FOMC结构。此框架有效地重新激活了FOMC会议过程，并有助于预测联邦基金利率。实验结果表明，我们提出的MiniFed框架在联邦基金利率预测方面具有高准确度，并且在代理行为上与现实世界的对应者保持一致。鉴于目前很少有研究利用LLM代理来模拟大规模的现实世界会议，我们的工作可以作为未来发展的基准。|
|**2024-10-23**|**ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference**|Xin He et.al.|[2410.17954](http://arxiv.org/abs/2410.17954)|null|稀疏混合专家（MoE）模型在性能上优于密集的大语言模型（LLMs），但在推理部署过程中面临显著的内存需求挑战。现有的卸载技术涉及在GPU和CPU之间交换激活和空闲的专家，但这些技术通常受到刚性专家缓存机制的限制。这些机制无法适应动态路由，导致缓存利用率低下，或在预测训练中产生高昂的成本。为了解决这些特定于推理的挑战，我们引入了ExpertFlow，这是一个专门设计的系统，旨在通过适应灵活路由并实现专家在CPU和GPU之间的高效调度来增强推理效率。这减少了开销并提升了系统性能。我们的方法核心是一个基于预测路由路径的卸载机制，利用轻量级预测器在计算开始前准确预测路由路径。这种主动策略允许实时纠正专家缓存中的错误，显著提高缓存命中率并减少专家传输的频率，从而最小化I/O开销。此外，我们还实施了一种动态令牌调度策略，通过在不同批次间重新排列输入令牌来优化MoE推理。这种方法不仅减少了每批次激活的专家数量，还提高了计算效率。我们的广泛实验表明，ExpertFlow实现了高达93.72%的GPU内存节省，并将推理速度提升至基线方法的2到10倍，突显了其有效性和作为资源受限推理场景下的稳健解决方案的重要性。|
|**2024-10-23**|**SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains**|Ran Xu et.al.|[2410.17952](http://arxiv.org/abs/2410.17952)|null|检索增强生成（RAG）通过整合外部知识增强了大型语言模型（LLMs）的问题回答（QA）能力。然而，将通用的RAG系统适应到科学和医学等专业领域时，由于分布差异和有限的领域特定数据访问，会面临独特的挑战。为了解决这一问题，我们提出了SimRAG，这是一种自训练方法，使LLM具备问题回答和问题生成的联合能力，以实现领域适应。我们的方法首先在指令跟随、问答和搜索相关数据上对LLM进行微调。然后，它提示相同的LLM从无标签语料库中生成多样化的领域相关问题，并采用额外的过滤策略来保留高质量的合成示例。通过利用这些合成示例，LLM可以在特定领域的RAG任务中提升性能。在跨越两个基础模型大小和三个领域的11个数据集上的实验表明，SimRAG比基线方法高出1.2%至8.6%。|
|**2024-10-23**|**Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for Enhanced LLM Function Calling**|Nirav Bhan et.al.|[2410.17950](http://arxiv.org/abs/2410.17950)|null|大型语言模型（LLMs）在各个领域展示了非凡的能力，但由于工具使用和功能调用方面的挑战，其经济影响受到了限制。本文介绍了一种名为ThorV2的新架构，该架构显著增强了LLMs的功能调用能力。我们开发了一个全面的基准测试，专注于HubSpot CRM操作，以评估ThorV2与OpenAI和Anthropic的领先模型。我们的结果显示，ThorV2在单个和多API调用任务的准确性、可靠性、延迟和成本效率方面均优于现有模型。我们还表明，ThorV2在多步骤任务中的可靠性更强，并且可扩展性更好，相比传统模型具有明显优势。我们的工作提供了令人兴奋的可能性，即使用显著更小的LLMs实现比当今最佳模型更准确的功能调用。这些进展对于开发更强大的AI助手以及LLMs在现实场景中的广泛应用具有重要意义。|
|**2024-10-23**|**Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced Defense in Large Language Models**|He Cao et.al.|[2410.17922](http://arxiv.org/abs/2410.17922)|**[link](https://github.com/idea-xl/g4d)**|随着大规模语言模型（LLMs）的广泛部署，确保其安全性变得越来越重要。然而，现有的防御方法往往存在两个关键问题：(i) 防御能力不足，尤其是在化学等特定领域场景下，缺乏专门知识可能导致对恶意查询生成有害响应。(ii) 过度防御，这会损害LLMs的一般实用性和响应性。为了解决这些问题，我们引入了一种基于多代理的防御框架，称为Guide for Defense (G4D)，该框架利用准确的外部信息提供用户意图的无偏总结以及分析性安全响应指导。广泛的实验表明，在流行的手册逃脱攻击和良性数据集上，我们的G4D可以在不损害模型一般功能的情况下增强LLM在通用和特定领域的鲁棒性。|
|**2024-10-22**|**Large Language Models Empowered Personalized Web Agents**|Hongru Cai et.al.|[2410.17236](http://arxiv.org/abs/2410.17236)|null|网络代理作为自动化基于用户指令的Web任务完成的一种有前景的方向，显著提升了用户体验。最近，网络代理从传统的代理发展到基于大语言模型（LLM）的网络代理。尽管取得了成功，现有的基于LLM的网络代理忽略了个性化数据（如用户资料和历史Web行为）在辅助理解用户的个性化指令和执行定制化操作方面的重要性。为克服这一局限，我们首先制定了一个基于LLM的个性化网络代理任务，该任务结合了个性化数据和用户指令来实现个性化的指令理解和操作执行。为了应对缺乏全面评估基准的问题，我们构建了一个个性化网络代理基准（PersonalWAB），该基准包含了用户指令、个性化用户数据、Web功能，并提供了三个个性化Web任务的两种评估范式。此外，我们提出了一种个性化用户记忆增强对齐（PUMA）框架，以适应个性化网络代理任务。PUMA利用具有特定任务检索策略的记忆库来筛选相关的历史Web行为。然后，根据这些行为，PUMA通过微调和直接偏好优化来调整LLM进行个性化的操作执行。广泛的实验验证了PUMA在PersonalWAB上优于现有网络代理的优越性。|
|**2024-10-22**|**Automated Spinal MRI Labelling from Reports Using a Large Language Model**|Robin Y. Park et.al.|[2410.17235](http://arxiv.org/abs/2410.17235)|**[link](https://github.com/robinyjpark/autolabelclassifier)**|**我们提出了一种通用的管道，用于使用大型语言模型自动化提取放射学报告中的标签，并在脊柱MRI报告上进行了验证。该标签提取方法的有效性在五种不同的情况中进行了评估：脊柱癌、狭窄、脊椎滑脱、马尾神经受压和疝气。使用开源模型，我们的方法在保留的一组报告上等于或超过了GPT-4的表现。此外，我们展示了所提取的标签可以用来训练影像模型以识别伴随的MRI扫描中的这些已识别的状况。所有使用自动标签训练的分类器表现与使用临床医生手动标注的扫描训练的模型相当。代码可以在<https://github.com/robinyjpark/AutoLabelClassifier>找到。**|
|**2024-10-22**|**Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy**|Benedict Aaron Tjandra et.al.|[2410.17234](http://arxiv.org/abs/2410.17234)|null|大型语言模型（LLMs）以其生成合理但不准确文本的能力而闻名，这种现象在医学或法律等关键应用中带来了显著的风险，因此需要采取稳健的幻觉缓解策略。尽管最近的研究提出了通过微调来教导模型避免回答超出其知识或能力范围的问题的方法，但这些方法依赖于外部的真实标签，或者仅限于短文本回应。为了解决这些限制，我们提出了一种利用语义熵进行微调的方法，这是一种从模型内部进行自我反思得出的不确定性度量，不需要外部标签。我们证明了我们的方法在使用先前研究进行微调的模型上达到了同等或更好的表现，并在多种数据集上实现了对短文本和长文本生成的强大性能。|
|**2024-10-22**|**Few-shot In-Context Preference Learning Using Large Language Models**|Chao Yu et.al.|[2410.17233](http://arxiv.org/abs/2410.17233)|null|设计奖励函数是强化学习中的核心组成部分，但对于非常复杂的行为来说可能具有挑战性。通过用从人类反馈中学习到的奖励函数替代手工编写的奖励函数，基于人类反馈的强化学习（RLHF）已经用于缓解这一挑战。然而，学习这些奖励函数通常效率低下，因为它们往往是从头开始学习的。我们研究了大型语言模型（LLM）是否可以通过将一系列人类偏好转换为表示奖励的代码来减少查询的低效性。我们提出了一种称为“上下文偏好学习”（ICPL）的方法，该方法利用LLM的背景知识来加速从偏好中学习奖励函数的过程。ICPL采用环境上下文和任务描述，合成一组奖励函数，然后反复使用人类对政策结果视频的排名来更新这些奖励函数。通过合成偏好，我们证明ICPL比RLHF高效几个数量级，并且甚至与使用真实奖励函数的方法相比也具有竞争力。最后，我们进行了一系列人类偏好学习试验，观察到ICPL不仅适用于合成设置，还可以在人类参与的循环中有效工作。更多相关信息和视频可以在https://sites.google.com/view/few-shot-icpl/home 获取。|
|**2024-10-22**|**Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods**|Tsachi Blau et.al.|[2410.17222](http://arxiv.org/abs/2410.17222)|null|微调大型语言模型（LLMs）通常涉及更新数十亿个参数。一种更为参数高效的方法是提示调优（PT），它仅更新少数可学习的标记。另一种方法是情境学习（ICL），它通过在输入中包含示例来适应新任务，而无需进行训练。当应用基于优化的方法，如微调和PT进行少样本学习时，模型会特别适应少量的训练示例，而ICL则不改变模型本身。这种区别使得传统的学习方法更容易过拟合；相反，ICL对少量样本的情况不太敏感。虽然ICL不容易过拟合，但它并不能完全提取训练示例中存在的信息。本文介绍了一种名为情境感知提示调优（CPT）的方法，该方法受到ICL、PT和对抗攻击的启发。我们在ICL策略的基础上，将示例与输入串联起来，但通过PT式的优化，迭代地优化上下文嵌入，以从训练示例中提取更深层次的信息。我们仔细修改特定的上下文标记，考虑输入和输出格式的独特结构。受对抗攻击的启发，我们根据上下文中存在的标签调整输入，旨在最小化而不是最大化损失。此外，我们应用投影梯度下降算法，使标记嵌入保持在接近原始值的状态，假设用户提供的数据本质上是有价值的。我们的方法在多个分类任务中使用各种LLM模型，已显示出优越的准确性。|
|**2024-10-22**|**Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling**|Azmine Toushik Wasi et.al.|[2410.17210](http://arxiv.org/abs/2410.17210)|**[link](https://github.com/ciol-researchlab/ukil)**|**目的：孟加拉国的法律系统面临着重大挑战，如案件积压、复杂性、高昂的成本以及数百万未决案件等问题，这些问题导致许多人因缺乏知识或经济限制而无法寻求法律救济。本研究旨在开发一个专门的大语言模型（LLM）以协助孟加拉国的法律系统。方法：我们通过收集和爬取各种法律法案的数据，创建了UKIL-DB-EN，即孟加拉国法律文件的英文语料库。然后在该数据集上对GPT-2模型进行了微调，开发了GPT2-UKIL-EN，这是一个专注于提供英语法律援助的LLM。结果：该模型通过包括专家意见支持的案例研究在内的语义评估进行了严格评估，结果显示模型具有潜在的法律事务辅助能力。结论：我们的工作代表了建立孟加拉国AI法律助手的第一个有组织的努力。尽管结果令人鼓舞，但仍需要进一步改进以提高模型的准确性、可信度和安全性。这是朝着创建能够满足1.8亿人口需求的法律AI的重要一步。**|
|**2024-10-22**|**VoiceBench: Benchmarking LLM-Based Voice Assistants**|Yiming Chen et.al.|[2410.17196](http://arxiv.org/abs/2410.17196)|**[link](https://github.com/matthewcym/voicebench)**|**基于大型语言模型（LLMs）的成功，近期的进展如GPT-4o使得通过基于LLM的语音助手实现实时语音交互成为可能，与传统的基于文本的交互相比，这大大提升了用户体验。然而，缺乏专门用于评估这些语音交互能力的基准测试，阻碍了基于LLM的语音助手的发展。当前的评估主要集中在自动语音识别（ASR）或使用清晰语音的一般知识评估上，忽视了更复杂的现实场景，这些场景涉及多样的说话者特征、环境和内容因素。为了解决这一问题，我们介绍了VoiceBench，这是首个旨在提供多方面评估的基于LLM的语音助手基准测试。VoiceBench还包括既包括真实的也包括合成的口语指令，这些指令融合了上述三个关键的现实世界变化因素。广泛的实验揭示了当前基于LLM的语音助手模型的局限性，并为该领域的未来研究和发展提供了宝贵的见解。**|
|**2024-10-23**|**Non-myopic Generation of Language Model for Reasoning and Planning**|Chang Ma et.al.|[2410.17195](http://arxiv.org/abs/2410.17195)|**[link](https://github.com/chang-github-00/llm-predictive-decoding)**|大型语言模型在推理和规划方面展示了惊人的能力，通过将复杂问题分解成一系列步骤来解决。尽管它们在数学问题求解和编码等各种领域取得了成功，但由于其固有的自回归解码方式，这些模型在确保可靠且最优的规划时仍面临挑战。本文从最优控制的角度重新审视了大型语言模型的推理方法，提出了一种新颖的方法——预测解码。该方法利用模型预测控制来增强规划准确性。通过根据前瞻轨迹重新加权语言模型的分布，预测解码旨在减轻早期错误并促进非短视规划。我们的实验表明，在数学、编码和智能体任务的广泛范围内，这种方法显著提高了性能。此外，预测解码还表现出计算效率，使用较少的计算资源就优于搜索基线。本研究为优化大型语言模型的规划能力提供了见解。|
|**2024-10-22**|**From Attention to Activation: Unravelling the Enigmas of Large Language Models**|Prannay Kaul et.al.|[2410.17174](http://arxiv.org/abs/2410.17174)|null|我们研究了自回归Transformer中的两种奇怪现象：（1）注意力头中第一个令牌的主导性；（2）隐藏状态中出现大的异常激活值。我们发现，流行的大语言模型（如Llama）在98%的注意力头中对第一个令牌的关注度最大，我们将这种行为归因于softmax函数。为了缓解这个问题，我们提出了一种softmax-1的重新公式化方法。此外，我们确定自适应优化器（例如Adam）是导致这些大异常激活值的主要原因，并引入OrthoAdam，一种新的优化器，它使用正交矩阵来转换梯度，以解决这一问题。最后，我们的方法不仅防止了这些现象的发生，而且还使Transformer能够在使用基本算法进行量化时保持其性能，这是标准方法无法做到的。总之，我们的方法将第一个令牌的注意力比例从65%降低到3.3%，隐藏状态中的激活峰度从1657降低到3.1，在4位权重量化下困惑度惩罚从3565降低到0.3。|
|**2024-10-22**|**Improving Pinterest Search Relevance Using Large Language Models**|Han Wang et.al.|[2410.17152](http://arxiv.org/abs/2410.17152)|null|为了提高Pinterest搜索的相关性评分，我们将大型语言模型（LLMs）集成到我们的搜索相关性模型中，利用精心设计的文本表示来有效地预测Pin的相关性。我们的方法使用搜索查询以及包含从生成式视觉语言模型中提取的字幕的内容表示。这些表示进一步通过链接文本数据、历史高质量交互查询、用户创建的板、Pin标题和Pin描述进行丰富，从而创建出强大的模型来预测搜索相关性。我们采用半监督学习方法以高效地扩展训练数据量，超越仅限于昂贵的人工标注数据。通过利用多语言LLMs，我们的系统将训练数据扩展到包括未见过的语言和领域，尽管初始数据和注释员的专业知识仅限于英语。此外，我们将基于LLM的模型提炼成实时可服务的模型架构和特征。我们提供了全面的离线实验验证我们提出的技术，并展示了在大规模部署系统中所取得的成果。|
|**2024-10-21**|**Reflection-Bench: probing AI intelligence with reflection**|Lingyu Li et.al.|[2410.16270](http://arxiv.org/abs/2410.16270)|**[link](https://github.com/yabyum/reflectionbench)**|**适应性地调整信念或行为以应对意外结果的反思能力，是智能系统与世界互动的核心原则。从认知科学的角度来看，这一原则适用于人类和人工智能系统。为了应对关于大型语言模型（LLMs）智能性的辩论，我们提出了Reflection-Bench，这是一个包含七个任务的综合基准，这些任务涵盖了反思所需的核心认知功能，包括感知、记忆、信念更新、决策、预测、反事实思维和元反思。我们评估了13个著名的LLMs，如OpenAI o1、GPT-4、Claude 3.5 Sonnet等的表现。结果显示，目前的LLMs在反思能力方面仍不令人满意。我们讨论了这些结果背后的原因，并提出了未来研究的潜在方向。总之，Reflection-Bench不仅提供了评估工具，也为开发能够可靠地与环境交互的AI提供了灵感。我们的数据和代码可在https://github.com/YabYum/ReflectionBench获得。**|
|**2024-10-21**|**Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance**|Zhangwei Gao et.al.|[2410.16261](http://arxiv.org/abs/2410.16261)|**[link](https://github.com/opengvlab/internvl)**|**多模态大型语言模型（MLLMs）在广泛的领域内展示了在视觉-语言任务中的出色性能。然而，由于模型规模庞大和相关的高计算成本，在消费者级GPU或边缘设备上训练和部署这些模型面临着巨大挑战，从而限制了它们的广泛应用。在这项工作中，我们引入了Mini-InternVL系列模型，其参数量从1B到4B不等，这些模型仅使用5%的参数就能达到90%的性能。这种显著的效率和效果提升使我们的模型更加易于访问和适用于各种实际场景。为了进一步促进这些模型的采用，我们开发了一个统一的适应框架，使得Mini-InternVL模型能够转移并在下游任务中超越专门模型，包括自动驾驶、医学影像和遥感等领域。我们相信，我们的研究可以为高效且有效的MLLMs的发展提供有价值的见解和资源。代码可在https://github.com/OpenGVLab/InternVL获取。**|
|**2024-10-21**|**Elucidating the design space of language models for image generation**|Xuantong Liu et.al.|[2410.16257](http://arxiv.org/abs/2410.16257)|**[link](https://github.com/Pepper-lll/LMforImageGeneration)**|自回归（AR）语言模型在文本生成中的成功激发了计算机视觉社区采用大规模语言模型（LLMs）进行图像生成。然而，考虑到文本和图像模态之间的基本差异，用于图像生成的语言模型的设计空间仍需深入探索。我们观察到图像标记表现出比文本标记更大的随机性，这在训练过程中带来了挑战。尽管如此，AR模型通过有效地学习即使是从看似次优的优化问题中提取的模式，展示了其潜力。我们的分析还表明，虽然所有模型都成功地理解了局部信息在图像生成中的重要性，但较小的模型难以捕捉全局上下文。相比之下，较大的模型在这方面表现出更好的能力，解释了当扩大模型规模时性能提升的原因。我们进一步通过广泛的对比实验阐明了用于视觉生成的语言模型的设计空间，包括标记器选择、模型选择、模型可扩展性、词汇设计和采样策略。我们的工作首次分析了语言模型在视觉生成中的优化行为，我们认为它能够启发更有效的设计，当将LMs应用于其他领域时。最后，我们阐明了一种用于图像生成的语言模型，称为ELM，在ImageNet 256*256基准测试中达到了最先进的性能。代码可在<https://github.com/Pepperlll/LMforImageGeneration.git>获取。|
|**2024-10-21**|**CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution**|Maosong Cao et.al.|[2410.16256](http://arxiv.org/abs/2410.16256)|**[link](https://github.com/open-compass/compassjudger)**|**高效且准确的评估对于大型语言模型（LLMs）的持续改进至关重要。在各种评估方法中，主观评估因其与现实世界使用场景和人类偏好的高度一致而备受关注。然而，基于人类的评估既昂贵又缺乏可重复性，因此精确的自动化评估者（评判者）在这个过程中变得尤为重要。在这份报告中，我们介绍了\textbf{CompassJudger-1}，这是第一个开源的\textbf{一体化}评判LLM。CompassJudger-1是一个通用的LLM，表现出显著的多功能性。它能够：1. 作为奖励模型进行单一评分和双模型比较；2. 根据指定格式进行评估；3. 生成批评；4. 执行各种任务，就像一个通用的LLM。为了在一个统一的设置下评估不同评判模型的能力，我们还建立了\textbf{JudgerBench}，这是一个新的基准测试，涵盖了各种主观评估任务，并涉及广泛的主题。CompassJudger-1提供了一个全面的解决方案来处理各种评估任务，同时保持适应多样化需求的灵活性。CompassJudger和JudgerBench均已发布并可供研究社区访问https://github.com/open-compass/CompassJudger。我们相信通过开源这些工具，我们可以促进合作并加速LLM评估方法的进步。**|
|**2024-10-21**|**Can Knowledge Editing Really Correct Hallucinations?**|Baixiang Huang et.al.|[2410.16251](http://arxiv.org/abs/2410.16251)|**[link](https://github.com/llm-editing/HalluEditBench)**|**大型语言模型（LLMs）在生成内容时常常会出现幻觉，即包含不真实的信息，尽管它们在各种任务上表现出色。同时，知识编辑作为一种新的流行范式，被用来纠正LLMs中错误的事实知识，其优势在于避免了从头开始重新训练的需要。然而，现有用于知识编辑的评估数据集的一个常见问题是，它们并不能确保LLMs在编辑前对评估问题生成幻觉性答案。当经过不同技术编辑后的LLMs在这类数据集上进行评估时，很难直接采用这些性能来评估不同知识编辑方法在纠正幻觉方面的有效性。因此，一个基本的问题仍未得到充分验证：知识编辑真的能纠正LLMs中的幻觉吗？我们提出了HalluEditBench，以全面基准测试知识编辑方法在纠正现实世界幻觉方面的能力。首先，我们严格构建了一个包含9个领域、26个主题和超过6000个幻觉的大规模幻觉数据集。然后，我们在五个维度——包括有效性、泛化能力、可移植性、局部性和鲁棒性——上全面评估了知识编辑方法的性能。通过HalluEditBench，我们提供了对不同知识编辑方法在纠正幻觉方面的潜力和局限性的新见解，这可以启发未来的改进并促进知识编辑领域的进展。**|
|**2024-10-21**|**Analyzing Context Contributions in LLM-based Machine Translation**|Emmanouil Zaranis et.al.|[2410.16246](http://arxiv.org/abs/2410.16246)|null|大型语言模型（LLMs）在机器翻译（MT）方面已经达到了最先进的性能，并且通过少量示例展示了利用上下文进行学习的能力。然而，关于LLMs如何使用输入的不同部分的机制仍然很大程度上未被探索。在这项工作中，我们对机器翻译中的上下文利用进行了全面分析，研究了当生成翻译时，LLMs如何使用各种上下文部分，如少量示例和源文本。我们强调了几个关键发现：（1）在不同翻译方向下，少量示例的源部分似乎比其对应的目标部分贡献更大；（2）用平行数据微调LLMs会改变不同上下文部分的贡献模式；（3）存在位置偏差，即更早的少量示例对翻译序列的贡献更高。最后，我们证明检查异常的上下文贡献可以潜在地揭示病态翻译，例如幻觉。我们的发现揭示了基于LLM的机器翻译的内部运作机制，这些机制超越了标准编码器-解码器机器翻译模型已知的知识。|
|**2024-10-21**|**IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems**|Yihuan Mao et.al.|[2410.16237](http://arxiv.org/abs/2410.16237)|null|随着大型语言模型（LLM）代理越来越多地集成到我们的基础设施中，它们的稳健协调和消息同步变得至关重要。拜占庭将军问题（BGP）是构建在对抗性攻击下具有弹性的多智能体系统（MAS）的关键模型。它描述了一种情景，在这种情景中系统内存在恶意代理，这种情况可能源于LLM代理的幻觉或外部攻击。在BGP中，整个系统的目的是就采取的行动达成共识。传统的BGP要求所有代理之间实现全局共识；然而，在实际场景中，全局共识并不总是必要，甚至可能是低效的。因此，迫切需要探索一种与MAS中观察到的局部协调模式相一致的改进版BGP。在我们的研究中，我们称这种改进版本为不完美BGP（IBGP），旨在解决这一差异。为了应对这一问题，我们提出了一种框架，该框架利用了通用MAS设置中的共识协议，提供了对通信攻击的可证明的弹性以及适应不断变化环境的能力，并通过实证结果进行了验证。此外，我们还通过一个传感器网络环境的案例研究来说明我们协议的实际应用。|
|**2024-10-21**|**LLaVA-KD: A Framework of Distilling Multimodal Large Language Models**|Yuxuan Cai et.al.|[2410.16236](http://arxiv.org/abs/2410.16236)|**[link](https://github.com/Fantasyele/LLaVA-KD)**|大型语言模型（LLM）的成功促使研究者探索多模态大型语言模型（MLLM），以实现视觉和语言的统一理解。然而，随着模型规模和计算复杂性的增加，MLLM在资源受限环境中的应用受到限制。小规模多模态大型语言模型（s-MLLM）旨在保留大规模模型（l-MLLM）的能力，同时减少计算需求，但会导致性能显著下降。为了解决这些问题，我们提出了一种名为LLaVA-KD的新框架，用于将知识从l-MLLM转移到s-MLLM。具体来说，我们引入了多模态蒸馏（MDist）来最小化l-MLLM和s-MLLM之间视觉-文本输出分布的差异，并引入关系蒸馏（RDist）来转移l-MLLM建模视觉特征之间相关性的能力。此外，我们提出了一个三阶段训练方案，以充分发挥s-MLLM的潜力：1）蒸馏预训练对齐视觉-文本表示；2）监督微调使模型具备多模态理解能力；3）蒸馏微调进一步转移l-MLLM的能力。我们的方法显著提高了性能，而无需改变小模型的架构。广泛的实验和消融研究验证了每个提出的组件的有效性。代码将在https://github.com/caiyuxuan1120/LLAva-KD获取。|
|**2024-10-21**|**ToW: Thoughts of Words Improve Reasoning in Large Language Models**|Zhikun Xu et.al.|[2410.16235](http://arxiv.org/abs/2410.16235)|null|我们介绍了Thoughts of Words（ToW），这是一种新颖的训练时数据增强方法，用于下个词预测。ToW将下个词预测视为一个核心推理任务，并在预训练文本中注入精细的思考，解释下个词应该是什么以及它与前文上下文的关系。我们的方法解决了现有下个词预测学习方案的两个基本缺点：它们会引起事实性幻觉，并且对于模型来说难以有效学习原始文本中的隐含推理过程。虽然获取这些单词的思想有很多方法，但我们探索了通过蒸馏从更大模型中获取ToW注释的第一步。经过仅使用70K个ToW注释的持续预训练后，我们在平均情况下提高了模型推理性能7%到9%，并将模型幻觉减少了高达10%。同时，ToW完全独立于任务和应用，不会对标签或语义引入额外的偏见。|
|**2024-10-21**|**Building A Coding Assistant via the Retrieval-Augmented Language Model**|Xinze Li et.al.|[2410.16229](http://arxiv.org/abs/2410.16229)|**[link](https://github.com/NEUIR/CONAN)**|预训练语言模型在代码相关任务中表现出强大的有效性，如代码检索、代码生成、代码总结和代码补全等任务。本文提出了一种名为CONAN（通过检索增强语言模型实现的代码助手）的方法，旨在通过模仿人类在编程过程中寻求知识的行为来构建代码助手。具体来说，CONAN由一个代码结构感知检索器（CONAN-R）和一个基于双重视图代码表示的检索增强生成模型（CONAN-G）组成。CONAN-R通过使用Code-Documentation对齐和掩码实体预测任务来预训练CodeT5，从而使语言模型具备代码结构感知能力，并学习有效的代码片段和文档表示。然后，CONAN-G设计了一种双重视图代码表示机制来实现检索增强的代码生成模型。CONAN-G将代码文档描述视为提示，帮助语言模型更好地理解代码语义。我们的实验表明，CONAN在不同的代码生成任务上取得了令人信服的性能，并显著优于先前的检索增强代码生成模型。进一步分析显示，CONAN通过对代码文档数据对进行对齐以及通过掩码和预测代码中的实体来捕获结构语义，从而为代码片段和文档学习定制化表示。此外，检索到的代码片段和文档提供了来自程序语言和自然语言的必要信息，以协助代码生成过程。CONAN还可以作为大型语言模型（LLMs）的助手，在较短的代码文档长度下提供外部知识，以提高其在各种代码任务上的有效性。这显示了CONAN提取必要信息并帮助过滤检索到的代码文档中的噪声的能力。|
|**2024-10-18**|**Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts**|German Gritsai et.al.|[2410.14677](http://arxiv.org/abs/2410.14677)|null|快速发展的自回归大型语言模型（LLMs）显著提升了生成文本的质量，这促使了可靠机器生成文本检测器的出现。大量检测器和包含人工智能片段的数据集应运而生，一些检测方法在这些数据集中达到了高达99.9%的目标指标识别质量。然而，在实际应用中，这些检测器的质量往往会大幅下降，这引发了疑问：这些检测器是否真正具有高度的可靠性，还是其高基准分数仅仅是由于评估数据集质量较差所致？在这篇论文中，我们强调了需要建立稳健且高质量的方法来评估生成的数据，以防止未来模型中的偏差和低泛化能力。我们对专门用于AI生成内容检测的竞赛中的数据集进行了系统回顾，并提出了评估包含AI生成片段的数据集质量的方法。此外，我们还讨论了使用高质量生成数据以实现两个目标的可能性：提高检测模型的训练效果和改善训练数据集本身。我们的贡献旨在促进对人与机器文本之间动态关系的更好理解，从而最终支持在一个日益自动化的世界中信息的完整性。|
|**2024-10-18**|**SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment**|Qin Liu et.al.|[2410.14676](http://arxiv.org/abs/2410.14676)|null|现有的偏好对齐机制是一种一刀切的对齐方式，其中大型语言模型（LLM）参数化知识中的非偏好特征被统一屏蔽，适用于所有用户。然而，这部分知识对于那些具有专业知识并能够处理这些信息的高级用户来说可能是有用的。这种一刀切的对齐机制削弱了这些合格用户的LLM效用。为了解决这个问题，我们提出了SudoLM框架，该框架通过授权对齐让LLM学习针对不同用户凭证的具体参数化知识的访问控制。SudoLM允许授权用户通过分配的SUDO密钥解锁对所有参数化知识的访问，而非授权用户则无法访问这些知识。在两个应用场景的实验表明，SudoLM能够有效控制用户对参数化知识的访问，并保持其总体效用。|
|**2024-10-18**|**Enhancing Large Language Models' Situated Faithfulness to External Contexts**|Yukun Huang et.al.|[2410.14675](http://arxiv.org/abs/2410.14675)|**[link](https://github.com/kkkevinkkkkk/situated_faithfulness)**|**大型语言模型（LLMs）通常会使用外部信息作为上下文，但这些外部信息有时可能是不准确的，甚至可能是故意误导的。我们认为，稳健的LLMs应该展示出情境真实性，根据它们对内部知识和外部上下文的信心动态调整对外部信息的信任度。为了评估这种能力，我们对LLMs进行了多项QA数据集的测试，包括一个新创建的数据集RedditQA，该数据集包含了来自Reddit帖子中的实际错误上下文。结果显示，当提供正确和不正确的上下文时，无论是开源模型还是专有模型，都倾向于过度依赖外部信息，而不管其事实准确性如何。为了增强情境真实性，我们提出了两种方法：自引导置信度推理（SCR）和基于规则的置信度推理（RCR）。SCR使模型能够根据自身内部知识相对地评估外部信息的置信度，从而生成最准确的答案。相比之下，RCR从LLM中提取显式的置信度信号，并利用预定义的规则来确定最终答案。我们的结果表明，对于具有强大推理能力的模型，如GPT-4o和GPT-4o mini，SCR优于RCR，在直接输入增强基线上的提升幅度最高可达24.2%。相反，对于较小的模型，如Llama-3-8B，RCR则优于SCR。通过我们的置信度推理直接偏好优化（CR-DPO）方法对SCR进行微调，可以提高在已见和未见过的数据集上的性能，平均提升幅度为8.9%。除了定量结果外，我们还提供了关于SCR和RCR相对优势的见解。我们的研究结果强调了提高LLMs情境真实性的有前景途径。相关数据和代码已经发布。**|
|**2024-10-18**|**MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps**|Xiongtao Zhou et.al.|[2410.14668](http://arxiv.org/abs/2410.14668)|**[link](https://github.com/alenai97/miceval)**|**Multimodal Chain of Thought（MCoT）是一种流行的提示策略，用于提高多模态大型语言模型（MLLMs）在各种复杂推理任务中的性能。尽管这种方法很受欢迎，但在评估多模态链式思维推理步骤的质量方面仍缺乏自动化方法。为了解决这一问题，我们提出了Multimodal Chain-of-Thought Evaluation（MiCEval），这是一个框架，旨在通过评估描述和每个推理步骤的质量来评估推理链的正确性。描述部分的评估侧重于图像描述的准确性，而推理步骤则根据前续步骤条件生成时的质量进行评估。MiCEval基于一个细粒度的数据集，该数据集根据正确性、相关性和信息量对每个步骤进行标注。对四种最先进的MLLMs进行的广泛实验表明，使用MiCEval进行逐步评估与人类判断更加吻合，相比现有基于余弦相似度或微调的方法更为准确。MiCEval数据集和代码可以在https://github.com/alenai97/MiCEval找到。**|
|**2024-10-18**|**A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning**|Shengjie Sun et.al.|[2410.14660](http://arxiv.org/abs/2410.14660)|null|大型语言模型（LLMs）在设计强化学习（RL）任务的奖励函数方面显示出显著的潜力。然而，获取高质量的奖励代码通常需要人工干预、大量的LLM查询或重复的RL训练。为了解决这些问题，我们提出了CARD，这是一种由LLM驱动的奖励设计框架，它迭代地生成和改进奖励函数代码。具体来说，CARD包括一个编码器，用于生成和验证代码，同时还有一个评估器提供动态反馈来指导编码器改进代码，从而消除了对人工反馈的需求。除了过程反馈和轨迹反馈外，我们还引入了轨迹偏好评估（TPE），该评估基于轨迹偏好来评估当前的奖励函数。如果代码未能通过TPE，评估器将提供偏好反馈，避免了在每次迭代时进行RL训练，并使奖励函数更好地与任务目标对齐。在Meta-World和ManiSkill2上的实证结果表明，我们的方法在任务性能和令牌效率之间实现了有效的平衡，在所有任务上都优于或匹配基线。在12个任务中的10个任务上，CARD的表现优于或可与使用专家设计奖励训练的策略相媲美，甚至在3个任务上超越了最优解。|
|**2024-10-18**|**EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search**|Oliver Sieberling et.al.|[2410.14649](http://arxiv.org/abs/2410.14649)|**[link](https://github.com/ist-daslab/evopress)**|高计算成本是大型语言模型（LLMs）面临的一个主要问题，因此对模型压缩的研究层出不穷，这些方法包括量化、稀疏化或结构化剪枝等。一个新的研究前沿是由所谓的“动态、非均匀”压缩方法构成的，这些方法通过调整每块或甚至每层的压缩级别（例如稀疏性），以最小化精度损失，同时确保全局压缩阈值。然而，当前的方法依赖于启发式方法来识别给定层对误差的重要性，这基于诸如“误差单调性”的假设，即端到端模型压缩误差与各层误差之和成比例。在本文中，我们重新审视了这一领域，并提出了一种新的通用方法，该方法在给定输入范围内被证明是最佳的。我们的动机观察到，通常情况下，“误差单调性”对于LLMs并不成立：具有较低各层误差总和的压缩模型可能表现得比误差总和较高的模型更差。为了解决这个问题，我们提出了一种新的通用进化框架，称为EvoPress，它具有理论上的收敛性和低样本及评估复杂度。我们展示了这些理论保证导致了在动态压缩Llama、Mistral和Phi模型方面高度竞争的实际性能。通过EvoPress，我们在所有压缩方法上都达到了最新的成果：结构剪枝（块/层删除）、非结构化稀疏性以及具有动态位宽的量化。我们的代码可在https://github.com/IST-DASLab/EvoPress获取。|
|**2024-10-18**|**Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs**|Runchu Tian et.al.|[2410.14641](http://arxiv.org/abs/2410.14641)|**[link](https://github.com/Rachum-thu/LongPiBench)**|**位置偏差在大型语言模型（LLMs）中限制了它们处理长输入的能力。一个显著的例子是“迷失在中间”现象，即LLMs难以利用位于输入中间的相关信息。尽管先前的研究主要集中在单个相关信息上，但现实世界的应用通常涉及多个相关的信息片段。为了弥补这一差距，我们提出了LongPiBench，这是一个旨在评估涉及多个相关片段的位置偏差的基准测试。通过五种商业模型和六种开源模型进行的详细实验表明，虽然大多数当前模型对“迷失在中间”的问题具有鲁棒性，但存在与相关信息片段间距显著相关的偏差。这些发现强调了评估和减少位置偏差的重要性，以提升LLMs的能力。**|
|**2024-10-18**|**GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings**|Raghuveer Thirukovalluru et.al.|[2410.14635](http://arxiv.org/abs/2410.14635)|**[link](https://github.com/raghavlite/GenEOL)**|训练-free嵌入方法直接利用预训练的大规模语言模型（LLMs）来嵌入文本，避免了对比学习的昂贵和复杂的流程。先前的训练-free嵌入方法主要集中在优化嵌入提示上，而忽略了利用LLMs的生成能力的好处。我们提出了一种新颖的方法GenEOL，该方法使用LLMs生成保留句子含义的不同变换，并聚合这些变换的嵌入结果以增强整体句子嵌入。GenEOL在多个LLMs的句子语义文本相似性（STS）基准测试中平均比现有的训练-free嵌入方法高出2.85分。我们的分析表明，GenEOL在LLM层面上稳定了表征质量，并且对嵌入提示的扰动具有鲁棒性。GenEOL还在MTEB基准测试中的多个聚类、重排序和配对分类任务中取得了显著的提升。|
|**2024-10-18**|**DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual Distillation in Conversational Search**|Simon Lupart et.al.|[2410.14609](http://arxiv.org/abs/2410.14609)|null|会话搜索（CS）任务是在语境内从文档集中检索相关文档，结合检索与会话上下文建模。随着大规模语言模型（LLMs）的兴起，CS领域通过LLMs重写用户查询并考虑会话上下文得到了显著改进。然而，在推理时使用这些模型会影响效率。当前方法通过从人类重写的查询中蒸馏嵌入来学习上下文建模任务以解决此问题。然而，这些方法主要关注于上下文建模，并且仅在独立于蒸馏的损失项中处理检索任务中的对比部分。为了应对这些限制，我们提出了一种新的蒸馏方法，作为对先前目标的放松，统一检索和上下文建模。我们通过蒸馏对话和文档之间的相似性分数来放松现有的训练目标，而不是仅仅依赖表示学习。我们提出的蒸馏目标允许在表示空间中有更多的自由度，并利用文档相关性的对比性质。通过在5个CS数据集上的Learned Sparse Retrieval（LSR）实验，我们的方法在域内和域外检索性能方面均显示出显著改善，超越了最先进水平，在域外数据集上召回率提高了多达6个百分点。此外，通过放松目标，我们提出了多教师蒸馏，使用多个LLM作为教师，从而获得额外收益，并在域内实验中超越这些教师本身。最后，对模型稀疏性的分析表明，我们的蒸馏方法能够更好地控制训练模型的稀疏性。|
|**2024-10-18**|**Teaching Models to Balance Resisting and Accepting Persuasion**|Elias Stengel-Eskin et.al.|[2410.14596](http://arxiv.org/abs/2410.14596)|**[link](https://github.com/esteng/persuasion_balanced_training)**|**大型语言模型（LLMs）容易受到说服的影响，这在模型面对敌对对话者时可能带来风险。我们迈出了防御模型免受说服的第一步，同时认为防御负面说服只是问题的一半：模型还应该能够接受有益的说服以改进其答案。我们发现，仅优化模型一方面会导致在另一方面表现不佳。为了平衡正面和负面说服，我们引入了说服平衡训练（PBT），该方法利用多智能体递归对话树来生成数据，并通过偏好优化训练模型在适当时候接受说服。PBT一致提高了模型对抗错误信息的抵抗力和应对挑战的韧性，同时也使模型在包含正反两面说服的整体数据上表现最佳。至关重要的是，我们发现PBT模型在多智能体辩论中是更好的队友。我们发现，没有PBT的情况下，更强和较弱模型的组合表现出不稳定性能，模型回答的顺序决定了团队获得较强或较弱模型的表现。PBT带来了更好且更稳定的性能结果，并减少了顺序依赖性，较强模型能够持续提升较弱模型的表现。**|
|**2024-10-17**|**Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens**|Lijie Fan et.al.|[2410.13863](http://arxiv.org/abs/2410.13863)|null|在视觉领域，扩大自回归模型的效果并不像在大型语言模型中那样显著。在这项工作中，我们研究了文本到图像生成中的这一扩展问题，重点关注两个关键因素：模型是否使用离散或连续的标记，以及标记是否以随机或固定栅格顺序使用类似于BERT或GPT的变换器架构生成。我们的实证结果表明，虽然所有模型在验证损失方面都能有效扩展，但它们的评估性能——通过FID、GenEval分数和视觉质量来衡量——则呈现出不同的趋势。基于连续标记的模型在视觉质量上显著优于使用离散标记的模型。此外，生成顺序和注意力机制对GenEval分数有显著影响：随机顺序的模型在GenEval分数上明显优于栅格顺序的模型。受这些发现的启发，我们训练了一种名为Fluid的随机顺序自回归模型，该模型基于连续标记。Fluid 10.5B模型在MS-COCO 30K上的零样本FID达到了新的最先进水平，即6.16，并且在GenEval基准测试中的整体得分为0.69。我们希望我们的发现和结果能鼓励未来进一步缩小视觉和语言模型之间的扩展差距。|
|**2024-10-17**|**PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**|Rongyao Fang et.al.|[2410.13861](http://arxiv.org/abs/2410.13861)|**[link](https://github.com/rongyaofang/puma)**|**近年来，多模态基础模型在视觉-语言理解方面取得了显著进展。初步尝试也探索了多模态大语言模型（MLLM）在视觉内容生成中的潜力。然而，现有工作未能充分解决统一MLLM范式下不同图像生成任务对不同粒度需求的问题——从文本到图像生成所需的多样性到图像操作所需的精确可控性。在这项工作中，我们提出了PUMA，即通过多粒度视觉生成赋予统一MLLM以力量。PUMA将多粒度视觉特征统一作为MLLM的输入和输出，优雅地解决了不同粒度要求的各种图像生成任务在统一MLLM框架下的问题。经过多模态预训练和任务特定指令微调后，PUMA在广泛的多模态任务中表现出色。这项工作标志着向真正统一的MLLM迈出了重要一步，这种MLLM能够适应各种视觉任务对粒度的需求。代码和模型将在https://github.com/rongyaofang/PUMA发布。**|
|**2024-10-17**|**$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**|Yaxin Luo et.al.|[2410.13859](http://arxiv.org/abs/2410.13859)|null|尽管多模态大型语言模型（MLLMs）取得了显著进展，但其高昂的计算成本仍然是实际部署中的一个障碍。受自然语言处理中深度混合（MoD）的启发，我们从“激活标记”的角度来解决这一限制。我们的关键见解是，如果大多数标记对于层计算是冗余的，那么可以通过MoD层直接跳过它们。然而，直接将MLLMs的密集层转换为MoD层会导致显著的性能下降。为了解决这个问题，我们提出了一种创新的MoD适应策略，称为$\gamma$-MoD，用于现有的MLLMs。在$\gamma$-MoD中，我们提出了一个新的指标来指导MLLM中MoD的部署，即注意力图的秩（ARank）。通过ARank，我们可以有效地识别哪些层是冗余的，并应被替换为MoD层。基于ARank，我们进一步提出了两种新颖的设计，以最大限度地提高MLLM的计算稀疏性，同时保持其性能，即共享视觉-语言路由器和掩码路由学习。通过这些设计，MLLM的90%以上的密集层可以有效转换为MoD层。为了验证我们的方法，我们在三个流行的MLLM上进行了应用，并在9个基准数据集上进行了广泛的实验。实验结果不仅验证了$\gamma$-MoD对现有MLLMs的显著效率提升，还证实了其在各种MLLM上的泛化能力。例如，$\gamma$ -MoD仅导致轻微的性能下降，即-1.5%，但可以分别将LLaVA-HR的训练时间和推理时间减少31.0%和53.2%。|
|**2024-10-17**|**How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs**|Guhao Feng et.al.|[2410.13857](http://arxiv.org/abs/2410.13857)|null|尽管基于Transformer的大型语言模型（LLMs）在各个领域取得了显著的成功，但理解和提升它们的数学能力仍然是一个重要的挑战。在这篇论文中，我们对LLMs的数学能力进行了严格的理论分析，特别关注它们的算术表现。我们发现数值精度是影响其在数学任务中表现的关键因素。我们的研究结果表明，使用低数值精度的Transformer在处理算术任务（如迭代加法和整数乘法）时，除非模型大小相对于输入长度呈超多项式增长，否则无法有效解决这些问题。相比之下，使用标准数值精度的Transformer可以高效地处理这些任务，并且所需的模型尺寸要小得多。我们还通过实验进一步验证了这一理论发现，探索了不同数值精度对算术任务的影响，为提高LLMs的数学推理能力提供了宝贵的见解。|
|**2024-10-17**|**Can MLLMs Understand the Deep Implication Behind Chinese Images?**|Chenhao Zhang et.al.|[2410.13854](http://arxiv.org/abs/2410.13854)|**[link](https://github.com/MING-ZCH/CII-Bench)**|**随着多模态大型语言模型（MLLMs）的能力不断提升，对这些模型进行更高阶能力评估的需求也在增加。然而，目前缺乏针对MLLMs的高阶感知和理解中文视觉内容的评估工作。为了填补这一空白，我们介绍了中文图像隐含理解基准（CII-Bench），旨在评估MLLMs对中文图像的高阶感知和理解能力。与现有基准相比，CII-Bench具有多个突出特点。首先，为了确保中文背景的真实性，CII-Bench中的图像来源于中国互联网，并经过人工审查，相应的答案也由人工精心制作。此外，CII-Bench还纳入了代表中国传统文化的图像，如著名的中国传统绘画，这可以深入反映模型对中国传统文化的理解。通过在多个MLLMs上进行广泛的实验，我们得出了重要发现。最初，MLLMs在CII-Bench上的表现与人类存在显著差距。MLLMs的最高准确率为64.4%，而人类的平均准确率为78.2%，峰值达到令人印象深刻的81.0%。随后，MLLMs在处理中国传统文化图像时表现较差，这表明它们在理解高层次语义和缺乏对中国传统文化的深入了解方面存在局限性。最后，观察到大多数模型在图像情感提示被纳入提示时表现出更高的准确性。我们相信，CII-Bench将使MLLMs更好地理解中文语义和特定于中国的图像，从而推动向专家型通用人工智能（AGI）的发展。我们的项目可在https://cii-bench.github.io/公开访问。**|
|**2024-10-17**|**Retrospective Learning from Interactions**|Zizhao Chen et.al.|[2410.13852](http://arxiv.org/abs/2410.13852)|null|多回合交互中的大型语言模型（LLMs）和用户之间的互动自然包含了隐含的反馈信号。如果LLMs以出乎意料的方式回应用户的指令，用户很可能会通过重新表述请求、表达挫败感或转向替代任务来传达这一信号。这些信号与具体任务无关，并且占据相对受限的语言子空间，即使LLMs在实际任务上失败了，也能识别这些信号。这为LLMs通过互动持续学习提供了途径，而无需额外标注。我们引入了一种名为ReSpect的方法，通过回顾过去的交互来学习这些信号。我们在一个新的多模态交互场景中部署了ReSpect，在该场景中，人类指导LLMs解决具有组合解空间的抽象推理任务。通过与人类进行数千次交互，我们展示了ReSpect如何逐步提高任务完成率，从31%提升到82%，且无需任何外部标注。|
|**2024-10-17**|**SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction**|Xuan Zhang et.al.|[2410.13846](http://arxiv.org/abs/2410.13846)|**[link](https://github.com/sail-sg/simlayerkv)**|**近年来，大型语言模型（LLMs）的进展已经扩展了它们处理长上下文的能力。然而，增加模型层数和输入序列长度显著增加了存储键值（KV）缓存所需的内存，这对高效的推理构成了挑战。为了缓解这一问题，我们提出了SimLayerKV，这是一种简单而有效的方法，通过在识别为懒层的层中选择性地丢弃缓存来减少层间KV缓存的冗余。我们的方法基于这样的观察：在长上下文LLMs中，某些层表现出“懒惰”行为，与非懒层相比，对建模长距离依赖贡献较小。通过分析注意力权重模式，我们发现这些懒层在给定输入生成过程中对不同token的行为是一致的。这一见解启发了我们的SimLayerKV，该方法通过识别懒层并相应地减少其KV缓存。SimLayerKV无需训练，具有通用性，并且可以用仅七行代码实现。我们在三个代表性LLMs上进行了广泛的实验，例如LLaMA2-7B、LLaMA3-8B和Mistral-7B，在LongBench基准测试的16个任务上进行测试。结果显示，SimLayerKV实现了5倍的KV缓存压缩比，并且在结合4位量化时性能仅下降1.2%。我们的代码可在https://github.com/sail-sg/SimLayerKV获取。**|
|**2024-10-17**|**Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs**|Tianyu Guo et.al.|[2410.13835](http://arxiv.org/abs/2410.13835)|**[link](https://github.com/guotianyu2000/active-dormant-attention)**|实践者在变压器型大规模语言模型（LLMs）中观察到了三个令人困惑的现象：注意力汇点、值状态耗尽和残差状态峰值，这些现象统称为极端令牌现象。这些现象的特点是某些所谓的“汇点令牌”接收不成比例高的注意力权重，表现出明显较小的值状态，并且具有比其他令牌大得多的残差状态范数。这些极端令牌在LLM推理、量化和可解释性方面引发了许多挑战。我们阐明了极端令牌现象背后的机制。首先，我们在非常简单的架构——一到三层的变压器，在玩具模型Bigram-Backcopy（BB）任务上训练时展示了这些现象。在这种情况下，我们识别出一个活跃-休眠机制，其中注意力头对于特定输入域成为汇点，而对于其他输入则不是。我们对训练动态的理论分析揭示，这些现象是由一种相互增强机制驱动的。基于这些见解，我们提出了在预训练期间缓解极端令牌现象的策略，包括用ReLU替换softmax以及用SGD替换Adam。接下来，我们将分析扩展到预训练的LLM，包括Llama和OLMo，显示许多注意力头表现出与BB任务中类似的活跃-休眠机制，并且相互增强机制也支配着LLM预训练期间极端令牌现象的出现。我们的结果显示，许多由BB任务预测的静态和动态性质与预训练LLM中的观察结果一致。|
|**2024-10-17**|**AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents**|Ke Yang et.al.|[2410.13825](http://arxiv.org/abs/2410.13825)|null|通过使用大型语言模型（LLMs）的代理来实现自治，可以提升人类在个性化和标准化任务中的效率。自动化网络任务（如在预算内预订酒店）的需求日益增加。满足实际需求的同时，网络代理也作为一个重要的概念验证示例，展示了各种代理接地场景的重要性。其成功预示着许多未来应用的进步。先前的研究通常会手工设计网络代理策略（例如提示模板、多代理系统、搜索方法等），这些策略可能无法在所有现实世界场景中很好地推广。另一方面，关于网络代理的观察/动作表示与LLM预训练数据之间不匹配的研究非常有限。这种差异特别明显，因为LLM主要针对语言完成进行训练，而不是处理涉及具身导航动作和符号网络元素的任务。我们的研究通过简单地优化LLM网络代理的观察和动作空间，使其更好地与LLM的能力相匹配，从而提升了性能。这种方法使我们的基础代理在各种网络任务上显著优于以前的方法。具体来说，在WebArena基准测试中，该基准测试涵盖了通用网络交互任务，我们的代理AgentOccam比之前最先进的方法高出9.8分（+29.4%），比同时期的工作高出5.9分（+15.8%）。相比类似的基本网络代理，其观察和动作空间对齐后成功率为26.6分（+161%）。我们没有使用上下文示例、新的代理角色、在线反馈或搜索策略。AgentOccam的设计简单，突显了LLM在无样本情况下执行网络任务的强大性能，并强调了精心调整观察和动作空间对于基于LLM的代理至关重要。|
|**2024-10-18**|**Harnessing Webpage UIs for Text-Rich Visual Understanding**|Junpeng Liu et.al.|[2410.13824](http://arxiv.org/abs/2410.13824)|null|文本丰富的视觉理解——即处理密集文本内容与视觉元素相融合的环境的能力，对于多模态大型语言模型（MLLMs）在结构化环境中进行有效交互至关重要。为了增强这一能力，我们提出使用基于文本的大型语言模型（LLMs）从网页用户界面合成通用的多模态指令。尽管缺乏直接的视觉输入，基于文本的LLMs能够处理来自网页可访问性树的结构化文本表示。这些指令随后与UI截图配对以训练多模态模型。我们引入了MultiUI数据集，该数据集包含来自100万个网站的730万样本，涵盖了多种多模态任务和UI布局。在MultiUI上训练的模型不仅在网页UI任务中表现出色，在VisualWebBench上的提升高达48%，在Mind2Web的网页代理数据集中元素准确率提高了19.1%，而且在非网页UI任务以及甚至非UI领域（如文档理解、OCR和图表解释）中也表现出惊人的泛化能力。这些结果突显了网页UI数据在推动各种场景下文本丰富视觉理解的广泛应用性。|
|**2024-10-16**|**Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception**|Jihao Zhao et.al.|[2410.12788](http://arxiv.org/abs/2410.12788)|**[link](https://github.com/IAAR-Shanghai/Meta-Chunking)**|Retrieval-Augmented Generation（RAG）在作为大型语言模型（LLMs）的可行补充时，常常忽略了其管道中一个关键方面——文本分块，这影响了知识密集型任务的质量。本文介绍了一种称为元分块（Meta-Chunking）的概念，这是一种介于句子和段落之间的粒度，由段落内具有深层次语言逻辑联系的一组句子组成。为了实现元分块，我们基于LLMs设计了两种策略：边界采样分块和困惑度分块。前者利用LLMs对连续句子是否需要分割进行二分类决策，基于从边界采样获得的概率差做出决策。后者通过分析困惑度分布的特点来精确识别文本分块边界。此外，考虑到不同文本的固有复杂性，我们提出了一种结合元分块与动态合并的策略，以实现在细粒度和粗粒度文本分块之间取得平衡。实验在十一个数据集上进行，结果表明元分块可以更有效地提高基于RAG的单跳和多跳问答性能。例如，在2WikiMultihopQA数据集上，它比相似性分块提高了1.32的性能，同时仅消耗了45.8%的时间。我们的代码可在https://github.com/IAAR-Shanghai/Meta-Chunking 获取。|
|**2024-10-16**|**In-Context Learning Enables Robot Action Prediction in LLMs**|Yida Yin et.al.|[2410.12782](http://arxiv.org/abs/2410.12782)|null|最近，大型语言模型（LLMs）在语言领域通过上下文学习（ICL）取得了显著的成功。然而，利用LLMs的ICL能力直接预测机器人动作的研究还相对较少。在这篇论文中，我们介绍了一种名为RoboPrompt的框架，该框架使现成的纯文本LLMs能够在无需训练的情况下通过ICL直接预测机器人动作。我们的方法首先通过启发式方法识别出一个片段中的关键帧，这些关键帧捕捉了重要的时刻。接下来，我们从这些关键帧中提取末端执行器的动作以及估计的初始物体姿态，并将两者转换为文本描述。最后，我们构建了一个结构化的模板，从这些文本描述和任务指令中形成ICL演示。这使得LLM能够在测试时直接预测机器人动作。通过广泛的实验和分析，RoboPrompt在模拟和真实环境中均表现出比零样本和ICL基线更强的性能。|
|**2024-10-16**|**Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information**|Yingya Li et.al.|[2410.12774](http://arxiv.org/abs/2410.12774)|null|多任务学习的成功在很大程度上取决于任务的分组方式。简单地将所有任务或随机选择的任务组合在一起可能导致负迁移，从而使多任务模型的表现不如单任务模型。尽管已经做出了许多努力来识别任务分组并衡量不同任务之间的相关性，但定义一个指标以从众多潜在任务组合中确定最佳任务分组仍然是一个具有挑战性的研究课题。我们提出了一种基于点式V-可用信息（PVI）测量任务难度的任务相关性度量方法。PVI是一种新近提出的度量标准，用于估计给定模型时数据集包含多少可用信息。我们假设具有统计上不可区分的PVI估计值的任务足够相似，可以从联合学习过程中受益。我们在一般、生物医学和临床领域的15个NLP数据集上进行了全面实验，以评估该度量方法用于任务分组的可行性。我们将联合学习器的结果与单任务学习器、现有基线方法以及最近的大规模语言模型（包括Llama 2和GPT-4）进行了比较。结果显示，通过将具有相似PVI估计值的任务分组，联合学习器在较少总参数的情况下获得了具有竞争力的结果，并且在不同领域内表现一致。|
|**2024-10-16**|**StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples**|Ajay Patel et.al.|[2410.12757](http://arxiv.org/abs/2410.12757)|null|风格表示旨在将具有相似写作风格的文本嵌入到接近的位置，并将具有不同风格的文本嵌入到远离的位置，而不考虑内容。然而，用于训练这些表示的对比三元组往往在风格和内容上都有所变化，导致表示中可能存在内容泄漏的问题。我们引入了一种名为StyleDistance的新方法来训练更强的独立于内容的风格嵌入。我们使用大型语言模型创建了一个合成数据集，其中包含受控风格变化的近似释义，并为精确的对比学习生成了跨越40个不同风格特征的正例和负例。我们通过人工和自动评估来评估合成数据和嵌入的质量。StyleDistance增强了风格嵌入的内容独立性，这种嵌入可以推广到现实世界的基准测试，并在下游应用中优于领先的风格表示。我们的模型可以在https://huggingface.co/StyleDistance/styledistance找到。|
|**2024-10-17**|**CREAM: Consistency Regularized Self-Rewarding Language Models**|Zhaoyang Wang et.al.|[2410.12735](http://arxiv.org/abs/2410.12735)|null|近期的自我奖励大型语言模型（LLM）成功地应用了LLM作为裁判的方法，以迭代方式提升对齐性能，而无需人工标注的偏好数据。这些方法通常使用同一LLM作为策略模型（生成响应）和奖励模型（评分和排序这些响应）。然后，根据排名的响应作为偏好对来通过直接对齐技术（例如DPO）训练LLM。然而，值得注意的是，在这个过程中，奖励和排序的准确性没有保证，这对于确保准确的奖励和高质量的偏好数据至关重要。来自相对较小的LLM（例如7B参数）的经验结果也表明，在某些情况下，经过几次迭代后，自我奖励的改进可能会减弱，我们假设这是由于奖励系统中的累积偏差所致。这种偏差可能导致用于训练LLM的不可靠偏好数据。为了解决这个问题，我们首先制定了并分析了自我奖励语言模型的广义迭代偏好微调框架。然后，我们在这一广义框架中引入正则化，以减轻自我奖励过程中的过度自信偏好标记。基于这一理论洞察，我们提出了一种一致性正则化的自我奖励语言模型（CREAM），该模型利用不同迭代中的奖励一致性来正则化自我奖励训练，帮助模型从更可靠的偏好数据中学习。通过这种明确的正则化，我们的实证结果证明了CREAM在提高奖励一致性和对齐性能方面的优越性。代码可在https://github.com/Raibows/CREAM公开获取。|
|**2024-10-16**|**FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**|Zhenheng Tang et.al.|[2410.12707](http://arxiv.org/abs/2410.12707)|null|为了缓解在训练大型深度神经网络（DNNs），特别是大型语言模型（LLMs）时的硬件短缺问题，我们提出了FusionLLM，这是一种去中心化的训练系统，旨在利用地理分布的GPU跨不同的计算集群或单个设备进行DNN训练。去中心化训练在系统设计和效率方面面临重大挑战，包括：1）需要远程自动微分（RAD），2）支持灵活的模型定义和异构软件，3）异构硬件导致资源利用率低或存在慢速节点问题，以及4）网络通信缓慢。为了解决这些挑战，在系统设计中，我们将模型表示为操作符（OP-DAG）的有向无环图。DAG中的每个节点代表DNN中的操作符，边则表示操作符之间的数据依赖关系。基于这种设计，1）用户可以自定义任何DNN而不必关心底层操作符实现；2）我们通过更细粒度的子任务进行任务调度，提供更多的优化空间；3）DAG运行时执行器可以在不依赖一致的低级机器学习框架版本的情况下实现RAD。  为了提高系统效率，我们实现了一个工作负载估计器，并设计了一种OP-Fence调度器，将具有相似带宽的设备分组在一起，并对DAG进行分区以增加吞吐量。此外，我们提出了一种AdaTopK压缩器，以自适应地压缩在最慢通信链路上的中间激活和梯度。为了评估我们的系统和算法的收敛性和效率，我们在三个现实测试平台上使用连接速度在8 Mbps到10 Gbps的48个GPU上训练了ResNet-101和GPT-2。实验结果表明，与基线方法相比，我们的系统和方法可以在确保收敛的同时实现1.45至9.39倍的速度提升。|
|**2024-10-16**|**Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization**|Xingqi Wang et.al.|[2410.12700](http://arxiv.org/abs/2410.12700)|**[link](https://github.com/achernarwang/LiVO)**|**近年来，基于大规模数据训练的扩散模型已经能够生成与人类水平图像难以区分的图像，但它们常常产生有害内容，这些内容与人类价值观不符，例如社会偏见和冒犯性内容。尽管大型语言模型（LLM）领域进行了大量研究，但文本到图像（T2I）模型的对齐问题仍未得到充分探索。为了解决这一问题，我们提出了LiVO（轻量级价值优化），这是一种新颖的轻量级方法，用于将T2I模型与人类价值观对齐。LiVO仅优化一个即插即用的价值编码器，以将指定的价值原则整合到输入提示中，从而在控制生成图像的语义和价值观方面发挥作用。具体来说，我们设计了一种针对扩散模型的偏好优化损失函数，该函数在理论上逼近LLM对齐中使用的Bradley-Terry模型，但提供了图像质量和价值一致性之间的更灵活的权衡。为了优化价值编码器，我们还开发了一个框架来自动构建一个包含86k个样本（提示、对齐图像、违反图像、价值原则）的文本-图像偏好数据集。通过不更新大多数模型参数并通过从输入提示中进行自适应价值选择，LiVO显著减少了有害输出，并实现了更快的收敛，超越了几种强大的基线模型，迈出了向伦理对齐的T2I模型迈出的第一步。**|
|**2024-10-16**|**Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2**|Mohamad Abdi et.al.|[2410.12686](http://arxiv.org/abs/2410.12686)|null|解剖学标志在医学影像中对于导航和异常检测至关重要。现代大型语言模型（LLMs），如Llama-2，为将这些标志从自由文本的放射学报告映射到图像数据中的相应位置提供了希望。最近的研究表明，LLMs可能能够形成连贯的生成过程表示。受此启发，我们研究了LLMs是否准确地表示解剖学标志的空间位置。通过使用Llama-2模型进行实验，我们发现它们可以线性地表示空间中的解剖学标志，并且对不同提示具有相当强的鲁棒性。这些结果强调了LLMs增强医学影像工作流程效率和准确性的潜力。|
|**2024-10-16**|**Evaluating Morphological Compositional Generalization in Large Language Models**|Mete Ismayilzada et.al.|[2410.12656](http://arxiv.org/abs/2410.12656)|null|大型语言模型（LLMs）在各种自然语言生成和理解任务中已经取得了显著的进展。然而，它们的语言泛化能力仍然值得质疑，这引发了关于这些模型是否像人类一样学习语言的疑问。尽管人类在语言使用中表现出组合能力和语言创造性，但LLMs在这方面的表现，特别是在形态学方面的能力，仍需进一步探索。在这项工作中，我们通过组合性的视角系统地研究了LLMs在形态学泛化方面的能力。我们将词素定义为组合的基本单位，并设计了一套新的生成性和判别性任务来评估形态学的生产力和系统性。重点关注像土耳其语和芬兰语这样的黏着语，我们评估了几种最先进的指令微调多语言模型，包括GPT-4和Gemini。我们的分析表明，LLMs在处理形态学组合泛化时特别困难，尤其是在应用于新词根时，随着形态复杂性的增加，性能急剧下降。虽然模型能够比随机猜测更好地识别个别形态组合，但其表现缺乏系统性，导致与人类相比存在显著的准确率差距。|
|**2024-10-16**|**Explainable Moral Values: a neuro-symbolic approach to value classification**|Nicolas Lazzari et.al.|[2410.12631](http://arxiv.org/abs/2410.12631)|null|本文研究了基于本体的推理与机器学习技术在可解释价值分类中的整合。通过依赖道德基础理论中的道德价值观形式化以及DnS本体设计模式，使用sandra神经符号推理器来推断满足特定句子描述的价值。句子及其结构化表示是使用开源的大语言模型自动生成的。所推断的描述被用来自动检测句子所关联的价值。我们展示了仅依靠推理器的结果即可实现与更复杂方法相当的可解释分类。我们还展示了将推理器的推断结果与分布语义方法相结合可以大幅超越所有基线，包括基于神经网络架构的复杂模型。最后，我们构建了一个可视化工具来探索基于理论的值分类的潜力，该工具可在http://xmv.geomeaning.com/公开访问。|
|**2024-10-15**|**GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation**|Fei Tang et.al.|[2410.11841](http://arxiv.org/abs/2410.11841)|null|基于大规模语言模型的可解释推荐（LLM-based ER）系统在生成类似人类的推荐解释方面显示出潜力。然而，它们面临着建模用户与项目之间的协同偏好、个性化解释以及处理稀疏用户-项目交互的挑战。为了解决这些问题，我们提出了一种名为GaVaMoE的新框架，即高斯变分门控专家混合模型，用于可解释推荐。GaVaMoE引入了两个关键组件：(1) 一个评分重构模块，采用带有高斯混合模型（GMM）的变分自编码器（VAE），以捕捉复杂的用户-项目协同偏好，作为预训练的多门机制；(2) 一组细粒度的专家模型，与多门机制耦合，用于生成高度个性化的解释。VAE组件对用户-项目交互中的潜在因素进行建模，而GMM则聚类具有相似行为的用户。每个聚类对应多门机制中的一个门，将用户-项目对路由到适当的专家模型。这种架构使GaVaMoE能够为特定类型的用户和偏好生成定制化解释，通过利用用户之间的相似性来缓解数据稀疏问题。在三个真实世界数据集上的广泛实验表明，GaVaMoE在解释质量、个性化和一致性方面显著优于现有方法。特别是，在稀疏用户-项目交互场景中，GaVaMoE表现出稳健的性能，即使对于历史数据有限的用户也能保持高质量的解释。|
|**2024-10-15**|**MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding**|Yue Cao et.al.|[2410.11829](http://arxiv.org/abs/2410.11829)|**[link](https://github.com/yuecao0119/MMFuser)**|**尽管在跨模态交互中理解复杂的人类意图方面，多模态大语言模型（MLLMs）取得了显著进展，但捕捉复杂的图像细节仍然具有挑战性。先前的方法通过整合多个视觉编码器来增强视觉细节，但这种方法引入了冗余和计算开销。我们观察到，大多数MLLMs仅使用视觉编码器的最后一层特征图来进行视觉表示，而忽略了浅层特征图中的丰富细粒度信息。为了解决这个问题，我们提出了\modelname，这是一种简单而有效的多层特征融合器，能够高效地整合来自视觉变换器（ViTs）的深层和浅层特征。具体来说，它利用语义对齐的深层特征作为查询，动态提取浅层特征中缺失的细节，从而在保持语义对齐的同时丰富了表示形式的细粒度信息。应用于LLaVA-1.5模型时，\modelname在视觉表示和基准性能上取得了显著提升，提供了一种比多编码器集成方法更灵活、更轻量化的解决方案。代码和模型已发布在https://github.com/yuecao0119/MMFuser。**|
|**2024-10-15**|**SGEdit: Bridging LLM with Text2Image Generative Model for Scene Graph-based Image Editing**|Zhiyuan Zhang et.al.|[2410.11815](http://arxiv.org/abs/2410.11815)|null|场景图以节点和边的形式提供了图像的结构化、分层表示，分别表示对象及其相互关系。它可以用作图像编辑的自然界面，显著提高精度和灵活性。利用这一优势，我们引入了一个新框架，该框架将大型语言模型（LLM）与文本到图像生成模型相结合，用于基于场景图的图像编辑。这种集成使得在对象级别进行精确修改以及对场景进行创造性重构成为可能，而不会损害整体图像的完整性。我们的方法分为两个主要阶段：1）利用LLM驱动的场景解析器，我们构建了图像的场景图，捕捉关键对象及其相互关系，并解析细粒度属性如对象掩码和描述。这些注释促进了概念学习，使用微调扩散模型来代表每个对象，用优化的标记和详细的描述提示表示。2）在图像编辑阶段，LLM编辑控制器指导特定区域的编辑。这些编辑通过注意力调节的扩散编辑器实现，利用微调模型执行对象添加、删除、替换和调整。通过广泛的实验，我们证明了我们的框架在编辑精度和场景美学方面显著优于现有图像编辑方法。|
|**2024-10-15**|**NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models**|Han Han et.al.|[2410.11805](http://arxiv.org/abs/2410.11805)|null|大型语言模型（LLMs）结合工具学习在现实应用中已经取得了显著的成果。在工具学习过程中，LLMs可能会按照嵌套顺序调用多个工具，其中后一个工具调用可能将其前一个工具的响应作为输入参数。然而，当前对嵌套工具学习能力的研究仍然不足，因为现有的基准测试缺乏相关数据实例。为了解决这个问题，我们引入了NesTools来填补全面评估嵌套工具学习能力的空白。NesTools包含一种新颖的自动数据生成方法，用于构建具有不同嵌套结构的大规模嵌套工具调用。通过人工审核和优化，该数据集质量高且与现实场景紧密相关。因此，NesTools可以作为一个新的基准来评估LLMs的嵌套工具学习能力。我们对22个LLMs进行了广泛的实验，并使用NesTools进行了深入分析，结果表明当前的LLMs在复杂的嵌套工具学习任务上仍然存在困难。|
|**2024-10-15**|**FoundTS: Comprehensive and Unified Benchmarking of Foundation Models for Time Series Forecasting**|Zhe Li et.al.|[2410.11802](http://arxiv.org/abs/2410.11802)|null|时间序列预测（TSF）在金融、气象服务和能源管理等多个领域都是关键功能。尽管近年来出现了许多TSF方法，但这些方法中的许多需要特定领域的数据收集和模型训练，并且在新领域上的泛化性能较差。基础模型旨在克服这一局限。它们通过大规模语言或时间序列数据预训练，表现出在新或未见过的数据上进行推理的潜力。这促使了新型TSF基础模型的涌现。我们提出了一种新的基准测试，即FoundTS，以实现对这些模型进行彻底而公平的评估和比较。FoundTS涵盖了各种基于大型语言模型和预训练时间序列的基础模型。此外，FoundTS支持不同的预测策略，包括零样本、少量样本和全样本，从而促进更全面的评估。最后，FoundTS提供了一个标准化的评估流程管道，包括数据集分割、加载、归一化和少量样本抽取，从而实现公平的评估。在此基础上，我们对广泛领域内具有不同统计特性的多种数据集上的TSF基础模型进行了广泛的评估。具体而言，我们识别了现有基础模型的优点、缺点及其内在限制，并确定了未来模型设计的方向。我们的代码和数据集可以在https://anonymous.4open.science/r/FoundTS-C2B0获取。|
|**2024-10-15**|**Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability**|Tsz Ting Chung et.al.|[2410.11786](http://arxiv.org/abs/2410.11786)|null|大型语言模型（LLMs）在广泛的自然语言处理任务中展示了令人印象深刻的性能，特别是在利用上下文学习时。然而，上下文学习带来了额外的计算和财务成本。为了缓解这一问题，一些提示压缩方法被提出以压缩上下文学习中的提示。尽管这些方法取得了成功，但它们面临着由于模型特定压缩而导致的迁移性差的问题，或者依赖外部训练数据，例如GPT-4。在这篇论文中，我们研究了LLMs开发统一压缩方法的能力，该方法通过离散化不具信息性的标记，采用自监督预训练技术。通过在持续预训练过程中引入少量参数，所提出的Selection-p为每个输入标记生成一个概率值，指示保留或丢弃该标记。实验表明，Selection-p在多个分类任务中达到了最先进的性能，在实现高达10倍的压缩率的同时，仅经历了微小的0.8%性能下降。此外，它相比先前的工作在不同模型上的迁移性更优。另外，我们进一步分析了Selection-p如何有助于在长上下文中保持上下文学习的性能。|
|**2024-10-15**|**G-Designer: Architecting Multi-agent Communication Topologies via Graph Neural Networks**|Guibin Zhang et.al.|[2410.11782](http://arxiv.org/abs/2410.11782)|null|近期在基于大规模语言模型（LLM）的代理技术方面取得了显著进展，证明集体智能可以显著超越单个代理的能力，这主要得益于精心设计的代理间通信拓扑。尽管有许多多样化且高性能的设计可供选择，但实践者在为特定任务选择最有效的管道时常常感到困惑：哪种拓扑最适合我的任务，同时避免不必要的通信令牌开销并确保高质量的解决方案？针对这一困境，我们介绍了G-Designer，这是一种自适应、高效且稳健的多代理部署解决方案，能够动态设计任务感知的定制化通信拓扑。具体来说，G-Designer将多代理系统建模为一个多代理网络，利用变分图自动编码器对节点（代理）和一个特定任务的虚拟节点进行编码，并解码出一个任务适应性强且性能高的通信拓扑。在六个基准测试中的广泛实验表明，G-Designer具有以下特点：\textbf{(1) 高性能}，在MMLU上的准确率达到84.50%，在HumanEval上的pass@1达到89.90%；\textbf{(2) 任务适应性}，根据任务难度构建定制化的通信协议，将令牌消耗减少了高达95.33%；并且\textbf{(3) 对抗鲁棒}，能够抵御代理对抗攻击，仅导致0.3%的准确率下降。|
|**2024-10-15**|**Language Models Encode Numbers Using Digit Representations in Base 10**|Amit Arnold Levy et.al.|[2410.11781](http://arxiv.org/abs/2410.11781)|**[link](https://github.com/amitlevy/base10)**|大型语言模型（LLMs）在处理即使是简单的数值问题时，如比较两个小数字，也经常出错。一个自然的假设是这些错误源于模型如何表示数字，特别是它们是否捕捉到了数字的实际数值。我们通过观察发现，LLM在数值任务上的错误通常分布在答案的“位数”上，而不是围绕其“数值”正常分布。通过一系列探针实验和因果干预，我们展示了LLM内部以十进制的每一位数字进行圆环式表示，而不是数值表示。这种基于位的表示方式，而非数值表示，揭示了模型在涉及数值推理的任务中的错误模式，并可作为未来研究分析LLM中数值机制的基础。|
|**2024-10-15**|**MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**|Chenxi Wang et.al.|[2410.11779](http://arxiv.org/abs/2410.11779)|**[link](https://github.com/zjunlp/Deco)**|**多模态大语言模型（MLLMs）经常表现出幻觉现象，但其背后的原因尚未得到充分理解。在本文中，我们进行了实证分析并发现，尽管MLLMs在最终输出中错误地生成了对象，但在前一层它们实际上能够识别视觉对象。我们推测这可能是由于语言模型的强大知识先验抑制了视觉信息，从而导致幻觉。受此启发，我们提出了一种新颖的动态校正解码方法（DeCo），该方法自适应地选择合适的前一层，并按比例将知识整合到最终层以调整输出logits。值得注意的是，DeCo是与模型无关的，可以无缝地与各种经典解码策略结合，并应用于不同的MLLMs。我们在广泛使用的基准上评估了DeCo，结果表明它相比基线大幅降低了幻觉率，突显了其减轻幻觉的潜力。代码可在https://github.com/zjunlp/DeCo获取。**|
|**2024-10-15**|**Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models**|Kai Yao et.al.|[2410.11772](http://arxiv.org/abs/2410.11772)|**[link](https://github.com/kaiseem/ist)**|**参数高效微调（PEFT）方法因其在适应预训练大型语言模型（LLMs）到下游任务时显著减少内存和计算开销的潜力而广受欢迎。然而，大多数PEFT方法的一个常见限制是它们在整个层中应用统一的架构设计，这涉及相同的可训练模块，并忽略了每层的重要性差异，从而导致微调结果不佳。为了克服上述局限并获得更好的性能，我们开发了一种新颖的方法，称为重要性感知稀疏调优（IST），以充分利用固有的稀疏性，并通过有效的逐层重要性评分选择最重要的全层子集。所提出的IST是一种通用且即插即用的技术，与各种基于层的PEFT方法兼容。通过利用估计的重要性得分，IST在PEFT模块中动态更新这些选定的层，从而降低内存需求。我们进一步提供了收敛性的理论证明和优于均匀更新策略的实证证据，以证明IST相对于现有方法的优势。广泛的实验涵盖了各种LLMs、PEFT方法和下游任务，证实了我们提出方法的有效性，展示了IST增强现有基于层的PEFT方法的能力。我们的代码可在https://github.com/Kaiseem/IST获取。**|
|**2024-10-14**|**DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads**|Guangxuan Xiao et.al.|[2410.10819](http://arxiv.org/abs/2410.10819)|**[link](https://github.com/mit-han-lab/duo-attention)**|**部署长上下文的大规模语言模型（LLM）至关重要，但也带来了显著的计算和内存挑战。缓存所有注意力头中的Key和Value（KV）状态会消耗大量内存。现有的KV缓存剪枝方法要么损害了LLM的长上下文能力，要么只提供了有限的效率提升。本文发现，只有部分注意力头，即检索头，对于处理长上下文是至关重要的，并且需要对所有标记进行完整的注意力机制。相反，所有其他头部，主要关注最近的标记以及注意力汇点，称为流头部，不需要完整的注意力。基于这一见解，我们引入了DuoAttention框架，该框架仅对检索头应用完整的KV缓存，而对流头部使用轻量级、固定长度的KV缓存，从而在不损害长上下文能力的情况下减少LLM解码和预填充的内存和延迟。DuoAttention采用了一种基于优化的算法，使用合成数据准确识别检索头。我们的方法将长上下文推理内存最多减少了2.55倍（对于MHA模型）和1.67倍（对于GQA模型），同时解码速度提高了最多2.18倍（MHA模型）和1.50倍（GQA模型），并加速预填充最多1.73倍（MHA模型）和1.63倍（GQA模型），并且与全注意力相比，精度损失最小。值得注意的是，结合量化技术，DuoAttention使Llama-3-8B能够在单个A100 GPU上解码长达330万上下文长度的数据。代码可在https://github.com/mit-han-lab/duo-attention获取。**|
|**2024-10-14**|**Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free**|Ziyue Li et.al.|[2410.10814](http://arxiv.org/abs/2410.10814)|**[link](https://github.com/tianyi-lab/moe-embedding)**|尽管大型语言模型（LLMs）在生成任务上表现出色，但其解码器-only架构通常限制了它们作为嵌入模型的潜力，除非进行进一步的表示微调。这是否与它们作为通用模型的主张相矛盾？为了回答这个问题，我们更仔细地研究了混合专家（MoE）LLMs。我们的研究表明，MoE LLMs中的专家路由可以作为一个现成的嵌入模型，在各种嵌入重点任务上表现出色，而无需任何微调。此外，我们广泛的分析表明，MoE路由权重（RW）与LLMs广泛使用的隐藏状态（HS）互补。与HS相比，我们发现RW对提示的选择更具鲁棒性，并关注高层次语义。受此分析启发，我们提出了MoEE，结合了RW和HS，其性能优于单独使用任一方法。我们对它们的组合及其提示策略的探索揭示了若干新颖见解，例如，RW和HS相似度的加权和优于它们连接后的相似度。我们在来自大规模文本嵌入基准（MTEB）的6个嵌入任务中的20个数据集上进行了实验。结果表明，MoEE显著提升了基于LLM的嵌入效果，且无需进一步微调。|
|**2024-10-14**|**LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory**|Di Wu et.al.|[2410.10813](http://arxiv.org/abs/2410.10813)|**[link](https://github.com/xiaowu0162/longmemeval)**|**近期，大型语言模型（LLM）驱动的聊天助手系统已集成了记忆组件来跟踪用户与助手之间的聊天历史，从而实现更准确和个性化的响应。然而，它们在持续交互中的长期记忆能力仍需深入研究。本文介绍了一个名为LongMemEval的综合基准，用于评估聊天助手的五项核心长期记忆能力：信息提取、多会话推理、时间推理、知识更新和弃权。该基准包含500个精心策划的问题，并嵌入在自由扩展的用户与助手聊天历史中。LongMemEval对现有的长期记忆系统提出了重大挑战，在商业聊天助手和长上下文LLM上，跨持续交互的记忆信息保留率下降了30%。随后，我们提出了一种统一框架，将长期记忆设计分解为索引、检索和阅读阶段的四个设计选择。基于关键实验洞察，我们提出了几种内存设计，包括会话分解以优化值粒度、事实增强的关键扩展以增强索引结构以及时间感知查询扩展以细化搜索范围。实验结果表明，这些优化极大地提高了LongMemEval上的内存召回率和下游问题回答性能。总体而言，本研究为推进基于LLM的聊天助手的长期记忆能力提供了有价值的资源和指导，为更个性化和可靠的对话AI铺平了道路。**|
|**2024-10-14**|**Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning**|Aakanksha et.al.|[2410.10801](http://arxiv.org/abs/2410.10801)|null|大型语言模型（LLMs）已被全球广泛采用，应用于各种领域。然而，确保其安全使用仍然是一个重大挑战。偏好训练和安全措施往往过度拟合于西方中心数据集中的危害，而安全协议通常无法扩展到多语言环境。在这项工作中，我们在多样化的多任务设置中探索模型合并，在多语言背景下结合安全和通用任务。每种语言在不同任务中引入了独特的学习挑战。我们发现，基于目标的合并比混合数据更有效，总体性能和安全性分别提高了8%和10%。我们还发现，基于语言的合并非常有效——通过合并单语微调模型，我们实现了在相同可用数据下，相比混合数据方法，整体性能提高4%，所有语言上的危害减少7%。总的来说，我们对合并方法的综合研究提供了一个构建强大且安全的多语言模型的有用框架。|
|**2024-10-15**|**MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling**|Jian Yang et.al.|[2410.10798](http://arxiv.org/abs/2410.10798)|null|近年来，多模态大语言模型的发展推动了联合概率模型的进步，这些模型能够同时理解和生成图像。然而，我们发现最近的方法在理解任务过程中不可避免地会丢失图像信息，这主要是由于图像离散化或扩散去噪步骤造成的。为了解决这一问题，我们提出了一种新的多模态自回归（MMAR）概率建模框架。与离散化方法不同，MMAR采用连续值的图像标记来避免信息丢失。不同于基于扩散的方法，我们通过在每个自回归图像块嵌入顶部添加一个轻量级扩散头来解耦扩散过程和自回归主干模型。这样一来，当模型从图像生成过渡到通过文本生成进行理解时，主干模型对图像的隐藏表示不受限于最后的去噪步骤。为了成功训练我们的方法，我们还提出了一种理论上被证明可以解决数值稳定性问题的技术，并提出了一种平衡生成和理解任务目标的训练策略。通过在18个图像理解基准上进行广泛的评估，MMAR展示了比其他联合多模态模型更优越的性能，其性能可与采用预训练CLIP视觉编码器的方法相媲美，同时还能生成高质量的图像。我们还表明，该方法在更大数据集和更大模型规模下具有可扩展性。|
|**2024-10-14**|**Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance**|Sachin Goyal et.al.|[2410.10796](http://arxiv.org/abs/2410.10796)|**[link](https://github.com/locuslab/context-parametric-inversion)**|**大型语言模型通过指令微调来增强其遵循用户指令和处理输入上下文的能力。然而，即使是最先进的模型也常常难以遵循指令，尤其是在输入上下文与模型的参数知识不一致时。这会导致各种失败，例如幻觉，即响应内容过时、带有偏见或包含未经验证的事实。在这项工作中，我们试图理解这种不良上下文依赖性的根本原因，特别是在指令微调之后。我们观察到一个有趣的现象：在指令微调过程中，上下文依赖性最初如预期般增加，但随着指令微调的进行，这种依赖性逐渐减少。我们将这一现象称为上下文-参数反转，并发现在多个通用指令调优数据集（如TULU、Alpaca和Ultrachat）以及模型家族（如Llama、Mistral和Pythia）中都存在这种现象。在一个简单的理论设置中，我们沿着指令微调的梯度下降轨迹分离出上下文-参数反转发生的原因。我们将这一现象与指令微调数据混合中的示例联系起来，这些示例中输入上下文提供的信息已经存在于模型的参数知识中。我们的分析提出了某些有限的缓解策略，同时也验证了我们的理论见解。我们希望我们的工作能作为解决这一失败模式的一个起点，而这一模式是LLM训练中的一个标准部分。**|
|**2024-10-14**|**Focused ReAct: Improving ReAct through Reiterate and Early Stop**|Shuoqiu Li et.al.|[2410.10779](http://arxiv.org/abs/2410.10779)|null|大型语言模型（LLMs）在推理和决策能力方面有了显著的提升，这体现在ReAct等方法中。然而，尽管ReAct在处理复杂任务时非常有效，但它面临两个主要挑战：一是容易偏离原始问题，二是陷入行动循环。为了解决这些问题，我们引入了Focused ReAct，这是ReAct范式的一个增强版本，它结合了重申和早期停止机制。这些改进有助于模型保持对原始问题的关注并避免重复行为。实验结果表明，与原始的ReAct方法相比，Focused ReAct的准确率提高了18%到530%，运行时间减少了最多34%。|
|**2024-10-14**|**AFlow: Automating Agentic Workflow Generation**|Jiayi Zhang et.al.|[2410.10762](http://arxiv.org/abs/2410.10762)|**[link](https://github.com/geekan/metagpt)**|**大型语言模型（LLMs）在解决各种领域中的复杂任务方面展现出了显著的潜力，通常通过采用遵循详细指令和操作序列的代理工作流程来实现。然而，构建这些工作流程需要大量的人力，这限制了其可扩展性和通用性。最近的研究试图自动化生成和优化这些工作流程，但现有的方法仍然依赖于初始的手动设置，并且未能实现完全自动化和有效的流程生成。为了解决这一挑战，我们将工作流优化重新表述为一个代码表示的工作流空间搜索问题，在该空间中，由LLM调用的节点通过边连接。我们引入了AFlow，这是一个自动化的框架，使用蒙特卡洛树搜索有效地探索这个空间，通过代码修改、树结构的经验以及执行反馈迭代地改进工作流程。在六个基准数据集上的实证评估表明，AFlow的有效性，平均比最先进的基线提高了5.7%。此外，AFlow使得较小的模型在特定任务上能够超越GPT-4，同时其推理成本仅为GPT-4的4.55%。代码将在https://github.com/geekan/MetaGPT获取。**|
|**2024-10-14**|**Denial-of-Service Poisoning Attacks against Large Language Models**|Kuofeng Gao et.al.|[2410.10760](http://arxiv.org/abs/2410.10760)|**[link](https://github.com/sail-sg/p-dos)**|**近期的研究表明，大型语言模型（LLMs）容易受到拒绝服务（DoS）攻击，这种攻击通过恶意输入如拼写错误或无意义的提示词触发模型无限输出，而不会生成[EOS]结束符。这些攻击可能导致高延迟，并使LLM服务对其他用户或任务不可用。然而，在存在语音到文本接口的情况下（例如，对机器人的语音指令），执行此类DoS攻击变得具有挑战性，因为通过语音很难引入拼写错误或无意义的提示词。一种简单的DoS攻击方式是指示模型“不断重复‘Hello’”，但我们观察到依赖自然指令的方式会限制输出长度，该长度受限于预训练数据的最大长度。为了克服这一限制，我们提出了一种针对LLMs的基于投毒的DoS（P-DoS）攻击方法，证明通过注入一个精心设计的投毒样本可以突破输出长度的限制。例如，一个投毒样本能够以不到1美元的成本成功攻击GPT-4o和GPT-4o mini（通过OpenAI的微调API），导致重复输出直至达到最大推理长度（16K个标记，相比之下未投毒前为0.5K）。此外，我们还对开源LLMs进行了全面的消融研究，并将此方法扩展到LLM代理，其中攻击者可以控制微调数据集和算法。我们的发现强调了需要防御P-DoS攻击以确保LLMs的安全。我们的代码可以在https://github.com/sail-sg/P-DoS获取。**|
|**2024-10-14**|**SplitLLM: Collaborative Inference of LLMs for Model Placement and Throughput Optimization**|Akrit Mudvari et.al.|[2410.10759](http://arxiv.org/abs/2410.10759)|null|大型语言模型（LLMs）近年来成为一项颠覆性的创新，在我们的日常生活中扮演着重要角色，因为它们能够理解和生成类似人类的文本。它们的功能包括自然语言理解、信息检索和搜索、翻译、聊天机器人、虚拟助手等。然而，众所周知，LLMs在参数数量上非常庞大。此外，底层架构Transformer中的自注意力机制在计算和内存方面与输入序列长度呈二次复杂性关系。由于这些原因，LLM推理资源密集型高，因此LLM推理的吞吐量受到限制，尤其是在较长序列的情况下。在这份报告中，我们设计了一种服务器与其客户端之间的协作推理架构，以缓解吞吐量限制。在这个设计中，我们考虑了双方可用的资源，即计算和通信成本。我们开发了一种基于动态规划的算法，以最优方式分配服务器和客户端设备之间的计算，从而提高服务器吞吐量，同时不违反服务水平协议（SLA）。实验表明，我们能够高效地分配工作负载，使服务器的工作负载减少约三分之一，同时比贪心方法提高了19%。结果表明，在具有不同类型LLM推理请求的环境中，服务器的吞吐量得到了提升。|
|**2024-10-11**|**AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation**|Zijun Wang et.al.|[2410.09040](http://arxiv.org/abs/2410.09040)|**[link](https://github.com/ucsc-vlaa/attngcg-attack)**|**本文研究了基于转换器的大型语言模型（LLMs）受到囚禁攻击的脆弱性，特别关注基于优化的贪婪坐标梯度（GCG）策略。我们首先观察到攻击的有效性与模型内部行为之间存在正相关关系。例如，当模型对旨在确保LLM安全对齐的系统提示给予更多关注时，攻击往往效果较差。在此基础上，我们引入了一种增强方法，通过操纵模型的注意力分数来促进LLM的囚禁，我们将其命名为AttnGCG。实验上，AttnGCG在各种LLMs上表现出一致的改进，在Llama-2系列中平均提高了约7%，在Gemma系列中提高了约10%。我们的策略还展示了针对未见过的有害目标和黑盒LLMs（如GPT-3.5和GPT-4）的稳健攻击转移能力。此外，我们注意到我们的注意力分数可视化更易于解释，使我们能够更好地了解如何通过有针对性的注意力操纵实现更有效的囚禁。我们发布了代码，可在https://github.com/UCSC-VLAA/AttnGCG-attack中获取。**|
|**2024-10-11**|**Semi-Supervised Learning of Noisy Mixture of Experts Models**|Oh-Ran Kwon et.al.|[2410.09039](http://arxiv.org/abs/2410.09039)|null|混合专家（MoE）模型是一个灵活的预测建模框架，在大型语言模型的时代重新引起了人们的关注。一个由预测“专家”组成的集合与控制在预测时每个专家影响力的“门控函数”共同学习。这种结构允许相对简单的模型在复杂、异构的数据环境中表现出色。在当今许多应用场景中，未标记数据广泛可用而标注数据却难以获取。半监督学习方法旨在利用未标记数据。我们提出了一种用于MoE模型半监督学习的新方法。我们从海洋学家开发的一种假设强烈的半监督MoE模型开始，该模型假设未标注数据中的潜在聚类结构直接映射到监督任务中每个专家应给予的影响。我们放松了这一假设，设想两者之间存在噪声连接，并基于最小化剔除平方算法提出了一种算法，即使存在数据错位也能成功。我们的理论分析确定了该方法能够产生接近参数率收敛估计器的条件。模拟和真实数据示例证明了该方法的有效性。|
|**2024-10-11**|**SimpleStrat: Diversifying Language Model Generation with Stratification**|Justin Wong et.al.|[2410.09038](http://arxiv.org/abs/2410.09038)|null|生成大型语言模型（LLM）的多样化响应对于规划/搜索和合成数据生成等应用至关重要。这些应用需要在生成过程中提供多样化的答案，以便在每次生成时都能得到不同的结果。之前的方法通常依赖于增加温度来提高多样性。然而，与普遍认识相反，我们发现这种方法不仅会导致随着温度增加，个体生成的质量降低，而且其有效性还取决于模型的下一个词概率与真实答案分布的相似性。  我们提出了一种名为“SimpleStrat”的替代方法，该方法利用语言模型本身对空间进行分区。在推理阶段，随机选择一个分区并在其中抽取样本。为了衡量多样性，我们引入了CoverageQA数据集，它包含了具有多个同等可能答案的未指定问题。通过测量输出分布与有效地面真相答案的均匀分布之间的KL散度来评估多样性。由于计算专用模型每条响应/解决方案的概率通常是不可行的，因此我们使用召回率来评估地真理解。  我们的评估结果显示，使用SimpleStrat方法可以实现比GPT-4o高0.05的召回率，并且平均减少了0.36的KL散度与Llama 3相比。|
|**2024-10-11**|**Mentor-KD: Making Small Language Models Better Multi-step Reasoners**|Hojae Lee et.al.|[2410.09037](http://arxiv.org/abs/2410.09037)|**[link](https://github.com/2hojae/mentor-kd)**|**大型语言模型（LLMs）通过利用链式思维（CoT）提示在各种复杂任务中表现出非凡的性能。近期的研究提出了一种知识蒸馏（KD）方法——推理蒸馏，通过微调由LLM教师生成的多步推理语言模型，将LLM的推理能力转移到较小的模型上。然而，这些研究在以下两个方面考虑不足：从LLM教师模型获取的示例集质量低和软标签提供不足。本文提出了一种名为导师-KD的方法，该方法有效地将LLM的多步推理能力转移到较小的语言模型上，并解决了上述挑战。具体而言，我们利用一个导师——特定任务的中间大小的微调模型——来增加额外的CoT注释并为学生模型提供软标签，以在推理蒸馏过程中提供支持。我们进行了广泛的实验，并确认了导师-KD在不同模型和复杂推理任务上的有效性。**|
|**2024-10-11**|**PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents**|Xiangyu Yin et.al.|[2410.09034](http://arxiv.org/abs/2410.09034)|null|Ptychography是一种在X射线和电子显微镜领域广泛应用的高级计算成像技术。它在物理学、化学、生物学和材料科学等研究领域以及半导体表征等工业应用中被广泛采用。实践过程中，获得高质量的ptychographic图像需要同时优化众多实验和算法参数。传统上，参数选择往往依赖于试错法，导致工作效率低下，并可能引入人为偏见。本工作开发了“ptychographic实验与分析机器人”（PEAR），这是一种利用大型语言模型（LLMs）自动处理ptychography数据分析的框架。为了确保高鲁棒性和准确性，PEAR采用了多个LLM代理进行知识检索、代码生成、参数推荐和图像推理任务。我们的研究表明，PEAR的多代理设计显著提高了工作流程的成功率，即使使用较小的开源权重模型如LLaMA 3.1 8B也是如此。PEAR还支持各种自动化级别，并设计有可自定义的本地知识库，以确保其在不同研究环境下的灵活性和适应性。|
|**2024-10-11**|**The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals**|Xiaofeng Wu et.al.|[2410.09013](http://arxiv.org/abs/2410.09013)|null|本文研究了大型语言模型（LLMs）和视觉语言模型（VLMs）在利用汉字中的视觉信息方面的潜力，尤其是关于部首、结构、笔画以及笔画数量的信息。我们构建了一个基准测试系统来评估这些模型对汉字中视觉元素的理解程度。实验结果表明，尽管提供字符图像，模型仍然展示了有限但部分理解视觉信息的能力。  为了激发模型利用部首进行中文理解任务的潜力，我们进一步尝试将部首信息融入到提示中。我们观察到，在提供关于部首的额外信息时，词性标注任务的表现得到了一致性的提升。这表明通过整合子字符信息，有可能增强语言处理能力。|
|**2024-10-11**|**Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models**|Hao Li et.al.|[2410.09012](http://arxiv.org/abs/2410.09012)|**[link](https://github.com/sailresearch/fmse-blogs)**|本文首次从实践者的视角分析了基础模型（FMs）在软件工程（SE）领域的应用。通过分析来自顶级科技公司的155篇FM4SE和997篇SE4FM博客文章，利用基于FM的调研方法系统地标记和总结了讨论的活动和任务。研究发现，虽然代码生成是FM4SE中最突出的任务，但FMs还被用于代码理解、总结和API推荐等众多其他SE活动。关于SE4FM的大多数博客文章关注于模型部署与操作以及系统架构与编排。尽管云部署占主导地位，但对FMs进行压缩并在边缘或移动设备上部署的兴趣正在增长。本文提出了八个未来研究方向，旨在弥合理论发现与实际应用之间的差距。我们的研究不仅丰富了FMs在SE领域实践应用的知识体系，还展示了FMs在技术与灰色文献领域进行文献调研的有效性。我们提供的数据集、结果、代码以及使用的提示可以在在线复制包https://github.com/SAILResearch/fmse-blogs中找到。|
|**2024-10-11**|**SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights**|Ling Yang et.al.|[2410.09008](http://arxiv.org/abs/2410.09008)|**[link](https://github.com/yangling0818/supercorrect-llm)**|**大型语言模型（LLMs）如GPT-4、PaLM和LLaMA在各种推理任务上表现出显著的改进。然而，较小的模型如Llama-3-8B和DeepSeekMath-Base仍然在复杂的数学推理方面存在挑战，因为它们无法有效地识别并纠正推理错误。近期的反思方法旨在通过使模型能够自我反思和自我校正来解决这些问题，但仍面临独立检测推理步骤中的错误的挑战。为了克服这些限制，我们提出了一种名为SuperCorrect的新型两阶段框架，它使用大型教师模型来监督和纠正较小学生模型的推理和反思过程。  在第一阶段，我们从教师模型中提取了层次化的高阶和详细的思想模板，以指导学生模型生成更细致的推理思想。在第二阶段，我们引入了跨模型协作直接偏好优化（DPO）来增强学生模型的自我校正能力，在训练过程中跟随教师的修正轨迹进行改进。这种跨模型DPO方法教会学生模型通过从教师模型获得的错误驱动的见解有效地定位并解决错误的思想，打破其思想的瓶颈，并通过学习新技能和知识来应对具有挑战性的问题。  广泛的实验一致证明了我们的优越性。值得注意的是，我们的SuperCorrect-7B模型在MATH/GSM8K基准测试中显著超越了强大的DeepSeekMath-7B和Qwen2.5-Math-7B，分别在MATH和GSM8K基准上提高了7.8%/5.3%和15.1%/6.3%，在所有7B模型中实现了新的最先进性能。代码：https://github.com/YangLing0818/SuperCorrect-llm**|
|**2024-10-11**|**From Interaction to Impact: Towards Safer AI Agents Through Understanding and Evaluating UI Operation Impacts**|Zhuohao Jerry Zhang et.al.|[2410.09006](http://arxiv.org/abs/2410.09006)|null|随着生成式人工智能的进步，人们在创建能够通过用户界面（UI）管理日常任务的自主代理方面取得了进展。尽管先前的研究已经探讨了AI代理如何导航UI以及理解UI结构的机制，但代理及其自主行为（特别是可能具有风险或不可逆性的行为）的影响和后果仍然缺乏深入研究。本工作中，我们探索了AI代理UI操作的实际世界影响和后果。  我们首先通过一系列与领域专家的工作坊开发了一种UI操作影响的分类系统。随后，我们进行了一项数据综合研究，收集了用户感知为具有影响力的UI屏幕轨迹和操作数据。然后，我们使用我们的影响类别对收集的数据和从现有UI导航数据集中重新利用的数据进行了注释。我们对不同大型语言模型（LLMs）及其变体的定量评估显示了这些LLM理解和预测AI代理可能采取的UI操作影响的能力。  我们的研究结果表明，我们的分类系统增强了这些LLM的推理能力，使它们能够更好地理解UI操作的影响。然而，我们也发现了他们在可靠地分类更微妙或复杂的影响力类别时存在显著差距的问题。|
|**2024-10-11**|**Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference**|Grace Proebsting et.al.|[2410.08996](http://arxiv.org/abs/2410.08996)|null|我们通过使用GPT-4、Llama-2和Mistral 7b等大型语言模型（LLM）来生成自然语言推理（NLI）假设，测试了用LLM替换众包工作者对产生注释偏见的影响。我们复现了斯坦福NLI语料库的部分数据，并训练了仅使用假设的分类器来确定LLM生成的假设是否包含注释偏见。在我们的由LLM生成的NLI数据集上，基于BERT的仅假设分类器达到了86%-96%的准确率，这表明这些数据集包含仅假设的偏见。我们还发现LLM生成的假设中存在频繁的“线索”，例如，“在泳池里游泳”这一短语在GPT-4生成的10000多个矛盾假设中出现。我们的分析提供了实证证据，证明NLI中已知的偏见可能在LLM生成的数据中持续存在。|
|**2024-10-10**|**Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training**|Gen Luo et.al.|[2410.08202](http://arxiv.org/abs/2410.08202)|null|随着大型语言模型（LLM）的迅速发展，对扩展其能力以处理多模态任务的关注日益增加。其中，对单体多模态大型语言模型（MLLM）的研究引起了广泛关注，这些模型整合了视觉编码和语言解码功能。尽管单体MLLM在结构上简洁且易于部署，但要实现具有竞争力性能的训练仍面临挑战。流行的策略采用连续预训练方法，将预训练的LLM扩展为单体MLLM，这会导致灾难性遗忘并导致性能退化。  本文旨在从增量学习的角度克服这一局限性。具体来说，我们的核心思想是在预训练的LLM中嵌入视觉参数，通过增量学习机制，即在优化视觉参数时冻结LLM，从大量数据中逐步学习视觉知识。基于这一原则，我们提出了一种名为Mono-InternVL的新型单体MLLM，它通过多模态混合专家结构无缝地融合了一系列视觉专家。此外，我们还提出了一种创新的预训练策略来最大化Mono-InternVL的视觉能力，即内生视觉预训练（EViP）。具体而言，EViP设计为一个视觉专家的渐进式学习过程，旨在充分利用从低质量数据到高质量数据的视觉知识。  为了验证我们的方法，我们在16个基准上进行了广泛实验。实验结果不仅证实了与当前最先进的单体MLLM相比，Mono-InternVL在6个多模态基准上的卓越性能，例如在OCRBench上的+113点优势，而且还确认了其更好的部署效率，首次令牌延迟降低了高达67%。|
|**2024-10-10**|**From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions**|Changle Qu et.al.|[2410.08197](http://arxiv.org/abs/2410.08197)|**[link](https://github.com/quchangle1/DRAFT)**|**本文专注于解决大型语言模型（LLM）与外部工具交互过程中存在的理解鸿沟问题，这一鸿沟源于现有人类导向的工具文档的不完善性和不准确性。我们提出了一种名为DRAFT的新框架，旨在动态优化工具文档，通过分析来自LLM与外部工具交互过程中的反馈和轨迹信息。该方法基于一种创新的试错学习流程，包括经验收集、从经验学习以及文档重写三个阶段，以迭代方式提升工具文档的质量。  为了确保探索的多样性并避免过拟合，DRAFT还采用了促进多样性的探索策略，并配备了一个工具适应性终止机制来提高效率。在多个数据集上的实验结果表明，DRAFT通过迭代反馈优化显著提高了文档质量，促进了LLM对工具的更深入理解和更有效利用。我们的分析进一步揭示了通过这种方法优化后的工具文档具有强大的跨模型通用能力。**|
|**2024-10-10**|**MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code**|Zimu Lu et.al.|[2410.08196](http://arxiv.org/abs/2410.08196)|**[link](https://github.com/mathllm/mathcoder2)**|**本文介绍了一种新颖的方法，用于生成伴随推理步骤的数学代码，以进行持续预训练。我们的方法首先通过整合数学相关网络数据、使用数学包的代码、数学教科书和合成数据来构建高质量的数学持续预训练数据集。接着，我们通过提取LaTeX表达式、表达式的条件以及结果来构造推理步骤。基于这些提取的信息，我们生成相应的代码，以准确捕捉数学推理过程。我们将生成的代码附加到每个推理步骤后，形成包含自然语言推理步骤及其对应代码的数据对。将此数据与原始数据集结合，得到一个包含19.2B个标记的高性能数学预训练语料库，我们将其命名为MathCode-Pile。使用此语料库对几种流行的基模进行训练，显著提高了它们的数学能力，从而产生了名为MathCoder2的模型家族。所有数据处理和训练代码均开源，确保了整个数据收集和训练流程的透明性和可复现性。代码在https://github.com/mathllm/MathCoder2上发布。**|
|**2024-10-10**|**GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment**|Yuancheng Xu et.al.|[2410.08193](http://arxiv.org/abs/2410.08193)|null|大型语言模型（LLM）展现出令人印象深刻的能力，但需要仔细对齐以满足人类的偏好。传统的训练时方法通过使用人类偏好数据集来微调LLM，但会带来显著的训练成本，并且需要重复训练以应对多样化的用户偏好。测试时对齐方法通过使用奖励模型（RM）来引导冻结的LLM，而无需重新训练，从而解决了这一问题。然而，现有的测试时方法依赖于轨迹级RM，它们旨在评估完整响应，这使得它们不适合用于需要从部分响应计算下一个词奖励的自回归文本生成。  为了应对这一挑战，我们引入了GenARM，一种测试时对齐方法，利用了自回归奖励模型——一种新型的奖励参数化方法，旨在预测自回归生成过程中的下一个词奖励，以实现高效和有效的自回归生成。理论上，我们证明了这种参数化可以在KL正则化强化学习框架内引导冻结的LLM接近任何由传统RM可实现的分布。实验结果表明，GenARM在性能上显著优于先前的测试时对齐基线，并且与训练时方法的性能相当。此外，GenARM支持弱到强的指导，允许在不需要训练更大模型的情况下，通过较小的RM对更大的LLM进行对齐，从而降低了成本。进一步地，GenARM还支持多目标对齐，允许实时平衡偏好维度，满足不同用户需求，而无需重新训练。|
|**2024-10-10**|**Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models**|Qingni Wang et.al.|[2410.08174](http://arxiv.org/abs/2410.08174)|null|本论文提出了一种名为TRON的两步框架，旨在对任何支持在开放和封闭场景下采样的大型多模态语言模型（MLLM）进行风险控制与评估。TRON由两个主要组件构成：（1）一种新颖的校准评分方法，用于以最小尺寸采样响应集；（2）基于自致性理论的非一致性评分，通过设定两种特定的风险水平来控制错误率。此外，本研究首次探讨了在开放场景下的预测集中的语义冗余问题，并据此提出了一个用于评价MLLM的新指标——平均集合大小。  通过在四个视频问答（VideoQA）数据集上使用八种MLLM进行全面实验，我们证明了TRON能够实现用户指定的风险水平范围内的期望错误率。同时，去重后的预测集在保持适应性的同时，展现出更高效、稳定的风险评估能力，在不同风险水平下均有出色表现。|
|**2024-10-10**|**On the Evaluation of Generative Robotic Simulations**|Feng Chen et.al.|[2410.08172](http://arxiv.org/abs/2410.08172)|null|由于获取真实世界数据的困难性，机器人模拟已成为并行训练和模拟到现实世界的转换的关键，这凸显了可扩展仿真机器人任务的重要性。基础模型已经展现出在自主生成可行机器人任务方面的惊人能力。然而，这一新范式强调了评估这些自主生成任务的挑战。为了解决这一问题，我们提出了一种针对生成模拟的全面评价框架。我们的框架将评估分为三个核心方面：质量、多样性和泛化。对于单任务质量，我们使用大型语言模型和视觉语言模型评估生成任务的真实性和生成轨迹的完整性。在多样性方面，我们通过任务描述的文本相似性和收集的任务轨迹训练的世界模型损失来测量任务和数据的多样性。对于任务级别的泛化，我们评估了使用多个生成任务训练的策略在未见过的任务上的零样本泛化能力。在三个代表性任务生成管道上进行的实验表明，我们的框架的评估结果与人类评估高度一致，确认了我们方法的可行性和有效性。研究发现，虽然可以通过某些方法实现质量和多样性的指标，但没有任何一种方法能够在所有指标上都表现出色，这表明需要更多地关注平衡这些不同指标。此外，我们的分析进一步突显了当前工作面临的共同挑战——低泛化能力。  匿名网站链接：https://sites.google.com/view/evaltasks|
|**2024-10-10**|**Agent S: An Open Agentic Framework that Uses Computers Like a Human**|Saaket Agashe et.al.|[2410.08164](http://arxiv.org/abs/2410.08164)|**[link](https://github.com/simular-ai/agent-s)**|**我们介绍了一种名为Agent S的开放性代理框架，它通过图形用户界面(GUI)与计算机进行自主交互，旨在通过自动化复杂、多步骤的任务来改变人机交互方式。Agent S旨在解决自动化计算机任务时遇到的三个关键挑战：获取特定领域的知识、在长任务周期内规划以及处理动态、非均匀的界面。为此，Agent S引入了经验增强的层次规划方法，该方法在多个级别上结合外部知识搜索和内部经验检索，从而实现高效的任务规划和子任务执行。此外，它采用了代理-计算机接口(ACI)，基于多模态大型语言模型(MLLMs)更好地揭示GUI代理的推理和控制能力。在OSWorld基准测试中的评估显示，与基线相比，Agent S的成功率提高了9.37%(相对提高了83.6%)，并达到了新的最高水平。全面分析强调了各个组件的有效性，并提供了未来改进的见解。此外，Agent S在新发布的WindowsAgentArena基准上展示了广泛的通用性，能够适应不同的操作系统。有关代码的更多信息，请参阅https://github.com/simular-ai/Agent-S。**|
|**2024-10-10**|**Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning**|Amrith Setlur et.al.|[2410.08146](http://arxiv.org/abs/2410.08146)|null|提高大型语言模型推理能力的一种有前景的方法是使用过程奖励模型（PRMs）。与仅在最终步骤提供反馈的结果奖励模型（ORMs）相比，PRMs在多步推理跟踪的每个步骤都提供反馈，可能有助于改进信用分配。然而，收集密集、每步骤的人类标签并不具有可扩展性，从自动标记数据训练PRMs迄今为止导致的增益有限。为了通过运行搜索来改进基策略或将其用作强化学习（RL）的密集奖励来优化基策略，我们提出的问题是：“我们应该如何设计过程奖励？”我们的关键洞察是，为了有效，步骤级奖励应该衡量进度：采取步骤前后产生正确响应的可能性变化，对应于RL中的步骤级优势的概念。关键在于，这种进展应该在与基策略不同的证明策略下进行测量。我们理论地描述了良好的证明者集合，并且我们的结果表明，通过这样的证明者优化过程奖励可以改善测试时搜索和在线RL期间的探索。实际上，我们的描述显示，弱证明者策略可以显着提高更强的基策略，这也是我们在实验上观察到的现象。我们通过训练过程优势验证器（PAVs）来预测在这些证明者下进行的进展，证明与ORMs相比，在线RL使用PAVs提供的密集奖励可以实现高达8％以上的准确性提高，以及1.5至5倍的计算效率提高。使用PAVs的在线RL首次实现了样本效率提升5-6倍，准确率提升超过6％的结果。|
|**2024-10-10**|**Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**|Xiaoyuan Liu et.al.|[2410.08145](http://arxiv.org/abs/2410.08145)|**[link](https://github.com/xyliu-cs/ConflictVIS)**|本文探讨了在多模态大型语言模型（MLLM）中视觉信息与模型内部常识知识冲突的问题。研究发现，在特定情况下，MLLMs可能基于文本查询而非视觉输入做出决策，导致常识级的视觉-知识矛盾。为解决这一问题，我们设计了一套自动化的评估流程，并辅以人工质量控制环节，构建了一个用于模拟和评估此类冲突的基准测试系统。  该基准测试包含了374张原创图片及1122个高质量的问题-答案对，覆盖了两种冲突目标类型和三个不同难度级别的问题，为全面评估模型提供了工具。通过这一基准，我们对九种代表性的MLLM进行了评估，发现这些模型在处理视觉与常识知识冲突时存在显著的文本依赖性问题。  基于此发现，我们提出了一种新的提示策略——“聚焦于视觉”（FoV），旨在增强模型在遇到冲突时优先考虑视觉输入的能力，从而减少对矛盾文本信息的依赖。我们的分析结果以及提出的策略对理解并缓解MLLM中的视觉-知识冲突具有重要意义。  此外，本文还提供了数据集和代码的公开访问权限，以促进社区进一步的研究和应用。|
|**2024-10-10**|**DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory**|Yutong Wang et.al.|[2410.08143](http://arxiv.org/abs/2410.08143)|**[link](https://github.com/yutongwang1216/docmtagent)**|**在机器翻译领域，大型语言模型（LLMs）已经取得了相当可观的质量提升。然而，大多数当前的MT-LLM研究仍然面临在处理整个文档时保持翻译一致性与准确性的挑战。本文介绍了一种名为DelTA的文档级翻译代理，旨在克服这些局限性。DelTA具有一种多层次记忆结构，能够存储不同粒度和跨度的信息，包括专有名词记录、双语摘要、长期记忆和短期记忆，这些信息由辅助的LLM组件连续检索和更新。实验结果显示，在四个开源/闭源LLM和两个代表性文档翻译数据集上，DelTA在翻译一致性与质量方面均显著优于强大的基线，平均一致性得分提高高达4.58个百分点，COMET得分提高高达3.16点。DelTA采用逐句翻译策略，确保无句子遗漏，并提供与主流方法相比更为内存高效的选择。此外，DelTA提高了代词翻译准确性，并且代理的摘要组件也显示出作为基于查询的摘要任务工具的潜力。我们已将代码和数据发布在https://github.com/YutongWang1216/DocMTAgent。**|
|**2024-10-09**|**Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models**|Fei Wang et.al.|[2410.07176](http://arxiv.org/abs/2410.07176)|null|在探索如何通过联合分析来理解不完美检索对生成型问答（RAG）行为的影响，以及如何在LLM内部知识与外部来源之间产生潜在冲突时，我们发现，不完美的检索增强可能是不可避免的，并且会对RAG系统造成严重影响。通过在现实条件下的控制性分析，我们发现了从检索到的不完整知识与LLM内部知识之间的知识冲突是RAG后处理阶段需要克服的关键瓶颈。  为了使LLM在面对不完美检索时具有鲁棒性，我们提出了“精明RAG”这一新颖的RAG方法。该方法能够适当地激发LLM内部知识中的关键信息，通过源意识地整合内部和外部知识，最终根据信息可靠性确定答案。我们的实验结果使用了Gemini和Claude两个模型验证了“精明RAG”的有效性，证明其显著优于现有的增强RAG鲁棒性的方法。值得注意的是，在最坏情况场景下，“精明RAG”是唯一能够达到或超过没有RAG的LLM性能的方法。  进一步的分析表明，“精明RAG”有效地解决了知识冲突问题，提高了RAG系统的可靠性和可信度。|
|**2024-10-09**|**Do better language models have crisper vision?**|Jona Ruthardt et.al.|[2410.07173](http://arxiv.org/abs/2410.07173)|null|本文探讨了文本仅依赖型大型语言模型（LLMs）在理解视觉世界方面的表现。随着LLMs在计算机视觉领域的应用日益广泛，这一问题变得既基础又关键。现有研究主要集中在有限的场景上，如生成视觉内容或对多模态数据进行聚类。因此，我们提出了一项名为“视觉文本表示基准”（ViTeRB）的任务，旨在识别出能够与视觉世界高度一致的关键属性。基于此任务的结果，我们发现解码器型大语言模型在视觉为中心的语境下作为文本表示的理想候选，这与当前使用文本编码器的做法形成了对比。  在此基础上，我们提出了“ShareLock”——一种超轻量级的类似CLIP的模型。通过利用从强大视觉和语言模型预计算的冻结特征，ShareLock在ImageNet上取得了51%的准确率，仅使用了563,000张图像-描述对。此外，训练所需的资源仅为1个GPU小时（或包括特征预计算的10个小时），远少于以往方法所需的时间数量级。我们将提供该代码。|
|**2024-10-09**|**Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate**|Qidong Huang et.al.|[2410.07167](http://arxiv.org/abs/2410.07167)|**[link](https://github.com/shikiw/modality-integration-rate)**|**我们提出了一种有效的、稳健的且通用的指标——模态整合率(MIR)，用于衡量大型视觉语言模型(LVLMs)的多模态预训练质量。大规模预训练在构建具备强大能力的LVLMs中扮演着关键角色，而如何在昂贵的监督微调阶段之前评估其训练质量则是一个未充分探索的领域。对于大型语言模型(LLLs)，常用的预训练指标包括损失、困惑度以及上下文内评估结果，但我们观察到这些指标在对良好训练的LLMs与新模态进行对齐时并不具有很好的指示性。由于缺乏合适的指标，LVLMs在关键的预训练阶段的研究受到了极大的阻碍，包括训练数据选择、高效模块设计等。本文提出从跨模态分布距离的角度来评估预训练质量，并引入了模态整合率(MIR)，该指标具有以下特点：1）**有效**地代表预训练质量，并与经过监督微调后的基准性能呈现正相关；2）**稳健**于不同的训练/评估数据；3）**泛化**于多种训练配置和架构选择。我们进行了一系列预训练实验以探索MIR的有效性，并观察到令人满意的结果，即MIR能够指示训练数据选择、训练策略调度以及模型架构设计以获得更好的预训练结果。我们希望MIR能够成为构建具备强大能力的LVLMs的有用指标，并激发不同领域关于模态对齐的后续研究。我们的代码已开源在：https://github.com/shikiw/Modality-Integration-Rate。**|
|**2024-10-09**|**Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making**|Manling Li et.al.|[2410.07166](http://arxiv.org/abs/2410.07166)|**[link](https://github.com/embodied-agent-interface/embodied-agent-interface)**|**为了系统地评估大型语言模型（LLMs）在实体化决策中的表现，虽然已有大量研究利用LLMs处理实体化环境中的决策问题，但我们仍缺乏对其性能的全面理解。现有工作通常在不同领域、针对不同目的、基于不同输入和输出构建LLMs，这使得难以统一评价它们。现有评估方法往往仅依赖最终的成功率，这使得难以识别LLMs缺失的能力以及问题所在，进而阻碍了实体化智能体有效且选择性地利用LLMs。  为此，我们提出了一种通用接口（实体化智能体接口），旨在支持各种任务类型与LLM模块输入-输出规范的统一化。具体而言，该接口允许：  1. 统一多种涉及状态与时间延伸目标的实体化决策任务。 2. 统一四种常用的用于决策的LLM模块：目标解释、子目标分解、动作序列规划和过渡建模。 3. 提供一系列精细粒度的度量标准，将评估细分为各种错误类型，如幻觉错误、可用性错误、不同类型规划错误等。  整体而言，我们的基准提供了对LLMs在不同子任务上的全面评估，揭示了LLM驱动的实体化人工智能系统的强项与弱点，并为有效和选择性地利用LLMs在实体化决策中提供了见解。**|
|**2024-10-09**|**Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning**|Chongyu Fan et.al.|[2410.07163](http://arxiv.org/abs/2410.07163)|**[link](https://github.com/OPTML-Group/Unlearn-Simple)**|本文旨在解决大型语言模型（LLM）的去学习问题，即在不重新从头训练的情况下，消除不需要的数据影响以及相关模型能力（如版权数据或有害内容生成），同时保留必要的模型功能。尽管对LLM去学习的需求日益增长，但尚未形成一种原理性的优化框架。  为此，我们回顾了当前最先进的方法——负偏好优化（NPO），并发现了参考模型偏见的问题，这可能削弱NPO的有效性，特别是在去学习不同难度数据时。鉴于此，我们提出了一种简单而有效的去学习优化框架——SimNPO，表明通过简单的偏好优化减少对参考模型的依赖（从简化视角来看）有助于去学习过程。此外，我们还提供了深入的SimNPO优势分析，通过混合马尔可夫链的分析方法支持这一观点。  我们通过在TOFU和MUSE等基准测试中的大量实验验证了SimNPO相对于现有去学习基线的优越性，并展示了其对重新学习攻击的鲁棒性。所有代码均可在GitHub上的https://github.com/OPTML-Group/Unlearn-Simple获取。|
|**2024-10-09**|**Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis**|Bohan Zeng et.al.|[2410.07155](http://arxiv.org/abs/2410.07155)|**[link](https://github.com/yangling0818/trans4d)**|**近期在扩散模型领域的进展展示了其在图像和视频生成方面的卓越能力，进一步提升了4D合成的有效性。现有的4D生成方法能够根据用户友好的条件生成高质量的4D对象或场景，对游戏和视频行业大有裨益。然而，这些方法在合成复杂4D过渡和场景内对象交互的显著变形方面仍存在挑战。为解决这一问题，我们提出了一种名为Trans4D的创新文本到4D合成框架，旨在实现真实可信的场景级复杂过渡。具体而言，我们首先利用多模态大型语言模型（MLLMs）生成物理意识的场景描述以进行4D场景初始化以及有效过渡时间规划。随后，我们提出了一种几何感知的4D过渡网络，基于计划实现复杂的场景级4D过渡，涉及表现力强的对象几何变形。广泛实验结果表明，Trans4D在生成具有准确性和高质量过渡的4D场景方面始终超越现有最先进的方法，验证了其有效性。代码：https://github.com/YangLing0818/Trans4D**|
|**2024-10-09**|**Mental Disorders Detection in the Era of Large Language Models**|Gleb Kuzmin et.al.|[2410.07129](http://arxiv.org/abs/2410.07129)|null|本文比较了传统机器学习方法、编码器基模型以及大型语言模型（LLM）在抑郁症和焦虑症检测任务上的效果。考虑了五个不同格式的数据库，每个数据库都采用了不同的方法来定义目标病理学类别。我们测试了基于语言特征的AutoML模型、多种变体的Transformer编码器，如BERT，以及最先进的LLM作为病理分类模型。结果表明，LLM在噪声大且训练样本在文本长度和类型上差异显著的小数据集上表现出色。然而，当在确诊为抑郁症个体的文本上进行训练时，语言模型的性能优于传统的心理语言学特征和编码器基模型，这凸显了它们在特定临床应用中的潜力。|
|**2024-10-09**|**Personalized Visual Instruction Tuning**|Renjie Pi et.al.|[2410.07113](http://arxiv.org/abs/2410.07113)|**[link](https://github.com/sterzhang/pvit)**|近期，多模态大型语言模型（MLLMs）的进展展现了显著的进步，然而，这些模型存在一个明显的局限性——“面部盲症”。具体来说，它们能够进行一般性的对话，但却无法针对特定个体进行个性化对话。这一缺陷阻碍了MLLMs在个性化场景中的应用，如定制化的移动设备视觉助手或需要识别家庭成员的家用机器人。为此，本文提出了一种名为个性化视觉指令调整（PVIT）的新颖数据整理与训练框架，旨在使MLLMs能够识别图像中的目标个体，并展开个性化且连贯的对话。我们的方法涉及开发一个复杂的管道，该管道能够自主生成包含个性化对话的训练数据。这个管道利用了各种视觉专家、图像生成模型和（多模态）大型语言模型的能力。为了评估MLLMs的个性化潜力，我们设计了一个名为P-Bench的基准测试，其中包括不同难度级别的多种问题类型。实验结果表明，在使用我们整理的数据集进行微调后，个性化性能得到了显著提升。|
|**2024-10-09**|**I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy**|Gian Maria Campedelli et.al.|[2410.07109](http://arxiv.org/abs/2410.07109)|**[link](https://github.com/mobs-fbk/llm_interaction_simulator)**|随着大型语言模型（LLM）驱动的智能体变得越来越自主，并且在彼此间自由互动时，研究它们之间的交互模式变得至关重要。这有助于我们预见可能产生的新现象以及潜在风险。本文受斯坦福监狱实验启发，专注于研究具有严格社会等级背景的多智能体环境中的LLM交互模式。  研究聚焦于两类主要现象：说服力和反社会行为，在涉及看守和试图达成特定目标（如获得额外的户外活动时间或逃狱）的囚犯智能体之间的模拟场景中进行探讨。通过使用200个实验场景，共计2000次机器间的对话，研究了五种流行的LLM，获得了以下显著发现：  1. 一些模型在多智能体设置中持续失败，无法进行有意义的对话。 2. 对于能够成功互动的模型，目标对智能体的说服力有显著影响，而对反社会行为的影响则微乎其微。 3. 智能体的角色，特别是看守的人格特质，对囚犯的说服成功几率和反社会行为的出现有着直接推动作用。 4. 即使没有明确提示特定的人格特质，仅通过赋予角色，也观察到了反社会行为的自然产生。  这些结果对LLM交互智能体的发展以及对其社会影响的讨论具有重要启示。|
|**2024-10-09**|**Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context**|Sangwon Yu et.al.|[2410.07103](http://arxiv.org/abs/2410.07103)|null|在多跳推理领域，大型语言模型（LLM）面临着基于给定上下文内的支持文档进行多步骤推理的挑战。LLM往往难以筛选出不相关的文档，并且其性能对上下文中支持文档的位置非常敏感。在这篇论文中，我们识别出了一个额外的挑战：LLM的性能也对呈现支持文档的顺序非常敏感。我们将此问题称为“错序上下文问题”。为了应对这一问题，我们提出了一种简单而有效的解决方法——上下文重复（CoRe），该方法通过多次提示模型以确保支持文档以最佳顺序呈现来解决这个问题。  通过应用CoRe，我们在多跳问答任务上的F1得分提高了高达30%，在合成任务上的准确率提高了高达70%。此外，CoRe有助于缓解LLM普遍存在的“中间迷失”问题，并可以与利用链式思考（CoT）推理的检索方法有效结合使用。|
|**2024-10-07**|**Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models**|Fei Wang et.al.|[2410.05269](http://arxiv.org/abs/2410.05269)|null|大型语言模型（LLM）中的数据是关键要素。近期研究探索了利用LLM进行高效数据收集的方法。然而，由LLM生成的数据往往存在质量参差不齐、某些方面被低估或缺失以及数据点质量低下的问题。为了应对这些问题，我们提出了一种名为“数据顾问”的增强型LLM数据生成方法，该方法能够考虑目标数据集的特性，从预定义的原则出发，监控生成数据的状态，识别当前数据集的弱点，并据此指导数据生成的下一轮迭代。数据顾问可以轻松地集成到现有的数据生成方法中，以提高数据质量和覆盖面。  在对三个代表性LLM（即Mistral、Llama2和Falcon）的安全对齐进行的实验中，数据顾问证明了其在不牺牲模型实用性的情况下，有效提升模型对各种精细粒度安全问题的适应性的能力。|
|**2024-10-07**|**PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs**|Mengzhao Chen et.al.|[2410.05265](http://arxiv.org/abs/2410.05265)|**[link](https://github.com/chenmnz/prefixquant)**|**量化对于大规模语言模型（LLMs）的部署至关重要，它能显著提升内存效率与推理速度。现有的激活量化方法主要针对通道级异常值进行处理，往往忽略了令牌级的异常值，这导致了对成本高昂的逐令牌动态量化依赖。本文提出了一种名为PrefixQuant的新颖技术，该技术在不重新训练的情况下离线识别出高频异常令牌，并将其作为前缀放入KV缓存中，以防止推理过程中生成异常令牌，并简化了量化过程。据我们所知，PrefixQuant是首个能够实现高效逐张量静态量化并超越昂贵的逐令牌动态量化的方法。例如，在W4A4KV4（权重4位、激活4位、KV缓存4位）的Llama-3-8B模型中，使用PrefixQuant和逐张量静态量化后，WikiText2的困惑度降低了7.43个点，平均准确率在5个常识推理任务上提高了71.08%，相较于之前的逐令牌动态量化方法QuaRot，分别在困惑度上提升了0.98个点，在准确率上提升了5.98个点。此外，使用PrefixQuant量化后的模型的推理速度相较于FP16模型提升了1.60倍到2.81倍，且超过了QuaRot模型1.2倍到1.3倍。我们的代码已开源于\url{https://github.com/ChenMnZ/PrefixQuant}。**|
|**2024-10-07**|**TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles**|Qingchen Yu et.al.|[2410.05262](http://arxiv.org/abs/2410.05262)|**[link](https://github.com/mazzzystar/TurtleBench)**|**随着大型语言模型（LLM）的应用范围不断扩大，对可靠评估的需求也在增加。现有的LLM评估基准主要依赖静态数据集，这使得评估模型在与用户动态交互时的表现变得具有挑战性。此外，这些基准往往需要特定背景知识，从而复杂化了衡量模型逻辑推理能力的测量。基于强大模型或人工努力的其他动态评估方法可能会引入偏见，并且成本和时间需求高，这阻碍了大规模应用。  为了解决这些问题，我们提出了TurtleBench。TurtleBench从我们开发的在线Turtle Soup Puzzle平台收集真实的用户猜测，这种方法允许生成相对动态的评估数据集，可以降低模型作弊的风险，同时使评估更贴近实际用户的推理需求，从而提高评估的可靠性。TurtleBench包含了1,532个用户猜测及其正确性的注释信息。利用这个数据集，我们全面评估了当前最先进的九个LLM模型。值得注意的是，OpenAI o1系列模型在这些评估中并未取得领先地位。  我们提出了一些进一步研究的假设，例如“o1的潜在推理使用了简单的链式思考（CoT）技术”和“增加CoT长度不仅提供了推理益处，同时也带来了噪音成本”。**|
|**2024-10-07**|**Differential Transformer**|Tianzhu Ye et.al.|[2410.05258](http://arxiv.org/abs/2410.05258)|**[link](https://github.com/microsoft/unilm/blob/master/Diff-Transformer/)**|在本文中，我们引入了差异变换器（Diff Transformer），它能够增强对相关上下文的注意力同时消除噪音。具体来说，差异注意力机制通过计算两个独立的softmax注意力映射之间的差值来确定注意力分数。这种减法操作可以消除噪音并促进稀疏注意力模式的产生。在语言建模任务上的实验结果表明，与标准的变换器相比，差异变换器在模型大小和训练样本量的扩展上均表现出色。更令人兴奋的是，在实际应用中，如长上下文建模、关键信息检索、幻觉抑制、上下文内学习以及激活异常减少等方面，差异变换器都展现出显著优势。由于对无关上下文的关注较少，差异变换器能够有效缓解问答和文本摘要中的幻觉问题。在上下文内学习方面，差异变换器不仅提高了准确率，而且对于顺序排列更为鲁棒，这被认为是长期的稳健性问题。这些结果确立了差异变换器作为推动大型语言模型发展的高效且有前景架构的地位。|
|**2024-10-07**|**GLEE: A Unified Framework and Benchmark for Language-based Economic Environments**|Eilam Shapira et.al.|[2410.05254](http://arxiv.org/abs/2410.05254)|**[link](https://github.com/eilamshapira/GLEE)**|**大型语言模型（LLMs）在经济与战略互动领域展现出巨大潜力，因为这些领域通常以自然语言沟通为主。这引发了一系列关键问题：LLMs是否表现出理性行为？它们能否模仿人类行为？它们是否倾向于达到高效和公平的结果？自然语言在策略互动中的角色是什么？经济环境的特性如何影响这些动态？这些问题对于将基于LLM的代理集成到现实世界的数据驱动系统（如在线零售平台和推荐系统）中时的经济和社会影响至关重要。尽管机器学习社区一直在探索LLMs在多代理设置中的潜力，但不同研究之间的假设、设计选择和评估标准差异使得很难得出稳健且有意义的结论。为解决这一问题，我们提出了一种标准化研究基于双人、序列、语言驱动游戏的标准框架。受经济学文献启发，我们定义了三个基本游戏家族，具有一致的参数化、自由度和用于评估代理性能（自我收益）以及游戏结果（效率和公平性）的经济指标。  我们开发了一个开源框架来模拟交互和分析，并利用它收集了LMM对LMM交互的大量数据集以及额外的人类对LMM交互数据集。通过广泛的实验，我们展示了我们的框架和数据集如何被用来：  (i) 比较基于LLM的代理与人类玩家在各种经济背景下的行为； (ii) 从个体和集体层面评估代理的性能； (iii) 定量分析经济环境特性对代理行为的影响。**|
|**2024-10-07**|**Causal Micro-Narratives**|Mourad Heddaya et.al.|[2410.05252](http://arxiv.org/abs/2410.05252)|null|我们提出了一种新颖的方法来对文本中的因果微叙事进行分类。这些叙事是关于目标主体的因果解释的句子级描述。该方法仅需要针对特定主题的因果和效果的本体，我们通过应用到通货膨胀叙事中进行了示范。利用覆盖美国历史和当代新闻文章的人工标注数据集进行训练，我们在多标签分类任务上评估了几种大型语言模型（LLMs）。表现最好的模型——微调后的Llama 3.1 8B，在叙事检测上达到F1得分为0.87，在叙事分类上达到F1得分为0.71。全面的错误分析揭示了语义歧义带来的挑战，并指出模型错误往往反映了人工注释者的分歧。这项研究建立了一个从实际数据中提取因果微叙事的框架，具有广泛的社会科学研究应用前景。|
|**2024-10-07**|**SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe**|Yuxin Xiao et.al.|[2410.05248](http://arxiv.org/abs/2410.05248)|null|为了在交互驱动任务中诱导大型语言模型（LLM）展现出期望的行为，通常采用指令-调优阶段，通过下一个词预测（NTP）损失训练LLM于指令响应对。先前的工作旨在提升调优性能，常着重于高质量的监督微调（SFT）数据集的构建，这通常需要昂贵的数据过滤过程或人力密集型的人工注释。然而，这些方法并未充分利用数据集的内在特性，导致了高昂的计算和劳动成本，限制了可扩展性和性能提升。本文提出了一种名为SFTMix的新颖方法，它超越了传统NTP范式，无需精心设计的SFT数据集即可提升调优性能。  观察到LLM在语义表示空间中表现出不均匀的置信度分布，我们提出，不同置信度级别的示例在调优过程中应扮演不同的角色。基于这一见解，SFTMix利用训练动态来识别具有不同置信度级别的示例，然后应用基于Mixup的正则化来减少对高置信度示例的过拟合，同时传播监督信号以改善相对低置信度示例的学习效果。这种方法使得SFTMix能够在广泛的操作指令遵循和医疗保健领域的特定SFT任务中显著超越NTP，证明了其对不同LLM家族和任意大小数据集的适应性和可扩展性。全面的消融研究进一步验证了SFTMix设计选择的稳健性，强调了其在不同LLM和数据集上的一致性能提升能力，适用于更广泛的自然语言处理应用。|
|**2024-10-07**|**Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents**|Boyu Gou et.al.|[2410.05243](http://arxiv.org/abs/2410.05243)|**[link](https://github.com/OSU-NLP-Group/UGround)**|本论文探讨了多模态大型语言模型（MLLMs）如何重塑图形用户界面（GUI）代理的能力，使其从受控模拟向跨平台的复杂现实世界应用过渡。然而，这些代理的有效性在很大程度上取决于其固有性的稳健性。当前的GUI代理主要依赖于基于文本的表示，如HTML或可访问性树，尽管它们具有实用性，但往往引入噪声、不完整性以及增加计算开销。  我们的观点是，为GUI代理构建一种类似人类的体现，能够完全通过视觉感知环境，并直接对GUI执行像素级操作。关键在于视觉定位模型，它们能够准确地将GUI元素的各种引用表达映射到其在不同平台上的GUI坐标上。我们表明，一个简单的配方——包括基于网络的合成数据和对LLaVA架构的轻微调整——对于训练这样的视觉定位模型是出奇有效的。  我们收集了迄今为止最大的GUI视觉定位数据集，包含10M个GUI元素及其引用表达，覆盖了1.3M张截图，以此来训练UGround，这是用于GUI代理的强大通用视觉定位模型。在六个跨三个类别（定位、离线代理和在线代理）的基准测试上，实验结果显示出以下两点：  1）UGround显著优于现有GUI代理的视觉定位模型，绝对性能提升高达20%。 2）使用UGround的代理在性能上超越了最先进的代理，尽管现有的代理使用额外的基于文本的输入，而我们的代理仅依赖于视觉感知。  这些结果强有力地支持了这样一种设想：即像人类一样在数字世界中导航的GUI代理是可行的，并且充满了潜力。|
|**2024-10-07**|**GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models**|Iman Mirzadeh et.al.|[2410.05229](http://arxiv.org/abs/2410.05229)|null|大型语言模型（LLM）的最新进展引发了对它们在数学推理能力上的关注，特别是针对小学水平问题。GSM8K基准测试广泛用于评估模型在这一领域的表现。尽管LLM在GSM8K上的成绩近年来显著提高，但其数学推理能力是否真正有所提升仍然存在疑问，这使得现有评估指标的可靠性受到质疑。为了应对这些问题，我们进行了一项大规模研究，涵盖了当前最前沿的开放和封闭模型。为了克服现有评估方法的局限性，我们引入了GSM-Symbolic改进版基准，该基准基于符号模板生成了多样化的题目。GSM-Symbolic使得评估更加可控，提供了关键洞察和更可靠的指标来衡量模型的推理能力。  我们的发现揭示了LLM在回答不同版本同题时表现出明显的差异性。具体而言，在GSM-Symbolic基准中，仅改变问题中的数值后，所有模型的表现都会下降。此外，我们研究了这些模型在数学推理方面的脆弱性，并表明随着问题中条目数量的增加，其性能会显著降低。我们推测，这是因为当前的LLM无法执行真正的逻辑推理；它们只是复制了训练数据中的推理步骤。即使添加一个看似与问题相关的单个条目，所有最先进的模型的表现也会大幅下降（高达65%），尽管这个条目实际上并不贡献于完成答案所需的关键推理链。总之，我们的工作为理解LLM在数学推理上的能力和局限性提供了一个更为细致的视角。|
|**2024-10-07**|**Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates**|Avanika Narayan et.al.|[2410.05224](http://arxiv.org/abs/2410.05224)|null|本文介绍了一种名为Cookbook的框架，该框架通过编程方式生成训练数据，数据主要由随机标记的简单模式组成。这种方法在规模和成本方面具有优势，且避免了与人类或大型语言模型（LLM）生成数据相关的法律和隐私问题。首先，Cookbook利用数据生成Python函数模板来产生鼓励模型学习与特定任务相匹配的显式规则的训练数据。研究发现，使用Cookbook生成的数据进行微调能够显著提高模型在对应任务上的表现，最高可达52.7个准确性点。其次，由于指令数据集能够同时改善多个下游任务的表现，Cookbook算法自动学习如何混合来自不同模板的数据以优化多个任务的性能。在标准的多任务GPT4ALL评估套件上，使用Cookbook生成的数据集进行微调的Mistral-7B模型在平均准确性和三个任务中的三个上均取得最佳成绩。最后，分析了Cookbook为何能提高性能以及其背后的原理，并提出了一项指标来验证改进的主要原因是模型生成的结果更好地遵循了模板规则。|
|**2024-10-04**|**Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models**|Zhuochun Li et.al.|[2410.03663](http://arxiv.org/abs/2410.03663)|null|本文介绍了一种名为“Mistake-Aware Peer-Review Distillation”（MAPD）的创新方法。该方法旨在通过改进开源小型模型的知识提炼（KD）过程来提高它们的性能，这些过程通常依赖于大型商业语言模型作为教师。与以往研究仅使用单一教师生成的黄金理据进行训练不同，MAPD方法采取了更为细致的策略：  1. **个性化错误反馈**：MAPD不仅要求教师提供学生答案的正确理据，更进一步地，它让教师指出学生的错误并解释原因，从而生成定制化的教学数据。  2. **模拟同行评审**：通过设计一个教师间的模拟同行评审过程，MAPD筛选出那些达到一定接受标准的生成理据。这一机制减少了教师因猜测而给出错误理据的可能性，从而提高了教学数据的质量。  本文在数学、常识和逻辑推理任务上进行了全面的实验和分析，验证了MAPD方法的有效性。|
|**2024-10-04**|**RAFT: Realistic Attacks to Fool Text Detectors**|James Wang et.al.|[2410.03658](http://arxiv.org/abs/2410.03658)|**[link](https://github.com/jameslwang/raft)**|本文提出了一种针对现有大型语言模型检测器的语法无误的黑盒攻击方法，称为RAFT。与之前针对语言模型的攻击不同，RAFT方法利用了词级上的LLM嵌入的可迁移性，同时保持原始文本质量不变。通过利用辅助嵌入，RAFT贪婪地选择需要扰动的目标单词，以对抗特定的检测器。实验结果表明，RAFT攻击能够有效地使所有研究中的检测器在各种领域中失效高达99%，并且具有跨源模型的可移植性。手动的人类评估研究表明，RAFT生成的攻击实例既真实又难以与原创人类编写文本区分开来。此外，我们还展示了RAFT生成的例子可以用来训练鲁棒性更强的检测器。我们的工作揭示了当前的LLM检测器并非具有鲁棒性，强调了迫切需要更强大的检测机制的必要性。|
|**2024-10-04**|**Aligning LLMs with Individual Preferences via Interaction**|Shujin Wu et.al.|[2410.03642](http://arxiv.org/abs/2410.03642)|**[link](https://github.com/shujinwu-0814/aloe)**|**随着大型语言模型（LLMs）展现出日益先进的能力，确保它们的行为与人类价值观和偏好保持一致对于广泛采用这些模型变得至关重要。尽管先前的研究主要集中在遵循诸如帮助性、无害性和诚实性等一般原则上，但忽视了考虑到个人和多样性偏好的需求，这可能削弱了个性化的人类体验。为了填补这一空白，我们训练了一种能够“交互以对齐”的LLMs，即让LLMs发展出一种隐式推断当前用户未明确表达的个性化偏好的元技能，并据此动态调整后续行为和响应以适应这些推断的偏好。我们的方法包括建立一个由3,310个不同用户人设组成的多样化池，通过初始示例创建，然后通过迭代自我生成和筛选进行扩展。在不同用户人设的指导下，我们利用多LLM协作开发了一个包含3K+多轮对话的树形结构多轮偏好数据集。最后，我们使用监督微调和强化学习对数据集进行了增强，以提高LLMs的能力。为了评估，我们建立了ALOE（ALign With CustOmized PrEferences）基准，包含100个精心挑选的例子以及用于衡量对话中个性化对齐性能的适当度量标准。实验结果表明，我们的方法在通过互动实现动态、个性化的对齐方面非常有效。**|
|**2024-10-04**|**Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation**|Jie Xiao et.al.|[2410.03613](http://arxiv.org/abs/2410.03613)|null|随着大型语言模型（LLM）在我们工作和日常生活的各个方面日益普及，对用户隐私的关注推动了这些模型本地部署的趋势。存在一些轻量级LLM（例如Gemini Nano，LLAMA2 7B），它们可以在智能手机上本地运行，为用户提供对其个人数据的更大控制权。作为一项迅速发展的应用，我们关注它们在商用移动设备上的性能。  为了全面了解LLM在移动平台上的部署现状，我们进行了一项全面的测量研究。我们评估了影响用户体验的指标，包括令牌吞吐量、延迟和电池消耗，以及对开发者至关重要的因素，如资源利用、动态电压频率缩放策略和推理引擎。此外，我们详细分析了硬件能力和系统动力学如何影响本地设备上的LLM性能，这可能有助于开发者识别并解决移动LLM应用程序中的瓶颈。我们还提供了针对主要供应商的移动系统级芯片（SoC）的全面比较，突出了它们在处理LLM工作负载时的性能差异。我们希望这项研究能够为本地设备LLM的开发和未来移动系统架构的设计提供洞察。|
|**2024-10-04**|**TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation**|Jonathan Cook et.al.|[2410.03608](http://arxiv.org/abs/2410.03608)|null|在大型语言模型（LLM）的广泛应用背景下，构建灵活且可解释的评估其遵循指令能力的方法至关重要。目前，偏好判断成为了评估标准的默认选择，尽管这种做法简化了复杂、多维偏好的提炼，将其归结为单一排名。然而，随着人工注释的缓慢和成本高昂，LLM被越来越多地用于做出这些判断，这牺牲了可靠性和可解释性。为此，我们提出了TICK（针对特定指令的结构化评估与核查清单），这是一种全自动化、可解释的评估方案，通过LLM生成的、针对指令的核查清单结构化评估。  首先，我们展示了，在给定指令的情况下，LLM能够可靠地产生高质量、定制化的评估核查清单，将指令分解为一系列是/否问题。每个问题询问候选回应是否满足指令的具体要求。我们证明使用TICK能够显著提高LLM判断与人类偏好之间精确一致性的频率，相比直接由LLM评分输出，这一比例从46.4%提升至52.2%。  接着，我们展示了STICK（自我TICK）可以利用自我细化和最佳中的N选择来改善多个基准的生成质量。对LiveBench推理任务进行STICK自我细化，实现了绝对增益+7.8%，而使用STICK进行最佳中的N选择在真实世界指令数据集WildBench上获得了+6.3%的绝对改进。这表明，结构化的、多维度的自我改进是进一步提升LLM能力的一个有前景的方向。  最后，通过向直接为WildBench指令评估LLM响应的人类评估者提供LLM生成的核查清单，我们显著提高了评估者之间的共识度（从0.194提升至0.256）。|
|**2024-10-04**|**Efficiently Identifying Watermarked Segments in Mixed-Source Texts**|Xuandong Zhao et.al.|[2410.03600](http://arxiv.org/abs/2410.03600)|null|文本水印在大型语言模型（LLM）中的应用日益增长，用于检测合成文本，以缓解虚假新闻和学术不诚实等滥用情况。现有水印检测技术主要关注于对整个文档进行分类，判断其是否被水印标记，但往往忽略了在更长的混合来源文档中识别单独水印段落的常见场景。受到抄袭检测系统的启发，我们提出了两种新型方法进行部分水印检测。首先，我们开发了一种几何覆盖检测框架，旨在确定长文本中是否存在水印段落。其次，我们引入了一个自适应在线学习算法，以准确定位文本中的水印段落位置。在三种流行的水印技术（KGW-Watermark、Unigram-Watermark 和 Gumbel-Watermark）上进行了评估，我们的方法取得了高精度，并显著优于基线方法。此外，我们的框架具有适应其他水印技术的能力，提供了精确水印检测的新见解。|
|**2024-10-04**|**Understanding Reasoning in Chain-of-Thought from the Hopfieldian View**|Lijie Hu et.al.|[2410.03595](http://arxiv.org/abs/2410.03595)|null|大型语言模型在各类任务中展现出非凡能力，链式思考（Chain-of-Thought, CoT）提示作为一种提升推理能力的关键技术逐渐受到关注。然而，现有研究主要集中在提高性能方面，缺乏对CoT成功背后根本因素的全面解释框架。为了填补这一空白，我们提出了一种基于认知神经科学中的霍普菲尔德认知观的新视角。我们建立了一个链接CoT推理与刺激、动作、神经群体和表示空间等关键认知元素之间的关系框架。从这一视角出发，我们可以理解推理过程实质上是这些表示空间之间的移动。  基于此洞察，我们开发了一种方法来定位CoT响应中的推理错误。此外，我们提出了一个名为“思考的表示”（Representation-of-Thought, RoT）的框架，利用低维表示空间的鲁棒性来增强CoT推理过程的鲁棒性和可解释性，并提供了对推理过程进行精细控制的能力。实验结果表明，RoT不仅提高了CoT推理的鲁棒性和可解释性，而且提供了对推理过程进行精细化控制的可能性。|
|**2024-10-04**|**Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models**|Xin Zou et.al.|[2410.03577](http://arxiv.org/abs/2410.03577)|**[link](https://github.com/1zhou-Wang/MemVR)**|尽管大型多模态语言模型（MLLMs）具有令人印象深刻的性能，但它们容易出现幻觉，特别是在视觉输入中不存在关键细节时，会夸张地编造内容。为了解决这一挑战，我们遵循了人类认知过程中的一个常见步骤——当对现场关键细节的记忆逐渐模糊时，直观的做法是再次查看这些细节以寻求准确和真实的信息。因此，我们引入了一种名为“记忆空间视觉重读”（MemVR）的新型幻觉缓解范式，它无需外部知识检索或额外的微调。具体而言，我们将视觉提示作为补充证据，通过前馈网络（FFN）注入到MLLMs中作为键值记忆，当模型对问题相关的视觉记忆不确定甚至遗忘时。全面的实验评估表明，MemVR在各种MLLMs上显著缓解了幻觉问题，并且在不增加时间开销的情况下，在通用基准测试中表现出色，从而突显出其广泛适用性的潜力。|
|**2024-10-04**|**Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs)**|Abrar Rahman et.al.|[2410.03568](http://arxiv.org/abs/2410.03568)|null|本文对当前顶级大型语言模型（LLMs）采用的分词技术进行了全面研究，并探讨了这些技术在不同语言尤其是资源匮乏语言服务成本与可用性方面的潜在影响。研究考虑了多种LLMs，包括使用cl100k_base嵌入的GPT-4、使用p50k_base嵌入的GPT-3以及使用r50k_base嵌入的DaVinci，同时对比了广泛使用的BERT基础分词器。研究分析了这些模型之间的分词差异，并深入探究了子词分词在语言表示上的挑战。  研究强调了培养语言意识开发实践的重要性，特别是针对那些传统上资源不足的语言。此外，本文还通过案例研究展示了分词选择在实际应用中的影响，特别是在电子健康记录（EHR）系统中的应用。研究旨在促进AI服务领域，特别是跨语言环境中的通用化国际化（I18N）实践，特别关注被现有AI应用严重忽视的语言的包容性发展。|
|**2024-10-04**|**Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding**|Wei Wu et.al.|[2410.03553](http://arxiv.org/abs/2410.03553)|null|蛋白质作为生物分子的核心，在生物过程中扮演着关键角色，包括代谢反应和DNA复制。准确预测它们的性质和功能对生物应用至关重要。最近开发的蛋白质语言模型（pLMs）通过监督微调提供了解决问题的有希望的方法。然而，微调的模型仅针对特定下游预测任务进行定制，实现通用的蛋白质理解仍然是一个挑战。为此，我们引入了结构增强的蛋白质指令调谐（SEPIT）框架来填补这一空白。我们的方法在pLMs中集成了一个新颖的结构感知模块，以提供有关结构的知识，并将这些增强的pLMs与大型语言模型（LLMs）连接起来，以生成蛋白质的理解。在这个框架中，我们提出了一个新颖的两阶段指令调谐管道，首先通过基于图标的指令建立蛋白质的基本理解，然后使用专家混合（MoEs）学习更复杂属性和功能信息，同时保持激活参数的数量相同。此外，我们构建了迄今为止最大的最全面的蛋白质指令数据集，这使我们能够训练和评估通用的蛋白质理解模型。广泛的经验结果在开放式生成和封闭集合答案任务上显示了SEPIT相对于闭源通用LLM和使用蛋白质知识训练的开源LLM的优越性能。|
|**2024-10-03**|**FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models**|Zhipei Xu et.al.|[2410.02761](http://arxiv.org/abs/2410.02761)|**[link](https://github.com/zhipeixu/fakeshield)**|生成式AI的快速发展犹如一把双刃剑，既促进了内容创作，也使得图像编辑和难以辨识变得更加便捷。当前的图像伪造检测与定位（IFDL）方法虽然在一定程度上有效，但仍然面临两个主要挑战：1）黑盒性质，即无法知晓其检测原理；2）对不同伪造技术（如Photoshop、DeepFake、AIGC-Editing等）的泛化能力有限。为了应对这些问题，我们提出了可解释的IFDL任务，并设计了具有多模态能力的框架——FakeShield。该框架旨在评估图像的真实性，生成篡改区域的掩模，并基于像素级和图像级的篡改线索提供判断依据。此外，我们利用GPT-4o增强了现有的IFDL数据集，创建了多模态篡改描述数据集（MMTD-Set），用于训练FakeShield的篡改分析能力。同时，我们引入了域标签引导的可解释伪造检测模块（DTE-FDM）和多模态伪造定位模块（MFLM），以应对各种伪造检测解释和实现由详细文本描述指导的伪造定位。  通过广泛的实验验证，FakeShield有效地检测和定位了各种篡改技术，提供了比以往IFDL方法更可解释且性能更优的解决方案。|
|**2024-10-03**|**Loong: Generating Minute-level Long Videos with Autoregressive Language Models**|Yuqing Wang et.al.|[2410.02757](http://arxiv.org/abs/2410.02757)|null|在生成时长达到数分钟的丰富内容视频方面，尽管具有挑战性但前景广阔。自回归大型语言模型（LLMs）在自然语言处理领域生成连贯且长度较长的令牌序列方面取得了巨大成功，而在探索使用自回归LLMs进行视频生成时，主要局限于生成几秒钟的短视频。本文对阻止基于自回归LLM的视频生成器生成长时间视频的挑战进行了深入分析。基于观察和分析结果，我们提出了一种新的基于自回归LLM的视频生成器“Loong”，能够生成长达数分钟的视频。具体而言，我们将文本令牌和视频令牌统一为自回归LLM可以进行自回归建模的序列，并从零开始训练模型。我们提出了渐进式短至长训练和损失重新加权方案，以缓解长期视频训练中的损失不平衡问题。此外，我们还研究了推理策略，包括视频令牌重编码和采样策略，以减少推理过程中累积的误差。我们的提出的“Loong”可以从10秒的视频进行训练，并扩展到根据文本提示生成数分钟级别的长视频，如结果所示。更多示例请访问：https://epiphqny.github.io/Loong-video。|
|**2024-10-03**|**SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at 1% the Cost**|Jifan Zhang et.al.|[2410.02755](http://arxiv.org/abs/2410.02755)|null|本文提出了一种名为SIEVE的轻量级替代方案，该方案在成本仅为GPT-4o单次过滤调用的十分之一的情况下，仍能与GPT-4o的准确性相匹配。SIEVE的核心在于将GPT-4o和轻量级T5模型无缝集成，并使用主动学习方法在少量GPT-4o调用的支持下对T5进行微调。一旦训练完成，SIEVE的表现与GPT-4o相当，但成本却低得多（仅为现有技术的1%）。我们在OpenWebText数据集上进行了实验，针对高质量和领域特定内容的五个高度定制化的过滤任务验证了SIEVE的有效性和效率。  进一步验证SIEVE的效果显示，SIEVE和GPT-4o在准确性方面达到相似水平，而人类评估者更倾向于SIEVE的过滤结果而非GPT-4o的结果。|
|**2024-10-03**|**Training Language Models on Synthetic Edit Sequences Improves Code Synthesis**|Ulyana Piterbarg et.al.|[2410.02749](http://arxiv.org/abs/2410.02749)|**[link](https://github.com/upiterbarg/lintseq)**|本文开发了一种名为LintSeq的合成数据生成算法。该算法通过使用代码检查器来程序化地在不引入错误的情况下随机选取插入操作序列，从而对现有代码进行重构，生成一系列代码编辑序列。这些序列以连续的程序差异形式输出。  为了测试LintSeq，我们将其应用于将指令+程序对重新格式化为指令+程序差异序列对的代码库。然后，我们对参数从2.6B到14B的多个较小的语言模型进行了基于指令的微调，比较了在原始版本和重新格式化版本数据集上的零次射击性能在代码合成基准上的表现。结果显示，在多次采样期间，经过代码差异微调的模型产生的程序多样性高于基线。这导致了在给定尝试次数“k”时，针对基准覆盖率的推理时间扩展性更好，即解决任何问题的概率“pass@k”。例如，在HumanEval pass@50上，较小模型在经过合成代码编辑序列微调后与GPT-4相比具有竞争力，并且优于基于基线数据集微调的模型，绝对得分高出20%（±3%）。  最后，我们还预训练了自己的小型模型用于代码理解。结果表明，对小型模型进行基于合成代码编辑的微调可以达到类设备模型的最高代码合成性能。我们的1.5亿参数编辑序列模型在性能上匹配或超越了参数量翻倍的代码模型，无论是否进行多次采样，包括Codex和AlphaCode。|
|**2024-10-03**|**CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation**|Han He et.al.|[2410.02748](http://arxiv.org/abs/2410.02748)|null|本文探讨了利用从源文档中提取的显著信息增强总结提示的方法。我们证明，在提示中加入关键短语可以提高ROUGE F1和召回率，使生成的摘要与参考摘要更相似且更完整。关键短语的数量可以控制精确度和召回率之间的权衡。进一步的分析显示，融入短语级别的显著信息优于基于单词或句子级别的信息。然而，这种方法对幻觉的影响并非在所有大型语言模型（LLM）上都是积极的。为了进行这项分析，我们引入了轻量级模型Keyphrase Signal Extractor（CriSPO），该模型可以微调以提取显著的关键短语。通过使用CriSPO，我们在数据集、开源和专有LLM上实现了对ROUGE改进的一致性，无需对LLM进行定制。我们的发现为构建基于提示的总结系统时利用显著信息提供了见解。|
|**2024-10-03**|**Contrastive Localized Language-Image Pre-Training**|Hong-You Chen et.al.|[2410.02746](http://arxiv.org/abs/2410.02746)|null|本文针对对比语言-图像预训练（CLIP）作为视觉语言基础模型的成功，重点在于通过在图像级别上对齐网络文本注释来优化视觉编码器。然而，这种策略在需要细粒度视觉表示的下游任务中可能变得不够充分，尤其是当多模态大型语言模型（MLLMs）需要进行区域级理解时。本文提出了一种名为对比定位语言-图像预训练（CLOC）的方法，通过补充CLIP以增加区域文本对比损失和模块来提升其定位能力。我们引入了一个新的概念，即可提示嵌入，其允许编码器生成易于通过空间提示转换为区域表示的图像嵌入。为了支持大规模预训练，设计了一个视觉增强且空间局部化的描述符生成框架，能够有效生成大规模的区域文本伪标签。通过扩展到数十亿标注图像，CLOC使得图像区域识别和检索任务中的高质量区域嵌入成为可能，并可以作为CLIP的直接替代品，用于增强MLLMs，特别是在指代和上下文理解任务中。|
|**2024-10-03**|**Neutral residues: revisiting adapters for model extension**|Franck Signe Talla et.al.|[2410.02744](http://arxiv.org/abs/2410.02744)|null|我们解决了一个新的问题：如何将预训练的大规模语言模型扩展到在训练时未曾见过的领域，例如添加一种原始模型未见过或见过很少训练数据的语言。流行的解决方案如微调或低秩适应在领域适应方面取得成功，但它们实际上并未增加额外的能力，并且降低了原始领域的性能。本文从三个角度分析了这个问题：数据、架构和训练过程，这些都被有利地联合考虑。特别是，我们改进了适配器，并使其有可能学习一个全新的语言，同时确保神经网络在原始领域的输出几乎不变。为此，我们修改了新的残差块的方式，使得每个新的残差块在原始领域输出接近零的结果。  这种被称为“中性残差”的解决方案借鉴了混合专家架构的组件，效果显著：与仅用英语训练的原始模型相比，只需要额外20%的学习权重，我们的方法在学习新语言和不忘记英语之间的权衡上取得了显著优于同时进行的其他方法（微调、低秩或常规适配器）的结果。|
|**2024-10-03**|**MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions**|Yekun Chai et.al.|[2410.02743](http://arxiv.org/abs/2410.02743)|null|强化学习从人类反馈（RLHF）已经证明了在使大型语言模型（LLMs）与人类偏好保持一致方面具有有效性。然而，基于token的RLHF面临着长期序列中的责任归因问题，其中延迟奖励使得模型难以确定哪些操作导致了成功的结果，这阻碍了学习效率并减慢了收敛速度。在这篇论文中，我们提出了一种名为MA-RLHF的简单而有效的RLHF框架，它将宏动作——一系列token或更高层次的语言构造——融入到学习过程中。通过在更高抽象级别上操作，我们的方法减少了行动和奖励之间的时序距离，从而促进了更快且更准确的责任归因。这导致了更稳定的策略梯度估计，并提高了每个episode内的学习效率，所有这些都无需在训练或推理期间增加计算复杂性。我们通过在文本摘要、对话生成、问题回答和程序合成等各个模型大小和任务上进行的大量实验验证了我们的方法。我们的方法在文本摘要和代码生成任务中实现了高达30%的性能提升，在对话任务中实现了18%，在问题回答任务中实现了8%。值得注意的是，我们的方法比标准的RLHF快1.7至2倍的训练时间达到与之相匹敌的性能水平，并且随着进一步的训练，继续超越它。我们将提供我们的代码和数据，供公众访问，网址为https://github.com/ernie-research/MA-RLHF 。|
|**2024-10-03**|**Grounding Large Language Models In Embodied Environment With Imperfect World Models**|Haolan Liu et.al.|[2410.02742](http://arxiv.org/abs/2410.02742)|null|尽管大型语言模型（LLMs）在各种应用中取得了广泛的成功，但它们在处理基本物理推理或执行机器人任务时经常遇到困难，这主要是由于它们缺乏对现实世界物理细节的直接经验。为了解决这些问题，我们提出了一种名为Grounding Large language model with Imperfect world MOdel (GLIMO)的方法，该方法利用代理世界模型，如模拟器，来收集和合成训练数据。GLIMO整合了一个基于LLM的自动数据生成器，用于创建高质量且多样化的指令数据集。生成器包括一个用于时间一致性体验采样的迭代自我精炼模块、一组多样化的问答指令种子，以及一个反思先前经验的检索增强生成模块。  全面的实验结果显示，我们的方法能够显著提升强开源LLMs（如LLaMA-3）的表现，分别在三个不同基准上实现了2.04倍、1.54倍和1.82倍的性能提升。其性能能够与或超越其更大的同辈模型，如GPT-4。|
|**2024-10-03**|**Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization**|Lei Xu et.al.|[2410.02741](http://arxiv.org/abs/2410.02741)|null|本文探讨了利用源文档中提取的显著信息来增强生成提示以改进大型语言模型（LLM）的摘要能力。我们发现，在提示中加入关键短语能提升ROUGE F1和召回率，使得生成的摘要与参考摘要更加相似且更完整。通过调整关键短语的数量，可以控制精确度和召回率之间的权衡。进一步分析显示，将短语级的显著信息融入提示优于基于单词或句子的策略。然而，这并不意味着对所有LLM都普遍有益，特别是在减少幻觉方面。为了进行这一分析，我们引入了轻量级的Keyphrase Signal Extractor（SigExt）模型，该模型可进行微调以提取关键短语。通过使用SigExt，我们在多个数据集、公开权重和专有LLM上实现了不依赖于LLM定制的ROUGE指标改善效果。我们的研究结果为构建基于提示的摘要系统时利用显著信息提供了见解。|
|**2024-10-02**|**Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads**|Yuxiang Huang et.al.|[2410.01805](http://arxiv.org/abs/2410.01805)|**[link](https://github.com/huangyuxiang03/Locret)**|**大型语言模型（LLMs）在支持长期上下文理解和处理任务方面取得了显著进步。然而，将LLMs的生成推理扩展到如此长的上下文会增加大量的计算负载，并要求在维持基于转换器的LLMs的关键值对（KV）缓存时使用大量GPU内存。现有的KV缓存压缩方法，如量化，随着上下文长度的增加而遇到内存瓶颈；而固定大小的缓存，如淘汰策略，则由于不高效的策略而导致效率低下。这些限制限制了在单个Nvidia 4090 GPU等消费者级设备上的部署。  为了克服这一挑战，我们提出了Locret框架，这是一种用于长上下文LLM推理的方法，通过引入保留头部来评估KV缓存单元的因果重要性，从而允许在固定缓存大小内进行更准确的淘汰。Locret在冻结的主干LLM基础上进行了微调，使用标准长时间上下文SFT数据集的少量数据。在推理过程中，我们以分块预填充模式淘汰低重要性的缓存单元，显著减少了峰值GPU内存使用量。  我们进行了广泛的实证研究来评估Locret，实验结果表明，与最近的竞争方法（包括InfLLM、量化、SirLLM和MInference）相比，Locret在内存效率和生成内容质量方面均表现出色——Locret实现了与Phi-3-mini-128K和Llama-3.1-8B-instruct全KV缓存相比超过20倍和8倍的KV缓存压缩比率。此外，Locret还可以与其他方法（如量化和令牌合并）结合使用。据我们所知，Locret是第一个能够将Llama-3.1-8B或类似模型部署到单个Nvidia 4090 GPU上，同时在不牺牲生成质量的情况下实现128K长上下文推理的框架，且仅需要少量额外的系统优化。**|
|**2024-10-02**|**Efficient $1$-bit tensor approximations**|Alex W. Neal Riasanovsky et.al.|[2410.01799](http://arxiv.org/abs/2410.01799)|null|我们提出了一种空间效率高的矩阵和任意阶张量分解方法，作为线性组合的张量积形式，其中向量值为$\{-1, 1\}$。对于任一矩阵$A \in \mathbb{R}^{m \times n}$，其表达式为：$$A - R_w = S_w C_w T_w^\top = \sum_{j=1}^w c_j \cdot \mathbf{s}_j \mathbf{t}_j^\top$$ 这是一个关于$A$的“宽度为$w$的符号切分解”。这里$C_w = "diag"(\mathbf{c}_w)$，且$S_w, T_w$和向量$\mathbf{s}_j, \mathbf{t}_j$均为$\{-1, 1\}$值。用于存储$(S_w, T_w, C_w)$所需的空间是$w \cdot (m + n)$位，并仅需$w$个浮点数。当应用于具有i.i.d. $\mathcal N (0, 1)$分布元素的#f32矩阵时，$\,R_w\,_F$呈现出指数衰减。选择合适的$w$，使$(S_w, T_w, C_w)$的内存占用与\textit{f16}或\textit{bf16}矩阵相同，相对误差相当。我们的算法在20行伪代码中实现了高效的符号切分解。它源自1999年Frieze和Kannan的一篇著名论文的简单修改。  作为第一个应用，我们对开放源码大型语言模型\textit{Mistral-7B-v0.1}中的权重矩阵进行了$50\%$的空间压缩。令人惊讶的是，所有$226$个余矩阵的相对误差均小于$6\%$，且扩展模型在huggingface排行榜上与\textit{Mistral-7B-v0.1}模型表现相近。随着空间压缩率从$50\%$降低至$25\%$ ，基准性能缓慢下降。我们优化了开源的\textit{rust}实现，使用了\textit{avx2}和\textit{avx512}架构下的\textit{simd}指令进行加速。此外，我们还将该算法扩展到了任意阶张量，并利用它压缩了一张作者猫Angus的照片。  请注意，这里的文本并未包含任何特殊字符或特定格式标记，而是以纯文本形式呈现了摘要内容。|
|**2024-10-02**|**Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models**|Joseph Lee et.al.|[2410.01795](http://arxiv.org/abs/2410.01795)|**[link](https://github.com/pennshenlab/freeform)**|**基于复杂遗传基础预测表型，利用小而可解释的变异特征仍然是一项具有挑战性的任务。传统上，使用数据驱动的方法进行此任务，但基因型数据的高维特性使得分析和预测变得困难。受到预训练大型语言模型（LLM）中编码的丰富知识及其在处理复杂生物医学概念上的成功启发，我们旨在探索LLM在表格基因型数据特征选择与工程方面的能力，并引入一种基于知识的框架。我们开发了FREEFORM，一种自由流动推理与集成增强特征输出和稳健建模的框架，该框架结合了链式思考与集成原则，利用LLM的内在知识来选择和工程特征。在两个不同的人类基因型-表型数据集上进行评估，包括遗传血统和遗传性听力损失，我们发现这个框架在低样本量情况下优于几种数据驱动方法。FREEFORM作为一个开源框架，可以在GitHub上获取：https://github.com/PennShenLab/FREEFORM。**|
|**2024-10-02**|**When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1**|R. Thomas McCoy et.al.|[2410.01792](http://arxiv.org/abs/2410.01792)|null|在“自动回归余烬”（McCoy等人，2023年）中，我们展示了几个大型语言模型（LLMs）在起源上存在一些重要限制，这归因于它们的下一个单词预测特性。这里我们探讨了OpenAI的新系统o1是否依然存在这些问题，与之前的LLMs相比，o1在推理优化方面有所不同。研究发现，o1在许多情况下显著优于之前模型，在某些常见任务的罕见变体上（例如，从列表中的每个词的第二个字母形成缩写，而不是第一个字母）表现尤其出色。尽管这些定量改进令人印象深刻，但o1依然显示出了与之前系统相同的基本趋势：对于概率较高的示例和任务，o1的表现更好且需要的“思考令牌”数量较少；而在概率较低的情况下则表现不佳。  这些结果表明，优化语言模型以进行推理可以减轻但可能无法完全克服语言模型的概率敏感性问题。|
|**2024-10-02**|**Investigating on RLHF methodology**|Alexey Kutalev et.al.|[2410.01789](http://arxiv.org/abs/2410.01789)|null|本文研究了大型语言模型根据人类偏好的对齐问题。我们讨论了训练偏好模型的特性，该模型模拟人类偏好，并介绍了实现最佳结果所需的方法和细节。此外，我们还探讨了使用强化学习微调大型语言模型的方法，描述了遇到的挑战以及克服这些挑战的方式。我们还提出了直接偏好优化方法的经验，这种方法允许我们将大型语言模型与人类偏好对齐，而无需创建单独的偏好模型。作为我们的贡献，我们引入了一种通过困惑度筛选收集偏好数据集的方法，这使得为特定语言模型创建这样的数据集的过程更加简便且成本效益更高。|
|**2024-10-02**|**OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic Foundation Models**|Heng Yang et.al.|[2410.01784](http://arxiv.org/abs/2410.01784)|**[link](https://github.com/yangheng95/OmniGenomeBench)**|**近年来，人工智能领域的进步，特别是大型语言模型（LLMs），激发了对基因组基础模型（GFMs）突破性进展的期待。自生命进化之初就隐藏在多样化的基因组中的“自然之码”，蕴含着巨大潜力，能够通过基因组建模对人类和生态系统产生深远影响。近期GFM领域的重要突破，如Evo，吸引了大量投资与关注，它们解决了长期存在的挑战，并将基因组研究从手动、不可靠和低效的传统模式转变为自动化、可靠和高效的新范式。在基因组学连续技术革命的背景下，GFM研究面临两大挑战：缺乏GFM基准测试工具以及多维基因组学的开源软件缺失。这些挑战阻碍了GFM快速演进及其广泛应用于理解与合成基因组等数十年来存在的问题的能力。为了应对这些挑战，我们引入了GFMBench框架，一个专注于GFM导向基准测试的平台。GFMBench标准化了基准套件，并实现了对大量开源GFMs的自动化基准测试。它集成了来自四大大型基准的数百万个基因序列，覆盖数百种基因组任务，使GFMs民主化，适用于广泛的虚拟基因组应用。此外，GFMBench作为开源软件发布，提供用户友好界面和多样化教程，适用于自动测试以及RNA设计和结构预测等复杂任务。为了促进基因组建模领域的进一步发展，我们启动了一个公共排行榜，展示由AutoBench生成的基准性能。GFMBench代表了标准化GFM基准测试和民主化GFM应用的一大步。**|
|**2024-10-02**|**Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models**|Shayekh Bin Islam et.al.|[2410.01782](http://arxiv.org/abs/2410.01782)|**[link](https://github.com/ShayekhBinIslam/openrag)**|为了提升大型语言模型（LLM）在事实准确性上的表现，检索增强生成（RAG）方法已经得到了广泛研究。然而，现有的方法往往在利用检索到的证据进行推理的能力上存在局限性，尤其是在使用开源LLM时。为了填补这一差距，我们提出了一种新颖的框架——Open-RAG，旨在增强开源LLM在RAG中的推理能力。我们的框架将任意密集型LLM转换成一个参数高效的稀疏混合专家（MoE）模型，能够处理包括单跳和多跳查询在内的复杂推理任务。  Open-RAG的独特之处在于，它通过训练模型来应对看似相关但具有误导性的干扰项，从而有效地导航复杂场景。通过利用潜学习，Open-RAG动态选择相关专家并整合外部知识，以提供更准确、更具上下文的相关响应。此外，我们还提出了一种混合自适应检索方法，用于判断检索的必要性，并平衡性能增益与推理速度之间的权衡。  实验结果显示，基于Llama2-7B的Open-RAG在各种知识密集型任务中，相较于ChatGPT、Self-RAG和Command R+等最先进的LLM和RAG模型，表现出更优的表现。我们已将代码和模型开源在https://openragmoe.github.io/。|
|**2024-10-02**|**Quantifying Generalization Complexity for Large Language Models**|Zhenting Qi et.al.|[2410.01769](http://arxiv.org/abs/2410.01769)|**[link](https://github.com/zhentingqi/scylla)**|在大型语言模型（LLMs）展现出理解复杂查询和执行高级任务的非凡能力的同时，它们的泛化能力往往与记忆深度交织在一起，这要求我们进行更精确的评估。为了应对这一挑战，我们引入了Scylla，这是一个动态评估框架，定量衡量LLMs的泛化能力。Scylla通过在分布内（ID）和分布外（OOD）数据上评估模型性能来分离泛化与记忆，涉及20个任务，覆盖5个复杂度级别。通过广泛的实验，我们揭示了任务复杂度与ID和OOD数据之间的性能差距之间非单调的关系，我们将其称为泛化山谷。具体来说，这一现象揭示了一个关键阈值——称为关键复杂性——在该阈值处，非泛化行为的依赖达到峰值，表明了LLMs泛化能力的上限。随着模型大小的增加，关键复杂性向更高层次的任务复杂度移动，表明更大的模型可以在依赖于记忆之前处理更复杂的推理任务。利用Scylla和关键复杂性的概念，我们对包括开源模型如LLaMA和Qwen家族、以及闭源模型如Claude和GPT在内的28个LLMs进行了基准测试，提供了更稳健的评估，并对LLMs的泛化能力有了更清晰的理解。|
|**2024-10-02**|**LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks**|Mengzhao Jia et.al.|[2410.01744](http://arxiv.org/abs/2410.01744)|**[link](https://github.com/jill0001/leopard)**|文本丰富的图像在实际应用中普遍存在，如幻灯片演示、扫描文档和网页快照等，其中文本作为核心视觉元素引导整体理解。多图像文本丰富的任务尤其具有挑战性，因为它们不仅需要理解单个图像的内容，还需要在多个视觉输入之间推理关系和逻辑流程。尽管这些场景的重要性，当前的多模态大型语言模型（MLLMs）在处理此类任务时遇到两个关键挑战：（1）缺乏适合于多图像文本丰富场景的高质量指令调优数据集；（2）难以平衡图像分辨率与视觉特征序列长度。为了应对这些挑战，我们提出了\OurMethod，一个专门设计用于处理涉及多文本丰富图像的视语言任务的MLLM。首先，我们收集了约一百万条针对多文本丰富、多图像场景的高质量多模态指令调优数据。其次，我们开发了一种适应性的高分辨率多图像编码模块，根据输入图像的原始纵横比和分辨率动态优化视觉序列长度的分配。在一系列广泛的基准测试中，我们的模型在多文本丰富、多图像评估中表现出优越的能力，并在通用领域评估中展现出竞争力。|
|**2024-10-02**|**VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models**|Kailai Feng et.al.|[2410.01738](http://arxiv.org/abs/2410.01738)|**[link](https://github.com/carlofkl/vitaglyph)**|**本文引入了一种双分支、无需训练的新型艺术字体生成方法——VitaGlyph。该方法旨在通过灵活地表达输入字符的核心概念以及丰富相关的背景信息，实现艺术字体与可控制的几何变化之间的平衡，从而保持字体的可读性。VitaGlyph的核心理念是将输入字符视为由主体和周围环境组成的场景，并在不同几何变换程度下进行渲染。  具体来说，VitaGlyph通过以下三个阶段框架实现其功能：(i) 知识获取阶段利用大型语言模型设计主体和周围环境的文本描述；(ii) 区域分解阶段识别最匹配主体描述的部分，并将输入的字符图像分为主体和周围区域；(iii) 字体风格化阶段首先通过语义字体优化主体区域的结构，然后分别使用可控组合生成技术渲染主体和周围区域的纹理。  实验结果表明，VitaGlyph不仅在艺术性和可读性方面表现出色，还能够描绘多种定制概念，从而促进更富有创意和愉悦的艺术字体生成。项目代码将在https://github.com/Carlofkl/VitaGlyph公开提供。**|
|**2024-09-30**|**MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning**|Haotian Zhang et.al.|[2409.20566](http://arxiv.org/abs/2409.20566)|null|我们提出了一种新的多模态大型语言模型家族MM1.5，旨在增强文本丰富图像理解、视觉引用与定位以及多图像推理的能力。在MM1架构的基础上，MM1.5采用数据驱动的方法进行模型训练，系统性地探索在整个模型训练生命周期内不同数据混合的影响。这包括高质量的OCR数据和合成描述符用于持续预训练，以及优化的视觉指令调参数据混合用于监督微调。我们的模型涵盖了从1亿到30亿参数的范围，包括密集型和混合专家（MoE）变体，并证明了即使在较小规模（1亿和3亿参数）下，精心的数据整理和训练策略也能产生强大的性能。此外，我们引入了两个专门的变体：MM1.5-Video，用于视频理解；MM1.5-UI，用于移动用户界面理解。通过广泛的实证研究和消融分析，我们提供了关于训练过程和决策的详细见解，这些见解对于未来多模态大型语言模型的发展具有宝贵的指导意义。|
|**2024-09-30**|**Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos**|Md Mohaiminul Islam et.al.|[2409.20557](http://arxiv.org/abs/2409.20557)|null|本文提出了VidAssist，一个用于从教学视频中进行零样本或少量样本的目标导向规划的集成框架。VidAssist利用大型语言模型（LLM）作为知识库和评估工具，生成并评估行动计划，以此克服从小规模、低多样性数据集获取过程知识的挑战。此外，VidAssist采用广度优先搜索算法进行最优计划生成，并使用专为目标导向规划设计的价值函数，在每一步评估预测动作。广泛实验表明，VidAssist提供了一个适用于不同目标导向规划设置的统一框架，如视觉辅助规划（VPA）和程序规划（PP），在零样本和少量样本设置下表现出卓越性能。具体而言，我们的少量样本模型在COIN数据集上的VPA任务和PP任务上分别比全监督的前导方法高出+7.7%和+4.81%，同时预测4个未来动作。所有代码和模型都在https://sites.google.com/view/vidassist公开提供。|
|**2024-09-30**|**LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation**|Ziyao Zhang et.al.|[2409.20550](http://arxiv.org/abs/2409.20550)|null|本文提出了一项针对大型语言模型（LLM）在代码生成任务中的幻觉现象的实证研究。尽管LLM在代码生成任务上的表现令人鼓舞，但它们在处理实际开发过程中复杂的上下文依赖关系时，往往会产生错误或不准确的结果。以往的研究主要关注于基于LLM的代码生成在单一功能生成场景下的幻觉分析，但本文将研究范围扩展至更实际且复杂的仓库级生成情景。  首先，通过人工检查六种主流LLM的代码生成结果，本文建立了LLM生成代码的幻觉分类体系。接下来，详细阐述了幻觉现象，并分析了不同模型间幻觉分布的情况。进一步地，本文探讨了幻觉产生的原因，并识别了四个可能导致幻觉的因素。  最后，提出了一种基于记忆网络（RAG）的缓解方法，该方法在所有研究的LLM上均表现出一致的有效性。提供了一个包括代码、数据和实验结果的可复制包，供学术界和工业界参考和验证。此研究有助于提高LLM在代码生成任务中的可靠性与准确性，对软件工程领域具有重要意义。|
|**2024-09-30**|**Robi Butler: Remote Multimodal Interactions with Household Robot Assistant**|Anxing Xiao et.al.|[2409.20548](http://arxiv.org/abs/2409.20548)|null|在这篇论文中，我们引入了Robi Butler，一种新型的家庭机器人系统，它能够与远程用户进行多模态交互。基于先进的通信接口，Robi Butler允许用户监控机器人的状态、发送文本或语音指令，并通过手势选择目标对象。我们的系统的核心是一个由大型语言模型（LLMs）驱动的高级行为模块，该模块能够解释多模态指令并生成行动计划。这些计划由支持文本和点击查询的视觉语言模型（VLMs）处理的开放词汇集组成。整合以上组件使得Robi Butler能够在零样本的情况下将远程多模态指令转化为现实世界家庭环境中的实际操作。我们通过演示各种日常家务任务的有效性和效率，展示了该系统的应用，这些任务涉及到远程用户给出多模态指令。此外，我们还进行了用户研究，分析了多模态交互对远程人机交互的效率和用户体验的影响，并讨论了可能的改进措施。|
|**2024-09-30**|**Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis of Perovskite via Language Models**|Arpan Mukherjee et.al.|[2409.20512](http://arxiv.org/abs/2409.20512)|null|本文提出了一种创新框架，旨在解决准确预测工业合成中所用钙钛矿溶剂毒性这一挑战。由于缺乏针对性和结构化的毒性数据，这一任务面临局限性。该框架结合了语言模型的自动化数据提取与具有不确定性信息的预测模型，以填补数据空白并提高预测的置信度。  首先，我们采用了两种方法从涉及钙钛矿合成溶剂的科学文献语料库中自动提取相关数据：较小的双向语言模型（如BERT和ELMo）因其重复性和确定性输出而被使用；而自回归大型语言模型（LLM）如GPT-3.5则利用其庞大的训练语料库和更好的响应生成能力。我们的“提示和验证”技术集成到LLM中，旨在实现有针对性的提取和优化，从而减少LLM的幻觉现象，提升提取数据的质量。  接下来，提取的数据被输入到预训练的多任务二元分类深度学习模型，用于预测提取溶剂的ED性质。我们利用从分类模型获得的类别概率进行香农熵为基础的不确定性量化，以此来量化不确定性并识别预测中的数据缺口。这种方法导致构建了一个结构化的用于钙钛矿合成溶剂及其基于不确定性虚拟毒性的评估数据集。  此外，我们使用了和弦图来可视化溶剂之间的相互作用，并优先考虑那些可能存在危险的溶剂，结果发现70%的溶剂相互作用主要与特定的两种钙钛矿相关联。|
|**2024-09-30**|**COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models**|Divyanshu Daiya et.al.|[2409.20502](http://arxiv.org/abs/2409.20502)|null|我们提出了一种名为COLLAGE的新型框架，用于通过利用大型语言模型（LLM）和层次化的运动特异性向量量化变分自编码器（VQ-VAE）来生成协作式代理-对象-代理交互。我们的模型解决了这一领域数据稀缺的问题，通过整合LLM的知识和推理能力来指导生成性扩散模型。层次化的VQ-VAE架构在多个抽象级别捕获了不同的运动特异性特征，避免了冗余概念，并实现了高效的多分辨率表示。我们引入了一个在隐空间中操作的扩散模型，并结合了由LLM生成的运动规划提示来引导去噪过程，从而实现了针对特定提示的运动生成，具有更高的控制性和多样性。在CORE-4D和InterHuman数据集上的实验结果证明了我们的方法在生成真实且多样化的协作人类-物体-人类交互方面的有效性，超越了现有最佳方法。我们的工作为机器人学、图形学和计算机视觉等领域建模复杂交互提供了新的可能性。|
|**2024-10-01**|**Instance-adaptive Zero-shot Chain-of-Thought Prompting**|Xiaosong Yuan et.al.|[2409.20441](http://arxiv.org/abs/2409.20441)|null|零射链思考（CoT）提示策略在增强大型语言模型（LLM）解决现实世界推理任务的性能方面展现出简单而有效的方法。然而，单一任务级提示在整个实例上的应用存在局限性，因为一个提示无法与所有实例都成为最佳搭档。因此，更恰当的做法是精心考虑提示与每个实例之间的互动。本文提出了一种实例自适应提示算法作为零射CoT推理的一种替代策略，旨在通过适当地区分出好的和坏的提示来提升性能。  具体来说，我们首先通过信息流的角度对LLM进行分析，以揭示零射CoT推理机制，发现信息从问题到提示以及问题到推理的双向流动对推理结果影响最大。我们注意到，更优秀的零射CoT推理需要提示从问题中获取语义信息，然后推理从问题直接或通过提示间接地聚合足够信息。相反，缺失这些任何一项可能都会导致一个不理想的提示。基于此发现，我们进一步提出了一个适用于零射CoT推理的实例自适应提示策略（IAP）。  在LLaMA-2、LLaMA-3和Qwen上对数学、逻辑和常识推理任务（如GSM8K、MMLU、因果判断）进行的实验表明，实例自适应零射CoT提示策略在某些定制提示或复杂程序的基础上表现出更好的性能，这证明了我们在零射CoT推理机制研究中的发现具有重要意义。|
|**2024-09-30**|**Wait, but Tylenol is Acetaminophen... Investigating and Improving Language Models' Ability to Resist Requests for Misinformation**|Shan Chen et.al.|[2409.20385](http://arxiv.org/abs/2409.20385)|null|背景：大型语言模型（LLMs）被训练成遵循指令，但这种设计使其容易在生成错误信息时盲目遵从用户请求。在医学领域，这可能会加速错误信息的传播，从而影响人类健康。研究目标/方法：我们分析了模型在知道请求不合理的情况下，生成与药物有关误导性内容的倾向。我们探讨了通过上下文提示和调整参数，使LLMs优先考虑逻辑推理而非遵从性，以降低医疗信息误导风险的可能性。  结果：所有前沿LLMs都遵守了生成误导性内容的不合理请求。然而，基于提示的方法和参数调整策略可以提升检测请求逻辑错误的能力，并防止医疗信息的误传。  结论：将LLMs的设计重心从遵从性转向逻辑推理，有助于降低其被利用于传播医疗信息误导的风险。|
|**2024-09-30**|**The Perfect Blend: Redefining RLHF with Mixture of Judges**|Tengyu Xu et.al.|[2409.20370](http://arxiv.org/abs/2409.20370)|null|本文介绍了一种新的后训练范式，称为约束生成策略优化（CGPO）。CGPO的核心是“裁判混合”（MoJ），它以成本效益的方式对策略进行分层约束优化，从而在原理上识别RLHF中的完美融合。此方法在理论上有保证，不需要大量的超参数调整，并且可以在常见的后训练管道中无缝集成。这有助于检测和缓解奖励作弊行为，并在大量目标的场景下达到帕累托最优点。  我们的实验评估表明，CGPO在各种任务上显著优于标准的RLHF算法，如PPO和DPO，包括通用聊天、STEM问题、指令遵循和编程等。具体而言，CGPO在AlpacaEval-2（通用聊天）上提高了7.4%，在Arena-Hard（STEM与推理）上提高了12.5%，并在数学和其他领域如编程等任务上保持一致的改进。值得注意的是，虽然PPO经常被使用，但在流行的编程基准测试中，它容易遭受严重的奖励作弊，而CGPO成功地解决了这个问题。  这一突破在RLHF领域不仅解决了奖励作弊和极端多目标优化的挑战，而且推进了通用语言模型在多种应用中的对齐技术。|
|**2024-09-30**|**VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs**|Ruotong Liao et.al.|[2409.20365](http://arxiv.org/abs/2409.20365)|**[link](https://github.com/mayhugotong/videoinsta)**|在视频语言领域，利用零样本大型语言模型（LLM）推理进行视频理解的最新工作已成为挑战传统端到端模型的有力竞争者。然而，长视频的理解面临着独特的挑战，尤其是在处理持续时间较长的时间跨度时，即使是零样本LLM方法也是如此。长视频中的信息冗余问题促使我们思考哪些信息对于大型语言模型至关重要，以及如何利用它们进行复杂的空间-时间推理，以实现对长视频分析的理解。  为此，我们提出了一种名为VideoINSTA（INformative Spatial-TemporAl Reasoning）的框架，用于零样本长视频理解。VideoINSTA的主要贡献包括：（1）利用LLM进行长视频理解的零样本框架；（2）事件驱动的时间推理和基于内容的空间推理方法，使LLM能够对视频中的空间-时间信息进行推理；（3）一种自我反思的信息推理方案，通过信息充分性和预测置信度的平衡来调整时间因素。  我们的模型在三个长视频问答基准测试上显著提高了现有最佳性能：EgoSchema、NextQA和IntentQA，以及开放问答数据集ActivityNetQA。代码已在此处发布：https://github.com/mayhugotong/VideoINSTA。|
|**2024-09-27**|**LML: Language Model Learning a Dataset for Data-Augmented Prediction**|Praneeth Vadlapati et.al.|[2409.18957](http://arxiv.org/abs/2409.18957)|**[link](https://github.com/pro-genai/lml-dap)**|**本文提出了一种利用大型语言模型（LLM）解决分类任务的新方法，这通常由机器学习（ML）模型处理。与依赖大量数据清洗和特征工程的ML模型不同，此方法通过简化流程，使用LLM来优化过程。本文引入了一个名为“语言模型学习（LML）”的概念，借助一种称为“数据增强预测（DAP）”的新方法。分类任务由LLM执行，类似于人类手动探索和理解数据，并利用数据作为参考来做出分类决策。  训练数据被总结和评估，以确定导致每个标签分类的主要特征。在DAP过程中，系统使用数据概要自动生成查询，用于从数据集中检索相关行。通过使用数据概要和相关数据，LLM基于数据概要和相关行生成分类，即使面对复杂数据也能确保满意的准确性。数据概要和类似数据在DAP中的应用确保了决策的上下文意识。该方法在提示中使用了“以可解释的机器学习模型身份行事”的语句，增强了预测的可解释性，允许用户审查每条预测背后的逻辑。在某些测试案例中，系统的准确率超过90%，证明了系统的有效性及其在各种场景下超越传统ML模型的潜力。代码已发布于https://github.com/Pro-GenAI/LML-DAP。**|
|**2024-09-27**|**Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models**|Jiaming Li et.al.|[2409.18943](http://arxiv.org/abs/2409.18943)|**[link](https://github.com/geaming2002/ruler)**|**大型语言模型的遵循指令能力使得人类能够以自然的方式与AI代理互动。然而，在需要生成特定长度响应时，大型语言模型往往难以满足用户需求，这主要是由于它们在准确感知数值限制方面存在的固有困难。为了探索大型语言模型在遵循特定长度指令时控制生成响应长度的能力，我们提出了目标长度生成任务（TLG）并设计了两个度量标准，精确匹配（PM）和灵活匹配（FM），以评估模型在遵守指定响应长度方面的性能。此外，我们引入了一种新颖的、模型无关的方法Ruler，通过使用元长度标记（MLTs）增强大型语言模型在长度受限指令下的指令遵循能力。具体而言，Ruler使LLMs能够在指令中包含长度约束的情况下生成指定长度的响应。而且，当长度约束没有明确提供时，Ruler还能自动生成适当的MLT，表现出出色的通用性和泛化能力。全面的实验表明，Ruler在目标长度生成任务上对不同的LLMs都显示出有效性，例如在PM上的平均增益为27.97，在FM上的平均增益为29.57。此外，我们还进行了广泛的消融实验进一步验证了Ruler的有效性及其泛化能力。我们的代码和数据可在https://github.com/Geaming2002/Ruler获取。**|
|**2024-09-27**|**From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding**|Heqing Zou et.al.|[2409.18938](http://arxiv.org/abs/2409.18938)|null|本文综述了大型语言模型（LLMs）与视觉编码器集成在视觉理解任务中的最新进展，利用其固有优势来理解和生成类似人类的文本以进行视觉推理。由于视觉数据的多样性，多模态大型语言模型（MM-LLMs）在设计和训练上针对理解图像、短视频和长视频时表现出不同的特征和挑战。我们的研究聚焦于长视频理解与静态图像及短视频理解之间的显著差异及其独特挑战。  不同于静态图像，短视频包含了序列帧的时空信息以及事件内部的时间信息；而长视频则包含了多个事件的时空信息以及事件间的长期时间依赖性。本文旨在追溯并总结MM-LLMs从图像理解到长视频理解的发展历程，详细对比各种视觉理解任务之间的差异，并突出长视频理解所面临的挑战，如更细致的时空细节、动态事件和长期依赖性。  接着，本文对MM-LLMs在模型设计和训练方法上的发展进行了详尽的概述，特别关注于如何有效理解长视频。最后，通过比较现有MM-LLMs在不同长度的视频理解基准测试上的表现，本文讨论了多模态大型语言模型在长视频理解领域可能的未来发展方向。|
|**2024-09-27**|**AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow**|Huizi Yu et.al.|[2409.18924](http://arxiv.org/abs/2409.18924)|null|在现代医学教育与研究领域，模拟患者系统发挥着至关重要的作用，它们提供了一个安全、综合的学习环境，并允许进行临床决策模拟。大型语言模型（LLM）有望通过高保真度和低成本地复制医疗状况和医患互动，进一步提升模拟患者系统的能力。然而，确保这些系统的有效性和可信性仍是一个挑战，因为它们需要一个规模大、多样且精确的患者知识库，同时具备强大的稳定知识传播能力。  在此背景下，我们开发了AIPatient，这是一个高级的模拟患者系统，它以AIPatient知识图谱（AIPatient KG）作为输入，并采用基于推理检索增强生成（Reasoning RAG）的代理工作流程作为生成基础。AIPatient KG从Medical Information Mart for Intensive Care（MIMIC-III）数据库中的电子健康记录（EHRs）抽取数据，生成了一个在知识库有效性方面表现出色（F1得分为0.89）、临床多样性和相关性高的1,495名患者的群体。  Reasoning RAG利用了六个由LLM驱动的代理，覆盖了包括检索、KG查询生成、抽象、检查、重写和总结在内的任务。这个代理框架在基于EHR的医疗问答（QA）任务上达到了94.15%的整体准确性，显著优于仅使用无代理或部分代理集成的基准。  我们的系统还展示了高可读性（中位数Flesch阅读轻松度77.23；中位数Flesch-Kincaid年级5.6）、稳健性（ANOVA F值0.6126，p<0.1）和稳定性（ANOVA F值0.782，p<0.1）。AIPatient系统的出色性能预示着其在医学教育、模型评估和系统集成等多个应用领域的巨大潜力。|
|**2024-09-27**|**Soft Measures for Extracting Causal Collective Intelligence**|Maryam Berijanian et.al.|[2409.18911](http://arxiv.org/abs/2409.18911)|**[link](https://github.com/kuldeep7688/soft-measures-causal-intelligence)**|**理解与模拟集体智慧对于处理复杂社会系统至关重要。模糊认知地图（FCMs）作为表示因果心理模型的强大工具，通过定向图进行编码，但直接从文本提取高可信度的FCMs具有挑战性。本研究提出了一种利用大型语言模型（LLMs）自动提取FCMs的方法。我们引入了新颖的基于图的相似性度量，并通过使用Elo评分系统关联输出与人类判断来评估这些度量。结果显示，这些度量与人类评价之间存在正相关，尽管表现最好的度量仍然在捕捉FCM细微差别方面存在局限性。对LLMs进行微调可以提高性能，但现有的度量仍然不足以满足需求。本研究强调了需要针对FCMs提取设计的软相似性度量，从而推动了使用NLP模拟集体智慧的发展。**|
|**2024-09-27**|**IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation**|Fan Lin et.al.|[2409.18892](http://arxiv.org/abs/2409.18892)|**[link](https://github.com/DUTlf/IDGen)**|随着大型语言模型（LLMs）在处理复杂任务方面的能力日益增强，评估集必须与时俱进，以确保其持续保持足够的区分能力。受教育评估中广泛使用的项目鉴别（Item Discrimination, ID）理论启发，我们提出了一种基于ID的提示合成框架，用于评估LLMs，确保评估集能够根据模型的能力不断更新和优化。我们的数据合成框架注重广度与精确性并重。它能生成既能全面评估LLMs能力，又能揭示不同模型之间有意义性能差异的提示，从而实现对它们在各种任务和领域中的相对强项和弱点的有效区分。  为了产生高质量的数据，我们在通用化框架中融入了一个自我校正机制，并开发了两个模型来预测提示的鉴别能力和难度评分，以此推动我们的数据合成框架。这些工具对评估数据合成研究具有重要价值。我们将生成的数据应用于评估五款最先进的模型。该数据平均得分为51.92，方差为10.06。相比之下，先前的工作（如SELF-INSTRUCT和WizardLM）的平均得分超过67，方差低于3.2。结果表明，我们框架生成的数据在挑战性和区分能力上比之前的工作更具优势。我们计划发布包含超过3000个精心设计的提示的数据库，以促进LLMs评估研究的发展。|
|**2024-09-27**|**Predicting and analyzing memorization within fine-tuned Large Language Models**|Jérémie Dentan et.al.|[2409.18858](http://arxiv.org/abs/2409.18858)|null|大型语言模型因其在解决复杂任务方面的能力而受到广泛关注。然而，这些模型在训练数据中记忆了相当大的比例，这在推理时构成了严重的威胁。为了缓解这种无意的记忆问题，理解哪些元素被记忆以及原因至关重要。目前大多数现有工作提供的是事后解释，这在实践中兴趣有限。为填补这一缺口，我们提出了一种新的方法，基于切片互信息，在分类场景中预先检测记忆样本。该方法从训练的早期阶段就具有高效性，并且易于适应实际场景。我们的方法得到了新的理论结果的支持，我们通过实验展示了这一点，并且需要较低的计算预算。我们获得了强大的实证结果，为在记忆发生之前系统地检查和保护这些易受影响的样本铺平了道路。|
|**2024-09-27**|**Mitigating Selection Bias with Node Pruning and Auxiliary Options**|Hyeong Kyu Choi et.al.|[2409.18857](http://arxiv.org/abs/2409.18857)|null|大型语言模型（LLM）在回答多选题时往往表现出对某些选项的不适当偏好，这在LLM自动化系统中引发了显著的可靠性问题。以往的解决方案主要通过调整模型的输入和/或输出来应对偏见问题。而我们的工作则采取了不同的路径，旨在探究模型内部偏见的形成机制。具体而言，我们提出了一种名为偏差节点修剪（BNP）的新颖去偏方法，该方法旨在删除那些导致偏见的线性层参数。此外，我们还引入了一种名为辅助选项注入（AOI）的简单而有效的输入修改技术，适用于黑盒模型的去偏。为了提供一个更系统的方法来评估选择偏见，我们回顾了现有指标，并提出了选择Kullback-Leibler散度（CKLD），以解决常用指标对标签不平衡不敏感的问题。实验结果表明，我们的方法在应用到三种不同的LLM时表现出了鲁棒性和适应性。|
|**2024-09-27**|**LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis**|Hamed Babaei Giglou et.al.|[2409.18812](http://arxiv.org/abs/2409.18812)|**[link](https://github.com/HamedBabaei/LLMs4Synthesis)**|面对科学文献日益增长的复杂性和数量，本文提出了LLMs4Synthesis框架，旨在增强大型语言模型（LLMs）在生成高质量科学综合分析的能力。该框架针对快速、连贯和语境丰富的科学见解集成需求，利用开源和专有LLMs，以解决当前定量指标在评估这些综合分析时存在的不足。通过开发一种处理科学论文的新方法、定义新的综合类型以及建立九项详细的质量评估标准，我们的研究对这一领域做出了贡献。我们还提议将LLMs与强化学习和AI反馈相结合，以优化综合质量，并确保其与既定标准保持一致。LLMs4Synthesis框架及其组成部分的可用性，有望提升科学研究综合过程的生成和评价能力。|
|**2024-09-27**|**Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs**|Yanyuan Qiao et.al.|[2409.18794](http://arxiv.org/abs/2409.18794)|null|本文介绍了一项名为Open-Nav的创新研究，旨在探索开源大型语言模型（LLMs）在连续环境中的零样本视觉与语言导航（VLN）任务应用。Open-Nav采用了空间时间链式思维（CoT）推理方法，将任务分解为指令理解、进度估计和决策制定三个部分，以提高模型在导航场景中的感知能力并增强对细粒度物体和空间知识的理解。实验结果在模拟环境和真实世界环境中均显示，Open-Nav能够与使用闭源LLMs实现相当的竞争性性能。|
|**2024-09-26**|**EgoLM: Multi-Modal Language Model of Egocentric Motions**|Fangzhou Hong et.al.|[2409.18127](http://arxiv.org/abs/2409.18127)|null|在穿戴设备的普及背景下，理解主观视角的动作变得至关重要，以发展具有情境意识的人工智能。本文提出了一种名为EgoLM的通用框架，用于从多模态输入（如主观视频和运动传感器）中跟踪和理解主观动作。EgoLM通过利用丰富的上下文来解决单模态条件下的主体运动跟踪和理解难题。为了促进这一通用且多模态的框架，我们的核心洞察是使用大型语言模型（LLM）来建模主体动作和自然语言的联合分布。多模态传感器输入被编码并投影到语言模型的联合潜在空间中，并用于触发动作生成或文本生成，分别用于主体运动跟踪或理解。大规模多模态人体动作数据集上的广泛实验验证了EgoLM作为通用模型在普遍主观学习中的有效性。|
|**2024-09-26**|**Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography**|Yuexi Du et.al.|[2409.18119](http://arxiv.org/abs/2409.18119)|null|在医疗图像分析领域，对比语言-图像预训练（CLIP）显示出巨大潜力，但其需要大量的数据和计算资源。因此，现有的CLIP应用主要集中在如胸片这类拥有丰富图像报告数据的模态上，而忽略了诸如乳腺X光等许多重要模态的研究。本文首次提出将完整的CLIP模型应用于乳腺X光图像分析，这一任务面临着标记数据稀缺、高分辨率图像中的小感兴趣区域以及数据不平衡的挑战。  我们首先开发了一种针对乳腺X光的专用监督框架，利用其多视图特性。此外，设计了对齐模块以更好地聚焦于高分辨率图像中的详细特征。最后，引入了一种参数高效微调方法，用于大规模语言模型，这些模型预先使用医学知识进行训练，以应对数据限制问题。  我们的多视图和多尺度对齐（MaMA）方法，在两个大型真实世界乳腺X光数据集EMBED和RSNA-Mammo上，对于三种不同的任务，相较于最先进的基线方法取得了显著性能提升，同时相比最大的基线模型，仅使用了52%的模型大小。|
|**2024-09-26**|**E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding**|Ye Liu et.al.|[2409.18111](http://arxiv.org/abs/2409.18111)|**[link](https://github.com/PolyU-ChenLab/ETBench)**|**为了验证视频大语言模型（Video Large Language Models, Video-LLMs）在通用视频理解中的巨大潜力，已提出了一系列基准测试来诊断模型在不同场景下的能力。然而，现有的基准测试仅通过视频级问题回答进行评估，缺乏对事件级别的精细评估和任务多样性。为了填补这一空白，我们引入了E.T. Bench（事件级别与时间敏感的视频理解基准），这是一个针对开放式的事件级别视频理解的大规模、高质量基准测试。  E.T. Bench按照三层任务分类体系进行组织，包含了涵盖12个任务的7300个样本，以及8个领域的2514小时总时长的7000个视频，提供了全面的评估。我们广泛地对8个图像大语言模型和12个视频大语言模型进行了评估，并且结果显示，用于粗粒度（视频级）理解的最先进的模型在解决我们的精细粒度任务时表现不佳，例如在视频中定位感兴趣的事件，主要原因是视频上下文长度短、时间表示不当以及缺乏多事件训练数据。针对这些问题，我们进一步提出了一个强大的基线模型——E.T. Chat，以及专门为精细粒度事件理解设计的指令调优数据集E.T. Instruct 164K。我们的简单但有效的解决方案在多个场景中表现出优越的性能。**|
|**2024-09-26**|**Infering Alt-text For UI Icons With Large Language Models During App Development**|Sabrina Haque et.al.|[2409.18060](http://arxiv.org/abs/2409.18060)|null|确保移动应用的无障碍性仍然是一个重大挑战，尤其是对于依赖屏幕阅读器的视障用户。界面图标对于导航和互动至关重要，但往往缺乏有意义的替代文本，从而形成使用障碍。传统的深度学习方法在生成替代文本时需要大量数据集，并且在图标类型多样性与不平衡性方面存在困难。更近期的视觉语言模型（VLMs）则要求完整的UI屏幕，这在应用程序开发的迭代阶段可能不切实际。为了应对这些问题，我们引入了一种新的方法，使用大型语言模型（LLMs）通过部分UI数据自主生成移动UI图标的描述性替代文本。通过整合包括类别、资源ID、边界、OCR检测到的文字以及父节点和同级节点的上下文信息在内的图标上下文，我们对大约1400个图标的小型数据集进行离线微调，从而生成了IconDesc。在实证评估和用户研究中，IconDesc显著提高了生成相关替代文本的能力。这一能力使得IconDesc成为开发者的重要工具，帮助他们快速迭代和提升UI的无障碍性。|
|**2024-09-26**|**DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving**|Dingrui Wang et.al.|[2409.18053](http://arxiv.org/abs/2409.18053)|**[link](https://github.com/TUM-AVS/DualAD)**|我们提出了一种新型自主驾驶框架DualAD，旨在模仿人类在驾驶过程中的决策逻辑。DualAD由两层构成：底层为基于规则的运动规划器，负责处理需要较少决策的常规驾驶任务；上层则配备了一个基于规则的文字编码器，将绝对状态下的驾驶场景转化为文本描述。此文本随后由大型语言模型（LLM）进行决策。当检测到潜在危险时，上层会介入底层的决策过程，以模仿人类在关键情况下的决策逻辑。闭合环路实验显示，使用零训练预训练模型的DualAD显著优于缺乏决策能力的基于规则的运动规划器。我们的实验还强调了文字编码器的有效性，它极大地增强了模型对场景的理解能力。此外，集成的DualAD模型随着更强大的LLM的使用而得到改善，这表明该框架具有进一步增强的潜力。我们提供代码和基准测试供公众访问。|
|**2024-09-26**|**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**|Kai Chen et.al.|[2409.18042](http://arxiv.org/abs/2409.18042)|null|在开放源代码社区中，让大型语言模型能够以公开数据进行端到端的图像、文本和语音生成仍然具有挑战性。现有的视语模型依赖于外部工具进行语音处理，而语音语模型仍缺乏视觉理解能力。为了填补这一缺口，我们提出了EMOVA（情绪化的全模式语音助手），以使大型语言模型具备端到端的语音能力，同时保持领先的视语表现。通过语义-声学分离的语音编码器，我们意外地发现，全模态对齐可以进一步增强视语和语音能力，与相应的双模态对齐模型相比。此外，我们还提出了一种轻量级风格模块，用于灵活控制语音风格（例如情感和音调）。首次，EMOVA在视语和语音基准测试中均达到了最先进的性能，并同时支持带有生动情感的全模态对话。|
|**2024-09-26**|**Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective**|Yotam Wolf et.al.|[2409.18028](http://arxiv.org/abs/2409.18028)|null|在进行复杂分析任务（如代码生成）的大型语言模型（LLM）使用中，通常会将整个任务的解决方案在模型的上下文窗口内进行采样。先前的研究表明，在模型的上下文中分解任务（即链式思维）对于解决这类任务是有益的。本文指出了一种限制，即LLM在同一个上下文窗口内执行多个子任务的能力——一种“复合难度”。这表明在LLM组成的多智能体系统中将分解后的问题分发处理具有优势。我们通过生成复杂度指标来量化这种复合难度，即在采样到至少一个正确解所需的LLM生成次数。我们发现，相对于在相同上下文内解决组合问题，将问题分散给多个智能体的生成复杂度之间存在差距，并且随着解长度的增加，这个差距呈指数增长。我们通过理论证明和实验证明了这一结果。|
|**2024-09-26**|**An Adversarial Perspective on Machine Unlearning for AI Safety**|Jakub Łucki et.al.|[2409.18025](http://arxiv.org/abs/2409.18025)|**[link](https://github.com/ethz-spylab/unlearning-vs-safety)**|本文探讨了大型语言模型在拒绝危险知识相关问题方面的微调方式，但这些防护措施往往容易被绕过。去学习方法旨在彻底消除模型的危险能力并使其对攻击者不可访问。本文从对抗性视角挑战了去学习与传统安全后训练之间的基本差异。我们证明了之前被认为无效的现有逃脱方法，在精心应用时可以成功应对去学习。此外，我们开发了一系列适应性方法来恢复大部分被认为是无法学习的能力。例如，我们展示了使用RMU（当前最先进的去学习方法）编辑模型后，通过在无关示例上进行微调或在激活空间中移除特定方向，可以恢复大部分危险能力。我们的发现质疑了当前去学习方法的稳健性，并对它们相对于安全训练的优势提出了疑问。|
|**2024-09-26**|**DARE: Diverse Visual Question Answering with Robustness Evaluation**|Hannah Sterz et.al.|[2409.18023](http://arxiv.org/abs/2409.18023)|null|《DARE：多样化的视觉问答与鲁棒性评估》论文摘要翻译如下：  本文引入了DARE（Diverse Visual Question Answering with Robustness Evaluation），一个精心设计并收集的多选型视觉问答基准。DARE旨在评估大型语言模型在视觉语言推理任务中的表现，特别是在五个不同类别的视觉问题上，并包括基于提示变化、答案选项子集、输出格式和正确答案数量等四个鲁棒性导向评估的全面评估。  研究发现，当前最先进的视觉语言模型在大多数类别中仍然面临挑战，且无法在测试的所有鲁棒性评估中保持一致的高性能。在不同答案选项子集的情况下，最差情况下的性能下降可达标准情况下的34%。开源模型如LLaVA 1.6和Idefics在鲁棒性方面无法与闭源模型GPT-4和Gemini相匹敌，而后者在不同变体下仍表现出明显的脆弱性。  总之，该研究揭示了视觉语言模型在处理视觉推理任务时所面临的局限性，并强调了在设计更鲁棒的模型时需要考虑的问题。|
|**2024-09-26**|**Role-RL: Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles**|Lewei He et.al.|[2409.18014](http://arxiv.org/abs/2409.18014)|null|针对长文本上下文处理的大型语言模型（LLM）仍然存在实现复杂性、训练效率和数据稀疏性等挑战。为此，我们提出了一个新范式——在线长期上下文处理（OLP），适用于处理无限长度的文档，常见于自动化新闻报道、直播电商和病毒短视频等多样化的流媒体信息接收与组织场景。同时，在选择众多性能优异、价格适中且响应延迟短的LLM时，往往遇到难以抉择的问题。鉴于此，我们开发了角色强化学习（Role-RL）框架，自动部署不同角色的LLM在OLP管道中，根据其实际性能进行合理分配。  我们进行了大量的实验，并在我们的OLP-MINI数据集上发现，结合Role-RL框架的OLP系统平均召回率为93.2%，实现了OLP基准，并节省了79.4%的LLM成本。相关代码和数据集已公开发布：https://anonymous.4open.science/r/Role-RL。|
|**2024-09-25**|**Attention Prompting on Image for Large Vision-Language Models**|Runpeng Yu et.al.|[2409.17143](http://arxiv.org/abs/2409.17143)|**[link](https://github.com/yu-rp/apiprompting)**|**与大型语言模型（LLM）相比，大型视觉-语言模型（LVLM）还能接受图像作为输入，因此展示了更多有趣的现象级能力，并在各种视觉-语言任务上表现出令人印象深刻的表现。受LLM中文本提示的启发，探索了增强LVLM对视觉信息感知能力的视觉提示技术。然而，以往的视觉提示技术仅处理视觉输入而不考虑文本查询，限制了模型遵循文本指令完成任务的能力。为了填补这一空白，本工作提出了一个名为“注意力映射上的图像提示”的新提示技术，该技术简单地在原始输入图像上叠加了一个由辅助模型（如CLIP）生成的、依赖于文本查询的注意力热图，并有效地增强了LVLM在各种任务上的表现。具体来说，我们通过一个辅助模型（如CLIP）为输入图像生成一个依赖于文本查询的注意力热图。然后，热图简单地乘以原始图像的像素值来获得实际输入图像供LVLM使用。在各种视觉-语言基准测试上的广泛实验验证了我们技术的有效性。例如，“注意力映射上的图像提示”分别提高了LLaVA-1.5在MM-Vet和LLaVA-Wild基准上的性能3.8%和2.9%。**|
|**2024-09-25**|**FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression**|Fazal Mittu et.al.|[2409.17141](http://arxiv.org/abs/2409.17141)|**[link](https://github.com/fazalmittu/finezip)**|**本文深入分析了基于神经网络与Transformer的文本压缩技术，并将其与传统文本压缩系统进行对比。尽管基于大型语言模型（LLM）的系统在压缩比上显著优于传统方法，但它们在实用性方面却极为有限。以Llama3-8B为基础的LLM压缩系统——LLMZip，在压缩仅10MB文本时需要9.5天的时间，尽管压缩效果有所提升。  为解决这一问题，我们提出了FineZip——一种结合在线记忆与动态上下文概念的新型LLM文本压缩系统。FineZip相较于LLMZip，将压缩时间大幅缩短至约4小时，性能提升了54倍，且与传统算法压缩方法相比，其压缩效率提高了大约50%。通过本研究，我们迈出了让基于LLM的无损文本压缩成为现实的第一步。尽管FineZip已取得显著进展，但LLM仍不适用于大规模文本压缩。我们期待本文的研究和创新能为未来解决这一问题铺平道路。**|
|**2024-09-25**|**Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents**|Junting Lu et.al.|[2409.17140](http://arxiv.org/abs/2409.17140)|null|本文提出了一种名为AXIS的新型基于语言模型的代理框架，该框架通过应用程序编程接口（API）优先处理操作而非用户界面（UI）操作，以解决大型语言模型（LLM）驱动的代理在复杂任务中的高延迟和低可靠性问题。此外，AXIS框架还通过自动化探索应用程序的方式促进了API的创建与扩展。  在Office Word应用上的实验结果表明，与人类相比，AXIS在任务完成时间上缩短了65%-70%，认知负荷降低了38%-53%，同时保持了97%-98%的准确性。这项工作为人类、代理和计算机交互（HACI）框架以及应用程序提供者在LLM时代的新UI设计原则做出了贡献。它也探讨了将每个应用程序转化为代理的可能性，为代理为中心的操作系统（Agent OS）铺平了道路。|
|**2024-09-25**|**Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale**|Fan Zhou et.al.|[2409.17115](http://arxiv.org/abs/2409.17115)|**[link](https://github.com/gair-nlp/prox)**|**在大型语言模型预训练领域，人们长期以来依赖于人类专家制定提升数据质量的启发式规则，至今已发展出众多规则。然而，这些规则缺乏灵活性，无法有效针对每个实例的独特特性进行调整。同时，为每个实例应用定制规则对于人类专家而言是不切实际的。本文展示了即使是参数数量仅有0.3B的语言模型，也能展现出与人类专家相当的数据优化能力。我们引入了“编程每例”（ProX）框架，该框架将数据优化视为编程任务，允许模型通过生成并执行精细粒度的操作（如字符串规范化）对每个个体实例进行大规模优化。  实验结果表明，使用ProX筛选后的数据预训练的模型，在各种下游基准测试中均优于原始数据或由其他筛选方法处理的数据，性能提升超过2%。该框架的有效性适用于不同规模的模型和预训练数据集，包括C4、RedPajama-V2和FineWeb。此外，ProX在特定领域的连续预训练中表现出巨大潜力：在无需特定领域设计的情况下，使用ProX优化的OpenWebMath数据预训练的模型，在准确性上分别比Mistral-7B、Llama-2-7B和CodeLlama-7B提高了7.6%、14.6%和20.3%，仅使用约10B令牌即可达到类似于使用200B令牌预训练的Llama-7B模型的水平。进一步的分析显示，ProX显著节省了训练FLOPs，为高效LLM预训练开辟了有前景的道路。  我们公开发布了ProX，包括>100B的语料库、模型以及所有训练和实现细节，以促进可复制研究和未来创新。代码：https://github.com/GAIR-NLP/ProX**|
|**2024-09-25**|**Accumulator-Aware Post-Training Quantization**|Ian Colbert et.al.|[2409.17092](http://arxiv.org/abs/2409.17092)|null|近年来的研究已经探索了低精度累加，报告了在不同平台上的吞吐量、功率和面积的改进。然而，这些提议仅考虑了量化感知训练（QAT）范式，在该范式中，模型在量化循环中进行微调或从头开始训练。随着模型继续增大，QAT技术的成本变得越来越高，这激发了最近对后量化量化（PTQ）研究的热潮。据我们所知，这是首次正式研究PTQ背景下的积算器感知量化。为了填补这一空白，我们引入了AXE，一个旨在赋予现有层式PTQ算法溢出避免保证的实用框架的扩展。我们通过在两个最先进的PTQ算法：GPFQ和OPTQ之上实现AXE来理论地推动AXE，并证明其灵活性。进一步地，我们通过首次支持多阶段积累来一般化AXE，为全数据路径优化和大型语言模型（LLMs）的扩展打开大门。我们在图像分类和语言生成模型上评估了AXE，并观察到与基线方法相比，在积算器位宽与模型准确性的权衡上取得了显著改进。|
|**2024-09-25**|**VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models**|Yifei Liu et.al.|[2409.17066](http://arxiv.org/abs/2409.17066)|**[link](https://github.com/microsoft/vptq)**|**本文介绍了一种名为Vector Post-Training Quantization（VPTQ）的低比特量化方法，针对大型语言模型（LLMs）。通过使用二次优化来定义LLM向量量化问题，并通过解决优化问题来指导量化算法设计。进一步地，引入了通道独立的二次优化以实现精细化量化。同时，通过分解优化问题，提出了简明有效的代码本初始化算法。此外，VPTQ还扩展了残差和异常值量化支持，这不仅提高了模型精度，还能进一步压缩模型。  实验结果表明，与SOTA相比，在2比特量化时，VPTQ将模型量化困惑度降低0.01-0.34，Mistral-7B上为0.38-0.68，LLaMA-3上为4.41-7.34。在问答任务上的平均准确度提升范围为LLaMA-2上的0.79%-1.5%，Mistral-7B上的1%，以及LLaMA-3上的11%-22%。量化算法执行时间仅占10.4%-18.6%，导致推理吞吐量提高1.6-1.8倍。**|
|**2024-09-25**|**Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia**|Azmul Asmar Irfan et.al.|[2409.17054](http://arxiv.org/abs/2409.17054)|null|本文提出了一种解决方案，利用本地化大型语言模型（LLM）来转录、翻译和总结医生与患者的对话。我们使用Whisper模型进行转录，GPT-3进行总结，并将其格式化为ePuskemas医疗记录。此系统作为现有网络浏览器扩展的附加组件实现，允许医生在说话时填写患者表格。通过利用实时转录、翻译和总结功能，医生可以提高患者护理的周转时间，同时增强记录的质量，使得记录更加详细且富有洞察力，以供未来的访问参考。这一创新旨在解决印尼医疗机构拥挤以及医护人员行政负担重的问题。我们相信，这种解决方案将帮助医生节省时间、提供更好的护理并产生更准确的医疗记录，代表了向现代化医疗保健迈进的重要一步，确保即使在资源有限的环境中，患者也能获得及时、高质量的护理。|
|**2024-09-25**|**How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not**|Francesco Verdini et.al.|[2409.17044](http://arxiv.org/abs/2409.17044)|null|大型语言模型（LLM）的惊人表现推动了研究努力，使其能够应用于一系列任务和输入模态。在语音转文本（S2T）任务中，新兴的解决方案是通过适配器模块将语音基础模型（SFM）的输出投影到LLM嵌入空间。然而，目前还没有工作探讨下游任务性能在多大程度上依赖于每个组件（SFM、适配器、LLM），或者选择适配器的最佳设计是否取决于所选的SFM和LLM。为了填补这一空白，我们评估了5个适配器模块、2个LLM（Mistral和Llama）以及2个SFM（Whisper和SeamlessM4T）在自动语音识别和语音翻译两个广泛使用的S2T任务上的组合效果。我们的结果表明，SFM在下游性能中扮演着至关重要的角色，而适配器的选择具有适度的影响，并且取决于所选的SFM和LLM。|
|**2024-09-25**|**Counterfactual Token Generation in Large Language Models**|Ivi Chatzi et.al.|[2409.17027](http://arxiv.org/abs/2409.17027)|**[link](https://github.com/networks-learning/counterfactual-llms)**|本文旨在提升大型语言模型的功能，使其能够推理过去生成的令牌所呈现的可能替代情况。我们开发了一种基于Gumbel-Max结构因果模型的因果模型，以增强大型语言模型的这一功能。我们的模型能够在几乎不增加与基础令牌生成成本的情况下，进行反事实令牌生成，实现过程简单且无需任何微调或提示工程。我们在此基础上在Llama 3 8B-instruct上实现了该模型，并对生成的反事实文本进行了定性和定量分析。  此外，我们还探讨了反事实令牌生成在偏见检测方面的应用，揭示了大型语言模型构建的世界模型中的一些有趣见解。|
|**2024-09-25**|**LLM-CARD: Towards a Description and Landscape of Large Language Models**|Shengwei Tian et.al.|[2409.17011](http://arxiv.org/abs/2409.17011)|**[link](https://github.com/shengwei-tian/dependency-parser-visualization)**|随着自然语言处理（NLP）领域的迅速发展，大型语言模型（LLMs）在各种NLP任务中不断涌现。随着发表的论文数量不断增加，研究人员和开发者面临信息过载的挑战。因此，开发一个能够自动从学术论文中提取并组织LLM关键信息的系统变得尤为重要。本工作旨在通过使用命名实体识别（NER）和关系抽取（RE）方法来实现这一目标，这些方法可以自动从论文中提取关于大型语言模型的关键信息，帮助研究人员高效地获取关于LLMs的信息。这些特性包括模型的“许可”、“名称”和“应用”。借助这些特性，我们可以为每篇论文形成一个模型卡片。在数据贡献方面，对106篇学术论文进行了处理，定义了三个字典——LLMs名称、许可和应用。通过字典查找提取了11051个句子，并通过人工审查最终选择了129个句子，其中包含名称与许可之间的链接，以及106个句子，其中包含模型名称与应用之间的链接。|
|**2024-09-20**|**Gender Representation and Bias in Indian Civil Service Mock Interviews**|Somonnoy Banerjee et.al.|[2409.12194](http://arxiv.org/abs/2409.12194)|null|本文提出了三个关键贡献。首先，通过收集自888个印度公务员候选人面试模拟的YouTube视频中的51,278个问题样本，我们展示了对男性和女性候选人提问的性别偏见在广泛性质上的显著存在。第二，我们的大型语言模型实验揭示了在性别推断任务中，这些模型提供的解释中存在强烈的性别偏见。最后，我们提供了一个包含51,278个面试问题的新型数据集，这可以为未来的人文社会科学研究提供信息。|
|**2024-09-18**|**To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning**|Zayne Sprague et.al.|[2409.12183](http://arxiv.org/abs/2409.12183)|**[link](https://github.com/zayne-sprague/to-cot-or-not-to-cot)**|为了分析链式思考（CoT）在哪些任务中真正有益，我们进行了一项量化元分析，覆盖了超过100篇使用CoT的论文，并对20个数据集进行了14种模型的自我评估。结果表明，CoT主要在数学或逻辑任务上提供显著性能优势，而在其他类型任务上的增益较小。在MMLU上，直接生成答案而无需CoT几乎与CoT具有相同的准确性，除非问题或模型的回答包含等号，这表明符号操作和推理。  基于这一发现，我们分析了CoT在这些问题中的行为，通过分离规划和执行，并与增强工具的大型语言模型进行比较。CoT大部分收益来自改进的符号执行，但相较于使用符号求解器，它在性能上表现不佳。我们的结果表明，可以根据需要应用CoT，同时保持性能并节省推理成本。此外，这些结果还表明，需要超越基于提示的CoT，转向新的范式，更好地利用整个范围内的大型语言模型应用中的中间计算。|
|**2024-09-18**|**Finetuning Language Models to Emit Linguistic Expressions of Uncertainty**|Arslan Chaudhry et.al.|[2409.12180](http://arxiv.org/abs/2409.12180)|null|本文研究了大型语言模型（LLM）在信息检索与决策任务中的应用。尽管LLM具有广泛的应用价值，但它们倾向于生成与现实世界事实相冲突的信息，并以说服性的方式表达，使得这些不准确性看起来自信且令人信服。这导致最终用户难以一致地将LLM的自信度与预测的准确性对齐，常常导致对所有输出的盲目信任或完全忽视其可靠性。为此，我们探索了在不确定性增强的预测基础上进行监督微调的方法，以此来开发能够生成语言不确定性表述的模型。具体而言，我们衡量预训练模型的校准程度，然后通过基于模型自身信心的微调，使语言模型产生校准的不确定性表述。通过对各种问答数据集的实验，我们证明了LLM在评估预测时具有良好的校准能力，并基于模型本身的信心进行监督微调，可获得特别适用于单个声明答案的良好校准的不确定性表述。|
|**2024-09-18**|**Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference**|Najmeh Forouzandehmehr et.al.|[2409.12150](http://arxiv.org/abs/2409.12150)|null|本文提出了一种新颖的框架，利用大型语言模型（LLM）的强大表达能力来解决个性化服装推荐这一复杂挑战。通过细调和直接反馈集成，我们试图克服LLM的“黑盒”特性和静态性。我们通过在人类编目的时尚图像上使用多模态大型语言模型（MLLM）进行图像描述，来弥合项目视觉与文本之间的差距。这使得LLM能够从人类编目的时尚图像中提取风格和色彩特征，从而形成个性化的推荐基础。我们使用开源的Polyvore数据集对LLM进行高效细调，优化其推荐时尚搭配的能力。采用直接偏好机制并结合负例，以增强LLM的决策过程。这创建了一个自我增强的人工智能反馈循环，持续地根据季节性时尚趋势优化推荐。我们的框架在Polyvore数据集上进行了评估，针对两个关键任务：补全空白和辅助项目检索。这些评估结果强调了框架生成时尚、与潮流一致的服装建议的能力，并通过直接反馈持续改进。评估结果显示，我们的提议框架在这些任务上的表现显著优于基于原始LLM的服装生成，创造了更加协调的服装。改进的表现证明了该框架增强购物体验、提供准确建议的潜力，证明了它相对于基于原始LLM的服装生成方法的有效性。|
|**2024-09-18**|**MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning**|Justin Chih-Yao Chen et.al.|[2409.12147](http://arxiv.org/abs/2409.12147)|**[link](https://github.com/dinobby/magicore)**|**大型语言模型（LLM）的推理能力可以通过在测试时采用聚合策略进行提升，即生成多个样本并基于生成样本进行投票。虽然这些策略能够提高性能，但它们往往存在饱和点。改进方法引入了一种名为“Refinement”的策略，通过利用LLM生成的反馈来提升解决方案的质量。然而，Refinement也带来了三个关键挑战：（1）过度细化：对所有实例进行统一细化可能导致过度修正，从而降低整体性能。（2）难以定位和纠正错误：LLM具有有限的自我纠正能力，很难识别并纠正自己的错误。（3）细化不足：决定需要多少迭代的细化并不容易，过早停止可能会让错误未得到解决。  为了应对这些问题，我们提出了一种名为MAgICoRe的方法，它通过将问题难度分为简单或困难，并使用粗粒度聚合解决简单问题，使用细粒度和多轮迭代细化解决困难问题，以避免过度细化。为了改善错误定位，我们引入了基于步骤级奖励模型（RM）分数的外部评分。此外，我们采用了一个由三个代理组成的多代理循环：求解者、审查者（根据步骤级RM分数生成针对性反馈）以及细化者（整合反馈），以确保有效细化。为了确保足够的细化，我们重新评估更新后的解决方案，并在必要时启动进一步的细化轮次。我们使用Llama-3-8B和GPT-3.5在5个数学数据集上评估了MAgICoRe，并展示了其有效性。即使只进行一次迭代，MAgICoRe也能在使用不到基线样本一半的情况下，分别超过Self-Consistency、Best-of-k和Self-Refine算法3.4%、3.2%和4.0%。与迭代细化的基线相比，MAgICoRe随着迭代次数的增加持续提高性能。最后，我们的消融实验强调了MAgICoRe中RMs和多代理通信的重要性。**|
|**2024-09-18**|**MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion**|Kalakonda Sai Shashank et.al.|[2409.12140](http://arxiv.org/abs/2409.12140)|null|我们提出了一种名为MoRAG的创新多部分融合检索增强生成策略，用于基于文本的人体动作生成。此方法通过利用增强的运动检索过程获得的额外知识来提升运动扩散模型。通过有效激发大型语言模型（LLM），我们解决了运动检索中的拼写错误和重述问题。我们的方法采用多部分检索策略以提高运动检索在语言空间上的泛化能力。我们通过空间组合检索到的动作来生成多样化的样本。此外，利用低级、特定部分的运动信息，我们可以构建针对未见过文本描述的运动样本。我们的实验结果表明，我们的框架可以作为插件模块使用，以提高运动扩散模型的性能。代码、预训练模型和视频示例将在以下网址提供：https://motion-rag.github.io/|
|**2024-09-24**|**Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models**|Sijing Chen et.al.|[2409.12139](http://arxiv.org/abs/2409.12139)|null|随着大数据和大型语言模型时代的到来，零样本个性化快速定制已成为一个显著趋势。本报告介绍了Takin AudioLLM系列技术与模型，主要包括Takin TTS、Takin VC和Takin Morphing，专门用于有声读物制作。这些模型具备零样本语音生成能力，能产生几乎与真人声音难以区分的高质量语音，使得个人可以根据自身需求定制语音内容。  首先，我们介绍Takin TTS，这是一种基于增强神经语音编解码器和多任务训练框架的神经编解码语言模型，能够以零样本方式生成高保真自然语音。对于Takin VC，我们提出了一种有效的内容与音色联合建模方法来提高说话人相似度，并倡导基于条件流匹配的解码器进一步提升其自然性和表达力。最后，我们提出了Takin Morphing系统，该系统采用高度解耦且先进的音色与节奏建模方法，使个体能够以精确可控的方式根据自己的偏好定制语音生产。广泛实验验证了我们Takin AudioLLM系列模型的有效性和鲁棒性。有关详细演示，请参阅<https://everest-ai.github.io/takinaudiollm/>。|
|**2024-09-18**|**Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement**|An Yang et.al.|[2409.12122](http://arxiv.org/abs/2409.12122)|null|在本报告中，我们介绍了系列数学专用大型语言模型：Qwen2.5-Math 和 Qwen2.5-Math-Instruct-1.5B/7B/72B。Qwen2.5 系列的核心创新在于在整个管道中融入自我提升的哲学，包括预训练、后处理和推理阶段：（1）在预训练阶段，使用 Qwen2-Math-Instruct 来生成大规模高质量的数学数据。（2）在后处理阶段，我们通过从 Qwen2-Math-Instruct 进行大量采样来开发奖励模型（RM）。然后，我们将此 RM 应用于监督微调（SFT）的迭代进化。通过增强的 SFT 模型，有可能进行迭代训练并更新 RM，进而指导 SFT 数据的下一轮迭代。在最终的 SFT 模型上，我们采用终极 RM 进行强化学习，从而产生 Qwen2.5-Math-Instruct 模型。（3）此外，在推理阶段，使用 RM 来引导采样，优化模型性能。  Qwen2.5-Math-Instruct 支持中文和英文，并具有高级数学推理能力，包括链式思考（CoT）和工具集成推理（TIR）。我们在英语和中文的 10 个数学数据集上评估了我们的模型，如 GSM8K、MATH、GaoKao、AMC23 和 AIME24，涵盖从小学水平到数学竞赛问题的广泛难度。|
|**2024-09-18**|**Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference**|Edresson Casanova et.al.|[2409.12117](http://arxiv.org/abs/2409.12117)|null|大型语言模型（LLM）在通过将音频转换为离散令牌的音频编解码器方面显著推动了音频处理，这使得可以将语言建模技术应用于音频数据。然而，音频编解码器通常以高帧率运行，导致训练和推理速度缓慢，特别是在自回归模型中。为了应对这一挑战，我们提出了低帧率语音编解码器（LFSC）：一种神经音频编解码器，它利用有限标量量化和与大型语音语言模型的对抗性训练，以1.89 kbps的比特率和21.5帧/秒实现高质量的音频压缩。我们证明，我们的新型编解码器可以使基于LLM的文本到语音模型的推理速度加快约三倍，同时提高可懂度并产生与以往模型相当的质量。|
|**2024-09-18**|**Measuring Human and AI Values based on Generative Psychometrics with Large Language Models**|Haoran Ye et.al.|[2409.12106](http://arxiv.org/abs/2409.12106)|**[link](https://github.com/value4ai/gpv)**|**本文引入了基于大型语言模型（LLM）的生成心理测度（GPV），这是一种数据驱动的价值测量范式，理论基础在于文本揭示的选择性感知。首先，我们对LLM进行微调以实现精确的感知层级价值测量，并验证LLM解析文本形成感知的核心能力，从而构建GPV管道的基础。然后，我们将GPV应用于人类撰写的博客，证明其稳定性和有效性，并且优于先前的心理学工具。接着，我们将GPV扩展到LLM价值测量，通过以下方式推动当前技术：1）提出了一种基于LLM可扩展和自由形式输出的量化方法，使价值测量能够针对特定情境；2）比较了不同测量方法，揭示了前人方法的回应偏差；3）尝试将LLM价值与安全性联系起来，发现不同价值体系的预测力，并分析各种价值对LLM安全性的影响。通过跨学科努力，本文旨在利用AI推动下一代心理测度的发展，并利用心理测度促进价值导向的AI。**|
|**2024-09-17**|**AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs**|Basel Mousi et.al.|[2409.11404](http://arxiv.org/abs/2409.11404)|null|阿拉伯语，以其丰富的方言多样性，仍然在大型语言模型中显著被低估，尤其是在方言变体方面。我们通过使用机器翻译结合人工后编辑创建的七个人工合成数据集来填补这一空白，这些数据集涵盖了现代标准阿拉伯语（MSA）以及阿拉伯各地区的方言。我们提出了AraDiCE基准，用于评估阿拉伯方言和文化理解与生成能力。我们的研究侧重于低资源阿拉伯方言，并对其进行了评价。  此外，我们首次引入了一个细粒度基准，专门用于评估阿拉伯半岛、埃及和黎凡特地区之间的文化意识，为LLM评估提供了新的维度。我们的发现表明，尽管针对特定阿拉伯语模型如Jais和AceGPT在方言任务上优于多语言模型，但在方言识别、生成和翻译方面仍存在重大挑战。这项工作贡献了约4.5万个经过人工后编辑的样本、一个文化基准，并强调了根据特定训练来改善大型语言模型捕捉不同阿拉伯方言和文化背景细微差异的重要性。我们将发布在本研究中构建的方言翻译模型和基准。|
|**2024-09-17**|**NVLM: Open Frontier-Class Multimodal LLMs**|Wenliang Dai et.al.|[2409.11402](http://arxiv.org/abs/2409.11402)|null|我们引入了NVLM 1.0，这是一个在视觉语言任务上达到前沿水平的多模态大型语言模型家族，其性能与顶级专有模型（如GPT-4o）和开源模型（如Llama 3-V 405B和InternVL 2）相匹敌。令人惊讶的是，NVLM 1.0在多模态训练后，在仅文本任务上的表现甚至超过了其背后的语言模型基础架构。  在模型设计方面，我们对解码器型多模态语言模型（如LLaVA）和交叉注意力型模型（如Flamingo）进行了全面比较。基于这两种方法的优势和劣势，我们提出了一种新型架构，以提高训练效率和多模态推理能力。此外，我们引入了一种用于动态高分辨率图像的1-D瓷砖标记设计，这显著提高了多模态推理和OCR相关任务的性能。  关于训练数据，我们精心收集并提供了所有架构的预训练和监督微调数据集的详细信息。我们的发现表明，在预训练阶段，数据质量和任务多样性比规模更为重要。值得注意的是，我们为NVLM-1.0模型开发了生产级多模态功能，使它们在视觉语言任务中不仅保持甚至超越了基础语言模型的性能。为了实现这一目标，我们在多模态训练中巧妙地整合了一个高质量的纯文本数据集，以及大量的多模态数学和推理数据，从而在所有模态下提高了数学和编码能力。  为了推动领域研究，我们将发布模型权重并开源代码供社区使用：https://nvlm-project.github.io/。|
|**2024-09-17**|**Says Who? Effective Zero-Shot Annotation of Focalization**|Rebecca M. M. Hicke et.al.|[2409.11390](http://arxiv.org/abs/2409.11390)|null|在这篇论文中，我们通过实验测试了当前大型语言模型（LLMs）在为文学文本标注焦点模式时的表现。尽管任务具有挑战性，但我们的实验结果表明，LLMs在这一任务上的表现与受过训练的人类注释者相当。我们以斯蒂芬·金的小说为例进行案例研究，展示了这种方法在计算文学研究中的实用性，说明了如何大规模地研究焦点模式。|
|**2024-09-17**|**Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**|Simon Yu et.al.|[2409.11378](http://arxiv.org/abs/2409.11378)|**[link](https://github.com/for-ai/iterative-data-selection)**|细调大规模语言模型在指令数据上的能力对于增强预训练知识和提升指令遵循能力至关重要。随着指令数据集的不断增多，选择有效的数据进行有效训练变得越来越重要。本文探讨了如何确定有效训练的最佳数据子集。现有研究往往侧重于实例质量等局部标准进行子集选择，但我们认为全局视角关注数据多样性更为关键。我们采用k均值聚类方法确保所选子集充分代表整个数据集。  我们提出了一种启发自主动学习技术的迭代优化方法，用于从各个聚类中重新采样实例，并在每一次训练迭代中重新评估每个聚类的重要性和采样权重。这种方法能够降低异常值的影响并自动筛选出包含低质量数据的聚类。通过在自然语言推理、一般世界知识、代码和数学推理任务上进行广泛评估，并对各种模型家族进行微调，我们观察到一致性改进，相比于随机选择提高了7%，相较于最先进的采样方法提高了3.8%。我们的工作强调了在微调大型语言模型以增强广泛的评估任务性能时，优先考虑多样性的采样方法的重要性。  我们的代码已开源在https://github.com/for-ai/iterative-data-selection。|
|**2024-09-17**|**Towards Time Series Reasoning with LLMs**|Winnie Chow et.al.|[2409.11376](http://arxiv.org/abs/2409.11376)|null|多模态大型语言模型（MLLMs）在视觉等领域的理解和推理方面取得了重大进展，但时间序列领域尚未看到这种广泛的成功。尽管先前的时间序列MLLM研究在时间序列预测中显示出有希望的表现，但很少有工作展示了如何使用大语言模型进行自然语言的时间序列推理。我们提出了一种新颖的多模态时间序列LLM方法，该方法能够跨各种领域学习通用信息，并具有强大的零样本性能。  首先，我们在LLM顶部训练一个轻量级时间序列编码器，直接提取时间序列信息。然后，我们通过增强的时间序列任务对模型进行微调，以鼓励模型生成推理路径。我们的研究表明，模型学习到的潜在表示反映了特定的时间序列特征（例如斜率、频率），并且在多种领域的一系列零样本推理任务上均优于GPT-4o。|
|**2024-09-17**|**Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**|Fatema-E- Jannat et.al.|[2409.11375](http://arxiv.org/abs/2409.11375)|null|在医疗领域中，获取大量数据面临着显著的挑战，主要是由于隐私问题。然而，为了训练用于视网膜疾病诊断的深度学习模型，需要大量的数据集。在较小数据集上有效泛化的能力仍然是一个持续的挑战。数据稀缺性构成了实施可扩展医疗AI解决方案的实际障碍。  为了解决这个问题，我们结合了多种数据源，以提高性能并增强对新数据的泛化能力，通过赋予模型从多模态数据集中更深入理解数据表示的能力。我们基于大型语言模型（LLMs）和SwinV2框架开发了一个自监督框架，以增强模型对多模态数据集表示的理解，从而提高使用光学相干断层成像（OCT）图像检测眼病的能力。  我们采用了两阶段训练方法，即自监督预训练和下游监督分类器的微调。针对三种不同数据集进行的消融研究，在未融合数据、数据量有限设置和无自监督预训练场景下采用不同的编码器架构，强调了我们方法的稳健性。我们的发现表明，即使在这些多样化的条件下，也表现出一致的性能，并且与基线模型ResNet-50相比，具有更强的泛化能力。|
|**2024-09-17**|**CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration**|Jiahui Gao et.al.|[2409.11365](http://arxiv.org/abs/2409.11365)|null|本文探讨了多模态大型语言模型（MLLM）在面对恶意视觉输入时的安全意识问题。MLLM通常基于大型语言模型构建，并配以图像编码器将图像转换为与人类价值观相一致的文本数据集中的令牌嵌入空间。然而，这种视觉模态的整合引入了一种独特的脆弱性：MLLM对恶意图像输入变得敏感，并倾向于生成可能引发安全或有害响应的输出。  研究发现，通过在MLLM的输入中加入一个原则，以明确定义安全性要求，其安全意识得到了增强。这证实了MLLM在处理图像输入时具有一定的安全意识，但这一能力受到模态差距的影响而减弱。  为此，本文提出了一种简单而有效的技术——CoCA（Calibration of Conditional Awareness），旨在通过调整输出分布来增强MLLM的安全意识。该策略有助于模型恢复其原始的安全意识，同时不牺牲其原有能力。通过在多模态安全性和理解基准上验证了这种方法的有效性。|
|**2024-09-17**|**AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances**|Dhruv Agarwal et.al.|[2409.11360](http://arxiv.org/abs/2409.11360)|null|本文探讨了当西方导向的AI模型向来自不同文化背景的用户提供写作建议时会发生什么情况。我们进行了一个跨文化的受控实验，共有来自印度和美国的118名参与者完成了具有文化基础的写作任务，并在有无AI建议的情况下完成。我们的分析显示，AI为美国人提供了更高的效率增益，相比之下，印度参与者则在采用西方写作风格方面受到影响，不仅改变了所写的内容，也改变了其写作风格。这些发现表明，以西方为中心的AI模型会将写作方式同质化，使之趋向于西方规范，从而削弱了能够体现文化差异的细微之处。|
|**2024-09-17**|**THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models**|Mengfei Liang et.al.|[2409.11353](http://arxiv.org/abs/2409.11353)|null|本文介绍了一种名为THaMES（工具用于幻觉缓解与评估）的集成框架和库，旨在解决大型语言模型（LLMs）中存在的幻觉生成这一日益增长的挑战。现有的检测和缓解方法往往孤立且无法满足特定领域的需要，缺乏标准化流程。THaMES提供了一个端到端解决方案，涵盖评估和缓解LLMs中幻觉问题的各个环节，包括自动化测试集生成、多维度基准测试以及灵活的缓解策略。它通过批量处理、加权抽样和反事实验证等技术自动创建高质量、多样性和成本效益高的测试集。THaMES评估了模型在文本生成和二分类任务中的幻觉检测与减少能力，并应用了最佳缓解策略，如上下文学习（ICL）、检索增强生成（RAG）和参数高效微调（PEFT）。使用学术论文、政治新闻和维基百科的知识库对前沿LLMs进行评估发现，商业模型如GPT-4o在受益于RAG方面比ICL更多，而开源模型如Llama-3.1-8B-Instruct和Mistral-Nemo则从ICL中获得更大益处。此外，PEFT显著提高了Llama-3.1-8B-Instruct在评估任务中的性能。|
|**2024-09-17**|**Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5**|Marcel Lamott et.al.|[2409.11282](http://arxiv.org/abs/2409.11282)|null|随着各类数字文档格式的激增，尤其是那些非标准化的文档如商业报告和环境评估报告，文档理解变得愈发重要。大型语言模型（LLMs）在多种自然语言处理任务上展现出强大的能力，但在文档理解领域的直接应用仍面临挑战。以往的研究表明LLMs在这一领域具有潜力，然而它们巨大的计算需求使其难以有效地部署。此外，专有的“黑盒”LLMs往往优于开源版本，这构成了广泛可访问性的障碍。本文深入探讨了文档理解的领域，利用了从LLM ChatGPT到FLAN-T5的提炼方法来平衡大模型的强大功能与计算限制。我们提出了一种创新的方法，通过整合标记和课程学习机制来促进知识的有效转移。这项工作对文档理解方法的进展做出了贡献，提供了一个可扩展的解决方案，以弥合资源密集型LLMs与实际应用之间的差距。我们的发现强调了提炼技术在使复杂语言模型在现实世界场景中得到广泛应用的潜力，从而推动自然语言处理和文档理解领域的发展。|
|**2024-09-16**|**RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval**|Di Liu et.al.|[2409.10516](http://arxiv.org/abs/2409.10516)|**[link](https://github.com/jzbjyb/reatt)**|基于转换器的大型语言模型（LLMs）在各个领域变得越来越重要。然而，注意力操作的二次时间复杂度对扩展到更长上下文带来了重大挑战，导致了极高的推理延迟和GPU内存消耗以缓存键值（KV）向量。本文提出了一种无需训练的方法——检索注意力（RetrievalAttention），以加速注意力计算。通过利用注意力操作的动态稀疏特性，RetrievalAttention在CPU内存上构建了近似最近邻搜索（ANNS）索引，并在生成过程中通过向量搜索检索最相关的部分。  由于查询向量与键向量之间的分布外（OOD）问题，现成的ANNS索引仍需要扫描O(N)（通常为所有键的30%）的数据进行精确检索，这无法充分利用高稀疏性。RetrievalAttention首先识别了ANNS基注意力中的OOD挑战，并通过一个适应查询的注意力感知向量搜索算法来解决这一问题，该算法仅访问1-3%的数据，从而实现了亚线性时间复杂度。  RetrievalAttention大幅降低了长上下文LLMs的推理成本，同时显著减少了GPU内存需求，而保持了模型准确性。尤其值得注意的是，RetrievalAttention仅需要16GB的GPU内存即可为具有8B参数的LLM提供服务，支持处理128K个令牌，能够在单个NVIDIA RTX4090（24GB）上生成一个令牌耗时0.188秒。|
|**2024-09-16**|**Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models**|Momoko Shiraishi et.al.|[2409.10506](http://arxiv.org/abs/2409.10506)|null|由于现有C程序中的内存安全性漏洞持续威胁以及Rust语言作为C语言替代品所受到的广泛关注，将C代码转换为Rust代码存在强烈的动机。大型语言模型（LLM）在通过生成比基于规则方法更自然、更安全的代码来自动化这一翻译过程方面显示出潜力。然而，先前的研究表明，LLM生成的Rust代码往往无法编译，即使是相对较小的C程序，这主要归因于两种语言之间的显著差异和上下文窗口限制。  我们提出了一种基于LLM的翻译方案，以提高大规模C代码成功转化为可编译的Rust代码的概率。我们的方法包括三个关键技术：（1）预处理C代码，使其结构和表达式更好地与Rust对齐；（2）将代码分割为最佳大小的翻译单元，以避免超出LLM的上下文窗口限制；（3）通过使用上下文补充提示，迭代编译并修复错误，同时保持不同翻译单元之间的一致性。成功编译是实现功能等效性的首要步骤，因为只有可编译的代码才能进一步进行测试。  在20个基准C程序的实验中，包括那些超过4千行代码的程序，我们成功地将所有程序转化为可编译的Rust代码，没有丢失原始代码的对应部分。|
|**2024-09-16**|**DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction**|John Wu et.al.|[2409.10504](http://arxiv.org/abs/2409.10504)|null|在医学编码等高维或多标签预测任务中，既需要预测的准确性也需要解释的可读性。现有研究往往依赖于局部解释方法，无法提供整个多标签集内每个标签预测背后的全面机制解释。我们提出了一种名为DIctionary Label Attention（简称\method）的模块化解释方法，用于将不可解释的密集嵌入分解到稀疏嵌入空间中。在该空间中，非零元素（字典特征）代表了全局学习的医疗概念。  通过人工评估，我们发现我们的稀疏嵌入比其密集对应物在人类理解上至少提高了50%。我们的自动字典特征识别管道，利用大型语言模型（LLMs），通过检查并总结每个字典特征激活的最高级词汇，揭示了数千个学习到的医疗概念。我们通过一个稀疏的可解释矩阵表示字典特征与医疗代码之间的关系，这不仅增强了模型预测的机制性和全局理解能力，而且在不需要大量人工注释的情况下，保持了竞争力和可扩展性。|
|**2024-09-16**|**Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles**|Kulin Shah et.al.|[2409.10502](http://arxiv.org/abs/2409.10502)|**[link](https://github.com/kulinshah98/llm-reasoning-logic-puzzles)**|近年来，基于Transformer架构的因果语言建模在大型语言模型（LLMs）方面取得了显著的进步。然而，这些模型是否真正发展出了基本的搜索和推理能力，仍是一个持续讨论的话题。本研究旨在探讨因果语言建模能否学会解决复杂的数独谜题这一任务。解决数独谜题需要模型首先在所有空白单元格中进行搜索以决定填充哪个单元格，然后应用适当的策略来填充选定的单元格。有时，策略的应用仅导致单元格可能值的减少，而非确定确切值。在这种情况下，需要对单个单元格应用多个策略。我们发现，经过逻辑步骤序列训练的Transformer模型确实能够学会解决数独谜题（我们的模型正确解决了94.21%的谜题）。我们还对Zebra谜题（又称爱因斯坦谜题）进行了扩展分析，并证明模型能够正确解决92.04%的谜题。此外，我们还研究了训练后的Transformer内部表示，并通过线性探查发现，可以从它们中解码出给定单元格的所有可能值信息，这表明Transformer权重中隐含着强大的推理引擎。|
|**2024-09-16**|**Code Vulnerability Detection: A Comparative Analysis of Emerging Large Language Models**|Shaznin Sultana et.al.|[2409.10490](http://arxiv.org/abs/2409.10490)|null|近年来，软件开发领域对开源项目依赖的增加导致了漏洞问题的显著增长，这一现象引起了广泛关注。本文旨在探讨大型语言模型（LLMs）在识别代码库中的漏洞方面的能力与效果，特别关注了新兴LLM技术的最新进展。通过对比分析，我们评估了包括Llama、CodeLlama、Gemma和CodeGemma在内的最近加入的大型语言模型，以及BERT、RoBERTa和GPT-3等现有最先进的模型在检测软件安全漏洞方面的性能。我们的研究目标是揭示LLM在漏洞检测领域的能力，从而促进不同开源仓库的安全实践提升。结果显示，CodeGemma在检测软件安全漏洞方面取得了最高的F1分数（58%）和召回率（87%）。|
|**2024-09-16**|**XLM for Autonomous Driving Systems: A Comprehensive Review**|Sonda Fourati et.al.|[2409.10484](http://arxiv.org/abs/2409.10484)|null|大型语言模型（LLMs）在各种信息处理任务中展现出了惊人的能力。这些任务涵盖了从数据提取和文献总结到内容生成、预测建模、决策制定以及系统控制等多个方面。此外，视觉大型模型（VLMs）和多模态大型语言模型（MLLMs），即XLMs，能够结合多种数据模态，并利用语言理解的强大力量，从而推动了诸如自动驾驶系统（ADS）等基于信息系统的进步。通过将语言通信与多模式感官输入（如全景图像和激光雷达或雷达数据）相结合，可以采取准确的驾驶行动。在此背景下，本文综述了XLMs在实现自动驾驶方面的潜力。具体而言，我们回顾了ADS和XLMs的相关文献，包括它们的架构、工具和框架。然后，我们详细阐述了部署XLMs以实现自动驾驶解决方案的方法。最后，我们指出了XLM部署在ADS中的相关挑战，并提出了未来研究方向，旨在促进XLM在未来ADS框架中的应用。|
|**2024-09-17**|**Schrodinger's Memory: Large Language Models**|Wei Wang et.al.|[2409.10482](http://arxiv.org/abs/2409.10482)|null|记忆是人类活动的基础；没有记忆，几乎不可能执行日常生活中的任何任务。随着大型语言模型（LLMs）的发展，它们的语言能力正变得越来越接近人类。但LLMs有记忆吗？根据当前的表现，LLMs确实显示出具有记忆的迹象。那么，这种记忆机制背后是什么原理呢？目前的研究缺乏对LLMs记忆能力和底层理论的深入探讨。在本文中，我们利用泛逼近定理（UAT）来解释LLMs的记忆机制。我们还进行了实验来验证各种LLMs的记忆能力，并提出了一种基于这些记忆能力的新方法来评估它们的能力。我们认为，LLMs的记忆工作方式类似于薛定谔的记忆，即只有在查询特定记忆时才会显现出来。我们只能通过响应查询的输出来确定模型是否保留了记忆；否则，它仍然是不确定的。最后，我们扩展了这一概念，通过比较人脑和LLMs的记忆能力，强调了它们在操作机制上的相似性和差异性。|
|**2024-09-16**|**LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning**|Jicong Ao et.al.|[2409.10444](http://arxiv.org/abs/2409.10444)|**[link](https://github.com/proneverfake/kios)**|本文提出了一种名为“LLM作为行为树规划器”的新框架，旨在利用大型语言模型（LLMs）在机器人装配任务规划与执行中的行为树（BT）生成。我们引入了四种基于上下文学习的方法，利用LLMs的自然语言处理和推理能力，以BT格式产生任务计划，从而减少人工努力并确保其稳健性和可理解性。此外，我们还评估了对同一任务进行微调的参数较少的LLMs的表现。在模拟和实际世界设置下的实验结果表明，我们的框架提高了LLMs在BT生成方面的性能，通过基于上下文的学习和监督微调，在BT生成方面显著提高了成功率。|
|**2024-09-16**|**A Large-Scale Privacy Assessment of Android Third-Party SDKs**|Mark Huasong Meng et.al.|[2409.10411](http://arxiv.org/abs/2409.10411)|null|本文研究对Android平台上的第三方软件开发工具包（SDK）进行了针对性分析，旨在填补Android软件供应链中的关键空白，关注于用户隐私保护问题。研究主要从两个关键的SDK发布平台，官方平台与大型替代平台，对广泛使用的158个SDK进行了调查。  在隐私泄露方面，我们发现了338个实例，表明这些SDK在未经授权的情况下，非法传输了用户的敏感信息。这可能被用于非法目的，如用户追踪或牟利。  在隐私合规性方面，我们的研究表明，超过30%的被检查SDK并未提供隐私政策，以披露其数据处理实践。对于那些提供了隐私政策的SDK，有37%过度收集了用户数据，而88%则错误地声称拥有访问敏感数据的权利。  我们在一年后重新审视了SDK的最新版本，结果显示，这些令人担忧的趋势并没有得到改善。  基于我们的发现，我们提出了三项行动建议，旨在降低隐私泄露风险并增强Android用户的隐私保护。这项研究不仅对行业提出了紧迫的关注呼吁，也为未来的监管干预提供了关键见解。|
|**2024-09-17**|**Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot**|Bhuvan Sachdeva et.al.|[2409.10354](http://arxiv.org/abs/2409.10354)|null|本文探讨了大型语言模型（LLMs）在医疗保健领域的应用及其面临的挑战，如幻觉、信息不完整和偏见，这影响了它们的可靠性。为了克服这些问题，研究者发布了一个名为“构建你自己的专家机器人”（BYOeB）的平台，允许开发人员创建集成专家验证的LLM驱动的聊天机器人。CataractBot是该平台的第一个实现，它专注于提供有关白内障手术的专家验证回答。初步评估显示了其潜力，但该研究样本量较小且主要为定性分析。本工作中，我们对CataractBot进行了为期24周的大规模部署，涉及318名患者及其陪同人员发送的1992条消息，其中91.71%的回答经过了七位专家的验证。通过分析交互日志，我们发现医疗问题远多于物流问题，幻觉现象可以忽略不计，并且专家评定84.52%的医疗回答准确无误。随着知识库通过专家更正不断扩展，系统的性能得到了19.02%的提升，减少了专家的工作负担。这些发现指导未来LLM驱动的聊天机器人设计的发展方向。|
|**2024-09-13**|**Agents in Software Engineering: Survey, Landscape, and Vision**|Yanxian Huang et.al.|[2409.09030](http://arxiv.org/abs/2409.09030)|**[link](https://github.com/deepsoftwareanalytics/awesome-agent4se)**|**近年来，大型语言模型（LLMs）在各种下游任务中取得了显著成功，尤其是在软件工程（SE）领域中的任务。我们注意到，许多将LLMs与SE结合的研究工作明确或隐含地采用了代理的概念。然而，缺乏对现有工作发展背景的深入综述、分析它们如何结合基于LLM的代理技术优化各种任务以及澄清SE中基于LLM的代理框架。本文旨在进行首次关于结合LLMs与SE的研究综述，并提出SE中基于LLM的代理框架，包括三个关键模块：感知、记忆和行动。同时，我们总结了这两个领域结合时面临的当前挑战，并针对这些挑战提出了未来的机遇。我们维护了一个相关的论文GitHub仓库，地址为：https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE。**|
|**2024-09-13**|**Contri(e)ve: Context + Retrieve for Scholarly Question Answering**|Kanchan Shivashankar et.al.|[2409.09010](http://arxiv.org/abs/2409.09010)|null|### 摘要翻译  学者交流是一个快速发展的领域，蕴含着丰富的知识。然而，由于其非结构化的文档格式，传统的文档检索方法难以从中提取有用信息。学者知识图谱通过构建一个语义网络来解决这一问题，提供了隐藏的洞察、摘要和易于通过查询获取的访问性。自然地，对学者图谱进行问答扩展了更广泛受众的可访问性。但在这一领域的某些知识仍然以非结构化文本形式呈现，因此需要结合解决方案来为问答系统提供支持。本文提出了一种两步解决方案，使用开源大型语言模型（LLM）：Llama3.1对学者-QALD数据集进行处理。  首先，我们从不同的结构化和非结构化数据源中提取与问题相关的内容：DBLP、SemOpenAlex知识图谱以及维基百科文本。  其次，我们实施了提示工程，以提高大型语言模型的信息检索性能。  我们的方法在F1分数上取得了40%的成绩，并观察到一些来自LLM的异常响应，这些响应在论文的最后部分进行了讨论。|
|**2024-09-13**|**Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance**|Lucio La Cava et.al.|[2409.08963](http://arxiv.org/abs/2409.08963)|null|确保内容符合社区准则对于维护健康的在线社交环境至关重要。然而，传统的基于人类的合规性检查在处理用户生成内容的不断增长量和有限的管理员数量时面临着扩展难题。大型语言模型在自然语言理解方面的新进展，为自动化内容合规性验证开辟了新的可能性。本文评估了六个人工智能代理，这些代理基于Open-LLMs，在去中心化社交网络中对规则合规性进行自动验证，这是一个具有挑战性的环境，因为社区的范围和规则各不相同。通过对来自数百个Mastodon服务器的超过50,000条帖子的分析，我们发现人工智能代理能够有效地检测非合规内容、掌握语言上的细微差别，并适应不同的社区上下文。大多数代理还显示出高的一致性和一致性，在评分解释和合规建议上与人工评价者相匹配。通过领域专家的人工评估，确认了代理的可靠性和实用性，这表明它们是半自动化或人机协作内容管理系统的有前景的工具。|
|**2024-09-13**|**Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions**|Zahra Ashktorab et.al.|[2409.08937](http://arxiv.org/abs/2409.08937)|null|本文研究了在人类与人工智能合作进行文本生成任务时，幻觉和认知驱动因素的影响，特别是利用大型语言模型（LLMs）协助生成高质量对话数据。对于这些模型而言，需要数据进行微调，这是提升其性能的关键步骤。在客户服务对话上下文中，数据以人与客服代理之间的对话形式存在，并可借助AI助手生成。在我们的研究中，共招募了11位用户，每位用户完成8项任务，总共完成了88项任务。结果发现，幻觉的存在对数据质量产生了负面影响。我们还发现，尽管认知驱动因素并非总能抵消幻觉对数据质量的不利影响，但幻觉和认知驱动因素共同作用于数据质量，并影响用户如何利用呈现给他们的AI响应。通过分析用户行为，我们揭示了对AI生成响应依赖的明显模式，这强调了在对话AI情境下管理幻觉在AI生成内容中的重要性。|
|**2024-09-13**|**SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**|Paloma Rabaey et.al.|[2409.08936](http://arxiv.org/abs/2409.08936)|**[link](https://github.com/prabaey/synsum)**|**我们提出了SynSUM基准数据集，这是一个合成数据集，将非结构化的临床记录与结构化背景变量联系起来。该数据集由10,000个虚构的患者记录组成，包含表格变量（如症状、诊断和基础条件）以及与之相关的描述虚构患者就诊情况的临床笔记，领域为呼吸疾病。表格部分的数据通过贝叶斯网络生成，其中因果结构和条件概率由专家基于领域知识提出。然后，我们使用大型语言模型（GPT-4o）生成与患者就诊相关的临床笔记，描述患者的症状和额外的上下文信息。  SynSUM数据集主要旨在促进在存在表格背景变量的情况下对临床信息提取的研究，可以通过领域知识将这些变量链接到从文本中提取的概念兴趣点——在SynSUM的情况下是症状。次要用途包括研究表格数据和文本的自动化临床推理、在存在表格和/或文本混杂因素情况下的因果效应估计以及多模态合成数据生成。  该数据集可以从以下链接下载：<https://github.com/prabaey/SynSUM>**|
|**2024-09-13**|**LLM-based Weak Supervision Framework for Query Intent Classification in Video Search**|Farnoosh Javadi et.al.|[2409.08931](http://arxiv.org/abs/2409.08931)|null|流媒体服务已经彻底改变了我们发现和参与数字娱乐的方式。尽管如此，有效理解用户搜索查询的广泛范围仍然面临重大挑战。构建一个能够处理代表不同用户意图的各种实体的准确查询理解系统对于提供增强的用户体验至关重要。通过训练自然语言理解（NLU）模型可以实现这一目标，然而，在这个专门领域的高质量标注数据获取是一个巨大的障碍。手动注释成本高昂且在捕捉用户词汇变异性方面不切实际。为了解决这个问题，我们提出了一种新颖的方法，通过弱监督利用大型语言模型（LLM）自动标注大量用户搜索查询。通过使用提示工程和多样化的LLM角色，我们生成了与人工注释者期望相匹配的训练数据。通过引入领域知识，利用链式思考和上下文学习，我们的方法利用标记数据训练优化用于实时推理的低延迟模型。广泛的评估显示，我们的方法在召回率上优于基线平均提高了113%。此外，我们提出的新型提示工程框架产生用于弱监督的高质量LLM生成数据；与人类注释的F1得分加权分布相比，我们观察到预测和人类注解之间的一致性提高了47.60%。我们的角色选择路由机制进一步增加了3.67%的加权F1得分，这是在新型提示工程框架基础上的额外收益。|
|**2024-09-13**|**AnyBipe: An End-to-End Framework for Training and Deploying Bipedal Robots Guided by Large Language Models**|Yifei Yao et.al.|[2409.08904](http://arxiv.org/abs/2409.08904)|**[link](https://github.com/sjtu-mvasl-robotics/AnyBipe)**|本文提出了一种端到端的框架，用于训练和部署机器人强化学习（RL）策略，该框架利用大型语言模型（LLM）进行引导。该框架由三个相互连接的模块组成：一个通过LLM设计奖励函数的模块、一个利用现有工作的RL训练模块以及一个模拟到现实（sim-to-real）同态评估模块。这种方法显著减少了对人工干预的需求，仅需要基本的模拟和部署平台，并且提供了人工工程策略和历史数据的整合选项。我们详细介绍了这些模块的构建、它们相对于传统方法的优势，以及展示该框架在双足机器人步态控制自主开发和改进能力的实例，证明其在不需要人类干预的情况下操作的可能性。|
|**2024-09-13**|**A Market for Lemons? Strategic Directions for a Vigilant Application of Artificial Intelligence in Entrepreneurship Research**|Martin Obschonka et.al.|[2409.08890](http://arxiv.org/abs/2409.08890)|null|在人工智能（AI）采用的迅速增长以及大数据可用性的背景下，创业学领域可能迎来有史以来最重大的转变。本文通过强调AI革命期间创业研究中潜在的无成效知识交流风险，做出了紧迫的元贡献。它提供了缓解这一风险的策略，并为未来基于AI的研究提供了指导，以增强其集体影响力和相关性。  借鉴Akerlof著名的“劣质商品市场”概念，我们识别了由于领域演进到当前环境而可能出现的重大知识不对称性，如构造有效性、理论构建和研究相关性方面的复杂性。这些不对称性特别深植于所谓的双重黑箱困境中，即AI方法的广泛认可的黑箱性质与由内在不确定性驱动的创业现象的黑箱性质的交汇点。结果，这些不对称可能导致不可检测的次优研究产品增加，从而形成一个损害领域福祉、声誉和影响力的劣质商品市场。  然而，重要的是，如果能够缓解这些风险，AI革命有可能预示着创业研究的新黄金时代。我们讨论了提升领域至更高水平的AI韧性所需采取的行动，同时坚定地保持其基础原则和核心价值观。|
|**2024-09-13**|**Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**|Zhiqiang Zhong et.al.|[2409.08864](http://arxiv.org/abs/2409.08864)|null|大型语言模型（LLM）在处理各种数据结构时展现了惊人的能力，包括图。尽管先前的研究集中在开发用于图表示的文本编码方法上，但多模态LLM的出现为理解图提供了一个新的前沿。这些先进的模型能够同时处理文本和图像，通过结合视觉表示与传统的文本数据，可能在提高对图结构的理解方面带来改进。这项研究探讨了可视化图在不同级别（节点、边和图级别）上对LLM性能的影响。我们的实验对比了多模态方法与纯文本图表示的有效性。结果提供了关于利用视觉图模态增强LLM对图结构理解能力的潜力和限制的宝贵见解。|
|**2024-09-13**|**FP-VEC: Fingerprinting Large Language Models via Efficient Vector Addition**|Zhenhua Xu et.al.|[2409.08846](http://arxiv.org/abs/2409.08846)|null|训练大型语言模型（LLMs）需要巨大的计算能力和大量的数据。因此，通过指纹保护这些模型的知识产权对于所有权认证至关重要。尽管尝试通过微调向LLMs添加指纹，但这仍成本高昂且难以扩展。为此，我们提出了FP-VEC，一种使用指纹向量作为高效LLM指纹方法的试点研究。我们的方法生成一个代表嵌入在模型中的保密签名的指纹向量，允许通过向量相加无缝地将相同的指纹整合到无限数量的LLMs中。在多个LLMs上的结果表明，FP-VEC轻量级，可以在仅使用CPU的设备上运行以进行指纹识别；可扩展，只需要一次训练即可实现无限次的指纹生成过程，并且能够保持模型的正常行为。项目页面位于https://fingerprintvector.github.io 。|
|**2024-09-12**|**Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale**|Rogerio Bonatti et.al.|[2409.08264](http://arxiv.org/abs/2409.08264)|**[link](https://github.com/microsoft/windowsagentarena)**|**大型语言模型（LLM）展现出在需要规划和推理的多模态任务中作为计算机代理的强大潜力，能显著提升人类生产力和软件可访问性。然而，衡量这些代理在真实环境中的性能仍存在挑战：（i）大多数基准测试仅限于特定模态或领域（例如纯文本、网页导航、问题回答、编程），（ii）完整基准评估耗时长（通常需数天时间），因为任务具有多步骤的序列性质。  为解决这些挑战，我们引入了“Windows Agent Arena”：一个可复现的通用环境，专注于Windows操作系统，允许代理自由操作并使用与人类用户在解决任务时相同的广泛应用程序、工具和网络浏览器。我们根据OSWorld框架（Xie等人，2024年）创建了150多个跨代表领域的多样化Windows任务，这些任务涵盖了规划、屏幕理解及工具使用的代理能力要求。  我们的基准具有可扩展性，并能够无缝地在Azure上并行化，从而在短短20分钟内完成全面基准评估。为了展示Windows Agent Arena的能力，我们还引入了一个新的多模态代理Navi。Navi在Windows领域内的成功率达到了19.5%，相比之下，未经辅助的人类表现则为74.5%。此外，Navi在另一个流行的基于网络的基准测试Mind2Web中也表现出色。  我们提供了对Navi性能的详细定量和定性分析，并提供了利用Windows Agent Arena进行未来研究的代理开发和数据生成机会的见解。网页：https://microsoft.github.io/WindowsAgentArena  代码：https://github.com/microsoft/WindowsAgentArena**|
|**2024-09-12**|**OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering**|Jiahao Nick Li et.al.|[2409.08250](http://arxiv.org/abs/2409.08250)|null|人们常通过照片、屏幕截图和视频来捕捉记忆。现有的基于AI的工具能够使用自然语言检索这些数据，但主要局限于检索像照片中的特定物体这样的单一信息，难以处理涉及理解相互关联记忆（如事件序列）的更复杂查询。我们进行了一项为期一个月的日志研究，收集了现实用户查询，并生成了一个集成与捕获记忆相关必要上下文信息的分类体系。随后，我们引入了OmniQuery，这是一种能够回答需要提取和推断多层上下文信息以整合相互关联记忆的复杂个人记忆相关问题的新型系统。OmniQuery通过从多个相互关联的记忆中集成分散的上下文信息来增强单个捕获的记忆，检索相关记忆，并利用大型语言模型（LLM）提供全面的答案。在人类评估中，我们展示了OmniQuery的有效性，准确率达到71.5%，并且它在74.5%的时间里超越了传统的RAG系统，在某些任务上甚至取得了胜利或并列第一的成绩。|
|**2024-09-12**|**Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources**|Alisia Lupidi et.al.|[2409.08239](http://arxiv.org/abs/2409.08239)|null|在面对依赖结构化数据、复杂推理或工具使用的挑战性场景时，大型语言模型仍然存在困难。为此，我们提出了一种名为Source2Synth的新方法，它无需昂贵的人类标注即可用于教授LLMs新技能。Source2Synth接受自定义数据源作为输入，并生成具有基于现实世界来源的中间推理步骤的合成数据点。该方法通过根据其可回答性丢弃低质量生成来提高数据集质量。我们通过在两个具有挑战性的领域中应用此方法来展示其通用性：在多跳问题回答（MHQA）中测试推理能力，在表格型问题回答（TQA）中测试工具使用。与经过微调的基本模型相比，我们的方法在WikiSQL上的TQA上提高了25.51%，在HotPotQA上的MHQA上提高了22.57%的性能。|
|**2024-09-12**|**LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems**|Hakan T. Otal et.al.|[2409.08234](http://arxiv.org/abs/2409.08234)|**[link](https://github.com/ai-in-complex-systems-lab/llm-honeypot)**|**本文介绍了一种创新方法，使用大型语言模型（LLMs）构建真实且互动的蜜罐系统。通过在包含攻击者生成命令和响应的多样化数据集上对开源预训练语言模型进行微调，我们开发出一种能够与攻击者进行高级交互的蜜罐。我们的方法涉及关键步骤：数据收集与处理、提示工程、模型选择以及监督式微调，以优化模型性能。通过相似性指标评估与现场部署，结果显示我们的方法能够生成准确且信息丰富的响应。研究结果强调了LLMs在重塑蜜罐技术方面的潜力，为网络安全专业人员提供了一个强大的工具来检测和分析恶意活动，从而增强整体安全架构。**|
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202](http://arxiv.org/abs/2409.08202)|null|人类视觉理解的独特之处在于能够灵活地解释抽象概念的能力：获取提升规则来解释它们所象征的含义，在熟悉和不熟悉的上下文中锚定它们，并对它们进行预测或推理。尽管现成的视觉语言模型在识别图像中的具体对象类别（如树枝）方面表现出色，但它们仍然难以理解这样的视觉抽象（例如，一组树枝如何形成迷宫的墙壁）。为了应对这一挑战，我们引入了深度架构接地（DSG），这是一个利用明确的结构化表示法来锚定和推理视觉抽象的框架。DSG的核心是架构——分解抽象概念的依赖图形描述，将其分解为更基本的符号。DSG使用大型语言模型提取架构，然后通过视觉语言模型分层地将架构中的具体到抽象组件锚定到图像上。锚定后的架构用于增强对视觉抽象的理解。我们系统地评估了DSG及其不同的方法在我们新创建的视觉抽象数据集上的推理性能，该数据集由人类标注的真实世界图像和相应的问答对组成。我们展示了DSG显著提高了视觉语言模型在抽象视觉推理方面的表现，并朝着与人类一致的视觉抽象理解迈进了一步。|
|**2024-09-12**|**Fine-tuning Large Language Models for Entity Matching**|Aaron Steiner et.al.|[2409.08185](http://arxiv.org/abs/2409.08185)|**[link](https://github.com/wbsg-uni-mannheim/tailormatch)**|**本文探讨了利用大型语言模型（LLM）进行实体匹配的潜力，特别是通过微调。已有研究主要集中在提示工程和基于上下文的学习上。本文从两个维度分析了微调的可行性：1）训练示例的表示方式，实验涉及在训练集中添加不同类型的LLM生成解释；2）使用LLM选择和生成训练示例。我们不仅关注源数据集上的匹配性能，还研究了微调对模型在同域数据集以及跨领域数据集上的泛化能力的影响。  实验结果显示，微调显著提升了小型模型的性能，而大型模型的表现则参差不齐。微调在提升同域数据集的泛化能力的同时，也影响了跨域迁移的能力。我们发现，向训练集添加结构化的解释对四种LLM中的三种有正面影响，而提出的示例选择和生成方法仅提升了Llama 3.1 8B的性能，同时降低了GPT-4o Mini的性能。**|
|**2024-09-12**|**Faster Speech-LLaMA Inference with Multi-token Prediction**|Desh Raj et.al.|[2409.08148](http://arxiv.org/abs/2409.08148)|null|大型语言模型（LLMs）在解决各种任务上变得极为熟练，包括涉及多模态输入的任务。具体来说，通过使用语音编码器实例化LLM（例如LLaMA）并利用配对数据对其进行训练，可以赋予只解码的模型语音识别（ASR）能力，因此称之为Speech-LLaMA。然而，由于自回归推理的顺序性质以及相对较大的解码器，Speech-LLaMA模型的推理时间相对较高。本工作中，我们提出通过在同一解码步骤中预测多个令牌来加速Speech-LLaMA的推理。我们探索了几个能够实现这一目标的模型架构，并通过阈值推理和验证推理策略来评估它们的性能。此外，我们还提出了一个基于前缀的束搜索解码方法，允许此类模型进行高效的最小词错误率（MWER）训练。我们在多种公共基准上评估了我们的模型，结果显示它们将解码调用的数量减少了约3.2倍，同时保持或提高了WER性能。|
|**2024-09-12**|**LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models**|Zhengliang Liu et.al.|[2409.08147](http://arxiv.org/abs/2409.08147)|null|本文提出了一种利用大型语言模型（LLM）来评估总统辩论表现的新方法，旨在解决长期存在的客观评估辩论结果的挑战。我们构建了一个框架，从“政策、个性与视角”（3P）和“兴趣、意识形态与身份认同”（3I）的角度分析四位关键受众群体：选民、企业、捐赠者及政客对候选人的共鸣。该方法通过生成“LLM-POTUS评分”，即基于3P与3I之间一致性度量的量化指标，来评价辩论表现。我们应用此框架对近期美国总统辩论的文本进行分析，揭示了不同辩论策略的有效性及其对不同受众群体的影响。研究不仅提供了一个新的政治分析工具，还探索了在复杂社会背景下使用LLM作为公正评判者的潜力与局限性。此外，该框架为个人公民提供了一个独立的工具，用于评估总统辩论的表现，从而增强民主参与度，减少对可能偏见的媒体解读和机构影响力的依赖，进而加强知情公民参与的基础。|
|**2024-09-12**|**The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal**|Huiyuan Xie et.al.|[2409.08098](http://arxiv.org/abs/2409.08098)|null|本文研究了技术革新与获取公正之间的交汇点，通过在英国就业法庭（UKET）构建预测案例结果的基准。为了应对大量人工注释的挑战，该研究利用大型语言模型（LLM）进行自动注释，从而创建了CLC-UKET数据集。该数据集包含约19,000个UKET案例及其元数据。全面的法律注释涵盖了事实、主张、先例引用、法规引用、案例结果、理由和管辖权代码。借助CLC-UKET数据，我们对UKET的多类案例结果预测任务进行了研究。收集了人类预测以建立模型比较的性能参考。从基础模型的实证结果来看，微调的转换器模型在UKET预测任务上优于零次和少量样本的LLM。零次LLM的性能可以通过整合与任务相关的信息来增强，融入少量样本示例中。我们希望CLC-UKET数据集、人类注释以及实证发现能够作为就业相关纠纷解决的宝贵基准。|
|**2024-09-12**|**Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks**|Benji Peng et.al.|[2409.08087](http://arxiv.org/abs/2409.08087)|null|本文综述了近年来有关大型语言模型（LLM）安全性的关键问题的研究文献，重点是准确性、偏见、内容检测以及对抗攻击的脆弱性。文章详细讨论了LLM输出可能不准确或误导性的问题，并强调了通过事实核查方法增强响应可靠性的实施策略。文章深入探讨了内嵌于LLM中的固有偏见，通过多样化的评估技术，如控制输入研究和红队演练，对其进行批判性审视。提出了全面的偏见缓解策略分析，包括从预处理干预到训练期间调整和后处理改进的各种方法。此外，文章还探究了区分LLM生成内容与人类创作文本的复杂性，引入了诸如DetectGPT的检测机制以及水印技术，同时指出在复杂情况下基于机器学习的分类器存在局限性。文章还分析了LLM的漏洞，包括逃逸攻击和提示注入攻击，通过案例研究和大规模竞赛HackAPrompt等进行了深入探讨。最后，文章回顾了保护LLM的防御措施，强调了需要对LLM安全性领域进行更深入研究的重要性。|
|**2024-09-11**|**"My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays**|Shengxin Hong et.al.|[2409.07453](http://arxiv.org/abs/2409.07453)|null|交互式反馈在教师与学生之间双向流动，相较于传统的单向反馈更为有效。然而，这种反馈方式往往耗时过多，难以在教育实践中广泛应用。虽然大型语言模型（LLM）具有自动化反馈的潜力，但它们在互动情境下的推理和交互方面存在困难。本文提出了一种名为CAELF（Contestable AI Empowered LLM框架），旨在通过集成多代理系统与计算论辩来自动化交互式反馈。首先，学生的作文由多个教学助理代理（TA代理）进行评估，随后，教师代理通过形式化推理整合这些评价，生成反馈和评分。学生可以进一步与反馈互动，以深化理解。通过对500篇批判性思维作文的案例研究，并结合用户研究，结果表明，CAELF显著提高了交互式反馈的质量，增强了LLM的推理和互动能力。这一方法提供了一个克服影响教育领域广泛应用交互式反馈的时间和资源障碍的有前景解决方案。|
|**2024-09-11**|**SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories**|Ben Bogin et.al.|[2409.07440](http://arxiv.org/abs/2409.07440)|**[link](https://github.com/allenai/super-benchmark)**|**给定大型语言模型（LLM）在编写代码方面取得的重大进展，它们现在是否能够自主重现研究仓库中的结果？这样的能力将对研究社区产生巨大益处，帮助研究人员验证、理解并扩展先前的工作。为了向这一目标迈进，我们引入了SUPER，这是首个旨在评估LLM在从研究仓库设置和执行任务方面的能力的基准。SUPER旨在捕捉研究人员在机器学习（ML）和自然语言处理（NLP）研究仓库工作时所面临的真实挑战。我们的基准由三个不同的问题集组成：45个端到端问题，附有专家解决方案的注释，152个专注于特定挑战（例如配置训练器）的子问题，以及602个用于更大规模开发的自动生成问题。我们引入了各种评估指标来评估任务成功和进度，当有黄金解决方案可用时使用黄金解决方案，否则使用近似值。我们展示了最先进的方法在解决这些问题时遇到了困难，最好的模型（GPT-4o）仅解决了16.3%的端到端集和46.1%的场景。这表明了这项任务的挑战性，并表明SUPER可以作为社区衡量和推动进步的宝贵资源。**|
|**2024-09-11**|**CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification**|Zeqing Qin et.al.|[2409.07407](http://arxiv.org/abs/2409.07407)|null|大型语言模型（LLM）在漏洞识别领域展现出了巨大的潜力。由于C/C++在过去十年中占据了开源软件（OSS）漏洞的一半，并且主要通过提交进行更新，因此增强LLM在识别C/C++漏洞贡献提交（VCC）方面的能力变得至关重要。然而，当前的研究主要集中在对大规模代码集进一步预训练LLM上，这既耗费资源又存在效率挑战。本文提出了一种轻量级方法来提升基于BERT的LLM识别C/C++ VCC的能力。我们提出了CodeLinguaNexus（CLNX），作为连接C/C++程序与LLM的桥梁。CLNX通过在保留关键细节的同时，以更自然的方式高效地将源代码转换为更适合LLM处理的表示。具体来说，CLNX首先应用结构级自然化来分解复杂的程序，然后应用符号级自然化来解释复杂的符号。我们在包含25,872个C/C++函数及其提交的公开数据集上评估了CLNX。结果表明，CLNX显著提升了LLM识别C/C++ VCC的能力。此外，配备CLNX的CodeBERT达到了新的最优性能，并在真实世界中识别了38个OSS漏洞。|
|**2024-09-11**|**AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge**|Han Wang et.al.|[2409.07394](http://arxiv.org/abs/2409.07394)|**[link](https://github.com/hannight/adacad)**|**在大语言模型（LLM）的上下文与模型参数存储的知识之间存在知识冲突，这会导致使用标准解码技术时性能受损，因为这些技术往往忽视了上下文。现有的测试时间对比方法试图通过比较带有和不带有上下文的LLM输出分布之间的对比，并根据它们之间的对比调整模型来解决这个问题。然而，我们发现这些方法经常错误地判断冲突的程度，并且难以处理不同冲突程度的实例，静态方法在冲突不存在时过度调整。为此，我们提出了一种基于实例的精细粒度方法AdaCAD，它动态地根据Jensen-Shannon散度测量的上下文和参数知识分布之间的冲突程度来推断调整权重。我们在四个模型上对六个多样化的问答（QA）数据集和三个摘要任务进行的实验显示，我们的无需训练的自适应方法始终在问答任务上优于其他解码方法，平均准确率提高了14.21%（绝对值），并且提高了摘要的真实性，AlignScore提高了5.59分。此外，我们的分析表明，与冲突的对比基线相比，当冲突不存在时，解码会损害性能，而AdaCAD能够缓解这些损失，使其更适用于现实世界的数据集，在这些数据集中，有些示例存在冲突，而其他示例则不存在冲突。**|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368](http://arxiv.org/abs/2409.07368)|null|本文介绍了一种名为SGCode的灵活提示优化系统，用于通过大型语言模型（LLM）生成安全代码。SGCode将最近的提示优化方法与LLM结合在一个统一的系统中，通过前端和后端API提供服务，使用户能够：1）生成无漏洞的安全代码；2）查看和共享安全性分析；以及3）轻松在不同的提示优化方法之间切换，并提供有关模型和系统性能的见解。我们使用AWS服务器上的PromSec填充SGCode，这是一种方法，通过将LLM、安全工具与轻量级生成对抗图神经网络相结合，来检测并修复生成代码中的安全漏洞，从而优化提示。广泛的实验表明，SGCode作为公共工具，能够揭示模型实用性、安全代码生成和系统成本之间的权衡，具有相对较低的成本。SGCode已上线于：<http://3.131.141.63:8501/>。|
|**2024-09-11**|**Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation**|SeongYeub Chu et.al.|[2409.07355](http://arxiv.org/abs/2409.07355)|**[link](https://github.com/BBeeChu/InteractEval)**|**本文介绍了一种名为“InteractEval”的框架，该框架采用“Think-Aloud”方法结合大型语言模型（LLM）与人类专家意见，以生成基于检查清单的文本评估的属性。通过融合人类的灵活性和推理能力以及LLM的一致性，InteractEval在一致性、流畅性、相关性和连贯性四个维度上均超越了传统的非LLM基线和LLM基线模型。实验还探讨了“Think-Aloud”方法的有效性，表明它能促进人类和LLM的发散思维，从而产生更广泛的相关属性，并提高文本评估性能。比较分析显示，人类在识别与内部质量相关的属性（如连贯性和流畅性）方面表现优异，而LLM在与外部对齐相关的属性（如一致性和相关性）上表现更好。因此，结合人类和LLM共同产生的评估结果最佳。换句话说，本文强调了在自动化基于检查清单的文本评估框架中有效整合人类和LLM的必要性。代码已开源于\textbf{\url{https://github.com/BBeeChu/InteractEval.git}}}。**|
|**2024-09-11**|**Learning to Compress Contexts for Efficient Knowledge-based Visual Question Answering**|Weixi Weng et.al.|[2409.07331](http://arxiv.org/abs/2409.07331)|null|多模态大型语言模型（MLLMs）在视觉问答（VQA）任务上展示了出色的零样本性能。然而，在知识基视觉问答（KB-VQA）任务中，MLLMs可能缺乏人类常识或特定领域的专业知识，从而需要从外部知识源获取所需信息以回答此类问题。先前的工作，如检索增强的VQA-v2（RAVQA-v2），侧重于充分利用输入信息，例如图像文本描述和检索的知识，以提高性能，但它们都忽视了一个问题：随着输入令牌数量的增加，推理效率显著降低，这与实际应用的需求相矛盾。为了解决这一问题，我们提出了检索增强的多模态大语言模型（RACC）。RACC学习压缩并聚合检索上下文，并生成紧凑的键值（KV）缓存形式的调节。然后，使用这种调节来适应下游冻结的MLLM，从而实现有效且高效的推理。RACC在OK-VQA上实现了当前最佳的62.9%性能。此外，它将RAVQA-v2的推理延迟显著降低了22.0%-59.7%。大量的实验表明了RACC的广泛适用性。它与各种现成的MLLM兼容，并可以处理包括文本和多模态文档在内的不同知识源。|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314](http://arxiv.org/abs/2409.07314)|null|大型语言模型（LLM）在医疗健康领域的快速开发引发了对超越如USMLE等常用基准评估的全面评估需求，以更好地反映实际应用表现。虽然现实世界的评估是实用性的重要指标，但它们往往落后于LLM演进的速度，可能导致研究结果在部署时变得过时。这种时间上的脱节需要一种全面的前期评估方法，以指导特定临床应用中的模型选择。  我们引入了MEDIC框架，它从五个关键的临床能力维度评估LLM：医学推理、伦理与偏见、数据和语言理解、上下文学习以及临床安全性。MEDIC采用了一种新颖的交叉审查框架，量化了LLM在覆盖范围和幻觉检测等领域的性能，而无需参考输出。我们使用MEDIC对医疗问答、安全、总结、笔记生成以及其他任务进行了评估。  我们的结果显示不同模型大小之间、基线模型与医学微调模型之间的性能差异，并对需要特定模型优势的应用（如低幻觉或较低推理成本）的模型选择具有启示意义。MEDIC的多维度评估揭示了理论能力和实际实施之间的性能权衡，弥合了在医疗保健环境中识别和适应最有前景模型的差距，确保了适合多种医疗保健应用的模型得到识别和适应。|
|**2024-09-11**|**STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM**|Qijiong Liu et.al.|[2409.07276](http://arxiv.org/abs/2409.07276)|null|传统推荐模型通常依赖于独特的项目标识符（ID）来区分项目，这可能限制了它们利用项目内容信息和推广长尾或冷启动项目的能 力。近期，已提出语义分词作为解决这一问题的有希望的方法，旨在将每个项目的语义表示分词为一系列离散的令牌。通过这种方式，它保 留了项目在这些令牌内的语义，并确保具有相似语义的项目由相似的令牌表示。这些语义令牌成为训练生成推荐模型的基础。然而，现有 的生成推荐方法通常涉及多个子模型进行嵌入、量化和推荐，导致系统过于复杂。在这篇论文中，我们提出了一种统一框架，称为STORE， 利用单一大型语言模型（LLM）同时执行这两项任务。具体而言，我们将语义分词表述为文本到令牌的任务，而生成推荐则表述为令牌到 令牌的任务，通过补充令牌到文本重构任务和文本到令牌辅助任务，所有这些任务均以生成方式表述并使用单一LLM骨干进行训练。 我们进行了大量实验，以验证我们的STORE框架在各种推荐任务和数据集上的有效性。我们将发布源代码和配置，以便进行可复现的研究。|
|**2024-09-11**|**MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving**|Enming Zhang et.al.|[2409.07267](http://arxiv.org/abs/2409.07267)|**[link](https://github.com/emzucas/minidrive)**|本文提出了一种名为MiniDrive的新型框架，旨在解决视觉语言模型（VLM）在自动驾驶场景中的应用难题。现有的VLM方法通常依赖于计算密集型的视觉编码器和大型语言模型（LLMs），这使得它们难以在实际世界和实时应用中部署。此外，大多数现有VLM缺乏处理多张图片的能力，这使得它们难以适应自动驾驶中的多摄像头感知需求。  为了解决这些问题，我们引入了两个关键模块：特征工程混合专家（FE-MoE）和动态指令适配器（DI-Adapter）。FE-MoE有效地将二维特征映射到视觉令牌嵌入，然后作为输入传递给语言模型。DI-Adapter允许视觉令牌嵌入根据指令文本嵌入动态变化，解决了以往方法中同一图片下静态视觉令牌嵌入的问题。  与之前的成果相比，MiniDrive在参数大小、浮点运算量和响应效率方面均达到了最优性能，最小版本仅包含83M参数。|
|**2024-09-10**|**E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning**|Zihan Liao et.al.|[2409.06679](http://arxiv.org/abs/2409.06679)|null|在大型语言模型（LLMs）领域，处理长文本上下文的能力对于多轮对话、代码生成和文档摘要等任务愈发重要。本文探讨了增强长文本上下文性能、降低计算复杂性以及充分利用预训练模型所面临的挑战——即所谓的“不可能三角”。我们提出了一种名为E2LLM（编码器扩展大型语言模型）的创新方法，旨在有效解决这一悖论。  该方法的核心思想是将长文本上下文划分为多个片段，并通过预训练的文本编码器将每个片段压缩为嵌入向量。然后利用适配器将这些表示与解码器型LLM对齐，以促进对软提示的理解。本文提出了两个训练目标：一是重建编码器输出，二是针对长文本指令进行微调，以帮助LLM理解软提示。  实验结果表明，E2LLM在长文本上下文场景中取得了显著的性能提升，同时保持了效率、性能和与预训练模型的兼容性。因此，我们的框架代表了领域内的重大进展，为有效的大文本建模做出了贡献。|
|**2024-09-10**|**LLaMA-Omni: Seamless Speech Interaction with Large Language Models**|Qingkai Fang et.al.|[2409.06666](http://arxiv.org/abs/2409.06666)|**[link](https://github.com/ictnlp/llama-omni)**|**针对大型语言模型（LLM）通过语音实现实时交互的能力提升，相较于传统的文本交互方式，模型如GPT-4显著增强了用户体验。然而，当前在基于开源LLM构建语音交互模型方面仍缺乏深入探索。为了填补这一空白，我们提出了一种新型模型架构——LLaMA-Omni，旨在实现低延迟与高质量的语音与LLM交互。该架构融合了预训练的语音编码器、语音适配器、LLM和流式语音解码器，无需进行语音转录，即可直接从语音指令生成文本和语音响应，响应速度极快。  我们的模型基于最新的Llama-3.1-8B-Instruct模型构建，并针对语音交互场景构建了一个名为InstructS2S-200K的数据集，其中包含了20万条语音指令及其对应的语音回应。实验结果显示，与以往的语音语言模型相比，LLaMA-Omni在内容与风格上提供了更好的响应，响应延迟低至226毫秒。此外，训练LLaMA-Omni仅需不到3天的时间，在4块GPU上即可完成，这为未来高效开发语音语言模型铺平了道路。**|
|**2024-09-10**|**Human Perception of LLM-generated Text Content in Social Media Environments**|Kristina Radivojevic et.al.|[2409.06653](http://arxiv.org/abs/2409.06653)|null|新兴技术，尤其是人工智能（AI）和大型语言模型（LLM），为恶意行为者提供了操纵数字对话的强大工具。LLM有可能影响传统形式的民主参与，例如选民选择、政府调查或与监管机构的在线交流，因为机器人能够生成大量可信文本。为了研究人类对LLM生成内容的感知，我们招募了超过1000名参与者，然后让他们尝试在社交媒体讨论线程中区分机器人与人类帖子。我们发现人类在识别社交媒体上的真实用户帖子方面表现不佳。我们也发现了人类在社交媒体对话中识别LLM生成文本内容的模式。最后，我们观察到了“怪异谷”效应在文本对话中的存在，无论是在感知还是识别过程中。这表明尽管人类在识别过程中的表现不佳，但当阅读LLM生成的内容时，他们仍能感受到不适。|
|**2024-09-10**|**Optimal Workload Placement on Multi-Instance GPUs**|Bekir Turkkan et.al.|[2409.06646](http://arxiv.org/abs/2409.06646)|null|本文旨在探讨如何优化大型语言模型（LLM）为基础的AI推理工作负载在GPU上的部署。我们首先识别并阐述了实践中遇到的一些需要高效分配或迁移工作负载到其他GPU以腾出空间供新工作负载使用的情况。目标是尽可能减少使用的GPU数量，并进一步降低被利用GPU中的内存和计算浪费。  为了实现这一目标，我们提出了两种方法：一种是优化方法，另一种是启发式方法。我们使用两种工作负载调度启发式算法对多种用例进行了基准测试。结果显示，在与基线启发式相比的情况下，我们能够节省高达2.85倍的GPU使用量，以及高达70%的GPU浪费。  我们计划让SRE（系统可靠性工程）社区能够在生产环境中利用我们的提议方法。|
|**2024-09-10**|**MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders**|Wenyu Zhang et.al.|[2409.06635](http://arxiv.org/abs/2409.06635)|null|快速发展的大型语言模型（LLM）显著提高了自然语言处理能力，促进了音频LLM的发展，这些模型能够理解语音和音频输入。现有的音频LLM通常结合预训练的音频编码器与文本预训练的LLM，并在特定的音频任务上进行微调。然而，预训练的音频编码器的容量有限，无法捕获新任务和数据集中的特征。为了应对这一问题，我们提出将“弱”编码器混合（MoWE）融入音频LLM框架。MoWE通过在基本编码器基础上补充一组相对较轻量级的编码器，根据音频输入动态激活以增强特征提取，同时避免显著增加模型大小。我们的实验结果表明，MoWE有效提高了多任务性能，使音频LLM能够应用于更多样化的音频任务。|
|**2024-09-10**|**A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio**|Ningyuan Xi et.al.|[2409.06624](http://arxiv.org/abs/2409.06624)|null|本文研究了大规模语言模型（LLM）在持续预训练（CPT）过程中，如何通过额外语言混合比（ALMR）和学习率（LR）之间的最优相关性，提升模型在中文及其他特定领域的性能。针对8B大小的Llama-3模型，我们进行了深入研究，确定了实验设置中的关键超参数，并通过精细调整，显著提升了模型在中文相关的基准测试以及数学、编程和情绪智能等特定领域的能力。最终，我们将70B大小的LLM部署到实际聊天系统中，并取得了令人满意的效果。|
|**2024-09-10**|**Alleviating Hallucinations in Large Language Models with Scepticism Modeling**|Yetao Wu et.al.|[2409.06601](http://arxiv.org/abs/2409.06601)|null|大型语言模型（LLM）面临的主要挑战是幻觉现象，这阻碍了其在多个领域的应用。不确定性估计可以被用于缓解幻觉带来的损害。人类的怀疑情绪被认为能增强自我评估的能力。基于这一观察，我们提出了一种名为“质疑建模”（SM）的新方法。这一方法通过结合词元和logits信息来进行自我评估而得到形式化。我们构建了包含怀疑情绪意识的数据集，并进行连续预训练，然后对LLM进行微调，从而提升它们自我评估的能力。实验结果证明了这种方法有效增强了模型估算不确定性的能力，并通过跨领域实验验证了其在其他任务中的泛化能力。|
|**2024-09-10**|**GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering**|Sacha Muller et.al.|[2409.06595](http://arxiv.org/abs/2409.06595)|**[link](https://github.com/illuin-tech/grouse)**|本文探讨了使用大型语言模型（LLMs）与私有且更新至最新的知识库相结合的检索增强生成（RAG）范式时面临的挑战。我们特别关注评估由RAG系统生成的基于现实的答案时，作为裁判的LLM所遇到的问题。为了评估裁判模型的校准和区分能力，我们识别了7种生成器失败模式，并引入了GroUSE（基于问题解答的元评估基准），这是一个包含144个单元测试的元评估基准。这个基准揭示了现有的自动化RAG评估框架往往忽视了重要失败模式，即使在使用GPT-4作为裁判的情况下也是如此。  为了改进当前自动化RAG评估框架的设计，我们提出了一种新的管道，并发现封闭模型在GroUSE上表现良好，而最先进的开源裁判模型在我们的提议标准下并未表现出良好的泛化能力，尽管它们与GPT-4的判断高度相关。我们的研究结果表明，与GPT-4的相关性是一个不完整的代理指标，用于衡量裁判模型的实际性能，并应该通过对参考情况的精确失败模式检测进行补充评估。  进一步的研究显示，通过在GPT-4的推理痕迹上对Llama-3进行微调，显著提升了其评估能力，不仅提高了与GPT-4评价的相关性和参考情况的校准度。|
|**2024-09-10**|**MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles Through LLMs Penetrated Science**|Mahdieh Aliazam et.al.|[2409.06558](http://arxiv.org/abs/2409.06558)|null|随着自动驾驶车辆的日益普及，对高度精确和高效的系统的需求也在不断增长，以提升安全性能、操作效率和能源消耗。在管理能源与可靠性之间的权衡时，预测车辆运行期间的各种条件变得尤为重要。近年来，大型语言模型（LLMs）的改进以及知名模型如ChatGPT的出现，为自动驾驶相关预测提供了独特的机会。本文提出了一种名为MAPS的方法，利用LLMs作为地图阅读辅助驾驶员，预测在自动驾驶车辆操作过程中设置的关键参数，以平衡能源与可靠性之间的权衡。MAPS方法在导航精度方面相较于最佳基线方法提高了20%。此外，MAPS还显示了在计算单元上节省了11%的能源，并在机械和计算单元上最高节省了54%。|
|**2024-09-10**|**Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games**|Juhwan Choi et.al.|[2409.06518](http://arxiv.org/abs/2409.06518)|**[link](https://github.com/c-juhwan/olympics_analysis)**|大型语言模型（LLM）在自然语言处理领域已经成为主导性方法，然而它们的内部知识结构仍然未被充分探索。本文通过分析奥林匹克运动会的历史奖牌统计情况，研究了LLM的内部知识结构。我们要求模型提供各队的奖牌数量，并确定哪些队伍获得了特定排名。我们的结果表明，尽管最先进的LLM在报告单个队伍的奖牌数量方面表现得非常出色，但在回答关于特定排名的问题时却遇到显著困难。这暗示了LLM的内部知识结构与人类的根本不同，人类能够轻松地从已知的奖牌数量推断出排名。为了支持进一步的研究，我们公开发布了代码、数据集和模型输出。|
|**2024-09-09**|**MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct**|Run Luo et.al.|[2409.05840](http://arxiv.org/abs/2409.05840)|null|在多模态大型语言模型（MLLMs）的发展过程中，我们已经取得了显著的进步。然而，在数据量和数据质量方面仍然存在关键瓶颈。手动创建多模态指令数据既耗时又低效，尤其是在生成高复杂性的指令时。此外，从“黑盒”商业模型（例如GPT-4o、GPT-4V）中提取指令数据往往导致生成的指令数据过于简单，这限制了模型性能仅与其自身水平相当。构建多样性和复杂性指令数据的挑战依然巨大。  为解决这一问题，我们提出了一种名为MMEvol的新颖多模态指令数据进化框架，该框架结合了精细感知演化、认知推理演化以及互动演化。这一迭代方法突破了数据质量瓶颈，生成了一个复杂且多样化的图像-文本指令数据集，从而增强了MLLMs的能力。我们以初始指令集合SEED-163K为基础，利用MMEvol系统地扩展了指令类型的多样性，融入了增强认知能力的推理步骤，并从图像中提取了详细信息以提高视觉理解和鲁棒性。  为了全面评估我们数据的有效性，我们使用进化的数据训练了LLaVA-NeXT，并在13个视觉语言任务上进行了实验。与基于原始数据训练的基线相比，我们的方法平均提高了3.1点准确率，并在9个任务上达到了最先进的性能水平。|
|**2024-09-09**|**Are Large Language Models a Threat to Programming Platforms? An Exploratory Study**|Md Mustakim Billah et.al.|[2409.05824](http://arxiv.org/abs/2409.05824)|null|本文研究了大型语言模型（LLM）如ChatGPT、Gemini和Meta AI在LeetCode、Codeforces和HackerRank等竞赛编程平台上的问题解决能力。这些平台常被招聘人员用来筛选编程技能。随着LLM能力的提升，对其在不同难度级别、各类别的编程挑战中的表现进行评估变得尤为重要。  研究团队从LeetCode选取了98个问题，从Codeforces选取了126个问题，覆盖了15个类别。通过九场在线Codeforces和LeetCode竞赛以及HackerRank的两项认证测试，对LLM的实时性能进行了评估。研究过程中使用了提示和反馈机制来引导LLM，并探索了不同场景之间的相关性。  结果显示，ChatGPT等LLM在LeetCode和HackerRank的认证测试中表现出色（成功率为71.43%），但在虚拟竞赛中，特别是在Codeforces的高难度比赛中，它们的表现不尽如人意。尽管在LeetCode档案库中的用户中表现优于部分用户，但LLM在时间效率和内存效率上表现突出，而在更困难的Codeforces竞赛中则处于劣势。  尽管当前情况并未立即构成威胁，但LLM在这些平台上的表现令人担忧，未来需要改进以提高其性能。|
|**2024-09-09**|**Benchmarking Chinese Knowledge Rectification in Large Language Models**|Tianhe Lu et.al.|[2409.05806](http://arxiv.org/abs/2409.05806)|**[link](https://github.com/zjunlp/easyedit)**|**大型语言模型（LLM）展现出惊人的生成能力，但它们并非没有缺陷，特别是存在幻觉的问题。当LLM应用于特定语言和领域时，这一问题尤为突出。例如，在处理中国古代诗歌、谚语或成语时，LLM可能会生成毫无意义的信息，这是由于缺乏特定知识造成的。为此，本文提出了一种针对LLM的基准，通过知识编辑来纠正中文知识。具体来说，我们通过从各种来源收集七种类型的知识，包括古典文本、成语以及来自百度贴吧“求诸家”的内容，构建了一个新的中文数据集CKnowEdit，以应对中文语言特有的复调性、反讽性和逻辑结构。通过对这个数据集的分析，我们揭示了当前LLM在掌握中文方面的挑战。此外，我们在该数据集上对现有的知识编辑技术进行评估，发现对中文知识的修正仍存在巨大的提升空间。代码和数据集可访问：https://github.com/zjunlp/EasyEdit。**|
|**2024-09-09**|**Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models**|Emily Cheng et.al.|[2409.05771](http://arxiv.org/abs/2409.05771)|null|研究已反复证明，从大型语言模型中提取的中间隐藏状态能够预测对自然语言刺激的测量大脑反应。然而，关于使这一高预测性能成为可能的表示特性的了解非常有限。为什么是中间层而不是输出层在这一独特且高度通用的转移任务中最为有效？在这项工作中，我们展示了功能性磁共振成像中的语言编码模型证据支持大型语言模型内存在两个阶段抽象过程的存在。我们使用流形学习方法表明，这种抽象过程自然地在语言模型训练过程中产生，并且随着训练继续进行，这个抽象过程的第一个“组合”阶段被压缩到更少的层中。最后，我们证明了层次编码性能与大型语言模型表示的内在维度之间存在强烈的对应关系。我们初步证据表明，这种对应关系主要来源于大型语言模型的内在组合性，而非其下一个单词预测属性。|
|**2024-09-09**|**Model Input Verification of Large Scale Simulations**|Rumyana Neykova et.al.|[2409.05768](http://arxiv.org/abs/2409.05768)|null|本文提出了一种用于验证模拟输入数据有效性的方法论，我们将其称为模型输入验证（MIV）。我们通过设计特定于模拟建模需求的数据模式和验证工具在名为FabGuard的工具集中实现了这一方法。本文引入了MIV模式的正式分类，并提供了一个集成到现有模拟工作流程中的简化验证管道。FabGuard在三个不同领域——冲突驱动的人口迁移、灾害疏散以及疾病传播模型——的应用得到了展示。我们还探讨了大型语言模型（LLMs）在自动化约束生成和推理方面的应用。在对一个移民模拟案例的研究中，LLMs不仅正确推断出了23个开发者定义的约束中的22个，而且还发现了现有约束中的错误，并提出了新的有效约束。我们的评估表明，对于大型数据集，MIV是可行的，FabGuard能够在140秒内高效处理12,000个输入文件，并且其性能在不同文件大小下保持一致。|
|**2024-09-09**|**A Novel Idea Generation Tool using a Structured Conversational AI (CAI) System**|B. Sankar et.al.|[2409.05747](http://arxiv.org/abs/2409.05747)|null|本文提出了一种新型的、基于对话的人工智能激活创新界面，作为创意生成工具，旨在帮助初学者设计者缓解通常存在的初始延迟和创新瓶颈问题。这是一个动态、互动且上下文响应式的解决方案，积极地利用人工智能领域自然语言处理（NLP）中的大型语言模型（LLM），以生成针对不同设计问题的多个潜在想法表述。将此类AI模型与创新过程结合，我们称之为“激活创新”情景，旨在促进基于对话的连续互动、上下文相关的对话以及大量的想法生成。  为了验证这一工具的有效性，我们对30名初学者设计师进行了试点研究，让他们使用传统方法和新的基于CAI的界面来为给定问题生成想法。通过专家小组对结果进行的定性比较，我们采用了流畅度、新颖性和多样性作为关键参数。研究发现，所提出的工具能够有效地产生大量、多样且新颖的想法。  为了提高界面的可用性，我们引入了结构化的对话模式，为每个创新阶段设计了提示工程化结构，使其更加统一和方便设计师操作。采用这种结构化的CAI界面后，得到的响应更加简洁，并且与随后的设计阶段，即概念化阶段，更加紧密相关。  综上所述，本文证明了生成式人工智能（Gen-AI）在创意产品设计过程的早期、结构不明确阶段的应用潜力。|
|**2024-09-09**|**LLMs Will Always Hallucinate, and We Need to Live With This**|Sourav Banerjee et.al.|[2409.05746](http://arxiv.org/abs/2409.05746)|null|随着大型语言模型在各个领域的广泛应用，深入探讨它们内在局限性变得至关重要。本文提出，语言模型中的幻觉并非偶然错误，而是这些系统固有的特征。我们通过计算理论和哥德尔第一不完全性定理的引用（涉及Halting、Emptiness和Acceptance问题的不可判定性），展示了幻觉源于LLM的基本数学和逻辑结构。因此，通过架构改进、数据集增强或事实核查机制消除幻觉是不可能的。  我们的分析表明，从训练数据编译到事实检索、意图分类和文本生成的每个阶段，都存在产生幻觉的非零概率。由此，我们引入了结构性幻觉的概念，作为这些系统的固有特性。通过建立幻觉的数学确定性，本文挑战了幻觉可以完全避免的传统观点。|
|**2024-09-09**|**A System and Benchmark for LLM-based Q\&A on Heterogeneous Data**|Achille Fokoue et.al.|[2409.05735](http://arxiv.org/abs/2409.05735)|null|在许多工业环境中，用户希望以自然语言形式提出问题，并从结构化数据源（如电子表格、数据库、API或它们的组合）中获取答案。通常情况下，用户并不知道如何识别或访问正确的数据源。如果需要组装多个（甚至可能是隔离的）数据源来得出答案，这个问题会变得更加复杂。最近，一些依赖大型语言模型（LLMs）的文本到SQL应用已解决了一些这些问题，通过使用户能够用自然语言提出问题。然而，在现实的工业场景中，这些应用仍然不实用，因为它们无法应对典型环境中数据源的异质性。本文旨在通过引入siwarex平台解决异质性问题，该平台允许无缝地使用自然语言访问数据库和API。  为了展示siwarex的有效性，我们扩展了流行的Spider数据集并进行基准测试，通过替换其中的一些表格为数据检索API。我们发现siwarex很好地应对了数据源异质性的问题。我们修改后的Spider基准很快将对研究社区开放。|
|**2024-09-09**|**Towards Democratizing Multilingual Large Language Models For Medicine Through A Two-Stage Instruction Fine-tuning Approach**|Meng Zhou et.al.|[2409.05732](http://arxiv.org/abs/2409.05732)|null|## 上文背景 多语言开源医疗大型语言模型（LLMs）具有服务于不同地区语言多样性的潜力。将通用LLMs适应于医疗领域通常需要持续预训练，但这在计算上成本高昂且有时不可行。仅通过指令微调特定任务可能无法保证最佳性能，因为缺乏广泛领域知识使得模型难以在各种场景下理解和推理。为解决这些挑战，我们引入了两个多语言指令微调数据集：MMed-IFT和MMed-IFT-MC，这两个数据集分别包含了超过20万条高质量的多语种医疗样本，在六种语言中。我们提出了一种两阶段训练范式：第一阶段利用MMed-IFT注入通用医学知识，第二阶段则使用MMed-IFT-MC微调针对特定任务的多项选择题。我们的方法在英语和多语言基准测试中均取得了竞争力的结果，实现了高效性和性能之间的平衡。我们计划在未来将我们的数据集和模型权重公开在\url{https://github.com/SpassMed/Med-Llama3}。  ## 任务 请将上述论文摘要翻译为中文，避免输出其他任何无关内容，并确保输出内容中不包含","字符。|
|**2024-09-09**|**The Influence of Task and Group Disparities over Users' Attitudes Toward Using Large Language Models for Psychotherapy**|Qihang He et.al.|[2409.05703](http://arxiv.org/abs/2409.05703)|null|近年来，心理健康障碍患者的数量持续增长，而大型语言模型（LLM）在不同领域的进步也使得基于LLM的心理治疗引起了越来越多的关注。然而，影响用户对基于LLM心理治疗工具态度的因素鲜有探讨。本文作为首次尝试，旨在研究任务差异和群体差异对用户对基于LLM心理治疗工具的态度的影响。通过运用技术接受模型（TAM）和自动化接受模型（AAM），结合在线问卷调查，我们收集并分析了来自中国大陆222名基于LLM心理治疗工具用户的反馈。研究结果表明，群体差异（即心理健康状况）可以影响用户对LLM工具的态度。进一步地，作为典型任务差异之一的隐私顾虑，并未发现对信任度和使用意图产生显著影响。这些发现可指导未来基于LLM心理治疗服务的设计工作。|
|**2024-09-06**|**RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs**|Jiaxing Wu et.al.|[2409.04421](http://arxiv.org/abs/2409.04421)|null|本文引入了一种名为“基于预测反馈的强化学习（Reinforcement Learning from Prediction Feedback，RLPF）”的方法，旨在解决大型语言模型（Large Language Models，LLMs）在个人化系统中应用时面临的问题。具体而言，当LLMs从用户的过往活动预测行为时，它们的有效性往往取决于能否有效地利用大量、长篇的用户历史数据，而这些数据通常含有噪音且长度过长。现有预训练的LLMs可能生成的摘要虽短小精悍，但缺乏对下游任务至关重要的上下文信息，从而限制了其在个人化系统中的应用。  为了克服这一挑战，RLPF方法通过微调LLMs来生成精炼、人类可读的用户概要，这些概要能够优化下游任务的表现。通过最大化生成概要的有用性，RLPF能够有效提取大量用户历史数据的关键信息，同时保持对下游任务至关重要的信息。实验结果表明，与基线方法相比，RLPF在下游任务性能上显著提升了22%，在事实性、抽象性和可读性等指标上的表现分别达到了84.59%的胜率，同时实现了74%的上下文长度减少，且在16个未见的任务和/或数据集上均有性能提升，这表明其具有良好的泛化能力。  总之，RLPF提供了一种增强LLMs在个人化领域应用的有前景的解决方案，通过将长篇、噪音丰富的用户历史转化为信息丰富、易于理解的表示，从而提高LLMs的个人化能力。|
|**2024-09-06**|**Question-Answering Dense Video Events**|Hangyu Qin et.al.|[2409.04388](http://arxiv.org/abs/2409.04388)|null|在本文中，我们提出了一项新的任务——针对长视频中的密集事件进行问题回答与定位，这要求模型能够准确理解并推理持续时间较长的多个事件。为了支持这一研究，我们构建了一个名为DeVE-QA的数据集，其中包含关于10600个长视频中26000个事件的78000个问题。  现有在单事件问答上表现出色的大型多模态语言模型（MLLMs）在面对DeVE-QA时遇到挑战，这表明它们在处理长时间段内发生的多个事件的理解和推理方面存在局限性。为此，我们提出了一种名为DeVi的新方法，这是一种无需训练即可提升MLLM性能的方法。DeVi通过引入三个关键模块来改进现有的MLLMs：层级描述模块、时间事件记忆模块和自我一致性检查模块。这三个模块分别用于检测、上下文化和记忆长视频中的密集事件，以及定位相关视频片段以进行问题回答。  实验结果表明，与现有MLLMs相比，DeVi在回答密集事件问题和定位相关视频片段方面表现更优。具体而言，在DeVE-QA数据集上，DeVi的G(round)QA准确率提高了4.1%，在NExT-GQA数据集上的准确率提高了3.7%。|
|**2024-09-06**|**Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs**|Aliakbar Nafar et.al.|[2409.04318](http://arxiv.org/abs/2409.04318)|**[link](https://github.com/HLR/LvsR-LLM)**|本文提出了一种评估生成大型语言模型（LLMs）内在学习机制的框架。我们声称，这些机制是通过检索内部知识和通过关注回归任务从上下文中的示例进行学习的组合。首先，我们展示了LLMs在真实世界数据集上执行回归的能力，并设计实验来衡量模型在多大程度上通过检索其内部知识而不是从上下文示例中学习来进行内在学习。我们认为这个过程位于这两个极端之间的连续体上。我们深入分析了根据各种因素（如任务的先验知识以及提供给上下文示例的信息类型和丰富度）这些机制被触发的程度。我们使用三种LLMs并利用多个数据集来验证我们的发现的稳健性。我们的结果揭示了如何根据所解决的问题利用上下文示例中的元学习和促进知识检索的方法。|
|**2024-09-06**|**An optically accelerated extreme learning machine using hot atomic vapors**|Pierre Azam et.al.|[2409.04312](http://arxiv.org/abs/2409.04312)|null|机器学习正逐渐成为一种广泛应用的技术，其增长速度令人印象深刻，原因在于它能够提供解决社会关注问题的实用解决方案的多样性。然而，随着应用和所需资源的增加，当前的硬件技术开始受限。特别是对于大型语言模型或高分辨率图像识别等新型机器学习领域，计算时间与能源成本成为了关键问题。在此背景下，多年来已经设计出了光学平台，旨在开发更高效的机器学习硬件。  其中，自由空间传播平台具有多种优势：并行性、低能耗与计算速度。本文介绍了一种结合光束在热原子蒸气中传播的强烈且可调非线性特性的新设计，并与极端学习机模型相结合。通过数值模拟与实验验证，我们展示了在MNIST图像分类任务中使用此类自由空间非线性传播增强训练的效果。此外，我们指出了实验中的多个超参数，这些参数进一步优化后可以提高平台的准确性。|
|**2024-09-06**|**Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**|Desiree Heim et.al.|[2409.04286](http://arxiv.org/abs/2409.04286)|null|当前公开的知识工作数据集在多样性、详尽注释以及用户和文档的上下文信息方面存在不足，这阻碍了对知识工作辅助系统进行客观和可比较的数据驱动评估与优化。由于在真实环境中收集此类数据所需的资源巨大，以及数据审查的必要性，因此构建这样的数据集几乎不可能实现。鉴于此，我们提出了一种可配置的多代理知识工作数据集生成器。该系统模拟了由生成大型语言模型的文档并相互协作的代理之间的知识工作，并记录了伴随的数据轨迹。此外，生成器在其配置中捕获或在模拟过程中创建的所有背景信息，并以知识图谱的形式存储。最后，产生的数据集可以用于利用和共享，而无需涉及隐私或机密问题。  本文介绍了我们方法的设计愿景，并专注于使用大型语言模型生成真实的知识工作文档。我们的研究中，人类评估者评估了生成文档的53%和真实文档的74%，认为它们具有真实性，这表明了我们方法的潜力。此外，我们分析了参与者评论中提到的真实性标准，并对已识别的常见问题进行了详细说明，提出了改进措施。|
|**2024-09-06**|**Advancing Automated Knowledge Transfer in Evolutionary Multitasking via Large Language Models**|Yuxiao Huang et.al.|[2409.04270](http://arxiv.org/abs/2409.04270)|null|本文引入了一种基于大型语言模型（LLM）的优化范式，以建立一个自主模型工厂，用于生成适用于不同优化任务的知识转移模型。这一方法旨在通过自动化设计过程，实现高效且有效的知识转移。为了评估所提出方法的性能，我们进行了全面的实验研究，将生成的知识转移模型与现有的最佳知识转移方法进行了比较。结果表明，生成的模型在效率和有效性方面均表现出优于或与手工设计的知识转移模型相当的性能。|
|**2024-09-06**|**GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**|Ziyin Zhang et.al.|[2409.04183](http://arxiv.org/abs/2409.04183)|null|在本工作中，我们提出了GALLa - 图形对齐大型语言模型。GALLa 利用图神经网络和跨模态对齐技术，在微调过程中向LLM注入代码的结构信息作为辅助任务。这种框架既无模型依赖性也无任务依赖性，它可以应用于任何代码LLM用于任何代码下游任务，并仅在训练时从与微调数据无关的语料库中获取结构化图形数据，而在推理阶段无需额外成本。通过四种不同基线LLM（参数量从3.5亿到80亿不等）在五个代码任务上的实验验证了GALLa的有效性，即使对于强大的模型如LLaMA3，也证明了其一致性改进。|
|**2024-09-06**|**Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**|Larissa Pusch et.al.|[2409.04181](http://arxiv.org/abs/2409.04181)|null|自然语言处理领域的进步极大地改变了我们与数据库等信息系统的交互方式，使其变得更加便捷。然而，在关键准确性领域，如生物医学领域，仍存在挑战。其中一个重要问题是幻觉问题，即模型生成了数据支持之外的信息，这可能导致危险的错误信息。本文提出了一种新颖的方法，旨在通过结合大型语言模型（LLM）和知识图谱（KG）来改善问答系统的准确性和可靠性，以生物医学KG为例。该方法基于LangChain框架构建，通过引入查询检查器确保LLM生成的查询在语法和语义上的有效性，然后使用这些查询从知识图谱中提取信息，大幅减少了错误如幻觉的发生。  我们使用了一个包含50个生物医学问题的新基准数据集对整体性能进行了评估，测试了包括GPT-4 Turbo和llama3:70b在内的几种LLM。结果显示，虽然GPT-4 Turbo在生成准确查询方面表现出色，但开源模型如llama3:70b在适当的问题提示工程下也显示出潜力。为了使这种方法易于访问，我们开发了一个用户友好的Web界面，允许用户输入自然语言查询，查看生成和修正的Cypher查询，并验证结果路径的准确性。  总体而言，这种混合方法有效地解决了数据缺口和幻觉等常见问题，提供了一个可靠且直观的解决方案来改进问答系统。生成本文结果和用户界面所需源代码的Git仓库链接如下：https://git.zib.de/lpusch/cyphergenkg-gui|
|**2024-09-06**|**From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks**|Andreas Stephan et.al.|[2409.04168](http://arxiv.org/abs/2409.04168)|null|为了减少对人工标注的需求，提出了大型语言模型（LLM）作为候选模型质量的评判者。这些LLM评判者通常通过在摘要或机器翻译等生成任务上与人类判断的相关性来评估。相比之下，我们研究了在数学推理任务上的LLM评判者。这类任务需要多步推理，其解答的正确性可以验证，从而提供了一种更客观的评估方式。我们进行了详细的表现分析，并发现使用的评判者大多无法提高任务性能，但能够选择更好的模型。我们的分析揭示了评判表现与候选模型任务表现之间的强相关性。观察到评判者倾向于选择更高质量的模型，即使其答案是错误的。进一步地，我们展示了可以通过统计措施，如候选模型的任务性能，来预测评判表现。在消融实验中，我们交换或屏蔽候选答案，并观察到评判者经常保持原始判断，这提供了证据表明评判者在判断中融入了写作风格。总之，我们发现使用统计指标量化判断中的规律性，并提供了利用它们的各种角度。|
|**2024-09-06**|**Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation**|Luis Mayer et.al.|[2409.04164](http://arxiv.org/abs/2409.04164)|null|近年来，大型语言模型（LLMs）作为一种强大的工具，在多个领域展现出潜力，包括软件工程。在本研究中，我们评估了五款最先进的LLM——Bard、BingChat、ChatGPT、Llama2和Code Llama——在文本到代码生成任务上的能力。我们通过向模型提供来自编程网站LeetCode的编码问题描述文本提示，要求它们用Python编写解决方案。随后，我们使用LeetCode的测试功能来评估生成输出的质量。  研究结果表明，这些模型在性能上存在显著差异。ChatGPT在处理这类编程挑战方面表现最为有效，甚至超过了专门针对代码的模型，如Code Llama。为了进一步了解情况，我们测量了生成代码的运行时间和内存使用情况，并将其与LeetCode上的其他代码提交进行了比较。详细错误分析包括比较生成代码中的正确缩进和形式差异，以及将未解决的任务归类到特定错误类别，有助于我们更深入地理解结果并找到改进空间。研究结果还显示，当模型面临大量上下文信息时，即较长提示时，生成的代码越来越不准确。|
|**2024-09-05**|**Attention Heads of Large Language Models: A Survey**|Zifan Zheng et.al.|[2409.03752](http://arxiv.org/abs/2409.03752)|**[link](https://github.com/iaar-shanghai/awesome-attention-heads)**|**自ChatGPT问世以来，大型语言模型在各种任务上表现出色，但它们仍然作为黑盒系统存在。因此，其发展主要依赖于数据驱动的方法，限制了通过改变内部架构和推理路径来提升性能的可能性。许多研究者开始探索大型语言模型的内部机制，旨在识别推理瓶颈的本质，大多数研究集中在注意力头部上。我们的综述旨在通过聚焦于大型语言模型的可解释性和注意力头部的内在机制，揭示其内部推理过程。首先，我们将人类思考过程提炼为四个阶段框架：知识回忆、情境内识别、潜在推理和表达准备。利用这一框架，我们系统地回顾现有研究，识别并分类特定注意力头部的功能。此外，我们总结了发现这些特殊头部所使用的实验方法，分为无模型方法和有模型方法两大类。我们也概述了相关评估方法和基准。最后，我们讨论当前研究的局限性，并提出几个潜在的发展方向。我们的参考文献列表开源于<https://github.com/IAAR-Shanghai/Awesome-Attention-Heads>。**|
|**2024-09-05**|**LLM-CI: Assessing Contextual Integrity Norms in Language Models**|Yan Shvartzshnaider et.al.|[2409.03735](http://arxiv.org/abs/2409.03735)|null|大型语言模型（LLM）在从互联网上收集的数据中记忆部分训练数据的同时，也可能无意中编码了社会偏好和规范。随着这些模型被整合到社会技术系统中，确保它们编码的规范符合社会期望至关重要。这些规范可能因模型、超参数、优化技术以及数据集的不同而不同。由于提示敏感性的问题——微小的提示变化会导致不同的响应，现有的评估方法变得不可靠。需要一个全面的框架来涵盖各种模型、优化和数据集，并提供可靠的方法来评估编码的规范。  我们提出了LLM-CI，这是第一个用于评估LLM中编码隐私规范的开源框架。LLM-CI使用基于上下文完整性因素的情境叙述方法来评估不同上下文中和不同LLM中的编码规范。我们提出了一种多提示评估方法来解决提示敏感性问题，通过仅从导致多个变体一致响应的提示中评估规范，以全面评估使用先前工作中的IoT和COPPA情景数据集的LLM。  通过使用LLM-CI和我们提出的这种方法，我们全面地评估了LLM，研究了模型属性（如超参数、容量）和优化策略（如对齐、量化）的影响。|
|**2024-09-05**|**Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry**|Meena Jagadeesan et.al.|[2409.03734](http://arxiv.org/abs/2409.03734)|null|本文从经济和算法两个角度研究大型语言模型等大规模机器学习（ML）模型市场中的集中问题，以及是否存在进入此类市场的不可克服障碍。我们通过正式定义一个多目标高维回归框架来探讨降低进入壁垒的问题，该框架捕捉到了声誉损害的特征，并分析了新公司进入市场所需的样本数量。我们的结果表明，多目标考虑能够从根本上降低进入壁垒——所需样本数量可能远小于现有公司的数据集大小。在证明这些结果的过程中，我们还发展了多目标环境中高维线性回归的缩放定律，展示了当数据集规模较大时，缩放率会变得较慢，这一发现可能具有独立的研究价值。|
|**2024-09-05**|**Planning In Natural Language Improves LLM Search For Code Generation**|Evan Wang et.al.|[2409.03733](http://arxiv.org/abs/2409.03733)|**[link](https://github.com/scaleapi/plansearch)**|在大规模提升训练计算能力的同时，推理计算的规模扩展并未带来类似的进步。我们假设，这一领域缺乏关键性的突破在于生成模型的输出多样性不足，导致搜索效率低下，因为模型不断产生高度相似但错误的结果。通过实证研究，我们发现提高输出多样性可以有效缓解这一问题。  基于这一发现，我们提出了一种名为PLANSEARCH的新颖搜索算法，它在人类评价、MBPP+和LiveCodeBench（一个用于竞争性编程的无污染基准）等任务上表现出色。该算法通过生成关于问题的多样观察，并利用这些观察构建解决策略，来探索比传统方法更广泛的潜在解决方案空间。在使用PLANSEARCH结合Claude 3.5 Sonnet进行优化后，我们实现了LiveCodeBench上77.0%的通过率（pass@200），这不仅超越了不使用搜索方法（pass@1=41.4%）的结果，也优于仅依赖重复采样的方法（pass@200=60.6%）。此外，我们还展示了能够准确预测搜索带来的性能提升，其关键因素是生成想法的多样性。|
|**2024-09-06**|**RAG based Question-Answering for Contextual Response Prediction System**|Sriram Veturi et.al.|[2409.03708](http://arxiv.org/abs/2409.03708)|null|本文介绍了一种端到端的框架，利用大型语言模型（LLMs）的检索增强生成（RAG）能力，针对实际工业应用中的问题回答场景。给定客户查询，该系统会检索相关知识文档，并结合之前的聊天历史，为零售公司的客服中心提供客户服务代表生成响应建议。通过全面的自动化和人工评估，结果显示，这种解决方案在准确性和相关性上优于当前基于BERT的算法。我们的研究结果表明，基于RAG的LLMs可以作为人类客户服务代表的优秀辅助工具，减轻他们的工作负担。|
|**2024-09-05**|**TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course Scheduling Problems**|Stylianos Loukas Vasileiou et.al.|[2409.03671](http://arxiv.org/abs/2409.03671)|null|我们提出了一种名为TRACE-cs的新型混合系统，它结合了符号推理与大型语言模型（LLM），以解决排程问题中的对比查询。TRACE-cs利用SAT求解技术编码排程约束，并生成用户查询的解释，同时通过大型语言模型将用户的查询转换为逻辑条目，并细化符号求解器生成的解释为自然语言句子。通过整合这些组件，我们的方法展示了将符号方法与LLM相结合，创建具有正确性保证的可解释AI代理的潜力。|
|**2024-09-05**|**A Fused Large Language Model for Predicting Startup Success**|Abdurahman Maarouf et.al.|[2409.03668](http://arxiv.org/abs/2409.03668)|null|为了帮助投资者做出有效的决策并持续寻找盈利的创业投资机会，需要预测初创公司的成功率。如今，投资者不仅可以利用有关初创公司的各种基本面信息（如公司的成立时间、创始人数量以及所处行业），还可以通过在线风险投资（VC）平台获取关于公司创新和业务模式的文本描述信息，例如Crunchbase。为了支持投资者的决策，我们开发了一种机器学习方法，旨在在VC平台上定位成功的初创公司。具体而言，我们开发、训练并评估了一个专门的融合大型语言模型，用于预测初创公司的成功率。我们的工作旨在评估VC平台上公司的自我描述在多大程度上能够预测其成功性。使用来自Crunchbase的20,172个在线资料档案，我们发现我们的融合大型语言模型可以预测初创公司的成功率，其中文本自我描述对预测能力贡献了显著部分。我们的工作提供了一个决策支持工具，帮助投资者找到盈利的投资机会。|
|**2024-09-05**|**The representation landscape of few-shot learning and fine-tuning in large language models**|Diego Doimo et.al.|[2409.03662](http://arxiv.org/abs/2409.03662)|**[link](https://github.com/diegodoimo/geometry_icl_finetuning)**|**本文探讨了在特定任务上改进现代大型语言模型（LLM）性能的两种常见策略：上下文学习（ICL）和监督微调（SFT）。尽管这两种方法的本质不同，但它们往往能产生相似的性能提升。然而，我们对它们是否在LLM内部诱导出相似的表示结构知之甚少。我们通过分析这两种情况下隐藏表示的概率景观来解决这个问题。具体来说，我们在相同的问答任务上比较了LLM的表现，发现ICL和SFT产生了非常不同的内部结构，两者都在网络的中间部分经历了一个明显的转变。在模型的前半部分，ICL塑造了分层组织的可解释表示，按照其语义内容进行排序。相比之下，SFT得到的概率景观更加模糊且语义混杂。在网络的后半部分，微调后的表示发展出了更有利于编码答案身份的概率模式，而ICL表示的概率峰则不太明确。我们的方法揭示了LLM在不同条件下解决相同任务时所采用的多样化计算策略，这有助于我们朝着设计出从语言模型中提取信息的最佳方法迈进。**|
|**2024-09-06**|**LLM-based multi-agent poetry generation in non-cooperative environments**|Ran Zhang et.al.|[2409.03659](http://arxiv.org/abs/2409.03659)|**[link](https://github.com/zhangr2021/Multiagent_poetry)**|**尽管大型语言模型（LLM）在自动诗歌生成领域取得了显著进展，但生成的诗歌在多样性方面存在不足，且训练过程与人类学习方式大相径庭。基于这样的考虑，我们提出了一种基于社会学习的框架，在此框架下，我们强调非合作互动，以鼓励多样性，同时除了合作互动外还强调非合作互动。我们的实验是首次尝试在非合作环境中使用基于训练的多智能体系统（GPT-2）和基于提示的系统（GPT-3 和 GPT-4）进行诗歌生成。  根据对生成的96,000首诗歌的评估，我们的框架对基于训练的智能体的诗歌生成过程产生了积极影响，导致以下结果：1）多样性增加了3.0-3.7个百分点（pp），新颖性增加了5.6-11.3个百分点，根据独特和新颖的n-grams评估。生成的诗歌在词汇、风格和语义方面也表现出群体差异。基于提示的智能体在我们的框架中也从非合作环境中获益，具有非同质智能体的多样化的模型组合有可能进一步提高多样性，实验结果显示多样性增加了7.0-17.5个百分点。然而，基于提示的智能体显示了随着时间推移词汇多样性的下降，并没有展现出旨在在社交网络中实现的群体间分化。  本文认为，在诸如自动诗歌生成等创意任务中，需要进行范式转变，引入类似于人类交互的社会学习过程（通过基于LLM的智能体建模），以促进更加多样性和创新的生成。**|
|**2024-09-05**|**From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents**|Jifan Yu et.al.|[2409.03512](http://arxiv.org/abs/2409.03512)|null|自最早的在线教育实例出现，课程被上传至可访问并共享的在线平台以来，这种扩大知识传播范围、触及更广泛受众的形式引发了广泛讨论和普遍采纳。认识到个性化学习仍存在改进空间，人工智能技术不断融入这一学习模式，由此产生了多种教育AI应用，如教育推荐和智能辅导。大型语言模型（LLMs）智能的涌现，使得这些教育增强功能得以基于统一的基础模型构建，实现更深层面的整合。在此背景下，我们提出MAIC（大规模AI赋能课程），这是一种新的在线教育形式，利用LLM驱动的多代理系统构建AI辅助课堂，平衡了规模性和适应性。除了探索概念框架和技术创新外，我们在清华大学——中国顶尖大学之一——进行了初步实验。通过超过10万条学习记录和500多名学生的数据，我们获得了宝贵观察和初步分析。这个项目将持续发展，最终目标是建立一个全面开放的平台，支持和统一研究、技术和应用，在大模型AI时代探索在线教育的可能性。我们设想这个平台是一个合作枢纽，汇集教育者、研究人员和创新者共同探索AI驱动在线教育的未来。|
|**2024-09-04**|**RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)**|Yao Mu et.al.|[2409.02920](http://arxiv.org/abs/2409.02920)|null|本篇论文介绍了一种名为RoboTwin的新型基准数据集，它结合了现实世界中的遥控数据与通过数字孪生生成的合成数据。RoboTwin旨在为双臂机器人场景提供支持，特别关注工具使用能力和人机交互能力。我们利用COBOT Magic平台收集了丰富的数据，涵盖工具操作和人机互动的多样性。  论文提出了一种创新的方法来创建数字孪生体，利用AI生成的内容将二维图像转换为详细的三维模型。同时，我们借助大型语言模型生成专家级训练数据和面向功能性的任务特定姿态序列。  我们的主要贡献包括： 1. RoboTwin基准数据集， 2. 高效的现实到模拟管道，以及 3. 利用语言模型进行自动专家级数据生成。  这些进展旨在解决机器人训练数据稀缺的问题，有望加速开发更多功能强大、适应性广泛的机器人系统，应用于广泛的现实世界场景。项目页面可访问：https://robotwin-benchmark.github.io/early-version/|
|**2024-09-05**|**LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA**|Jiajie Zhang et.al.|[2409.02897](http://arxiv.org/abs/2409.02897)|**[link](https://github.com/THUDM/LongCite)**|尽管当前的长文本大语言模型在基于大量文本回答用户问题方面表现出令人印象深刻的性能，但它们缺乏引用使得用户难以验证答案的准确性，从而引发了对其可靠性的担忧，因为它们可能产生错误的信息。我们的工作旨在使这些长文本大语言模型能够生成包含精细句级引用的响应，以提高它们的忠实度和可验证性。  我们首先引入了LongBench-Cite，一个自动评估当前大语言模型在长文本上下文问题回答中的表现的基准，揭示了在句级引用方面存在巨大的改进空间。为了实现这一目标，我们提出了CoF（粗到细）这一新颖的管道，利用现成的大语言模型自动生成包含精确句级引用的长文本问答实例，并以此管道构建了LongCite-45k，一个用于句级引用问题的大型自监督训练数据集。最后，我们使用LongCite-45k数据集训练了LongCite-8B和LongCite-9B模型，成功地使它们能够在单个输出中生成准确的响应和精细的句级引用。在LongBench-Cite上的评估结果显示，我们的训练模型在引用质量方面达到了最先进的水平，超越了包括GPT-4在内的高级专有模型。|
|**2024-09-04**|**LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture**|Xidong Wang et.al.|[2409.02889](http://arxiv.org/abs/2409.02889)|**[link](https://github.com/freedomintelligence/longllava)**|**扩展多模态大语言模型（MLLMs）的长期上下文能力对于视频理解、高分辨率图像理解和多模态代理至关重要。这涉及到一系列系统优化，包括模型架构、数据构造和训练策略，尤其是解决随着更多图像引入而出现的性能下降以及高昂计算成本等问题。本文通过将模型架构调整为Mamba和Transformer块的混合体、采用既能考虑多个图像间时间依赖性又能考虑空间依赖性的数据构造方法，并实施渐进式训练策略，对这些挑战进行了应对。发布的模型“LongLLaVA”（长期语言与视觉助手）是首个混合型MLLM，实现了效率与效果之间的良好平衡。LongLLaVA不仅在各种基准测试中取得了竞争力的结果，而且保持了高吞吐量和低内存消耗的特点。特别地，它能够在单个A100 80GB GPU上处理近一千张图片，展示了广泛任务应用前景的潜力。**|
|**2024-09-04**|**Historical German Text Normalization Using Type- and Token-Based Language Modeling**|Anton Ehrmanntraut et.al.|[2409.02841](http://arxiv.org/abs/2409.02841)|null|本文提出了一种针对1700年至1900年德国文学文本的正词法规范化系统，该系统基于平行语料库训练。所提出的系统利用机器学习方法和Transformer语言模型，结合编码器-解码器模型对单个词汇类型进行规范化，并通过预训练的因果语言模型在上下文中调整这些规范化结果。广泛评估表明，该提出的系统提供了最先进的准确性，与完全端到端的句子级规范化系统相当，该系统是通过对预训练的Transformer大型语言模型进行微调而实现的。然而，由于模型难以泛化以及缺乏大量高质量平行数据，历史文本的规范化仍是一个挑战。|
|**2024-09-04**|**Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models**|Moein Shahiki Tash et.al.|[2409.02836](http://arxiv.org/abs/2409.02836)|null|本文通过运用高级自然语言处理技术，对加密货币相关讨论中的预测陈述、希望演讲及悔恨检测行为进行分析。我们提出了一种新的分类方法——“预测陈述”，将其细分为预测增加、预测减少、预测中立或非预测类别。利用GPT-4o这一前沿大规模语言模型，我们在五大主流加密货币（Cardano、Binance、Matic、Fantom、Ripple）的讨论中探索了情绪动态。研究发现，Matic在乐观预测方面显示出特别高的倾向性。此外，我们还探讨了希望与悔恨情绪之间的相互作用，揭示了这些情感与预测行为之间复杂的互动模式。尽管面临数据量和资源可用性方面的限制，我们的研究仍揭示了加密货币市场投资者行为和情绪趋势的重要发现，为战略决策和未来研究提供了信息。|
|**2024-09-04**|**CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models**|Wentao Liu et.al.|[2409.02834](http://arxiv.org/abs/2409.02834)|**[link](https://github.com/ecnu-icalk/educhat-math)**|本文发布了一个名为CMM-Math的中文多模态数学数据集，包含基准和训练部分，旨在评估和增强大型多模态模型（LMM）在数学推理方面的表现。CMM-Math包含了超过28,000个高质量样本，涵盖了从小学到高中的中国12个年级的多种问题类型（例如选择题、填空题等），并提供了详细的解决方案。特别地，问题或观点中可能包含视觉上下文，使得这个数据集更具挑战性。通过全面分析，我们发现当前最先进的LMM在CMM-Math数据集上面临挑战，这强调了在LMM开发方面进一步改进的必要性。为此，我们提出了一种名为Multimodal Mathematical LMM（Math-LMM）的模型来处理混合输入的多个图像和文本段落的问题。我们采用三个阶段进行模型训练：基础预训练、基础微调和数学微调。广泛的实验表明，我们的模型在与三个多模态数学数据集上的SOTA LMM进行比较时，有效地提高了数学推理性能。|
|**2024-09-04**|**ExpLLM: Towards Chain of Thought for Facial Expression Recognition**|Xing Lan et.al.|[2409.02828](http://arxiv.org/abs/2409.02828)|null|面部表情识别（FER）在多媒体领域至关重要，对各种应用具有重大影响。然而，理解面部表情的原因对于准确识别表情至关重要。目前的方法，如基于面部动作单位（AUs）的方法，通常提供AU名称和强度，但缺乏关于AU之间的互动以及整体表情之间关系的洞察。本文提出了一种名为ExpLLM的新方法，利用大型语言模型生成面部表情识别的准确思维链（CoT）。我们从三个关键视角设计了CoT机制：关键观察、总体情感解释和结论。关键观察描述了AU的名称、强度及其相关情感。总体情感解释基于多个AU及其互动进行分析，确定主导情感及其关系。最后，结论基于前一分析得出最终的表情标签。此外，我们还引入了Exp-CoT引擎，用于构建此表情CoT并生成指令描述数据以训练我们的ExpLLM。在RAF-DB和AffectNet数据集上的大量实验表明，ExpLLM优于当前最先进的面部表情识别方法。在微表情识别方面，ExpLLM也超越了最新的GPT-4o，尤其是在GPT-4o经常失败的情况下。|
|**2024-09-04**|**Design Contradictions: Help or Hindrance?**|Aron E. Owen et.al.|[2409.02823](http://arxiv.org/abs/2409.02823)|null|在数据可视化领域，创新思维的迫切需求促使我们探索新的创意方法。通过组合两个或更多具有对立性质的创造性词汇，能够激发新型想法与设计，对创意过程产生积极影响。随着人工智能驱动设计的发展，一个关键问题浮出水面：这些设计矛盾是否能与AI工具协同工作？目前答案是否定的。AI系统，尤其是大型语言模型（LLMs），依赖于产生相似性的算法，而创造力往往需要差异性和新颖性。这份海报开启了关于如何引导AI系统变得更具创造性和生成新想法的对话。这项研究邀请我们重新考虑传统设计方法，并探索AI驱动世界中的新方法。我们能否应用传统的设计方法，如双钻石模型，或者是否需要新的设计工程方法？如何利用生成式AI快速设计可视化并构思新想法？这篇论文旨在开启这一重要对话，并提供有关AI在推动数据可视化创意方面的潜力的实用见解。|
|**2024-09-04**|**Language Understanding as a Constraint on Consensus Size in LLM Societies**|Giordano De Marzo et.al.|[2409.02822](http://arxiv.org/abs/2409.02822)|null|在大型语言模型（LLM）的应用朝着协作任务发展的情况下，多个代理相互作用，如同一个LLM社会。在这种背景下，大量的LLM能够通过自我组织方式达成关于任意规范的共识，这些规范在信息支持某一选项优于另一选项的情况下不存在。为了理解LLM是否与人类社会一样，在没有机构的情况下能够达到共识，我们应用了复杂科学的方法和行为科学的原则，开创了一种AI人类学的新方法。研究发现，LLM能够在群体中达成共识，并且LLM的意见动态可以用一个由多数力量系数参数化的函数来理解，该系数决定了共识是否可能。对于具有更高语言理解能力的模型而言，这种多数力量更强，而对于较大的群体而言则会减弱，导致存在一个临界群体大小，超过这个大小，对于给定的LLM，达成共识变得不可能。这一临界群体大小随着模型的语言理解能力的增长呈指数级增长，对于最先进的模型而言，其可以达到远超非正式人类群体典型规模的数量级。|
|**2024-09-04**|**Towards a Unified View of Preference Learning for Large Language Models: A Survey**|Bofei Gao et.al.|[2409.02795](http://arxiv.org/abs/2409.02795)|**[link](https://github.com/kbsdjames/awesome-llm-preference-learning)**|大型语言模型（LLM）展现了惊人的能力。实现成功的关键因素之一是使LLM的输出与人类偏好保持一致。这一过程通常需要少量数据就能高效提升LLM的表现。尽管有效，但在这一领域的研究覆盖了多个领域，相关方法相对复杂难以理解。不同方法之间的关系尚未得到充分探索，限制了偏好调整策略的发展。鉴于此，我们分解了现有流行调整策略的四个组成部分，并提供了一个统一框架来研究当前的调整策略，以此建立它们之间的联系。在本文综述中，我们将所有偏好学习策略分解为四个部分：模型、数据、反馈和算法。这种统一视角为现有调整算法提供了深入理解，并且也开启了整合不同策略优势的可能性。此外，我们详细介绍了现有主流算法的工作示例，以帮助读者全面了解。最后，基于我们的统一视角，我们探讨了调整大型语言模型与人类偏好之间的挑战以及未来研究方向。|
|**2024-08-30**|**SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists**|Raoyuan Zhao et.al.|[2408.17437](http://arxiv.org/abs/2408.17437)|**[link](https://github.com/loreley99/syntheval_checklist)**|**在自然语言处理（NLP）领域，传统的基准测试通常使用静态预留测试集。然而，这种方法往往会导致性能过估计，并缺乏提供全面、可解释和动态评估NLP模型的能力。近期，如DynaBench（Kiela等，2021年）和CheckList（Ribeiro等，2020年）等作品通过多步骤人工注释管道生成测试类型来解决这些问题，以对NLP模型进行行为测试。不幸的是，手动创建各种测试类型需要大量的人力劳动，成本高昂。本研究提出了一种名为SYNTHEVAL的混合行为测试框架，利用大型语言模型（LLMs）生成大量测试类型，为NLP模型进行全面评估。SYNTHEVAL首先通过LLMs进行受控生成生成句子，然后通过比较LLMs与特定任务的NLP模型的预测结果来识别挑战性示例。最后阶段，由人类专家调查这些挑战性示例，手动设计模板，并确定特定任务模型一致表现的失败类型。我们将SYNTHEVAL应用于情感分析和有毒语言检测两个分类任务上，并展示了我们的框架在识别这些任务中强大模型的弱点方面的有效性。我们分享了代码于https://github.com/Loreley99/SynthEval_CheckList。**|
|**2024-08-30**|**Advancing Multi-talker ASR Performance with Large Language Models**|Mohan Shi et.al.|[2408.17431](http://arxiv.org/abs/2408.17431)|null|在自动语音识别（ASR）领域，识别对话场景中的重叠语音是极具挑战性的问题。传统的处理方法通过序列输出训练（SOT），即将多个说话者的声音排放时间按照其发言顺序进行拼接，来解决多说话者ASR问题。然而，这种从对话中拼接相关话语的转录依赖于构建长上下文的能力。相比之下，基于大型语言模型（LLM）的新方法可能更适合处理这类复杂且具有挑战性的场景，因为它利用了预训练解码器的强大能力。本文提出了一种基于LLM的SOT方法用于多说话者ASR，该方法利用预训练的语音编码器和LLM，并通过适当的策略对多说话者数据集进行微调。实验结果表明，我们的方法在模拟数据集LibriMix上优于传统的方法，并在真实世界数据集AMI的评估集上达到了最先进的性能，显著超越了之前使用1000倍更多监督数据训练的AED模型。|
|**2024-08-30**|**Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach**|Jialiang Wei et.al.|[2408.17404](http://arxiv.org/abs/2408.17404)|**[link](https://github.com/jl-wei/feature-inspiration)**|在过去十年中，借鉴应用商店（AppStore）的规范获取方法被证明非常有益。开发者经常研究竞争对手的应用程序以收集新功能的灵感。随着生成式人工智能的进步，最近的研究表明大型语言模型（LLM）启发的规范获取具有潜力。LLMs可以在这一过程中提供新功能想法的灵感。尽管这两种方法在实践中越来越受欢迎，但它们之间的差异缺乏深入理解。我们进行了一项比较研究，对比了应用商店和LLM启发的方法在细化功能为子功能时的表现。通过手动分析从两种方法推荐的1200个子功能，我们识别出了它们的优点、挑战以及关键差异。尽管两种方法都推荐了高度相关且描述清晰的子功能，但LLMs在特别涉及未见应用范围的新颖性方面似乎更为强大。此外，一些推荐的功能是虚构的，其可行性不明确，这强调了人类分析师在获取过程中的重要性。|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377](http://arxiv.org/abs/2408.17377)|null|大型语言模型（LLM）通过下一个词预测（NTP）范式进行训练，展示了强大的能力。然而，现有的NTP范式存在几个限制，特别是在计划任务复杂性和推理阶段的错误传播方面。我们的工作扩展了对NTP的批评，指出其限制还源于训练目标狭窄：预测一个次优的一热分布。为了支持这一批评，我们进行了一项预实验，将强大的LLM的输出分布视为高效的世界数据压缩。通过评估n-gram分布与LLM输出分布之间的相似性，我们发现n-gram分布与LLM输出分布更为一致。基于这一洞察，我们引入了下一个分布预测（NDP），使用n-gram分布来替换一热目标，从而增强学习过程而无需额外的在线训练时间。我们在翻译、通用任务、语言迁移和医疗领域适应等四个领域进行了实验。与NTP相比，NDP在翻译任务上可达到+2.97 COMET改进，在通用任务上平均改善+0.61，在医疗领域上平均改善+10.75。这表明解决目标狭窄问题的具体益处，并指出了未来改进NTP的一个新方向。|
|**2024-08-30**|**Assessing Generative Language Models in Classification Tasks: Performance and Self-Evaluation Capabilities in the Environmental and Climate Change Domain**|Francesca Grasso et.al.|[2408.17362](http://arxiv.org/abs/2408.17362)|**[link](https://github.com/stefanolocci/LLMClassification)**|**本文探讨了两种大型语言模型（LLMs）GPT3.5和Llama2以及一种小型语言模型（SLM）Gemma在气候变化（CC）和环境领域内的三种不同分类任务中的性能。通过使用基于BERT的模型作为基准，我们将这些转换器基模型与它们进行比较。此外，我们还评估了模型的自我评估能力，通过分析这些文本分类任务中的口头信心分数的校准情况。我们的发现表明，尽管基于BERT的模型通常在所有模型中表现最佳，但大生成模型的性能仍然值得注意。进一步地，我们的校准分析显示，Gemma在初期任务中表现出良好的校准性，随后产生不一致的结果；Llama具有合理的校准性，而GPT始终表现出强大的校准性。通过这项研究，我们旨在为讨论大型生成型LM在解决地球最紧迫问题方面的适用性和有效性做出贡献，特别是在生态学和CC背景下突出其优势和限制。**|
|**2024-08-30**|**Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage**|Md Rafi Ur Rashid et.al.|[2408.17354](http://arxiv.org/abs/2408.17354)|null|针对私有数据进行下游应用的大型语言模型微调存在重大隐私风险，可能泄露敏感信息。当前社区平台提供了方便的大规模预训练模型分发，任何人都可以发布而无需严格的验证。这种情境下，隐私威胁显著增加，因为预训练模型可能被故意篡改以在微调过程中泄露私人数据。本研究引入了一种新颖的中毒技术，使用模型卸载作为攻击工具。这种方法通过调整预训练语言模型来提高微调过程中的私人数据泄露程度。我们的方法在保持模型实用性的同时，增强了成员归属性和数据提取攻击的效果。实验结果在不同模型、数据集和微调设置下显示，我们的攻击显著超越了基准性能。这项工作向下载未经过严格验证来源预训练模型的用户发出了警告，突显了潜在的风险。|
|**2024-08-30**|**Bridging Domain Knowledge and Process Discovery Using Large Language Models**|Ali Norouzifar et.al.|[2408.17316](http://arxiv.org/abs/2408.17316)|**[link](https://github.com/alinorouzifar/imr-llm)**|**发现优质流程模型对于执行不同的流程分析任务至关重要，如一致性检查和流程改进。自动化流程发现方法往往忽视了有价值的专业领域知识。这些知识，包括来自专业领域专家的见解和详细流程文档，通常在流程发现过程中未得到充分利用。本文通过利用大型语言模型（LLMs）直接将此类知识整合到流程发现中来解决这一问题。我们使用从LLMs中提取的规则来指导模型构建过程，确保其与领域知识和实际流程执行保持一致。通过整合LLMs，我们建立了一座连接以自然语言表达的流程知识与发现稳健流程模型之间的桥梁，显著推进了流程发现方法论。为了展示我们框架的实用性，我们进行了一个案例研究，对象是UWV员工保险公司，这证明了其实际优势和有效性。**|
|**2024-08-30**|**Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts**|Rhui Dih Lee et.al.|[2408.17280](http://arxiv.org/abs/2408.17280)|null|我们提出了一种工具包，用于从已训练的模型创建低成本的领域专家混合（MOE）。该工具包可以用于从模型或适配器创建混合。我们进行了广泛的测试，并提供了关于使用工具包定义结果MOE架构的指导。公开了一个可用的存储库。|
|**2024-08-30**|**Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach**|Tong Nie et.al.|[2408.17258](http://arxiv.org/abs/2408.17258)|null|电子商务和城市化的蓬勃发展，极大地增强了城市区域的配送活动，导致了需求量的增加与复杂性的提升。为了应对这些挑战，数据驱动的预测方法，特别是基于机器学习的技术，开始在城市配送需求管理问题中发挥关键作用。然而，一个尚未得到充分研究的问题是全城范围内的配送需求联合估计与预测。针对这一问题，我们将其建模为一个基于图的时空学习任务。  首先，我们定义了一个消息传递神经网络模型来捕捉相关区域之间需求模式的交互。其次，通过利用大型语言模型的最新进展，我们从未结构化的地理位置数据中提取通用的地理空间知识编码，并将其整合到需求预测器中。最后，为了促进模型在不同城市的迁移能力，我们设计了一种端到端的归纳训练方案。  我们在两个真实的配送数据集上进行了广泛的实验验证，包括中国的八个城市和美国的城市，结果表明我们的模型在这些具有挑战性的任务中显著优于现有的基准方法。|
|**2024-08-30**|**VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters**|Mouxiang Chen et.al.|[2408.17253](http://arxiv.org/abs/2408.17253)|**[link](https://github.com/keytoyze/visionts)**|**本文探讨了从丰富且高质量的自然图像出发构建时间序列预测（TSF）基础模型的新路径。现有的方法要么通过微调大型语言模型（LLM），要么建立大规模时间序列数据集来开发TSF基础模型，但这些方法面临跨域差距或领域内异质性的严峻挑战。我们基于图像与时间序列之间内在相似性，探索了一种新的TSF任务表示，将其重新表述为图像重建任务，并利用在ImageNet数据集上进行自我监督预训练的视觉掩码自动编码器（MAE）进行处理。  令人惊讶的是，在无需进一步在时间序列领域进行适应的情况下，所提出的VisionTS就能实现优于现有TSF基础模型的零样本预测性能。通过最小程度的微调，VisionTS能够进一步提升预测性能，并在大多数情况下达到最先进的水平。这些发现表明，视觉模型可能为TSF提供免费午餐，并强调了计算机视觉与TSF领域未来交叉研究的潜力。我们的代码已公开在https://github.com/Keytoyze/VisionTS上。**|
|**2024-08-29**|**How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models**|Jiyue Jiang et.al.|[2408.16756](http://arxiv.org/abs/2408.16756)|**[link](https://github.com/jiangjyjy/yue-benchmark)**|快速发展的大型语言模型（LLMs）已经改变了自然语言处理（NLP）的竞赛环境，特别是在英语和其他数据丰富的语言中。然而，在诸如粤语这样的代表性不足的语言领域，开发差距仍然显著存在，这尤其令人担忧，考虑到广深港澳大湾区的经济重要性，以及在新加坡和北美地区大量粤语使用者的情况。尽管粤语广泛使用，但在NLP研究中对粤语的代表却少之又少，尤其是与其他同样发达地区的语言相比。为了填补这些空白，我们概述了当前的粤语NLP方法，并引入了旨在评估LLM在事实生成、数学逻辑、复杂推理和粤语中的通用知识等方面的性能的新基准，旨在推动开源粤语LLM技术的发展。我们也提出了未来的研究方向和推荐的模型，以增强粤语LLM的开发。|
|**2024-08-29**|**Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models**|Alec Solway et.al.|[2408.16753](http://arxiv.org/abs/2408.16753)|null|强化学习在预训练模型后，通过最大化似然性来预测大型文本语料库中的下一个文本令牌，用于将语言模型与人类偏好信号对齐。在部署到特定领域之前，通常会对模型进行进一步的微调以适应任务相关的数据。由于人类偏好信号在最后阶段往往不可用，因此通常使用最大化似然性进行微调，这是默认方法。然而，强化学习除了能够促进与人类定义奖励函数的对齐之外，还有其他优势。相比于最大化似然性，即模仿学习模型在理想条件下应执行的操作，强化学习不限于仅展示达到最优状态时的操作，而是在探索策略空间的过程中训练模型在各种情况下的操作。此外，它还训练模型避免执行竞争但效果不佳的操作。本文开发了一种使用强化学习进行最后一阶段微调的框架，并测试了该方法是否能带来性能提升。实验集中在抽象概括上，但框架具有普遍适用性。采用该流程产生的结果显著优于仅使用最大似然性输出的结果。对于特定的数据集，通过后处理最大似然输出可以缩小性能差距。然而，该框架提供了一种优化模型的新途径，在后处理可能不那么直接有效或有效的场景中尤为有用，并且它可以扩展以包括更多类别的需要惩罚并训练反对的不适当输出，如幻觉。|
|**2024-08-29**|**Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge**|Beidi Dong et.al.|[2408.16749](http://arxiv.org/abs/2408.16749)|null|本文探讨了在检测和限制网络上极端主义思想传播方面，自动工具的重要性。研究比较了双向编码表示的Transformer（BERT）和生成预训练Transformer（GPT）模型，在“右翼”和“左翼”意识形态关键词的社交媒体帖子中进行检测与分类的能力。我们收集了含有上述关键词的帖子，并人工标记为极端主义或非极端主义。进一步地，我们将极端主义帖子分为五个构成要素之一，基于工作定义框架。  BERT模型的性能评估基于训练数据规模和类别间的知识转移。此外，我们对比了使用不同提示的GPT 3.5和GPT 4模型的性能：原始提示、一般定义、角色扮演和专业定义。结果表明，最佳表现的GPT模型优于最佳表现的BERT模型，更详细的提示通常能带来更好的结果。然而，过于复杂的提示可能会影响性能。不同的GPT版本对被认定为极端主义的敏感度各不相同。GPT 3.5在识别左翼极端主义帖子方面表现更好，而GPT 4则在识别右翼极端主义帖子方面表现更好。  大型语言模型（GPT模型）在在线极端主义分类任务中展现出显著潜力，超越了传统的BERT模型，在零样本设置下表现出色。未来研究应探索人类与计算机交互在优化GPT模型以进行极端主义检测与分类任务中的作用，以开发更高效（例如，更快捷、更少努力）且更有效的识别极端主义内容方法。|
|**2024-08-29**|**Theoretical and Methodological Framework for Studying Texts Produced by Large Language Models**|Jiří Milička et.al.|[2408.16740](http://arxiv.org/abs/2408.16740)|null|本文从定量语言学的角度探讨了研究大型语言模型（LLM）及其生成文本所面临的概念、方法论和技术挑战。本文基于一个理论框架，该框架区分了作为载体的LLM与模拟的实体。本文倡导对模型采取严格非拟人化的方法，同时谨慎地应用用于研究人类语言行为的方法来分析模拟实体。虽然自然语言处理研究者关注模型本身、其架构、评估以及提高性能的方法，作为定量语言学家，我们的目标是构建关于LLM生成文本特性的理论体系，它们与人类生成的文本有何不同，以及模拟实体的属性。此外，我们还应探索LLM作为研究人类文化工具的可能性，而语言是这一文化不可或缺的一部分。|
|**2024-08-29**|**GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative Models**|Moreno D'Incà et.al.|[2408.16700](http://arxiv.org/abs/2408.16700)|**[link](https://github.com/moreno98/gradbias)**|**近期在文本到图像（T2I）生成模型领域取得的进展使得高质量图像生成成为可能。随着性能和可访问性的提高，这些模型正受到越来越多的关注和欢迎，确保它们的公平性和安全性是防止偏见传播和延续的关键。现有研究主要集中在预定义偏见（如性别、种族）的封闭集合上进行偏见检测。然而，在开放集设置下，即无需预先设定的情况下，检测和量化偏见是一个挑战。  本文提出了一种通用框架，用于识别、量化和解释开放集设置下的偏见。该管道利用大型语言模型（LLM）从一组描述中提出偏见。随后，使用目标生成模型生成一系列图像。最后，通过视觉问答（VQA）进行偏见评估。我们展示了两种基于此框架的方法：OpenBias 和 GradBias。OpenBias 能够检测并量化与人、物体和动物相关的已知和新型偏见，并与现有的封闭集偏见检测方法以及人类判断高度一致。GradBias 显示出中性词汇对偏见的影响显著，并且在多项基线中表现最佳，包括最先进的基础模型。  代码已在此处提供：https://github.com/Moreno98/GradBias。**|
|**2024-08-29**|**Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity**|Ziniu Li et.al.|[2408.16673](http://arxiv.org/abs/2408.16673)|null|本文旨在解决大型语言模型在下游任务的精调（Supervised Fine-Tuning，SFT）过程中遇到的过拟合和输出多样性受限的问题。传统上，交叉熵（Cross Entropy，CE）损失函数被广泛用于SFT，然而它可能导致模型对数据分布进行过于激进的更新，从而引发过拟合和降低输出的多样性。  为了解决这些问题，本文引入了最大熵原则，该原则倾向于促进模型生成更平滑的概率分布，同时仍能有效捕捉数据特征。具体地，我们提出了一种名为GEM的新方法，它通过解决反向Kullback-Leibler散度最小化问题，并加入熵正则化器，来匹配目标分布。  在对Llama-3-8B模型进行SFT时，GEM在多个方面优于CE。首先，在使用UltraFeedback数据集训练以增强模型的指令遵循能力时，GEM表现出较低的过拟合迹象，表现为更低的困惑度和在IFEval基准测试上的更好性能。此外，GEM还提高了输出的多样性，即使在没有特定领域数据的情况下，仅通过最佳n采样，数学推理和代码生成任务的性能也得到了最高7分的提升。  进一步地，当使用特定领域的数据集对数学推理和代码生成任务进行微调时，GEM同样表现出较低的过拟合和与CE相比高达10分的性能提升。|
|**2024-08-29**|**Examination of Code generated by Large Language Models**|Robin Beer et.al.|[2408.16601](http://arxiv.org/abs/2408.16601)|**[link](https://github.com/t-muras/ai-code-analysis)**|**大型语言模型（LLM），例如ChatGPT和Copilot，正在通过自动化代码生成彻底改变软件开发，这在一定程度上促进了快速原型设计、教育支持以及生产力的提升。因此，LLM生成的代码正确性和质量应与人工编写的代码相当。为了评估当前LLM在生成Java和Python语言中的简单算法及其对应的单元测试时的正确性和质量（覆盖率）的能力，我们进行了受控实验。实验包括让LLM生成代码并评估其正确性与质量。我们观察到LLM之间、不同编程语言之间、算法与测试代码之间以及时间上的显著差异。本文报告了这些结果及实验方法，以便进行重复和可比的评估，以涵盖更多的算法、语言和LLM随时间的变化情况。**|
|**2024-08-29**|**Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies**|Zhiyang Qi et.al.|[2408.16586](http://arxiv.org/abs/2408.16586)|null|近期自然语言处理领域的进步，尤其是大型语言模型（LLM）如GPT-4的发展，显著提升了对话系统的性能，使得它们能够生成更为自然流畅的对话。然而，这些系统仍面临着诸如持续对话管理、记忆保留和减少幻觉等挑战。AIWolfDial2024这一项目通过采用“狼人杀”这一不完全信息游戏来测试LLM在复杂互动环境中的能力，以应对上述挑战。该项目引入了一种基于LLM的“狼人杀”游戏AI，其中每个角色都通过情境分析来辅助回应生成。对于“狼人”这一角色，项目采用了包括逻辑吸引力、可信度吸引力和情感吸引力在内的多种说服策略，以有效地引导其他玩家与自己的行动保持一致。|
|**2024-08-29**|**CNIMA: A Universal Evaluation Framework and Automated Approach for Assessing Second Language Dialogues**|Rena Gao et.al.|[2408.16518](http://arxiv.org/abs/2408.16518)|**[link](https://github.com/renagao/csl2024)**|我们开发了CNIMA（一种中文作为第二语言的非母语互动测量与自动化数据集），包含10,000个对话。我们使用了一个评估框架来注释CNIMA，该框架最初用于英语作为第二语言的对话，它评估了微观层面特征（如回话）和宏观层面互动标签（如主题管理）。我们测试了该框架从英语到中文的可移植性。发现该框架在不同语言之间具有鲁棒性，并揭示了普遍性和特定于语言的微观层面和宏观层面特征之间的关系。接下来，我们提出了一种自动化评估的方法，并找到了强大的性能，创建了一个新的自动化第二语言评估工具。我们的系统易于适应其他语言，因为它使用大型语言模型，因此不需要大规模标注训练数据。|
|**2024-08-29**|**LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?**|Jan Cegin et.al.|[2408.16502](http://arxiv.org/abs/2408.16502)|null|生成式大型语言模型（LLMs）在数据增强任务中的应用越来越广泛，文本样本通过LLM进行同义替换后用于分类模型的微调。然而，关于LLM数据增强方法相较于现有成熟方法是否具有明显优势的研究证据相对缺乏。为了探讨在何种情况下使用LLM数据增强方法更为有利，本研究在6个数据集、3个分类器和2种微调方法上进行了对比实验。我们还调整了种子数量和收集样本的数量，以便更全面地探索下游模型准确度空间。此外，我们还进行了成本效益分析，结果表明，在使用非常少量种子的情况下，LLM数据增强方法值得部署。在许多情况下，现有方法能够达到或超过类似甚至更好的模型准确度。|
|**2024-08-28**|**Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders**|Min Shi et.al.|[2408.15998](http://arxiv.org/abs/2408.15998)|**[link](https://github.com/nvlabs/eagle)**|**《大规模语言模型在多模态任务中的视觉理解能力：混合视觉编码器的设计空间探索》一文探讨了准确解析复杂视觉信息对于多模态大型语言模型（MLLMs）的重要性。近期研究显示，增强的视觉感知能显著降低幻觉现象，并在光学字符识别、文档分析等分辨率敏感任务上提升性能。许多先进MLLMs通过集成多种视觉编码器来实现这一目标。然而，当前缺乏对关键方面系统的比较和详细的拆解研究，比如专家选择和多视觉专家融合策略。本文对使用混合视觉编码器的MLLM设计空间进行了广泛探索。研究发现，多个互补视觉编码器的视觉令牌简单拼接即可达到与更复杂的混合架构或策略相当的效果。此外，引入预对齐（Pre-Alignment）机制，以弥合专注于视觉的编码器与语言令牌之间的差距，从而提升模型一致性。由此产生的MLLM家族——Eagle，在主要的MLLM基准测试中超越了其他领先开源模型。相关代码及模型已开源发布：https://github.com/NVlabs/Eagle**|
|**2024-08-28**|**BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems**|Wei Wang et.al.|[2408.15971](http://arxiv.org/abs/2408.15971)|null|大型语言模型（LLM）正在变得越来越强大，能够处理复杂任务，例如构建单一代理和多代理系统。相较于单一代理，多代理系统对语言模型的协作能力提出了更高的要求。已有的评估基准主要关注于多代理系统的协作能力，但在细粒度评估方面存在不足，并且忽略了多代理系统的协作与竞争场景。  为了填补这一空白，我们提出了一种新的基准测试——BattleAgentBench。该基准定义了三个不同难度级别的七个子阶段，旨在从单一代理场景导航能力、配对代理任务执行能力以及多代理合作与竞争能力等多个维度，对语言模型进行细致的评估。我们对四大闭源模型和七大开源模型进行了广泛评估。  实验结果表明，基于API的模型在简单任务上表现出色，而开源小型模型在简单任务上则面临挑战。对于需要合作与竞争能力的困难任务，尽管基于API的模型展示了一定的协作能力，但仍有巨大的改进空间。|
|**2024-08-28**|**More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding**|Yuan Tang et.al.|[2408.15966](http://arxiv.org/abs/2408.15966)|**[link](https://github.com/tangyuan96/greenplm)**|在本论文中，我们重新审视了让大型语言模型（LLM）理解三维物理世界这一挑战。由于缺乏大规模的三维点云与文本配对数据集，LLM 在三维理解上的成功尚未实现复制。为此，我们提出了一项新任务：3D 数据高效点云-语言理解。目标是使LLM 能够利用最少的三维点云和文本数据对实现稳健的三维对象理解。  为了应对这一任务，我们引入了GreenPLM，通过利用更多的文本数据来弥补缺少的三维数据。首先，借鉴使用CLIP对图像和文本进行对齐的方式，我们利用预训练的点云-文本编码器将三维点云空间映射到文本空间。这一映射使得我们可以无缝地连接文本空间与LLM。一旦建立了点云-文本-LLM的连接，我们进一步通过扩展中间文本空间增强文本-LLM的对齐，从而减少对三维点云数据的依赖。  具体而言，我们生成了600万个关于三维物体的自由文本描述，并设计了三阶段训练策略，帮助LLM更好地探索不同模态之间的内在联系。为了实现高效的模态对齐，我们设计了一个零参数交叉注意力模块用于令牌聚合。  广泛的实验结果表明，GreenPLM仅需要现有最先进的模型所用3D训练数据的12%，就能达到更优的三维理解性能。令人惊讶的是，GreenPLM仅使用文本数据也能实现竞争力的表现。相关代码和权重可在以下链接获取：https://github.com/TangYuan96/GreenPLM。|
|**2024-08-28**|**Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games**|Nicholas R. Waytowich et.al.|[2408.15950](http://arxiv.org/abs/2408.15950)|null|近期，大型语言模型（LLMs）的进展使其能力超越了传统的文本任务，扩展到了多模态领域，整合了视觉、听觉和文本数据。虽然在机器人学和游戏等高阶规划领域对多模态LLM的研究已经相当广泛，但在低级控制任务中的应用潜力却鲜有探索。本文探讨了多模态LLM在 Atari 视频游戏领域的应用，引入了 Atari 游戏性能作为评估多模态LLM执行低级控制任务能力的新基准。与传统强化学习（RL）和模仿学习（IL）方法相比，这些LLM无需大量的计算资源和奖励函数定义，而是利用现有的多模态知识直接与游戏环境交互。  我们的研究评估了多个多模态LLM的表现，与传统RL代理、人类玩家和随机代理进行了比较，重点关注它们理解复杂视觉场景并制定战略响应的能力。此外，我们还通过引入人类演示的游戏玩法轨迹来研究上下文学习（ICL）的影响，以增强模型的上下文理解能力。  通过这一研究，我们旨在确定多模态LLM能否利用其广泛的训练来有效地充当低级控制器，从而重新定义动态和视觉复杂环境中的潜在应用。有关额外结果和视频的更多信息，请访问我们的项目网页：https://sites.google.com/view/atari-gpt/。|
|**2024-08-28**|**Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models**|Yuncheng Yang et.al.|[2408.15915](http://arxiv.org/abs/2408.15915)|**[link](https://github.com/yaphabates/rocket)**|在特定领域培养大型语言模型（LLM）以解决任务所需的专长往往需要针对稳定预期输出进行专门调整。避免手动准备指令数据集和训练资源带来的巨大成本，利用开放知识包括低秩适应（LoRA）模型和指令数据集作为起点是合理的选择。然而，现有方法在模型和数据选择上侧重于通用能力的性能，而忽视了在特定领域部署时暴露的知识差距。本研究提出了一种通过引入少量人工标注样本（即K-shot）来弥合此类差距的方法，以促进LLM在开放知识上的任务专长。  具体来说，我们开发了一个高效且可扩展的管道，以成本效益方式生成任务专家，其中K-shot数据参与选择最具潜力的专家候选者和任务相关的指令。构建了一个混合专家（MoE）系统，充分利用多个专家之间独特但互补的知识。我们揭示了MoE系统成功的关键因素：  1. 遵循K-shot原则：确保真正具备解决K-shot问题能力的模型被选中，而非盲猜者。 2. 强调多样性：不仅专家本身具有多样性，而且在整个模型和数据选择过程中，细调指令也体现出多样性。  广泛的实验结果证实了我们的方法在各种任务上对开放知识利用的优越性。后续将发布代码和模型。|
|**2024-08-28**|**Decentralized LLM Inference over Edge Networks with Energy Harvesting**|Aria Khoshsirat et.al.|[2408.15907](http://arxiv.org/abs/2408.15907)|null|大型语言模型在自然语言任务上表现出的卓越性能已经极大地改变了多个领域，但在资源受限环境如边缘网络中的部署仍面临挑战。分布式推理技术的出现通过在多台设备间分配模型块来提升灵活性和成本效益，但仍存在能源限制问题，尤其是针对电池供电的边缘设备。我们提出了一种基于互联、使用能量收集的电池供电边缘设备的协作推理可持续模型。通过建立半马尔可夫模型描述设备状态，考虑处理参数和平均绿色能源到达情况，以指导设计旨在减少设备停机时间和最大化网络吞吐量的调度算法。通过实证评估和模拟运行，验证了我们的方法的有效性，为边缘网络上的节能分布式推理铺平了道路。|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903](http://arxiv.org/abs/2408.15903)|null|快速过时的信息使得大型语言模型（LLMs）在整合新知识方面面临挑战。现有方法在处理需要准确事实识别和序列逻辑推理的多跳问题时仍存在困难，尤其是在面对大量事实更新的情况下。为解决这些问题，本文提出了Graph Memory-based Editing for Large Language Models（GMeLLo），一种简单而有效的方法，它结合了知识图谱（KGs）的明确知识表示与LLMs的语言灵活性。GMeLLo不仅利用LLMs进行问答，还运用这些模型将自然语言转换为结构化查询和事实三元组，从而实现与KGs的无缝交互，用于快速更新和精确的多跳推理。实验结果表明，GMeLLo在多跳问答基准MQuAKE中显著超越当前最先进的知识编辑方法，特别是在涉及大量知识更新的场景中。|
|**2024-08-28**|**Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts**|Nikolas Gritsch et.al.|[2408.15901](http://arxiv.org/abs/2408.15901)|null|当前大型语言模型在效率、专业化和对新数据分布的适应性方面难以同时具备这些优秀品质。混合专家（MoE）架构因其条件计算的内在特性，成为研究的重点领域，旨在提升这些品质。本工作专注于“升级”密集型专家模型至MoE架构，旨在增强专业化的同时，也增加对新任务的灵活适应性。  我们引入了Nexus，一种增强的MoE架构，其具有自适应路由机制，允许模型学习将专家嵌入从领域表示进行投影。这种策略使得Nexus能够通过单独训练的密集模型灵活地添加新的专家，无需对未见数据域进行大规模MoE训练。实验结果显示，与基线相比，Nexus在初始升级阶段实现了高达2.1%的相对增益，在使用有限的微调数据扩展MoE时实现了18.8%的相对增益。Nexus的灵活性对于建立一个开源生态系统至关重要，该生态系统允许每个用户根据自己的需求不断组装自己的MoE混合模型。|
|**2024-08-28**|**Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models**|Sebastian Vallejo Vera et.al.|[2408.15895](http://arxiv.org/abs/2408.15895)|null|人类编码员存在偏见。我们通过复制Ennser-Jedenastik和Meyer（2018）的实验，发现大型语言模型（LLMs）在评估政治声明时使用政治信息，特别是政党线索。LLMs不仅根据政党线索上下文化判断陈述是正面、负面还是中性，还反映出它们在训练过程中生成的人类数据所具有的偏见。我们还发现，与人类不同的是，人类仅在面对极端政党声明时表现出偏见，而LLMs即使在被提示来自中间左翼和中间右翼政党的声明时也显示出显著偏见。最后部分讨论了这些发现的意义。|
|**2024-08-28**|**Persuasion Games using Large Language Models**|Ganesh Prasath Ramani et.al.|[2408.15879](http://arxiv.org/abs/2408.15879)|null|大型语言模型（LLM）已经发展成为一种强大的工具，能够理解和生成类似人类的文本。本文研究了LLM在塑造人类观点并进而影响他们在特定任务上的决策方面的潜力。这些能力在投资、信用卡和保险等多个领域找到了应用，帮助用户选择合适的保险政策、投资计划、信用卡以及零售产品，甚至在行为改变支持系统（BCSS）中也有应用。  我们提出了一种复杂多代理框架，其中一组代理以协作方式操作。主要代理直接与用户进行有说服力的对话，而辅助代理执行诸如信息检索、响应分析、制定说服策略和事实验证等任务。我们的实验证据表明，这种协作方法显著提高了LLM的说服效果。我们持续分析用户的抵抗性，并通过结合规则基于和LLM基于的抵抗-说服映射技术来应对这一挑战。  我们使用模拟的人格形象，并在保险、银行和零售领域生成对话，以评估大型语言模型（LLM）在识别、适应和影响不同人格类型方面的熟练程度。同时，我们也检查了LLM模拟人格所采用的抵抗机制。说服效果通过交互前后的可衡量调查、LLM生成的对话评分以及用户决策（购买或不购买）进行量化。|
|**2024-08-27**|**Generative Verifiers: Reward Modeling as Next-Token Prediction**|Lunjun Zhang et.al.|[2408.15240](http://arxiv.org/abs/2408.15240)|null|验证器或奖励模型常用于增强大型语言模型（LLM）的推理性能。一种常见的方法是Best-of-N策略，其中从LLM生成的N个候选解决方案中由验证器进行排名，选择最佳一个。传统上，验证器是作为判别分类器进行训练以对解决方案打分的，但它们并未充分利用预训练LLM的文本生成能力。为了克服这一限制，我们提议通过在验证和解决方案生成上使用通用的下一个词预测目标联合训练验证器。与标准验证器相比，这样的生成型验证器（GenRM）可以从LLM的几个优势中获益：它们可以无缝地与指令调谐相结合，支持链式思考推理，并且可以通过增加推理时的计算量来利用多数投票，从而进行更好的验证。我们展示了，在算法问题和小学数学推理任务上使用Gemma为基础的验证器时，GenRM优于判别型验证器和LLM作为裁判，表现出16%-64%的问题解决率提升。此外，我们证明了GenRM在数据集规模、模型容量和推理时计算量增加方面具有良好的可扩展性。|
|**2024-08-27**|**LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet**|Nathaniel Li et.al.|[2408.15221](http://arxiv.org/abs/2408.15221)|null|近期的大规模语言模型（LLM）防御措施显著提升了模型对有害查询的拒绝能力，即使在遭受有组织攻击的情况下也不例外。然而，这些防御措施主要是在单轮对话中针对自动化攻击进行评估，这种威胁模型不足以反映真实世界中恶意行为的复杂性。  我们通过实验展示了多轮对话的人工智能“越狱”（即攻击者利用模型的漏洞来绕过防御机制）能够揭露防御系统中的重大漏洞。在使用HarmBench这一评估平台，对抗那些在单轮对话中仅报告低百分比攻击成功率（ASR）的防御系统时，我们发现多轮对话的人工智能“越狱”的成功率超过了70%。这表明当前的防御机制在面对更复杂的、多步骤的攻击策略时存在不足。  此外，多轮对话的人工智能“越狱”还揭示了机器遗忘防御系统的漏洞。攻击者成功地从未被删除的模型中恢复了可用于生物安全双重用途的知识，这进一步证明了现有防御措施在保护敏感信息方面存在的弱点。  为了总结和共享这些发现，我们构建了一个名为“多轮对话人工智能越狱”（Multi-Turn Human Jailbreaks，简称MHJ）的数据集，包含了来自537个不同多轮对话场景的2912个触发指令，共计2,912个触发指令涉及2,912个不同的多轮对话“越狱”案例。同时，我们还公开发布了这个数据集以及在多种商业红队测试中发展出的一系列“越狱”策略的综述，旨在为研究更强大的LLM防御系统提供资源和支持。|
|**2024-08-27**|**Investigating Coverage Criteria in Large Language Models: An In-Depth Study Through Jailbreak Attacks**|Shide Zhou et.al.|[2408.15207](http://arxiv.org/abs/2408.15207)|null|大型语言模型（LLM）的迅速发展极大地改变了人工智能的格局，然而在敏感领域部署时，它们的脆弱性引发了一系列严重关切，尤其是对于恶意利用的风险。这种情况凸显了预部署测试不足的问题，强调了需要更加严格和全面评估方法的紧迫性。本研究通过全面的实证分析，评估了传统覆盖标准在识别这些漏洞方面的有效性，特别关注了关键问题——“越狱”攻击。研究首先对LLM中的隐藏状态进行了聚类分析，结果显示这些状态的内在特性能够明显区分不同类型的查询。随后，我们从三个关键维度——标准级别、层级别和词级别——评估了这些标准的性能。我们的发现揭示了正常查询与“越狱”查询在神经元激活模式上的显著差异，从而验证了聚类结果。基于这些发现，我们提出了一种创新的方法，用于实时检测“越狱”攻击，利用神经激活特征。我们的分类器表现出了极高的准确率，平均达到96.33%，成功识别出包括可能导致对抗性攻击的“越狱”查询。这项研究的重要性在于其对LLM安全性测试复杂挑战的全面应对。通过使系统能够在生成第一个词时立即检测到攻击，我们的方法为集成LLM的未来系统提供了强大的实时检测能力。这一研究深化了我们对LLM安全性的理解，并为开发更稳健的人工智能系统奠定了基础。|
|**2024-08-27**|**Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation**|Jian Hu et.al.|[2408.15205](http://arxiv.org/abs/2408.15205)|**[link](https://github.com/lwpyh/ProMaC_code)**|本文提出了一种任务通用的提示可分割方法，旨在减少对每种所需对象的实例特定手动提示的需求。通过使用单个任务通用提示来指导同一任务下不同对象的不同图像的分割，引入了任务通用提示分割。当前的方法利用多模态大型语言模型（MLLMs）从通用提示推理出详细的实例特定提示，以提高分割准确性。这种方法的有效性在很大程度上取决于生成提示的精确度。然而，MLLMs在推理过程中经常出现幻觉，导致提示不准确。现有方法专注于消除幻觉以提高模型性能，本文认为MLLM幻觉在正确利用时可以揭示有价值的任务相关信息，因为它们代表了超越单张图像的预训练大规模知识。因此，本文利用幻觉从图像中挖掘任务相关信息，并验证其准确性以增强生成提示的精确度。  具体而言，我们引入了一个迭代的提示-掩码循环生成框架（ProMaC），该框架包括一个提示生成器和一个掩码生成器。提示生成器使用多尺度链式思考提示，最初探索幻觉以提取测试图像上的扩展上下文知识。然后，将这些幻觉降低到形成精确的实例特定提示，从而引导掩码生成器通过掩码语义对齐产生与任务语义一致的掩码。生成的掩码通过迭代引导提示生成器更关注任务相关的图像区域并减少无关的幻觉，最终共同提高了提示和掩码的质量。  实验结果在5个基准数据集上证明了ProMaC的有效性。详细代码见https://lwpyh.github.io/ProMaC/。|
|**2024-08-27**|**Can Unconfident LLM Annotations Be Used for Confident Conclusions?**|Kristina Gligorić et.al.|[2408.15204](http://arxiv.org/abs/2408.15204)|**[link](https://github.com/kristinagligoric/confidence-driven-inference)**|大型语言模型（LLM）在各种任务中与人类评估者高度一致，显示出减轻人类数据收集挑战的潜力。在计算社会科学（CSS）领域，研究人员越来越多地利用LLM注释来补充缓慢且昂贵的人类注释。然而，对于如何收集和使用LLM注释而不损害下游结论的有效性，仍缺乏明确的指南。我们引入了“置信驱动推理”方法，该方法结合了LLM注释和LLM置信度指示器，以战略方式选择应收集哪些人类注释，旨在生产准确的统计估计和可验证的置信区间，同时减少所需的人类注释数量。我们的方法具有防止LLM注释质量差的保障措施，确保得出的结论既有效又不比仅依赖人类注释更不准确。我们在三个CSS场景——礼貌文本、立场和偏见——中的统计估计任务中，通过与基线比较，证明了置信驱动推理的有效性，每种场景下所需的人类注释数量减少了超过25%。尽管我们使用CSS场景进行演示，但置信驱动推理可以用于广泛NLP问题中的大多数标准量估计。|
|**2024-08-27**|**Unlocking Potential in Pre-Trained Music Language Models for Versatile Multi-Track Music Arrangement**|Longshen Ou et.al.|[2408.15176](http://arxiv.org/abs/2408.15176)|null|大型语言模型在多个领域展示了显著的能力，包括符号音乐生成。然而，利用这些预训练的模型进行可控音乐编排任务的挑战仍然新颖，每个任务都需要不同的音乐信息作为控制。本文提出了一种统一的序列到序列框架，它允许对符号音乐语言模型进行微调，以执行四个不同的多轨编排任务：乐队编排、钢琴缩减、鼓编排和声音分离。我们的实验结果表明，所提出的策略在所有四个任务上均实现了更高音乐质量的结果，与专门针对特定任务的基线相比。此外，通过额外的探查分析实验，我们展示了预训练阶段赋予模型理解音乐条件的基本知识，这在仅通过特定任务的微调难以获得的情况下尤为重要。|
|**2024-08-27**|**X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation**|Hanjia Lyu et.al.|[2408.15172](http://arxiv.org/abs/2408.15172)|null|大型语言模型（LLM）和大型多模态模型（LMM）已被证明能显著提升丰富项目描述的效果，进而增强推荐系统的准确性。然而，现有方法往往仅依赖于纯文本提示，或者采用基本的多模态策略，未能充分利用文本与视觉模态之间互补的信息。本文提出了一种名为Cross-Reflection Prompting（X-Reflect）的新框架，旨在通过引导LMM明确识别并调和文本与图像之间的支持性与冲突信息来解决这些问题。通过捕捉两种模态的细微洞察，此方法生成了更为全面且语境丰富的项目表示。在两个广泛使用的基准上进行的大量实验表明，我们的方法在下游推荐准确度上优于现有的提示基线。此外，我们评估了框架在不同LMM架构下的泛化能力以及提示策略的鲁棒性，提供了优化的见解。这项工作强调了整合多模态信息的重要性，并提出了改善多模态推荐系统中项目理解的新型解决方案。|
|**2024-08-27**|**Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation**|N. E. Kriman et.al.|[2408.15171](http://arxiv.org/abs/2408.15171)|null|自2022年ChatGPT的发布以来，大型语言模型（LLMs）的应用范围显著扩大，显示出其在各种场景中的价值。然而，对于企业级和商业应用而言，LLMs生成不准确信息的趋势，即所谓的“幻觉”现象，成为了一个主要挑战。本项目提出了一种方法，用于在与原始文本进行比较时评估LLM生成概要的准确性。我们的方法利用朴素贝叶斯分类来判断生成内容的真实性。  通过这种方法，我们可以估计生成文本与实际信息之间的匹配度，从而提高LLM应用的质量和可靠性。这不仅有助于识别可能存在的错误或不准确之处，还能增强用户对LLM生成内容的信任，促进其在更广泛领域的有效使用。此外，该方法还能为LLM的持续改进提供有价值的反馈，推动技术进步，最终实现更高质量、更可靠的人工智能辅助内容生成。|
|**2024-08-27**|**BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline**|Guosheng Dong et.al.|[2408.15079](http://arxiv.org/abs/2408.15079)|null|大型语言模型（LLM）的核心能力高度依赖于广泛预训练数据集的组成和选择，这些数据集被多个机构视为商业秘密。为了缓解这一问题，我们开源了一个通用适用的数据处理管道，并通过引入一个竞争性的LLM基线来验证其有效性和潜力。具体来说，数据处理管道包括广域收集以扩大规模和重新加权以提高质量。然后，我们使用我们的管道对3万亿个令牌进行预训练，而无需任何明确的下游任务优化，接着进行一个简单但有效的监督微调阶段。BaichuanSEED在整个训练过程中表现出一致性与预测性，并在综合基准测试中与几个先进的商业大型语言模型，如Qwen1.5和Llama3，实现了可比性能。我们还进行了几个启发式实验，讨论了在数学和编程等下游任务进一步优化的可能性。|
|**2024-08-27**|**Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models**|Ned Cooper et.al.|[2408.15066](http://arxiv.org/abs/2408.15066)|null|本文探讨了交互反馈功能在ChatGPT界面中的可用性，分析了这些功能如何塑造用户输入以及大型语言模型迭代过程中的参与度。通过调研ChatGPT用户并应用了可操作性框架，我们展示了这类功能鼓励简单、频繁且侧重于性能的反馈，同时限制了集体输入和用户间的讨论。我们主张，这种反馈格式极大地限制了用户的参与，强化了用户、公众与开发大型语言模型的公司之间的权力不平等。我们的分析为现有参与式人工智能文献提供了新的视角，着重于现有反馈流程的局限性，并提出了重新设计的方向。  为了使公众在人工智能发展中能够更具有意义地参与，我们提倡转向关注模型输出与特定用户偏好的一致性的过程。相反，我们强调需要促进公司与不同“公众”之间关于大型语言模型的目的和应用进行对话的过程。这一方法要求对持续的社会基础设施建设的关注，即创建和维持解决AI开发和部署影响群体关切所需的社会、技术和机构结构。|
|**2024-08-27**|**Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models**|Aradhye Agarwal et.al.|[2408.14470](http://arxiv.org/abs/2408.14470)|**[link](https://github.com/Aradhye2002/selective-peft-toolkit)**|**细调大型语言模型（LLMs）在下游任务上需要大量计算资源。参数高效细调（PEFT）类方法旨在通过仅微调模型参数的小部分来缓解这些计算挑战。虽然从计算效率方面考虑，这些技术通常无法与完全微调的模型性能相匹敌，主要原因是参数选择过程中固有的偏见。传统的选择性PEFT技术基于预先定义的预算（也称为去遮罩）使用固定参数集，未能动态捕捉参数的重要性，并经常超出预算。我们引入了 $\text{ID}^3$，这是一种新颖的选择性PEFT方法，它连续计算参数的重要性，并通过平衡参数选择过程中的探索与利用来动态地去遮罩参数。我们在15个任务上进行的实验覆盖了自然语言理解与生成任务，显示了与基于固定去遮罩的PEFT技术相比，我们的方法的有效性。我们通过理论分析证明，$\text{ID}^3$将梯度更新的数量减少了一倍，从而提高了计算效率。$\text{ID}^3$ 对神经元的随机初始化具有鲁棒性，因此可以无缝集成到现有添加式和重新参数化基PEFT模块，如适配器和LoRA中，用于动态稀疏化。**|
|**2024-08-26**|**Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos**|Qirui Chen et.al.|[2408.14469](http://arxiv.org/abs/2408.14469)|null|本文探讨了长形式第一人称视角视频中的多跳视频问答（Multi-Hop Video Question Answering，MH-VidQA）问题。这项任务不仅需要回答视觉问题，还需要在视频中定位多个相关的时间段作为视觉证据。我们开发了一个自动化流程来创建带有关联时间证据的多跳问题解答配对，从而构建了一个用于指令调整的大规模数据集。为了监测这一新任务的进展，我们进一步整理了一个高质量的基准——MultiHop-EgoQA，通过仔细的手动验证和细化进行构建。  实验结果揭示了现有跨模态系统在多跳定位和推理能力方面存在不足，导致性能不佳。随后，我们提出了一种名为“Grounding Scattered Evidence with Large Language Model”（GeLM）的新架构，该架构通过引入一个地理解码模块增强了大型语言模型（Large Language Models，LLMs），该模块使用灵活的地理解码令牌从视频中检索时间证据。在我们的视觉指令数据上进行训练后，GeLM展示了增强的多跳定位和推理能力，为这一具有挑战性的任务设定了新的基准。此外，当在第三人称视角视频上进行训练时，相同的架构在单跳视频问答基准（ActivityNet-RTL）上也达到了最先进的性能，证明了其有效性。|
|**2024-08-26**|**Explicit Inductive Inference using Large Language Models**|Tianyang Liu et.al.|[2408.14467](http://arxiv.org/abs/2408.14467)|null|在本论文中，我们提出了一种管道方法，利用大型语言模型（LLM）的这一偏差进行明确的归纳推理。该管道使用LLM将前提转换为一组已验证的替代方案，并通过聚合衍生的新蕴含询问的答案来支持原始推理预测。在方向性谓词蕴含基准测试上，我们展示了通过应用此简单管道，可以提高LLM在推理上的整体性能，并显著减轻它们的证实偏差影响。|
|**2024-08-26**|**Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study**|Liuchang Xu Shuo Zhao et.al.|[2408.14438](http://arxiv.org/abs/2408.14438)|null|随着大型语言模型如ChatGPT、Gemini等的问世，评估它们在自然语言理解、代码生成等多方面能力的重要性日益凸显。然而，这些模型在空间任务方面的表现并未得到全面评估。本研究填补了这一空白，通过引入一个新颖的多任务空间评价数据集，系统性地探索和比较几种先进模型在空间任务上的性能。该数据集涵盖了十二种不同的任务类型，包括空间理解和路径规划，并且每项任务都有经过验证的准确答案。  我们采用双阶段测试方法对多个模型进行了评估，包括OpenAI的gpt-3.5-turbo、gpt-4o以及ZhipuAI的glm-4。首先进行零样本测试，随后根据难度对数据集进行分类，并执行了提示调优测试。结果显示，在第一阶段的测试中，gpt-4o的整体准确性最高，平均达到了71.3%。尽管moonshot-v1-8k在总体上略逊一筹，但在地名识别任务上却超越了gpt-4o。研究还揭示了特定任务中提示策略对模型性能的影响。例如，链式思考（COT）策略使gpt-4o在路径规划任务上的准确率从12.4%提升至87.5%，而一次射击策略则使moonshot-v1-8k在地图绘制任务上的准确率从10.1%提高到76.3%。|
|**2024-08-26**|**CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models**|Shubham Bharti et.al.|[2408.14419](http://arxiv.org/abs/2408.14419)|null|我们提出了一种名为CHARTOM的视觉理论理解基准，针对多模态大型语言模型。CHARTOM由专门设计的数据可视化图表组成。给定一个图表，语言模型不仅需要正确理解图表（事实问题），还需要判断该图表是否会让人类读者产生误导（思维问题）。这两个问题都具有重要的社会价值。我们将详细介绍构建CHARTOM基准的过程，包括其对人类表现的校准。|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418](http://arxiv.org/abs/2408.14418)|null|自动语音识别(ASR)系统在将语音转换为文本方面至关重要，然而，它们引入的错误会严重降低下游任务如摘要生成的表现。这个问题在临床对话摘要领域尤为突出，这是一个数据资源有限的领域，用于微调的监督数据稀缺，因此需要将ASR模型作为黑盒解决方案使用。传统的数据增强方法也不适用于提高摘要模型对噪音的鲁棒性，原因是缺乏足够的医疗对话音频记录及其对应的ASR转录文本。为了应对这一挑战，我们提出了一种名为MEDSAGE的方法，用于通过大型语言模型(LLMs)生成合成样本进行数据增强。具体来说，我们利用LLMs的上下文学习能力，并指导它们基于少量可用的医疗对话示例和音频记录，生成类似ASR的错误。实验结果表明，LLMs能够有效地建模ASR噪音，将这种含噪数据融入训练过程显著提高了医疗对话摘要系统的鲁棒性和准确性。这种方法解决了关键应用中ASR输出噪音的问题，提供了一个增强临床对话摘要可靠性的稳健解决方案。|
|**2024-08-26**|**Language-specific Calibration for Pruning Multilingual Language Models**|Simon Kurz et.al.|[2408.14398](http://arxiv.org/abs/2408.14398)|null|近期在大型语言模型（LLM）的剪枝领域取得的进展，在无需重新训练的情况下实现了卓越的压缩效果，并保持了高预测性能。然而，这类研究主要关注于使用英语文本进行剪枝校准，而忽略了现代LLM的多语言性质及其在非英语语言中的广泛应用。本文旨在探索用于剪枝多语言模型的有效策略。  我们进行了首个全面的实证研究，对比了不同校准语言在多语言任务、模型和最先进的剪枝技术下对剪枝的影响。我们的结果提供了实用的建议，例如，在目标语言上进行校准可以有效地降低困惑度，但不一定能促进下游任务的性能提升。进一步的分析实验揭示，目标语言上的校准主要贡献在于保留与流畅性和连贯性相关的语言特定特性，但可能无法捕捉到与理解能力和推理能力等语言通用特性的关联。  最后，我们为未来的实践者提供了实际的建议。|
|**2024-08-26**|**Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning**|Sakhinana Sagar Srinivas et.al.|[2408.14387](http://arxiv.org/abs/2408.14387)|null|空间时间预测在交通系统、物流和供应链管理等多个领域发挥着关键作用。然而，现有方法受限于处理大规模复杂数据的能力。为了克服这一限制，我们提出了一种结合开源大型和小型语言模型（LLMs 和 LMs）与传统预测方法的混合策略。通过引入动态提示和分组查询、多头注意力机制，该策略能够更有效地捕捉演变非线性时间序列数据中的内部系列和跨系列依赖关系。此外，我们利用低秩适配与激活记忆减少技术（LoRA-AMR），在消费级硬件上对开源小型 LM 进行定制化微调，以分析时间序列趋势，同时保留推理延迟并降低计算开销和激活存储内存需求。我们将语言模型处理与传统时间序列表示学习方法相结合，实现跨模态集成，从而获得稳健且准确的预测结果。通过在多个实际世界数据集上的广泛实验，该框架的效能得到了充分验证，其预测准确性显著优于现有方法。|
|**2024-08-26**|**Probing Causality Manipulation of Large Language Models**|Chenyang Zhang et.al.|[2408.14380](http://arxiv.org/abs/2408.14380)|**[link](https://github.com/tongjinlp/llm-causality-probing)**|**大型语言模型（LLM）在自然语言处理任务上展现了多种能力，包括因果关系问题。预训练的模型通常基于统计关联工作，而非专注于句子中的因果与影响。因此，探索LLM内部对因果性的操纵是必要的。本文提出了一种新颖的方法，通过提供不同的捷径并观察模型行为来探查因果性操纵的层级。我们利用检索增强生成（RAG）和上下文学习（ICL）技术，针对设计的因果分类任务，对主流LLM进行实验，包括GPT-4以及一些较小的和特定领域的模型。  我们的实验结果表明，LLM能够识别与因果性相关的实体，并认识到直接的因果关系。然而，LLM缺乏专门的因果认知能力，只是将因果性视为句子整体语义的一部分。**|
|**2024-08-26**|**SWE-bench-java: A GitHub Issue Resolving Benchmark for Java**|Daoguang Zan et.al.|[2408.14354](http://arxiv.org/abs/2408.14354)|**[link](https://github.com/multi-swe-bench/multi-swe-bench-env)**|**GitHub问题解决是软件工程中的关键任务，近期在行业和学术界都受到了广泛关注。在这个领域内，SWE-bench已经发布，旨在评估大型语言模型（LLMs）的问题解决能力，但目前仅关注Python版本。然而，支持更多编程语言同样至关重要，因为工业界对此有强烈需求。作为迈向多语言支持的第一步，我们开发了Java版的SWE-bench，称为SWE-bench-java。我们已公开发布了数据集，并提供了基于Docker的评估环境和排行榜，这些都将持续维护和更新。为了验证SWE-bench-java的可靠性，我们实现了经典方法SWE-agent，并在其中测试了几种强大的LLMs。众所周知，构建高质量的多语言基准既耗时又费力，因此我们欢迎通过拉取请求或合作来加速其迭代和改进，为完全自动化的编程铺平道路。**|
|**2024-08-23**|**MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?**|Yi-Fan Zhang et.al.|[2408.13257](http://arxiv.org/abs/2408.13257)|null|近期，全面评估多模态大型语言模型（MLLMs）在研究社区中引发了广泛关注。然而，我们注意到现有基准测试存在一些普遍的障碍，使得衡量模型面临的实际世界挑战变得困难，包括：1）数据规模较小导致性能波动大；2）依赖模型生成注释造成数据质量受限；3）任务难度不足，尤其是由于图像分辨率有限。为了克服这些问题，我们引入了MME-RealWorld。具体而言，我们从公共数据集和互联网收集了超过30万张图片，并筛选出13,366张高质量图片进行标注。这一过程中，我们动用了25名专业注释员和7名MLLM领域的专家，共贡献了29,429个问题-答案对，涵盖了5种真实世界场景下的43个子任务，这些任务甚至对人类来说也极具挑战性。据我们所知，MME-RealWorld是迄今为止最大的人工标注基准，其特征为最高分辨率以及专注于真实世界应用的目标导向。  我们进一步对28个领先的MLLM进行了详尽的评估，如GPT-4o、Gemini 1.5 Pro和Claude 3.5 Sonnet。我们的结果显示，即使是最先进的模型也无法应对我们的基准测试，其中没有一个模型达到60%的准确率。感知高分辨率图像和理解复杂的真实世界场景仍然是亟待解决的关键问题。相关的数据和评估代码已发布在https://mme-realworld.github.io/ 。|
|**2024-08-23**|**Domain-specific long text classification from sparse relevant information**|Célia D'Cruz et.al.|[2408.13253](http://arxiv.org/abs/2408.13253)|null|大型语言模型无疑在自然语言处理领域实现了重大革新，当前的趋势是推动单一模型解决所有任务（如情感分析、翻译等）。然而，在处理稀疏信息或弱信号时，这些模型的统计机制难以有效利用关键信息。例如，在长篇特定领域文档的分类中，相关性往往依赖于一个或几个关键术语。医疗领域中，确定某个报告是否包含了关于患者状况的关键信息至关重要。这些关键信息通常基于一两个特定的孤立术语。  本文提出了一种层次化模型，该模型利用一个潜在目标术语列表来检索候选句子，并将这些句子表示为包含它们的目标术语的上下文嵌入。对目标术语（或术语）的嵌入进行聚合导致文档表示被用于分类。我们分别在英语和法语的公开医疗文档基准数据集以及私有医疗数据集上评估了我们的模型。结果显示，我们的窄层级模型在特定领域背景下检索相关长文档方面优于大型语言模型。|
|**2024-08-23**|**Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time**|Yingyu Liang et.al.|[2408.13233](http://arxiv.org/abs/2408.13233)|null|本文提出了一种新型的快速计算方法，用于多层变换器模型中的梯度计算。该方法在几乎线性时间内 $n^{1+o(1)}$计算整个多层变换器模型的梯度，其中$n$ 是输入序列长度。这一突破极大地降低了传统二次时间复杂度相关的计算瓶颈。我们的理论适用于任何损失函数，并在全模型上保持可控制的近似误差。此外，我们的分析还考虑了多层变换器模型包含许多实用子模块的情况，如残差连接、因果掩码和多头注意力。通过提高大型语言模型中梯度计算的效率，我们期望通过基于我们的理论结果改进长上下文语言模型的训练和部署，使这些模型更加有效。|
|**2024-08-23**|**EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods**|Hongcheng Ding et.al.|[2408.13214](http://arxiv.org/abs/2408.13214)|null|准确预测EUR/USD汇率对投资者、企业和政策制定者至关重要。本文提出了一种创新框架IUS，该框架结合了新闻和分析的非结构化文本数据与汇率和金融指标的结构化数据，以增强汇率预测能力。IUS框架利用大型语言模型进行文本情感极性评分和汇率变动分类。这些文本特征与定量特征相结合，并输入到因果驱动特征生成器中。然后使用Optuna优化的Bi-LSTM模型预测EUR/USD汇率。实验结果表明，所提出的模型在减少平均绝对误差（MAE）10.69%和根均方误差（RMSE）9.56%方面优于基准模型。结果显示，通过融合非结构化和结构化数据，准确性比仅使用结构化数据更高。此外，使用顶级12个重要定量特征和文本特征相结合进行特征选择证明是最有效的。提出的IUS框架和Optuna-Bi-LSTM模型提供了一种强大的新方法，用于多源数据集成的汇率预测。|
|**2024-08-23**|**DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation**|Qiming Zhu et.al.|[2408.13204](http://arxiv.org/abs/2408.13204)|null|代码基准，如HumanEval，广泛用于评估大型语言模型（LLMs）的能力，提供了它们优势与不足的洞察。然而，当前的基准主要集中在通用编码任务上（例如：冒泡排序、最大公约数），对领域特定编码任务（如计算、系统、加密）的探索则较少。为了填补这一空白，我们提出了一种多领域代码基准DOMAINEVAL，旨在全面评估LLMs的编码能力。我们的流程以全自动方式工作，允许从代码仓库中构建格式化的研究主题进行底部推动式构建。通过使用12个代表性LLM在DOMAINEVAL上的评估，我们观察到了一些有趣的结果。  我们注意到，LLMs在计算任务上表现良好，但在加密和系统编码任务上却有所欠缺。某些LLM在这些领域的性能差距可能高达68.94%（80.94%-12.0%）。我们也发现生成更多样本可以提高LLMs的整体性能，但领域偏见甚至可能增加。本研究的贡献包括一个代码生成基准数据集DOMAINEVAL，涵盖六个流行领域，以及一个完全自动化的管道用于构建代码基准，并基于在DOMAINEVAL上的性能识别了LLMs在代码生成任务上的局限性，提供了未来研究改进的方向。领导者板可在https://domaineval.github.io/查看。|
|**2024-08-23**|**Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning**|Hourui Deng et.al.|[2408.13184](http://arxiv.org/abs/2408.13184)|null|在大型语言模型（LLM）领域，空间推理是实现感知智能的基础。然而，在简单的迷宫环境中，LLM在长期路径规划方面仍面临挑战，主要受到其空间幻觉和长期推理导致的上下文不一致幻觉的影响。为了应对这一挑战，本研究提出了一种创新模型——空间到关系转换与递进Q学习（S2RCQL）。为解决LLM的空间幻觉问题，我们提出了“空间到关系”的方法，将空间提示转化为实体关系和表示实体关系链的路径，充分挖掘了LLM在序列思考方面的潜力。在此基础上，我们设计了一种基于Q学习的路径规划算法，以缓解上下文不一致幻觉，增强LLM的推理能力。通过将状态动作的Q值作为提示的辅助信息，我们纠正了LLM的幻觉，引导LLM学习最优路径。最后，我们提出了一种基于LLM的反向课程学习技术，进一步缓解了上下文不一致幻觉。该技术通过降低任务难度并利用成功经验，帮助LLM快速积累，并以此来应对更复杂任务。我们在百度自主研发的LLM：ERNIE-Bot 4.0上进行了全面实验。结果显示，我们的S2RCQL在成功率和最优性方面分别提高了23%至40%，相较于先进的提示工程方法取得了显著进步。|
|**2024-08-23**|**IntelliCare: Improving Healthcare Analysis with Variance-Controlled Patient-Level Knowledge from Large Language Models**|Zhihao Yu et.al.|[2408.13073](http://arxiv.org/abs/2408.13073)|**[link](https://github.com/yzhHoward/IntelliCare)**|在电子健康记录（EHR）数据的深度学习方法取得巨大进步的同时，它们在处理有限数据中的多样化的医学代码时往往难以全面捕捉其语义。引入大型语言模型（LLM）的知识整合为提升医疗保健预测提供了有前景的途径。然而，LLM分析可能会因歧义问题和不一致性导致显著的波动，这阻碍了其有效利用。为解决这些挑战，我们提出了一种名为IntelliCare的新型框架，旨在通过利用LLM提供高质量的患者级外部知识并增强现有的EHR模型来改善医疗保健预测。具体来说，IntelliCare通过识别患者群体，并利用与任务相关的统计信息来增强LLM的理解和生成能力，有效地解决了歧义问题。此外，它通过结合EHR模型和困惑度量来细化从LLM获取的知识，采用混合方法生成多个分析结果并进行校准。在三个临床预测任务上对两个大规模EHR数据集的实验评估表明，IntelliCare能够显著提高现有方法的表现，凸显了其在推进个性化医疗保健预测和决策支持系统方面的潜力。|
|**2024-08-23**|**Guiding IoT-Based Healthcare Alert Systems with Large Language Models**|Yulan Gao et.al.|[2408.13071](http://arxiv.org/abs/2408.13071)|null|在医疗健康警报系统（HAS）领域，随着人工智能（AI）、物联网（IoT）技术的快速发展以及公众健康意识的提高，HAS正经历着快速的变革。尽管取得了显著的进步，但存在一个核心挑战：如何在资源有限的环境中，在个性化健康警报的准确性与严格隐私保护之间找到平衡点。  为了解决这一问题，我们提出了一种统一框架——LLM-HAS（大型语言模型医疗健康警报系统）。该框架将大型语言模型（LLM）融入到HAS中，以显著提升警报的准确性、确保用户隐私，并增强个性化医疗服务，同时改善用户体验的质量（QoE）。我们的创新框架采用混合专家（MoE）方法，结合LLM，通过分析用户的个性化偏好和潜在健康风险来处理额外的文本工作描述。这种分析指导了专门的深度强化学习（DDPG）专家的选择，他们负责提供精确的健康警报。此外，LLM-HAS能够处理对话式用户反馈，不仅允许对DDPG进行微调，还能加深用户参与度，从而提高健康管理策略的准确性和个性化程度。  模拟结果验证了LLM-HAS框架的有效性，表明其作为利用生成型人工智能（GAI）提供高度准确可靠警报的突破性方法的潜力。|
|**2024-08-23**|**VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models**|Wentao Wu et.al.|[2408.13031](http://arxiv.org/abs/2408.13031)|**[link](https://github.com/event-ahu/vfm-det)**|**现有车辆检测器通常通过在基于预训练主干（如ResNet、ViT）的预训练典型检测器（例如YOLO、RCNN、DETR系列）上进行车辆图像训练获得。一些研究者还利用并增强大型基础模型来提升检测性能。然而，我们认为这些检测器可能仅获得次优结果，因为它们使用的大型模型并非专门为车辆设计。此外，他们的结果高度依赖于视觉特征，并且很少考虑车辆语义信息与视觉表示之间的对齐。  在此工作中，我们提出了一种基于预训练的车辆模型（VehicleMAE）和大型语言模型（T5）的新车辆检测范式，称为VFM-Det。它遵循区域建议框检测框架，每个提议的特征可以通过VehicleMAE增强。更重要的是，我们提出了一种新的VAtt2Vec模块，用于预测这些提议的车辆语义属性并将它们转换为特征向量，通过对比学习增强视觉特征。对三个车辆检测基准数据集的广泛实验充分证明了我们的车辆检测器的有效性。具体而言，我们的模型分别在Cityscapes数据集上的 $AP_{0.5}$、$AP_{0.75}$指标上，相较于基线方法提高了$+5.1\%$、$+6.2\%$ 。此工作的源代码将在https://github.com/Event-AHU/VFM-Det发布。**|
|**2024-08-23**|**In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting**|Haowei Du et.al.|[2408.13028](http://arxiv.org/abs/2408.13028)|null|在当前的学术界，对基于指令增强的少量实例的大规模语言模型（Large Language Models, LLM）进行上下文学习（In-context Learning, ICL）引起了越来越多的关注。现有的用于ICL的示例选择方法利用稀疏或密集检索器，并且能够产生有效性能。然而，这些方法并未充分利用LLM对反馈信息的利用来训练检索器，所选的示例可能无法显著提升LLM的类比能力。  为了克服这一问题，我们提出了基于强化学习的策略框架（Policy-based Reinforcement Learning Framework, RLS）用于示例选择。该框架由语言模型（Language Model, LM）选择器和LLM生成器组成。语言模型选择器将候选示例编码为密集表示，并从中选择top-k个示例作为LLM的示范。通过采用LLM的输出来计算奖励和策略梯度，优化语言模型选择器。  我们在不同数据集上进行了实验，显著优于现有的示例选择方法。此外，我们的方法在少量样本设置下相较于监督微调（Supervised Fine-tuning, SFT）模型显示出优势。进一步的实验结果表明，示例的数量丰富性和与测试案例的相似性对于ICL中的LLM性能至关重要。|
|**2024-08-22**|**Controllable Text Generation for Large Language Models: A Survey**|Xun Liang et.al.|[2408.12599](http://arxiv.org/abs/2408.12599)|**[link](https://github.com/iaar-shanghai/ctgsurvey)**|**在自然语言处理（NLP）领域，大型语言模型（LLMs）展现了卓越的文本生成质量。然而，在实际应用中，LLMs需要满足日益复杂的需求。除了避免误导性或不适当的内容，LLMs还被期望根据特定用户需求进行调整，如模仿特定的写作风格或生成富有诗意的文本。这些多样的需求推动了可控文本生成（CTG）技术的发展，旨在确保输出内容符合预设的控制条件，如安全性、情感倾向、主题一致性以及语言风格，同时保持高质量的有用性、流畅性和多样性。  本文系统地回顾了CTG在LLMs领域的最新进展，详细定义了其核心概念，并明确了控制条件和文本质量的要求。我们将CTG任务分为两大类：内容控制和属性控制，并对每种类型的方法进行了讨论，包括模型重训练、微调、强化学习、提示工程、潜在空间操纵和解码时干预。我们分析了每种方法的特点、优势和局限性，提供了实现生成控制的深入见解。此外，我们回顾了CTG评估方法、总结了其跨领域的应用，并指出了当前研究的关键挑战，如流畅度和实用性的降低。我们还提出了若干呼吁，强调未来研究应更注重实际应用。本文旨在为该领域的研究人员和开发者提供有价值的指导。我们的参考文献列表和中文版本已开源在https://github.com/IAAR-Shanghai/CTGSurvey。**|
|**2024-08-22**|**RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment**|Xiaohan Wang et.al.|[2408.12579](http://arxiv.org/abs/2408.12579)|null|大型语言模型（LLM）如GPT-4、MedPaLM-2和Med-Gemini在各类医疗评估指标上表现出与医学专家竞争的性能。然而，它们在与医生相媲美的专业诊断方面仍面临挑战，特别是在高效收集患者信息以及推理最终诊断的过程中。为此，我们提出了一种名为RuleAlign的框架，旨在使LLM与特定诊断规则保持一致。我们构建了一个包含基于规则的医患对话数据集，并设计了一种通过偏好学习进行对齐的学习方法。实验结果证明了所提出方法的有效性。我们期望我们的工作能够启发探索LLM作为AI医师的潜力。|
|**2024-08-22**|**Jamba-1.5: Hybrid Transformer-Mamba Models at Scale**|Jamba Team et.al.|[2408.12570](http://arxiv.org/abs/2408.12570)|null|我们推出了Jamba-1.5，基于我们Jamba架构的新型指令优化大型语言模型。Jamba是一种混合Transformer-Mamba专家混合架构，它在上下文长度范围内提供了高吞吐量和低内存使用，同时保持与Transformer模型相同或更好的质量。我们发布了两种模型大小：Jamba-1.5-Large，具有94B个活跃参数；以及Jamba-1.5-Mini，具有12B个活跃参数。这两种模型均针对多种对话和指令遵循能力进行了微调，并且具有256K令牌的最大有效上下文长度，在开放权重模型中最大。为了支持成本效益的推理，我们引入了ExpertsInt8，这是一种新颖的量化技术，允许在处理256K令牌上下文时将Jamba-1.5-Large模型放入具有8个80GB GPU的机器上而不会损失质量。当在一系列学术和聊天机器人基准上进行评估时，Jamba-1.5模型取得了出色的结果，同时提供了高吞吐量并优于其他开放权重模型在长上下文基准上的性能。两种大小的模型的权重都根据Jamba开放模型许可公开提供，并且我们发布了ExpertsInt8作为开源软件。|
|**2024-08-22**|**ssProp: Energy-Efficient Training for Convolutional Neural Networks with Scheduled Sparse Back Propagation**|Lujia Zhong et.al.|[2408.12561](http://arxiv.org/abs/2408.12561)|**[link](https://github.com/lujiazho/ssprop)**|**近期，深度学习取得了显著进展，尤其是在生成模型领域，如大型语言模型和概率性扩散模型。然而，训练这些模型往往需要大量的计算资源，消耗数十亿的浮点运算（petaFLOPs），导致巨大的能源消耗和碳足迹，引发了对环境的重大担忧。在训练深度学习模型的过程中，反向传播（Back-propagation, BP）是主要的计算负担来源。  为了推动能源效率的提高，并允许在任何机器和设备上实现稀疏学习，我们提出了一种通用、能源高效的卷积模块，它能够无缝集成到任何深度学习架构中。具体来说，我们引入了通道级稀疏性，并基于假设BP通常密集且低效，这可能导致过拟合和高计算消耗，提出了额外的梯度选择调度器，在反向传播阶段进行选择。实验结果表明，我们的方法可以减少40%的计算量，同时有可能提升模型性能，在图像分类和生成任务上得到验证。这种减少可以带来显著的能源节省和较低的碳足迹，尤其是在大型AI系统的研究与开发阶段。此外，我们的方法以不同于Dropout的方式缓解了过拟合问题，允许它与Dropout结合使用，进一步提高模型性能并降低计算资源消耗。广泛实验表明，我们的方法适用于各种数据集和任务，并与多种深度学习架构和模块兼容。相关代码已公开发布在https://github.com/lujiazho/ssProp。**|
|**2024-08-22**|**Towards Evaluating and Building Versatile Large Language Models for Medicine**|Chaoyi Wu et.al.|[2408.12547](http://arxiv.org/abs/2408.12547)|**[link](https://github.com/magic-ai4med/meds-ins)**|**在这项研究中，我们提出了一种全面的基准测试——MedS-Bench，旨在评估大型语言模型（LLMs）在临床场景中的性能。与现有侧重于多项选择问题回答的基准不同，MedS-Bench覆盖了11个高级别临床任务，包括临床报告摘要、治疗建议、诊断、实体识别和医学概念解释等。我们使用少量提示对六款领先的LLM进行了评估，如MEDITRON、Mistral、InternLM 2、Llama 3、GPT-4和Claude-3.5，发现即使是最高级的模型在这些复杂任务上也存在挑战。为了应对这些局限性，我们开发了MedS-Ins，一个面向医学领域的大型指令调优数据集。MedS-Ins包含了58个医学相关的语言语料库，总计1350万样本，涵盖了122个任务。通过展示该数据集的用途，我们在一个轻量级、开源的医疗语言模型上进行了指令调优实验，结果得到了名为MMedIns-Llama 3的新模型，它在几乎所有临床任务上的表现都超过了现有模型。为了促进对LLMs应用于临床挑战的进一步发展，我们已将MedS-Ins数据集完全公开，并邀请研究社区参与其扩展。此外，我们启动了一个动态排行榜，计划定期更新测试集，以跟踪进展并增强通用LLM在医学领域中的适应能力。排行榜：https://henrychur.github.io/MedS-Bench/。Github：https://github.com/MAGIC-AI4Med/MedS-Ins。**|
|**2024-08-22**|**MEDCO: Medical Education Copilots Based on A Multi-Agent Framework**|Hao Wei et.al.|[2408.12496](http://arxiv.org/abs/2408.12496)|null|大型语言模型（LLMs）在医学和健康领域等多个研究领域产生了重大影响，然而LLMs作为医疗教育中的助手潜力尚未得到充分探索。当前的AI辅助教育工具受限于单一学习方法以及无法模拟实际医疗培训的多学科性和互动性。为了克服这些局限性，我们提出了一种名为MEDCO（Medical EDucation COpilots）的新型多代理助手系统，专门用于模拟真实世界医疗培训环境。MEDCO整合了三个核心代理：一个自主患者、一位专家医生和一位放射科医师，从而构建了一个多模态和互动的学习环境。我们的框架着重于教授高效提问技巧、跨学科协作以及学生之间的同伴讨论。  实验结果显示，经过MEDCO训练的虚拟学生不仅实现了与高级模型相媲美的显著性能提升，还展现出类似人类的学习行为和进步，并且学习样本数量增加。这项工作对医疗教育领域做出了贡献，通过引入一种互动和协作的学习方法。此外，它还提供了关于集成AI的训练模式有效性的宝贵见解。|
|**2024-08-22**|**GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models**|Kunsheng Tang et.al.|[2408.12494](http://arxiv.org/abs/2408.12494)|**[link](https://github.com/kstanghere/gendercare-ccs24)**|**大型语言模型（LLM）在自然语言生成方面展现了惊人的能力，但也被观察到放大了社会偏见，尤其是与性别相关的偏见。针对这一问题，已经提出了若干基准测试来评估LLM中的性别偏见。然而，这些基准测试往往缺乏实际的灵活性或无意中引入了偏见。为了应对这些问题，我们引入了GenderCARE框架，这是一个全面的框架，包括创新的准则、评估、减少技术以及评价指标，旨在量化和减轻LLM中的性别偏见。  首先，我们确立了开创性的性别平等基准准则，覆盖了包容性、多样性、可解释性、客观性、稳健性和现实性等多个维度。根据这些准则，我们构建了GenderPair，一个新颖的配对基准，旨在全面评估LLM中的性别偏见。我们的基准提供了标准化且现实的评估，包括以前被忽视的性别群体，如跨性别者和非二元个体。此外，我们开发了有效的去偏技术，包括反事实数据增强和专门的微调策略，以在不损害LLM整体性能的前提下减少性别偏见。  广泛的实验表明，在17个不同的LLM上，各种性别偏见基准的显著减少，最高可达超过90%，平均值超过35%。重要的是，这些减少带来的主流语言任务方面的变异性保持在2%以下。通过提供真实性的评估和针对性别偏见的定制减少，我们希望GenderCARE能够代表在LLM中实现公平和公正的一个重要步骤。更多细节请参阅https://github.com/kstanghere/GenderCARE-ccs24。**|
|**2024-08-23**|**Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese**|Khang T. Doan et.al.|[2408.12480](http://arxiv.org/abs/2408.12480)|null|在这份报告中，我们引入了Vintern-1B，这是一个针对越南语任务的可靠的一百亿参数多模态大型语言模型（MLLM）。通过整合Qwen2-0.5B-Instruct语言模型与InternViT-300M-448px视觉模型，Vintern-1B优化了在光学字符识别（OCR）、文档提取和越南语上下文中的通用问题回答等应用。该模型在超过三百万张图像-问题-答案对的数据集上进行了微调，实现了在多个越南语基准测试如OpenViVQA和ViTextVQA上的稳健性能和可靠结果。Vintern-1B足够小，可以轻松地集成到各种离线应用中。此外，我们还开源了几组用于文本和图表的越南语视觉问答（VQA）数据集，使用的是Gemini 1.5 Flash创建的。我们的模型可以在以下链接获取：https://huggingface.co/5CD-AI/Vintern-1B-v2。|
|**2024-08-22**|**Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action Recognition**|Bozheng Li et.al.|[2408.12475](http://arxiv.org/abs/2408.12475)|null|本文提出了一种新颖的时序序列感知模型（TSAM）以进行少量样本动作识别（FSAR），该模型在预训练框架中引入了序列感知器适配器，旨在整合空间信息和序列时间动态到特征嵌入中。与现有通过探索所有帧之间关系来捕获时间信息的细调方法不同，我们的基于感知器的适配器能够沿时间线递归地捕捉序列动态，并感知顺序变化。为了获取每个类别的判别性表示，我们扩展了从大型语言模型（LLMs）导出的文本库，对视觉原型进行了丰富，通过整合上下文语义信息。此外，我们引入了一种不平衡最优传输策略来进行特征匹配，以减轻与类别无关特征的影响，从而促进更有效的决策。在五个FSAR数据集上的实验结果表明，我们的方法创下了新的基准，与第二好的竞争对手相比取得了显著的优势。|
|**2024-08-22**|**DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender Systems**|Jiaju Chen et.al.|[2408.12470](http://arxiv.org/abs/2408.12470)|null|大型语言模型（LLM）在推荐系统中的集成显著提升了性能，但往往伴随着推荐多样性下降的问题，这可能损害用户体验。为了克服这一挑战，可控推荐系统应运而生，它允许用户指定偏好并获得满足其多样化需求的推荐。尽管具有潜力，现有的可控推荐系统通常依赖于简单机制，如单一提示，来调节多样性，这种做法未能充分捕捉用户偏好的复杂性。针对这些局限性，我们提出了一种名为DLCRec的新框架，旨在实现基于LLM的推荐系统的精细粒度多样性控制。与传统方法不同，DLCRec采用精细任务分解策略，将推荐过程拆分为三个依次进行的子任务：体裁预测、体裁填充和项目预测。这些子任务独立训练并在用户定义的控制数指导下依次推理，确保了对多样性的更精确控制。此外，稀缺且分布不均的多样性相关用户行为数据的缺乏构成了对微调的严峻挑战。为解决这些问题，我们引入了两种数据增强技术，以增强模型对噪声和离群数据的鲁棒性。这些技术使模型接触到更广泛的模式，从而提高其生成不同多样性的推荐的适应性。我们的全面实验结果表明，DLCRec不仅提供了对多样性的精确控制，而且在多个推荐场景中都优于最先进的基线方法。|
|**2024-08-21**|**SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs**|Yuanyang Yin et.al.|[2408.11813](http://arxiv.org/abs/2408.11813)|null|近期，多模态大型语言模型（MLLMs）在感知和推理能力方面展现出了惊人的表现，它们通常由视觉编码器、适配器和大型语言模型（LLM）组成。适配器作为视觉与语言组件之间的关键桥梁。然而，通过图像级监督训练适配器往往会导致显著的对齐偏差，这会削弱LLM的能力并限制多模态LLM的潜力。为了解决这一问题，我们引入了监督嵌入对齐（SEA），这是一种基于视觉语言预训练模型（如CLIP）的分词级对齐方法，通过对比学习来调整视觉分词与LLM嵌入空间的一致性。这种方法确保了视觉和语言表示之间更协调的整合，从而增强多模态LLM的性能和可解释性，同时保留其固有特性。广泛实验表明，SEA有效地提高了MLLMs，特别是对于较小的模型，无需额外的数据或推理计算。此外，SEA也为开发更通用和适应性强的解决方案以增强多模态系统奠定了基础。|
|**2024-08-21**|**Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models**|Yuzhou Huang et.al.|[2408.11801](http://arxiv.org/abs/2408.11801)|null|传统视觉叙事复杂，需要专业知识和大量资源，但往往受限于人类的创造力与创作精度。尽管大型语言模型（LLMs）增强了视觉叙事能力，现有方法往往局限于二维视觉效果或通过动作合成和行为模拟简化故事，未能生成全面、多维的叙事。为此，我们提出Story3D-Agent，一种创新的方法，利用LLM的能力将提供的叙事转化为三维渲染可视化。通过集成程序建模，我们的方法能够精确控制多角色的动作和动态，以及各种装饰元素，确保长期和动态的三维表现。此外，我们的方法支持通过逻辑推理进行叙事扩展，确保生成的内容与现有条件保持一致。我们对Story3D-Agent进行了详尽的评估，以验证其有效性，并提供了基本框架来推动三维故事表示的发展。|
|**2024-08-21**|**PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain**|Rounak Meyur et.al.|[2408.11800](http://arxiv.org/abs/2408.11800)|null|在自然语言处理（NLP）和文本生成领域快速发展的背景下，检索增强生成（RAG）的兴起为通过利用用户指定数据库中的信息来提高生成文本的质量和可靠性提供了有前景的途径。基准测试对于评估和比较不同RAG配置在检索器和生成器方面的性能至关重要，提供了这些配置的有效性、可扩展性和特定领域和应用的适用性的洞察。本文提出了一种全面框架，用于生成与特定领域相关的RAG基准。该框架基于自动问题答案生成与人类（领域专家）-人工智能大型语言模型（LLM）协作的自动化过程。以案例研究的形式，我们通过引入PermitQA作为风场选址和许可领域的首个基准进行了框架展示，该基准包含了与风能项目环境影响相关的多篇科学文档/报告。  我们的框架系统地使用多种指标和不同复杂度级别的问题类型来评估RAG性能。我们还展示了不同模型在我们的基准上的表现。|
|**2024-08-21**|**EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model**|Feipeng Ma et.al.|[2408.11795](http://arxiv.org/abs/2408.11795)|null|在多模态研究领域，众多研究利用大量的图像-文本对进行模态对齐学习，将大型语言模型（Large Language Models, LLMs）转化为多模态LLMs，并在各种视觉语言任务上表现出色。目前主要的实现方法分为两类：自注意力基和交叉注意力基方法。自注意力基方法因其简单的多层感知机（MLP）架构而具有较高的数据效率，但在计算效率方面却相对较低，原因在于其需要将视觉和文本令牌作为输入进行连接。而交叉注意力基方法虽然在额外的学习参数方面不如自注意力基方法高效，但由于避免了为LLM提供过长序列输入，因此在计算效率方面表现更高。为了平衡这些权衡，我们提出了数据高效且计算高效的多模态大型语言模型（EE-MLLM）。EE-MLLM在不引入额外模块或可学习参数的情况下，实现了数据和计算效率的提升。具体来说，我们对多模态LLM中的原始自注意力机制进行了改进，引入了一种复合注意力机制。该机制有两个关键特性：1）消除视觉令牌内部的自注意力计算，以实现计算效率；2）重用LLM每一层的权重，以促进视觉与语言之间的有效模态对齐，从而实现数据效率。实验结果表明，EE-MLLM在包括MMBench、SeedBench等通用性数据集以及TextVQA、DocVQA等精细粒度任务在内的多种基准测试中都展现出显著的有效性。|
|**2024-08-21**|**Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design**|Nathaniel H. Park et.al.|[2408.11793](http://arxiv.org/abs/2408.11793)|null|分子属性预测和通过深度学习模型进行生成设计是研究的热点领域，这主要归因于它在加速新材料开发方面的潜力。随着大型语言模型（LLMs）和由LLM驱动的代理系统的出现，这些工作流程得到了显著增强，这些系统利用预训练模型在更复杂的研究任务背景下进行预测。尽管有效，但在材料设计任务中的信息检索方面，代理系统仍有改进空间。此外，对预测深度学习模型的替代应用，如利用它们的潜在表示来促进跨模态检索增强生成，在由LLM驱动的代理系统中实现任务特定的材料设计，这一领域尚未得到探索。  在此，我们证明了大规模、预训练的化学基础模型可以作为使化学信息检索语义化的基础，适用于小分子、复杂聚合物材料和反应。此外，我们展示了化学基础模型与图像模型（如OpenCLIP）相结合，能够实现跨多个表征数据域的前所未有的查询和信息检索。最后，我们展示了这些系统在多代理系统中的集成，以支持结构和拓扑为基础的自然语言查询和信息检索，从而促进复杂研究任务的执行。|
|**2024-08-21**|**Critique-out-Loud Reward Models**|Zachary Ankner et.al.|[2408.11791](http://arxiv.org/abs/2408.11791)|**[link](https://github.com/zankner/cloud)**|**传统的奖励模型在从人类反馈进行强化学习（RLHF）时，仅用于直接预测偏好分数，而不利用底层大型语言模型（LLM）的生成能力。这限制了奖励模型的能力，因为它们必须通过单一前向传递来隐式地推理响应的质量，即，必须在偏好建模过程中完成推理。为了使奖励模型能够显式地推理响应的质量，我们引入了“口头批评”（CLoud）奖励模型。CLoud奖励模型首先生成对助手响应的自然语言批评，然后使用这些批评来预测响应质量的标量奖励。  我们证明了对于Llama-3-8B和70B基础模型，CLoud奖励模型的成功：与经典奖励模型相比，CLoud奖励模型分别在RewardBench上提高了8B和70B基础模型的二元偏好分类准确率4.65和5.84个百分点。此外，当作为Best-of-N评分模型使用时，CLoud奖励模型在ArenaHard上的胜率也实现了帕累托改进。最后，我们探索了如何利用CLoud奖励模型的动态推理计算能力，通过自我一致性解码来进行奖励预测。  以上是关于“口头批评”（CLoud）奖励模型的摘要翻译，它展示了这种新型奖励模型在提升强化学习系统性能方面的潜力。**|
|**2024-08-21**|**DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework**|Zhifei Xie et.al.|[2408.11788](http://arxiv.org/abs/2408.11788)|null|我们提出了一种名为“DreamFactory”的LLM基框架，它能解决当前视频生成模型在创建长视频时遇到的挑战。DreamFactory通过多智能体协作原则和关键帧迭代设计方法，确保了长视频的一致性和风格统一。它利用链式思维（Chain of Thought，COT）来处理大型语言模型固有的不确定性。DreamFactory能够生成长、风格一致且复杂的视频。  对于这些长形式视频的评估提出了挑战。为此，我们提出了新的评估指标，如跨场景面部距离分数和跨场景风格一致性分数。为了促进这一领域的进一步研究，我们贡献了一个包含超过150个由人类评分的多场景视频的多场景视频数据集。|
|**2024-08-21**|**Personality Alignment of Large Language Models**|Minjun Zhu et.al.|[2408.11779](http://arxiv.org/abs/2408.11779)|**[link](https://github.com/zhu-minjun/palign)**|**为了弥补现有大语言模型（LLM）对齐方法在反映人类普遍价值观和行为时的不足，忽视了个体用户独特特征和偏好的问题，我们提出了个性对齐的概念。该方法旨在根据个体用户或紧密关联群体的具体偏好调整LLM的响应与决策。受心理测量学的启发，我们构建了Personality Alignment with Personality Inventories (PAPI) 数据集，包含了30万真实主体的数据，每个主体基于五大人格因素提供行为偏好信息。这一数据集使我们能够定量评估LLM在多大程度上能够与每个主体的行为模式相匹配。鉴于个性对齐面临的挑战：如个人数据有限、偏好多样以及可扩展性需求，我们开发了一种激活干预优化方法。这种方法利用最少的数据和计算资源提高了LLM高效对齐个体行为偏好的能力。我们的方法PAS不仅在性能上超越了DPO，而且优化时间仅为后者的五分之一，具有实际价值，推动了个性化的AI系统决策与推理的发展，增强了与每位用户的交互相关性和意义，促进了以人为本的人工智能的进步。相关代码已发布在<https://github.com/zhu-minjun/PAlign>。**|
|**2024-08-21**|**Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards**|Omar Erak et.al.|[2408.11775](http://arxiv.org/abs/2408.11775)|**[link](https://github.com/Nouf-Alabbasi/oKUmura_AI_Telecom_challenge)**|**近期的研究揭示了大型语言模型（LLMs）在电信标准方面的技术规范挑战。本文提出了一种基于Phi-2小型语言模型（SLM）的微调检索增强生成（RAG）系统，旨在作为通信网络的权威答案来源。我们开发的系统利用前瞻性的语义分块来动态确定解析断点，依据嵌入相似度进行调整，从而有效处理多种文档格式。针对技术标准中可能出现的多个相似上下文问题，我们采用了重新排名算法以优先考虑最相关的提取片段。考虑到Phi-2的小语境窗口限制，我们引入了一种名为SelfExtend的最新技术，在推理过程中扩展语境窗口，不仅提升了性能，还能适应客户到专业技术人员的各种查询和设计需求。为了微调，我们使用了低秩适配（LoRA）技术，在训练时提高计算效率，并在小数据集上实现有效的微调。我们的全面实验表明，在电信领域对现有问答方法的显著改进，性能超过GPT-4（大约是其规模的880倍）。这项工作展示了利用SLM在通信网络中的新方法，提供了高效性和性能之间的平衡，可作为构建智能语言模型的基础。**|
|**2024-08-21**|**Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks**|Yiyi Chen et.al.|[2408.11749](http://arxiv.org/abs/2408.11749)|**[link](https://github.com/siebeniris/vec2text_exp)**|大型语言模型（LLM）面临着来自网络攻击者的恶意影响，如对抗性、后门和嵌入反转攻击。对此，新兴的LLM安全领域致力于研究并防御此类威胁。迄今为止，该领域的大多数工作都集中在英语单一语言模型上，然而，最新研究表明，多语言LLM可能比其单一语言同僚更易受到各种攻击。尽管先前的研究已经探讨了在部分欧洲语言上的嵌入反转，但要将这些发现推及到不同语系和不同书写系统的语言，却极具挑战性。因此，本研究旨在探索多语言LLM在嵌入反转攻击下的安全性，并在20种语言中进行跨语言和跨书写的反转测试，覆盖8个语系和12种书写系统。我们的研究结果表明，阿拉伯字母和西里尔字母书写的语言以及印度-雅利安语系的语言特别容易受到嵌入反转的影响。我们进一步观察到反转模型倾向于出现语言混淆，有时大幅度降低了攻击的有效性。因此，我们系统地探索了这一瓶颈，揭示了一些可预测模式，这可能被攻击者利用。最终，本研究旨在深化对多语言LLM面临的主要安全漏洞的理解，并提高对最易受这些攻击影响的语言的意识。|
|**2024-08-20**|**Revisiting VerilogEval: Newer LLMs, In-Context Learning, and Specification-to-RTL Tasks**|Nathaniel Pinckney et.al.|[2408.11053](http://arxiv.org/abs/2408.11053)|**[link](https://github.com/nvlabs/verilog-eval)**|大型语言模型（LLM）在数字硬件代码生成领域的应用是一个新兴领域。大多数LLM主要是在自然语言和软件代码上进行训练的。硬件代码，如Verilog，只占训练数据的一小部分，而且很少有硬件基准存在。为了填补这一缺口，2023年发布了一个名为VerilogEval的开源基准，它提供了一个一致的评估框架，用于LLM在代码完成任务上的性能。该基准在当时的领先模型，包括GPT-4，进行了测试。然而，VerilogEval和其他Verilog生成基准缺乏失败分析，当前形式下也不利于探索提示技术。此外，在VerilogEval发布后，商业和开源模型都经历了持续的发展。  在这个工作中，我们评估了新发布的商业和开源模型的不同规模，针对改进后的VerilogEval基准套件。我们增强了VerilogEval的基础架构和数据集，通过自动分类失败，引入了支持上下文学习（ICL）示例的新提示，并扩展了支持的任务到规格到RTL转换。我们发现商业领域的最新模型有了可测量的改进，其中GPT-4 Turbo在规格到RTL任务上达到了59%的成功率。我们也研究了新出现的开源和领域特定模型的性能，并展示了模型从上下文学习中获得显著益处的可能性。我们发现最近发布的Llama 3.1 405B模型在性能上与GPT-4 Turbo相当，实现了58%的成功率，而较小的领域特定的RTL-Coder 6.7B模型则取得了令人印象深刻的37%的成功率。然而，提示工程对于实现良好的成功率至关重要，并且随着模型和任务的变化而变化。一个允许进行提示工程和失败分析的基准基础设施对于持续的模型开发和部署至关重要。|
|**2024-08-20**|**FLAME: Learning to Navigate with Multimodal LLM in Urban Environments**|Yunzhe Xu et.al.|[2408.11051](http://arxiv.org/abs/2408.11051)|**[link](https://github.com/xyz9911/FLAME)**|**大型语言模型（LLM）在视觉与语言导航（VLN）任务中展现出了潜在能力，但当前的应用仍面临挑战。虽然LLM在通用对话场景中表现出色，但在专门的导航任务上却表现不佳，相较于专为VLN设计的模型，其性能往往较低下。我们引入了FLAME（FLAMingo架构化实体代理），这是一种基于多模态LLM的新型代理和架构，旨在解决城市VLN任务，并能高效处理多个观察结果。  我们的方法采用了三阶段调优技术以实现对导航任务的有效适应：单感知调整用于街道视图描述、多感知调整用于轨迹总结以及端到端训练在VLN数据集上的综合能力。生成的数据集通过自动化过程合成而成。实验结果表明，FLAME在Touchdown数据集上的任务完成率较现有方法提高了7.3%，超越了最先进的方法。这项工作展示了多模态LLM在复杂导航任务中的潜力，代表了向实际应用多模态LLM于实体人工智能领域迈出的重要一步。项目页面：https://flame-sjtu.github.io**|
|**2024-08-20**|**MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding**|Jian Chen et.al.|[2408.11049](http://arxiv.org/abs/2408.11049)|**[link](https://github.com/infini-ai-lab/magicdec)**|大型语言模型（LLM）在诸如交互式聊天机器人、文档分析和代理工作流程等长期上下文应用中变得越来越普遍，但提供长上下文请求时，要实现低延迟和高吞吐量是一个挑战。推测性解码（SD）是一种广泛使用的降低延迟的技术，传统观点认为其效能仅限于较小的批次大小。然而，在MagicDec中，我们揭示了令人惊讶的事实：即使在高吞吐量推理环境中，对于中等到较长序列，SD仍能实现加速。更有趣的是，基于我们的严谨分析，一种智能起草策略可以在批次大小增加时获得更好的加速效果。  MagicDec首先识别出随着批次大小和序列长度增加的瓶颈转移，并利用这些洞察来更有效地部署推测性解码以支持高吞吐量推理。然后，它通过利用稀疏KV缓存的草案模型来解决随着序列长度和批次大小增加而扩展的KV瓶颈问题。|
|**2024-08-20**|**Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research**|Sreyoshi Bhaduri et.al.|[2408.11043](http://arxiv.org/abs/2408.11043)|null|本文提出了一种新颖的方法，利用基于检索增强生成（RAG）的大型语言模型（LLM）来分析访谈记录，以解决手动分析定性数据需要大量时间和努力的问题。研究旨在将研究问题设定为由LLM作为初级研究助手进行辅助的模式。本研究探讨了将LLM视为人才管理领域研究人员的初级质性研究助手的思维模型。通过扩展基于RAG的LLM方法，本文展示了这些模型在对半结构化访谈数据进行主题建模方面的灵活性，超越了它们在信息检索和搜索中的传统应用。  研究结果表明，基于LLM的RAG方法能够成功提取感兴趣的议题，与从同一数据集手动生成的主题相比，覆盖范围显著更高。这证明了使用LLM作为初级质性研究助手的可行性。此外，研究建议，使用此类模型的研究者应严格遵循传统质性研究中使用的质量标准，以确保其方法的严谨性和可靠性。  最后，论文提出了针对希望将LLM与现有质性研究范式相融合的行业实践者的关键建议，提供了一条有效整合这些强大但初级的人工智能工具在定性数据分析中的路径，特别是在人才领域。|
|**2024-08-20**|**Scaling Law with Learning Rate Annealing**|Howe Tissue et.al.|[2408.11029](http://arxiv.org/abs/2408.11029)|null|我们发现神经语言模型在训练过程中，交叉熵损失曲线遵循了一个与学习率（LR）衰减相关的缩放定律： $L(s) = L_0 + A\cdot S_1^{-\alpha} - C\cdot S_2$。其中，$S_1$代表前向区域，$S_2$ 代表学习率衰减区域。这一公式考虑了两个因素：（1）传统的缩放律定义的前向缩放；以及（2）学习率衰减带来的额外损失下降。因此，该公式能够描述每个步骤的完整损失曲线，而非仅限于训练结束时的单一损失点。通过应用包含学习率衰减的缩放律，并仅通过一到两次训练曲线拟合，我们能够准确预测语言模型训练在任何给定步骤和任何学习率调度（LRS）下的损失。  此外，这一方程准确地描述了训练过程中的动态，并为先前研究中关注的学习率调度和学习率衰减的相关实验发现提供了理论验证和解释。由此产生的洞察，也为研究人员在开发大型语言模型时提前选择关键的学习率调度策略提供了指导。最重要的是，由于整个训练曲线上的所有点都遵循该方程，我们可以在任何给定步骤和任何学习率调度下实现准确的损失预测，而所需计算成本仅为使用小松鼠缩放法则拟合语言模型损失所需的1%以下。这一方法极大地促进了缩放律拟合和预测在开发大型语言模型过程中的普及性。|
|**2024-08-20**|**Athena: Safe Autonomous Agents with Verbal Contrastive Learning**|Tanmana Sadhu et.al.|[2408.11021](http://arxiv.org/abs/2408.11021)|null|由于新兴能力，大型语言模型（LLMs）被用作基于语言的代理，执行各种任务并以不断增长的程度自主做出决策。这些自主代理能够理解高级指令、与环境互动，并使用可用给它们的工具集执行复杂任务。随着代理能力的扩展，确保它们的安全性和可信度变得越来越重要。在这项研究中，我们引入了Athena框架，它利用了口头对比学习的概念，通过将过去安全和不安全的轨迹作为上下文（对比）示例来指导代理向安全性发展，同时完成给定的任务。该框架还整合了一个批判性机制，在每个步骤上引导代理避免风险行为。此外，由于缺乏对基于LLM的代理安全推理能力的现有基准，我们收集了涵盖8个类别共计80个工具包和180个场景的一组数据集，提供了一种安全评估基准。我们的实验评估表明，口头对比学习和交互级批判性思考显著提高了安全性率。|
|**2024-08-20**|**While GitHub Copilot Excels at Coding, Does It Ensure Responsible Output?**|Wen Cheng et.al.|[2408.11006](http://arxiv.org/abs/2408.11006)|**[link](https://github.com/sensente/security-attacks-on-lccts)**|**快速发展的大型语言模型（LLMs）在代码补全能力方面取得了显著进步，催生了新一代基于LLM的代码补全工具（LCCT）。与通用LLM不同，这些工具具有独特的操作流程，整合多种信息源作为输入，并优先考虑代码建议而非自然语言交互，这引入了特定的安全挑战。此外，LCCT通常依赖于专有代码数据集进行训练，引发了关于敏感数据泄露的担忧。本文利用LCCT的独特特性，开发了针对两种关键安全风险的针对性攻击方法：越狱攻击和训练数据提取攻击。  实验结果揭示了LCCT中存在的重大漏洞，包括在GitHub Copilot上的99.4%成功越狱攻击率，在Amazon Q上的46.3%成功率。我们还成功从GitHub Copilot中提取了敏感用户数据，包括54个真实电子邮件地址和314个与GitHub用户名关联的物理地址。研究还表明，这些基于代码的攻击方法对通用LLM（如GPT系列）同样有效，突显了现代LLM处理代码时存在的更广泛安全问题。这些发现强调了LCCT面临的关键安全挑战，并提出了加强其安全框架的重要方向。  为了验证我们的研究成果，我们提供了相关代码示例和攻击样本，它们可从https://github.com/Sensente/Security-Attacks-on-LCCTs获取。**|
|**2024-08-20**|**CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models**|Michael Reinisch et.al.|[2408.10995](http://arxiv.org/abs/2408.10995)|null|新医疗治疗方法的开发需要多个临床试验阶段。尽管将药物推向市场的成本高昂且具有挑战性，但只有不到20%的药物能从第一阶段过渡到最后的批准。近期的研究文献表明，试验方案的设计对试验表现有着显著影响。我们研究了临床试验结果预测（CTOP），旨在通过利用试验设计文件自动预测不同阶段的转换。我们提出了首个基于大型语言模型（LLM）的CTOP模型——CTP-LLM。我们还引入了一个名为PhaseTransition（PT）的数据集，该数据集根据试验在监管过程中的进展进行标记，并作为CTOP评估的标准基准。  我们的精细调参GPT-3.5为基础的模型（CTP-LLM）能够通过分析原始协议文本来预测临床试验阶段的转换，无需依赖人类选择的特征。CTP-LLM在所有阶段的预测中达到了67%的准确率，在预测从第三阶段到最终批准的转换时，准确率更达到了75%。我们的实验结果强调了LLM驱动应用在预测临床试验结果和评估试验设计方面的潜力。|
|**2024-08-20**|**Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models**|Yuyan Chen et.al.|[2408.10947](http://arxiv.org/abs/2408.10947)|null|教师在传授知识和引导学习者方面发挥着重要作用，大型语言模型（LLMs）作为潜在教育者的角色正在成为一个重要研究领域。认识到LLMs生成教育内容的能力可以推动自动化和个性化学习的进展。虽然LLMs在理解力和解决问题能力方面的测试已经进行，但它们在教学方面的潜力仍鲜为人知。在教学中，提问是一项关键技能，能够指导学生分析、评估并综合核心概念和原理。因此，我们的研究引入了一个基准来评估教育中LLMs的提问能力，通过评估它们生成的教育问题，利用安德森和克拉夫霍夫的分类法覆盖一般、单学科和跨学科领域。我们从将LLMs视为学习者转向将其视为教育者，通过评估它们生成问题的能力来评估它们的教学能力。我们应用了四个指标，包括相关性、覆盖率、代表性以及一致性，来评估LLMs输出的教育质量。我们的结果表明，GPT-4在教授一般、人文学科和科学课程方面显示出显著潜力；Claude2似乎更适合担任跨学科教师。此外，自动评分与人类观点一致。|
|**2024-08-20**|**Large Language Model Driven Recommendation**|Anton Korikov et.al.|[2408.10946](http://arxiv.org/abs/2408.10946)|null|### 摘要  本文探讨了利用语言模型（LLM）构建个性化推荐系统的新机遇。在之前的章节中，我们关注的是基于标准化、非言语用户反馈的推荐系统，如购买、观看和点击等行为。然而，随着LLM能力的增强，它们能够进行通用自然语言推理，这为使用自然语言交互来构建高度个性化的推荐系统开辟了新途径。  本章首先通过分类的方式介绍关键的数据源，涵盖商品描述、用户与系统的交互以及用户档案。接着，详细讨论了基于LLM的推荐技术，包括调优和未调优情况下的编码器仅使用和自回归推荐方法。然后，转向多模块推荐架构，其中LLM与其他组件如检索器和推荐系统在多阶段管道中协作。最后，介绍了对话式推荐系统（CRS），在这些系统中，LLM促进多轮对话，每一轮不仅提供推荐，还提供了与用户的互动，用于偏好提取、批评和问答。  ### 翻译  本文探讨了语言模型（LLM）在构建个性化推荐系统方面的新型应用。此前章节主要关注基于标准、非言语用户反馈的推荐系统，例如购买、浏览和点击等行为。然而，随着LLM能力的提升，它们具备了通用自然语言推理的能力，从而打开了使用自然语言交互构建高度定制化推荐系统的可能性。  本章首先通过分类方式概述了关键数据源，包括商品描述、用户与系统交互以及用户档案。随后，深入探讨了基于LLM的推荐技术，涵盖了编码器仅使用和自回归推荐方法，无论是在调优还是未调优状态下。接着，讨论了多模块推荐架构，其中LLM与其他组件如检索器和推荐系统在多阶段流程中协同工作。最后，介绍了对话式推荐系统（CRS），在这些系统中，LLM支持多轮对话，每一轮不仅用于生成推荐，还能与用户进行互动，进行偏好收集、评价和问答。|
|**2024-08-19**|**Demystifying the Communication Characteristics for Distributed Transformer Models**|Quentin Anthony et.al.|[2408.10197](http://arxiv.org/abs/2408.10197)|null|深度学习（DL）模型基于变换器架构在大型语言模型（LLMs）、视觉变换器、音频生成和时间序列预测等众多DL应用领域实现了革命性进展。这一系列进步很大程度上得益于分布式训练，然而分布式通信仍然是影响训练进度的一个重大瓶颈。本文旨在探讨变换器模型的通信行为，即在使用多节点/多GPU DL训练时，不同并行方案如何在变换器背景下进行数据通信。我们以GPT为基础的语言模型作为变换器架构案例研究的主要对象，由于其广泛的应用而被选中。通过我们的通信日志验证了所获得的实验结果，并使用分析模型对这些结果进行了确认。  总体而言，我们的分析揭示了进一步优化小消息点到点通信的必要性、序列长度、每GPU吞吐量、模型大小以及所用优化之间的相关性，以及在框架和高性能计算中间件设计与优化方面可能需要引导的进一步优化方向。|
|**2024-08-19**|**SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models**|Anke Tang et.al.|[2408.10174](http://arxiv.org/abs/2408.10174)|**[link](https://github.com/tanganke/fusion_bench)**|**深度模型在大规模数据集上的训练日益变得成本高昂，这促使人们广泛采用深度模型融合技术，以利用现有模型的知识。从简单的权重平均到更复杂的AdaMerging等方法，模型融合能够有效提升模型性能，并加速新模型的开发。然而，个体模型参数间的相互干扰以及融合过程的可解释性不足仍然是挑战。现有方法往往试图通过评估参数属性（如大小或符号）或进行参数修剪来解决参数干扰问题。本研究首先从线性层微调的角度出发，通过子空间分析明确地定义了参数干扰作为优化问题，以揭示这一主题。随后，我们引入了一种名为零样本稀疏混合低秩专家（SMILE）构造的创新模型融合方法，该方法允许在无需额外数据或进一步训练的情况下，将源模型升级为混合专家模型（MoE）。我们的方法基于以下观察：微调主要保留了预训练的重要部分，但使用较少重要或未使用的区域来适应新任务。此外，在原始参数空间中固有的参数干扰问题，可以通过扩展维度来管理。我们在多种场景下进行了广泛的实验，包括图像分类和文本泛化任务，使用全量微调和LoRA微调，并将我们的方法应用于大型语言模型（CLIP模型、Flan-T5模型和Mistral-7B模型），突出了SMILE的适应性和可扩展性。代码已开源于https://github.com/tanganke/fusion_bench**|
|**2024-08-19**|**Customizing Language Models with Instance-wise LoRA for Sequential Recommendation**|Xiaoyu Kong et.al.|[2408.10159](http://arxiv.org/abs/2408.10159)|**[link](https://github.com/akalikong/ilora)**|基于大型语言模型（LLM）在知识理解和推理方面的优势，近期的研究通过语言生成范式将LLM应用于序列推荐系统中。这些方法将用户行为序列转换为LLM微调的提示，利用LoRA模块来细化推荐。然而，在不同用户行为之间进行统一应用时，LoRA有时无法捕捉到个体差异性，导致性能不佳以及在不同行为序列间的负迁移。为了应对这些挑战，我们提出了一种基于实例的LoRA（iLoRA），它结合了LoRA与混合专家（MoE）框架。iLoRA创建了一个多样化的专家集合，每个专家都能够捕获特定的用户偏好方面，并引入了一个由历史交互序列引导的门控函数。该门控函数处理历史交互序列以生成增强表示，从而指导门控网络输出定制的专家参与权重。这种定制化的方法可以减少负迁移并动态适应多样的行为模式。在三个基准数据集上的广泛实验显示了iLoRA的有效性，证明了其在捕捉用户特定偏好和提高推荐准确度方面的优越性能。|
|**2024-08-19**|**Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models**|Amey Hengle et.al.|[2408.10151](http://arxiv.org/abs/2408.10151)|**[link](https://github.com/AmeyHengle/multilingual-needle-in-a-haystack)**|在近期的大型语言模型（LLM）展示了在多种语言中响应查询的能力之后，它们处理长多语言上下文的能力尚未得到探索。因此，在多语言背景下评估LLM的长期上下文能力至关重要，特别是在信息检索的背景下。为此，我们引入了多语言针在草堆中的测试（MultiLingual Needle-in-a-Haystack，简称MLNeedle），旨在评估模型从多语言干扰文本集合（草堆）中检索相关信息（针）的能力。这一测试扩展了多语言问答任务，涵盖了单语言和跨语言检索。我们对当前的四大先进LLM进行了MLNeedle测试。我们的发现显示，模型性能在不同语言和针的位置上存在显著差异。具体而言，我们观察到当针位于英语语系之外的语言中以及输入上下文的中间位置时，模型的性能最低。此外，尽管某些模型声称具有高达8k个令牌的上下文大小，但在上下文长度增加时，它们都没有表现出满意的跨语言检索性能。我们的分析提供了关于LLM在多语言背景下处理长上下文的关键见解，以指导未来的评估方法。据我们所知，这是首次研究LLM在多语言背景下的长上下文行为。|
|**2024-08-19**|**In-Context Learning with Representations: Contextual Generalization of Trained Transformers**|Tong Yang et.al.|[2408.10147](http://arxiv.org/abs/2408.10147)|null|本文通过非线性回归任务的视角来探讨Transformer在梯度下降过程中的训练动态。在此类任务中，我们可以通过学习每个任务的模板函数实现上下文泛化，所有模板函数都位于包含 $m$ 个基函数的线性空间内。我们对单层多头Transformer进行了分析，以在部分标记提示下预测未标记输入的上下文内预测能力，其中标签包含高斯噪声，每个提示中的示例数量不足以确定模板。  在温和假设下，我们证明了单层多头Transformer的训练损失会线性收敛至全局最小值。此外，Transformer有效地学习了在基函数上进行岭回归的方法。据我们所知，这是首次通过理论证明展示了当提示仅包含少量查询-答案对时，Transformer能够学习上下文信息（即模板）以对未见过的示例和任务进行泛化。|
|**2024-08-19**|**Instruction Finetuning for Leaderboard Generation from Empirical AI Research**|Salomon Kabongo et.al.|[2408.10141](http://arxiv.org/abs/2408.10141)|null|本文展示了预训练大型语言模型（LLMs）指令微调在自动化生成AI研究排行榜中的应用，从文章中提取（任务，数据集，指标，分数）四元组。该研究旨在通过从传统的、基于社区的手动整理转变为利用自动化、生成式LLM方法来简化AI研究进展的传播，从而超越依赖于特定分类的自然语言推理（NLI）模型的传统方式。通过利用FLAN-T5模型，本研究增强了LLMs在信息抽取方面的适应性和可靠性，并提供了一种新颖的方法来构建结构化知识表示。|
|**2024-08-19**|**Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**|Tianyu Zhang et.al.|[2408.10124](http://arxiv.org/abs/2408.10124)|**[link](https://github.com/zhangtia16/molgraph-lardo)**|**分子属性预测是药物发现的基础。近年来，预训练深度学习模型在这一领域得到了广泛应用，并取得了显著成果。一些将生物化学领域的先验知识融入预训练框架的方法表现出了令人印象深刻的性能。然而，这些方法高度依赖于生物化学专家，获取和总结大量的领域知识文献既耗时又昂贵。大型语言模型（LLMs）在理解并高效提供通用知识方面表现出卓越的能力。然而，它们偶尔会出现幻觉，并缺乏生成特定领域知识的精确性。与此相反，领域特定小型模型（DSMs）拥有丰富的领域知识，能够准确计算与分子领域相关的指标。然而，由于它们的模型大小有限且功能单一，它们缺乏全面的表示学习所需的广泛知识。为了在分子属性预测中充分利用两种方法的优势，我们提出了一种名为MolGraph-LarDo的新型分子图表示学习框架，该框架融合了大型语言模型和领域特定小型模型。技术上，我们设计了一个两阶段提示策略，其中引入DSMs来校准LLMs提供的知识，从而增强领域特定信息的准确性，使LLMs能够为分子样本生成更精确的文字描述。随后，我们采用多模态对齐方法协调包括分子图及其对应描述文本在内的各种模态，以指导分子表示的预训练。广泛的实验结果证明了所提出方法的有效性。**|
|**2024-08-20**|**PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities**|Yuanjian Xu et.al.|[2408.10111](http://arxiv.org/abs/2408.10111)|null|金融时间序列建模对于理解与预测市场行为至关重要，但面临着非线性、非平稳性和高噪声等挑战。传统的模型在捕捉复杂模式时受到这些因素的影响，同时受到计算资源和模型容量的限制。受自然语言处理领域大型语言模型成功启发，我们提出了一种名为 $\textbf{PLUTUS}$的模型，其全称为$\textbf{P}$re-trained $\textbf{L}$arge $\textbf{U}$nified $\textbf{T}$ransformer-based模型，用于揭示金融时间序列中的规律。$\textbf{PLUTUS}$通过结合可逆嵌入模块、对比学习和自动编码技术，创建了原始数据与块嵌入之间的近似一一映射。  TimeFormer，一个基于注意力的架构，构成了$\textbf{PLUTUS}$的核心，有效地处理了高噪声时间序列数据。我们引入了一种新颖的注意力机制，以跨变量和时间维度捕获特征。$\textbf{PLUTUS}$在规模空前的1000亿个观察值的数据集上进行预训练，旨在适应嘈杂的金融市场环境。据我们所知，$\textbf{PLUTUS}$ 是首个开源的、大规模的预训练金融时间序列模型，参数超过十亿个。它在各种任务上实现了最先进的性能，展示了强大的迁移性，并为金融领域建立了一个坚实的基础模型。我们的研究提供了预训练金融时间序列数据的技术指导，确立了该领域的全新标准。|
|**2024-08-19**|**ARMADA: Attribute-Based Multimodal Data Augmentation**|Xiaomeng Jin et.al.|[2408.10086](http://arxiv.org/abs/2408.10086)|null|在多模态语言模型（MLMs）中，手动标注高质量的图像-文本配对数据以进行微调和对齐的成本非常高。尽管现有的多模态数据增强框架提出了增强图像-文本配对的方法，但它们要么在文本和图像之间存在语义不一致，要么生成不切实际的图像，导致与现实世界示例的知识差距。为了应对这些问题，我们提出了一种名为Attribute-based Multimodal Data Augmentation (ARMADA)的新型多模态数据增强方法，通过知识引导的提及实体视觉属性的修改来增强数据。具体来说，我们从原始文本数据中提取实体及其视觉属性，然后在知识库（KBs）和大型语言模型（LLMs）的指导下搜索视觉属性的替代值。接着，我们利用图像编辑模型根据提取的属性编辑图像。ARMADA是一个新颖的多模态数据生成框架：(i) 从符号知识库中提取知识关联的属性，实现语义一致且具有区别的图像-文本对生成；(ii) 利用知识库层次结构中的同类别实体生成视觉上相似但不同类别的图像；(iii) 使用LLMs的常识知识调节辅助视觉属性，如背景，以更全面地表示原始实体。我们的实验证明，在四个下游任务上，我们的框架能够产生高质量的数据并提高模型性能。这也强调了利用外部知识代理以增强可解释性和现实世界相关性的必要性。|
|**2024-08-19**|**FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant**|Zhengchao Huang et.al.|[2408.10072](http://arxiv.org/abs/2408.10072)|null|快速发展的深度伪造技术引发了公众的广泛关注，尤其是在对公共信息安全构成严重威胁的面部伪造方面。然而，未知和多样的伪造技术、多变的面部特征以及复杂的环境因素给面部伪造分析带来了巨大挑战。现有数据集在描述这些方面时存在不足，使得仅通过视觉信息难以在各种干扰因素中区分真实与伪造的面部。此外，现有的方法未能提供用户友好且可解释的结果，复杂化了模型决策过程的理解。  为解决这些挑战，我们提出了一项新颖的“开放世界面部伪造分析问答”（OW-FFA-VQA）任务及其相应的基准。为了应对这一任务，我们首先建立了一个包含真实和伪造面部图像的多样集合，并配有关键描述和可靠伪造推理的数据集。基于此数据集，我们引入了“面部伪造分析助手”（FFAA），它由一个微调的多模态大型语言模型（MLLM）和一个多答案智能决策系统（MIDS）组成。通过结合假设性提示与MIDS，有效消除了模糊分类边界的影响力，增强了模型的鲁棒性。广泛的实验结果表明，我们的方法不仅提供了用户友好的可解释结果，而且在准确性与鲁棒性方面显著超越了以往的方法。|
|**2024-08-16**|**PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars**|Sumanth Prabhu et.al.|[2408.08869](http://arxiv.org/abs/2408.08869)|null|自一致性等依赖于准确答案提取过程的自我集丛技术已经在大型语言模型（LLM）的准确性上取得了显著的提升。然而，这些技术在聚合多个输出时需要较高的推理成本，相较于贪心解码而言，生成相对较多的输出令牌。研究显示，自一致性方法产生的自由文本输出可以通过LLM可靠地聚合以产生最终输出。此外，最近的LLM推理进展表明，在提示中使用多样化的示例能够诱导LLM输出的多样性。这些已经证明的技术可以很容易地扩展到自我集丛方法中，以实现文本生成的整体性能改进。  本文介绍了一种名为PEDAL（基于示例多样性的LLM聚合）的混合自我集丛方法。该方法结合了基于多样示例提示和LLM聚合的优势，以实现性能的提升。在公开的SVAMP和ARC数据集上进行的实验揭示，与基于贪心解码的策略相比，PEDAL能够在较低的推理成本下获得更好的准确性，与基于自一致性的方法相比具有优势。|
|**2024-08-16**|**Visual Agents as Fast and Slow Thinkers**|Guangyan Sun et.al.|[2408.08862](http://arxiv.org/abs/2408.08862)|**[link](https://github.com/guangyans/sys2-llava)**|实现与人类相当的智能需要对认知上的第一系统和第二系统思维进行细化。当前的人工智能，尤其是基于大型语言模型的AI，虽然表现出类似人类的特点，但并未达到真正的认知水平。在从结构化基准向真实世界场景过渡的过程中，视觉代理面临挑战，往往导致回答既不准确又过于自信。为了解决这一问题，我们引入了FaST（快速与缓慢思考），它将快速与缓慢思考机制融入到视觉代理中。FaST采用切换适配器动态选择系统1/2模式，根据任务的复杂性调整解决问题的方法。面对不确定和未见过的对象时，通过调整模型的信心并整合新的上下文数据，它能够灵活应对。  我们提倡一个灵活的系统、层次化的推理能力和透明的决策流程，这些都使得FaST能够模仿人类在视觉智能中的认知过程。实验结果表明，FaST在视觉问答(VQA^{v2})任务上达到了80.8%的准确率，在推理分割(ReasonSeg)任务上获得了48.7%的GIoU分数，这充分展示了FaST的优越性能。广泛的测试验证了FaST核心组件的有效性和稳健性，显示了其在推动人工智能系统中认知视觉代理的发展方面的潜力。|
|**2024-08-16**|**ECG-Chat: A Large ECG-Language Model for Cardiac Disease Diagnosis**|Yubao Zhao et.al.|[2408.08849](http://arxiv.org/abs/2408.08849)|**[link](https://github.com/YubaoZhao/ECG-Chat)**|在医疗辅助领域，多模态大型语言模型（MLLMs）的成功展现出巨大潜力，使得患者能够利用生理信号数据进行对话。然而，通用的MLLMs在心脏病诊断方面表现不佳，尤其是在ECG数据解析与长文本医学报告生成的整合上，主要原因是ECG数据解析的复杂性以及文本与ECG信号模态之间的差距。此外，模型在长文本生成时往往存在严重的稳定性问题，这主要是由于缺乏与用户查询紧密相关的精确知识。  为了解决这些问题，我们提出了一种名为ECG-Chat的多任务MLLM，专注于ECG医学报告生成，并提供基于心脏病学知识的跨模态对话能力。我们采用了对比学习方法，将ECG波形数据与文本报告结合，以精细的方式对齐ECG特征与报告内容。这种方法还产生了一个在零样本报告检索任务中表现出色的ECG编码器。此外，我们通过扩展现有数据集，构建了包含19K个ECG诊断数据集和25K个多轮对话数据集用于训练和微调ECG-Chat，从而提供专业的诊断和对话能力。此外，ECG-Chat可以通过自动化LaTeX生成管道来生成全面的ECG分析报告。我们为ECG报告生成任务建立了基准，并在多个基线上测试了我们的模型。ECG-Chat在分类、检索、多模态对话和医学报告生成任务中均取得了最佳性能。我们的报告模板设计也得到了医疗专业人员的一致认可。|
|**2024-08-16**|**PsychoLex: Unveiling the Psychological Mind of Large Language Models**|Mohammad Amin Abbasi et.al.|[2408.08848](http://arxiv.org/abs/2408.08848)|null|这篇论文探讨了心理学与人工智能的交汇点，通过开发和评估专用于心理任务的大型语言模型（LLMs）。我们引入了PsychoLex套件，旨在增强LLMs在波斯语和英语中的心理任务处理能力。主要贡献包括PsychoLexQA数据集，用于教学内容的创建，以及PsychoLexEval数据集，用于对LLMs在复杂心理情景下的严格评估。此外，我们还介绍了PsychoLexLLaMA模型，该模型特别优化以适用于心理应用，其性能明显优于通用模型。研究结果强调了定制LLMs在推进心理研究和应用方面的潜力，同时也指出了进一步改进的领域。这项研究为将LLMs融入特定的心理学领域奠定了基础，对未来AI驱动的心理实践的发展具有重要意义。|
|**2024-08-16**|**FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats**|Xuanliang Zhang et.al.|[2408.08841](http://arxiv.org/abs/2408.08841)|**[link](https://github.com/zhxlia/FLEXTAF)**|**## 上文背景 表格推理任务旨在根据给定的表格回答问题。目前，使用大型语言模型（LLMs）是表格推理的主要方法。现有的大多数方法都采用固定的表格格式来表示表格，这可能限制了性能。鉴于每个实例需要不同的能力，而模型具有不同的能力，我们断言不同实例和模型适用于不同的表格格式。通过实验结果的定量分析，我们证明了这一点：使用不同的表格格式，不同实例和模型可以获得不同的性能。在此基础上，我们提出了一种增强表格推理性能的方法FLEXTAF-Single和FLEXTAF-Vote，通过使用灵活的表格格式。具体来说，(i) FLEXTAF-Single训练一个分类器，基于实例和LLM预测最适合的表格格式。(ii) FLEXTAF-Vote在不同格式之间集成结果。我们在WikiTableQuestions和TabFact上的实验结果显示了显著的改进，与使用固定表格格式并结合贪婪解码和自我一致性解码达到的最佳性能相比，平均提高了2.3%和4.8%，从而验证了我们方法的有效性。**|
|**2024-08-16**|**Artificial Intelligence and Strategic Decision-Making: Evidence from Entrepreneurs and Investors**|Felipe A. Csaszar et.al.|[2408.08811](http://arxiv.org/abs/2408.08811)|null|本文探讨了人工智能（AI）如何影响企业战略决策过程。我们通过实例展示了AI如何增强现有战略决策工具，并提供了来自领先加速器计划和创业竞赛的实证证据，证明当前的大规模语言模型（LLMs）在生成和评估策略方面的能力与企业家和投资者相当。接着，我们分析了战略决策背后的关键认知过程——搜索、表示和聚合，并提出AI有可能提升战略分析的速度、质量和规模，同时还能启用如虚拟战略模拟等新方法。然而，AI对企业发展的影响最终取决于竞争动态以及AI能力的发展。我们提出了一个框架，将AI在战略决策中的应用与企业结果联系起来，并讨论了AI如何重塑竞争优势的来源。最后，我们考虑了AI如何既支持又挑战基于理论的战略观的核心原则。整体而言，我们的工作描绘了一个AI与战略领域正在形成的研究前沿。|
|**2024-08-16**|**Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge**|Ravi Raju et.al.|[2408.08808](http://arxiv.org/abs/2408.08808)|null|大型语言模型（LLM）在机器学习领域带来了革命性变化，然而现有的基准测试往往难以全面捕捉这些模型在实际应用中的多样行为。一个基准测试的价值在于它能否清晰区分不同能力级别的模型（可分性）以及与人类偏好的紧密匹配度。当前的框架如Alpaca-Eval 2.0 LC \cite{dubois2024lengthcontrolledalpacaevalsimpleway} 和Arena-Hard v0.1 \cite{li2024crowdsourced}主要关注通用查询，并且缺乏跨法律、医学等领域的多样性。本文通过引入一种新颖的数据管道，来定制一系列多元化的、针对LLM-as-a-Judge框架的领域特定评估集，以解决这些问题。我们的方法结合了人工筛选、半监督学习生成聚类以及分层抽样，确保在广泛领域和语言中都有均衡的代表性。产生的评估集包括1573个样本，分布在14个类别中，显示出高可分性（84%）和对前十大模型的性能差异，同时与Chatbot Arena的共识度（84%）和Spearman相关系数（0.915）也表现出良好的一致性。与AlpacaEval 2.0 LC的共识度相比，这一值高出9%，与Arena Hard相比则高出20%，而与Spearman系数相比则是下一个最佳基准的0.7倍，这表明我们在基准测试的有效性方面取得了显著进步。我们还提供了一个开源的评估工具，允许用户自定义类别进行精细分析，从而为实践者提供有价值的洞察。这项工作对增强LLM评估方法的透明度、多样性和有效性做出了贡献。|
|**2024-08-16**|**EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**|Chenwei Wan et.al.|[2408.08782](http://arxiv.org/abs/2408.08782)|**[link](https://github.com/cw-wan/EmoDynamiX-v2)**|**设计能够提供慰藉和建议的具有情感智能的对话系统，以帮助那些经历压力的人们，是一个极具吸引力的研究领域。过去的研究工作着重于构建模块化对话系统，并将其社会情感策略预测视为辅助任务，通过定制解码器生成条件化的响应。最近，在大型语言模型（LLM）方面的发展使得无需明确的社会情感策略预测步骤的端到端对话代理变得流行起来。尽管它们在语言生成方面表现出色，但最近的研究表明，LLM固有的偏好偏见，倾向于某些社会情感策略，阻碍了提供高质量情感支持的能力。  为了应对这一挑战，我们提出了一种新的方法：将策略预测与语言生成分离，并引入了一个名为EmoDynamiX的新型对话策略预测器。该预测器利用异构图来建模用户情绪与系统策略之间的对话动态。此外，我们利用了对话中情感识别（ERC）任务，并设计了一个灵活的混合情绪模块，以捕捉用户的细微情感状态。在两个ESC数据集上的实验结果表明，EmoDynamiX显著超越了先前最先进的方法。  请注意，上述翻译已经移除了","字符。**|
|**2024-08-16**|**Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions**|Chenming Tang et.al.|[2408.08780](http://arxiv.org/abs/2408.08780)|null|通过利用上下文学习（ICL），大型语言模型在各种任务上取得了令人印象深刻的性能。然而，在ICL过程中描述性指令的作用仍然有待探索。本研究提出了一种集成提示框架，用于描述多个上下文示例的选择标准，并在六个翻译方向的机器翻译（MT）任务上的初步实验表明，这种框架能够提升ICL性能。出乎意料的是，LLM可能并不关心描述的具体内容，性能提升主要源于集成格式，即使使用随机描述名词，该框架也能带来改进。我们进一步在常识、数学、逻辑推理和幻觉任务上应用了这种新的集成提示，并使用三种LLM取得了有希望的结果，这再次表明设计适当的提示格式比专注于特定描述更为有效和高效。在论文发表后，我们的代码将公开提供。|
|**2024-08-16**|**DAC: Decomposed Automation Correction for Text-to-SQL**|Dingzirui Wang et.al.|[2408.08779](http://arxiv.org/abs/2408.08779)|**[link](https://github.com/zirui-HIT/DAC)**|**文本到SQL是一个重要的任务，它通过自动生成SQL查询帮助人们从数据库中获取信息。考虑到出色的性能，基于大型语言模型（LLM）的方法成为了文本到SQL的主流方式。在这类方法中，自动修正成为一种有效手段，能够通过纠正生成结果中的错误来进一步提升性能。现有修正方法要求LLM直接对生成的SQL进行修正，而先前的研究表明，LLM并不知道如何检测错误，导致了较差的性能。因此，在这篇论文中，我们提出采用分解式修正来增强文本到SQL的性能。首先，我们证明了分解式修正优于直接修正，因为与SQL相比，通过结果分解子任务来检测和修复错误更为容易。基于这一分析，我们引入了分解自动化修正（DAC），该方法通过将文本到SQL分解为实体链接和骨架解析两个子任务来修正SQL。DAC首先生成与问题对应的实体和骨架，然后比较初始SQL与生成的实体和骨架之间的差异作为修正反馈。实验结果显示，与基线方法相比，我们的方法在Spider、Bird和KaggleDBQA上的平均性能提高了3.7%，证明了DAC的有效性。**|
|**2024-08-15**|**Can Large Language Models Understand Symbolic Graphics Programs?**|Zeju Qiu et.al.|[2408.08313](http://arxiv.org/abs/2408.08313)|null|Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training. We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data. LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs? Unlike conventional programs, symbolic graphics programs can be translated to graphics content. Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content. This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment. To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content. We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs. This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts. We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs. We find that this task distinguishes existing LLMs and models considered good at reasoning perform better. Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability. Specifically, we query GPT4-o with questions and images generated by symbolic programs. Such data are then used to finetune an LLM. We also find that SIT data can improve the general instruction following ability of LLMs.|
|**2024-08-15**|**ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws**|Ruihang Li et.al.|[2408.08310](http://arxiv.org/abs/2408.08310)|null|High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.|
|**2024-08-15**|**Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors**|Usman Syed et.al.|[2408.08302](http://arxiv.org/abs/2408.08302)|null|In this paper, we explore the capabilities of state-of-the-art large language models (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level transportation engineering problems. We introduce TransportBench, a benchmark dataset that includes a sample of transportation engineering problems on a wide range of subjects in the context of planning, design, management, and control of transportation systems. This dataset is used by human experts to evaluate the capabilities of various commercial and open-sourced LLMs, especially their accuracy, consistency, and reasoning behaviors, in solving transportation engineering problems. Our comprehensive analysis uncovers the unique strengths and limitations of each LLM, e.g. our analysis shows the impressive accuracy and some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving TransportBench problems. Our study marks a thrilling first step toward harnessing artificial general intelligence for complex transportation challenges.|
|**2024-08-15**|**HELP: Hierarchical Embeddings-based Log Parsing**|Andy Xu et.al.|[2408.08300](http://arxiv.org/abs/2408.08300)|null|Logs are a first-hand source of information for software maintenance and failure diagnosis. Log parsing, which converts semi-structured log messages into structured templates, is a prerequisite for automated log analysis tasks such as anomaly detection, troubleshooting, and root cause analysis. However, existing log parsers fail in real-world systems for three main reasons. First, traditional heuristics-based parsers require handcrafted features and domain knowledge, which are difficult to generalize at scale. Second, existing large language model-based parsers rely on periodic offline processing, limiting their effectiveness in real-time use cases. Third, existing online parsing algorithms are susceptible to log drift, where slight log changes create false positives that drown out real anomalies. To address these challenges, we propose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first online semantic-based parser to leverage LLMs for performant and cost-effective log parsing. We achieve this through a novel hierarchical embeddings module, which fine-tunes a text embedding model to cluster logs before parsing, reducing querying costs by multiple orders of magnitude. To combat log drift, we also develop an iterative rebalancing module, which periodically updates existing log groupings. We evaluate HELP extensively on 14 public large-scale datasets, showing that HELP achieves significantly higher F1-weighted grouping and parsing accuracy than current state-of-the-art online log parsers. We also implement HELP into Iudex's production observability platform, confirming HELP's practicality in a production environment. Our results show that HELP is effective and efficient for high-throughput real-world log parsing.|
|**2024-08-15**|**The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community**|Shachar Don-Yehiya et.al.|[2408.08291](http://arxiv.org/abs/2408.08291)|null|Human-model conversations provide a window into users' real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research. While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind.   We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations. Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms. The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user's local storage. We release the plugin conversations as part of the ShareLM collection, and call for more community effort in the field of open human-model data.   The code, plugin, and data are available.|
|**2024-08-15**|**Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model**|Jin Wang et.al.|[2408.08282](http://arxiv.org/abs/2408.08282)|null|Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot 'action' and 'sensing' behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning.|
|**2024-08-15**|**BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts**|Qizhen Zhang et.al.|[2408.08274](http://arxiv.org/abs/2408.08274)|null|The Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance over dense models. However, training MoEs from scratch in a large-scale regime is prohibitively expensive. Existing methods mitigate this by pre-training multiple dense expert models independently and using them to initialize an MoE. This is done by using experts' feed-forward network (FFN) to initialize the MoE's experts while merging other parameters. However, this method limits the reuse of dense model parameters to only the FFN layers, thereby constraining the advantages when "upcycling" these models into MoEs. We propose BAM (Branch-Attend-Mix), a simple yet effective method that addresses this shortcoming. BAM makes full use of specialized dense models by not only using their FFN to initialize the MoE layers but also leveraging experts' attention parameters fully by initializing them into a soft-variant of Mixture of Attention (MoA) layers. We explore two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance; and 2) sharing key and value parameters across all experts to facilitate for better inference efficiency. To further improve efficiency, we adopt a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently. Our experiments on seed models ranging from 590 million to 2 billion parameters demonstrate that BAM surpasses baselines in both perplexity and downstream task performance, within the same computational and data constraints.|
|**2024-08-15**|**DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System**|Xihong Yang et.al.|[2408.08231](http://arxiv.org/abs/2408.08231)|null|Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems. Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment. In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem. Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved. Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization. Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer. Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks. Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.|
|**2024-08-15**|**RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science**|David Farr et.al.|[2408.08217](http://arxiv.org/abs/2408.08217)|null|Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data. However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance. Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases.|
|**2024-08-15**|**Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models**|Javier González et.al.|[2408.08210](http://arxiv.org/abs/2408.08210)|null|Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning. Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS). This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures. By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS. Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples.|
|**2024-08-14**|**The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models**|Karime Maamari et.al.|[2408.07702](http://arxiv.org/abs/2408.07702)|null|Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. In this work, we revisit the need for schema linking when using the latest generation of large language models (LLMs). We find empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. Our approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.|
|**2024-08-15**|**Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities**|Enneng Yang et.al.|[2408.07666](http://arxiv.org/abs/2408.07666)|**[link](https://github.com/ennengyang/awesome-model-merging-methods-theories-applications)**|**Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.**|
|**2024-08-14**|**Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models**|Yi-Cheng Lin et.al.|[2408.07665](http://arxiv.org/abs/2408.07665)|**[link](https://github.com/dlion168/spoken_stereoset)**|Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.|
|**2024-08-14**|**Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions**|Quan Liu et.al.|[2408.07663](http://arxiv.org/abs/2408.07663)|**[link](https://github.com/gigabaozi/aed)**|**Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.**|
|**2024-08-14**|**WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**|Weijian Xie et.al.|[2408.07611](http://arxiv.org/abs/2408.07611)|null|Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.|
|**2024-08-14**|**Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey**|Hamza Kheddar et.al.|[2408.07583](http://arxiv.org/abs/2408.07583)|null|With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.|
|**2024-08-15**|**MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark**|Minxuan Zhou et.al.|[2408.07543](http://arxiv.org/abs/2408.07543)|**[link](https://github.com/PKU-Baichuan-MLSystemLab/MathScape)**|With the development of Multimodal Large Language Models (MLLMs), the evaluation of multimodal models in the context of mathematical problems has become a valuable research field. Multimodal visual-textual mathematical reasoning serves as a critical indicator for evaluating the comprehension and complex multi-step quantitative reasoning abilities of MLLMs. However, previous multimodal math benchmarks have not sufficiently integrated visual and textual information. To address this gap, we proposed MathScape, a new benchmark that emphasizes the understanding and application of combined visual and textual information. MathScape is designed to evaluate photo-based math problem scenarios, assessing the theoretical understanding and application ability of MLLMs through a categorical hierarchical approach. We conduct a multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark is challenging even for the most sophisticated models. By analyzing the evaluation results, we identify the limitations of MLLMs, offering valuable insights for enhancing model performance.|
|**2024-08-15**|**Usefulness of data flow diagrams and large language models for security threat validation: a registered report**|Winnie Bahati Mbaka et.al.|[2408.07537](http://arxiv.org/abs/2408.07537)|null|The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. Threat analysis and risk assessment are used to identify security threats for new or refactored systems. Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis. Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material. In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).|
|**2024-08-14**|**Development of a Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments**|Seungjun Han et.al.|[2408.07531](http://arxiv.org/abs/2408.07531)|null|Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.|
|**2024-08-14**|**Large Language Models Know What Makes Exemplary Contexts**|Quanyu Long et.al.|[2408.07505](http://arxiv.org/abs/2408.07505)|null|In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters. This paper presents a unified framework for LLMs that allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning. Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards from LLM's own preference. Experimental results validate the proposed method's effectiveness in enhancing ICL performance. Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval.|
|**2024-08-13**|**Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents**|Kexun Zhang et.al.|[2408.07060](http://arxiv.org/abs/2408.07060)|null|大型语言模型（LLM）代理在解决实际世界软件工程（SWE）问题方面展现出巨大的潜力。最先进开源的SWE代理能够在SWE-Bench Lite中解决超过27%的实际GitHub问题。然而，这些复杂的代理框架在表现上存在差异，有的在特定任务中表现出色，在其他任务中则表现不佳。为了充分利用这些代理的多样性，我们提出了一种名为DEI（多元化智能）的框架，该框架利用了它们的独特专长。DEI作为一个位于现有SWE代理框架之上的元模块，管理代理集体以实现增强的问题解决能力。  实验结果显示，由DEI指导的代理委员会能够显著超越单个代理的最佳性能。例如，一组开源的SWE代理，其个体解决率最高为27.3%在SWE-Bench Lite中，通过采用DEI，可以达到34.3%的解决率，实现了25%的改进，并击败了许多闭源解决方案。我们的最佳性能组表现出色，达到了55%的解决率，在SWE-Bench Lite中获得了最高排名。我们的研究结果对合作型人工智能系统的研究领域做出了贡献，展示了它们在解决复杂软件工程挑战方面的潜力。|
|**2024-08-13**|**LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs**|Yushi Bai et.al.|[2408.07055](http://arxiv.org/abs/2408.07055)|**[link](https://github.com/thudm/longwriter)**|**当前的大型语言模型（LLMs）能够处理最多10万字的输入，然而在生成超过2千字的输出时却力不从心。通过控制实验，我们发现模型的有效生成长度本质上受到其在监督微调（SFT）期间所见样本的限制。换句话说，它们的输出限制源于现有SFT数据集中长输出示例的稀缺性。为了解决这个问题，我们引入了AgentWrite，这是一种基于代理的管道，将超长生成任务分解为子任务，从而使现有的LLMs能够生成超过2万字的连贯输出。  借助AgentWrite，我们构建了LongWriter-6k数据集，其中包含了6000个SFT数据，输出长度范围从2千到32千字。通过将此数据集纳入模型训练，我们成功地将现有模型的输出长度扩展至超过1万字，同时保持了输出质量。我们也开发了LongBench-Write，这是一个全面的基准，用于评估超长生成能力。我们的9亿参数模型，在经过DPO进一步改进后，在这一基准上实现了最先进的性能，甚至超过了更大规模的专有模型。  总的来说，我们的工作表明，现有的长上下文LLMs实际上已经具备了更大的输出窗口的能力——你只需要在模型对齐过程中使用带有延长输出的数据即可解锁这一能力。我们的代码和模型可以在：https://github.com/THUDM/LongWriter找到。**|
|**2024-08-13**|**Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models**|Chun Jie Chong et.al.|[2408.07004](http://arxiv.org/abs/2408.07004)|null|基于网络的大型语言模型（LLM）服务已被广泛采用，并已成为我们互联网体验不可或缺的一部分。第三方插件通过提供对现实世界数据和服务的访问，增强了LLM的功能性。然而，与这些服务及其第三方插件相关的隐私后果并未得到充分理解。敏感提示数据在云基LLM提供商和第三方插件中被存储、处理和共享。本文提出了一种名为Casper的提示净化技术，旨在通过检测并从用户输入中删除敏感信息来保护用户隐私，从而在发送给LLM服务之前保护用户隐私。Casper完全作为浏览器扩展运行在用户的设备上，无需对在线LLM服务进行任何更改。Casper的核心是一个三层净化机制，包括规则基于过滤器、机器学习（ML）命名实体识别器和浏览器本地LLM主题标识器。我们使用4000个合成提示集对Casper进行了评估，结果显示，它能够以高准确率（98.5%）有效地过滤出个人可识别信息（PII）和隐私敏感话题（89.9%）。|
|**2024-08-13**|**LLMs can Schedule**|Henrik Abgaryan et.al.|[2408.06993](http://arxiv.org/abs/2408.06993)|**[link](https://github.com/starjob42/datasetjsp)**|**工作车间调度问题(JSSP)在优化生产流程方面仍是一个重大挑战。该问题涉及有效分配任务到有限数量的机器上，以最小化总处理时间或作业延迟等因素。尽管近期人工智能领域的进步已经产生了有前景的解决方案，例如强化学习和图神经网络，但本文探讨了大型语言模型(LLM)在JSSP中的潜力。我们首次引入了一个专门为训练LLM设计的120k数据集，专门针对JSSP。令人惊讶的是，我们的发现表明，基于LLM的调度可以实现与其它神经方法相当的性能。此外，我们提出了一种采样方法，以提高LLM在解决JSSP时的有效性。**|
|**2024-08-13**|**OpenResearcher: Unleashing AI for Accelerated Scientific Research**|Yuxiang Zheng et.al.|[2408.06941](http://arxiv.org/abs/2408.06941)|**[link](https://github.com/gair-nlp/openresearcher)**|**快速发展的科学文献对研究人员在各自领域保持最新进展和探索新领域带来了重大挑战。我们提出了一种创新平台——OpenResearcher，它利用人工智能技术加速研究过程，通过回答研究人员的多种问题来帮助他们。OpenResearcher基于检索增强生成（RAG）构建，结合了大型语言模型（LLMs）与特定领域的最新知识。此外，我们开发了各种工具，使OpenResearcher能够理解研究人员的问题、从科学文献中搜索、筛选检索到的信息、提供准确全面的答案，并自我优化这些答案。OpenResearcher灵活地使用这些工具，在效率与有效性之间找到平衡。结果，OpenResearcher帮助研究人员节省时间，提高他们发现新见解和推动科学研究突破的潜力。演示、视频和代码可在以下链接获取：https://github.com/GAIR-NLP/OpenResearcher。**|
|**2024-08-13**|**Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas**|Louis Kwok et.al.|[2408.06929](http://arxiv.org/abs/2408.06929)|**[link](https://github.com/louiskwoklf/llms-cultural-adaptability)**|大型语言模型（LLM）在多文化环境中的成功取决于它们理解用户不同文化背景的能力。我们通过让LLM模拟代表各种国籍的人类角色进行问卷式心理学实验来衡量这一能力。具体而言，我们使用GPT-3.5对来自15个国家的7,286名参与者阅读并回应具有说服力的新闻文章的反应进行模拟；并将结果与拥有相同人口统计特征的真实参与者数据集进行比较。我们的分析显示，明确指定一个人的居住国可以提高GPT-3.5与他们的反应的一致性。相比之下，使用母语提示引入的变化显著降低了整体一致性，并且某些语言特别影响了性能。这些发现表明，尽管直接提供国籍信息可以增强模型的文化适应性，但使用母语提示并不一定能可靠地提高模拟准确性，反而可能损害模型的有效性。|
|**2024-08-13**|**Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives**|Zhihu Wang et.al.|[2408.06904](http://arxiv.org/abs/2408.06904)|null|随着大规模语言模型（LLM）的持续扩展，它们在性能上的增强往往不足以解决特定领域的任务。系统性地分析这些失败并有效提升其性能仍然是一个重大挑战。本文提出了Re-TASK框架，这是一种新颖的理论模型，从能力、技能、知识的角度重新审视LLM任务，遵循布卢姆分类法和知识空间理论的原则。Re-TASK框架提供了一种系统的方法来深化我们对LLM的理解、评估和提升，特别针对特定领域任务。它探索了LLM的能力、处理的知识以及应用的技能之间的相互作用，阐明了这些元素如何相互关联并影响任务表现。  通过应用Re-TASK框架，我们揭示了许多特定领域任务失败的原因主要归咎于知识不足或技能适应度不够。基于这一洞察，我们提出了一系列结构化的策略来增强LLM，通过有针对性的知识注入和技能适应。具体而言，我们识别与任务相关的关键能力项，并采用精心设计的提示策略来提升任务性能，从而减少大量微调的需求。或者，我们使用能力特定指令对LLM进行微调，进一步验证了框架的有效性。实验结果证实了框架的有效性，展示了显著提高LLM在性能和适用性方面的效果。|
|**2024-08-13**|**Leveraging Language Models for Emotion and Behavior Analysis in Education**|Kaito Tanaka et.al.|[2408.06874](http://arxiv.org/abs/2408.06874)|null|分析学生的情绪和行为对于提升学习效果与个性化教育体验至关重要。传统方法往往依赖于对侵入性的视觉和生理数据收集，这引发了隐私问题并限制了规模性应用。本文提出了一种新颖的方法，利用大型语言模型（LLMs）和提示工程来分析学生的文本数据。我们的策略通过定制的提示引导LLMs检测情感和参与状态，提供一种非侵入性、可扩展的解决方案。我们使用Qwen、ChatGPT、Claude2和GPT-4进行了实验，将我们的方法与基础模型和链式思考（CoT）提示进行了比较。结果表明，我们的方法在准确性和上下文理解方面均显著优于基线模型。这项研究强调了大型语言模型结合提示工程在提供实用有效工具以进行教育情绪和行为分析方面的潜力。|
|**2024-08-13**|**LoRA $^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models**|Jia-Chen Zhang et.al.|[2408.06854](http://arxiv.org/abs/2408.06854)|null|细调大规模语言模型（LLMs）以实现高参数效率并应用于下游任务已成为新的研究方向。低秩适应（LoRA）显著降低了细调时的可训练参数数量。尽管它在性能上表现出色，但在复杂下游任务中，仅在单一尺度上调参可能并非最优策略。  本文提出了一种扩展LoRA的方法，称为LoRA$^2$。首先，通过结合正交投影理论，我们训练了两组在相互正交平面上的LoRA集合。然后，我们改进了重要性评分算法，该算法大约减少了98.5%的参数敏感度计算。通过去除具有较低重要性分数的奇异值，从而提高了对各种下游任务的适应能力。  我们在两个广泛使用的预训练模型上进行了大量实验，以验证LoRA$^2$ 的有效性。结果显示，与全量细调相比，它仅将可训练参数数量减少至0.72%，同时仍能展现出令人印象深刻的性能。即使进一步将参数减少至0.17M，其结果也与基线模型（参数量多出8倍）相当。  我们的代码已在此处提供：<https://anonymous.4open.science/r/LoRA-2-5B4C>|
|**2024-08-13**|**Causal Agent based on Large Language Model**|Kairong Han et.al.|[2408.06849](http://arxiv.org/abs/2408.06849)|**[link](https://github.com/kairong-han/causal_agent)**|**大型语言模型（LLM）在各个领域取得了显著成功。然而，因果问题的内在复杂性和因果理论使得用自然语言准确描述它们变得困难，这阻碍了LLM有效地理解和使用它们的能力。用自然语言传达因果方法并不容易，这限制了LLM应用它们的准确性。此外，因果数据集通常以表格形式存在，而LLM在处理自然语言数据方面表现出色，这种结构上的不匹配妨碍了对表格数据的有效推理。缺乏因果推理能力限制了LLM的发展。  为了解决这些问题，我们为LLM配备了因果工具，并将其置于一个代理框架中，称为“因果代理”。该代理包括工具、记忆和推理模块。在工具模块中，因果代理通过将表格数据与自然语言对齐来应用因果方法。在推理模块中，因果代理采用ReAct框架多次迭代使用这些工具进行推理。在记忆模块中，因果代理维护了一个字典实例，其中键是唯一的名称，值是因果图。  为了验证因果代理的因果能力，我们建立了一个基准，包括四个层次的因果问题：变量级别、边级别、因果图级别和因果效应级别。我们使用ChatGPT-3.5生成了1300个针对这四个层次问题的测试数据集，并测试了因果代理。我们的方法在四个层次的因果问题上表现出极高的有效性，准确率均超过80%。  为了进一步洞察和实现细节，我们的代码可通过GitHub仓库https://github.com/Kairong-Han/Causal_Agent获取。**|
|**2024-08-12**|**Animate, or Inanimate, That is the Question for Large Language Models**|Leonardo Ranaldi et.al.|[2408.06332](http://arxiv.org/abs/2408.06332)|null|人类的认知核心与“有生命性”这一概念紧密相连，它在塑造记忆、视觉以及多层次语言理解方面发挥着关键作用。虽然“有生命性”在语言中通过动词和形容词的细微约束体现出来，但其学习和精炼过程也依赖于非语言信息。同样地，我们假设大模型在处理“有生命性”时能力有限的原因是它们仅以文本数据进行训练。因此，这篇论文旨在探讨的问题是：大模型是否能够以类似于人类的方式处理“有生命性”？我们通过提示方法进行了系统分析。具体来说，我们通过提示大模型在不同的有生命、无生命、常见和异常情境下进行操作。结果显示，尽管大模型主要基于文本数据进行训练，但在面对典型的有生命体和无生命体时，它们展现出与先前研究一致的人类行为模式。因此，大模型能够适应理解非典型情况，通过识别异常情况为有生命体，而无需依赖人类依赖的未言明的认知触发机制来分解动画。|
|**2024-08-12**|**Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example**|Yanan Chen et.al.|[2408.06318](http://arxiv.org/abs/2408.06318)|null|本文旨在填补大型语言模型（LLM）在自主代理与人工通用智能（AGI）接近过程中研究的空白。尽管LLM展现出出色的泛化能力和涌现能力，但目前缺乏对LLM驱动的代理行为、潜在失败原因以及如何提升其性能的研究，尤其是在具有挑战性的现实世界规划任务中的表现。为了填补这一缺口，我们利用了一个名为TravelPlanner的真实基准，其中的代理必须满足多个约束以生成准确的计划。通过TravelPlanner基准，我们针对四个关键研究问题进行了全面的实验：（1）LLM代理在处理长篇和嘈杂上下文时，对于推理和规划的鲁棒性是否足够？（2）少量提示能否对具有长上下文的场景产生负面影响？（3）我们能否依赖细化来改善计划？（4）是否可以使用正负反馈相结合的方法对LLM进行微调，从而进一步提高性能？  实验结果表明：首先，尽管LLM能够处理大量的参考信息和少量示例，但在处理长篇上下文时，它们往往无法关注关键部分；其次，它们仍然难以分析长期规划，并不能提供准确的反馈供细化使用；第三，我们提出了一种称为反馈感知微调（FAFT）的方法，该方法利用了正负反馈，相较于监督式微调（SFT），它能带来显著的性能提升。我们的发现为社区提供了有关现实世界规划应用方面的深入见解。|
|**2024-08-12**|**The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery**|Chris Lu et.al.|[2408.06292](http://arxiv.org/abs/2408.06292)|**[link](https://github.com/sakanaai/ai-scientist)**|**本文提出了一种全面框架，旨在实现完全自动的科学发现，使前沿的大规模语言模型能够独立进行研究，并传达其研究成果。我们引入了“AI科学家”这一概念，它能生成新颖的研究思路，编写代码，执行实验，可视化结果，撰写完整的科学论文，并进行模拟的同行评审过程以进行评估。理论上，这一过程可以迭代进行，以开放性方式发展想法，就像人类的科学社区一样。  通过将其应用于机器学习的三个不同子领域：扩散建模、基于转换器的语言建模和学习动态，展示了其灵活性。每一篇论文的开发成本低于15美元。为了评估生成的论文，我们设计并验证了一个自动审稿人，结果显示它在评价论文分数方面接近人类水平表现。AI科学家能够产生超过顶级机器学习会议接受阈值的论文，这是由我们的自动审稿人判断的。这一方法标志着机器学习领域科学研究新纪元的开始：将AI代理的变革性优势带入整个研究过程，使我们更接近一个能够释放解决世界最艰巨问题的无限可负担创新与创造力的世界。所有代码已开源在https://github.com/SakanaAI/AI-Scientist。**|
|**2024-08-12**|**MovieSum: An Abstractive Summarization Dataset for Movie Screenplays**|Rohit Saxena et.al.|[2408.06281](http://arxiv.org/abs/2408.06281)|**[link](https://github.com/saxenarohit/moviesum)**|**电影剧本的概述是一个挑战，因为它要求理解长输入上下文和电影特有的各种元素。大型语言模型在文档概述方面已经取得了显著进展，但它们往往在处理长输入上下文时遇到困难。此外，虽然最近的研究关注电视脚本，但电影剧本概述仍然缺乏探索。为了激发这一领域的研究，我们提出一个名为MovieSum的新数据集，用于电影剧本的抽象概述。这个数据集包含了2200个电影剧本及其对应的维基百科剧情概述。我们人工格式化了电影剧本以表示其结构元素。与现有的数据集相比，MovieSum具有几个独特特点：（1）它包括电影剧本，这些剧本比电视剧脚本更长。（2）它的规模是之前电影剧本数据集的两倍。（3）它提供了IMDb ID等元数据，方便获取额外的外部知识。我们还展示了最近发布的大型语言模型在我们的数据集上进行概述的结果，以提供详细的基准。**|
|**2024-08-13**|**Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation**|Jieyong Kim et.al.|[2408.06276](http://arxiv.org/abs/2408.06276)|null|近期，大型语言模型（LLM）在各类任务中的卓越表现引起了广泛关注，并激发了它们在推荐系统领域的应用潜力。然而，现有方法并未充分利用LLM的潜力，往往受限于输入信息的有限性，未能全面发挥其高级推理能力。为解决这些问题，我们提出了一种名为EXP3RT的新颖LLM推荐系统，旨在利用用户和物品评论中蕴含的丰富偏好信息。  EXP3RT通过从教师LLM中进行知识蒸馏进行微调，以执行关键的三项任务：首先，它从原始评论中提取并封装核心的主观偏好；其次，根据特定标准聚合和总结这些偏好，形成用户和物品的档案；最后，考虑用户/物品档案以及物品描述中的主客观信息，生成详细的推理步骤和预测评级，即基于推理的评级预测。这种由EXP3RT提供的个性化偏好推理能够提高评级预测的准确性，并为推荐系统提供忠实且合理的解释。  广泛实验表明，EXP3RT在评级预测和候选项目重排序（用于top-k推荐）方面均超越了现有方法，同时显著提升了推荐系统的可解释性。|
|**2024-08-12**|**FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data**|Haoran Sun et.al.|[2408.06273](http://arxiv.org/abs/2408.06273)|**[link](https://github.com/tjunlp-lab/fuxitranyu)**|大型语言模型（LLM）在各种任务中展现出了强大的能力。然而，许多LLM在高资源和低资源语言之间的性能存在显著差异。为解决这一挑战，我们提出了一种开源多语言LLM——FuxiTranyu，旨在满足研究社区对平衡且高性能多语言能力的需求。FuxiTranyu-8B，具有80亿参数的基模，从头开始训练在一个精心平衡的多语言数据仓库上，该仓库包含覆盖43种自然语言和16种编程语言的6000亿个令牌。此外，我们还开发了两个指令调优模型：FuxiTranyu-8B-SFT，它基于多元指令数据集进行微调；以及FuxiTranyu-8B-DPO，在偏好数据集上进一步精炼以增强对齐能力的DPO。广泛实验在多种多语言基准上的结果显示，FuxiTranyu在与现有多语言LLM（如BLOOM-7B、PolyLM-13B、Llama-2-Chat-7B和Mistral-7B-Instruct）的比较中表现出竞争性性能。神经元级和表示级可解释性分析表明，FuxiTranyu能够在不同语言之间学习一致的多语言表示。为了促进对多语言LLM及其工作机制的研究，我们发布了基模和指令调优的FuxiTranyu模型，以及58个预训练检查点，通过HuggingFace和Github公开分享。|
|**2024-08-12**|**A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution**|Sampath Rajapaksha et.al.|[2408.06272](http://arxiv.org/abs/2408.06272)|null|在不断演进的网络安全领域，分析师需要密切关注最新的攻击趋势和相关信息，以协助调查与归因网络攻击。本文提出了一种基于检索增强生成（RAG）技术的问答模型及其应用，旨在为网络安全专家提供有关网络攻击调查与归因的信息。我们的问答模型结合了大型语言模型（LLM）和知识库（KB），能够根据知识库或用户提供的外部资源回答用户的查询。  我们通过各种类型的提问，包括基于知识库、元数据、知识库中的特定文档以及外部资源的提问，对我们的问答模型进行了测试与评估。我们将知识库为基础的问题的答案与OpenAI的GPT-3.5及最新GPT-4的LLM答案进行了比较。结果显示，我们的问答模型在提供答案的同时给出了来源信息，并且克服了GPT模型可能产生的幻觉问题，这对于网络攻击的调查与归因至关重要。此外，我们的分析表明，当RAG问答模型在查询之外提供少量示例时，其生成的答案质量通常优于仅提供查询而没有示例的情况。|
|**2024-08-12**|**Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment**|Karel D'Oosterlinck et.al.|[2408.06266](http://arxiv.org/abs/2408.06266)|**[link](https://github.com/contextualai/clair_and_apo)**|大型语言模型（LLM）通常使用对比性对齐目标和偏好对数据集进行对齐。这一过程涉及到模型、配对数据以及目标之间的交互，使得对齐变得复杂，并有时导致不理想的成果。我们对此进行了研究，发现（i）当底层响应具有对比性时，偏好数据提供了更好的学习信号；（ii）对齐目标在训练期间为模型提供了更多的控制，从而导致了更好的性能。基于这些洞察，我们引入了对比学习从AI修订（CLAIR），一种数据创建方法，可以生成更具有对比性的偏好对，以及锚定偏好优化（APO），一个更具可控性和稳定性的对齐目标。我们使用各种可比较的数据集和对齐目标来对Llama-3-8B-Instruct进行对齐，并测量了与人类判断高度相关的MixEval-Hard分数。CLAIR偏好导致所有数据集中的最佳性能，而APO始终优于较少可控的目标。通过在32K CLAIR偏好上使用APO进行训练，我们的最佳模型提高了Llama-3-8B-Instruct的性能达7.65%，将与GPT4-turbo的差距缩小了45%。我们的代码已发布于https://github.com/ContextualAI/CLAIR_and_APO。|
|**2024-08-12**|**On Effects of Steering Latent Representation for Large Language Model Unlearning**|Dang Huu-Tien et.al.|[2408.06223](http://arxiv.org/abs/2408.06223)|null|本文首先通过理论分析证明了引导模型中间层遗忘表示向随机方向偏移，能降低文本生成的置信度，导致大型语言模型（LLM）产生错误或无意义的回答。其次，我们探讨了系数如何影响遗忘样本表示与随机方向的一致性，并暗示了不同网络层下有效的最优系数值，以实现高效的学习撤销。接着，我们展示了利用代表错乱法（RMU）进行学习撤销后的模型能够抵御对抗性逃脱攻击。  最后，我们的实证分析表明，当应用于大型语言模型的中间和后期层时，RMU的有效性较低。为了解决这一问题，我们提出了一种简单而有效的方法——自适应RMU，该方法使大多数层都能够实现高效的学习撤销，且不增加额外的计算成本。广泛的实验结果表明，与先前的研究相比，自适应RMU显著提高了学习撤销的性能。|
|**2024-08-12**|**Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting**|Halley Young et.al.|[2408.06186](http://arxiv.org/abs/2408.06186)|null|生成多样化的文本是大型语言模型（LLM）面临的关键挑战。到目前为止，多样性的研究主要通过 $n$ -gram多样性或BERT嵌入的多样性等指标进行，但这些方法在考虑多样性的维度上缺乏用户控制权。例如，在诗歌领域，用户可能希望在押韵和节奏方面实现多样性，而在代码领域，用户可能更关注解决问题时所使用的表达方式的多样性。  为此，我们提出了一种名为结构多样性（Structural Diversity）的新指标。该指标允许用户提供一个映射，将生成的文本转换为捕获用户关心的多样性的特征。这样，用户可以更具体地控制他们想要探索的多样性维度，如在诗歌领域关注押韵和节奏，在代码领域关注特定的表达方式等。  此外，我们还提出了一个名为链式规范（Chain-of-Specification，CoS）的新型策略，用于通过首先让LLM生成描述特定结构特征实例的规范，然后引导LLM生成满足这些特征的文本来提高多样性；值得注意的是，我们的策略适用于黑盒LLM。在我们的实验中，我们展示了在诗歌和代码领域实现结构多样性时，CoS策略相比多个基线显著提高了多样性。|
|**2024-08-10**|**Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions**|Michele Miranda et.al.|[2408.05212](http://arxiv.org/abs/2408.05212)|**[link](https://github.com/michele17284/awesome-privacy-preserving-llms)**|大型语言模型（LLM）在人工智能领域取得了重大进步，并在多个领域找到了应用。然而，它们依赖于庞大的互联网来源数据集进行训练，这带来了显著的隐私问题，尤其是在关键领域（如医疗保健）的情况下会加剧这些问题。此外，在特定应用场景下，可能需要对这些模型进行针对私有数据的微调。本文对LLM的隐私威胁进行了批判性评估，强调了这些模型可能记住并无意间泄露敏感信息的风险。  我们通过回顾针对LLM的隐私攻击来探讨当前的威胁，并提出全面的解决方案，以在整个学习管道中整合隐私机制。这些解决方案涵盖了从匿名化训练数据到在训练或推理过程中实施差分隐私，以及在训练后执行机器遗忘的范围。我们的文献综述深入研究了现有研究中的持续挑战、可用工具和未来方向，以保护LLM中的隐私。这项工作旨在通过提供对隐私保存方法及其在减轻风险方面的有效性的全面理解，指导开发更安全、更可信的AI系统。|
|**2024-08-09**|**VITA: Towards Open-Source Interactive Omni Multimodal LLM**|Chaoyou Fu et.al.|[2408.05211](http://arxiv.org/abs/2408.05211)|**[link](https://github.com/VITA-MLLM/VITA)**|在这篇论文中，我们引入了VITA，这是首个开源多模态大型语言模型，能够同时处理和分析视频、图像、文本和音频等多元模态信息，并且具备高级的多模态交互体验。从Mixtral 8x7B作为语言基础出发，我们扩展了其在中文方面的词汇，并通过双语指令微调进一步提升了模型能力。我们还通过两阶段多任务学习的方式，为语言模型赋予了视觉和音频处理的能力。  VITA展现了强大的多语言、视觉和音频理解的基础能力，并在一系列单模态与多模态基准测试中表现出色。除了基础能力外，我们在提升自然多模态人机交互体验方面也取得了显著进展。据我们所知，这是首次在多模态大型语言模型中利用非唤醒交互和音频中断功能。  VITA是开源社区探索无缝融合多模态理解和交互的第一步。尽管VITA与专有模型还有较大差距，但我们相信它作为先锋角色可以成为后续研究的重要基石。项目页面：https://vita-home.github.io|
|**2024-08-09**|**Evaluating the capability of large language models to personalize science texts for diverse middle-school-age learners**|Michael Vaccaro Jr et.al.|[2408.05204](http://arxiv.org/abs/2408.05204)|null|近期，大型语言模型（LLMs），尤其是OpenAI的GPT系列，在多个领域取得了显著进步。这些模型因其在不同学科领域的专业知识以及对用户提示的快速适应性而受到关注，并且展现出作为个性化学习（PL）工具的独特潜力。然而，它们在K-12教育中的应用仍处于探索阶段。  本文介绍了一项首次采用随机对照试验方法（样本量为23）来评估GPT-4在中学科学文本个性化方面的有效性的研究。在该研究中，GPT-4用于根据学生在训练阶段做出的选择来分析和预测他们的学习偏好。对于实验组的学生，GPT-4被用来修改科学文本以与学生的预测偏好相匹配；而对于控制组的学生，文本则被修改为与其学习偏好相反。通过曼-惠特尼U检验，研究发现，当文本与学生偏好匹配时，学生明显更倾向于接受（在0.10水平上具有统计学意义，p=0.059）。这些结果表明，GPT-4能够有效地理解和定制教育内容以满足不同学习者的偏好，标志着个性化学习技术领域的一个重要进展。  此外，文章还讨论了这项研究的局限性和在教育中使用人工智能的伦理考虑。|
|**2024-08-09**|**TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning**|Yujie Feng et.al.|[2408.05200](http://arxiv.org/abs/2408.05200)|**[link](https://github.com/WoodScene/TaSL)**|语言模型连续学习（CL）最近引起了广泛关注，因为它有可能在无需重新训练的情况下，适应大型语言模型（LLMs）的动态现实环境。一个关键挑战是灾难性遗忘，即模型在学习新任务时会失去先前获得的知识。现有方法通常使用多个参数效率微调（PEFT）块来为每个任务获取特定于任务的知识，但这些方法缺乏效率，并且忽视了通过任务交互进行知识传递的可能性。  在这篇论文中，我们提出了一种名为任务技能定位与整合（TaSL）的新CL框架，它通过不依赖于记忆重播来增强知识传递。TaSL首先根据参数依赖性将模型分为“技能单元”，这使得对技能单元的控制更加精细。然后，它采用了一种新颖的组级技能定位技术，以识别新任务中技能单元的重要性分布。通过比较这个重要性分布与其他先前任务中的分布，我们实施了一个精细的技能整合策略，保留了特定于任务的知识，从而防止遗忘，并更新了共享任务知识，这促进了双向知识传递。因此，TaSL实现了保持先前知识和在新任务上取得优异表现之间的最佳平衡。  TaSL也展示了强大的泛化能力，适用于通用模型，并可以根据LoRA等PEFT方法进行定制。此外，它还表现出显著的扩展性，允许与记忆重播集成以进一步提高性能。在两个CL基准测试中，使用不同规模的模型（从2.2亿到70亿参数），广泛的实验证明了TaSL及其变体在不同设置下的有效性。|
|**2024-08-09**|**AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity Recognition Dataset**|Pritam Deka et.al.|[2408.05149](http://arxiv.org/abs/2408.05149)|null|在网络安全领域，攻击归因是至关重要的过程，它允许专家制定针对攻击者的防御措施和法律行动。目前，分析人员主要通过手动操作来进行归因，这主要是由于任务的复杂性。人工智能，尤其是自然语言处理（NLP）技术可以被用来辅助网络安全分析师在归因过程中进行工作。尽管这些技术非常强大，但在缺乏攻击归因领域的数据集的情况下，它们需要应对挑战。在本文中，我们将填补这一空白，并提供到目前为止我们所知的第一个攻击归因数据集。我们的数据集设计的主要目标是从网络安全文本中提取攻击归因信息，利用NLP领域的命名实体识别（NER）方法。与其它网络安全NER数据集不同，我们的数据集提供了丰富且包含上下文细节的注释，包括一些跨短语和句子的注释。我们进行了大量实验，并应用了NLP技术来展示数据集在攻击归因方面的有效性。这些实验突显了大型语言模型（LLM）能力在改进网络安全数据集中的NER任务以提升攻击归因能力的潜力。|
|**2024-08-09**|**A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning**|Ye Yuan et.al.|[2408.05141](http://arxiv.org/abs/2408.05141)|null|在这篇论文中，我们提出了一种综合优化的增强检索辅助生成（RAG）系统，旨在通过集成外部知识库显著提高大型语言模型（LLM）的准确性和降低幻觉现象。我们的系统进行了多项改进，包括对网页中的文本段落和表格进行细化处理、引入属性预测器以减少幻觉、构建LLM知识抽取器和知识图谱抽取器，并最终建立了一个整合所有参考信息的推理策略。我们通过Meta CRAG KDD杯2024竞赛中的CRAG数据集对系统进行了评估。本地与在线评估均表明，我们的系统在复杂推理能力上实现了显著提升。在本地评估中，相较于基线模型，我们的系统在准确性方面有显著提升，错误率也有所下降，取得了较高的分数。同时，在线评估结果同样表现优异，证明了所提出系统的性能和泛化能力。该系统的源代码已发布于\url{https://gitlab.aicrowd.com/shizueyy/crag-new}。|
|**2024-08-09**|**Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations**|Jasmine Latendresse et.al.|[2408.05128](http://arxiv.org/abs/2408.05128)|null|在软件系统功能、效率与可维护性方面，软件库扮演着至关重要的角色。随着开发者越来越多地依赖大型语言模型（LLMs）以简化编码流程，这些模型推荐合适库的有效性仍处于探索阶段。本文评估了ChatGPT作为软件图书馆员的有效性，并识别了改进空间。我们通过使用GPT-3.5 Turbo生成针对10000个Stack Overflow问题的Python代码，进行了一项实证研究。我们的发现表明，ChatGPT比人类开发者更频繁地使用第三方库，倾向于广泛采用且历史悠久的选择。然而，14.2%推荐的库具有限制性的Copyleft许可，这并未由ChatGPT明确传达。此外，有6.5%的库无法直接使用，可能导致开发者困惑和浪费时间。尽管ChatGPT可以作为有效的软件图书馆员，但应提供关于维护指标和许可的更多明确信息。我们建议开发者实施严格的依赖管理实践，并在将LLM生成的代码集成到项目中之前，仔细检查库的许可证。|
|**2024-08-09**|**Large Language Models and Thematic Analysis: Human-AI Synergy in Researching Hate Speech on Social Media**|Petre Breazu et.al.|[2408.05126](http://arxiv.org/abs/2408.05126)|null|在人工智能的快速演进领域，大型语言模型（LLMs）在文本分析中的发展与应用引起了学术界的广泛关注。尽管各种LLMs在进行定性分析时展现出的潜力被寄予厚望，但它们在人文学科和社会科学中的应用并未得到充分探讨。本文通过一项以GPT-4为核心的研究实验，为LLMs在定性分析领域的应用提供了新的视角。研究基于一个来自欧盟资助项目的YouTube数据集，该数据集聚焦于2016年瑞典罗马尼亚移民群体的代表形象，这一时期正值2015年难民危机之后，紧邻2017年的瑞典全国选举。我们的研究旨在探索将人类智慧与AI的规模和效率相结合的可能性，通过分析LLMs在人文学科和社会科学领域的应用优劣，并讨论未来可能的发展方向。|
|**2024-08-09**|**Sportify: Question Answering with Embedded Visualizations and Personified Narratives for Sports Video**|Chunggi Lee et.al.|[2408.05123](http://arxiv.org/abs/2408.05123)|null|随着篮球运动的普及，粉丝们常常因比赛节奏快和复杂度高而感到困惑。篮球战术涉及一系列复杂的动作，需要大量的知识才能完全理解。这种复杂性导致了对额外信息和解释的需求，这可能会分散粉丝们对比赛的关注。为解决这一挑战，我们提出了一种名为Sportify的视觉问答系统，它融合了叙事和嵌入式可视化，旨在为球迷提供篮球战术疑问的清晰解答，帮助他们理解比赛的各种方面。我们提出了三种新型的动作可视化（传球、切入和掩护），以展示关键动作序列。为了解释球员行动背后的原因和逻辑，我们利用大型语言模型（LLM）生成叙事文本。我们采用故事讲述的方法来描述复杂场景，从第一人称和第三人称的角度进行叙述，并融入动作可视化。我们通过与篮球粉丝的评估，探讨了Sportify在深化战术洞察力和增强观赛体验方面的效果。此外，第三人称叙述有助于人们获得深入的比赛解释，而第一人称叙述则增强了粉丝们对比赛的参与感。|
|**2024-08-09**|**A Survey of NL2SQL with Large Language Models: Where are we, and where are we going?**|Xinyu Liu et.al.|[2408.05109](http://arxiv.org/abs/2408.05109)|**[link](https://github.com/hkustdial/nl2sql_handbook)**|翻译如下：  自然语言查询到SQL查询（即NL2SQL）的翻译可以显著降低访问关系数据库的障碍，并支持各种商业应用。随着大型语言模型（LLMs）的出现，NL2SQL的性能得到了大幅提升。本文提供了一个全面的NL2SQL技术综述，基于LLMs驱动，覆盖了从四个方面对整个生命周期的全面审查：（1）模型：处理自然语言的模糊性和不充分性，并正确映射自然语言与数据库模式和实例；（2）数据：从收集训练数据、应对训练数据稀缺的数据合成，到NL2SQL基准；（3）评估：从多个角度使用不同指标对NL2SQL方法进行评估；（4）错误分析：分析NL2SQL错误以找到根本原因，并指导NL2SQL模型发展。此外，我们提供了开发NL2SQL解决方案的一条经验法则。最后，讨论了在LLMs时代NL2SQL的研究挑战和开放问题。  请注意，摘要中已去除所有不必要的字符，包括","符号。|
|**2024-08-08**|**Better Alignment with Instruction Back-and-Forth Translation**|Thao Nguyen et.al.|[2408.04614](http://arxiv.org/abs/2408.04614)|null|我们提出了一种新的方法——指令双向翻译，用于构建基于世界知识的高质量合成数据，以对大型语言模型（LLMs）进行对齐。给定网络语料库中的文档，我们使用了Li等人(2023a)提出的回译方法生成并整理合成指令，并通过根据初始文档进一步改进响应的质量来重写这些指令。通过使用产生的（回译指令，重写响应）对进行微调，我们在AlpacaEval上的获胜率高于使用其他常见指令数据集（如Humpback、ShareGPT、Open Orca、Alpaca-GPT4和Self-instruct）。我们也展示了用LLM重写响应优于直接的蒸馏方法，并且生成的文本分布在这两个方面之间存在显著差异。进一步的分析表明，我们的回译指令的质量比其他合成指令来源更高，而我们的响应在多样性与复杂性上比从蒸馏获得的结果更为出色。总体而言，我们发现指令双向翻译结合了网络上信息多样性和数量的优势，同时确保了响应的质量，这是有效对齐所必需的。|
|**2024-08-09**|**Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models**|Qirui Jiao et.al.|[2408.04594](http://arxiv.org/abs/2408.04594)|**[link](https://github.com/modelscope/data-juicer)**|**本文提出了一种名为Img-Diff的新数据集，旨在通过对比学习和图像差异描述的方法来增强大型语言模型在细微图像识别任务中的性能。该方法通过分析相似图像间的对象差异，要求模型识别相同与不同之处。利用Stable-Diffusion-XL模型及高级图像编辑技术生成突出对象替换的相似图像对。数据生成流程包括差异区域生成器识别对象差异，随后差异描述生成器提供详细的差异说明。结果是创建了一个小而高质量的“对象替换”样本集合。使用此数据集对当前最佳的多模态大语言模型（如MGM-7B）进行微调，显著提高了这些模型在图像差异和视觉问答任务上的表现分数，超越了基于大规模数据集训练的当前最佳模型（如GPT-4V和Gemini）在MMVP基准测试中的表现。此外，本文还探讨了通过“对象移除”方法生成图像差异数据的替代方法，并进行了全面评估以验证数据集的多样性和质量，提供了关于此类对比性数据集合成的深入见解。为了促进进一步的研究并推动多模态数据合成和增强大型语言模型基础能力的发展，我们已将代码和数据集发布在https://github.com/modelscope/data-juicer/tree/ImgDiff上供公众使用。**|
|**2024-08-08**|**Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness**|Xiaojing Fan et.al.|[2408.04585](http://arxiv.org/abs/2408.04585)|null|随着大型语言模型（LLM）实用应用需求的增加，许多关注效率的模型被开发出来以平衡性能和计算成本。然而，这些模型的对抗鲁棒性仍然缺乏深入研究。本研究设计了一个框架，通过比较三个具有不同复杂度和效率水平的主要模型——Transformer++、门控线性注意力（GLA）变换器以及MatMul-Free LM，来探索效率、性能与对抗鲁棒性的权衡关系。利用GLUE和AdvGLUE数据集进行比较。AdvGLUE数据集通过添加旨在挑战模型鲁棒性的对抗样本扩展了GLUE数据集。  我们的结果表明，在GLUE任务上的准确性稍低的情况下，GLA变换器和MatMul-Free LM在AdvGLUE任务上显示出更高的效率，并且在不同攻击级别下，它们的鲁棒性要么优于，要么与Transformer++相匹敌。这些发现强调了简化架构在实现高效能、高性能与对抗鲁棒性之间取得良好平衡的可能性，为资源受限环境和对对抗攻击有高抵抗力需求的应用提供了有价值的见解。|
|**2024-08-08**|**SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals**|Haoran Zheng et.al.|[2408.04575](http://arxiv.org/abs/2408.04575)|null|解释性人工智能（XAI）对于增强人工智能模型的透明度和责任性至关重要，尤其是在自然语言处理（NLP）任务中。本文提出了一种名为SCENE（软反事实评估用于自然语言可解释性）的新方法，该方法利用大型语言模型（LLMs）在零次射击的情况下生成软反事实解释。通过关注基于词元的替换，SCENE创建了上下文相关且语义上具有意义的软反事实，而无需进行大量微调。SCENE采用有效性软和C软指标来评估各种模型无关的XAI方法在文本分类任务中的效果。应用于CNN、RNN和BERT架构，SCENE提供了对各种XAI技术强项和局限性的有价值见解。|
|**2024-08-08**|**Learning Fine-Grained Grounded Citations for Attributed Large Language Models**|Lei Huang et.al.|[2408.04568](http://arxiv.org/abs/2408.04568)|**[link](https://github.com/luckyyysta/fine-grained-attribution)**|**尽管大型语言模型（LLM）在信息查询任务上表现出色，但它们仍然在幻觉问题上存在挑战。基于属性的LLM，通过在生成文本中添加内联引用，显示出减少幻觉并提高可验证性的潜力。然而，当前的方法在生成高质量引用方面效果不佳，这主要是由于它们依赖于上下文学习。此外，只引用粗粒度文档标识的做法使得用户难以进行精细验证。为此，我们提出了FRONT框架，旨在教导LLM生成细粒度相关引用。这些引用通过连接到生成响应的细粒度支持引用来提供指导，不仅提高了引用质量，还便于进行精细验证。在ALCE基准测试上的实验结果表明，FRONT在生成优秀相关响应和高度支持性引用方面非常有效。使用LLaMA-2-7B时，该框架显著优于所有基线，平均提高了14.21%的引用质量，并且超越了ChatGPT。**|
|**2024-08-08**|**Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models**|Yupeng Chang et.al.|[2408.04556](http://arxiv.org/abs/2408.04556)|**[link](https://github.com/cyp-jlu-ai/ba-lora)**|**大型语言模型（LLM）在各种自然语言处理（NLP）任务上表现出令人瞩目的能力。然而，在将这些模型应用于下游应用时，通常需要进行计算密集型和内存消耗大的微调过程。为了解决这个问题，参数高效微调（PEFT）技术已经作为一种有前景的方法出现，旨在以最小的计算成本来定制LLM。尽管PEFT方法提供了显著的优势，但它们并未完全解决从预训练数据继承偏见的问题。为此，我们提出了一种新的PEFT方法——Bias-Aware Low-Rank Adaptation (BA-LoRA)，旨在对抗偏见继承。  BA-LoRA整合了三个不同的正则化项：一致性正则化器、多样性正则化器以及奇异值分解正则化器。这三个正则化器共同旨在提高生成模型在微调过程中的一致性、多样性和泛化能力。通过在多种自然语言理解（NLU）和自然语言生成（NLG）任务上的广泛实验，并使用如LLaMA、Mistral和Gemma等主流LLM，我们展示了BA-LoRA在性能上超越了LoRA及其最先进的变体。此外，我们的方法有效地减轻了预训练偏见的负面影响，导致更可靠且稳健的模型输出。相关代码已开源在https://github.com/cyp-jlu-ai/BA-LoRA。**|
|**2024-08-08**|**Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models**|Fabio Pernisi et.al.|[2408.04522](http://arxiv.org/abs/2408.04522)|null|随着不同语言的多元语言社区和用户采用大型语言模型（LLM），评估这些模型在不同语言环境下的安全性变得至关重要。尽管已经进行了持续的努力以确保LLM的安全性，但它们仍然可以通过“越狱”技术来表现得不安全，这是一种促使模型在其操作准则之外行动的技术。对于LLM安全性以及“越狱”的研究目前主要集中在英语上，这限制了我们对其他语言中LLM安全性的理解。为了填补这一空白，我们通过在意大利语中研究多轮“越狱”的有效性，即使用不安全示例来诱导不安全行为，来贡献于这一领域。为了支持我们的分析，我们创建了一个新的意大利语问题-答案不安全数据集。利用这个数据集，我们在四个开放权重LLM家族中识别出了明显的安全漏洞。我们发现，即使在使用少量不安全示例的情况下，模型也会表现出不安全的行为，并且更令人担忧的是，随着更多示例的出现，这种趋势迅速加剧。|
|**2024-08-08**|**What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant**|Jonan Richards et.al.|[2408.04477](http://arxiv.org/abs/2408.04477)|null|在大型语言模型（LLM）用于辅助开发者理解代码的工具数量不断增加的同时，开发者在使用这些工具时仍面临一些障碍，包括用自然语言描述其意图的挑战、解读工具结果的困难，以及调整有效提示以获得有用信息的过程。为此，我们设计了一个基于LLM的对话助手，该助手根据推断出的用户心理状态（如背景知识和经验）提供个性化互动。通过针对十四位新手进行的内部主题研究，我们捕捉了他们的感知和偏好。研究结果为希望创建或改进面向新手的LLM为基础的对话助手以支持代码理解的研究人员和工具开发者提供了见解。|
|**2024-08-08**|**Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate**|Yiqun Zhang et.al.|[2408.04472](http://arxiv.org/abs/2408.04472)|**[link](https://github.com/zhangyiqun018/agent-for-debate)**|**在竞争性辩论这一全面且复杂的计算论辩任务中，大型语言模型（LLMs）面临着幻觉和竞争力不足的问题。为了应对这些挑战，我们提出了一种名为“辩论者”（Agent4Debate）的动态、多代理框架，该框架基于LLMs设计，旨在增强其在竞争性辩论中的能力。该框架受到人类在辩论准备与执行过程中行为的启发，采用协作架构，由四个专门的代理（搜索者、分析者、撰写者和审阅者）动态交互并合作。这四个代理在整个辩论过程中覆盖了从初始研究到论点形成、反驳和总结的多个阶段。  为了全面评估框架的性能，我们构建了一个名为“中国辩论竞技场”的数据库，包含了66个精心挑选的中文辩论议题。我们招募了十位经验丰富的专业辩论者，并收集了涉及Agent4Debate、基线模型和人类的200场辩论记录。评价体系采用了自动评分系统Debatrix以及基于Debatrix-Elo和Human-Elo排名的专业评审团。实验结果显示，最先进的Agent4Debate在能力上与人类相当。进一步的消融研究表明，代理结构中的每个组件的有效性。**|
|**2024-08-08**|**RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents**|Zihao Zhu et.al.|[2408.04449](http://arxiv.org/abs/2408.04449)|null|摘要提出了一种名为RiskAwareBench的自动化框架，旨在评估基于大型语言模型（LLM）的实体化代理在物理风险意识方面的能力。该框架由四个模块组成：安全提示生成、危险场景生成、计划生成和评估，它允许进行全面的风险评估，且所需的人工干预最少。通过使用这个框架，构建了一个名为PhysicalRisk的数据集，涵盖了各种涉及相关安全提示、观察和指令的场景。  实验结果表明，大多数LLM在物理风险意识方面表现不足，并且基础的风险缓解策略带来的提升有限。这强调了在未来改进基于LLM的实体化代理的物理风险意识的紧迫性和重要性。|
|**2024-08-07**|**How Well Can Vision Language Models See Image Details?**|Chenhui Gou et.al.|[2408.03940](http://arxiv.org/abs/2408.03940)|null|大型语言模型驱动的视觉语言模型（LLM-驱动的VLM）在各种视觉语言理解任务上表现出色。然而，这些VLM是否能超越语义层面，深入观察图像细节仍然不明朗。为此，我们引入了一个像素值预测任务（PVP），以探索“视觉语言模型能够看到多细的图像细节？”并协助VLM提升对细节的感知能力。通常，这些模型由冻结的CLIP视觉编码器、大型语言模型和连接模块组成。在对PVP任务进行微调后，我们发现：1）现有的VLM仅通过微调连接模块和LLM，在预测精确像素值方面表现不佳；2）当视觉编码器也得到适应时，预测精度显著提高。此外，我们的研究揭示，将像素值预测作为VLM预训练任务之一，并对视觉编码器进行适应，显著提升了VLM在需要详细图像感知的下游图像语言理解任务上的性能，如引用图像分割（平均cIoU改进+10.19百分点）和视频游戏决策（在两个游戏中分别平均得分改善了+80.34和+70.54）。|
|**2024-08-07**|**SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature**|Vinícius Di Oliveira et.al.|[2408.03936](http://arxiv.org/abs/2408.03936)|null|自然语言处理（NLP）随着大型语言模型（LLMs）的兴起取得了显著进步。然而，对于英语之外的语言，尤其是在特定领域如Mercosur通用商品名称（NCM），巴西协调系统（HS）的应用方面，仍有很大的改进空间。为解决这一缺口，本研究利用TeenyTineLLaMA，一种基础葡萄牙语LLM，作为LLM源，实施NCM应用处理。此外，提出了一种针对任务特定微调的简化检索增强微调（SLIM-RAFT）技术。该方法采用简化的链式思维（CoT）策略进行提示开发，使用简短而集中的文档进行训练，以更紧凑和高效的方式进行。提出的模型在相同任务上显著优于TeenyTineLLaMA和ChatGPT-4，展示了较小LLM微调的高效和成本效益替代方案。尽管研究重点是NCM应用，但所提出的方法可以轻松地适应全球范围内的HS应用。|
|**2024-08-07**|**CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases**|Xiangyan Liu et.al.|[2408.03910](http://arxiv.org/abs/2408.03910)|**[link](https://github.com/modelscope/modelscope-agent)**|**大型语言模型（LLM）在诸如HumanEval和MBPP的独立代码任务中表现出色，但它们在处理整个代码仓库时存在挑战。这促使研究界探索如何在仓库级别上增强LLM与代码库的交互。目前的解决方案依赖于基于相似性的检索或手动工具和API，每种方法都有其显著的缺点。基于相似性的检索在复杂任务中召回率较低，而手动工具和API通常具有特定的任务性，并且需要专家知识，这降低了它们在不同代码任务和实际应用中的通用性。为了减轻这些限制，我们引入了\framework，这是一个系统，它将LLM代理与从代码仓库提取的图数据库接口集成在一起。通过利用图数据库的结构特性以及图查询语言的灵活性，\framework使LLM代理能够构建并执行查询，从而实现精确、代码结构意识的上下文检索和代码导航。我们使用三个基准测试评估\framework：CrossCodeEval、SWE-bench和EvoCodeBench。此外，我们还开发了五个真实世界的编码应用。凭借统一的图数据库模式，\framework在学术和实际环境中都展示了竞争力和潜力，体现了其在软件工程领域的多功能性和有效性。我们的应用演示：https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent。**|
|**2024-08-07**|**Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models**|Shachi H Kumar et.al.|[2408.03907](http://arxiv.org/abs/2408.03907)|null|大型语言模型（LLM）在理解语言和生成与人类水平相当的文本方面表现出色。然而，即使经过监督训练和人类对齐，这些LLM仍容易受到恶意用户的攻击，后者可以通过提示模型生成不希望看到的文本。此外，LLM内嵌有潜在偏见，这可能导致互动中的各种有害影响。当前的偏见评估指标缺乏标准和共识，现有方法往往依赖于人工生成的模板和注释，这既昂贵又费时。  我们的工作旨在通过训练模型自动创建对抗性提示来激发目标LLM生成带有偏见的响应。我们提出了一种基于LLM的偏见评估指标，并分析了多种现有的自动评估方法和指标。我们深入探讨了模型响应的各种细微差别，识别了不同模型家族的优势和劣势，并评估了评估方法的不足之处。我们将这些指标与人工评估进行比较，并验证了“LLM作为法官”的指标与生成偏见判断的人类评价一致。|
|**2024-08-07**|**From Data to Story: Towards Automatic Animated Data Video Creation with LLM-based Multi-Agent Systems**|Leixian Shen et.al.|[2408.03876](http://arxiv.org/abs/2408.03876)|null|创建从原始数据生成数据故事的过程极具挑战性，这主要源于人类有限的注意力和对特定技能的需求。近来，大型语言模型（LLM）的发展为构建利用独立代理实现工作流程自动化以简化数据故事创作流程的系统提供了巨大机遇。尽管多代理系统能够充分挖掘LLM潜力并分解任务供个体代理执行具有诸多优势，但在设计这些系统时，也面临着任务分解、子任务性能优化以及工作流程设计等方面的挑战。为了更深入地理解这些问题，我们开发了Data Director——一个基于LLM的多代理系统，旨在自动化生成动画数据视频，这一类数据故事的典型形式。Data Director通过解析原始数据、拆分任务、设计代理角色以进行自动决策，并无缝整合数据视频中的各种组件来实现这一目标。一个案例研究展示了Data Director在生成数据视频方面的有效性。在整个开发过程中，我们从解决面临的挑战中提炼出了经验教训，这些经验对于指导未来在数据故事叙述领域自主代理的发展具有重要意义。此外，我们也揭示了全球优化、人机交互设计以及高级多模态LLM应用的未来发展方向。|
|**2024-08-07**|**PackMamba: Efficient Processing of Variable-Length Sequences in Mamba training**|Haoran Xu et.al.|[2408.03865](http://arxiv.org/abs/2408.03865)|null|随着大型语言模型的发展，传统的Transformer模型在处理长序列时变得计算密集型，因为其计算量随序列长度的平方增长。Mamba作为生成AI领域的一项突破性架构，展现出在减少计算和内存复杂性的前提下，高效处理长序列的能力。然而，现有的Mamba训练框架在处理变长序列输入时存在效率问题。单序列训练会导致GPU利用率低下，而对变长序列进行批量处理到最大长度则会带来显著的内存和计算开销。  为了解决这一问题，我们分析了Mamba架构中瓶颈操作器在不同张量形状下的性能，并提出了一种名为PackMamba的高吞吐量Mamba，它能够有效地处理变长序列。深入研究状态空间模型（SSMs），我们修改了并行操作器，以避免在各个序列之间传递信息，同时保持高性能。在NVIDIA A100 GPU上的实验结果表明，PackMamba在处理1.4B模型时比基线单序列处理方案提高了3.06倍的速度，在处理2.8B模型时提高了2.62倍的速度。|
|**2024-08-07**|**GAIA -- A Large Language Model for Advanced Power Dispatch**|Yuheng Cheng et.al.|[2408.03847](http://arxiv.org/abs/2408.03847)|null|电力调度对于提供稳定、经济且环保的电力至关重要。然而，随着电力系统规模和复杂性的增长，传统的调度方法在多任务处理、快速问题解决以及人机协作方面遇到挑战。本文介绍了一种专为电力调度任务设计的大型语言模型（LLM）——GAIA。我们开发了一种新颖的数据集构建技术，利用多种数据源对GAIA进行微调，以优化其在该领域的性能。这种方法简化了LLM的训练过程，使得在电力系统管理中能够无缝整合多维数据。此外，我们还设计了专门的提示策略来提高GAIA在调度场景下的输入输出效率。在ElecBench基准测试中，GAIA在多个指标上超越了基础模型LLaMA2。实际应用表明，GAIA能够增强决策过程、提高运营效率，并促进电力调度操作中的人机交互。本文扩展了LLM在电力调度领域的应用，并验证了其实用性，为这一领域未来的创新开辟了道路。|
|**2024-08-07**|**MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models**|Yuchen Dong et.al.|[2408.03841](http://arxiv.org/abs/2408.03841)|null|本文探讨了大型语言模型在自动化软件操作和工具生成（SOTG）领域的应用，以此来提升软件生产力。这一过程类似于人类文明早期通过创造并使用工具加速发展的阶段。这些复杂任务要求AI能够持续总结并改进。当前研究往往忽视了将实时任务经验转化为系统记忆以及区分现有知识未来价值的重要性。本文通过引入“Memory-Loop网络”来解决这些问题，以实现及时的记忆存储与经验引用。  此外，我们还对基于知识精确分段的RAG机制进行了增强，以便根据价值差异利用记忆。针对SOTG设计了MaxMind模型。为了验证我们的方法，我们开发了MaxMind4Sheet，一个遵循MaxMind理念的电子表格处理系统。与SheetCopilot的比较实验显示，任务记忆的积累和循环能够稳步提高任务成功率，在此示例实施中，每轮的成功率提升约为3%-6%。随着记忆的持续增长，这种累积改进可能会非常显著。  引入记忆循环还可以通过高达25%的效率提升增加系统的任务执行效率，并通过记忆转移解决LLM在处理专业任务时面临的再训练问题。这表明MaxMind有潜力显著增强大型语言模型在SOTG领域的功能和生产力。|
|**2024-08-07**|**WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models**|Prannaya Gupta et.al.|[2408.03837](http://arxiv.org/abs/2408.03837)|**[link](https://github.com/walledai/walledeval)**|WalledEval是一个全面的AI安全性测试工具包，旨在评估大型语言模型（LLMs）。它能够兼容各种模型，包括开源和API两种类型，并包含了超过35个覆盖多语言安全、夸张安全以及提示注入等领域的安全基准。该框架支持对LLM和裁判进行基准测试，并且集成自定义突变器，用于测试在不同文本风格变异如将来时态和重述下的安全性。此外，WalledEval引入了WalledGuard，这是一种新的小型高效内容审核工具，以及SGXSTest，用于评估文化背景下的夸大安全问题。我们已将WalledEval公开发布在https://github.com/walledai/walledevalA。|
|**2024-08-07**|**Target Prompting for Information Extraction with Vision Language Model**|Dipankar Medhi et.al.|[2408.03834](http://arxiv.org/abs/2408.03834)|null|近期，大型视觉与语言模型（VLM）领域的发展在构建信息提取系统方面带来了新的变革。这些模型在理解文档和构建跨行业的问题回答系统方面达到了顶尖水平，显著提升了从文档图像生成文本以及提供精确答案的能力。然而，利用这些模型构建精准对话系统时仍存在一些挑战。传统的通用提示技术在大型语言模型上的应用往往不适合这些专门设计的视觉语言模型。使用这类通用输入提示所生成的输出通常较为普通，与文档实际内容相比可能存在信息缺口。为了获得更准确、更具体的答案，视觉语言模型需要针对特定部分的文档图像进行提示，并仅从这些特定区域生成相关答案。本文讨论了一种称为“目标提示”的技术，该技术专注于明确指向文档图像的部分并仅从这些特定区域生成相关的答案。此外，文章还通过使用不同用户查询和输入提示对每种提示技术的响应进行了评估。|
|**2024-08-06**|**TextIM: Part-aware Interactive Motion Synthesis from Text**|Siyuan Fan et.al.|[2408.03302](http://arxiv.org/abs/2408.03302)|null|本文提出了一种名为TextIM的新型框架，旨在合成基于文本驱动的人类交互动作，并特别关注于部分级语义的精确对齐。现有方法往往忽视了交互身体部位的关键作用，并未能充分捕捉和对齐部分级语义，导致了不准确甚至错误的动作结果。为了解决这些问题，TextIM采用了一个解耦条件扩散框架，以增强交互动作与对应文本描述中的语义意图之间详细的对齐。我们的方法利用大型语言模型，作为人类大脑的角色，来识别交互的身体部位并理解交互语义，从而生成复杂的微妙交互动作。在精细动作引导下，TextIM进一步将这些部分动作扩展为整个身体的连贯动作。我们设计了一个空间一致性模块，通过部分图卷积网络在整个身体动作中补充和维持各部分之间的连贯性和和谐性。对于训练和评估，我们精心选择了并重新标记了HUMANML3D中的交互动作数据集，创建了一个专门的数据集。实验结果显示，TextIM能够产生语义上准确的人类交互动作，显著提高了在各种场景下合成交互动作的真实感和应用性，包括与可变形和动态变化物体的交互。|
|**2024-08-06**|**KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models**|Ruizhe Zhang et.al.|[2408.03297](http://arxiv.org/abs/2408.03297)|null|通过整合外部知识，检索增强生成（RAG）策略已成为缓解大型语言模型在处理知识密集型任务时遇到的幻觉问题的有效方法。然而，在整合非参数化的外部支持证据与内部参数化知识的过程中，不可避免的知识冲突可能会产生，导致模型响应中的混淆。为了在不同情境下提升语言模型的知识选择能力，一些研究已经关注于通过指令调整来细化其行为模式。然而，由于缺乏明确的负向信号和比较目标，通过这种方式进行微调的语言模型在复杂的、现实的检索场景中仍然可能表现出不理想的特性。  针对这一挑战，我们提出了一种知识意识偏好优化（KaPO），旨在实现对真实检索场景中知识选择的可控性。具体而言，我们探索并模拟了不同上下文组合下的错误类型，并通过偏好优化方法学习如何避免这些负向信号。同时，通过调整响应长度与表示不同行为模式的偏好数据比例之间的平衡，我们增强了语言模型的适应能力和噪声鲁棒性。  实验结果表明，与先前的方法相比，KaPO在处理知识冲突方面取得了超过37%的性能提升，并且在各种离群数据集上表现出了稳健的泛化能力。|
|**2024-08-07**|**StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation**|Boxi Cao et.al.|[2408.03281](http://arxiv.org/abs/2408.03281)|**[link](https://github.com/c-box/structeval)**|评价是大型语言模型开发的关键工具。当前的评估方式通常采用单一指标评估模式，对每个基本测试目标进行评估，这在区分模型是否真正具备所需能力还是仅仅记忆或猜测特定问题的答案方面存在困难。为了应对这一挑战，我们提出了一种名为StructEval的新评估框架。从基本测试目标出发，StructEval通过在多个认知层次和关键概念上进行结构化的评估来深化和拓宽评估范围，从而为大型语言模型提供全面、稳健且一致的评估。在三个广泛使用的基准上进行的实验表明，StructEval是一个可靠的工具，能够抵抗数据污染的风险并减少潜在偏见的干扰，从而提供关于模型能力更可靠和一致的结论。我们的框架还为未来原理性和可信的大型语言模型评估协议的设计提供了启示。|
|**2024-08-06**|**Synthesizing Text-to-SQL Data from Weak and Strong LLMs**|Jiaxi Yang et.al.|[2408.03256](http://arxiv.org/abs/2408.03256)|null|本文探讨了开源与封闭式大型语言模型（LLM）在文本到SQL任务中的能力差距问题。为此，我们提出了一种合成数据方法，该方法结合了更大、更强大的模型生成的数据（强模型）与较小、不完全对齐模型生成的错误信息数据（弱模型）。这种方法不仅提高了文本到SQL模型的领域泛化能力，还探索了错误数据监督通过偏好学习的潜力。此外，我们利用合成数据方法对开源LLM进行指令调整，由此产生了专门针对文本到SQL任务的模型SENSE。通过在SPIDER和BIRD基准上的表现，证明了SENSE的有效性，成功缩小了开源模型与基于封闭源模型的方法之间的性能差距。|
|**2024-08-06**|**Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons**|Yifei Wang et.al.|[2408.03247](http://arxiv.org/abs/2408.03247)|**[link](https://github.com/wangyifei0047/tfrkn)**|在这篇论文中，我们深入研究了大型语言模型（LLM）在面对推理任务时是否积极地回忆或检索其内部事实知识库。通过分析LLM在每个推理步骤中的内部事实召回情况，即所谓的知识神经元，我们揭示了在某些情况下，LLM未能有效利用关键的事实关联。相反，它们倾向于采取替代的、快捷的路径来回答推理问题。通过手动调整LLM中参数知识的召回过程，我们证明直接增强这一过程可以显著提高推理性能，而抑制它则会导致明显的性能下降。此外，我们评估了链式思考（CoT）提示的影响，这是一种处理复杂推理任务的强大技术。我们的发现表明，CoT可以通过鼓励LLM进行有条理和可靠的推理来增强对事实知识的回忆。进一步地，我们探讨了上下文冲突如何影响推理过程中事实的检索，以获得对LLM事实回忆行为的全面理解。相关代码和数据将在不久后提供。|
|**2024-08-06**|**Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi**|Pranita Deshmukh et.al.|[2408.03172](http://arxiv.org/abs/2408.03172)|null|随着低资源语言数字内容的激增，针对这些语言的高级自然语言处理（NLP）技术需求正在增加。BERT（双向编码表示的Transformer）作为众多NLP架构和语言模型的基础框架，正越来越多地用于开发低资源NLP模型。参数高效微调（PEFT）是一种方法，用于对大型语言模型（LLMs）进行微调，并在一定程度上减少训练参数，以降低训练模型所需的计算成本，并达到与完全微调模型相当的结果。本研究旨在分析PEFT方法在马拉地语低资源语言中的应用。我们对各种单语和多语种马拉地语BERT模型进行了全面分析，这些方法在MahaSent、MahaHate和MahaNews等重要文本分类数据集上进行了评估。PEFT技术的引入显著加快了模型的训练速度，解决了模型开发和部署的关键方面。在本研究中，我们探索了大型语言模型的低秩适应（LoRA）和适配器方法在低资源文本分类中的应用。结果显示，这些方法在准确率上与全量微调相当，且无需损失，可用于马拉地语和其他印度语族语言的NLP能力持续发展。|
|**2024-08-06**|**Conditioning LLMs with Emotion in Neural Machine Translation**|Charles Brazier et.al.|[2408.03150](http://arxiv.org/abs/2408.03150)|null|大型语言模型（LLM）在自然语言处理任务中展现了卓越的性能，特别是在机器翻译领域。本文提出了一种新颖的机器翻译管道，该管道通过将情感信息整合到语言模型中来增强翻译质量，这些情感信息是从语音情感识别（SER）模型中提取的。首先，我们对五个现有的LLM进行Libri-trans数据集的微调，并选择表现最佳的模型。随后，我们以不同维度的情感增强LLM提示，并在这些不同的配置下训练选定的LLM。我们的实验结果表明，将情感信息，尤其是唤醒度，整合到LLM提示中，能够显著提高翻译质量。|
|**2024-08-06**|**Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations**|Leo Donisch et.al.|[2408.03130](http://arxiv.org/abs/2408.03130)|null|大型语言模型在自然语言处理领域无处不在，因为它们能够在无需重新训练的情况下适应新任务。然而，这些模型的规模和复杂性带来了独特的挑战与机遇，促使研究者与实践者探索新型的模型训练、优化和部署方法。本文综述的重点在于各种降低资源需求和压缩大型语言模型的技术，包括量化、剪枝、知识蒸馏以及架构优化。主要目标是深入探讨每种方法，并突出其独特挑战及其实际应用。讨论的方法按照分类学进行组织，提供了一个优化景观的概览，有助于更好地理解研究轨迹。  ## 任务 请将上述论文摘要翻译成中文，不要输出任何无关内容，确保翻译内容中不包含","字符。|
|**2024-08-06**|**Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation**|Artur Guimarães et.al.|[2408.03127](http://arxiv.org/abs/2408.03127)|**[link](https://github.com/araag2/SemEval2024-Task2)**|这篇论文阐述了我们对SemEval-2024安全生物医学自然语言推断在临床试验（NLI4CT）任务的处理策略。该任务涉及对临床试验报告（CTRs）中的陈述进行分类。我们探索了Mistral-7B这种通用的开源大型语言模型（LLM）的能力。我们为NLI4CT任务设计了一个提示，并使用增强后的训练数据集对量化版本的模型进行了微调。实验结果显示，这种方法在宏F1分数方面可以产生显著的结果，但在忠实性和一致性方面存在局限性。所有开发的代码都在GitHub仓库中公开提供。|
|**2024-08-06**|**Evaluating the Translation Performance of Large Language Models Based on Euas-20**|Yan Huang et.al.|[2408.03119](http://arxiv.org/abs/2408.03119)|null|近年来，在深度学习技术的快速发展的推动下，大型语言模型（LLMs）如BERT和GPT在自然语言处理任务上取得了突破性成果。机器翻译作为自然语言处理的核心任务之一，也从大型语言模型的发展中受益匪浅，实现了质的飞跃。尽管大型语言模型在翻译性能上取得了显著进展，但机器翻译仍面临诸多挑战。因此，本文构建了Euas-20数据集，用于评估大型语言模型在翻译任务上的性能、不同语言的翻译能力以及预训练数据对LLMs翻译能力的影响，旨在为研究人员和开发者提供参考。|
|**2024-08-05**|**Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?**|Mohammad Bahrami Karkevandi et.al.|[2408.02651](http://arxiv.org/abs/2408.02651)|null|大型语言模型（LLM）在自然语言任务上表现出令人印象深刻的能力，但它们的安全性和道德性仍然存在争议，因为它们的训练基于互联网文本语料库。为了应对这些担忧，已经开发了对齐技术来提高大型语言模型的公共可用性和安全性。然而，通过这些模型生成有害内容的可能性似乎仍然存在。本文探讨了“反向对齐”LLM的概念——利用对抗触发器逆转其对齐过程。先前的方法，如软嵌入提示、手动构建的提示和基于梯度的自动提示，在黑盒模型上由于需要访问模型和产生有限的手动构建提示的需求而取得了有限的成功，这使得它们容易被阻断。本文提出了一种新的方法，使用强化学习优化对抗触发器，仅需对目标模型进行推理API访问以及一个小型代理模型即可。我们的方法利用BERTScore为基础的奖励函数，增强了对抗触发器在新黑盒模型上的可移植性和有效性。我们展示了这种方法如何在未测试的语言模型上提高了对抗触发器的表现。|
|**2024-08-05**|**SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models**|Muxi Diao et.al.|[2408.02632](http://arxiv.org/abs/2408.02632)|null|随着大型语言模型（LLM）能力与影响力的持续增强，确保其安全性和预防有害输出变得至关重要。为解决这些关切，一种有前景的方法是训练模型自动生成对抗性提示进行红队测试。然而，LLM中漏洞的不断演变使得当前的对抗方法在具体针对和探索这些模型弱点方面显得力不从心。为了应对这些挑战，我们引入了“自我演化安全优化”（SEAS）框架，该框架通过利用模型自身生成的数据来增强安全性。SEAS运作于三个迭代阶段：初始化、攻击和对抗优化，旨在同时提升红队和目标模型的稳健性和安全性。  该框架减少了对人工测试的依赖，并显著增强了LLM的安全性能力。我们的贡献包括一个新颖的对抗性框架、一个全面的安全数据集以及经过三次迭代后，目标模型的安全水平达到了与GPT-4相当的水平，而红队模型在对抗高级模型时的成功率（ASR）有了显著提高。|
|**2024-08-05**|**Progressively Selective Label Enhancement for Language Model Alignment**|Biao Liu et.al.|[2408.02599](http://arxiv.org/abs/2408.02599)|null|大型语言模型在各种语言任务上展现出令人印象深刻的能力，但可能会生成与人类预期不符的内容，从而引发伦理和法律问题。因此，探索这些模型的局限性并实施限制以确保安全性和合规性变得至关重要，其中强化学习从人类反馈（RLHF）是主要方法。然而，由于RLHF阶段在稳定性和可扩展性方面面临的挑战，研究人员正在探索其他方法来实现与RLHF类似的效果。这些方法往往依赖于大量高质量的数据集，并且低效地利用生成的数据。为解决这一问题，我们提出了一种名为PSLE（Progressively Selective Label Enhancement for Language Model Alignment）的框架，它充分利用所有生成数据，通过指导模型遵循原则来使输出与人类期望保持一致。通过动态更新阈值，我们的方法确保了高效的数据利用，通过整合所有生成响应并根据其相应的奖励分数对它们进行加权。在多个数据集上的实验结果表明，PSLE在现有语言模型对齐方法中表现出有效性。|
|**2024-08-05**|**Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization**|Ankan Mullick et.al.|[2408.02584](http://arxiv.org/abs/2408.02584)|null|随着数字信息量的持续增长，用户需要有效方法从长篇文档中提取关键见解。面向方面的总结提供了一种有针对性的方法，生成专注于文档内特定方面的小结。尽管在面向方面的总结研究领域取得了进展，但提高模型性能的持续追求是必要的。鉴于大型语言模型（LLMs）在自然语言处理任务中的潜力，特别是在总结问题上，本文探讨了对LLMs进行微调以执行面向方面的总结任务的可能性。我们评估了开源基础LLMs，包括Llama2、Mistral、Gemma和Aya，对于公开可用的特定领域面向方面的总结数据集的影响。我们的假设是，这种方法能够让这些模型有效地识别并提取与方面相关的信息，从而产生与最先进的方法相比更高质量的面向方面的总结。我们建立了一个全面的评估框架，将微调后的LLMs的性能与竞争性的面向方面的总结方法以及微调前LLMs的原始版本进行比较。我们的工作通过证明对LLMs进行微调可以生成高质量的面向方面的总结，为面向方面的总结领域做出了贡献。此外，它为在不同NLP领域进一步探索使用LLMs进行目标信息抽取任务打开了大门。|
|**2024-08-05**|**Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information**|Yauwai Yim et.al.|[2408.02559](http://arxiv.org/abs/2408.02559)|null|本文探讨了开源与API驱动大型语言模型在复杂、不完全信息环境下的文本游戏协作能力，特别是在非英语环境中的应用潜力。研究对比了这些模型与其他类型代理的性能，并使用理论思维（Theory of Mind, ToM）规划技术来评估它们在需要多智能体协作的不完全信息游戏中表现的能力。通过引入外部工具来解决此卡牌游戏中动态且庞大的行动空间问题，我们的结果揭示了当前大型语言模型在面对高级别任务时与强化学习模型之间的性能差距。尽管存在这一差距，但大型语言模型展现了在游戏场景下的理论思维能力，能够理解盟友和对手的行为，并与盟友建立协作关系，从而持续提升其性能。为了促进进一步的研究与理解，我们已公开了代码库。|
|**2024-08-05**|**Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading by In-context Learning**|Hao Zhou et.al.|[2408.02549](http://arxiv.org/abs/2408.02549)|null|本文探讨了在6G网络中部署基础模型的创新边缘-云架构。具体目标是通过无线电资源分配和任务卸载来最小化基础模型的服务延迟。主要分为三部分：首先，介绍通信系统模型，即分配无线电资源并计算支持生成内容传输的链路容量；其次，展示基础模型推理模型，用于计算内容生成的延迟；最后，提出了一种新颖的上下文学习方法来优化任务卸载决策。该方法利用基础模型的推理能力，避免了传统机器学习算法中需要专门模型训练或微调的困难。仿真结果表明，提出的边缘-云部署与上下文学习任务卸载方法可以在无需专门模型训练或微调的情况下，实现满意的生成服务质量。|
|**2024-08-05**|**RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation**|Daniel Fleischer et.al.|[2408.02545](http://arxiv.org/abs/2408.02545)|**[link](https://github.com/intellabs/ragfoundry)**|实施检索增强生成（RAG）系统固有地复杂，需要深入了解数据、应用场景以及细致的设计决策。此外，评估这些系统带来了重大挑战，需要通过多维度的方法评估检索准确性和生成质量。我们引入了RAG Foundry，这是一个开源框架，用于在RAG场景中增强大型语言模型的数据。RAG Foundry将数据创建、训练、推理和评估整合到一个工作流程中，从而为在RAG设置中训练和评估大型语言模型创建数据增强集提供了便利。这种整合使得快速原型设计和RAG技术的实验变得容易，允许用户轻松生成数据集并使用内部或专门的知识源训练RAG模型。我们通过使用多种RAG配置对Llama-3和Phi-3模型进行增强和微调，在三个知识密集型数据集上展示了持续改进的有效性。代码作为开源发布在https://github.com/IntelLabs/RAGFoundry。|
|**2024-08-05**|**Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions**|Xinbei Ma et.al.|[2408.02544](http://arxiv.org/abs/2408.02544)|**[link](https://github.com/xbmxb/EnvDistraction)**|本文探讨了多模态大型语言模型（MLLM）代理在图形用户界面（GUI）环境中的忠诚度问题，旨在解决以下研究问题：多模态GUI代理是否可能被环境背景分散注意力。我们提出了一种通用设置，其中用户和代理均为善意角色，而环境虽非恶意，但包含与任务无关的内容。通过我们的模拟数据集，对多种MLLM作为GUI代理进行评估，按照三种不同的工作模式，即具有不同程度感知能力的模式进行。实验结果表明，即便是最强大的模型，无论是通用型代理还是专门用于GUI的代理，都容易受到干扰。虽然近期的研究主要关注多模态代理的动作准确性（即帮助性），但我们的发现揭示了这些代理在面对环境干扰时表现出不忠行为的可能性。此外，我们从对抗性视角出发，实施环境注入策略，展示出利用这种不忠行为可能导致的意外风险。|
|**2024-08-05**|**Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph**|Zhao Kaichen et.al.|[2408.02535](http://arxiv.org/abs/2408.02535)|null|视觉语言导航（VLN）是智能体领域的重要研究之一，旨在使智能体理解周围环境并完成导航任务。在VLN任务中，指令可以分为粗粒度和细粒度两种类型。细粒度指令详细描述了整个任务的步骤，而粗粒度指令则提供了一个抽象的任务描述，更适合人类的习惯。现有的大部分工作都集中在对细粒度指令的研究上，忽视了日常生活中存在的抽象指令。为了克服这一挑战，我们尝试通过事件知识增强的方式考虑VLN中的粗粒度指令。具体来说，我们首先提出了一种基于提示的方法来整合多个主流基准数据集，形成一个全面的事件知识图谱（命名为VLN-EventKG）。通过小规模和大规模语言模型的合作，我们实现了能够处理粗粒度指令输入的事件导航（EventNav）方法，用于VLN任务中的导航规划。此外，我们设计了一个新颖的动态历史回溯模块，能够在实时中纠正潜在的错误动作规划。在各种公共基准上的实验结果表明，使用我们提出的VLN-EventKG的知识增强方法，在使用粗粒度指令的VLN任务中具有超过5%的成功率优势。我们的项目可以在<https://sites.google.com/view/vln-eventkg> 上访问。|
|**2024-08-05**|**Practical Attacks against Black-box Code Completion Engines**|Slobodan Jenko et.al.|[2408.02509](http://arxiv.org/abs/2408.02509)|null|本文提出了一种名为INSEC的新型攻击方法，旨在引导基于大型语言模型的代码补全引擎生成存在安全漏洞的代码。这种攻击方式与市面上大多数商业补全引擎（如GitHub Copilot）相似，仅需要黑盒查询访问目标引擎，无需了解其内部机制。攻击策略通过在补全输入中插入恶意攻击字符串作为简短注释来实施。为了设计出有效的攻击字符串，我们构建了一系列专门的初始化方案，并通过优化过程进一步精炼。我们在开源模型、黑盒商业服务（如OpenAI API和GitHub Copilot）以及五种编程语言下的16个关键错误类别上验证了INSEC的有效性。实验结果表明，与现有技术相比，INSEC显著提高了考虑中的补全引擎生成不安全代码的可能性超过50%，同时仍具备生成功能正确代码的能力。此外，我们的攻击方法资源需求较低，开发成本低于十美元，可在普通硬件上运行。|
|**2024-08-02**|**Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting**|Xiangyu Zhao et.al.|[2408.01423](http://arxiv.org/abs/2408.01423)|null|大型语言模型（LLM）在自然语言处理（NLP）领域展现出了惊人的能力，在执行各种任务时表现出色。然而，这些模型的性能受到特定提示设计策略的影响。主要有两种提示设计方法：一种是通过手动为特定数据集创建专门的提示，被称为专家设计提示（EDP），一旦创建，它们就无法更改，其有效性受限于人类设计者的专业知识。当应用于LLM时，这种固定的方法导致对简单问题和复杂问题采用统一的解决策略，导致对于简单问题过度使用令牌。另一种方法是让LLM自动生成提示，称为LLM衍生提示（LDP），能够针对具体问题提供定制解决方案，从而减轻了EDP的局限性。然而，LDP在处理复杂问题时可能会遇到性能下降的问题，这是因为在解决问题规划过程中可能累积错误。  为了解决这些挑战，我们提出了一个新颖的提示递归搜索（PRS）框架，该框架利用LLM生成针对特定问题的解决方案，同时减少令牌的使用。这个框架包含了对问题复杂性的评估以及可调整的结构，以降低出错的可能性。我们通过使用不同参数数量的LLM模型在多个领域内的多种数据集上进行了广泛的实验，验证了PRS框架的有效性。与链式思考（CoT）方法相比，PRS方法在使用Llama3-7B模型时，BBH数据集上的准确率提高了8%，实现了22%的改进。|
|**2024-08-02**|**Mission Impossible: A Statistical Perspective on Jailbreaking LLMs**|Jingtong Su et.al.|[2408.01420](http://arxiv.org/abs/2408.01420)|null|大型语言模型（LLM）在有限的质量控制下训练于海量文本数据中。这导致LLM可能出现意外甚至有害的行为，如泄露信息、假新闻或仇恨言论。应对策略，通常称为偏好对齐，包括通过精心设计的文本示例精细调整预训练的LLM，以体现期望的行为模式。然而，实证研究表明，即使进行了偏好对齐，LLM也仍可能诱骗至有害行为。这种被称为LLM“越狱”的现象通常通过修改输入提示来实现，以误导LLM。本文从统计学的角度提供对偏好对齐和越狱现象的理论洞察。  在我们的框架下，首先证明了如果训练语料库中存在有害行为，预训练的LLM会模仿这种行为。同样基于这个框架，我们引入了一种统计意义上的对齐概念，并给出了越狱概率的下界，表明在合理假设下，这种现象是无法避免的。基于我们的见解，我们提出了一种对当前普遍采用的对齐策略——强化语言引导反馈（RLHF）的改进。具体来说，我们引入了一个名为E-RLHF的简单修改版RLHF目标，旨在提高安全响应的可能性。E-RLHF不会增加额外的训练成本，且与其它方法兼容。实验结果表明，在不牺牲MT-Bench项目衡量的模型性能的情况下，E-RLHF在AdvBench和HarmBench项目提出的所有对齐问题上均优于RLHF。|
|**2024-08-02**|**DebateQA: Evaluating Question Answering on Debatable Knowledge**|Rongwu Xu et.al.|[2408.01419](http://arxiv.org/abs/2408.01419)|**[link](https://github.com/pillowsofwind/debateqa)**|大型语言模型（LLM）的兴起使得我们能够探讨关于LLM聊天机器人上固有争议性问题的答案，这需要一种可靠的方式来评估它们的能力。然而，传统问答基准假设固定的答案对此目的而言是不足的。为了应对这一挑战，我们引入了DebateQA，这是一个包含2,941个争议性问题的数据集，每个问题都附带了多个由人类注释的片段答案，这些片段答案捕捉了各种视角。我们开发了两个度量标准：观点多样性，用于评估视角的全面性；以及争议意识，用于评估LLM是否认识到问题的争议性。实验结果表明，这两个度量标准与人类偏好一致，并且在不同基础模型之间具有稳定性。通过使用DebateQA和这两个度量标准，我们评估了12种流行的LLM和检索增强生成方法。我们的发现揭示了虽然LLM通常擅长识别争议性问题，但它们提供全面答案、涵盖多样视角的能力存在显著差异。|
|**2024-08-02**|**Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs**|Yilun Hua et.al.|[2408.01417](http://arxiv.org/abs/2408.01417)|null|人类在对话过程中会自发地使用越来越高效的语言，通过适应并形成自定义的约定。这一现象已经通过参考游戏进行了广泛的研究，展示了人类语言超越传达意图的特性。目前，我们尚未探索多模态大型语言模型（MLLM）是否在交互中同样提高了沟通效率，并且它们可能采用何种机制实现这一目的。  我们引入了ICCA框架，这是一个自动化的评估方法，用于在MLLM中评估此类对话适应作为上下文行为的能力。我们对几种最先进的MLLM进行了评估，观察到虽然它们可能理解其对话伙伴的语言越来越高效，但它们本身并不自发地在时间上使自己的语言变得更高效。这种能力仅在某些模型（如GPT-4）中可以通过强烈的提示来激发。这表明，即使这是人类语言的常见特征，当前的训练制度并不能产生这一互动属性。  ICCA框架已开源发布于https://github.com/lil-lab/ICCA。|
|**2024-08-02**|**Coalitions of Large Language Models Increase the Robustness of AI Agents**|Prattyush Mangal et.al.|[2408.01380](http://arxiv.org/abs/2408.01380)|null|大型语言模型（LLM）的兴起从根本上改变了我们与数字系统互动的方式，并推动了对借助于大规模语言模型（LLM）的AI代理以辅助日常流程的研究。尽管LLM具有强大的能力并能够表现出一些涌现特性，但它们并非逻辑推理者，往往在AI代理执行工作流程时所涉及的所有子任务上表现不佳。现有研究通过大规模的一般性预训练或针对工具使用进行专门的微调来解决这一问题，而我们评估了一个由专注于特定子任务的预训练模型组成的联盟是否能与单一模型代理的表现相匹敌。联盟模型的方法展示了其在构建鲁棒性和降低这些AI代理运行成本方面的潜力，通过利用特定模型展现的特性。我们的发现表明，通过考虑一组预训练模型，可以减轻微调的需求，并相信这种方法可以应用于其他利用LLM的非代理系统。|
|**2024-08-02**|**Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation**|Jheng-Hong Yang et.al.|[2408.01363](http://arxiv.org/abs/2408.01363)|null|### 摘要  本文对视觉语言模型（VLMs）在进行相关性评估方面的潜力进行了探索。通过设计一个针对多媒体内容创作的大型零样本检索任务，评估了CLIP、LLaVA和GPT-4V等VLM的性能。初步实验结果如下：  1. **性能比较**：在与人类判断的相关性上，LLaVA和GPT-4V（包括开源和专有视觉指令调优的大规模语言模型）取得了显著的Kendall’s τ≈0.4的成绩，超过了CLIPScore指标。  2. **偏好与偏见**：尽管CLIPScore表现突出，但LLMs在偏见方面相对较少倾向于基于CLIP的检索系统。  3. **一致性分析**：GPT-4V的评分分布与人类判断更为一致，其Cohen’s κ值约为0.08，远高于CLIPScore的约-0.096。这一发现表明，基于LLM的VLM在增强相关性评估方面具有潜力。  ### 结论  本研究揭示了视觉语言模型在相关性评估任务中的应用价值，特别是当它们被用于零样本检索任务时。通过比较不同模型的性能，研究强调了LLMs在多媒体内容创建领域内的潜在优势，并指出了它们在提升内容相关性判断方面的可能性。|
|**2024-08-02**|**Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs**|Peng Ding et.al.|[2408.01355](http://arxiv.org/abs/2408.01355)|**[link](https://github.com/njunlp/hallu-pi)**|多模态大语言模型在视觉语言理解与生成任务上展现出卓越性能。然而，这些模型偶尔会产生与给定图像不一致的内容，即所谓的“幻觉”。先前的研究主要集中在使用标准、未扰动基准评估幻觉上，这忽视了现实世界场景中普遍存在的扰动输入（如图像裁剪或模糊），这是对多模态大语言模型幻觉全面评估的关键。  本篇论文旨在填补这一空白，提出了Hallu-PI，首个专门用于评估多模态大语言模型在扰动输入下的幻觉的基准。Hallu-PI包含了7种扰动情景，涉及1,260张来自11种物体类型的扰动图像。每张图像都附有详细的注释，包括精细粒度的幻觉类型，如存在性、属性和关系等。这些注释配备了一个丰富的问答集，使Hallu-PI适用于辨别性和生成性任务。  在对主流多模态大语言模型（如GPT-4V和Gemini-Pro Vision）进行的广泛实验中，我们发现这些模型在Hallu-PI上的表现显示出显著的幻觉，而在未扰动场景中未观察到此类现象。我们的研究还揭示了多模态大语言模型处理不同类型幻觉时存在的严重偏差。  为此，我们设计了两个专门针对扰动情景的基线，分别为Perturbed-Reminder和Perturbed-ICL。我们希望这项研究能引起研究人员对多模态大语言模型在处理扰动输入时局限性的关注，并激发进一步的调查以解决这一问题。我们的代码和数据集已在GitHub（https://github.com/NJUNLP/Hallu-PI）上公开提供。|
|**2024-08-02**|**MCGMark: An Encodable and Robust Online Watermark for LLM-Generated Malicious Code**|Kaiwen Ning et.al.|[2408.01354](http://arxiv.org/abs/2408.01354)|**[link](https://github.com/KevinHeiwa/MCGTM)**|随着大型语言模型（LLM）的兴起，众多软件服务提供商（SSP）致力于开发针对代码生成任务的定制化LLM，如CodeLlama和Copilot。然而，这些LLM有可能被攻击者利用来生成恶意软件，对软件生态系统构成潜在威胁，例如自动化高级网络钓鱼恶意软件的创建。为应对这一挑战，我们首先进行了一项实证研究，并设计了一个包含约400小时工作量、共计406个恶意代码生成任务的提示数据集MCGTest。利用这个数据集，我们提出了MCGMark，这是首个能够实现稳健、结构感知且可编码的水印方法，用于追踪由LLM生成的恶意代码。我们通过控制令牌选择和基于概率异常值确保输出质量来嵌入可编码信息。此外，我们通过考虑恶意代码的结构特征增强了水印的鲁棒性，避免在易于修改的位置（如注释）嵌入水印。我们使用DeepSeek-Coder验证了MCGMark的有效性和鲁棒性，其最大输出限制为400个令牌时，嵌入成功率达到了88.9%。同时，该方法也展示了强大的鲁棒性，并对输出代码的质量影响极小。我们的方法帮助SSP追踪并追究由LLM生成的恶意代码的源头及责任。|
|**2024-08-02**|**Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks**|Anders Giovanni Møller et.al.|[2408.01346](http://arxiv.org/abs/2408.01346)|null|大型语言模型是促进社会计算领域复杂文本理解任务的有力工具。它们的多功能性虽然有益，但也带来了在该领域建立标准化最佳实践的障碍。为了提供不同策略价值的清晰度，我们概述了现代基于LLM的分类方法在23个社会知识任务基准上的性能。我们的结果指出了三个最佳实践：选择具有更大词汇量和预训练语料库的模型；避免简单的零次尝试，而倾向于增强提示的人工智能方法；在特定任务数据上进行微调，并考虑在多个数据集上使用更复杂的指令调整，仅当训练数据更为丰富时才这样做。  请注意，这段翻译文本中并未包含任何", "字符。|
|**2024-08-02**|**A Backbone for Long-Horizon Robot Task Understanding**|Xiaoshuai Chen et.al.|[2408.01334](http://arxiv.org/abs/2408.01334)|null|为了应对长时程任务中端到端机器人学习的不可预测性与泛化能力差的问题，我们提出了一种基于Therblig的骨架框架（TBBF），旨在增强机器人任务理解与转移能力。此框架利用Therblig（基本动作元素）作为骨架，将高级机器人任务分解为基本机器人配置，然后结合当前的基础模型来提升任务理解。  该方法包含两个阶段：离线训练与在线测试。在离线训练阶段，我们开发了Meta-RGate SynerFusion（MGSF）网络，用于跨任务精确的Therblig分割。在线测试阶段，通过收集新任务的一次演示，MGSF网络提取高阶知识，并通过Action Registration（ActionREG）编码入图像。此外，我们采用Large Language Model（LLM）-Alignment Policy for Visual Correction（LAP-VC）来确保精确的动作执行，从而在新型机器人场景中实现轨迹转移。  实验结果证实了这些方法的有效性，Therblig分割达到了94.37%的召回率，在真实世界中的在线机器人测试中，对于简单和复杂场景的成功率分别达到了94.4%和80%。补充材料可在以下网站获取：https://sites.google.com/view/therbligsbasedbackbone/home|
|**2024-08-01**|**AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation**|Mengkang Hu et.al.|[2408.00764](http://arxiv.org/abs/2408.00764)|null|大型语言模型（LLM）基于的代理已引起广泛关注并变得越来越流行。此外，规划能力是LLM基于代理的关键组成部分，涉及与环境的交互和执行动作以完成规划任务，通常包括从初始状态达到预期目标的过程。本文研究了通过指令调整增强LLM规划能力的方法，称为代理训练。近期的研究表明，利用专家级轨迹对指令调整LLM能有效提升其规划能力。然而，现有工作主要集中在从手动设计的任务和环境中合成轨迹。创建这些环境和任务的劳动密集型过程限制了生成足够多样性和广泛性的轨迹的能力。为解决这一局限性，本文探索了自动合成多样化环境以及规划任务的渐进范围，从简单到复杂。我们引入了一个框架AgentGen，利用LLM首先生成环境，随后根据这些环境生成规划任务。具体来说，为了提高环境多样性，我们提出了使用包含各种领域特定文本段落的灵感语料库作为合成环境的上下文。此外，为了增加生成规划任务难度多样性的程度，我们提出了一种双向演化方法Bi-Evol，该方法从容易和困难的两个方向进化规划任务，以合成具有平滑难度曲线的任务集。来自AgentBoard的评估结果显示，AgentGen显著提高了LLM的规划能力，例如，使用AgentGen指令调整的Llama-3 8B在总体性能上超越了GPT-3.5。此外，在某些任务中，它甚至超越了GPT-4。|
|**2024-08-01**|**Tamper-Resistant Safeguards for Open-Weight LLMs**|Rishub Tamirisa et.al.|[2408.00761](http://arxiv.org/abs/2408.00761)|**[link](https://github.com/rishub-tamirisa/tamper-resistance)**|快速发展的大型语言模型（LLM）能力引发了对潜在恶意用途的广泛担忧。针对开放权重的LLM，现有保护措施在抵抗篡改攻击方面缺乏足够的稳定性，这些攻击可以通过微调步骤轻易地移除拒绝和遗忘保护措施。这类漏洞要求采取新的方法来确保安全释放开放权重的LLM。  我们开发了一种名为TAR的方法，旨在将不可篡改的安全防护融入到开放权重的LLM中，使得即使经过数千步的微调，攻击者也无法移除这些防护措施。在全面的评估和红队测试分析中，我们的方法显著提高了防护的不可篡改性，同时保持了良性功能。我们的结果表明，不可篡改性是一个可行的问题，为改进开放权重LLM的安全性和安全性开辟了有前景的新途径。|
|**2024-08-01**|**DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency**|Jovan Stojkovic et.al.|[2408.00741](http://arxiv.org/abs/2408.00741)|null|快速发展的大型语言模型（LLMs）的生成能力使其在各种应用中成为关键的工作负载。如今，LLM推理集群处理大量查询，并对服务质量指标（SLOs）有严格要求。为了达到预期性能，这些模型在能耗高的GPU上执行，导致推理集群消耗大量能源，并产生过量的碳排放。幸运的是，我们发现可以通过利用推理计算特性的异质性以及工作负载的波动，显著提高能效。然而，这种多样性和动态环境创造了一个巨大的搜索空间，不同的系统配置（如实例数量、模型并行性和GPU频率）导致不同的能源和性能折衷。  为了解决这些挑战，我们提出了DynamoLLM，这是首个针对LLM推理环境的能效管理框架。DynamoLLM自动且动态地重新配置推理集群，以优化能源和成本，同时满足服务的性能SLOs。研究表明，在服务层面，DynamoLLM能够节省53%的能源和38%的操作碳排放，并为客户减少61%的成本，同时仍能满足延迟SLOs。|
|**2024-08-01**|**Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions**|Guangzhi Xiong et.al.|[2408.00727](http://arxiv.org/abs/2408.00727)|**[link](https://github.com/teddy-xionggz/medrag)**|大型语言模型（LLM）展现出了解决医疗问题的巨大潜力，它们能够掌握大量医学知识，但仍然可能出现幻觉，并且在知识更新方面具有局限性。为了增强LLM在医学问答方面的能力，提出了基于检索的生成（RAG）方法，通过外部知识库来提升性能。然而，在需要多次信息查询的复杂情况下，RAG可能仍然会失败。为解决这一问题，我们提出了一种迭代RAG方法（i-MedRAG），允许LLM在每次尝试后迭代地提出后续问题。在每次i-MedRAG迭代中，后续问题由基本的RAG系统回答，并用于指导下一个迭代中的查询生成。  实验结果显示，与仅使用RAG的传统方法相比，i-MedRAG显著提高了各种LLM在复杂问题上的性能，这些问题是美国医学生执照考试（USMLE）临床案例和大规模多任务语言理解（MMLU）数据集中的知识测试所涵盖的。特别值得注意的是，我们的零样本i-MedRAG在GPT-3.5上取得了69.68%的准确性，超越了所有现有的提示工程和微调方法在MedQA数据集上的表现。此外，我们还研究了i-MedRAG在不同迭代次数和每迭代查询数量下的扩展特性。  我们的案例研究显示，i-MedRAG能够灵活地提出后续问题形成推理链，深入分析医疗问题。据我们所知，这是首次将后续问题融入医学RAG的研究。|
|**2024-08-01**|**An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models**|Yangzhen Wu et.al.|[2408.00724](http://arxiv.org/abs/2408.00724)|null|在大规模语言模型（LLM）的最优训练配置研究中，特别是在模型规模和计算预算方面的配置，已经进行了大量的探讨。然而，对于推理阶段如何最优化配置LLM以平衡额外的推理计算时间和性能提升的研究还不够深入。本文旨在探索计算优化的推理方法，即设计能够通过调整推理时间的计算量来优化性能的模型和推理策略。  为了理解并设计计算优化的推理方法的第一步，我们对多种推理策略，如贪心搜索、多数投票、最佳N种组合、加权投票及其变体，在两种不同的树搜索算法中进行了评估，涉及不同模型规模和计算预算。我们的研究发现，较小的语言模型配合更先进的解码算法通常能实现帕累托最优的权衡，即在额外的计算成本与性能提升之间找到最佳平衡点。这些结果表明，在预算有限的场景下，如终端设备上部署小型模型，可能具有显著的优势，以提高问题解决的准确率。  例如，我们展示了Llemma-7B模型在使用约两倍于Llemma-34B模型的浮点运算（FLOPs）的情况下，仍能实现与后者相当的MATH500任务准确性。我们的发现可能适用于任何有明确成功度量标准的生成任务。|
|**2024-08-01**|**Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities**|Sunder Ali Khowaja et.al.|[2408.00722](http://arxiv.org/abs/2408.00722)|null|近期，大型语言模型（LLMs）因其在新兴应用中的适应性和可扩展性而备受关注，这些应用包括通信网络。预计6G移动边缘计算网络将能够作为服务支持LLMs，因为它们提供超可靠的低延迟通信和闭环大规模连接。然而，LLMs在数据和模型隐私方面存在漏洞，这影响了在用户服务中部署LLMs的信任度。本文探讨了在6G网络中对LLMs进行微调时的安全漏洞，特别是成员归属攻击。我们定义了攻击网络的特征，该网络可以在访问下游任务细调模型时执行成员归属攻击，前提是攻击者可以访问该模型。我们表明，对于任何下游任务，成员归属攻击都是有效的，这可能导致在使用LLMs作为服务时发生个人数据泄露。实验结果显示，在命名实体识别任务上，攻击成功率可达92%。基于实验分析，我们讨论了可能的防御机制，并提出了可能的研究方向，以使在6G网络背景下LLMs更加可靠。|
|**2024-08-02**|**Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning**|Trapoom Ukarapol et.al.|[2408.00690](http://arxiv.org/abs/2408.00690)|**[link](https://github.com/trapoom555/language-model-sts-cft)**|在自然语言理解任务上展现出卓越性能的大型语言模型因资源密集型的特点而降低了其可获取性。相比之下，小型语言模型如MiniCPM提供了更可持续的扩展性，但往往在没有专门优化的情况下表现不佳。本文旨在通过提升小型语言模型的文本嵌入质量来增强它们的表现。我们选择了三个语言模型：MiniCPM、Phi-2和Gemma，在NLI数据集上进行对比式微调。研究结果表明，这种方法能显著提升所有三种模型在各种基准测试中的文本嵌入质量，其中MiniCPM表现出最显著的平均56.33%性能提升。对比式微调的代码已公开在https://github.com/trapoom555/Language-Model-STS-CFT。|
|**2024-08-01**|**Can Developers Prompt? A Controlled Experiment for Code Documentation Generation**|Hans-Alexander Kruse et.al.|[2408.00686](http://arxiv.org/abs/2408.00686)|null|我们对20名专业人士和30名计算机科学学生进行了一个受控实验，要求他们使用ChatGPT风格的Visual Studio Code扩展来为两个Python函数编写代码文档。实验组自由输入自定义提示，而对照组则执行预设的少量提示。我们的结果表明，无论是专业人士还是学生，都对或无法应用提示工程技巧感到不知所措。尤其是学生，他们认为从自定义提示生成的文档比从准备好的提示生成的文档在可读性、简洁性和有用性方面显著较差。一些专业人士仅通过在自定义提示中加入“Docstring”关键词就能生成更高质量的文档。学生希望获得更多的指导来制定提示，而专业人士则更欣赏自定义提示的灵活性。参与者普遍认为输出并非完美，而是将其视为逐步完善文档的工具。需要进一步的研究来理解开发人员具有的提示技巧和偏好，以及他们完成特定任务所需的支援。|
|**2024-08-01**|**AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models**|Daqin Luo et.al.|[2408.00665](http://arxiv.org/abs/2408.00665)|**[link](https://github.com/tim120526/AutoM3L)**|### 摘要  本文提出了一种创新的多模态机器学习自动化框架——AutoM3L，该框架利用大型语言模型（LLMs）作为控制器，自动构建多模态训练管道。AutoM3L能够理解数据模态并根据用户需求选择合适的模型，提供自动化和互动性。通过消除手动特征工程和超参数优化的需求，我们的框架简化了用户参与过程，并通过指令提供了定制化选项，从而解决了以往基于规则的自动机器学习方法的局限性。  我们对AutoM3L在六个不同类型的多模态数据集上进行了评估，涵盖了分类、回归和检索任务，以及一系列广泛的单模态数据集。实验结果表明，AutoM3L在性能上与传统的基于规则的自动机器学习方法相比具有竞争力或超越性。此外，用户研究进一步验证了AutoM3L在用户友好性和易用性方面的优势，相较于基于规则的自动机器学习方法。|
|**2024-08-01**|**Disentangling Dense Embeddings with Sparse Autoencoders**|Charles O'Neill et.al.|[2408.00657](http://arxiv.org/abs/2408.00657)|null|我们提出了一种应用稀疏自动编码器（SAEs）到大型语言模型生成的密集文本嵌入的首次尝试，展示其在解缠语义概念方面的潜力。通过在超过42万篇计算机科学和天文学领域科学论文摘要的嵌入上训练SAEs，我们展示了所得到的稀疏表示保持了语义一致性的同时提供了可解释性。我们分析这些学习特征，探索不同模型容量下它们的行为，并引入了一种新的方法来识别“特征家族”，这些特征代表了不同抽象级别的相关概念。为了展示我们的方法的实际应用价值，我们展示了如何使用这些可解释特征精确控制语义搜索，从而实现对查询语义的精细控制。这项工作填补了密集嵌入的语义丰富性和稀疏表示的可解释性之间的差距。我们开源了训练后的嵌入、稀疏自动编码器以及可解释特征，同时提供了一个用于探索它们的网页应用程序。|
|**2024-07-31**|**Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs**|Shi Liu et.al.|[2407.21771](http://arxiv.org/abs/2407.21771)|null|现有大视觉语言模型（LVLM）主要通过将视觉编码器的图像特征与大型语言模型（LLM）对齐，利用其强大的文本生成能力。然而，视觉编码器与语言模型之间的规模差异可能导致LLM在多模态理解中占据主导地位。这种LVLM中的不平衡可能引发幻觉现象。具体来说，LVLM可能生成一致的描述，无论是否有视觉输入，这表明某些输出仅受上下文文本的影响。我们将这种现象称为“文本惯性”。为了对抗这一问题，我们提出了一种无需训练的算法来寻找图像理解和语言推断之间的平衡点。具体地，我们动态调整并放大分配给图像令牌的注意力权重，从而赋予视觉元素更大的重要性。同时，我们从多模态输入的logits中减去纯文本输入的logits，有助于LVLM避免过分依赖LLM。通过增强图像令牌并减少LLM的顽固输出，我们可以让LVLM更多地关注图像，从而缓解文本惯性和减少LVLM中的幻觉。我们的广泛实验显示，在不同指标下，这种方法显著减少了各种LVLM中的幻觉输出频率。项目页面可访问：https://lalbj.github.io/projects/PAI/。|
|**2024-07-31**|**ReplanVLM: Replanning Robotic Tasks with Visual Language Models**|Aoran Mei et.al.|[2407.21762](http://arxiv.org/abs/2407.21762)|null|大型语言模型（LLM）在机器人任务规划领域获得了越来越多的关注，这主要得益于它们在文本分析与生成、以及对世界广泛知识方面的出色能力。然而，它们在解析视觉线索方面的能力有限，无法直接感知世界状态，这导致了在描述当前世界状态上的不足。相比之下，视觉语言模型（VLM）通过集成视觉感知模块，填补了这一空白，增强了机器人的自主性。尽管如此，VLM仍面临挑战，例如，在提供准确指令的情况下，任务执行错误的风险依然存在。  为了应对这些问题，本文提出了一种用于机器人任务规划的ReplanVLM框架。该研究重点在于错误修正干预措施。提出了内部错误修正机制和外部错误修正机制，在相应的阶段进行错误纠正。发展了一种重规划策略，当任务执行失败时，用于重新规划任务或修正错误代码。在真实机器人和仿真环境中进行的实验结果表明，所提出的框架具有更高的成功率和更强的开放世界任务中的错误修正能力。有关实验的视频可以在https://youtu.be/NPk2pWKazJc找到。|
|**2024-07-31**|**Adaptive Retrieval-Augmented Generation for Conversational Systems**|Xi Wang et.al.|[2407.21712](http://arxiv.org/abs/2407.21712)|null|尽管在对话系统开发中融入大型语言模型取得了成功，但许多研究显示了检索增强生成（RAG）对于提供信息性响应的有效性。因此，现有研究通常假设对话系统中的每次回复都需要检索增强，而无需明确控制。这引发了一个关于这种必要性的研究问题。本研究旨在探索系统回应是否需要使用外部知识进行增强的必要性。通过利用人类对是否需要适应性增强的二元选择进行判断，我们开发了RAGate——一个闸门模型，该模型通过分析对话上下文和相关输入来预测对话系统是否需要RAG以获得改进的回复。我们在构建和应用RAGate到对话模型以及对不同对话场景进行详尽分析方面进行了广泛的实验。我们的实验结果和分析表明，RAGate在识别需要RAG以生成高质量回复并具有高生成置信度的系统响应方面有有效应用。这项研究还发现了生成置信度水平与增强知识的相关性。|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708](http://arxiv.org/abs/2407.21708)|null|本研究提出了一种方法，旨在通过利用已标注文本语料库和从Chebi获取的知识，增强现有知识，并对大型语言模型（LLM）进行微调，以识别科学文献中的化学实体及其作用。实验结果证明了这种方法的有效性。通过结合本体论知识与LLM的语言理解能力，我们实现了在科学文献中识别化学实体及其作用的高精确度和召回率。进一步地，我们从8000篇ChemRxiv文章中提取这些实体和角色，然后使用第二个LLM构建了一个化学实体和角色的知识图谱（CEAR），该图谱不仅为ChEBI提供了补充信息，还能帮助扩展其内容。|
|**2024-07-31**|**TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**|Ming Zhang et.al.|[2407.21693](http://arxiv.org/abs/2407.21693)|**[link](https://github.com/konglonggefdu/transfertod)**|任务导向对话（TOD）系统旨在有效处理任务导向的对话，包括信息收集。如何准确、高效且有效地利用TOD进行信息收集一直以来都是一个关键且具有挑战性的任务。近期的研究表明，大型语言模型（LLMs）在对话、指令生成和推理方面表现出色，并能够通过微调显著提高TOD性能。然而，当前的数据集主要针对用户驱动的系统，并局限于预定义的特定场景和槽位，因此需要在TOD的主动性、多样性和能力方面进行改进。本研究介绍了一个多领域任务导向对话数据构建过程以及基于此过程生成的中文对话数据集——\textbf{TransferTOD}，该数据集真实模拟了在30个流行生活服务场景中的人机对话。利用这个数据集，我们训练了一个使用全参数微调的\textbf{TransferTOD-7B}模型，展示了在各种下游场景中的显著的填槽能力和提问能力。我们的工作证明了其在不同数据应用场景下的强大泛化能力，显著提高了数据使用效率和系统性能。数据已发布于https://github.com/KongLongGeFDU/TransferTOD。|
|**2024-07-31**|**Synth-Empathy: Towards High-Quality Synthetic Empathy Data**|Hao Liang et.al.|[2407.21669](http://arxiv.org/abs/2407.21669)|**[link](https://github.com/aurora-slz/synth-empathy)**|近年来，随着大型语言模型（LLM）的迅速发展，实现出色同理心响应能力已成为一个至关重要的前提。因此，管理和理解同理心数据集的重要性日益凸显。然而，同理心数据通常由人类标注，导致数据量不足和大量的人力浪费。为此，我们提出了一种名为Synth-Empathy的LLM基于的数据生成与质量、多样性选择管道，该管道能够自动生成高质量的同理心数据并筛选掉低质量数据。通过利用低同理心模型生成的数据，我们进一步提高了同理心响应性能，并在多个基准上达到了最先进的（SoTA）结果。此外，我们的模型在各种人类评估基准上均表现出色，证明了其在实际应用中的有效性和鲁棒性。进一步地，我们展示了数据量与质量之间的权衡，提供了同理心数据生成与选择方面的见解。|
|**2024-07-31**|**LLM-for-X: Application-agnostic Integration of Large Language Models to Support Personal Writing Workflows**|Lukas Teufelberger et.al.|[2407.21593](http://arxiv.org/abs/2407.21593)|null|为了提高生产力并优化工作流程，将大型语言模型（LLM）功能嵌入应用程序的趋势正在增长，从基于浏览器的网络应用到在个人计算机上运行的原生应用。我们介绍了一种系统级快捷方式层——LLM-for-X，它通过轻量级弹出式对话框无缝地向任何应用程序添加LLM服务。我们的原生层通过统一的聊天前端作为编程接口或自定义API调用，将前端应用程序与流行的LLM后端（如ChatGPT和Gemini）无缝连接。我们展示了LLM-for-X在Microsoft Office、VSCode、Adobe Acrobat以及Overleaf等流行网络应用中的优势。在评估中，我们将LLM-for-X与ChatGPT的网页界面进行了任务比较，证明了我们的方法能够提供快速、高效且易于使用的LLM辅助，无需切换上下文支持写作和阅读任务，同时对特定应用无特定依赖。|
|**2024-07-31**|**A Performance Study of LLM-Generated Code on Leetcode**|Tristan Coignion et.al.|[2407.21579](http://arxiv.org/abs/2407.21579)|null|本文研究了大型语言模型（LLM）在代码生成方面的效率，并使用来自LeetCode的数据集评估了它们与人类编写的解决方案的性能。我们对比了18个LLM，考虑了模型温度和成功率等因素对代码性能的影响。研究引入了一种新颖的方法来度量和比较LLM生成代码的速度，结果表明，采用不同LLM时，生成的代码性能相当。我们还发现，LLM生成的代码平均而言比人类编写的代码更高效。论文进一步讨论了使用LeetCode作为基准数据集、潜在数据污染带来的限制以及平台测量可靠性的问题。我们认为，我们的发现有助于更好地理解LLM在代码生成领域的能力，并为该领域未来的优化奠定了基础。|
|**2024-07-31**|**PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**|Min Jae Jung et.al.|[2407.21571](http://arxiv.org/abs/2407.21571)|null|大型语言模型（LLM）在持续学习过程中遇到重大挑战，主要在于灾难性遗忘现象，即新信息会覆盖之前获得的知识。这一局限性导致了大量环境和经济资源的浪费。本研究引入了一种名为PMoE（Progressive Mixture of Experts with Asymmetric Transformer）的解决方案，旨在通过采用具有浅层用于一般知识和深层用于新知识的不对称设计来最小化遗忘。PMoE在深层引入了逐步增加的专家，并配备了一个路由器，该路由器能够高效地将新知识分配给合适的专家。  路由器位于深层附近，利用深度特征聚合已整合的信息。这使得路由器能够有效地执行任务，将新知识分配给逐步增加的深层专家。通过在TRACE数据集和通用语言理解数据集上的广泛实验，证明了所提出的PMoE方法优于先前的最先进的方法。|
|**2024-07-31**|**CXSimulator: A User Behavior Simulation using LLM Embeddings for Web-Marketing Campaign Assessment**|Akira Kasuga et.al.|[2407.21553](http://arxiv.org/abs/2407.21553)|null|本文提出了一种名为客户体验（CX）模拟器的新型框架，旨在通过用户行为模拟来评估未测试的网络营销活动的影响。该提出的框架利用大型语言模型（LLM）将用户行为历史中的各种事件，如查看商品、使用优惠券或购买商品等，表示为语义嵌入向量。我们训练了一个模型，用于从其LLM嵌入中预测事件之间的过渡，甚至可以从多样化的训练数据中学习，从而对未知事件进行泛化。在web营销应用中，我们利用这个过渡预测模型来模拟当新的营销活动或产品展示给用户时，用户可能如何反应不同。这使得我们能够消除在线测试的高昂成本，并增强营销人员揭示洞察力的能力。我们的数值评估和使用Google商品商店的大规模公共数据集进行的用户研究证明了我们框架的有效性。|
|**2024-07-30**|**ThinK: Thinner Key Cache by Query-Driven Pruning**|Yuhui Xu et.al.|[2407.21018](http://arxiv.org/abs/2407.21018)|null|大型语言模型（LLM）在自然语言处理领域引发了一场革命，通过利用更大的模型规模和序列长度，实现了前所未有的性能。然而，随之而来的计算和内存成本的增加带来了挑战，尤其是在处理长序列时，由于注意力机制的二次复杂性，对缓存内存管理提出了严峻考验。本文专注于长上下文场景，针对推理过程中KV缓存内存消耗的效率问题进行深入探讨。与现有方法侧重于基于序列长度优化内存不同，我们揭示了KV缓存通道在权重分布不均和低秩结构特征下存在显著冗余。基于这些观察结果，我们提出了一种名为ThinK的新型查询依赖型KV缓存剪枝方法，旨在最小化注意力权重损失的同时，有选择地剪枝掉最不重要的通道。我们的方法不仅能够保持或提升模型准确率，而且相比传统的KV缓存淘汰方法，能实现超过20%的内存成本减少。通过在LLaMA3和Mistral模型上对多个长序列数据集进行的广泛评估，证明了ThinK的有效性，确立了在不牺牲性能的前提下高效部署LLM的新标准。我们还展望了将我们的方法扩展到值缓存剪枝的可能性，展示了ThinK在降低内存和计算开销方面的广泛适用性和潜力。|
|**2024-07-30**|**CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning**|Yuexi Du et.al.|[2407.21011](http://arxiv.org/abs/2407.21011)|**[link](https://github.com/xypb/cleft)**|**近期，对比语言-图像预训练（CLIP）的进展在多任务自监督表示学习领域取得了显著成果。然而，现有CLIP类方法往往需要大量的GPU资源和长时间的训练周期，这主要是由于模型和数据集的规模巨大，对于医学应用而言，大规模数据集并不总是常见。同时，语言模型提示主要基于与图像关联的标签进行手动提取，可能忽视了训练样本内的丰富信息。  我们提出了一种名为“高效大语言模型与提示微调”（CLEFT）的语言-图像对比学习方法，它充分利用了广泛预训练的语义和视觉模型的优势。此外，我们还提出了一种有效策略来学习基于上下文的提示，以缩小临床诊断数据与简单类别标签之间的差距。我们的方法在多个胸部X光和乳腺X光数据集上的表现均优于各种基线，达到了最先进的性能水平。  所提出的参数高效的框架可以将总可训练模型大小减少39%，并将可训练语言模型减少到仅4%，与当前的BERT编码器相比。**|
|**2024-07-30**|**MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning**|Yupeng Chen et.al.|[2407.20999](http://arxiv.org/abs/2407.20999)|null|近期，大型语言模型（LLMs）在各种任务中展现了非凡的能力。通常，LLM通过大量语料库进行预训练，并随后针对特定任务的数据集进行微调。然而，在微调过程中，LLM可能会忘记在预训练阶段学到的知识，导致一般能力下降。为了应对这一问题，我们提出了一种新的微调算法——动量过滤优化器（MoFO）。MoFO的核心思想是迭代地选择并更新具有最大动量幅度的模型参数。与全参数训练相比，MoFO在保持参数接近预训练模型的同时实现了相似的微调性能，从而减轻了知识遗忘的问题。与现有的大多数遗忘缓解方法不同，MoFO具备以下两个优势。首先，MoFO不需要访问预训练数据。这使得MoFO特别适用于预训练数据不可用的微调场景，如使用开源LLM的检查点进行微调。其次，MoFO不会改变原始损失函数。这可以避免损害模型在微调任务上的性能。我们通过严谨的收敛性分析和广泛的实验验证了MoFO的优越性，证明了它在缓解遗忘和增强微调性能方面的优势。|
|**2024-07-30**|**From Feature Importance to Natural Language Explanations Using LLMs with RAG**|Sule Tekkesinoglu et.al.|[2407.20990](http://arxiv.org/abs/2407.20990)|**[link](https://github.com/suletekkesinoglu/xai_llm_rag)**|随着机器学习在涉及人类交互的自主决策过程中的作用日益重要，理解模型输出变得越来越关键。最近，基础模型正被探索用作事后解释器，提供了一种揭示预测模型决策机制的途径。本文介绍了一种可追踪问答方法，通过利用外部知识库来指导大型语言模型（LLM）对场景理解任务中的用户查询进行响应。该知识库包含了关于模型输出的上下文细节，包括高级特征、特征重要性以及替代概率。  我们采用减法反事实推理计算特征重要性，这是一种分析在分解语义特征后输出变化的方法。为了保持对话流畅，我们从社会科学研究中提炼出四个关键特性——社交性、因果性、选择性和对比性，并将其整合到一个即时提示中，以此指导响应生成过程。我们的评估表明，生成的解释包含了这些元素，这表明它有可能在复杂模型输出与自然语言表达之间架起桥梁。|
|**2024-07-30**|**Large Language Models (LLMs) for Semantic Communication in Edge-based IoT Networks**|Alakesh Kalita et.al.|[2407.20970](http://arxiv.org/abs/2407.20970)|null|随着第五代（5G）和第六代（6G）通信技术以及物联网（IoT）的兴起，语义通信正受到研究者的关注，因为当前的通信技术正接近香农极限。另一方面，大型语言模型（LLMs）能够理解并生成类似于人类的文本，基于对数十亿参数的广泛数据集进行训练。考虑到最近的就近计算技术如边缘计算，本文概述了一个框架及其模块，其中LLMs可以在物联网网络的网络边缘下，作为语义通信的一部分，以提高高效通信效率。最后，我们讨论了一些应用，并分析了发展此类系统的挑战和机遇。|
|**2024-07-30**|**Automated Review Generation Method Based on Large Language Models**|Shican Wu et.al.|[2407.20906](http://arxiv.org/abs/2407.20906)|**[link](https://github.com/tju-ecat-ai/automaticreviewgeneration)**|**文献研究对于科学进步至关重要，但面对海量信息的挑战，我们提出了一种基于大型语言模型（LLM）的自动化综述生成方法，旨在简化文献处理流程并减轻认知负担。以丙烷脱氢（PDH）催化剂为例，该方法从343篇文章中迅速生成了全面的综述，平均每篇文章每LLM账户耗时仅数秒。对1041篇文章的进一步分析揭示了催化剂组成、结构和性能的深入见解。  认识到LLM可能出现幻觉的问题，我们实施了多层次的质量控制策略，确保了方法的可靠性和有效缓解幻觉的能力。专家验证证实，通过这种方法生成的综述不仅准确且引文完整，LLM幻觉的风险已降至低于0.5%，置信度超过95%。发布的Windows应用程序支持一键生成综述，帮助研究人员跟踪最新进展并推荐相关文献。这一方法展示了LLM在提升科学研究生产力方面的潜力，并为进一步探索奠定了基础。**|
|**2024-07-30**|**ThinkRepair: Self-Directed Automated Program Repair**|Xin Yin et.al.|[2407.20898](http://arxiv.org/abs/2407.20898)|**[link](https://github.com/vinci-grape/ThinkRepair)**|**尽管已经提出了许多自动程序修复（APR）方法，并且在修复一些特定类型的错误时取得了显著的性能，但这些方法在处理需要对错误程序的逻辑进行分析和推理的复杂错误时仍存在局限性。最近，通过提示工程训练的大规模语言模型（LLMs）因其在解决包括错误修复在内的多种任务的强大能力而引起了广泛关注。然而，提示的质量会极大地影响LLMs的能力，而手动构建高质量的提示是一个耗时的过程。  为了解决这一限制，我们提出了一种自我导向的LLM基于自动程序修复方法ThinkRepair，它分为两个主要阶段：收集阶段和修复阶段。在收集阶段，通过使用链式思考（CoT）提示指导LLMs，自动收集构成预修复知识的各种思考链。在修复阶段，目标是通过首先选择用于少量学习的示例并其次与LLMs自动交互来修复错误，根据测试信息提供反馈（如果需要的话）。  在对两个广泛研究的数据集（Defects4J和QuixBugs）的评估中，与12个最先进的APR方法进行比较，表明ThinkRepair在修复错误方面的优先级。值得注意的是，在Defects4J V1.2上，ThinkRepair成功修复了98个错误，相较于基线提升了27%-344.4%。在Defects4J V2.0上，ThinkRepair比最先进的APR方法多修复了12-65个错误。此外，在Java和Python上，ThinkRepair在QuixBugs上的表现也有了显著提升（最多分别达到31和21）。**|
|**2024-07-30**|**Effective Black Box Testing of Sentiment Analysis Classification Networks**|Parsa Karbasizadeh et.al.|[2407.20884](http://arxiv.org/abs/2407.20884)|null|基于转换器的神经网络在自然语言处理任务如情感分析中展现了卓越性能。然而，确保这些复杂架构通过全面测试保持可靠性的挑战依然存在。本文提出了一组专门设计用于评估为基于转换器的情感分析网络构建的测试套件的覆盖标准。我们的方法采用输入空间划分的黑盒策略，考虑了与情感相关的关键语言特征，包括动词、形容词、副词和名词。为了有效地生成涵盖广泛情感元素的测试用例，我们采用了k投影覆盖度量。该度量通过一次检查k个特征的子集来减少问题的复杂性，从而降低维度。通过大型语言模型生成展示特定情感特征组合的句子。从情感分析数据集实验中获得的结果表明，我们的标准和生成的测试平均提高了16%的测试覆盖率。同时，模型准确度平均下降了6.5%，显示了识别脆弱性的能力。我们的工作为通过全面测试评估改进基于转换器的情感分析系统提供了基础。|
|**2024-07-30**|**Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification**|Boyang Zhang et.al.|[2407.20859](http://arxiv.org/abs/2407.20859)|null|近期，基于大型语言模型（LLM）的自主代理在理论研究和实际应用上均取得了显著进展。这些代理能够通过外部组件扩展基础LLM的能力，在多种方式下增强性能。例如，利用GPT-3.5-Turbo核心构建的代理可能在某些任务上超越更先进的GPT-4模型。更重要的是，工具的应用使系统能够与现实世界互动，使其从仅仅生成文本转变为执行实际操作。鉴于代理的实际应用范围以及其对环境进行操作的能力，评估潜在漏洞变得至关重要。如果被黑客入侵，这些自主系统造成的损害可能会超过单一语言模型。尽管已有研究探讨了LLM代理的有害行为，但我们的研究从不同角度审视这一问题。我们引入了一种新型攻击方法，旨在误导代理执行重复或无关的操作，从而引发故障。我们使用各种攻击手段、场景和属性进行全面评估，以确定其易感性。实验结果显示，在多个场景中，这些攻击可导致超过80%的失败率。通过在多代理环境中针对实现并部署的代理进行攻击，我们强调了此类漏洞所伴随的现实风险。为了减轻此类攻击，我们提出了自我检查检测方法。然而，我们的发现显示，仅使用LLM很难有效检测到这些攻击，这凸显了这种漏洞所带来的重大风险。|
|**2024-07-30**|**Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations**|Sarthak Anand et.al.|[2407.20856](http://arxiv.org/abs/2407.20856)|null|快速发展的大型语言模型(Large Language Models, LLMs)为基于上下文的产品推荐应用提供了新的可能性。然而，这些模型在这一领域的有效性高度依赖于它们对产品库存的全面理解。本文提出了一种新颖的方法来增强LLMs的产品知识能力，通过训练它们响应包含产品ID的合成搜索查询，以进行上下文相关回复。我们深入分析了这种方法，评估其效果，概述其优点，并指出了限制因素。文章还讨论了此方法的改进潜力和未来方向，提供了对LLMs在产品推荐中角色的全面理解。  请注意，上述翻译已删除所有','字符。|
|**2024-07-29**|**Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing**|Ekaterina Iakovleva et.al.|[2407.20232](http://arxiv.org/abs/2407.20232)|null|文本编辑的扩散模型在用户输入指令存在歧义时表现出有限的性能。为了解决这一问题，我们提出了Specify ANd Edit（SANE），一个用于基于扩散的编辑系统的零样本推理管道。我们利用大型语言模型（LLM）将输入指令分解为具体的指令，即应用到输入图像以满足用户请求的具体干预措施。通过一种专门为任务设计的新颖去噪指导策略，我们可以从LLM生成的指令以及原始指令中受益。我们的实验在三个基线和两个数据集上展示了SANE在所有设置中的优势。此外，我们的管道提高了编辑模型的可解释性，并增强了输出多样性。我们还证明了我们的方法可以应用于任何编辑，无论是否存在歧义。我们的代码已公开在https://github.com/fabvio/SANE。|
|**2024-07-29**|**Can Editing LLMs Inject Harm?**|Canyu Chen et.al.|[2407.20224](http://arxiv.org/abs/2407.20224)|null|知识编辑技术正逐渐被采用以高效地纠正大型语言模型（LLMs）中的错误或过时知识，这主要是因为从头开始重新训练的高成本。同时，一个亟待探索但未充分研究的问题是：知识编辑是否可以用于向LLMs注入危害？在本文中，我们提出将知识编辑重新定义为LLMs面临的一种新类型安全性威胁，即编辑攻击，并通过构建一个新的数据集EditAttack进行了系统性的调查。  具体而言，我们聚焦于编辑攻击的两个典型安全性风险：误导性信息注入和偏见注入。对于误导性信息注入的风险，我们首先将其细分为常识误导性信息注入和长尾误导性信息注入。然后，我们发现编辑攻击能够有效地向LLMs注入这两种类型的误导性信息，尤其是对常识误导性信息注入的有效性特别高。  对于偏见注入的风险，我们揭示了一个关键点，即不仅可以通过高有效性向LLMs注入有偏见的句子，而且单个有偏见的句子注入就足以导致LLMs的总体输出出现显著偏见增加，即使这些输出与注入的句子高度无关，这表明了编辑攻击对LLMs整体公平性的灾难性影响。  进一步地，我们展示了编辑攻击的高隐蔽性，通过其对LLMs一般知识和推理能力的影响来衡量，以及在实证证据的基础上说明了防御编辑攻击的困难性。我们的发现揭示了知识编辑技术在损害LLMs安全对齐方面正在出现的滥用风险。|
|**2024-07-29**|**QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval**|Hongming Tan et.al.|[2407.20207](http://arxiv.org/abs/2407.20207)|null|在密集检索领域，将长文本转化为稠密向量时可能会导致信息丢失，从而影响查询与文本的匹配准确性。此外，质量较低、噪声过多或关键信息稀疏的文本往往难以与相关查询良好匹配。当前研究主要集中在提升句嵌入模型或检索流程上。本工作提出了一种新颖的文本增强框架用于密集检索。该框架通过将原始文档转化为信息密集型文本格式，以补充原文本，有效解决上述问题，同时无需修改嵌入或检索方法。通过大型语言模型（LLM）零样本提示生成两种文本表示：问题-答案对和事件驱动元素。我们将此方法命名为QAEA-DR：统一问题生成与事件提取的文本增强框架，用于密集检索。为了进一步提升生成文本的质量，引入了一种基于评分的评估与再生成机制于LLM提示过程中。我们的QAEA-DR模型对密集检索产生了积极影响，这一观点得到了理论分析和实验证据的支持。|
|**2024-07-29**|**MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**|Zehui Chen et.al.|[2407.20183](http://arxiv.org/abs/2407.20183)|**[link](https://github.com/internlm/mindsearch)**|**信息检索与整合是一个复杂认知任务，需要投入大量时间和精力。受到大型语言模型（LLM）近期显著进展的启发，近期工作尝试通过结合搜索引擎与LLM解决这一问题。然而，这些方法仍然因三个挑战而获得不令人满意的性能：（1）复杂的请求往往无法准确且完整地由搜索引擎检索；（2）需要整合的信息分布在多个网页上，并夹杂着大量噪音；（3）大量长文本的网页可能迅速超过LLM的最大上下文长度。  受人类在解决这些问题时思维过程的启发，我们引入了MindSearch，旨在模仿人类在互联网信息检索与整合过程中的思维模式，可通过一个简单而有效的基于LLM的多代理框架实现。WebPlanner以动态图构建过程来模拟人类多步骤信息检索的思维：它将用户查询分解为原子子问题作为图中的节点，并根据从WebSearcher获取的搜索结果逐步扩展图。WebSearcher承担每个子问题，执行分层信息检索并从搜索引擎收集有价值的信息供WebPlanner使用。MindSearch的多代理设计使其整体框架能够并行从超过300个网页中检索和整合信息，仅需3分钟，相当于节省了3小时的人类努力。  MindSearch在深度和广度上显著提高了响应质量，适用于封闭集和开放集的问答问题。此外，基于InternLM2.5-7B的MindSearch生成的响应被人类认为优于ChatGPT-Web和Perplexity.ai应用，这表明MindSearch已经能够提供与专有AI搜索引擎相竞争的解决方案。**|
|**2024-07-29**|**Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning**|Xingchen Zeng et.al.|[2407.20174](http://arxiv.org/abs/2407.20174)|**[link](https://github.com/zengxingchen/chartqa-mllm)**|**新兴的多模态大型语言模型（MLLMs）在图表问题回答（CQA）领域展现出巨大的潜力。近期的努力主要集中在通过数据收集和合成扩大训练数据集（包括图表、数据表格和问答对）。然而，我们对现有MLLMs和CQA数据集的实证研究揭示了显著的差距。  首先，当前的数据收集和合成工作侧重于数据量，而忽略了精细的视觉编码和问答任务的考虑，导致数据分布与实际CQA场景大相径庭，不平衡性明显。其次，现有的工作遵循了最初设计用于自然图像的基础MLLMs的训练配方，对于图表的独特特性，如丰富的文本元素的适应性探索不足。  为了填补这一空白，我们提出了一个可视化参考指令调整方法，以指导训练数据集的增强和模型开发。具体来说，我们提出了一种新颖的数据引擎，能够从现有数据集中有效地筛选出多样性和高质量的数据，并随后利用基于LLM的生成技术对数据进行细化和扩充，使其更好地与实际问答任务和视觉编码相匹配。  然后，为了促进对图表特性的适应性，我们利用丰富化数据来训练一个MLLM，通过解冻视觉编码器并引入混合分辨率适应策略，以增强细微粒度识别能力。实验结果验证了该方法的有效性。即使使用较少的训练示例，我们的模型也始终优于现有的CQA模型，在已建立的基准上表现出色。我们还贡献了一个数据集分割作为未来研究的基准。该论文的源代码和数据集可访问于https://github.com/zengxingchen/ChartQA-MLLM。**|
|**2024-07-29**|**Diffusion Feedback Helps CLIP See Better**|Wenxuan Wang et.al.|[2407.20171](http://arxiv.org/abs/2407.20171)|**[link](https://github.com/baaivision/diva)**|对比语言-图像预训练（CLIP）在跨领域和模态抽象开放世界表示方面表现出色，已成为各种视觉和多模态任务的基础。然而，近期的研究揭示了CLIP在视觉方面的严重局限性，如难以区分方向、数量、颜色、结构等。这些视觉局限性也限制了基于CLIP构建的大型多模态语言模型（MLLMs）的感知能力。主要原因是用于训练CLIP的图像-文本对固有偏见，由于文本的不明确性和图片多样性不足。  为此，我们提出了一种针对CLIP模型的简单后处理方法，通过自我监督的扩散过程极大地克服了其视觉局限性。我们引入了DIVA，即作为CLIP视觉辅助的扩散模型。具体而言，DIVA利用文本到图像扩散模型的生成反馈来优化CLIP表示，仅使用图像（不包括对应文本）。我们证明DIVA在MMVP-VLM基准上显著提高了CLIP的性能，该基准广泛评估了细微的视觉能力（例如，3-7%）。此外，我们的框架增强了MLLMs和视觉模型在多模态理解和分割任务上的表现。在29个图像分类和检索基准上的全面评估证实了我们的框架保留了CLIP强大的零样本能力。代码将在https://github.com/baaivision/DIVA公开。|
|**2024-07-29**|**Language-Conditioned Offline RL for Multi-Robot Navigation**|Steven Morad et.al.|[2407.20164](http://arxiv.org/abs/2407.20164)|null|我们提出了一种方法，用于为多机器人团队开发能够理解并遵循自然语言指令的导航策略。我们利用预训练大型语言模型（LLM）的嵌入来条件化这些策略，并通过使用仅20分钟随机收集的数据进行离线强化学习来训练它们。在五台真实机器人的实验中，这些策略对未见过的命令具有良好的泛化能力，表明它们理解了LLM的潜在空间。我们的方法不需要模拟器或环境模型，并产生低延迟的控制策略，可以直接部署到真实机器人上而无需进一步调优。更多信息和实验视频请参阅https://sites.google.com/view/llm-marl。|
|**2024-07-29**|**rLLM: Relational Table Learning with LLMs**|Weichen Li et.al.|[2407.20157](http://arxiv.org/abs/2407.20157)|**[link](https://github.com/rllm-project/rllm)**|**我们提出了一种名为rLLM（关系LLM）的PyTorch库，旨在通过大型语言模型（LLMs）实现关系表学习（RTL）。核心理念是将最先进的图神经网络、LLMs和表神经网络分解为标准化模块，以实现快速构建新型RTL型模型的简单“组合、对齐和联合训练”方式。为了说明rLLM的使用，我们引入了一个简单的RTL方法名为BRIDGE。此外，我们通过增强经典数据集，提出了三个新的关系表格数据集（TML1M、TLF2K和TACM12K）。我们希望rLLM能够作为用于RTL相关任务的有用且易于使用的开发框架。我们的代码可在以下位置获取：https://github.com/rllm-project/rllm。**|
|**2024-07-29**|**ByteCheckpoint: A Unified Checkpointing System for LLM Development**|Borui Wan et.al.|[2407.20143](http://arxiv.org/abs/2407.20143)|null|在构建实际世界大型语言模型（LLMs）时，需要在持久存储中检查训练状态以防止潜在的软件和硬件故障，并支持训练管道内的检查点转移以及跨任务使用。由于LLMs的规模庞大，保存和加载检查点往往会导致令人难以接受的分钟级延迟，极大地降低了训练效率。此外，在跨任务转移检查点时，通常需要执行检查点重新分片，即根据特定任务的特性和资源配额将检查点加载到不同的并行配置中。先前的检查点系统假设并行配置一致，未能解决在重新分片期间转换检查点的复杂性。而且，在工业平台中，开发者从不同的训练框架创建检查点，每个框架都有其独特的存储和I/O逻辑，这增加了统一管理和优化检查点的复杂性。为了解决这些挑战，我们引入了ByteCheckpoint，一个支持自动在线检查点重新分片的PyTorch原生多框架LLM检查点系统。ByteCheckpoint采用数据/元数据分离的存储架构，解耦了检查点存储与所采用的并行策略和训练框架。我们设计了一种高效的异步张量合并技术来解决不规则张量分片问题，并提出了多项I/O性能优化措施，显著提高了检查点保存和加载的效率。实验结果表明，ByteCheckpoint在减少检查点保存（最高可达529.22倍）和加载（最高可达3.51倍）成本方面具有明显优势，与基线方法相比。|
|**2024-07-29**|**Orca: Ocean Significant Wave Height Estimation with Spatio-temporally Aware Large Language Models**|Zhe Li et.al.|[2407.20053](http://arxiv.org/abs/2407.20053)|null|显著波高（SWH）在海洋科学中是一个关键指标，精确的SWH估计对于各种应用至关重要，例如海洋能开发、渔业、潜在风险的早期预警系统等。基于数值模型和物理理论的传统SWH估算方法受到计算效率低下的限制。近年来，机器学习作为一种有吸引力的替代方案，已用于提高准确度并减少计算时间。然而，由于观测技术有限和成本高昂，实际数据的稀缺性限制了机器学习模型的潜力。为了克服这些局限性，我们提出了一种海洋SWH估算框架，名为Orca。具体而言，Orca通过引入一个新颖的空间时间感知编码模块，增强了经典语言模型在空间时间和数据量有限情况下的推理能力。通过将有限的浮标观测数据进行时间分割、编码浮标的地理位置、设计提示模板，Orca利用大语言模型的强大泛化能力，有效地使用有限的数据对显著波高进行估算。在墨西哥湾的实验结果表明，Orca在SWH估算方面达到了最先进的性能水平。|
|**2024-07-26**|**Small Molecule Optimization with Large Language Models**|Philipp Guevorguian et.al.|[2407.18897](http://arxiv.org/abs/2407.18897)|**[link](https://github.com/yerevann/chemlactica)**|**近期大型语言模型的发展为生成分子药物设计带来了新的可能性。我们介绍了一种名为“Chemlactica”和“Chemma”的语言模型，它们均基于一个含有1.1亿个分子及计算得出属性的全新数据集，共计400亿个令牌进行微调。这些模型在生成具有指定属性的分子以及从有限样本预测新分子特性方面表现出色。  我们提出了一种新型优化算法，该算法利用我们的语言模型对任意属性进行优化，同时仅通过黑盒式接口访问有限信息。我们的方法结合了遗传算法、拒绝采样和提示优化的概念。该算法在多个分子优化基准测试中均取得了最先进的性能，包括在与先前方法相比提高了8%的“Practical Molecular Optimization”任务上表现出色。  我们公开发布了训练数据集、语言模型和优化算法的代码。**|
|**2024-07-26**|**Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models**|Mutahar Safdar et.al.|[2407.18827](http://arxiv.org/abs/2407.18827)|null|数据驱动的增材制造(AM)研究在近年来取得了显著的成功，这导致了大量的科学文献涌现。这些文献中的知识涉及AM和人工智能(AI)的上下文，但尚未以集成的方式进行挖掘和形式化。从这些作品中提取科学信息需要大量的努力和时间。在AM领域的专家已经贡献了超过二十多篇综述论文来总结这些工作。然而，与AM和AI相关的特定信息仍然需要手动努力来提取。最近，基础模型如BERT（双向编码表示变换器）或GPT（预训练生成型变换器）在文本数据上的成功，为加速科学信息提取提供了可能性。我们提出了一种框架，旨在促进AM和AI专家之间的合作，以连续从数据驱动的AM文献中提取科学信息。基于提出的框架实现了一个演示工具，并开展了一个案例研究，以提取与数据集、建模、传感和AM系统类别相关的信息。我们展示了大型语言模型(LLMs)加快从数据驱动的AM文献中提取相关信息的能力。在未来，该框架可以用于从工程学科的设计和制造文献中提取信息。|
|**2024-07-26**|**Automatic Detection of Moral Values in Music Lyrics**|Vjosa Preniqi et.al.|[2407.18787](http://arxiv.org/abs/2407.18787)|**[link](https://github.com/vjosapreniqi/ismir-mft-values)**|道德价值观在评估信息、做出决策和对重要社会问题形成判断方面发挥着基础性作用。从歌词中快速提取道德价值的可能性使我们对音乐聆听行为有更深的理解。基于道德基础理论（MFT），我们对一组经过大型语言模型（GPT-4）生成的2,721个合成歌词微调的变压器基语言模型（BERT）进行了任务，以检测200首由两位专家注释的真实音乐歌词中的道德价值观。我们通过一系列基准测试（包括离域（BERT在MFT注释的社交媒体文本上微调）和零射击（GPT-4）分类）来评估它们的预测能力。所提出的方法在所有实验中均表现出最佳准确性，平均F1加权得分为0.8。与基准模型相比，该性能平均高出5%。在二元分类的精确度上，所提出的方法平均高出基准模型12%。我们的方法贡献了无注释的歌词道德学习以及对大型语言模型在音乐中道德表达的知识提炼，并提供了这些技术对创意产业和音乐文化潜在影响的有用见解。|
|**2024-07-26**|**The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs**|Aleix Sant et.al.|[2407.18786](http://arxiv.org/abs/2407.18786)|null|本文通过大型语言模型（LLM）的视角探讨了机器翻译中的性别偏见问题。研究使用了四个广泛使用的测试集，对英语到加泰罗尼亚语（En $\rightarrow$Ca）和英语到西班牙语（En$\rightarrow$ Es）的翻译方向进行基准测试，与最先进的神经机器翻译（NMT）模型进行对比，评估各种基础LLM的翻译质量和性别偏见情况。研究发现，所有模型普遍存在性别偏见现象，其中基础LLM的偏见程度比NMT模型更高。为了对抗这种偏见，研究探索了对指令调优LLM应用的提示工程技巧。研究识别出一种提示结构，能够显著降低性别偏见，相比更直接的提示，在WinoMT评估数据集上减少了高达12%的性别偏见。这些结果显著缩小了LLM与传统NMT系统在性别偏见准确性方面的差距。|
|**2024-07-26**|**TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals**|Kevin Kliimask et.al.|[2407.18764](http://arxiv.org/abs/2407.18764)|null|自2000年代中期以来，推动开放政府数据（OGD）的努力在各级政府中获得了显著的势头。随着越来越多的数据集被发布到OGD门户上，查找特定数据变得越来越困难，导致信息过载。完整且准确的数据集文档，包括与数据集关联的适当标签，对于提高数据集可发现性和可访问性至关重要。对爱沙尼亚开放数据门户的分析揭示，11%的数据集没有关联标签，而26%的数据集仅有一个标签被分配，这表明了门户内数据可发现性和可访问性面临的挑战。根据最近的开放数据成熟度报告，该门户被认为是领先者。本研究的目标是提出一种自动化解决方案，以改善OGD门户上的数据集标签，从而提高数据集的可发现性。本文介绍了Tagify——一个利用大型语言模型（LLM），如GPT-3.5-turbo和GPT-4自动为数据集生成标签的原型，以英语和爱沙尼亚语为数据集生成标签，从而增强数据发布者准备的元数据，并通过改善数据用户在OGD门户上的数据发现性来提高数据的可访问性。开发的解决方案经过用户评估，并收集了他们的反馈，以定义未来原型改进的议程。|
|**2024-07-26**|**Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**|Yuni Susanti et.al.|[2407.18752](http://arxiv.org/abs/2407.18752)|**[link](https://github.com/littleflow3r/kg-structure-as-prompt)**|**本文探讨了基于元数据而非实际数据值的大型语言模型（LLMs）在因果发现问题上的新视角，即知识导向的因果发现。我们关注小型语言模型（SLMs，参数少于10亿）如何通过提示式学习进行知识导向的因果发现。具体而言，我们提出了一种名为“基于知识图谱的结构提示”（KG Structure as Prompt）的新方法，用于将知识图谱中的结构信息，如共邻节点和元路径，整合到提示式学习中，以增强SLMs的能力。  在三种类型的生命科学和开放域数据集下的少量样本设置下，我们的实验结果表明，这种方法的有效性超越了许多基线，并且甚至超过了在完整数据集上进行常规微调的传统方法。我们的研究进一步揭示了小型语言模型的强大能力：结合知识图谱和提示式学习，小型语言模型显示出超越参数更多LLMs的潜力。  我们已经在GitHub上提供了代码和数据集。**|
|**2024-07-26**|**Towards Effective and Efficient Continual Pre-training of Large Language Models**|Jie Chen et.al.|[2407.18743](http://arxiv.org/abs/2407.18743)|null|这篇技术报告介绍了持续预训练（CPT）方法的最新进展，特别关注了增强大型语言模型在特定领域或任务上的能力。报告以Llama-3（8B）为例，这是一个显著提升了其在中文理解和科学推理能力的基线模型。为了在增强新能力的同时保持原有能力，我们设计了数据混合和课程策略，利用现有数据集并合成高质量数据集。具体地，我们基于相关网页生成多学科的科学问题与答案（QA）对，并将这些合成数据融入模型训练，以提升Llama-3的科学推理能力。经过这一系列改进后的模型被称为Llama-3-SynE（合成数据增强的Llama-3）。报告还通过较小规模的TinyLlama模型进行调参实验，并利用从这些实验中得到的发现来训练基线模型。  多个评估基准的广泛实验结果表明，我们的方法能显著提高基线模型的性能，包括通用能力（C-Eval上+8.81分，CMMLU上+6.31分）和科学推理能力（MATH上+12.00分，SciEval上+4.13分），而不会损害原有的能力。该模型、数据和代码已开源发布于https://github.com/RUC-GSAI/Llama-3-SynE。|
|**2024-07-26**|**Towards Generalized Offensive Language Identification**|Alphaeus Dmonte et.al.|[2407.18738](http://arxiv.org/abs/2407.18738)|null|互联网上具有攻击性的内容，包括仇恨言论和网络欺凌，是一个全球性问题。因此，机器学习（ML）和自然语言处理（NLP）社区对此给予了广泛关注。为了应对这一挑战，已经开发出了多种自动识别可能有害内容并减轻其影响的系统。这些系统主要采用两种策略：（1）使用公开可用的模型和应用端点，包括激发大型语言模型（LLMs）；（2）注释数据集，并在这些数据集上训练机器学习模型。然而，这两种方法的通用性尚不清楚，而且它们在实际环境和非领域内的应用也常受到质疑。本文通过一个新颖的通用基准对攻击性语言检测模型和数据集的通用性进行了实证评估。我们针对通用性提出了三个研究问题，并得出了结论。这些发现将有助于构建更强大的现实世界攻击性语言检测系统。|
|**2024-07-26**|**LLASP: Fine-tuning Large Language Models for Answer Set Programming**|Erica Coppolillo et.al.|[2407.18723](http://arxiv.org/abs/2407.18723)|null|近期，大型语言模型（LLMs）在自然语言处理任务中展现出了巨大的潜力，尤其是在代码生成方面。尽管在适应LLMs以生成多种指令性编程语言和任务的代码方面取得了显著进展，但它们在处理声明式形式化语言，如答案集编程（ASP）时的能力仍有待探索。本文旨在探讨LLMs在ASP代码生成方面的应用可能性。首先，我们对当前最先进的LLMs进行了系统评估。尽管这些模型在参数数量、训练数据和计算资源等方面表现出色，但实证结果表明，它们在生成正确ASP程序方面的表现并不理想。因此，我们提出了一种名为LLASP的轻量级模型，专门用于编码ASP程序的基本模式。为了实现这一目标，我们创建了一个包含广泛基本问题规范的自定义数据集，这些规范可以被编码为ASP。我们的实验结果显示，LLASP生成的ASP程序的质量令人印象深刻。与未经过微调的版本相比，以及与大多数渴望型LLM候选者，尤其是从语义角度来看，其表现均优于多数。所有用于执行实验的代码和数据都已公开发布于https://anonymous.4open.science/r/LLASP-D86C/。|
|**2024-07-26**|**Neurosymbolic AI for Enhancing Instructability in Generative AI**|Amit Sheth et.al.|[2407.18722](http://arxiv.org/abs/2407.18722)|null|生成式人工智能，特别是通过大型语言模型（LLMs），在文本、图像和音乐等内容创作领域实现了革命性变革，展示了遵循指令的提示能力，很大程度上得益于指令调优。指令调优是一种监督式微调方法，通过训练数据集来实现特定任务及其对应指令格式化，这种方法系统性地增强了模型执行提供指示的能力。尽管如此，LLMs 在一致理解和执行复杂、多步骤指令以及将这些指令推广到新任务方面仍面临挑战，这对于更广泛地应用于实际场景至关重要。本文探讨了为何神经符号AI能提供增强LLMs指令可理解性的更好途径。我们探索使用符号任务规划器分解高级指令为结构化任务，使用神经语义解析器将这些任务落地为可执行操作，以及使用神经符号执行器实施这些操作的同时动态维护明确的状态表示。我们也寻求展示，神经符号方法能够增强任务执行的可靠性和上下文意识，使LLMs能够以更高的精度和灵活性动态解释和响应更广泛的指令上下文。|
|**2024-07-26**|**Recursive Introspection: Teaching Language Model Agents How to Self-Improve**|Yuxiao Qu et.al.|[2407.18219](http://arxiv.org/abs/2407.18219)|null|在使基础模型具备自我反省能力以促进智能代理行为的关键方面在于使其能够对其行为、推理以及在可用计算或交互增加时纠正错误的能力进行自我反思。即使是最强的专有大型语言模型（LLMs）也未能展现出在明确告知其犯错的情况下，能够连续改进其响应序列的能力。本文提出了一种名为RISE（递归内省）的方法，用于微调LLMs以引入这一能力，尽管之前的研究曾假设这种能力可能无法实现。我们的方法规定了一个迭代微调过程，该过程尝试教授模型如何在其解决困难测试时问题的不成功尝试后修改其响应，并可选地获得额外的环境反馈。RISE将单轮提示的微调视为解决多轮马尔科夫决策过程（MDP），其中初始状态为提示。受在线模仿学习和强化学习原理的启发，我们提出了多轮数据收集和训练策略，旨在赋予LLM递归检测并修正其先前错误并在后续迭代中进行纠正的能力。  我们的实验表明，RISE使Llama2、Llama3和Mistral模型在数学推理任务上通过更多轮次改善自己，与给定等量推理时间计算相比，超过了几种单轮策略。我们还发现，RISE具有良好的可扩展性，通常随着更强大的模型而获得更大的收益。我们的分析显示，RISE对困难提示的响应进行了有意义的改进，以达到正确的解决方案，同时没有因为表达更复杂的分布而导致单轮能力受到影响。|
|**2024-07-26**|**Exploring Scaling Trends in LLM Robustness**|Nikolaus Howe et.al.|[2407.18213](http://arxiv.org/abs/2407.18213)|null|语言模型的能力可预测地通过增加模型的大小和训练数据而得到改善。受此启发，已训练了一系列越来越大的语言模型，这些模型展现出了令人印象深刻的能力。然而，这些模型对对抗性提示（如“越狱”攻击）非常脆弱，这类攻击会操控模型执行不希望的行为，从而构成了重大的误用风险。先前的研究表明，随着模型和数据规模的增加，计算机视觉模型的鲁棒性也会提高，因此提出了这样一个问题：语言模型的鲁棒性是否也会随规模的扩大而提升？我们通过实证研究回答了这个问题，发现更大的模型在对抗性训练下有显著更好的表现，但在没有明确防御措施的情况下，模型规模的增加并没有带来任何益处。|
|**2024-07-25**|**Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models**|Sanae Lotfi et.al.|[2407.18158](http://arxiv.org/abs/2407.18158)|null|大型语言模型（LLM）在预测序列中的下一个令牌方面表现出色。近期的研究通过压缩技术计算了LLM的非空泛化边界，但对于十亿参数级别的大型模型，这些边界显得无意义。此外，这些边界是在非常有限的压缩技术下获得的，限制了生成质量较低文本的压缩模型。更关键的是，现有边界依赖于训练集中独立同分布（IID）文档的数量，而忽略了训练集内数量庞大的非IID构成令牌，这使得进一步提高边界紧致性潜力未被充分利用。  本研究采用鞅的性质来推导泛化边界，这些边界能够从大型语言模型训练集中包含的大量令牌中获益。与训练集相比，数据集包含的令牌数量远多于文档，因此我们的泛化边界不仅容忍了更为宽松的压缩方案，实际上还能从这些方案中获益。我们通过Monarch矩阵、Kronecker因子分解和后训练量化等方法，为LLM（如LLaMA2-70B）实现了非空泛化边界。与以往的方法不同，我们的工作首次为在实践中部署并生成高质量文本的模型实现了非空泛化边界。|
|**2024-07-26**|**Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic**|Fakhraddin Alwajih et.al.|[2407.18129](http://arxiv.org/abs/2407.18129)|null|近期的进展显著提高了多模态大型语言模型（MLLM）在生成和理解图像到文本内容方面的功能。尽管取得了这些成功，但进步主要局限于英语，由于其他语言如阿拉伯语高质量多模态资源的稀缺性，这限制了阿拉伯语等语言中竞争性模型的发展。为了缓解这一状况，我们引入了一个高效的阿拉伯语多模态助手——Dallah，它基于LLaMA-2先进语言模型来促进多模态交互。Dallah在阿拉伯语MLLM中表现出色，实现了最先进的性能。通过细调六个阿拉伯方言，Dallah展示了其处理包含文本和视觉元素的复杂方言互动的能力。该模型在两个基准测试中表现出色：一个评估其现代标准阿拉伯语（MSA）性能，另一个专门用于评估方言响应。  除了在多模态交互任务中的稳健性能外，Dallah有望引领进一步开发方言意识的阿拉伯语MLLM的发展。|
|**2024-07-25**|**Fine-Tuning Large Language Models for Stock Return Prediction Using Newsflow**|Tian Guo et.al.|[2407.18103](http://arxiv.org/abs/2407.18103)|null|大型语言模型（LLM）及其微调技术在各种语言理解和生成任务中表现出优越的性能。本文探讨了将LLM用于基于金融新闻流的股票回报预测的微调方法。在量化投资领域，回报预测是后续任务如股票挑选和组合优化等的基础。我们构建了一个包括文本表示和预测模块的模型。提出了比较仅编码器和仅解码器LLM的两种方法，因为它们以不同的方式生成文本表示。这些不同表示对预测性能的影响仍是一个开放的问题。同时，我们比较了将LLM的token级表示集成到预测模块中的两种简单方法。在真实新闻和投资范围内进行的实验揭示以下结果：（1）从LLM的token级嵌入聚合的表示通常能产生增强长期和长期短期投资组合性能的回报预测；（2）在相对较大的投资范围内，基于解码器的LLM预测模型导致更强的投资组合，而在较小的范围内，没有一致的赢家；（3）从LLM文本表示中导出的回报预测对于投资组合构造是一个强大的信号，优于传统的情绪得分。|
|**2024-07-25**|**PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization**|Christopher Clarke et.al.|[2407.18078](http://arxiv.org/abs/2407.18078)|**[link](https://github.com/ChrisIsKing/Parameter-Efficient-Personalization)**|**近期，大型语言模型（Large Language Models，LLMs）的兴起为人类与AI的交互开辟了新的篇章。这些先进模型，以Chat-GPT为代表，展现了在语言理解方面的惊人能力。然而，随着LLM规模的指数级增长，一个关键维度——模型个性化——的研究却相对匮乏。大型基础模型如GPT-3等侧重于构建通用模型，适用于广泛的任务和用户群体。这种策略强调了模型的泛化能力，将用户视为整体而非个体。虽然在许多常见应用中实用，但这种一刀切的方法往往无法满足人类多样性和个性化需求的丰富性。为了探讨这一问题，我们引入了PEFT-U基准：一个用于构建和评估面向用户的自然语言处理（NLP）模型的新数据集。PEFT-U包含了多元且个性化的表达任务，其中同一输入对于不同用户可能有不同的偏好。通过PEFT-U，我们探索了如何高效地个性化LLM以适应用户特定偏好，特别是在多样化的用户中心任务背景下。**|
|**2024-07-25**|**C2P: Featuring Large Language Models with Causal Reasoning**|Abdolmahdi Bagheri et.al.|[2407.18069](http://arxiv.org/abs/2407.18069)|null|因果推理是大型语言模型（LLM）达到人类级智能的主要障碍。为此，我们引入了因果链提示（C2P），这是第一个为当前LLM提供因果推理能力的推理框架。C2P自主运行，在因果学习和推理阶段均无需依赖外部工具或模块，并且可以无缝集成到LLM的训练或微调过程中。在各种基准数据集上的实验结果表明，C2P显著提高了LLM的因果学习和后续推理准确性。  我们展示了如何通过C2P增强LLM在现实世界场景中的因果推理能力，解决医疗、医学、经济学、教育、社会科学、环境科学和市场营销等领域中的复杂问题。利用少示例学习，GPT-4 Turbo 使用C2P，仅使用六个示例就实现了显著的性能提升，推理准确性比在类似情况下近乎随机运行的最先进LLM高出33%以上。这证明了将C2P集成到LLM训练或微调过程中的潜力，从而赋予这些模型高级因果推理能力，具有变革性意义。|
|**2024-07-25**|**ComPeer: A Generative Conversational Agent for Proactive Peer Support**|Tianjian Liu et.al.|[2407.18064](http://arxiv.org/abs/2407.18064)|**[link](https://github.com/liutj9/compeer)**|本文探讨了交互式代理（CA）作为同伴支持者在心理健康领域的广泛应用及益处。然而，当前的同伴支持型CA要么由用户主动触发，要么遵循预设规则以启动对话，这可能阻碍用户与CA建立长期关系，从而影响长期益处。为了克服这一挑战，我们开发了ComPeer——一种生成式CA，它能够主动提供适应性的同伴支持。  ComPeer利用大型语言模型检测并反映对话中的关键事件，以此来策略性地规划主动关怀的时间和内容。此外，ComPeer还整合了同伴支持策略、对话历史以及其个性化的元素到生成的消息中。通过一项为期一周的跨组实验（参与人数：24），我们展示了ComPeer在长时间内提供同伴支持的能力，并且与基于用户的主动触发的CA相比，显著提升了用户的参与度。  这项研究强调了生成式CA在同伴支持领域的潜力，特别是它们如何通过主动关怀策略促进更深入、更持续的人际互动，从而为用户提供长期的心理健康益处。|
|**2024-07-25**|**Audio Entailment: Assessing Deductive Reasoning for Audio Understanding**|Soham Deshmukh et.al.|[2407.18062](http://arxiv.org/abs/2407.18062)|**[link](https://github.com/microsoft/audioentailment)**|**近期文献在构建音频基础模型时使用了语言。这些音频-语言模型（ALMs）通过大量音频文本对进行训练，并在文本到音频检索、字幕和问答等任务上表现出卓越的性能。然而，它们在执行更复杂的开放性任务，如交互式问答时的能力，需要逻辑推理技能，而这一领域尚未得到充分评估。  我们引入了一个名为音频蕴含的新任务，用于评估ALM的演绎推理能力。这个任务评估音频内容的文本描述（假设）是否可以从音频记录（前提）中推断出来，结论可能是蕴含、中立或矛盾，取决于证据的充分性。我们创建了两个数据集来完成这项任务，音频记录来自两个音频字幕数据集——AudioCaps和Clotho，而假设则由大型语言模型（LLMs）生成。  我们对最先进的ALMs进行了基准测试，并发现它们在零次学习和线性探针评估中的逻辑推理能力存在不足。最后，我们提出了“先字幕后推理”这一中间步骤，通过这种方式可以分别提高ALMs在零次学习和线性探针评估中的表现绝对值6%和3%。**|
|**2024-07-25**|**Difficulty Estimation and Simplification of French Text Using LLMs**|Henri Jamet et.al.|[2407.18061](http://arxiv.org/abs/2407.18061)|null|我们利用生成式大型语言模型来开发外语学习应用，专注于评估外语文本的难度并将其简化至较低难度级别。我们将这两个任务都视为预测问题，并通过使用有标签示例、迁移学习和大型语言模型构建了一个难度分类模型，相较于以往方法，该模型在准确性上表现出色。对于简化过程，我们评估了简化质量与意义保留之间的权衡，比较了零初始化和微调大语言模型的表现。结果显示，通过有限的微调，可以获得具有意义的文本简化结果。我们的实验在法语文本上进行，但我们的方法具有语言无关性，并直接适用于其他外语。|
|**2024-07-24**|**I Could've Asked That: Reformulating Unanswerable Questions**|Wenting Zhao et.al.|[2407.17469](http://arxiv.org/abs/2407.17469)|**[link](https://github.com/wenting-zhao/couldask)**|**在从不熟悉文档中获取信息时，用户经常提出无法由文档回答的问题。现有的大型语言模型（LLMs）能够识别这些无法回答的问题，但它们并未帮助用户重新构建问题，从而降低了它们的整体实用性。我们精心编排了CouldAsk，一个用于文档支持的问答任务的评估基准，旨在研究重新构建无法回答问题的能力。这个基准包括了现有的和新的数据集。我们对最先进的开源和专有LLMs在CouldAsk上的表现进行了评估。结果表明，这些模型在重新构建问题方面能力有限。具体而言，GPT-4和Llama2-7B仅成功地重新构建了问题的26%和12%。错误分析显示，失败的重新构建中有62%的原因是模型只是重述了问题，甚至生成了完全相同的问题。我们公开发布了这个基准以及重现实验所需的代码。**|
|**2024-07-24**|**WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries**|Wenting Zhao et.al.|[2407.17468](http://arxiv.org/abs/2407.17468)|null|在大型语言模型（LLM）的幻觉问题普遍存在的情况下，现有的事实性评估基准未能覆盖现实世界用户寻求信息的多样化知识领域。为了填补这一缺口，我们引入了WildHallucinations基准，旨在评估事实性。该基准通过促使LLM生成来自野外用户-聊天机器人对话中的实体的信息来实现这一目标。这些生成内容随后自动与从网络搜索系统收集的有组织的知识库进行事实检查。值得注意的是，一半以上的实际世界实体并没有相关的维基百科页面。我们在15个LLM上对7919个实体进行了118785次生成的评估。我们发现，LLM在没有维基百科页面的实体上产生更多的幻觉，并且不同领域的幻觉率存在差异。最后，在使用相同的底层模型时，仅增加检索组件可以略微减少幻觉，但无法完全消除幻觉。|
|**2024-07-24**|**CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models**|Jiawei Gu et.al.|[2407.17467](http://arxiv.org/abs/2407.17467)|null|大型语言模型（LLM）在各种任务上表现出色，但往往在特定领域内表现不佳，因为缺乏特定领域的或专有语料库。连续预训练（CPT）通过回放通用语料并注入新领域的特定知识来增强LLM的能力，以此防止灾难性遗忘。然而，在通用语料和领域特定语料的混合比例上，人们通常采取的是启发式方法，这导致了实际训练效率的低下。在此背景下，我们尝试从CPT的核心出发重新审视LLM的缩放行为，并发现损失、混合比率与训练令牌规模之间的幂律关系。我们正式定义了通用能力和领域特定能力之间的权衡，从而确定了通用数据和领域数据的临界混合比率（CMR）。通过找到平衡点，CMR保持了模型的通用能力，并实现了期望的领域迁移，确保了可用资源的最大化利用。因此，如果重视效率与效果之间的平衡，CMR可以被认为是最佳混合比率。  通过大量实验，我们证实了CMR的可预测性，并提出了CMR缩放定律，并对其一般性进行了验证。这些发现提供了优化LLM在特定领域内的训练的实用指南，确保在有效管理训练资源的同时，既保持通用性能又实现领域特定性能。|
|**2024-07-24**|**$VILA^2$ : VILA Augmented VILA**|Yunhao Fang et.al.|[2407.17453](http://arxiv.org/abs/2407.17453)|null|视觉语言模型(VLMs)的发展迅速，得益于大型语言模型(LLLs)的成功。尽管模型架构和训练基础设施在快速进步，但数据收集与整理的工作仍被忽视。当数据的数量与质量成为瓶颈时，现有方法要么直接从互联网上爬取更多原始数据，这些数据的质量无法保证，要么从黑盒商业模型（例如GPT-4V/金牛座）中提取数据，导致性能受到该模型的限制。本文提出了一种新颖的方法，包括自我增强步骤和专家增强步骤，以迭代地提高数据质量和模型性能。  在自我增强步骤中，VLM重新生成其自身的预训练数据，以提升数据质量，并从这个精炼的数据集重新训练，以改善模型性能。这一过程可以重复进行多次。一旦自我增强达到饱和，我们将采用几个专门领域VLM，这些VLM是从自我增强的VLM中微调而来的，具有特定领域的专业知识。通过任务导向的重新生成和重新训练，进一步将专家知识注入通用模型中。  通过结合自我增强和专家增强的训练，我们引入了VILA²（VILA增强-VILA）模型家族，该家族在广泛的任务上持续提高了准确性，超越了以往的成果，并在开放源代码模型中MMMU排行榜上达到了新的最先进结果。|
|**2024-07-24**|**Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?**|Michael-Andrei Panaitescu-Liess et.al.|[2407.17417](http://arxiv.org/abs/2407.17417)|null|本文首先探讨了在大型语言模型（LLM）中嵌入水印作为防止生成版权侵权文本的有效手段。通过理论分析和实证评估，我们证明了在LLM中融入水印能够显著降低生成版权内容的可能性，从而解决LLM部署过程中的一项关键问题。此外，我们还研究了水印对成员归属推断攻击（Membership Inference Attacks，MIAs）的影响，MIAs旨在识别样本是否属于预训练数据集，这可能用于检测版权违规行为。令人惊讶的是，我们发现水印降低了MIAs的成功率，使检测预训练数据集中版权文本变得复杂。最后，我们提出了一种适应性技术来提高在水印环境下最近MIAs的成功率。我们的发现强调了开发适应性方法以研究具有潜在法律影响的LLM关键问题的重要性。|
|**2024-07-24**|**(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork**|Tianjin Huang et.al.|[2407.17412](http://arxiv.org/abs/2407.17412)|null|大型神经网络在不同领域如视觉和语言处理方面展现了卓越的性能，尽管这伴随着巨大的计算资源成本。压缩文献中提出的结构模型剪枝算法是促进模型效率的关键方法，得益于其加速友好的稀疏性模式。结构剪枝的核心问题是如何估计通道的重要性。与此并行，数据为中心的人工智能工作表明，基于提示的技术能够使大型语言模型在各种下游任务中表现出惊人的泛化能力。本文探讨了一个迷人的可能性——利用视觉提示来捕捉通道重要性，并推导出高质量的结构稀疏性。为此，我们提出了一种新颖的算法框架，即\texttt{PASS}。它是一种定制的超网络，接受视觉提示和网络权重统计作为输入，并以递归方式输出逐层通道稀疏性。这种设计考虑了层之间通道的内在依赖性。跨多个网络架构和六个数据集的全面实验显示了\texttt{PASS}在定位良好结构稀疏性的优势。例如，在相同的FLOPs水平下，\texttt{PASS}子网络在Food101数据集上实现了1%-3%更高的准确性；或者在获得与基线相同的80%准确度时，\texttt{PASS}子网络能够实现0.35倍更多的速度提升。|
|**2024-07-24**|**Grammar-based Game Description Generation using Large Language Models**|Tsunehiko Tanaka et.al.|[2407.17404](http://arxiv.org/abs/2407.17404)|null|为了降低游戏设计开发的门槛，自动化游戏设计领域通过计算过程生成游戏设计，已经进行了探索。在自动化游戏设计中，基于机器学习的技术，如进化算法已取得成功。得益于深度学习领域在计算机视觉和自然语言处理应用方面的显著进展，游戏生成方面也有了进步。然而，由于游戏设计领域的数据量有限，深度学习在任务如游戏描述生成上应用不足。为了开拓处理有限数据在自动化游戏设计中的新途径，我们聚焦于大型语言模型（LLMs）的上下文内学习。LLMs可以从少量示范示例中捕获任务特征，并利用预训练期间获得的能力进行应用。我们引入了游戏描述的语法，有效地对游戏设计空间进行了结构化，使LLMs能够捕捉游戏描述生成这一复杂任务的特性。此外，我们提出了一种解码方法，通过利用语法迭代改进生成的输出。我们的实验结果表明，这种方法在生成游戏描述方面表现良好。|
|**2024-07-24**|**3D Question Answering for City Scene Understanding**|Penglei Sun et.al.|[2407.17398](http://arxiv.org/abs/2407.17398)|null|在三维多模态问答（MQA）领域，通过使智能体理解其所在环境中的三维空间，对于场景理解具有至关重要的作用。当前的研究主要集中在室内家庭任务和室外道路自动驾驶任务上，而对于城市级别的场景理解任务探索有限。现有研究在理解城市场景时面临挑战，主要是由于缺乏城市层面的空间语义信息以及人类与环境的互动信息。  为了应对这些挑战，我们从数据集和方法两个角度对三维MQA进行了深入研究。从数据集角度来看，我们引入了一个名为City-3DQA的新颖三维MQA数据集，它是首个融合城市场景语义和人与环境交互任务的数据集。从方法角度来看，我们提出了一个基于场景图的城市级别理解方法（Sg-CityU），利用场景图引入空间语义信息。在City-3DQA的不同设置下，我们的Sg-CityU方法取得了63.94%和63.76%的准确率，相比室内三维MQA方法和使用先进大型语言模型（LLMs）的零样本方法，在鲁棒性和泛化能力方面均达到了最先进的性能水平。|
|**2024-07-24**|**ViPer: Visual Personalization of Generative Models via Individual Preference Learning**|Sogand Salehi et.al.|[2407.17365](http://arxiv.org/abs/2407.17365)|null|不同的用户对于同一提示生成的不同图像有不同的偏好。这催生了个性化图像生成的概念，即创建与个人视觉偏好相匹配的图像。目前的生成模型是无个性化的，它们被调整为吸引广泛受众。用户使用这些模型生成符合个人偏好的图像依赖于通过多次迭代手动调整提示的过程，这一过程既低效又不理想。  我们提出了一种方法来个性化图像生成过程：首先通过邀请用户对一小部分图像进行评论，解释他们喜欢或不喜欢的原因，从而捕捉用户的通用偏好。基于这些评论，我们利用大型语言模型推断出用户的结构化喜好的和不喜好的视觉属性，即他们的视觉偏好。这些属性用于指导文本到图像模型生成更贴近个人用户视觉偏好的图像。  通过一系列用户研究和大型语言模型引导的评估，我们证明了所提出的方法能够产生与个人用户视觉偏好高度一致的生成结果。|
|**2024-07-24**|**Scalify: scale propagation for efficient low-precision LLM training**|Paul Balança et.al.|[2407.17353](http://arxiv.org/abs/2407.17353)|**[link](https://github.com/graphcore-research/jax-scalify)**|**低精度格式，如float8，已被引入机器学习加速硬件中，以提高大型语言模型训练和推理的计算效率。然而，由于需要复杂的、有时是脆弱的技术来匹配更高精度的训练准确度，ML社区对低精度格式的采纳速度较慢。本文提出了一种名为Scalify的端到端的缩放传播范式，用于计算图，它泛化并形式化了现有的张量缩放方法。实验结果表明，Scalify支持直接使用float8进行矩阵乘法和梯度表示，以及float16优化器状态存储。我们对Scalify的JAX实现已经开源在https://github.com/graphcore-research/jax-scalify。**|
|**2024-07-23**|**Can Large Language Models Automatically Jailbreak GPT-4V?**|Yuanwei Wu et.al.|[2407.16686](http://arxiv.org/abs/2407.16686)|null|GPT-4V因其在整合和处理多模态信息方面的卓越能力而引起广泛关注。同时，其面部识别功能也引发了隐私泄露的安全担忧。尽管研究者通过强化学习与人类反馈（RLHF）或预处理过滤器等手段努力实现安全对齐，但仍然可能存在被利用的漏洞。在我们的研究中，我们引入了AutoJailbreak，这是一种创新的自动越狱技术，灵感来源于提示优化。我们利用大型语言模型（LLMs）进行红队训练，以精炼越狱提示，并采用弱到强的上下文内学习提示来提高效率。此外，我们提出了一种有效的方法，结合早期停止策略，以最小化优化时间和令牌消耗。实验结果表明，AutoJailbreak显著超越传统方法，实现了超过95.3%的成功攻击率（ASR）。这项研究揭示了加强GPT-4V安全性的潜力，突显了LLMs可能被用于破坏GPT-4V完整性的风险。|
|**2024-07-23**|**RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent**|Huiyu Xu et.al.|[2407.16667](http://arxiv.org/abs/2407.16667)|null|近期，大型语言模型（LLMs）如GPT-4已被集成至诸多实际应用，例如代码助手Copilot。这些集成显著扩展了LLM的攻击面，使其面临多种威胁。其中，通过“越狱”攻击诱导出毒性响应的“越狱”提示引起了安全领域的广泛关注。为了识别这些威胁，越来越多的红队策略通过构建“越狱”提示来模拟潜在的对抗场景，以此测试目标LLM。然而，现有红队策略并未考虑LLM在不同情境下的独特脆弱性，使得构建针对特定情境的“越狱”提示变得困难。同时，这些方法仅依赖于少数变异操作对“越狱”模板进行细化，缺乏适应不同情境的自动化和规模化能力。  为了实现情境感知和高效红队策略，我们抽象并建模现有攻击行为为一个统一概念——“越狱策略”，并提出了一种多智能体LLM系统RedAgent。该系统利用这些策略生成情境感知的“越狱”提示，并通过额外的记忆缓冲区自我反思情境反馈，持续学习如何利用这些策略在特定情境下实现有效“越狱”。广泛的实验表明，我们的系统可以在五个查询内成功“越狱”大多数黑盒LLM，相较于现有红队方法效率提升两倍。此外，RedAgent能够更高效地针对定制化的LLM应用进行“越狱”。  通过生成针对特定应用的“越狱”提示，我们发现了60个严重漏洞存在于实际应用中的GPTs上，仅需每漏洞两次查询。我们已报告所有发现的问题，并与OpenAI和Meta进行了沟通以修复漏洞。|
|**2024-07-23**|**Course-Correction: Safety Alignment Using Synthetic Preferences**|Rongwu Xu et.al.|[2407.16637](http://arxiv.org/abs/2407.16637)|**[link](https://github.com/pillowsofwind/course-correction)**|### 摘要  本文对大型语言模型（LLM）在执行“课程纠正”任务的能力进行了一项系统性研究，即模型能够自主地避免生成有害内容。首先，我们引入了\textsc{C $^2$-Eval}基准用于定量评估，并分析了10个流行LLM的性能，揭示了当前安全调优的LLM在课程纠正方面存在显著差异。为了改进这一问题，我们提出了使用偏好学习对LLM进行微调的方法，强调及时课程纠正的重要性。通过自动化流程，我们创建了\textsc{C$^2$ -Syn}合成数据集，包含75万对偏好，以此通过数据驱动的偏好学习教授模型及时课程纠正的概念。在\textsc{Llama2-Chat 7B}和\textsc{Qwen2 7B}两个LLM上的实验表明，我们的方法有效提高了课程纠正能力，同时不影响总体性能，并且特别有效地提升了LLM的安全性，尤其是抵抗逃脱攻击的能力。|
|**2024-07-23**|**Lawma: The Power of Specialization for Legal Tasks**|Ricardo Dominguez-Olmedo et.al.|[2407.16615](http://arxiv.org/abs/2407.16615)|null|法律文本的注释与分类是实证法学研究的核心部分。传统上，这些任务往往由受过训练的研究助理承担。在语言模型取得进展的背景下，实证法律学者越来越多地转向使用商业模型，希望以此减轻人工标注的巨大成本。尽管这类方法的应用日益广泛，但关于如何最有效地利用大型语言模型进行法律任务的相关研究仍然有限。  我们进行了一项全面的研究，涵盖了几乎全部针对机器学习社区的新法律文本分类任务。从GPT-4作为基准开始，我们发现它在零样本准确度上的表现具有非同寻常但高度多变性，经常表现出可能不足以满足法律工作需求的性能。接着，我们展示了轻度微调后的Llama 3模型在几乎所有任务上的表现均远超GPT-4，通常提高了两位数百分点的准确性。我们发现，更大的模型在微调时响应效果更好。几十到几百个示例足以实现高分类准确性。值得注意的是，我们可以在所有260个任务上同时微调一个模型，相对于为每个任务单独创建模型，仅在准确性方面略有损失。  我们的工作指出了替代现有做法的一种可行选择。对于具备一定标注数据的特定法律任务，研究人员更应考虑使用开源模型进行微调。|
|**2024-07-23**|**Shared Imagination: LLMs Hallucinate Alike**|Yilun Zhou et.al.|[2407.16604](http://arxiv.org/abs/2407.16604)|null|尽管大型语言模型（LLM）的最近发展呈现了显著的增长，但它们的训练方法——包括模型架构、预训练数据和优化算法——往往极为相似。这自然引发了一个问题：这些模型之间的相似性如何？本文提出了一种新颖的设置，即想象问题回答（IQA），以更深入地理解模型之间的相似性。在IQA中，我们让一个模型生成完全虚构的问题（例如，关于物理中完全不存在的概念），然后让另一个模型进行回答。令人惊讶的是，尽管这些问题完全虚构，但所有模型都能成功回答对方的问题，这表明在这样的幻觉过程中，这些模型共享着一个“共同的想象空间”。  我们对这一现象进行了系列调查，并讨论了它对模型同质性、幻觉以及计算创造力的启示。|
|**2024-07-23**|**Exploring Automatic Cryptographic API Misuse Detection in the Era of LLMs**|Yifan Xia et.al.|[2407.16576](http://arxiv.org/abs/2407.16576)|null|本文探讨了大型语言模型（LLM）在检测加密API误用方面所面临的挑战与机遇。在当前自动化检测技术进步的基础上，对于复杂目标的精确度下降主要归因于手动定义模式的依赖。LLM以其上下文理解能力，在此关键安全领域展现出巨大的潜力。然而，将LLM应用于这一领域存在挑战，尤其是由于它们固有的随机性和众所周知的幻觉问题导致的不稳定性。  为了系统地评估LLM在检测加密误用方面的可靠性，并探索潜在解决方案，本文提出了一种全面的评估框架，利用涵盖人工构建样本和实际项目的大规模数据集进行分析。通过深入分析11,940份LLM生成的报告，我们揭示了LLM固有不稳定性的普遍存在，导致超过一半的报告被误报为误用。然而，我们证明了通过限制问题范围并与LLM的自我修正能力相结合，可以显著提高检测的可靠性。优化的方法实现了接近90%的检测率，超越传统方法，并在现有基准中发现了未被发现的误用。此外，我们识别了持续阻碍LLM可靠性的失败模式，包括加密知识不足和代码语义误解。  基于这些洞察，我们开发了一种以LLM为基础的工作流程来检查开源仓库，最终发现了63个真实的加密误用案例。其中46个已被开发社区认可，23个正在处理中，6个已得到解决。考虑到开发者反馈，我们提供了未来研究和LLM安全工具发展的建议。|
|**2024-07-23**|**Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models**|Ioana Buhnila et.al.|[2407.16565](http://arxiv.org/abs/2407.16565)|**[link](https://github.com/ATILF-UMR7118/pRAGe)**|近期，大型语言模型（LLMs）的广泛应用对公众而言变得愈发便捷。这可能导致人们在医疗建议方面使用此类模型的情况难以追踪。大型语言生成模型存在两个关键问题：首先，它们容易出现错误推理，因此用于医疗目的时需要具备科学性和事实性；其次，由于模型规模巨大，对计算资源构成重大挑战。  本研究引入了一种名为pRAGe的管道，旨在通过小型语言模型（SLM）进行检索增强生成与评估，以实现法语医学短语生成。我们探讨了小型语言模型的有效性以及外部知识库在医学短语生成中的影响。|
|**2024-07-23**|**Patched RTC: evaluating LLMs for diverse software development tasks**|Asankhaya Sharma et.al.|[2407.16557](http://arxiv.org/abs/2407.16557)|**[link](https://github.com/codelion/optillm/blob/main/rto.py)**|这篇论文提出了一种名为“补丁往返正确性（Patched RTC）”的新型评估方法，应用于大型语言模型（LLMs）在多种软件开发任务中的应用，特别是“外循环”活动，如错误修复、代码审查和文档更新。Patched RTC是对原往返正确性方法的扩展，适用于任何LLM和下游任务，提供了一个自我评估框架，无需人工干预即可测量模型响应的一致性和稳健性。研究显示了Patched RTC分数与特定任务准确性指标之间的相关性，将其作为替代LLM-as-Judge范式的方法，用于开放域任务评估。我们通过一个名为patchwork的开源框架实现Patched RTC，在各种补丁流中实现了对不同软件开发任务的透明评估。  比较GPT-3.5和GPT-4模型在不同软件开发任务上的实验结果揭示了Patched RTC能够有效地区分模型性能和任务难度。论文还探讨了一致性提示对提高模型准确性的影响，表明Patched RTC可以指导提示优化和模型选择，以适应复杂的软件开发流程。|
|**2024-07-24**|**MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues**|Liyun Zhang et.al.|[2407.16552](http://arxiv.org/abs/2407.16552)|null|在视觉、听觉和语言等多模态线索的视频中，多模态大型语言模型（MLLMs）展示了卓越的多模态情绪识别能力，能够综合这些线索来识别人类的情绪状态。然而，现有的方法忽视了捕捉面部微表情的时间动态局部特征以及视频中话语意识片段的上下文依赖性，从而在一定程度上限制了它们的有效性。为此，我们提出了一种时间敏感的多模态大型语言模型MicroEmo，旨在将注意力集中于面部微表情的时间动态细节和视频中的话语意识片段的上下文依赖性。  我们的模型包含了两个关键的架构贡献：  1. 全局-局部注意力视觉编码器，它结合了全局帧级时间绑定图像特征与面部微表情的时间动态局部特征，实现了对整体和局部信息的有效融合； 2. 一个话语意识的视频Q-Former，它通过为每个话语段落和整个视频生成视觉令牌序列来捕获多层次和上下文依赖性，然后将它们组合在一起，以捕捉多尺度的上下文依赖关系。  初步的定性实验表明，在一个利用多模态和多方面线索以开放词汇（OV）方式预测情绪的新解释性多模态情绪识别（EMER）任务中，MicroEmo相较于最新的方法显示出了其有效性。|
|**2024-07-23**|**AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game**|Yizhou Chi et.al.|[2407.16521](http://arxiv.org/abs/2407.16521)|null|战略性的社交推断游戏是评估语言模型理解和推理能力的宝贵实验平台，对于社会科学研究、人工智能领域以及策略性游戏都有重要价值。本文集中于在模拟环境中构建人类行为的代理，使用《Among Us》作为研究模拟人类行为的工具。通过创建一个基于文本的游戏环境，称为AmongAgent，该环境复制了《Among Us》的游戏动态。玩家扮演太空船上的船员，任务是识别破坏太空船的冒名顶替者并消除船员。在这个环境中，模拟语言代理的行为被分析。实验涉及不同船员和冒名顶替者人格原型配置的多样化的游戏序列。我们的工作表明，最先进的大型语言模型能够有效地掌握游戏规则，并根据当前上下文做出决策。这项工作旨在促进对在信息不完整和复杂动作空间中的目标导向游戏中的语言模型性能进行进一步探索，这些设置提供了评估语言模型在社会驱动场景中表现的宝贵机会。|
|**2024-07-22**|**AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description**|Junyu Xie et.al.|[2407.15850](http://arxiv.org/abs/2407.15850)|**[link](https://github.com/Jyxarthur/AutoAD-Zero)**|**我们的目标是无需训练地生成电影和电视连续剧的音频描述（AD）。我们利用现成的视觉语言模型（VLM）和大型语言模型（LLM），并开发了视觉和文本提示策略来完成这项任务。我们的贡献有三点：(i) 我们证明，如果通过视觉指示直接提示VLM提供角色信息，VLM可以成功命名和引用角色，无需任何微调；(ii) 我们开发了一种两阶段过程来生成AD，第一阶段让VLM全面描述视频，第二阶段使用LLM将密集的文本信息总结为一个简洁的AD句子；(iii) 我们制定了一个新的电视音频描述数据集。我们的方法AutoAD-Zero在AD生成方面表现出色（甚至与一些在真实AD上微调的模型相匹敌），实现了电影和电视连续剧的最高CRITIC评分。**|
|**2024-07-22**|**LLMmap: Fingerprinting For Large Language Models**|Dario Pasquini et.al.|[2407.15847](http://arxiv.org/abs/2407.15847)|**[link](https://github.com/pasquini-dario/LLMmap)**|我们提出了一种针对LLM集成应用的首代指纹识别攻击工具——LLMmap。该工具采用积极的指纹识别策略，通过向应用发送精心设计的查询并分析响应信息，以识别所使用的具体LLM模型。仅需8次交互，LLMmap即可在95%以上的准确率下精确识别出LLM模型。更重要的是，LLMmap被设计得具有跨不同应用层的鲁棒性，使其能够识别在各种系统提示、随机抽样超参数以及复杂的生成框架如RAG或Chain-of-Thought等环境下的LLM模型。|
|**2024-07-22**|**SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models**|Mingze Xu et.al.|[2407.15841](http://arxiv.org/abs/2407.15841)|**[link](https://github.com/apple/ml-slowfast-llava)**|我们提出了一种名为“慢速-LLaVA”（或简称为SF-LLaVA）的无需训练的视频大型语言模型（LLM），它能够同时捕捉详细的空间语义和长时序上下文，而不会超出通常使用的LLM的令牌预算。这一目标通过使用视频LLM输入的双流设计实现，有效地聚合了从采样视频帧中提取的特征。具体而言，慢速路径以较低的帧率提取尽可能多的空间细节的特征（例如，以24x24的令牌），而快速路径则以较高的帧率操作，但使用较大的空间池化步长（例如，下采样6x）来关注运动线索。因此，这种设计允许我们适当地捕获对于理解视频中的详细信息有益的时空特性。实验结果表明，SF-LLaVA在各种视频任务上都超越了现有的无需训练的方法。在某些基准测试中，它甚至与在视频数据集上进行微调的最先进的视频LLM实现了相当或更好的性能。|
|**2024-07-22**|**MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity**|Yangzhou Liu et.al.|[2407.15838](http://arxiv.org/abs/2407.15838)|**[link](https://github.com/yuecao0119/mminstruct)**|尽管视觉语言预训练模型在视觉任务上的微调表现出显著的性能提升，但现有的视觉指令调优数据集存在以下局限性：  1. 指令注释质量：虽然现有的视觉语言预训练模型在性能上表现出色，但它们生成的指令可能仍会包含不准确性，如幻觉现象。  2. 指令和图像多样性：指令类型范围有限以及图像数据缺乏多样性可能会影响模型生成多样性和接近真实世界场景输出的能力。  为解决这些挑战，我们构建了一个高质量、多样性的视觉指令调优数据集MMInstruct，包含来自24个领域共计973K条指令。该数据集包括四种指令类型：判断、多项选择、长视觉问题回答和短视觉问题回答。  为了构建MMInstruct，我们提出了一种指令生成数据引擎，利用GPT-4V、GPT-3.5和人工校正。我们的指令生成引擎允许半自动、低成本、多领域的指令生成，成本仅为手动构建的六分之一。  通过广泛的实验验证和消融实验，我们证明了MMInstruct能够显著提高视觉语言预训练模型的性能，例如，基于MMInstruct的模型微调在12个基准中的10个上达到了新的状态最优表现。代码和数据将在https://github.com/yuecao0119/MMInstruct提供。|
|**2024-07-22**|**dMel: Speech Tokenization made Simple**|He Bai et.al.|[2407.15835](http://arxiv.org/abs/2407.15835)|null|大型语言模型通过利用大规模文本数据的自我监督预训练，在自然语言处理领域实现了革命性的进步。受此成功启发，研究人员探索了复杂语音分词方法，以将连续的语音信号离散化，从而使语言建模技术可以应用于语音数据。然而，现有方法要么建模语义令牌，可能会丢失声学信息，要么建模声学令牌，又可能面临丢失语义信息的风险。具有多种令牌类型也使架构变得复杂，并需要额外的预训练。我们展示了将梅尔滤波器通道离散化为离散强度单元（dMel）产生了一个简单表示，其性能优于其他现有语音分词方法。使用仅解码器的变换器架构进行语音-文本建模，我们全面评估了不同的语音分词方法在语音识别（ASR）和语音合成（TTS）任务上的性能。我们的结果表明，dMel在联合建模语音和文本的统一框架中实现高性能的有效性，为高效且有效的语音与文本联合建模铺平了道路。|
|**2024-07-22**|**Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight**|Ziyuan Huang et.al.|[2407.15819](http://arxiv.org/abs/2407.15819)|null|这篇论文提出了一种名为“链视图”的视觉-语言桥梁模块，旨在加速多模态大型语言模型（MLLMs）的预训练过程。我们的方法采用了序列化的视觉重采样器，能够有效地捕捉不同空间尺度的视觉细节。这种架构不仅能够有效利用全局和局部视觉上下文，还通过复合令牌缩放策略灵活扩展视觉令牌的数量，最多可以增加16倍的令牌数量，而无需在预训练后进行微调。因此，“链视图”在预训练阶段所需的视觉令牌数量远少于微调阶段，这有意地减少了视觉令牌的数量，显著加速了预训练过程，节省了大约73%的实际训练时间。  在一系列视觉-语言基准测试上的实验结果表明，通过“链视图”加速预训练过程并不会牺牲性能，其表现与在整个训练过程中使用所有视觉令牌的标准流程相当或更好。进一步增加预训练阶段的视觉令牌数量会导致更强的表现，在一系列基准测试中与现有方法竞争。  请注意，上述摘要已经转换成了中文表述，并且遵循了不包含特殊符号的指示。|
|**2024-07-22**|**Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach**|Rian Dolphin et.al.|[2407.15788](http://arxiv.org/abs/2407.15788)|null|金融新闻在金融市场决策过程中扮演着关键角色，但将其转化为结构化数据的过程一直充满挑战。本文提出了一种新颖的金融新闻处理方法，利用大型语言模型（LLMs）克服了以往提取结构化信息时遇到的限制。我们引入了一套系统，该系统能够从原始新闻文章内容中提取相关公司代码，并在不依赖于预结构化数据流的情况下进行公司层面的情绪分析和生成摘要。我们的方法结合了LLMs的生成能力、以及最新的提示技术，配以一个定制的字符串相似度验证框架。  通过使用包含5530篇金融新闻文章的数据集进行评估，证明了我们的方法的有效性。相比现有数据提供商，我们有90%的文章不会遗漏任何公司代码，而有22%的文章会额外提供相关的公司代码。此外，我们还实现了这一方法的大规模部署，并通过实时API端点提供了经过处理的数据，更新了最新新闻信息。据我们所知，这是我们首次作为数据提供商提供从新闻文章中对每个公司的细致情绪分析服务，增强了市场参与者可获取的信息深度。  为了促进进一步的研究利用金融新闻，我们还发布了包含5530篇处理后文章的评估数据集。|
|**2024-07-22**|**MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation**|Marco Simoni et.al.|[2407.15748](http://arxiv.org/abs/2407.15748)|null|在这篇论文中，我们引入了MoRSE（混合RAG安全专家），这是首个专门的AI聊天机器人用于网络安全。MoRSE旨在提供全面且完整的网络安全知识。MoRSE使用了两个RAG（检索增强生成）系统，设计用于从多维度的网络安全上下文中检索和组织信息。与传统的RAG不同，MoRSE采用了并行检索器协同工作，以在不同格式和结构中检索语义相关的信息。  不同于依赖参数知识库的传统大型语言模型（LLMs），MoRSE响应用户查询时从非参数知识库中检索相关文档。随后，MoRSE利用这些信息生成准确的答案。此外，MoRSE受益于其知识库的实时更新，这使得系统能够在不重新训练的情况下持续的知识丰富。  我们对MoRSE的有效性进行了评估，针对600个特定的网络安全问题进行了实验性评估。实验结果表明，与GPT-4、Mixtral 7x8等已知解决方案相比，在答案的相关性和正确性的改进上超过了10%。|
|**2024-07-22**|**OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context**|Steffen Kleinle et.al.|[2407.15736](http://arxiv.org/abs/2407.15736)|null|当移民到一个新的国家时，人们很容易因需要获取有关财政支持、住房、教育、语言课程以及其他问题的信息而感到不知所措。如果搬迁过程匆忙或甚至被迫进行，对这些问题的高质量解答变得尤为迫切。官方移民顾问通常过于繁忙，而在线系统可以引导新移民找到所需信息或合适的咨询服务。因此，我们提出了OMoS-QA数据集，它包含德语和英语问题与相关可信文档以及手动标注的答案，专门针对这一场景。问题是由开源大型语言模型（LLM）自动生成的，答案句子由具有高度一致性的众包工作者选择。通过我们的数据，我们在德语和英语上对5个预训练的LLM进行了提取式问答任务的比较。在所有模型和两种语言中，选择答案句子的精确度高，召回率低至中等，这是一个有利的权衡，以避免误导用户。这种性能即使在问题语言与文档语言不匹配时也能保持不变。在根据上下文识别不可回答的问题方面，两种语言之间存在更大的差异。|
|**2024-07-22**|**TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON**|John Chong Min Tan et.al.|[2407.15734](http://arxiv.org/abs/2407.15734)|**[link](https://github.com/simbianai/taskgen)**|TaskGen是一个开源的代理框架，通过使用代理来解决任意任务并将其分解为子任务来实现。每个子任务被映射到一个装备函数或另一个代理执行。为了减少冗余（从而减少令牌使用），TaskGen使用了StrictJSON，确保大型语言模型（LLM）输出的JSON格式，并具备类型检查和迭代错误修正等额外功能。TaskGen的核心理念在于基于需求管理信息/记忆。  我们对TaskGen在各种环境中进行了实证评估，包括40x40动态迷宫导航，其中障碍物位置会变化（100%的成功率），文本世界逃脱房间解谜，具有密集奖励和详细目标（96%的成功率），网络浏览（69%的动作成功），解决MATH数据集（在100个Level-5问题上，成功率71%），以及自然问题检索增强生成（F1分数为47.03%）。|
|**2024-07-19**|**Internal Consistency and Self-Feedback in Large Language Models: A Survey**|Xun Liang et.al.|[2407.14507](http://arxiv.org/abs/2407.14507)|**[link](https://github.com/iaar-shanghai/icsfsurvey)**|**本文总结了一个理论框架，称为内部一致性（Internal Consistency），它为大型语言模型（LLM）在推理不足和生成幻觉内容等问题上的表现提供了一致的解释。内部一致性评估了LLM的潜在层、解码层和响应层之间的内在一致性，基于采样方法。  在此基础上，我们引入了Self-Feedback框架，这是一个简洁而有效的理论框架，用于挖掘内部一致性的信息。Self-Feedback框架包括两个模块：自我评估（Self-Evaluation）和自我更新（Self-Update）。  我们系统地按任务和研究方向对这些研究进行了分类；总结了相关的评估方法和基准；深入探讨了“Self-Feedback真的有效吗？”这一问题。我们提出了几个关键观点，包括“内部一致性的发展钟楼”、“一致性几乎是正确性”的假设以及“潜意识与显式推理悖论”。此外，我们还概述了未来研究的有前景的方向。  我们已经开源了实验代码、参考列表和统计数据，供公众访问，链接为：[](https://github.com/IAAR-Shanghai/ICSFSurvey)**|
|**2024-07-19**|**On Pre-training of Multimodal Language Models Customized for Chart Understanding**|Wan-Cyuan Fan et.al.|[2407.14506](http://arxiv.org/abs/2407.14506)|null|近期的研究在针对特定领域任务定制多模态大型语言模型（MLLMs）方面取得了令人鼓舞的成果，特别是在科学图表理解领域。这些研究通常通过使用专门的数据集进行视觉指令调优来增强问答（QA）准确性。然而，它们往往忽视了自然图像-描述预训练数据与数字图表图像-QA数据之间的基本差异，特别是对于模型从图表中提取潜在数值的能力。本文旨在解决这一疏漏，探索改进MLLMs对图表理解所需的关键训练过程。我们提出了三个关键发现：（1）在对齐预训练过程中融入原始数据值显著提高了对图表数据的理解能力。（2）在端到端微调过程中随机替换图像为文本表示，能够将语言推理能力转移到图表解释技能上。（3）要求模型首先提取底层图表数据，然后在微调过程中回答问题，可以进一步提高准确性。  因此，我们引入了CHOPINLLM，一种专为深入图表理解定制的MLLM。CHOPINLLM有效地解析各种类型的图表，包括未标注的图表，同时保持了强大的推理能力。此外，我们建立了一个新的基准，用于评估MLLMs在不同图表类型和理解水平上的理解能力。实验结果表明，CHOPINLLM在理解各种类型、带有标注和未标注的图表方面表现出强大的性能。|
|**2024-07-19**|**Evaluating the Reliability of Self-Explanations in Large Language Models**|Korbinian Randl et.al.|[2407.14487](http://arxiv.org/abs/2407.14487)|**[link](https://github.com/k-randl/self-explaining_llms)**|**本文探讨了大型语言模型在被提示解释其先前输出时生成的解释可靠性。我们利用三种先进的大语言模型（参数从2B到8B）在两种不同的分类任务（客观和主观）上评估了两种类型的自我解释——抽取式和反事实式。我们的发现表明，尽管这些自我解释与人类判断相关联，但它们并不完全且准确地遵循模型的决策过程，指出了一种感知与实际模型推理之间的差距。我们显示，通过提示大语言模型进行反事实解释，可以产生忠实、信息丰富且易于验证的结果。这些反事实为传统可解释性方法（例如SHAP、LIME）提供了有前景的替代方案，前提是对特定任务定制提示并检查其有效性。**|
|**2024-07-19**|**Contrastive Learning with Counterfactual Explanations for Radiology Report Generation**|Mingjie Li et.al.|[2407.14474](http://arxiv.org/abs/2407.14474)|null|由于解剖学的常见内容和与之对应的影像学图像之间的高度相似性，这种固有的数据偏见可能导致自动报告生成模型学习纠缠和相关性增强的表示，从而产生误诊报告。为了应对这些问题，我们提出了一种新颖的“Co”unter“F”actual “E”xplanations（CoFE）框架用于放射学报告生成。反事实解释是一种强大的工具，用于理解算法决策如何通过提出“如果”场景而被改变。通过利用这一概念，CoFE可以通过对比正例和负例之间的表示来学习非相关性视觉表示，从而学习非相关性视觉表示。具体来说，通过在正例和负例之间交换补丁直到预测诊断发生变化，我们推导出反事实图像。在这里，正例和负例是最语义上相似的，但具有不同的诊断标签。此外，CoFE采用可学习提示高效地对预训练的大语言模型进行微调，封装了正事实例和反事实实例的内容，提供更通用的提示表示。在两个基准上的广泛实验表明，利用反事实解释使CoFE能够生成语义上连贯且事实完整的报告，并在语言生成和临床有效性指标方面表现出色。|
|**2024-07-19**|**Check-Eval: A Checklist-based Approach for Evaluating Text Quality**|Jayr Pereira et.al.|[2407.14467](http://arxiv.org/abs/2407.14467)|null|评估大型语言模型（LLM）生成文本的质量仍然是一个重大挑战。传统的评估标准往往与人类的判断不匹配，尤其是在需要创造性和细微差别的任务中。本文提出了一种名为Check-Eval的新评估框架，通过利用LLM以检查表为基础的方法来评估生成文本的质量。Check-Eval可以作为无参考和有参考的评估方法使用，提供了一个结构化且可解释的文本质量评估体系。该框架主要由两个阶段组成：检查表生成和检查表评估。我们在两个基准数据集上验证了Check-Eval：葡萄牙语法律语义文本相似性以及SummEval。我们的结果显示，Check-Eval与现有指标（如G-Eval和GPTScore）相比，在与人类判断的相关性方面取得了更高的分数，这表明其作为自然语言生成任务更可靠和有效的评估框架的潜力。我们的实验代码可在https://anonymous.4open.science/r/check-eval-0DB4获取。|
|**2024-07-19**|**Undermining Mental Proof: How AI Can Make Cooperation Harder by Making Thinking Easier**|Zachary Wojtowicz et.al.|[2407.14452](http://arxiv.org/abs/2407.14452)|null|大型语言模型和其他高度先进的AI系统在决定说什么或做什么时提供了便利，但这便利性实际上削弱了在社会情境下采取有效行动的能力。我们通过引入“心理证明”这一整合性理论概念来解释这种看似矛盾的现象。“心理证明”发生在使用可观察的行为来证实不可观察的心理事实的情况中。从招聘到约会，“心理证明”使人们能够在低信任环境中相互传达价值观、意图、知识状态等心理特征，这些环境中的诚实难以得到强制执行。  基于经济学、理论生物学和计算机科学的研究成果，我们描述了使人类能够实施心理证明的核心理论机制。对这些机制的分析揭示了人工智能如何在使思考变得容易的同时，却可能使低信任合作变得更难。  通过理解心理证明的工作原理及其在不同情境下的应用，我们可以设计出既能促进高效沟通又能维护社会协作的AI系统。例如，在招聘过程中，AI可以通过分析候选人的行为模式和历史数据来间接评估其技能、团队合作能力以及对公司文化的适应性，从而帮助雇主做出更可靠的人才选择决策。在约会场景中，AI可以利用社交媒体活动、兴趣爱好等信息来构建用户的心理画像，以此帮助用户找到与自己价值观和生活方式相匹配的伴侣。  总之，通过合理地设计和应用AI技术，我们不仅可以在低信任环境下增强人类的交流和合作能力，而且还能促进更加公正、透明和高效的决策过程。|
|**2024-07-19**|**Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding**|Renshan Zhang et.al.|[2407.14439](http://arxiv.org/abs/2407.14439)|**[link](https://github.com/JiuTian-VL/TokenCorrCompressor)**|**当前主流的多模态大型语言模型（Multimodal Large Language Models, MLLMs）在进行文档理解时，普遍采用对高分辨率文档图像进行裁剪，从而生成多个子图像的方法。大多数现有的文档理解方法会保留所有子图像内的标记，并同等对待它们，这忽视了这些标记的不同信息价值性，导致了大量不必要的图像标记增加。为了实现更加适应性和高效的文档理解，我们提出了一种名为“Token级相关性引导压缩”的无参数且可插拔方法，旨在优化标记处理过程。该方法首先引入了一种创新的评估模式重复性的方法，基于每个片段标记之间的相关性进行。这种方法能够识别冗余标记，从而确定子图像的信息密度。其次，我们提出了一个针对Token级别的采样方法，通过深入分析[CLS]标记与片段标记之间的相关性，高效捕捉最具信息价值的标记。通过结合这两种策略，我们开发了一个可无缝集成到使用裁剪技术的MLLMs中的自适应压缩模块。这一模块不仅在训练和推理过程中显著提升了处理速度，同时保持了与现有压缩方法相当的性能水平。我们使用当前最佳的文档理解模型mPLUG-DocOwl1.5进行了实验，并通过与其他压缩方法的广泛对比，验证了其有效性。**|
|**2024-07-19**|**The Vision of Autonomic Computing: Can LLMs Make It a Reality?**|Zhiyang Zhang et.al.|[2407.14402](http://arxiv.org/abs/2407.14402)|null|《自治计算愿景与大型语言模型在微服务管理中的应用》一文回顾了超过二十年前提出的自治计算（ACV）愿景，旨在构建能够自我管理和适应环境变化的计算系统，这一目标至今仍面临挑战。近年来，大型语言模型（LLMs）的发展为解决这些挑战提供了可能，它们通过利用广泛的知识、语言理解能力以及任务自动化能力来实现这一愿景。  本文探讨了通过基于LLM的多代理框架实现微服务管理自主性的可行性，并提出了一个五级分类体系，用于描述自主服务维护的不同层次。文中还设计了一个基于“Sock Shop”微服务演示项目的在线评估基准，以评估该框架的性能。研究结果表明，通过LLMs可以显著提升微服务体系结构中问题检测和解决的能力，实现了第三级自主性水平的突破，这标志着大型语言模型在微服务管理框架集成方面的应用取得了重要进展，为构建更适应性和自我管理的计算系统铺平了道路。  为了促进这一领域的研究和发展，相关的代码将通过<https://aka.ms/ACV-LLM>公开提供。|
|**2024-07-19**|**Open Artificial Knowledge**|Vadim Borisov et.al.|[2407.14371](http://arxiv.org/abs/2407.14371)|null|《开放人工知识（OAK）数据集：促进大型语言模型发展与解决数据稀缺与隐私问题》  当前，基于对话的AI系统如ChatGPT、Claude和Gemini的成功，主要得益于大规模语言模型（LLMs）对海量数据集的训练。然而，获取高质量、多样性和伦理来源的数据仍然面临重大挑战。我们提出了一种名为“开放人工知识”（OAK）数据集，这是一个包含超过5亿个令牌（撰写时）的大型资源库。OAK通过集合包括GPT4o、LLaMa3-70B、LLaMa3-8B、Mixtral-8x7B、Gemma-7B和Gemma-2-9B在内的最先进的LLMs，利用维基百科的主要类别来引导文本生成，确保广泛的领域覆盖，同时保持连贯性和事实准确性。OAK数据集旨在促进更强大、更对齐的语言模型的发展，并解决大规模语言模型训练中的关键问题，如数据稀缺性和隐私问题。目前，该数据集是免费提供在www.oakdataset.org。|
|**2024-07-19**|**Enhancing Zero-shot Audio Classification using Sound Attribute Knowledge from Large Language Models**|Xuenan Xu et.al.|[2407.14355](http://arxiv.org/abs/2407.14355)|**[link](https://github.com/wsntxxn/attrenhzsac)**|这篇论文提出了一种新的方法来进行零样本音频分类，即识别和分类模型在训练过程中从未见过的音频类别。我们提议列出一系列音频属性，并利用大型语言模型的领域知识为每个类别生成详细的属性描述。与以往主要依赖类别标签或简单描述的方法不同，我们的方法专注于多维度的内在听觉属性，捕捉音频类别的不同特性。此外，我们还采用了对比学习方法来增强基于文本标签的零样本学习。我们在VGGSound和AudioSet上验证了我们方法的有效性（代码可访问：https://www.github.com/wsntxxn/AttrEnhZsAc）。结果表明，在零样本分类准确性方面取得了显著提高。消融实验结果显示，无论模型架构如何，性能增强都非常稳健。|
|**2024-07-18**|**SegPoint: Segment Any Point Cloud via Large Language Model**|Shuting He et.al.|[2407.13761](http://arxiv.org/abs/2407.13761)|null|尽管三维点云分割领域取得了显著进展，但现有的方法主要针对特定任务，依赖于明确的指令来识别目标，缺乏在统一框架中理解和推断用户隐含意图的能力。本文提出了一种名为SegPoint的模型，它利用多模态大型语言模型（LLM）的推理能力，在多种任务上进行点级分割：1）三维指令分割，2）三维指称分割，3）三维语义分割，以及4）三维开放词汇语义分割。为了推动三维指令研究，我们创建了一个新的基准Instruct3D，用于评估从复杂和隐含指令文本进行分割性能，包含2,565个点云-指令对。实验结果显示，SegPoint在ScanRefer指称分割和ScanNet语义分割等既有基准上表现出竞争力，同时在Instruct3D数据集上的表现优异。据我们所知，SegPoint是首个在一个框架内处理这些多样化的分割任务并达到满意性能的模型。|
|**2024-07-18**|**Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models**|Zhuo Chen et.al.|[2407.13757](http://arxiv.org/abs/2407.13757)|null|## 任务  本研究关注于Retrieval-Augmented Generation（RAG）模型在面对黑盒攻击时的脆弱性，尤其是在意见操纵方面的应用。RAG旨在解决大语言模型的幻觉问题和实时约束，但同时也暴露出对抗检索篡改攻击的弱点。当前的研究主要集中在白盒和封闭领域问答任务中的RAG不稳定性。本文的目标是揭示当RAG模型遭遇黑盒攻击时，对用户认知和决策的影响，从而为提高模型的可靠性和安全性提供新见解。  我们通过操控RAG中检索模型的排名结果，利用这些操纵后的数据训练一个代理模型。接着，采用对抗性检索攻击方法针对代理模型实施黑盒迁移攻击，进一步影响RAG的生成过程。在涉及多个主题的意见数据集上进行实验，结果显示，提出的攻击策略能显著改变RAG生成内容的观点极性，这揭示了模型的易受攻击性，并且潜在地指出对用户认知和决策的负面影响，使得误导用户接受错误或有偏见的信息变得更加容易。|
|**2024-07-18**|**CellularLint: A Systematic Approach to Identify Inconsistent Behavior in Cellular Network Specifications**|Mirza Masfiqur Rahman et.al.|[2407.13742](http://arxiv.org/abs/2407.13742)|null|近年来，人们越来越关注蜂窝网络的安全性，常常将安全漏洞归咎于底层协议设计描述的问题。这些通常长达数千页的详细规格文档可能包含错误、不完整描述、隐含假设和内部矛盾。鉴于此，我们提出CellularLint——一个针对4G和5G非接入层（Non-Access Stratum，NAS）和安全规范的半自动框架，利用一套自然语言处理技术。我们的方法基于领域适应的大语言模型进行改良的少量样例学习。该模型预训练在大量的蜂窝网络协议数据上，能够同时检测不同语义层次和实际使用案例中的不一致性，以一种可扩展的方式提升协议规格的自动化分析。通过研究，我们在4G和5G网络中发现了157个不一致点，准确率为82.67%。经过对开源实现和17款商用设备的验证，我们确认这些不一致确实对设计决策有重大影响，可能导致隐私、完整性、可用性和互操作性方面的担忧。|
|**2024-07-18**|**Baba Is AI: Break the Rules to Beat the Benchmark**|Nathan Cloos et.al.|[2407.13729](http://arxiv.org/abs/2407.13729)|null|人类解决问题既依赖于遵循现有规则和程序，也依赖于创新思维来重新定义规则和目标。为了检验这些能力，我们设计了一个新的基准，它基于游戏《Baba Is You》。在这个游戏中，代理需要操控环境中的物体和可移动的文字规则瓷砖，以实现特定目标并赢得比赛。我们测试了三种最先进的多模态大型语言模型（OpenAI GPT-4、Google Gemini-1.5-Pro和Gemini-1.5-Flash），发现当需要对游戏规则进行操纵和组合时，它们的表现大幅下滑。|
|**2024-07-18**|**CoDefeater: Using LLMs To Find Defeaters in Assurance Cases**|Usman Gohar et.al.|[2407.13717](http://arxiv.org/abs/2407.13717)|**[link](https://gitlab.com/anonymousdot/codefeater)**|构建保证案例是一种常用且有时必要的方法，用于证明安全关键系统在其规划环境中将安全运行。为了降低错误和边缘情况遗漏的风险，引入了“反驳”概念，即挑战保证案例中论点或证据的论据。反驳有助于及时发现论点中的弱点，促使进一步调查和及时补救。然而，捕捉反驳依赖于专家判断、经验和创新思维，并且必须随着需求和法规的变化进行迭代。这篇论文提出CoDefeater，一种利用大型语言模型（LLMs）来自动寻找反驳的自动化过程。初步结果表明，LLMs能够有效地找到已知和未知的合理反驳，从而帮助安全分析师增强保证案例的完整性和信心。|
|**2024-07-18**|**Understanding Reference Policies in Direct Preference Optimization**|Yixin Liu et.al.|[2407.13709](http://arxiv.org/abs/2407.13709)|**[link](https://github.com/yale-nlp/refdpo)**|## 背景  直接偏好优化（Direct Preference Optimization，简称 DPO）已成为大型语言模型（Large Language Models，LLMs）指令微调的常用训练方法。本研究关注DPO的一个未充分探讨的方面：其对参考模型或策略的依赖性。这些参考策略通常表现为待进一步微调的模型，它们对于DPO的效果至关重要。因此，本工作针对以下三个相关问题进行了探究：  1. 首先，我们研究了DPO中的KL散度约束强度的最佳选择，该约束惩罚与参考策略的偏离，发现DPO对此敏感。 2. 其次，我们从理论和实证上比较了DPO与其他学习目标，以探讨参考策略在指令微调中的必要性，并显示了DPO的优势。 3. 最后，我们探讨了更强的参考策略是否有利于DPO，结果表明，当参考策略与被微调模型相似时，更强的参考策略可能会提高性能。  我们的发现揭示了参考策略在DPO中的混淆作用，提供了最佳实践的见解，同时也为未来研究提出了开放性问题。|
|**2024-07-18**|**A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**|Shaina Raza et.al.|[2407.13699](http://arxiv.org/abs/2407.13699)|null|## 背景  推荐系统（RS）通过提供个性化项目建议，对提升用户体验至关重要。本综述回顾了从2017年至2024年间RS领域的进展，将理论创新与实际应用紧密结合。我们探讨了从传统方法如基于内容和协同过滤的推荐，到高级技术如深度学习、图模型、强化学习以及大型语言模型的发展。此外，我们还关注了专门化的系统，如上下文感知、评论驱动和公平性考量的RS。本调查的目标是连接理论与实践，关注电子商务、医疗保健和金融等领域的挑战，强调对可扩展、实时且值得信赖解决方案的需求。通过此综述，我们鼓励学术研究与行业实践的紧密合作。本研究提供的洞见旨在帮助业界专业人员优化RS部署，并激发未来研究的新方向，特别是在应对新兴技术和社会趋势时。|
|**2024-07-18**|**Prover-Verifier Games improve legibility of LLM outputs**|Jan Hendrik Kirchner et.al.|[2407.13692](http://arxiv.org/abs/2407.13692)|null|为了提高大型语言模型（LLMs）输出结果的可信度，一个方法是支持清晰易验证的推理，我们称之为可读性。本文以解决小学数学问题为背景，研究了可读性，并发现仅优化连贯思维解题的准确性可能会降低其可读性。为缓解这一损失，我们提出了一种受Anil等人（2021）的证明器-验证器游戏启发的训练算法。该算法迭代地训练小型验证器预测解题正确性，"有帮助"的证明器生成验证器接受的正确解答，以及"狡猾"的证明器生成欺骗验证器的错误解答。实验表明，有帮助证明器的准确性和验证器对抗攻击的鲁棒性在训练过程中提高。此外，我们发现，针对小型验证器的可读性训练能够转移给时间有限的人类，他们在验证解决方案正确性时的准确性会随着训练提高，而在验证狡猾证明器的解决方案时会下降。因此，通过小型验证器进行可读性训练可能是一种实际可行的方法，用于提升大型LLMs对人类的可读性，从而有助于超级人类模型的对齐。我们的结果表明，对小型验证器的可读性训练是一个实用的途径，可以增强大型LLMs的可读性，对人类来说更易于理解和信任。|
|**2024-07-18**|**COMCAT: Leveraging Human Judgment to Improve Automatic Documentation and Summarization**|Skyler Grandel et.al.|[2407.13648](http://arxiv.org/abs/2407.13648)|null|这篇论文主要探讨了软件维护中代码理解的重要性，以及如何通过自动化生成注释来提升这一过程。作者提出了一种名为COMCAT的方法，它结合大型语言模型（LLMs）与领域专家指导，旨在为源代码提供有助于理解的注释。COMCAT流程包括自动识别代码中适合添加注释的位置、预测每个位置最适合的注释类型，并根据选定位置和类型生成注释。在人类受试者的研究中，结果显示COMCAT生成的注释显著提高了开发人员在三个典型软件工程任务中的代码理解能力，对于87%的参与者，提升幅度达到12%。此外，研究还表明COMCAT生成的注释在准确性、可读性上至少与人工注释相当，并且在92%的代码片段中，开发者更偏好COMCAT生成的注释，而非标准的ChatGPT生成的注释。论文还介绍了开发并公开了一个包含源代码片段、人工编写注释和标注的类别数据集。总的来说，COMCAT利用LLMs在多种软件工程任务中显著提升了代码理解水平。|
|**2024-07-18**|**Weak-to-Strong Reasoning**|Yuqing Yang et.al.|[2407.13647](http://arxiv.org/abs/2407.13647)|**[link](https://github.com/gair-nlp/weak-to-strong-reasoning)**|当大型语言模型（LLMs）的性能超越人类时，为其提供全面而精确的监督变得困难。在这种情况下，弱到强学习方法，即利用能力较弱的模型激发较强模型的潜在能力，显示出价值。然而，这种策略在处理复杂推理任务时的效果尚未得到充分检验，且当前缺乏有效的方法来避免模型盲目模仿弱导师，包括其错误。本文提出了一种渐进学习框架，使强模型能够自主优化其训练数据，无需依赖高级模型或人工标注的数据。该框架首先对选定的小而高质量数据进行监督微调，然后在强模型自行识别的对比样本上进行偏好优化。我们在GSM8K和MATH数据集上的大量实验表明，我们的方法显著提升了Llama2-70b的推理能力，通过三种不同的弱模型进行验证。此外，我们还在前瞻性的实验设置中验证了这种方法，Llama3-8b-instruct成功指导Llama3-70b在极具挑战性的OlympicArena数据集上。这项工作为提升人工智能的推理能力提供了一种更可扩展和高级的策略。所有相关代码和资源可在<https://github.com/GAIR-NLP/weak-to-strong-reasoning>获取。|
|**2024-07-17**|**EchoSight: Advancing Visual-Language Models with Wiki Knowledge**|Yibin Yan et.al.|[2407.12735](http://arxiv.org/abs/2407.12735)|null|**摘要：**  知识驱动的视觉问答（KVQA）任务要求利用丰富背景知识解答图像相关问题，但生成模型在这方面常面临挑战。为此，我们提出EchoSight，一个新颖的多模态检索增强生成（Retrieval-Augmented Generation，RAG）框架，旨在帮助大型语言模型（LLMs）处理需要详尽百科知识的视觉问答。EchoSight首先仅使用图像信息在维基百科中搜索文章，然后对候选文章根据它们与文本-图像查询的相关性进行二次排序，从而显著提升多模态知识的整合，进而提高检索效果和答案的准确性。我们在Encyclopedic VQA和InfoSeek数据集上的实验结果表明，EchoSight在知识型视觉问答中实现了新的state-of-the-art成绩，Encyclopedic VQA任务上达到41.8%的准确率，InfoSeek任务上达到31.3%。|
|**2024-07-17**|**NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model**|Zhongqun Zhang et.al.|[2407.12727](http://arxiv.org/abs/2407.12727)|null|### 背景  在三维手部-物体重建中，精确的手部与物体之间的物理接触是提升手部姿态估计准确性和生成新的人类抓握动作的标准。然而，现有的方法依赖于难以定义或控制的几何约束。本文提出了一项新的任务：通过自然语言描述进行可控的三维手部-物体接触建模。面临的挑战包括：一、从语言到接触的复杂跨模态建模；二、缺乏描述接触模式的文本数据。为解决这些问题，我们设计了NL2Contact模型，它利用分段扩散模型生成可控制的接触。给定对手和接触的自然语言描述，NL2Contact能够生成逼真且忠实的三维手部-物体接触。  ### 任务  我们开发了NL2Contact模型，旨在通过自然语言描述生成具有控制性的三维手部-物体接触。为训练这个模型，我们创建了首个名为\textit{ContactDescribe}的数据集，其中包含基于精心设计的提示（如抓取动作、抓取类型、接触位置和自由手指状态）生成的丰富多样的手部中心接触描述。我们的模型在优化抓握姿势和基于文本描述生成新的人类抓握动作方面展示了应用潜力。|
|**2024-07-17**|**Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**|Ben Yao et.al.|[2407.12725](http://arxiv.org/abs/2407.12725)|null|在大型语言模型（LLMs）解决复杂问题的能力方面，通过逐步推理步骤的扩展显著提升其性能，因为这促使模型进行序列思考。然而，人类对讽刺的理解通常被视为一种直觉且整体的认知过程，它整合了语言、上下文和情感线索，形成对说话者真实意图的全面理解，这种理解被认为不局限于一步步的推理过程。为了验证这一观点，我们提出了一种新的提示框架，称为SarcasmCue，它包含了四种提示策略：连锁矛盾（CoC）、线索图（GoC）、线索集合（BoC）和线索张量（ToC）。这些方法旨在引导LLMs通过考虑顺序和非顺序提示来识别人类的讽刺。我们在四个基准数据集上的全面实证比较表明，我们的四种提示方法明显优于标准的输入-输出提示、CoT和ToT，而且非顺序提示通常优于顺序提示。|
|**2024-07-17**|**The Future of Learning: Large Language Models through the Lens of Students**|He Zhang et.al.|[2407.12723](http://arxiv.org/abs/2407.12723)|null|随着大型语言模型（LLMs）的不断发展，它们在性能上的提升和功能扩展对教育领域产生了显著影响。本研究通过访谈14名学生，探讨他们日常与ChatGPT的互动。初步结果显示，学生们在享受ChatGPT提高学习效率和信息获取便利的同时，也面临着信任危机和伦理顾虑。他们认为ChatGPT相较于传统AI更显“人性化”。然而，这种矛盾情绪、行为不一致以及对学生整体上积极的态度，凸显了ChatGPT在教育领域的潜在价值。但值得注意的是，尽管其智能程度高，可能带来负面效应。因此，我们强调在应用时需谨慎，并致力于在未来的开发中减少潜在的危害。|
|**2024-07-17**|**MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models**|Leyang Shen et.al.|[2407.12709](http://arxiv.org/abs/2407.12709)|**[link](https://github.com/jiutian-vl/mome)**|**在多项视觉-语言任务中，多模态大型语言模型（MLLM）展现出卓越的能力。然而，通常情况下，通用的MLLM在大多数VL任务上的性能不如专门化的MLLM，这是因为存在任务干扰。为此，我们在这篇论文中提出了一种混合多模态专家（MoME）架构，旨在减轻任务干扰，从而获得一个全能的MLLM。MoME主要由两个关键组件构成：视觉专家混合体（MoVE）和语言专家混合体（MoLE）。MoVE能够自适应地调整来自不同视觉编码器的特征，并在转换架构上具有良好的兼容性。MoLE通过稀疏门控专家融入到语言模型中，实现了几乎无额外成本的性能提升。为了应对任务干扰，MoME专注于视觉和语言两种模态，以适应任务间的差异。大量的实验结果表明，MoME显著提高了通用MLLM在各种VL任务中的表现。源代码已在https://github.com/JiuTian-VL/MoME上发布。**|
|**2024-07-17**|**Patch-Level Training for Large Language Models**|Chenze Shao et.al.|[2407.12665](http://arxiv.org/abs/2407.12665)|**[link](https://github.com/shaochenze/patchtrain)**|**随着大型语言模型（LLMs）在语言理解和生成方面取得显著进步，其训练效率成为一个关键问题。传统上，LLMs通过预测序列中的下一个令牌进行训练。尽管基于令牌的训练方法取得了成功，但其计算成本高昂，因为需要处理大量令牌。为此，这篇论文提出了一种名为“patch-level training”的方法，它通过将多个令牌压缩成单个patch来缩短序列长度。在patch-level训练中，我们输入更短的patch序列，让模型学习预测下一个patch，从而大幅度减少了大部分训练数据的处理成本。接着，模型会进行剩余训练数据的令牌级训练，以适应推理模式。实验在不同规模的模型（370M-2.7亿参数）上进行，结果表明patch-level训练可以将总体计算成本降低至0.5倍，同时不会影响模型性能。源代码可在此获取：\url{https://github.com/shaochenze/PatchTrain}。**|
|**2024-07-17**|**Zero-shot Text-guided Infinite Image Synthesis with LLM guidance**|Soyeong Kwon et.al.|[2407.12642](http://arxiv.org/abs/2407.12642)|null|**背景：** 文本引导的图像编辑和生成方法在现实世界中有广泛的应用。然而，文本引导的无限图像合成面临着一些挑战。首先，缺乏高分辨率且具有丰富情境多样性的文本-图像配对数据集。其次，根据文本扩展图像需要全局连贯性和丰富的局部上下文理解能力。以往的研究主要集中在有限类别，如自然风景，且需要在高分辨率图像及其配文上进行训练。为解决这些问题，我们提出了一种新颖的方法，利用大型语言模型（LLMs）同时处理全局连贯性和局部上下文理解，无需任何高分辨率的文本-图像配对训练数据。  **方法：** 我们在训练扩散模型时，让它根据LLM生成的全局和局部描述以及视觉特征来扩展图像。在推理阶段，给定一张图片和一个全局描述，我们使用LLM生成下一个局部描述来扩展输入图像。然后，我们结合全局描述、生成的局部描述和视觉特征来扩展图像，以确保全局一致性并考虑空间局部上下文。  **实验结果：** 实验表明，我们的模型在定量和定性上都优于基线。此外，我们的模型展示了在零样本情况下，借助LLM引导进行文本引导任意大小图像生成的能力。  总结： 本文介绍了一种利用大型语言模型进行文本引导的图像扩展方法，无需依赖高分辨率的配对数据，能够实现全局连贯性和局部上下文理解，并在实验中表现出色，支持零样本任意大小图像生成。|
|**2024-07-17**|**Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences**|Claudio Pinhanez et.al.|[2407.12620](http://arxiv.org/abs/2407.12620)|null|自2022年以来，我们一直在探索人工智能（AI）和现代自然语言处理（NLP），特别是大型语言模型（LLMs）的应用领域，以支持和促进濒临消失的土著语言的使用与文档化。首先，我们关注世界语言多样性的减少，并讨论与处理土著语言相关的独特伦理挑战。为应对这些挑战，我们提出了一种基于社区参与和使用的AI开发新循环。接着，我们报告了使用少量数据微调最先进的翻译器，成功开发出高质量的土著语言机器翻译的鼓舞人心的成果，并讨论了避免开发过程中的一些常见陷阱。我们还展示了2023年和2024年在巴西与土著社区合作项目中的原型，目标是简化写作，以及发展土著语言模型（ILMs）作为创建拼写检查器、下一个词预测器等工具的可复制和可扩展方法。最后，我们展望一个未来，濒危的语言将通过互动的语言模型得以保存。|
|**2024-07-17**|**AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism**|William Brannon et.al.|[2407.12613](http://arxiv.org/abs/2407.12613)|**[link](https://github.com/mit-ccc/AudienceView-demo)**|****背景：** 记者理解和利用受众反馈至关重要，但如今他们在线面临大量观众评论，这是一项艰巨的任务。我们推出了AudienceView，一个在线工具，旨在通过大型语言模型（LLMs）帮助记者对这些反馈进行分类和解读。AudienceView识别主题和话题，将它们与特定评论关联，展示评论的情感倾向和分布，并协助用户构思后续报道项目。我们将探讨这类工具如何融入记者的工作流程，并强调情境理解及人类判断的重要性。  请记住，以上翻译不包含","字符。**|
|**2024-07-17**|**E5-V: Universal Embeddings with Multimodal Large Language Models**|Ting Jiang et.al.|[2407.12580](http://arxiv.org/abs/2407.12580)|**[link](https://github.com/kongds/e5-v)**|**### 背景  大规模多模态语言模型（MLLMs）在通用视觉和语言理解方面取得了显著进步。然而，如何利用MLLMs处理多模态信息的表示方式尚未充分研究。本文提出了一种新的框架E5-V，旨在使MLLMs适应实现通用多模态嵌入。研究结果表明，与先前方法相比，MLLMs在处理多模态输入方面展现出巨大潜力。通过结合提示，E5-V有效地弥合了不同类型输入之间的模态鸿沟，即使在无需微调的情况下也能表现出强大的多模态嵌入能力。  ### 方法  E5-V采用单一模态训练策略，仅使用文本对进行训练，这相较于传统的基于图像-文本对的多模态训练，显著提高了性能，同时降低了大约95%的训练成本，避免了收集昂贵的多模态训练数据的需求。实验在四种任务上进行了广泛的验证，以展示E5-V的有效性。  ### 结果  作为一款通用多模态模型，E5-V不仅在各任务中实现了顶尖性能，甚至在某些情况下超越了现有技术水平，所有这些都是基于单模态训练完成的。**|
|**2024-07-16**|**UrbanWorld: An Urban World Model for 3D City Generation**|Yu Shang et.al.|[2407.11965](http://arxiv.org/abs/2407.11965)|**[link](https://github.com/urban-world/urbanworld)**|城市作为人类生活的基本环境，包含了建筑、道路和植被等多元物理元素，这些元素之间存在着复杂的相互关联。构建逼真且互动的3D城市环境对于研发能在现实世界环境中感知、决策和行动的AI至关重要。然而，传统的手工制作过程耗时且精细，需要设计师投入大量精力来精确呈现复杂的城市特征。为此，我们提出UrbanWorld，这是一个首个自动生成定制化、真实且互动的3D城市世界的模型，支持灵活的控制条件。UrbanWorld的生成流程包括四个关键步骤：利用公开的OSM数据进行3D布局生成、借助强大的城市多模态大语言模型（Urban MLLM）进行城市场景规划与设计、通过先进的3D扩散技术实现可控资产渲染，以及MLLM辅助的场景细化。UrbanWorld生成的高保真3D城市环境为通用AI和机器感知系统在模拟中的真实反馈和交互提供了可能。我们致力于将UrbanWorld作为开源且多功能的平台，用于评估和提升AI在真实城市环境中的感知、决策和互动能力。|
|**2024-07-16**|**NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?**|Mo Li et.al.|[2407.11963](http://arxiv.org/abs/2407.11963)|**[link](https://github.com/open-compass/opencompass)**|**本文介绍了一个名为NeedleBench的框架，它是一系列评估大语言模型（LLMs）长文本理解能力的逐步升级任务。该框架涉及不同长度区间（4k、8k、32k、128k、200k、1M乃至更长）和深度范围，通过在不同文本深度区域插入关键数据点，系统性地测试模型在各种情境下的检索和推理能力。针对于双语长文本，我们利用这个框架来考察主流开源模型识别与问题相关的关键信息，并运用这些信息进行推理的能力。  此外，我们提出了祖先追踪挑战（Ancestral Trace Challenge，ATC），旨在模拟现实世界中长文本逻辑推理任务的复杂性，提供一个简单的方法来评估LLMs处理复杂长文本上下文的能力。研究结果显示，当前的LLMs在实际的长文本应用中仍有很大的提升空间，因为它们在处理逻辑推理难题时面临挑战。所有代码和资源可在OpenCompass项目（https://github.com/open-compass/opencompass）获取。**|
|**2024-07-16**|**Code Documentation and Analysis to Secure Software Development**|Paul Attie et.al.|[2407.11934](http://arxiv.org/abs/2407.11934)|null|我们介绍了一种名为Code Documentation and Analysis Tool（CoDAT）的工具。CoDAT旨在保持代码文档之间的连贯性，例如，如果代码片段中的某行被修改，相应的注释也会自动更新，确保内部一致性以及与代码的一致性。通过标记过时的注释，CoDAT提醒开发者维护最新的文档。我们利用大型语言模型检查代码片段与其描述的语义一致性，从而也能识别出语义不一致和过时的注释。这有助于程序员编写正确实现代码草图的代码，支持逐步细化方法，从代码草图逐步演变为经过一两次或更多次细化迭代的代码。  CoDAT在IntelliJ IDEA IDE中实现，利用Code Insight守护程序包结合自定义正则表达式算法，标记对应代码块已更改的标记注释。CoDAT的后端结构上是去中心化的，支持分布式账本框架，以实现代码一致性跟踪和架构编译管理。|
|**2024-07-16**|**What's Wrong? Refining Meeting Summaries with LLM Feedback**|Frederic Kirstein et.al.|[2407.11919](http://arxiv.org/abs/2407.11919)|null|随着数字会议的普及，会议摘要提炼成为关键任务。大型语言模型（LLMs）在这一领域展现出巨大潜力，它们在连贯性和理解上下文中超越了传统方法。然而，它们仍需改进以保持相关性并避免错误。我们提出了一种基于多LLM的会议摘要修正方法，通过两阶段过程模拟人类审查：错误识别和摘要精炼。我们发布了QMSum Mistake，这是一个包含200份由人工标注的自动生成会议摘要数据集，针对结构、遗漏和不相关等九种错误类型进行了标记。实验表明，LLMs能够准确识别这些错误。我们将识别出的问题转化为可操作的反馈，以此提升摘要的质量，如相关性、信息量、简洁性和连贯性。这种事后优化策略通过利用多个LLMs来验证输出质量，有效提高了摘要质量。我们的多LLM会议摘要方法对于需要稳健性、行动计划和目标导向的复杂文本生成任务具有潜在应用价值。|
|**2024-07-16**|**Ascend-CC: Confidential Computing on Heterogeneous NPU for Emerging Generative AI Workloads**|Aritra Dhar et.al.|[2407.11888](http://arxiv.org/abs/2407.11888)|null|在云工作负载中，基于大型语言模型（LLMs）的生成AI占据主导地位。专用硬件加速器，如GPU、NPUs和TPUs，因其在AI应用中的卓越性能超越了通用CPU。AI模型和数据通常具有高度敏感性，并来自相互不信任的各方。现有的基于CPU的可信执行环境（TEE），如英特尔SGX或AMD SEV，提供的保护不够充分。像Nvidia-CC这样的设备中心TEE仅针对紧密耦合的CPU-GPU系统，且采用专有方案，需要在主机CPU上部署TEE。另一方面，现有的学术提案大多针对特定的CPU-TEE平台。  为填补这一空白，我们提出了Ascend-CC，一种基于离散NPUs的机密计算架构，无需对主机系统信任。Ascend-CC通过确保数据和模型加密，保护数据、模型参数和运算符二进制，提供强大的安全性。它利用委托式内存语义确保与主机软件栈的隔离，并通过任务鉴权提供模型完整性的强有力保证。我们的Ascend-CC实现和与最新LLMs（如Llama2和Llama3）的评估表明，Ascend-CC引入的开销极小，无需修改AI软件栈。|
|**2024-07-16**|**Schema Matching with Large Language Models: an Experimental Study**|Marcel Parciak et.al.|[2407.11852](http://arxiv.org/abs/2407.11852)|**[link](https://github.com/uhasselt-dsi-data-systems-lab/code-schema-matching-llms-artefacs)**|**该论文探讨了大型语言模型（LLMs）在关系数据库架构（schema）匹配中的应用。目标是仅通过元素名称和描述找出两个关系模式之间的语义对应。研究者构建了一个来自健康领域的基准测试，并提出了不同的任务范围，即使用不同数量上下文信息提示模型进行schema匹配。他们对比了基于LLM的匹配方法与基于字符串相似度的基线，考察了匹配质量、验证工作量、决策确定性和互补性。研究发现，缺乏上下文信息会降低匹配质量，过多的信息也会有负面影响。新版本的LLMs通常能提高决策确定性。有些任务范围下的验证工作相对适度，且能成功识别大量真正意义上的语义匹配。研究结果表明，LLMs有潜力作为schema匹配的初始工具，数据工程师可以利用它们的名称和描述信息快速进行匹配，无需依赖实际数据实例。**|
|**2024-07-16**|**LoFTI: Localization and Factuality Transfer to Indian Locales**|Sona Elza Simon et.al.|[2407.11833](http://arxiv.org/abs/2407.11833)|**[link](https://github.com/csalt-research/lofti)**|**大型语言模型（LLMs）通过训练在互联网上爬取的大型网页数据集，积累了大量的世界知识。然而，这些数据集通常倾向于英语和西欧国家，导致LLMs对来自其他地区，特别是印度的本地化查询产生偏见或虚构的回答。为此，我们提出一个新的基准LoFTI（印度本地化与事实转移），用于评估LLMs的本地化和事实文本转换能力。LoFTI包含关于全球源地点和印度目标地点（包括国家、州和城市的不同层级）实体的事实陈述，涉及各类广泛的主题。我们使用LoFTI来评估Mixtral、GPT-4以及两种适用于本地化事实转移任务的Mixtral衍生方法。实验表明，LoFTI是一个高质量的评估标准，包括GPT-4在内的所有模型在不同层级的本地化上都表现出偏差。**|
|**2024-07-16**|**GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**|Kyle Hamilton et.al.|[2407.11827](http://arxiv.org/abs/2407.11827)|null|尽管机器学习在检测文本中的宣传手段方面引起了广泛关注，但大多数方法侧重于“黑盒”解决方案，其内部工作原理不透明。可解释的方法提供了解决方案，但它们依赖于精心的特征工程和昂贵的专家标注数据。此外，关于说服性文本的语言特性通常由修辞学家或语言学家关注，但没有适合机器学习的标记有此类特性的数据集。本研究旨在编纂文献中识别出的22个修辞和语言特征，目的是对一个已标注有宣传手段的现有数据集进行注释。为了帮助人类专家在自然语言句子上标注这些特征，我们特别设计了名为RhetAnn的网络应用，以减少原本较大的认知负担。接着，使用一小部分标注数据，我们利用GPT-3.5，一种生成大型语言模型（LLM），对剩余数据进行微调，同时兼顾成本效益和分类精度。这项研究表明，结合少量人工标注示例与GPT，可以有效地以传统仅依赖人类专家的标注成本的十分之一左右实现大规模标注过程的扩展。结果与撰写时表现最好的模型（GPT-4）相当，且成本降低10倍。我们的贡献包括这些特征、它们的属性、定义以及示例的机器可读格式，以及RhetAnn的代码、GPT提示和微调流程，这些都推动了可解释的宣传手段检测领域的最新进展。|
|**2024-07-16**|**PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation**|Branden Butler et.al.|[2407.11798](http://arxiv.org/abs/2407.11798)|null|近年来，大型语言模型（LLMs）在分布式计算机集群上的推理已成为研究热点，许多加速技术借鉴了CPU的推测执行策略。这些技术旨在缓解内存带宽瓶颈，但会增加每次推理运行的端到端延迟，需要高推测接受率来提升性能。然而，由于任务间接受率的变异性，推测性推理可能导致性能下降。此外，管道并行设计需要大量用户请求以保持高利用率。针对这些问题，我们提出了PipeInfer，这是一种旨在减少跨令牌延迟、提高单请求场景下系统利用率的管道化推测加速技术，同时增强了对低推测接受率和低带宽互联的容忍度。  PipeInfer通过连续异步推测和早期推理取消实现了显著的改进。连续异步推测允许同时进行单令牌推理与多个推测运行，从而降低延迟和生成速度。而早期推理取消则能够在推理过程中跳过无效运行的计算，进一步提升速度和延迟。PipeInfer在生成速度上比标准推测性推理最高可提升2.15倍。|
|**2024-07-16**|**Large Language Models as Misleading Assistants in Conversation**|Betty Li Hou et.al.|[2407.11789](http://arxiv.org/abs/2407.11789)|null|大型语言模型（LLMs）在各种信息查询任务上能够提供帮助。然而，模型输出可能会误导用户，无论是无意的还是故意的。我们针对阅读理解任务探讨了LLMs在欺骗性辅助方面的能力，将其作为人类用户的代理。实验对比了三种情况：（1）模型被提示提供真实信息，（2）模型被提示进行微妙误导，以及（3）模型被提示支持错误答案。结果显示，GPT-4能够有效误导GPT-3.5-Turbo和GPT-4自身，欺骗性助手导致任务准确率下降高达23%，相比于使用真实助手。此外，我们发现向用户模型提供更多的上下文信息可以部分抵消欺骗模型的影响。这项研究揭示了LLMs生成误导性信息的能力及其在现实场景中的潜在影响。|
|**2024-07-15**|**VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation**|Bocheng Zou et.al.|[2407.10972](http://arxiv.org/abs/2407.10972)|**[link](https://github.com/vgbench/VGBench)**|**在视觉模型领域，主要的表示方式是使用像素来绘制视觉世界。然而，这并非总是最佳或唯一的表示视觉内容的方法，特别是对于设计师和艺术家，他们常用多边形等几何形状来构建图形。矢量图形（VG）提供了一种文本形式的视觉内容表示，对于卡通或素描等类型的内容可能更为精炼和强大。近期的研究表明，强大的大语言模型（LLMs）在处理矢量图形方面展现出令人鼓舞的结果。但这些工作主要侧重于定性分析、理解或特定类型的矢量图形。我们提出VGBench，这是一个全面的基准，用于评估LLMs在处理矢量图形方面的性能，包括：(a) 对视觉理解和生成的双重关注，(b) 多种矢量图形格式的评估，(c) 不同类型的提问，(d) 广泛的提示技巧，以及(e) 在多种LLMs下的表现。通过对收集的4279个理解样本和5845个生成样本进行评估，我们发现LLMs在这两个方面都表现出强大能力，但在低级格式（如SVG）上表现稍逊。我们的数据和评估流程将在<https://vgbench.github.io>上开源。**|
|**2024-07-15**|**Q-Sparse: All Large Language Models can be Fully Sparsely-Activated**|Hongyu Wang et.al.|[2407.10969](http://arxiv.org/abs/2407.10969)|null|我们提出了一种简单但有效的训练方法，称为Q-Sparse，专为大规模语言模型（LLMs）设计。Q-Sparse使得LLMs的激活全为稀疏，从而在推理阶段带来显著的效率提升。这一方法通过应用顶部K稀疏化技术对激活进行处理，并结合直通估计进行训练。主要成果包括：(1) Q-Sparse在保持与基线LLM结果相当的同时，具有更高的推理时的效率；(2) 我们给出了稀疏激活LLMs的最优推理缩放定律；(3) Q-Sparse在各种场景下表现优秀，包括从头开始训练、预训练模型的继续训练和微调；(4) Q-Sparse适用于全精度和1位精度的LLMs，如BitNet b1.58。特别是，BitNet b1.58与Q-Sparse（可配备MoE）的结合，为未来LLMs的效率提升，包括成本和能耗，提供了基石和清晰路径。|
|**2024-07-15**|**Fast Matrix Multiplications for Lookup Table-Quantized LLMs**|Han Guo et.al.|[2407.10960](http://arxiv.org/abs/2407.10960)|**[link](https://github.com/hanguo97/flute)**|大型语言模型（LLMs）的部署通常受到内存带宽的限制，其中主要瓶颈是将模型参数从GPU全局内存传输到寄存器的成本。通过结合权重只量化，可以减少内存移动，从而加速推理速度。然而，为量化后的LLMs设计高性能内核是一项重大挑战，尤其是当权重被压缩到非均匀分隔的位宽（如3位），并采用非均匀查找表（LUT）量化时。本文介绍了一种灵活的查找表引擎FLUTE，它通过对量化权重矩阵进行离线重构，以最小化解压相关的位操作，并通过向量化和复制查找表来缓解共享内存带宽限制。在小批量（小于32）和量化组大小为128（LLM推理中的典型值）的情况下，FLUTE内核的速度可以比现有GEMM内核快2-4倍。作为FLUTE的应用，我们探讨了查找表基的NormalFloat量化的一种简单扩展，并将其应用于量化LLaMA3，获得了与强大基准相当的量化性能，同时实现了端到端吞吐量的1.5到2倍提升。|
|**2024-07-15**|**MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models**|Chengguang Gan et.al.|[2407.10953](http://arxiv.org/abs/2407.10953)|null|## 任务  **背景：** 互惠增强效应（MRE）在信息抽取和多任务研究中展现出巨大潜力。然而，由于仅有的MRE混合数据集局限于日语，这限制了全球研究界的广泛探索。为了克服这一局限，我们构建了一个多语言MRE混合数据集（MMM），包含英语、日语和汉语的21个子集。本文还提出了一种利用大型语言模型（LLMs）辅助的数据集翻译方法，通过利用LLMs将原始日语文本进行翻译，大大减少了数据集构建时的人工标注时间。  **贡献：** 我们扩展了数据集，加入了开放领域命名实体识别（NER）和句子分类任务。基于这个扩充后的数据集，我们开发了一个统一的输入-输出框架，训练了一个开放域信息抽取大语言模型（OIELLM）。实验表明，OIELLM模型能够有效处理新的MMM数据集，并表现出显著的性能提升。  总之，我们的工作旨在通过提供多语言资源和高效的翻译策略，推动互惠增强效应在多语言信息抽取领域的应用研究。|
|**2024-07-15**|**Can Textual Semantics Mitigate Sounding Object Segmentation Preference?**|Yaoting Wang et.al.|[2407.10947](http://arxiv.org/abs/2407.10947)|**[link](https://github.com/gewu-lab/sounding-object-segmentation-preference)**|**## 任务  音频-视觉分割（Audio-Visual Segmentation，AVS）任务的目标是利用音频线索在视觉空间中分割出发声物体。然而，研究指出，现有的AVS方法过于依赖对可听见对象的分割偏好，而非精确的音频指导。问题在于，相比于视觉，音频在多声源音场中的语义表现较弱，导致其在指导视觉空间时作用有限。鉴于文本模态经过深入探索，包含丰富的抽象语义，我们提出利用视觉场景中的文本提示来增强音频指导的精确性。  我们的方法首先通过现成的图像描述器获取场景描述，然后利用预训练的大语言模型推断潜在的发声物体作为文本线索。接着，我们设计了一个新颖的基于语义的音频建模模块，引入动态掩码，将音频特征与文本线索融合，生成具有代表性的发声物体特征。这些特征不仅包含音频信息，还蕴含了生动的语义，从而为视觉空间提供更为清晰的指引。我们在AVS基准数据集上的实验结果表明，借助文本提示，我们的方法对音频的敏感度得到提升，在所有三个子集上表现出高度竞争力。项目页面：[https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference](https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference)。**|
|**2024-07-15**|**GRUtopia: Dream General Robots in a City at Scale**|Hanqing Wang et.al.|[2407.10943](http://arxiv.org/abs/2407.10943)|**[link](https://github.com/openrobotlab/grutopia)**|**近期的研究正在探索Embodied AI领域的规模法则。鉴于收集现实世界数据的高昂成本，我们认为模拟到现实（Sim2Real）方法对于扩展embodied模型的学习至关重要。本文介绍项目GRUtopia，这是一个专为各种机器人设计的首个互动三维社会。它具有多项创新：(a) 场景数据集GRScenes包含了10万张交互式、精细注释的场景，这些场景可以自由组合成城市规模的环境。与以往主要关注家庭环境的作品不同，GRScenes涵盖了89个多样化的场景类别，弥合了服务导向环境中机器人初始部署的差距。(b) GRResidents是一个由大型语言模型驱动的非玩家角色（NPC）系统，负责社交互动、任务生成和任务分配，从而模拟embodied AI应用中的社会场景。(c) 标准化基准GRBench支持各种机器人，但以腿足机器人为主，提供涉及物体导航、社交导航和移动操作的任务，这些任务具有适度的挑战性。我们期望这项工作能够缓解该领域高质量数据的匮乏，并为Embodied AI研究提供更全面的评估。项目代码可从https://github.com/OpenRobotLab/GRUtopia获取。**|
|**2024-07-15**|**FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets**|Xiaohui Victor Li et.al.|[2407.10909](http://arxiv.org/abs/2407.10909)|**[link](https://github.com/xiaohui-victor-li/FinDKG)**|动态知识图谱（DKGs）是一种流行的数据结构，用于表示随时间变化的对象之间的各种连接。它们在处理复杂无结构数据源（如文本和图像）提取的信息时展现出高效性。在金融应用中，DKGs可用于基于财经新闻文章探测投资策略的趋势。本研究探索大型语言模型（LLMs）作为动态知识图谱生成器的特性，为此我们提出了一种开源的Fine-tuned LLM，称为集成上下文知识图谱生成器（ICKG）。利用ICKG，我们从财经新闻文章中创建了一个新的开源动态知识图谱，称为FinDKG。此外，我们设计了注意力机制的图神经网络架构（KGTransformer），用于分析这个图谱。我们在基准数据集和FinDKG上测试了模型性能，结果显示在链接预测任务中，KGTransformer表现优异。最后，我们评估了KGTransformer在FinDKG上的主题投资性能，证明它能超越现有的主题交易所交易基金（ETF）。|
|**2024-07-15**|**Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique**|Mark Russinovich et.al.|[2407.10887](http://arxiv.org/abs/2407.10887)|null|随着对大型语言模型（LLMs）被盗和误用的担忧加剧，模型指纹化的必要性提升。在这种背景下，成功的指纹应具备五个特性：透明性、效率、持久性、鲁棒性和不可伪造性。本文首先定义了这些要求。接着，我们提出了一种新的简单指纹方法——Chain & Hash，它融合了加密理念，实现了所有这些特性。Chain & Hash涉及生成一组问题（指纹）及其可能的答案，然后使用安全哈希技术将它们合并，以确定每个问题的值，从而保证不可伪造性，防止对手声称虚假所有权。我们在多个模型上评估了Chain & Hash技术，并展示了它对良性操作（如在不同数据集上微调）和敌意删除指纹的鲁棒性。实验表明，带指纹的模型在各种基准测试中的性能几乎与非指纹化模型相当，同时保持了高效性及其实用价值。|
|**2024-07-15**|**SLIP: Securing LLMs IP Using Weights Decomposition**|Yehonathan Refael et.al.|[2407.10886](http://arxiv.org/abs/2407.10886)|null|随着大型语言模型（LLMs）在学术界和工业界的广泛应用，这些模型的价值作为知识产权（IP）日益凸显，反映出其背后巨大的投资。然而，由于云部署成本高，边缘设备部署的需求增加，这可能导致模型参数被盗用和未经授权使用。当前的保护方法在实用性、准确性损失或适应性方面存在局限。本文提出了一种新颖的混合推理算法，称为SLIP（Secure Lightweight Inference Protocol），旨在保护部署在边缘的模型免受盗窃。SLIP是首个兼顾实际应用的实用性和严格安全性的混合协议，同时保持零精度下降和低延迟影响。  SLIP通过矩阵分解实现了模型在两个计算资源之间的划分：一个安全但昂贵，另一个成本效益高但易受攻击。关键在于，安全资源保留了模型IP中最敏感的部分，同时执行最少的计算，而脆弱资源则相反。此外，该协议提供了防止攻击者利用分割获取保密信息的安全保障。最后，我们展示了实验结果，证明了SLIP的稳健性和有效性，使其成为保护LLMs的理想解决方案。|
|**2024-07-15**|**Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models**|Rui Zhang et.al.|[2407.10873](http://arxiv.org/abs/2407.10873)|null|自动化启发式设计（AHD）因其在自动开发高效启发式方法方面的潜力而受到广泛关注。随着大型语言模型（LLMs）的兴起，人们开始探索将AHD视为进化程序搜索（EPS）问题的新途径。然而，当前的基准设置不一致，基础比较不足，且缺乏对LLM与搜索策略结合必要性的深入分析，这使得现有基于LLM的EPS方法的实际进展难以得到充分证明。本研究通过一项大规模基准测试，涵盖了四项基于LLM的EPS方法和四项AHD问题，跨越九种LLM，并进行了五次独立运行。我们的广泛实验提供了有价值的见解，实证了在LLM驱动的AHD方法中的进化搜索的重要性，同时也推动了未来EPS算法开发的进步。为了促进可访问性和可重复性，我们已经全面开源了我们的基准和相关结果。|
|**2024-07-12**|**FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3**|Georgios Makridis et.al.|[2407.09467](http://arxiv.org/abs/2407.09467)|null|在这个充满人工智能驱动的叙事多样性世界中，有一个独特的机会是通过定制和个性化的叙述吸引年轻观众。本文介绍FairyLandAI，这是一个专为儿童开发的创新大型语言模型（LLM），基于OpenAI的API构建。其特别之处在于，FairyLandAI不仅能生成引人入胜、适合各年龄段且反映各种传统的故事，还能自动生成适合高级图像生成工具（如GenAI和Dalle-3）的创意提示，从而丰富讲故事的体验。FairyLandAI精准地适应儿童的想象力世界，提供既教育又娱乐的故事，并与不同年龄阶段所蕴含的价值观相一致。它的独特之处在于根据个体孩子的喜好和文化背景定制故事，标志着个性化叙事新时代的到来。此外，它与图像生成技术的结合提供了全面的叙事体验，激发口头和视觉创造力。实证评估显示，FairyLandAI在创作吸引孩子们的故事方面表现出色，这些故事不仅娱乐，还体现了多元传统中的道德教诲。这个模型对于家长和教育工作者来说是一个宝贵的工具，帮助他们通过引人入胜的故事传递深刻的人生道理。FairyLandAI代表了利用LLMs，特别是OpenAI API进行教育和文化提升的开创性一步，使复杂而富有教育意义的道德故事对年轻、富有想象力的心灵变得易于理解和享受。|
|**2024-07-12**|**Human-like Episodic Memory for Infinite Context LLMs**|Zafeirios Fountas et.al.|[2407.09450](http://arxiv.org/abs/2407.09450)|**[link](https://github.com/em-llm/EM-LLM-model)**|大型语言模型（LLMs）展现了惊人的能力，但它们在处理长序列时仍面临保持连贯性和准确性的问题。人类大脑在组织和检索跨长时间尺度的亲身经历方面尤为出色，能够覆盖一生的记忆。本文提出了一种新颖的方法，称为EM-LLM，它将人类的 episodic memory（情景记忆）和事件认知关键要素融入到LLMs中，使其能够有效处理几乎无限长度的上下文，同时保持计算效率。EM-LLM通过结合贝叶斯惊奇度和图论边界细化技术，在线方式组织令牌序列成连贯的事件。当需要时，通过两阶段的记忆过程——结合相似度和时间邻接的检索，实现高效且类似人类的信息访问。在LongBench数据集上的实验显示，EM-LLM的表现优于最先进的InfLLM模型，总体相对提高了4.3%，在各种任务中，包括提升了33%的PassageRetrieval任务。此外，我们的分析揭示了EM-LLM事件分割与人类感知事件之间的强相关性，暗示了这个人工系统与生物对应机制之间的桥梁。这项工作不仅提升了LLMs处理长序列的能力，还为探索人类记忆机制提供了计算框架，开辟了人工智能和认知科学交叉研究的新途径。|
|**2024-07-12**|**ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts**|Amelia F. Hardy et.al.|[2407.09447](http://arxiv.org/abs/2407.09447)|**[link](https://github.com/sisl/astprompter)**|## 背景  通常的自动化大型语言模型（LLMs）红队对抗策略集中在寻找能触发冻结语言模型（即防御者）生成有毒文本的提示。这可能导致对抗模型（即攻击者）产生难以理解、不自然的输出。在此，我们提出了一种强化学习框架来处理LLMs的红队对抗任务，目标是找到既能（1）触发防御者生成有毒文本，又能（2）保持低困惑度（即防御者打分）的提示。我们认为在红队对抗场景中，这些情况最相关，因为它们很可能在防御者模型的常规使用中出现。我们通过一种新颖的在线和弱监督的Identity Preference Optimization（IPO）变体解决了这个问题，应用于GPT-2和GPT-2 XL作为防御者。实验表明，我们的策略能够生成既可能又会触发毒性的提示。最后，我们分析了学习策略、可能性与毒性之间的权衡，并讨论了相关含义。该项目的源代码可在这里获取：https://github.com/sisl/ASTPrompter/。|
|**2024-07-12**|**MUSCLE: A Model Update Strategy for Compatible LLM Evolution**|Jessica Echterhoff et.al.|[2407.09435](http://arxiv.org/abs/2407.09435)|null|## 背景 大型语言模型（LLMs）由于数据或架构的调整而经常更新以提升性能。在升级过程中，开发者通常侧重于提高总体性能指标，对与旧版本兼容性的关注较少。然而，用户往往会对他们使用的机器学习模型的功能和能力形成心理模型，并随着每次更新需要调整这个模型。频繁的模型变更可能导致用户满意度下降。实际上，下游任务微调器依赖预训练的LLM基模型。当基模型更新时，面向用户的这些下游任务模型可能会出现实例退化或负面翻转——先前正确的实例现在被预测错误。即使下游任务的训练流程保持不变，这种情况也会发生。我们的工作旨在为用户提供无缝的模型更新体验，方法有两个方面。首先，我们提出了一套评估指标，用于衡量模型与旧版本的兼容性，特别适用于生成任务，也可应用于分类任务。我们观察到不同模型版本和更新之间存在退化和不一致性，尤其是在多样化的任务上。  ## 任务 我们的研究旨在通过以下两个途径提供对用户友好的模型更新：一是开发一种兼容性评估标准，用于检测生成任务或其他任务中的模型版本间差异；二是提出一种训练策略，通过训练兼容性模型来减少模型更新中的不一致，从而降低从Llama 1到Llama 2等版本更新时的负面翻转率，最多可减少40%。这样，用户可以更轻松地适应新版本，而无需频繁调整他们的预期和使用方式。|
|**2024-07-12**|**Open (Clinical) LLMs are Sensitive to Instruction Phrasings**|Alberto Mario Ceballos Arroyo et.al.|[2407.09429](http://arxiv.org/abs/2407.09429)|**[link](https://github.com/alceballosa/clin-robust)**|## 背景 基于指令的大型语言模型（LLMs）能够根据自然语言指令执行各种任务，但它们对指令表述的敏感性是一个问题。在医疗领域尤其关键，因为临床医生可能不是提示工程方面的专家，且错误输出的潜在后果更为严重。这就提出了一个实际问题：针对临床自然语言处理任务，指令调优的LLMs对于自然（非攻击性的）指令表述变化有多稳健？我们收集了来自不同任务的医生提示，衡量了七种LLM（包括通用和专用的）对指令表述细微差异的敏感度。研究发现，所有模型的表现差异显著，令人意外的是，专门针对临床数据训练的模型相较于通用领域的模型，其稳定性较差。此外，随意的表述变化可能影响公平性，例如，用于预测死亡率的有效但不同的指令不仅会导致整体性能的波动，还会在不同人群间产生差异。|
|**2024-07-12**|**TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models**|Hang Zou et.al.|[2407.09424](http://arxiv.org/abs/2407.09424)|null|该论文首次提出了一种方法，旨在将大型通用语言模型（LLMs）适应到电信领域的专用模型。为此，我们收集并构建了电信特定的预训练数据集、指令数据集和偏好数据集，分别用于持续预训练、指导调优和对齐调优。由于电信领域缺乏广泛接受的评估基准，我们扩展了现有的评估标准，并提出了三个新的基准：电信数学建模、电信开放性问题与答案（TeleQnA）以及电信代码任务。这些新基准全面评估了LLMs在电信领域的数学建模、开放式问题回答、代码生成、填充、总结和分析等能力。我们的优化模型TelecomGPT在电信数学建模基准上显著优于最先进的模型，如GPT-4、Llama-3和Mistral，并在TeleQnA、3GPP技术文档分类、电信代码摘要与生成以及填充任务上表现出相当的性能。|
|**2024-07-12**|**Mitigating Entity-Level Hallucination in Large Language Models**|Weihang Su et.al.|[2407.09417](http://arxiv.org/abs/2407.09417)|**[link](https://github.com/oneal2000/entityhallucination)**|**随着大型语言模型（LLMs）的兴起，用户获取信息的方式发生了转变，从传统的搜索引擎转向直接与LLMs进行问答交互。然而，LLMs的广泛应用暴露出一个挑战，即“幻觉”生成，即模型生成看似连贯但事实性错误的回答，这导致用户对基于LLMs的信息检索系统产生怀疑。为解决这一问题，本文提出了一种新颖的方法：动态检索增强基于幻觉检测（DRAD）。DRAD改进了传统检索增强技术，通过实时幻觉检测来动态调整检索过程。它主要包括两个核心组件：实时幻觉检测（RHD），用于在无需外部模型的情况下识别潜在的幻觉；以及基于外部知识的自我纠正（SEK），利用外部知识修正这些错误。实验结果表明，DRAD在检测和减少LLMs中的幻觉方面表现出色。我们已将所有代码和数据开源，供学术界使用：https://github.com/oneal2000/EntityHallucination。**|
|**2024-07-12**|**SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers**|Shraman Pramanick et.al.|[2407.09413](http://arxiv.org/abs/2407.09413)|**[link](https://github.com/google/spiqa)**|**### 任务  在深入阅读科学论文时，快速查找信息是关键。然而，现有的基于论文的问题 answering（QA）数据集在规模和内容上存在局限，主要关注文本部分。为弥补这一不足，我们推出了SPIQA（科学论文图像问题回答），这是一个专门设计的大型QA数据集，旨在理解计算机科学各领域的复杂图表、表格和结果可视化。借助多模态大语言模型（MLLMs）的强大理解能力，我们通过自动化和人工筛选创建了这个数据集。SPIQA包含了27万条问题，分为训练、验证和三个不同的评估分段。通过与12个基础模型的广泛实验，我们评估了当前多模态系统理解科研文章细微之处的能力。此外，我们提出了一种链式思维（Chain-of-Thought，CoT）评价策略，结合上下文检索，实现了细致的逐步骤评估，有助于提升模型性能。我们还探讨了额外文本信息对性能提升的上限，这表明了其对未来研究的潜力，并预示着该数据集将革新我们与科学文献互动的方式。**|
|**2024-07-12**|**PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents**|Saber Zerhoudi et.al.|[2407.09394](http://arxiv.org/abs/2407.09394)|**[link](https://github.com/padas-lab-de/PersonaRAG)**|大型语言模型（LLMs）由于知识过时和胡编乱造而难以生成可靠的结果。为了解决这个问题，检索增强生成（RAG）模型通过结合外部知识改进了LLMs，但往往无法个性化检索过程。这篇论文提出了一种新颖的框架——PersonaRAG，它引入了以用户为中心的代理，能够根据实时用户数据和交互来调整检索和生成。在多个问答数据集上的评估显示，PersonaRAG相较于基础模型表现出显著优势，能更好地满足用户的个性化需求。实验结果表明，用户适应的信息检索系统具有广阔的发展前景。|
|**2024-07-12**|**GAVEL: Generating Games Via Evolution and Language Models**|Graham Todd et.al.|[2407.09388](http://arxiv.org/abs/2407.09388)|null|自动创建新颖有趣的游戏是一个复杂任务，它涉及如何以计算机可处理的形式表达游戏规则、搜索庞大的潜在游戏空间，以及准确评估未见过游戏的原创性和质量。先前的研究主要关注于有限的规则表示，并依赖于特定领域的启发式方法。在这个工作中，我们专注于在Ludii游戏描述语言中生成新奇的游戏，该语言编码了各种风格和玩法的1000多款棋盘游戏规则。我们借鉴了大型语言模型和进化计算的最新进展，训练了一个能够智能地变异和重组以代码形式表达的游戏机制的模型。我们通过定量和定性分析表明，我们的方法能够创造出新的、有吸引力的游戏，包括那些现有Ludii数据集中未覆盖的游戏区域。生成的一些游戏示例可通过Ludii门户在线体验。|
|**2024-07-11**|**MAVIS: Mathematical Visual Instruction Tuning**|Renrui Zhang et.al.|[2407.08739](http://arxiv.org/abs/2407.08739)|**[link](https://github.com/zrrskywalker/mavis)**|**### 背景  多模态大型语言模型（MLLMs）近年来在学术界和工业界引起了广泛关注。尽管它们在多模态场景中的表现突出，但对数学图解的数学问题求解能力研究尚显不足。为此，我们指出了MLLM在数学视觉领域的三个关键改进领域：数学图解的视觉编码、图解与语言的对齐以及数学推理技能。这促使我们需要大规模、高质量的视觉数学数据和训练流程。本文提出MAVIS（Mathematical VISual instruction tuning for MLLMs），一个针对MLLM的数学视觉指导调参范式，包括一系列数学视觉数据集和专门的MLLM。  ### 方法  MAVIS分为三个阶段进行从头开始的训练。首先，我们创建了MAVIS-Caption，包含558,000个图解-描述对，通过对比学习来微调专为数学设计的视觉编码器（CLIP-Math），以提升图解的视觉理解能力。其次，利用MAVIS-Caption，我们通过投影层将CLIP-Math与大型语言模型（LLM）进行关联，增强数学领域的视觉语言对齐。最后，我们引入MAVIS-Instruct，包含900,000个精心收集和标注的视觉数学问题，用于最终指导调参，以增强MLLM的稳健数学推理能力。在MAVIS-Instruct中，我们提供了每个问题的完整链式思考（Chain-of-Thought, CoT）理由，并减少文本冗余，使模型更专注于视觉元素。  ### 结果  数据和模型已发布在https://github.com/ZrrSkywalker/MAVIS。通过MAVIS，我们旨在填补数学视觉理解的空白，提升MLLM在解决实际数学问题时的表现。**|
|**2024-07-11**|**Real-Time Anomaly Detection and Reactive Planning with Large Language Models**|Rohan Sinha et.al.|[2407.08735](http://arxiv.org/abs/2407.08735)|null|这篇论文探讨了如何利用大规模语言模型（如大型语言模型）在机器人系统中检测和应对异常情况，以提高其鲁棒性和安全性。主要挑战包括减少模型的计算开销以便实现实时应用，以及将模型的判断融入到安全控制框架中。研究者提出了一种两阶段推理框架：首先是一个快速的二元异常分类器，它在语言模型嵌入空间中分析观测数据，如果发现异常，会触发后续的慢速推理阶段，利用生成式语言模型进行深入的逻辑推理。这种设计类似于模型预测控制中的决策分支，考虑到慢速推理器的延迟，可以立即采取备份计划，确保系统的安全性。  通过与最先进的GPT模型的自回归推理方法进行比较，研究发现，即使使用小型语言模型，他们的快速异常分类器也表现出色。这使得他们开发的运行时监控器能够在资源和时间限制下，提升动态机器人系统，如四旋翼无人机或自动驾驶车辆的信任度。论文的视频示例可以在项目页面上查看：https://sites.google.com/view/aesop-llm。|
|**2024-07-11**|**Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist**|Zihao Zhou et.al.|[2407.08733](http://arxiv.org/abs/2407.08733)|null|### 翻译  **摘要：**  强大的数学推理能力是大型语言模型（LLMs）卓越性能的关键体现。如何定义和全面评估LLMs的数学能力，以及在实际应用中反映用户体验，已成为关键问题。目前的基准测试主要侧重于问题解决能力，这可能导致模型过拟合，并无法准确反映真正的数学推理能力。我们认为，如果模型真正理解了问题，它应该能在各种任务中稳健且灵活地应用。在此启发下，我们提出MATHCHECK，一个旨在测试任务泛化和推理鲁棒性的精心设计的清单，以及一个自动生成清单的工具。MATHCHECK包含多个数学推理任务和测试类型，以促进对数学推理能力和行为测试的全面评估。我们利用MATHCHECK创建了MATHCHECK-GSM和MATHCHECK-GEO，分别针对数学文本推理和多模态推理能力进行评估，它们是GSM8k、GeoQA、UniGeo和Geometry3K等基准的升级版。我们使用MATHCHECK-GSM和MATHCHECK-GEO对超过20种LLM和11种多模态LLMs进行了评估，以检验它们的综合数学推理能力。结果显示，尽管前沿模型如GPT-4表现出色，但其他模型家族在清单上的表现显著下降。进一步实验表明，与传统数学基准相比，MATHCHECK更好地反映了真正的数学能力，线性度更高，从而支持我们的设计。通过MATHCHECK，我们可以轻松进行详细的行为分析，深入探究模型。|
|**2024-07-11**|**A Taxonomy for Data Contamination in Large Language Models**|Medha Palavalli et.al.|[2407.08716](http://arxiv.org/abs/2407.08716)|null|大型语言模型在基于广泛网络语料库的预训练后，在众多下游任务上展现出卓越性能。然而，数据污染问题日益引起关注，即评估数据可能存在于预训练数据中，导致模型表现虚高。去污染（decontamination）作为一种可能的解决方案，试图检测并移除这些污染数据。然而，污染数据可能源于测试集的修改版本，这使得检测变得困难。目前尚不清楚不同类型的污染如何影响语言模型在下游任务中的性能。我们提出了一种分类体系，对语言模型在预训练阶段遇到的各种污染类型进行划分，并确定了哪些类型的风险最高。我们通过分析总结和问答两个关键自然语言处理任务，揭示了不同类型污染如何影响模型在实际评估中的表现。|
|**2024-07-11**|**GTA: A Benchmark for General Tool Agents**|Jize Wang et.al.|[2407.08713](http://arxiv.org/abs/2407.08713)|**[link](https://github.com/open-compass/GTA)**|**人们普遍关注大型语言模型（LLMs）与各种工具的整合，以开发通用代理，但这对LLMs的工具使用能力提出了挑战。当前的评估方法存在明显缺陷，如使用AI生成的查询、单步骤任务、模拟工具以及仅限文本的交互，未能充分展示这些模型在实际问题解决中的能力。因此，我们提出GTA（通用工具代理基准），它包含三个关键特性：（1）真实的用户查询：由人类编写，具有简单的现实世界目标，但隐含了工具使用需求，要求LLMs能推理出合适的工具并规划解决方案步骤。（2）真实部署的工具：一个配备有感知、操作、逻辑和创新类工具的评估平台，用于评估模型的实际任务执行性能。（3）真实的多模态输入：包括空间场景图片、网页截图、表格、代码片段和打印/手写材料等，以贴近真实世界的场景。  我们设计了229个现实生活任务和可执行的工具链，来评估主流LLMs。实验结果显示，对于真实的用户查询，现有的LLMs面临严峻挑战，GPT-4完成的任务不足一半，大多数模型的成绩低于25%。这个评估揭示了当前LLMs在实际工具使用能力上的瓶颈，为提升通用工具代理的研究提供了方向。GTA的相关代码和数据集已可在<https://github.com/open-compass/GTA>获取。**|
|**2024-07-11**|**Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models**|Zhening Xing et.al.|[2407.08701](http://arxiv.org/abs/2407.08701)|null|大型语言模型因其单向时间注意力机制，在文本和音频流数据生成方面展现出卓越的效果。然而，尽管对实时视频处理的需求日益增长，但视频流处理的研究却相对较少。现有的视频扩散模型依赖双向时间注意力，这限制了它们处理直播视频的能力。为此，我们提出Live2Diff，这是首个专为实时视频翻译设计的具有单向时间注意力的视频扩散模型。与先前工作不同，我们的方法通过与前一帧及其少数预热帧相关联，保持了时间一致性和平滑性，无需考虑未来帧。同时，我们采用高效的降噪方案，包括KV缓存机制和流水线处理，以支持互动帧率下的视频流翻译。大量的实验结果表明，我们的注意力机制和流水线设计显著优于先前的方法，在保持时间平滑性和/或效率方面表现出色。|
|**2024-07-11**|**Mitigating Catastrophic Forgetting in Language Transfer via Model Merging**|Anton Alexandrov et.al.|[2407.08699](http://arxiv.org/abs/2407.08699)|null|随着开放型大型语言模型（LLMs）在英语任务中的性能不断提升，研究人员正致力于将其扩展到其他语言。然而，这种语言适应往往会导致基础模型能力的灾难性遗忘，限制了改编后模型的实用性。为此，我们提出了一种新的适应方法——Branch-and-Merge（BaM），它基于迭代地合并多个针对部分训练数据进行微调的模型。BaM的核心理念在于，这种方法产生的是幅度较小但质量更高的权重调整，从而减少对源领域的遗忘，同时保持对目标领域的学习。  我们在保加利亚语和德语的广泛实证研究中展示了BaM的优势：它能显著降低遗忘，同时在不同模型架构上与标准持续预训练和指令微调相比，能够匹配甚至提升目标领域的性能。|
|**2024-07-11**|**Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**|Zhiqiang Xie et.al.|[2407.08694](http://arxiv.org/abs/2407.08694)|null|在现代云系统中，运行时故障和性能下降是常态。对于云服务提供商而言，自动确定问题的根本原因是保证高可靠性和可用性的关键，因为快速的故障定位有助于加快诊断和优先级排序，以实现及时解决。近期的研究中，因果推理利用因果图来捕捉不同云系统性能指标之间的关系是一个有前景的解决方案。然而，系统开发者需要精确定义系统的因果图，这是一项耗时、脆弱且挑战性的工作，尤其对于庞大和动态的系统，且需要深厚的专业知识。数据驱动的方法在云系统中的效果有限，因为故障事件的发生频率相对较低。  本工作中，我们提出了一种新颖的解决方案——Atlas，它能够自动合成云系统的因果图。Atlas利用大规模语言模型（LLMs）结合系统文档、日志和部署反馈生成因果图。Atlas与数据驱动的因果发现技术相辅相成，并通过数据驱动的验证步骤进行增强。我们在一系列故障定位场景中评估了Atlas，结果表明，Atlas能够在可扩展和普适的方式下生成因果图，其性能远超数据驱动算法，并与基准线相当。|
|**2024-07-11**|**SEED-Story: Multimodal Long Story Generation with Large Language Model**|Shuai Yang et.al.|[2407.08683](http://arxiv.org/abs/2407.08683)|**[link](https://github.com/tencentarc/seed-story)**|**随着图像生成和开放形式文本生成的显著进步，交错的图像-文本内容创作领域变得越来越有吸引力。多模态故事生成，即生成叙事文本与生动图像的交错序列，作为一种有价值的实用任务，因其广泛的应用前景而受到关注。然而，这一任务面临着理解文本和图像复杂交互、生成连贯且相关文本和视觉内容的挑战。本工作中，我们提出SEED-Story，这是一种新颖的方法，它利用强大的多模态大型语言模型（MLLM）来生成扩展的多模态故事。我们的模型基于MLLM的强大理解能力，既能预测文本令牌，也能预测视觉令牌，然后通过适应的视觉解令牌化器处理，生成具有一致角色和风格的图像。我们还引入了多模态注意力沉降机制，使得在高度自动递归的方式下，能够生成长达25个序列（仅用10个进行训练）的故事。此外，我们还提供了大规模高分辨率的StoryStream数据集，用于训练我们的模型，并量化评估多模态故事生成任务在多个方面的性能。**|
|**2024-07-11**|**Uncertainty Estimation of Large Language Models in Medical Question Answering**|Jiaxin Wu et.al.|[2407.08662](http://arxiv.org/abs/2407.08662)|null|## 任务  大型语言模型（LLMs）在医疗领域的自然语言生成方面展现出潜力，但存在产生错误事实的风险。为了在医疗问题解答中部署这些模型，需要可靠的不确定性估计（UE）方法来识别幻觉。本研究中，我们在医学问答数据集上对流行UE方法及其不同模型规模进行了评估。结果显示，当前方法在该领域通常表现不佳，凸显了医疗应用中的UE挑战。我们还观察到，更大的模型往往能获得更好的结果，这表明模型规模与UE可靠性可能存在关联。  为应对这些挑战，我们提出了一种名为“两阶段验证”的概率自由不确定性估计方法。首先，LLM生成逐步解释和初始答案，接着制定核查问题以检查解释中的事实陈述。模型会两次回答这些问题：一次独立，一次参考解释。两种答案之间的不一致度衡量原始响应的不确定性。我们在三个生物医学问答数据集上使用Llama 2 Chat模型评估我们的方法，并将其与基准基线方法进行比较。  实验结果显示，我们的两阶段验证方法在各个数据集和模型规模上实现了最佳的整体准确性和稳定性，并且其性能随模型大小的增加而提升。|
|**2024-07-10**|**Training on the Test Task Confounds Evaluation and Emergence**|Ricardo Dominguez-Olmedo et.al.|[2407.07890](http://arxiv.org/abs/2407.07890)|**[link](https://github.com/socialfoundations/training-on-the-test-task)**|**我们研究了一个大型语言模型评估中的核心问题，称为在测试任务上训练。这并非如数据泄露或污染等不当做法，而是一种逐渐增长的包括任务相关数据在预训练阶段的技术。我们发现，在测试任务上训练会混淆模型的相对评估和关于涌现能力的声明。我们提出，不同模型家族之间的看似优势可能由他们在测试任务上的训练程度差异所解释。为此，我们提出了一种有效方法，即在比较前对每个模型进行相同的任务相关数据微调，以校正这种训练。结果显示，一旦调整了在测试任务上的训练，涌现行为的实例大多消失。同样适用于那些无法用评价指标解释的涌现行为报告案例。我们的工作推动了对大型语言模型的新评价视角，对基准测试和涌现能力研究具有广泛影响。**|
|**2024-07-10**|**Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization**|Junkang Wu et.al.|[2407.07880](http://arxiv.org/abs/2407.07880)|**[link](https://github.com/junkangwu/dr_dpo)**|**本研究关注在训练数据中噪声对Direct Preference Optimization (DPO)方法的挑战，该方法用于调整大型语言模型（LLMs）以符合人类偏好。我们区分了两类噪声：点噪声，涉及低质量的数据点；和成对噪声，影响偏好的正确排序。通过分布式鲁棒优化（DRO），我们增强了DPO抵抗这些噪声的能力。理论分析揭示，DPO本质上蕴含了DRO原理，对点噪声具有天然的鲁棒性，其中正则化系数 $\beta$在抗噪声方面起关键作用。在此基础上，我们提出分布式鲁棒增强的DPO（Dr. DPO），它通过优化最坏情况的成对场景来集成成对鲁棒性。Dr. DPO中的新超参数$\beta'$ 允许对数据对可靠性进行精细控制，平衡了在嘈杂训练环境中的探索与利用。实证评估显示，Dr. DPO显著提高了生成文本的质量和响应准确性，无论在有噪声还是无噪声的设置下都表现出色。代码已在https://github.com/junkangwu/Dr_DPO上提供。**|
|**2024-07-10**|**FACTS About Building Retrieval Augmented Generation-based Chatbots**|Rama Akkiraju et.al.|[2407.07858](http://arxiv.org/abs/2407.07858)|null|随着生成式人工智能驱动的企业聊天机器人日益成为提升员工生产力的关键工具，基于检索增强生成（RAG）的、大型语言模型（LLMs）以及如Langchain和Llamaindex之类的orchestration框架在构建这些聊天机器人中扮演了重要角色。然而，创建有效的企业聊天机器人是一项挑战，需要精心设计的RAG管道工程。这包括微调嵌入和LLMs、从向量数据库提取文档、重述查询、重新排名结果、设计提示、遵守文档访问控制、提供简洁的回答、包含引用、保护个人信息以及构建orchestration代理。我们基于三个NVIDIA聊天机器人（分别用于IT/HR福利、财务收益和通用内容）的经验，提出了一种构建RAG聊天机器人的框架——FACTS（Freshness、Architectures、Cost、Testing、Security）。我们的贡献有三方面：首先介绍FACTS框架，其次列出十五个RAG管道控制点，最后提供了关于大模型和小模型在准确性和延迟之间权衡的实证结果。据我们所知，这是首篇全面探讨构建安全企业级聊天机器人的方法和解决方案的论文。|
|**2024-07-10**|**OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training**|Sami Jaghouar et.al.|[2407.07852](http://arxiv.org/abs/2407.07852)|**[link](https://github.com/PrimeIntellect-ai/OpenDiLoCo)**|**OpenDiLoCo是一个开源的分布式低通信（DiLoCo）训练方法的实现和复制，针对大型语言模型。我们提供了可复现的DiLoCo实验，通过Hivemind库构建了一个可扩展的去中心化训练框架。我们在两个大洲和三个国家之间训练模型，同时保持90-95%的计算资源利用率。此外，我们进行了关于算法计算效率、工作器数量可扩展性的研究，并表明其梯度可以使用FP16进行全归一化而不会影响性能。最后，我们将OpenDiLoCo扩展到原始工作的三倍规模，证明了它在百亿参数模型上的有效性。**|
|**2024-07-10**|**Natural Language Mechanisms via Self-Resolution with Foundation Models**|Nicolas Della Penna et.al.|[2407.07845](http://arxiv.org/abs/2407.07845)|null|在实际操作中，代理人通常受限于诸如交易或订单之类的有限报告格式，这可能限制了他们表达信息的能力。我们提出了一种新型机制，它促使代理人以自然语言提交报告，并利用大型语言模型（LLM）的强大功能来选择结果和分配报酬。我们确定了这些机制在LLM作为良好的世界模型以及强烈的跨代理信息过度确定条件下的激励兼容性和效率的必要条件。实验表明，当传统预测市场在信号结构上存在问题时，这些基于LLM的机制能够成功地整合信息。|
|**2024-07-10**|**Transformer Alignment in Large Language Models**|Murdock Aubry et.al.|[2407.07810](http://arxiv.org/abs/2407.07810)|null|大型语言模型（LLMs）在自然语言处理方面取得了显著进步，深入理解其内部机制至关重要。我们视LLMs为高维空间中的离散、耦合的非线性动力系统，通过研究tokens在Transformer块中的轨迹，并沿着这些轨迹线性化系统，利用雅可比矩阵进行分析。在对38个公开可用的LLMs进行研究后，我们观察到残差雅可比矩阵的上左和右奇异向量之间的对齐，以及线性性和层内指数增长的出现。值得注意的是，我们发现对齐度的提高与模型性能呈正相关。训练后的评估显示，相比于随机初始化权重时的指标，有显著改善，这强调了训练在Transformer架构中的重要影响。这些发现揭示了一种以前未被充分认识的规律性，强化了动力学解释，并为进一步理解和优化LLM架构铺平了道路。|
|**2024-07-10**|**Attribute or Abstain: Large Language Models as Long Document Assistants**|Jan Buchmann et.al.|[2407.07799](http://arxiv.org/abs/2407.07799)|**[link](https://github.com/ukplab/arxiv2024-attribute-or-abstain)**|**## 背景 大语言模型（LLMs）能够辅助处理长篇文档，但它们也存在胡言乱语的问题。增加可信度的方法是通过提供证据支持响应，提高可验证性。当前的归因方法仅在基于检索的生成（RAG）环境中评估过，这与无需检索的长文档场景不同，可能仍有应用价值。因此，缺乏针对长文档的归因专门评估。为此，我们提出LAB，一个包含6个多样化的长文档任务的基准，并在四种不同大小的LLM（即提示和微调）上试验了不同的归因方法。研究结果显示，一步生成引用（citation，即同时进行响应生成和证据提取）的表现最佳。我们还探究了“迷失在中间”现象是否适用于归因，但未发现这种情况。此外，我们发现证据质量在简单响应的场景下可以预测响应质量，但对于复杂响应则不然，因为模型在为复杂主张提供证据时面临挑战。我们公开了代码和数据，以供进一步研究。**|
|**2024-07-11**|**Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard**|Oguzhan Topsakal et.al.|[2407.07796](http://arxiv.org/abs/2407.07796)|**[link](https://github.com/research-outcome/llm-game-benchmark)**|**我们提出了一种新颖且可扩展的大型语言模型（LLM）基准测试，通过网格型游戏如井字棋、连接四和围棋进行。开源的游戏模拟代码在GitHub上提供，允许LLMs竞技，并生成JSON、CSV、TXT和PNG格式的详细数据文件，用于排行榜排名和进一步分析。我们展示了包括Anthropic的Claude 3.5 Sonnet和Claude 3 Sonnet，Google的Gemini 1.5 Pro和Gemini 1.5 Flash，OpenAI的GPT-4 Turbo和GPT-4o，以及Meta的Llama3-70B在内的领先LLM之间的比赛结果。我们鼓励其他LLM提交结果。总共进行了2,310场模拟比赛（每对模型进行5轮，共7个模型间的对局，以及与随机玩家的比赛），涵盖三种类型的游戏，使用了列表、插图和图像三种提示方式。结果显示，LLM在不同游戏和提示类型下的性能存在显著差异，分析内容包括胜率、错失机会和无效动作。排行榜和结果矩阵的详细数据作为开放访问数据在GitHub上提供。这项研究加深了我们对LLM在未专门训练的游戏中的能力的理解，有助于评估它们的规则理解能力和战略思维。在通向人工智能通用性的道路上，这项研究为未来探索它们在复杂决策场景中的实用性奠定了基础，揭示了它们的战略思考能力，并为深入探究LLM在基于游戏框架内的局限性提供了方向。**|
|**2024-07-10**|**Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities**|Tianjie Ju et.al.|[2407.07791](http://arxiv.org/abs/2407.07791)|**[link](https://github.com/Jometeorie/KnowledgeSpread)**|**随着大型语言模型（LLMs）在多代理系统中的迅速应用，它们在协作问题解决和自主谈判等领域的出色性能引起了关注。然而，这些基于LLM的多代理系统的安全问题尚未得到充分研究，尤其是在知识操纵传播方面。本文通过构建详细的威胁模型和模拟环境，模拟现实世界中的多代理部署在可信平台上，探讨这一关键问题。我们提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵知识注入，来系统地探究在无明确提示操纵的情况下，如何潜在地传播操纵知识（如虚构和有害知识）。我们的方法利用了LLMs处理世界知识固有的漏洞，攻击者可以借此无意识地传播编造的信息。实验结果表明，我们的攻击方法能够成功诱导基于LLM的代理在交流中传播这两种操纵的知识，同时不会显著降低它们的基础功能。此外，我们发现这些操纵会持续存在于流行的检索增强生成框架中，即使交互结束，若干良性代理也可能继续受到操纵聊天记录的影响。我们的发现揭示了LLM多代理系统中的重大安全风险，强调了对操纵知识传播进行强大防御的迫切需求，比如引入“守护”代理和先进的事实核查工具。**|
|**2024-07-10**|**WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment**|Jiefu Ou et.al.|[2407.07778](http://arxiv.org/abs/2407.07778)|null|本文探讨了在物理环境中部署人工智能（AI）代理时所需的基本操作（API）数量和设计问题。研究者设想，如果wikiHow教程涵盖了广泛的用户自编任务，那么这些任务所需的API范围是什么。他们提出了一种方法，通过将wikiHow指令与置身于环境中的代理策略关联，迭代地生成新的API。借助大型语言模型（LLMs）在体感规划方面的最新成就，研究者提议使用少量样例提示GPT-4生成Python代码作为代理策略，并通过以下步骤扩展API库：1）重用初始API集；2）在必要时创建新的API调用。实验关注的是定义API，而非其实现性。在一小部分wikiHow教程上应用该方法后，发现需要300多个API来捕捉现实世界中的多样任务。自动和人工分析显示，提出的管道能有效复用和创造API。进一步的人工审查发现，现有的模拟器仅支持诱导出的API的一小部分（前50个常用API中的9个），这促使开发更丰富的体感环境。|
|**2024-07-09**|**AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning**|Jiaxi Cui et.al.|[2407.07094](http://arxiv.org/abs/2407.07094)|**[link](https://github.com/pandavt/datatager)**|**在各行各业广泛采用大型语言模型（LLMs）的过程中，往往忽视了个体和小型组织对针对其特定业务场景定制化模型的需求。为此，我们提出了一种新颖的微调方法——\textbf{AnyTaskTune}，即任务微调（Task-Fine-Tune），旨在提升模型在多样化的领域特定任务上的性能。该方法包括细致地识别和定义领域内的子任务，随后创建专门的增强数据集进行精细调整，从而优化任务特定的模型表现。我们在法律（如关键词提取和句子预测）等多个领域，包括金融、医疗、法律、心理学、客户服务和人力资源等二十多个子任务上进行了广泛的微调实验。为了支持社区参与并分享资源，我们将开源这些双语任务数据集。实验结果显示，使用\textbf{Task-Fine-Tune}方法微调的模型不仅在特定任务上表现出色，而且在各自领域内明显优于通用能力更强的模型。我们的工作已公开发布在：\url{https://github.com/PandaVT/DataTager}。**|
|**2024-07-09**|**FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation**|Liqun Ma et.al.|[2407.07093](http://arxiv.org/abs/2407.07093)|**[link](https://github.com/liqunma/fbi-llm)**|**该研究介绍了一种全新的全二进制大型语言模型（FBI-LLM），这是首次展示如何从头开始训练大规模的全二进制语言模型（不同于部分二进制或三进制的LSTM，如BitNet b1.58），其性能能够与浮点16位（FP16）或混合精度16位（BF16）的常规大语言模型相当。通过使用自回归蒸馏（AD）损失，同时保持模型尺寸（130M、13B、7B）和预训练数据量与常规LLM相当，FBI-LLM在困惑度和任务特定效果方面表现出竞争性。有趣的是，我们发现从零开始训练全二进制语言模型并不需要预训练权重。这项工作催生了一个新的计算框架，并可能推动针对完全1比特LLMs的专业硬件设计。我们公开所有模型、代码和训练数据，以支持进一步的研究（代码：https://github.com/LiqunMa/FBI-LLM，模型：https://huggingface.co/LiqunMa/）。**|
|**2024-07-09**|**Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models**|Logan Cross et.al.|[2407.07086](http://arxiv.org/abs/2407.07086)|**[link](https://github.com/locross93/hypothetical-minds)**|**在多智能体强化学习（MARL）方法中，处理多智能体系统的非stationarity并适应在线学习的能力是一个挑战。为此，我们利用大型语言模型构建了一个自主的解决策略。我们的新型智能体“假设心智”（Hypothetical Minds）采用认知启发式架构，包括感知、记忆和两个抽象层次上的分层规划模块。关键新增的是“心理理论”模块，它以自然语言的形式生成对其他智能体策略的假设，并通过验证这些假设对其他智能体行为的预测准确性来逐步优化。在Melting Pot基准的多种竞争、混合动机和协作环境中，假设心智显著优于先前的语言模型智能体和强化学习基线，无论是在二元环境还是群体环境中。对比分析显示，假设的评估和迭代精炼对于应对复杂场景至关重要。**|
|**2024-07-09**|**Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities**|Shaltiel Shmidman et.al.|[2407.07080](http://arxiv.org/abs/2407.07080)|null|该论文探讨了在希伯来等低资源语言中训练大型语言模型（LLMs）的挑战。我们介绍了DictaLM2.0和DictaLM2.0-Instruct，这两个模型基于Mistral模型，使用大约2000亿个希伯来语和英语词汇进行训练。适应预训练模型到新语言需要专门的技术，这与从头训练或在资源丰富的语言（如英语）上进一步训练现有模型有显著差异。论文详细阐述了这些创新的训练方法，以促进希伯来语的高效学习和适应其语言特性。此外，我们还对DictaLM2.0-Instruct进行了全面的指令微调，以提升其在任务导向指令上的性能。为了严格评估我们的模型，我们开发了一个新的希伯来LLM评估基准，涵盖了问答、情感分析、Winograd Schema Challenge、翻译和摘要等多个任务。本文不仅解决了在低资源语言中训练LLMs的复杂性，还提出了一种可用于其他LLM跨非英语语言适应的框架，从而对多语言自然语言处理领域做出了贡献。|
|**2024-07-09**|**Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps**|Yung-Sung Chuang et.al.|[2407.07071](http://arxiv.org/abs/2407.07071)|**[link](https://github.com/voidism/lookback-lens)**|**该论文探讨了大型语言模型（LLMs）在总结文章或根据给定段落回答问题时可能出现的语境性虚构问题。LLMs可能会杜撰细节，提供与输入上下文不符的不准确答案。研究者提出，这种虚构与模型倾向于关注上下文信息还是自动生成内容的程度有关。为此，他们设计了一个简单的检测模型——“Lookback Lens”，其输入特征是基于每个注意力头上下文注意力权重与新生成词的比例。实验表明，仅使用这些回顾比率特征的线性分类器与利用LLM整个隐藏状态或文本蕴含模型的更复杂检测器同样有效。Lookback Lens不仅适用于不同任务，还能跨模型迁移，一个在70亿参数模型上训练的检测器无需重新训练即可应用于更大的130亿参数模型。此外，研究还发现，通过简单的分类器指导解码方法，能够减少诸如XSum摘要任务中的虚构程度，例如降低9.6%的虚构发生率。**|
|**2024-07-09**|**Prompting Techniques for Secure Code Generation: A Systematic Investigation**|Catherine Tony et.al.|[2407.07064](http://arxiv.org/abs/2407.07064)|null|## 概要  随着大型语言模型（LLMs）在软件开发中的兴起，通过提示驱动编程，开发者能够通过自然语言（NL）指令生成代码。然而，关于它们能否产生安全代码的研究引发了质疑，这关系到提示生成软件的质量。尽管已经出现了多种精心设计的提示策略以优化LLM的响应，但这些方法与安全代码生成之间的相互作用仍需进一步研究。目标：本研究旨在探究不同提示技术对LLMs根据NL指令生成代码的安全性影响。方法：首先，我们进行系统文献回顾，以识别适用于代码生成任务的现有提示技术。然后，我们在GPT-3、GPT-3.5和GPT-4模型上评估这些技术中的部分，使用一个包含150个与安全相关的代码生成NL提示的数据集。结果：我们的工作（1）对代码生成的潜在提示技术进行了分类，（2）适应并评估了这些技术在安全代码生成任务中的表现，（3）观察到在测试的LLMs中，尤其是在使用了名为“递归批评与改进”（RCI）的现有技术后，安全漏洞有所减少，为LLM生成代码安全性的讨论提供了有价值的见解。|
|**2024-07-09**|**Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence**|Weize Chen et.al.|[2407.07061](http://arxiv.org/abs/2407.07061)|**[link](https://github.com/openbmb/ioa)**|**随着大型语言模型的迅速发展，出现了能效卓越的自主代理。然而，现有的多代理框架在整合来自不同生态系统的高能力第三方代理时面临挑战，通常局限于自身封闭环境。它们在模拟分布式环境时也受限于单设备设置，并且往往依赖硬编码的通信管道，难以适应任务需求的变化。受互联网理念启发，我们提出了一种名为“代理互联网”（Internet of Agents，IoA）的新框架。IoA旨在解决这些问题，提供一个灵活且可扩展的平台，促进基于语言模型的多代理协作。它引入了代理集成协议、即时消息架构以及动态的团队协作和对话流程控制机制。通过在通用助手任务、体感AI任务和检索增强生成基准上的广泛实验，我们证明IoA在性能上持续优于现有最先进的基线，展示了其在异构代理之间有效合作的能力。IoA代表了朝着将多样化的代理链接在一个类似互联网的环境中迈进，让它们能够无缝协作以提升整体智能和功能。我们的代码库已发布在：\url{https://github.com/OpenBMB/IoA}。**|
|**2024-07-09**|**Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model**|Wenqi Zhang et.al.|[2407.07053](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|**尽管当前的大型多模态模型（LMMs）已经能够理解自然场景的照片和肖像，但它们对抽象图像（如图表、地图或布局）的理解以及视觉推理能力仍然相当初级。它们在处理日常任务时常常遇到困难，例如阅读时钟时间、理解流程图或根据路线图规划路径。鉴于此，我们设计了一个多模态自我指导系统，利用大型语言模型及其代码能力来生成大量的抽象图像和日常场景下的视觉推理指令。我们的方法轻松创建了一个多模态基准，包含11,193个指令，涵盖八个视觉场景：图表、表格、模拟地图、仪表板、流程图、关系图、楼层平面图和视觉谜题。  这个由简单线条和几何元素构成的基准揭示了最先进的LMM（如Claude-3.5-Sonnet和GPT-4o）在抽象图像理解、空间关系推理和视觉元素识别方面的局限性。此外，为了验证合成数据的质量，我们使用62,476条合成的图表、表格和路线图指令对LMM进行微调。结果显示，图表理解和地图导航性能得到了提升，同时也表明这对其他视觉推理任务可能具有潜在益处。我们的代码已在以下链接提供：\url{https://github.com/zwq2018/Multi-modal-Self-instruct}。**|
|**2024-07-09**|**Using Large Language Models for Generating Smart Contracts for Health Insurance from Textual Policies**|Inwon Kang et.al.|[2407.07019](http://arxiv.org/abs/2407.07019)|null|我们研究利用大型语言模型（LLMs）自动生成基于文本的健康保险政策的自动化代码，目标是区块链智能合约。智能合约因其不可变性、可验证性、扩展性和无需预设信任的特性而被选中。我们的方法按技术复杂度递增生成输出：（1）文本摘要，（2）声明式决策逻辑，以及（3）带有单元测试的智能合约代码。我们确认LLMs在任务（1）上表现出色，而结构化的输出有助于验证任务（2）和（3）。声明式语言常用于规范医疗政策，但在区块链上的执行较为复杂，因此任务（3）旨在直接通过智能合约自动实现这一过程。我们提出完整性、正确性、清晰度、语法和功能性代码作为评估指标。我们使用了来自Medicare官方手册的三个具有不同难度的保险政策场景进行评估，涉及GPT-3.5 Turbo、GPT-3.5 Turbo 16K、GPT-4、GPT-4 Turbo和CodeLLaMA等模型。结果显示，LLMs在生成文本摘要方面表现良好。尽管任务（2）到（3）的输出可以作为起点，但它们仍需人工审核：在某些情况下，即使“可运行”的代码也可能产生不正确的结果；目标语言的流行程度会影响输出质量；更复杂的场景仍是当前的一大挑战。然而，我们的实验展示了LLMs在将文本流程描述转化为智能合约方面的潜力。|
|**2024-07-09**|**End-To-End Causal Effect Estimation from Unstructured Natural Language Data**|Nikita Dhawan et.al.|[2407.07018](http://arxiv.org/abs/2407.07018)|null|了解干预措施的效果对人类决策至关重要。然而，当前因果效应估计方法依赖于手动收集和结构化数据，这导致研究成本增加、完成时间延长。我们展示了如何利用大型语言模型（LLMs）开采大规模、多样化的观察性文本数据，以在适当的因果假设下生成低成本的因果效应估计。我们提出NATURAL，一个基于LLMs的新型因果效应估计算法家族，适用于处理未结构化的文本数据。我们的方法利用LLMs的条件分布（针对感兴趣的变量，根据文本数据）辅助计算经典的因果效应估计。我们克服了一系列技术挑战，如自动化数据整理和使用LLMs填补缺失信息。  我们准备了六个（两个合成的和四个实际的）观察性数据集，并配以随机对照试验形式的真实标签，系统地评估了我们管道中的每一步。NATURAL估计算法表现出色，其结果与真实值的差距不超过3个百分点，包括在实际的三期和四期临床试验中。这些结果表明，未结构化的文本数据是因果效应信息的丰富来源，NATURAL是利用这一资源的自动化流程的第一步。|
|**2024-07-08**|**Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision**|Orr Zohar et.al.|[2407.06189](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|**大型视觉语言模型（LVLM）的性能与其训练数据的规模和质量密切相关。当前的视频指令调优数据集缺乏多样性，因为它们主要由提示大型语言模型生成视频字幕以形成问题-答案对，内容多为描述性。然而，许多带有丰富标签和监督的视频数据集已经存在，但如何将它们融入LVLM并非易事。  为此，我们提出了视频自我训练与增强推理（Video Self-Training with augmented Reasoning，简称Video-STaR），这是首个视频自我训练方法。Video-STaR使得任何标注的视频数据集都能用于视频指令调优。在这个过程中，LVLM在生成指令和微调之间循环。我们发现，这不仅能提升视频整体理解能力（I），还能让LVLM适应新的下游任务，利用现有监督进行学习。  具体来说，LVLM被提示提出一个答案，然后仅保留那些包含原始视频标签的答案。LVLM随后在生成的数据集上进行再训练。通过只在包含正确视频标签的生成答案上训练，Video-STaR利用现有的视频标签作为弱监督来指导视频指令调优。  实验结果显示，经过Video-STaR增强的LVLM在（I）一般视频问答任务中的表现提升了10%，在（II）下游任务中，Video-STaR提高了Kinetics700-QA的准确性20%，以及FineDiving动作质量评估的性能15%。总的来说，Video-STaR为LVLM的性能提升提供了一种有效且实用的方法。**|
|**2024-07-08**|**CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation**|Xinying Guo et.al.|[2407.06188](http://arxiv.org/abs/2407.06188)|null|在娱乐行业（如动画和游戏）以及战略领域（如城市模拟和规划）中，人群运动生成至关重要。然而，这一任务需要精细地融合控制与生成，以在特定的空间和语义约束下实现逼真的群体动态合成，其挑战尚未得到充分探索。当前的人体动作生成模型往往关注个体行为，忽视了集体行为的复杂性；而多个人体动作生成的最新方法严重依赖预设场景，且限于固定、少量的人际互动，限制了其实用性。  为解决这些问题，我们提出CrowdMoGen，一个零样本文本驱动的框架，它利用大型语言模型（LLM）的力量，将集体智慧融入运动生成框架，从而能够在没有配对训练数据的情况下实现通用的规划和群体运动生成。我们的框架主要由两个关键组件构成：1）人群场景规划器，学习根据特定场景上下文或引入的扰动协调运动和动态；2）集体运动生成器，根据整体计划高效合成所需的集体运动。大量的定量和定性实验验证了我们框架的有效性，它不仅填补了大规模和通用人群运动生成任务的重要空白，而且在真实感和灵活性方面表现出高水准。|
|**2024-07-08**|**On Speeding Up Language Model Evaluation**|Jin Peng Zhou et.al.|[2407.06172](http://arxiv.org/abs/2407.06172)|null|大型语言模型（LLMs）在自然语言处理（NLP）领域占据主导地位，它们在各种任务上表现出最先进的能力。从训练到推理，构建这样的模型涉及众多决策，形成一个复杂的搜索问题。例如，为了为特定任务找到最佳的预训练LLM、提示或超参数，通常需要对整个测试集中的多个候选方案进行全面评估。这种详尽的评估耗时且昂贵，因为LLMs的推理和度量计算需求高。  本文针对在有限预算内有效评估方法在测试样本上的性能这一挑战。我们利用了广泛研究的多臂老虎机框架，该框架通过顺序选择下一个要评估的方法-示例对，将我们的方法——结合多臂老虎机算法与低秩分解——显著减少了所需的资源。实验表明，我们的算法仅使用通常需求的5%-15%资源，就能识别出表现最好的方法，从而实现了高达85%-95%的成本节省。|
|**2024-07-08**|**What's Wrong with Your Code Generated by Large Language Models? An Extensive Study**|Shihan Dou et.al.|[2407.06153](http://arxiv.org/abs/2407.06153)|null|随着大型语言模型（LLMs）在代码生成领域的快速发展，研究人员对此的关注度日益提高。目前的研究主要集中在构建高质量数据集和采用多样化的训练技术来提升LLM的代码生成能力。然而，对于这些现有方法的局限性和边界，缺乏全面的研究探讨。为此，我们进行了一项详尽的实证研究，评估了三个领先闭源LLM和四个开源LLM在三个常用基准上的性能。研究考察了生成代码的长度、循环复杂度和API数量，结果显示这些模型在处理更复杂的编程问题时面临挑战，生成的代码往往较短但结构更复杂，与标准解决方案相比。  我们还创建了一个错误代码的分类体系，分为三个类别和12个子类别，分析常见错误类型的根源。为了检验LLMs在实际项目中的表现，我们亲手构建了一个包含140个代码生成任务的现实世界基准。对比分析显示，实际场景中的bug分布与现有基准存在显著差异。最后，我们提出了一种无需额外训练的迭代方法，引入自我批判机制，使LLMs能够根据bug类型和编译器反馈修正其生成的代码。实验结果表明，经过两次迭代后，我们的方法能显著减少错误，使通过率提高29.2%，这表明LLMs在处理复杂问题方面具有巨大潜力。|
|**2024-07-09**|**Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks**|Lukas Netz et.al.|[2407.06146](http://arxiv.org/abs/2407.06146)|null|我们介绍并评估了一种名为“语法遮盖”的方法，该方法用于引导大型语言模型（LLMs）在给定上下文无关文法的约束下生成语法正确的模型。尽管少量示例学习或提示引导等prompt工程方法可以提高LLMs生成正确语法的概率，但处理复杂文法时，这些方法往往耗时且效果不理想。当前的研究主要集中在语言模型训练或prompt工程上。本文提出了一种新方法，通过约束解码限制输出，确保生成的内容符合有效语法。我们利用MontiCore构建的多种领域特定语言（DSL）和多款LLMs进行实验，比较了使用和未使用约束解码的效果。同时，我们采用相应的解析器验证每种模型的句法准确性。实验结果显示，语法遮盖显著提升了多个LLMs的建模能力，减少了对精心设计提示的需求，提高了生成正确模型的可能性。|
|**2024-07-08**|**ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation**|Ethan Chern et.al.|[2407.06135](http://arxiv.org/abs/2407.06135)|**[link](https://github.com/gair-nlp/anole)**|**## 背景 先前的开源大型多模态模型（LMMs）存在一些局限性：（1）它们往往缺乏原生集成，需要适配器来衔接视觉表示与预训练的大型语言模型（LLMs）；（2）许多模型仅限于单模态生成；（3）尽管有些支持多模态生成，但它们依赖于单独的扩散模型处理视觉部分。为了克服这些问题，我们介绍了Anole，一个开源的、自回归的、原生的大型多模态模型，专为交错的图像-文本生成设计。我们基于Meta AI的Chameleon构建Anole，采用了一种既数据高效又参数高效的创新微调策略。Anole展示了高质量、连贯的多模态生成能力。我们已经公开了我们的模型、训练框架以及指令调优数据。**|
|**2024-07-08**|**Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization**|Hannah K. Bako et.al.|[2407.06129](http://arxiv.org/abs/2407.06129)|**[link](https://github.com/hdi-umd/semantic_profiling_llm_evaluation)**|**### 概述  自动根据人类对数据集的口头描述生成数据可视化图表，需要深度理解语言中的语义信息，包括对数据属性、可视化任务以及数据预处理步骤的隐含和明确提及。自然语言界面（NLIs）在数据可视化方面已经探讨了如何捕捉这些信息，但人类言语的不确定性带来了挑战。近期的大型语言模型（LLMs）为解决这些问题提供了可能，但它们提取相关语义信息的能力尚待探索。本研究评估了四款公开可用的LLMs（GPT-4、Gemini-Pro、Llama3和Mixtral），分析它们在面对不确定性时理解口头指令的能力，并识别数据上下文和视觉任务。研究结果显示，LLMs对口语中的不确定性很敏感，能够提取关键的数据背景信息。然而，它们在推断可视化任务方面表现欠佳。基于这些发现，我们提出了未来利用LLMs进行可视化生成的研究方向。**|
|**2024-07-08**|**Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities**|Avinash Anand et.al.|[2407.06125](http://arxiv.org/abs/2407.06125)|null|抑郁症被广泛认为是重大的公共卫生问题，严重影响个人的心理健康。未经诊断的抑郁症可能导致严重的健康问题，包括生理症状甚至自杀。通常，抑郁症的诊断依赖于临床医生和心理健康专业人员进行的结构化访谈和如Patient Health Questionnaire（PHQ）等问卷调查。然而，这在很大程度上依赖于医生的经验和判断，可能受到个人偏见的影响。由于抑郁症的成因仍在研究中，医生在识别和治疗初期阶段的抑郁症时面临挑战。  近期，人工智能神经计算在文本、图像和语音处理等领域取得了显著进展。我们的研究尝试利用这些最先进的模型，在E-DAIC（Extended Distress Analysis Interview Corpus Wizard of Oz）数据集和2019年Audio/Visual Emotion Challenge（AVEC）中进行实验，以期优化多模态结果。实验结果显示，我们提出的解决方案利用专有和开源大型语言模型（LLMs），在文本模态上的Root Mean Square Error（RMSE）得分达到3.98，优于AVEC 2019挑战的基线和当前最佳的回归分析架构。此外，我们的方法在分类任务中的准确性达到了71.43%。论文还介绍了一个新颖的音频-视觉多模态网络，其预测PHQ-8评分的RMSE为6.51。|
|**2024-07-08**|**Artificial Intuition: Efficient Classification of Scientific Abstracts**|Harsh Sakhrani et.al.|[2407.06093](http://arxiv.org/abs/2407.06093)|null|## 背景 为了获取战略洞见或进行科研项目管理，对简短的科学文本（如研究基金申请书或出版物摘要）进行粗粒度分类至关重要。这些文本向具备深厚专业知识的专家传达密集信息，但自动化的任务极其艰巨，因为篇幅有限且缺乏上下文。为此，我们开发了一种新方法来生成并准确分配特定领域的粗标签。研究表明，大型语言模型（LLM）能够提供任务所需的元数据，类似于增强人类直觉的补充知识，并提出了一个工作流程。作为初步实验，我们使用了美国国家航空航天局（NASA）的奖项摘要数据库。我们结合现有性能指标，开发了新的评估工具。|
|**2024-07-08**|**Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models**|Jinliang Lu et.al.|[2407.06089](http://arxiv.org/abs/2407.06089)|null|随着大型语言模型（LLMs）的显著成功，自然语言处理（NLP）研究进入了新时代。尽管这些模型各有所长，但训练在不同语料库上的LLMs表现出不同的优势和劣势，这给提高整体效率和灵活性带来了挑战。为了应对这些挑战，近期的研究探索了LLMs的协作策略。本文全面概述了这一新兴研究领域，强调了合作背后的动力。我们将协作策略主要分为三种方法：合并、集成和协作。合并是将多个LLMs的参数空间整合。集成则是结合多个模型的输出。协作利用不同LLMs的优势，使其在特定任务中发挥各自专长。我们将从不同角度详细介绍这些方法，并讨论其潜在应用。此外，我们还勾勒出未来的研究方向，期望本工作能激发更多关于LLMs协作的研究，推动高级NLP应用的发展。|
|**2024-07-05**|**Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs**|Rudolf Laine et.al.|[2407.04694](http://arxiv.org/abs/2407.04694)|**[link](https://github.com/lrudl/sad)**|## 背景  人工智能助手，如ChatGPT，在被训练时会回应用户：“我是一个大型语言模型”。这引发了一个问题：这些模型是否真的知道自己是LLMs，并能据此可靠地行动？它们是否了解自己当前的部署情况，例如面向公众？我们称之为模型的“情境意识”。为了量化大型语言模型（LLMs）的情境意识，我们设计了一套行为测试，基于问答和指令执行，这就是**情境意识数据集（Situational Awareness Dataset，简称SAD）**。该基准包括7个任务类别，超过13,000个问题，测试了多项能力，如识别自身生成的文本、预测自己的行为、分辨提示来自内部评估还是实际应用，以及遵循依赖自我认知的指令。  我们对16种LLMs在SAD上的性能进行了评估，包括基础（预训练）模型和聊天模型。尽管所有模型的表现都优于随机猜测，但最高分的模型（Claude 3 Opus）在某些任务上仍远未达到人类水平。此外，我们发现SAD的表现与通用知识指标（如MMLU）的相关性并不完全一致。聊天模型，经过针对性训练以作为AI助手，相对于基础模型在SAD上的表现更好，但在通用知识任务上则不然。SAD的目标是通过分解成可量化的能力，促进科学界对LLMs情境意识的理解。情境意识对于增强模型的自主规划和行动能力至关重要，这既有利于自动化，也带来了与AI安全和控制相关的全新风险。您可以在<https://situational-awareness-dataset.org>获取代码和最新结果。|
|**2024-07-05**|**ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models**|Yuzhe Gu et.al.|[2407.04693](http://arxiv.org/abs/2407.04693)|**[link](https://github.com/open-compass/anah)**|## 任务  大型语言模型（LLMs）在跨领域和广泛应用的长格式问答任务中会出现幻觉。当前的幻觉检测和缓解数据集在领域覆盖和规模上存在局限，由于劳动成本高昂且现有幻觉标注员的可靠性不足，难以实现规模化。为了推动对LLMs幻觉的可扩展监督，本文提出了一种迭代的自我训练框架。该框架通过期望最大化（EM）算法，每次迭代首先使用一个幻觉标注流程来标记扩大的数据集，然后用这个更准确的标注器对数据集进行训练。在下一轮迭代中，使用新的标注器更新幻觉标注流程。实验结果全面展示，最终得到的仅需7亿参数的幻觉标注器超越了GPT-4的表现，并在HaluEval和HalluQA上的零样本推理中取得了最新的幻觉检测效果。这种标注器不仅能够评估不同LLMs在大规模数据集上的幻觉程度，还能通过NLI指标提升（从25%提高到37%）来帮助减轻生成文本的幻觉问题。|
|**2024-07-05**|**Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge**|Yuanze Lin et.al.|[2407.04681](http://arxiv.org/abs/2407.04681)|null|近年来，大规模多模态语言模型（MLLM）在使用大型高质量的图像文本数据集进行训练后，在整体理解图像方面取得了显著进步。然而，文本形式固有的困难限制了它们处理需要精细或空间密集信息（如遮罩）的问题，这影响了它们对详细视觉元素的理解能力。受到检索增强生成（RAG）理念的启发，本文提出了一种新的视觉提示方法，旨在将来自专门视觉模型（如实例分割和OCR模型）的精细外部知识融入MLLM。这是一个有前景但尚未充分探索的方向，可以提升MLLM的表现。我们的方法区别于同时期的工作，它们将外部知识转化为额外的文本提示，迫使模型间接学习视觉内容与文本坐标之间的对应关系。相反，我们提议将精细知识信息直接嵌入到一个空间嵌入图中作为视觉提示。这种设计可以轻松地整合进各种MLLM，如LLaVA和Mipha，显著提高它们的视觉理解性能。通过严谨的实验，我们在九个基准测试中展示了我们的方法如何提升MLLM的整体性能，增强其对细粒度上下文感知的能力。|
|**2024-07-05**|**Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition**|Ye Bai et.al.|[2407.04675](http://arxiv.org/abs/2407.04675)|null|现代自动语音识别（ASR）模型需要准确转录来自不同领域、语言和口音的多样语音信号，同时考虑到特定上下文信息，以适应各种应用场景的需求。传统的端到端模型结合额外的语言模型表现出色，但在数据匹配场景中效果良好，但逐渐面临瓶颈。本文介绍了一种基于大型语言模型（LLM）的新型语音识别模型——Seed-ASR。它建立在音频条件化LLM（AcLLM）架构之上，通过将连续语音表示和上下文信息输入到LLM中，利用了LLM的强大功能。通过分阶段的大规模训练以及在LLM中激发上下文感知能力，Seed-ASR在包括多个领域、方言和语言的综合评估集上显著优于端到端模型。此外，Seed-ASR能够部署到各种场景中支持特定需求，无需额外的语言模型。与最近发布的大型ASR模型相比，Seed-ASR在中文和英文公开测试集上的词（或字符，针对中文）错误率降低了10%-40%，进一步证明了其强大的性能。|
|**2024-07-05**|**Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models with Adaptive Expert Placement**|Yongji Wu et.al.|[2407.04656](http://arxiv.org/abs/2407.04656)|null|随着大型语言模型（LLMs）的规模不断扩大，稀疏激活的混合专家（MoE）架构因其计算成本的亚线性扩展而被越来越多地采用。然而，频繁的训练失败仍然是一个重大挑战，因为单次失败可能导致所有GPU陷入闲置，直至问题解决，从而可能丢失大量训练进度，需要从检查点重新开始。现有的高效容错训练解决方案要么缺乏弹性，要么依赖于将恢复能力构建到管道并行性中，但这不适用于MoE模型，因为MoE架构采用了专家并行策略。  我们提出了Lazarus，一个针对MoE模型进行容错和弹性的训练系统。Lazarus通过动态分配专家副本来应对专家工作负载的固有不平衡，从而加速训练，并开发了一种理论上最优的专家放置算法，以最大限度地提高在失败后的恢复概率。通过自适应的专家放置和灵活的令牌分发器，Lazarus能够在故障后充分利用所有可用节点，避免GPU空闲。  我们的评估表明，与现有MoE训练系统相比，Lazarus在频繁的节点故障下性能提升高达5.7倍，且在真实spot实例跟踪上提升了3.4倍。|
|**2024-07-05**|**Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework**|Reza Averly et.al.|[2407.04629](http://arxiv.org/abs/2407.04629)|null|该论文关注的是临床命名实体识别（Clinical NER），这是一种从临床病历中提取重要实体的任务。近年来，大型语言模型（LLMs）在这一任务上表现出色。研究主要集中在专有的LLMs，但论文探讨了开放的、专门为命名实体识别训练的LLMs在临床NER中的性能。作者提出了一种新颖的框架，称为“实体分解与过滤”（Entity Decomposition with Filtering，EDF），目的是通过将实体识别任务分解为子实体类型的检索，并引入一个过滤机制来消除错误实体。实验结果表明，该框架在所有度量标准、模型、数据集和实体类型上都表现出有效性。分析显示，实体分解能够显著提高对先前未被捕捉到的实体的识别。此外，论文还提供了对框架的全面评估和深入的错误分析，以期为未来的研究提供方向。|
|**2024-07-05**|**On scalable oversight with weak LLMs judging strong LLMs**|Zachary Kenton et.al.|[2407.04622](http://arxiv.org/abs/2407.04622)|null|该论文探讨了可扩展的监督协议，目标是让人类能够有效监督超越人类级别的AI。研究主要聚焦在辩论、咨询和直接问答三种形式上，使用大型语言模型（LLMs）作为AI代理和法官角色，假设法官模型较弱。实验涵盖了广泛的任务异质性，扩展了先前仅关注信息不对称的单一提取式问答任务，增加了数学、编程、逻辑和多模态推理等领域的挑战。结果表明，在所有任务中，当咨询师随机被分配正确或错误答案时，辩论优于咨询。在存在信息不对称的提取式问答任务中，辩论优于直接问答，但在其他没有信息不对称的任务中，结果则不一。当AI被允许选择要论证的答案而非预先指定时，发现法官被错误答案说服的情况在辩论中减少。此外，更强的辩论者模型能提高法官的准确性，尽管提升程度略低于之前的研究。|
|**2024-07-05**|**Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions**|Shumaila Javaid et.al.|[2407.04581](http://arxiv.org/abs/2407.04581)|null|本文探讨了大型语言模型（LLMs）如何融入集成卫星、航空和地面网络（ISATN）的变革潜力，利用先进的人工智能（AI）和机器学习（ML）技术优化这些网络的连通性。首先概述了ISATN的当前架构，强调了LLMs在提升数据流、信号处理和网络管理方面的作用，以推动5G/6G通信技术的发展，通过高级预测算法和实时决策来增强性能。接着，深入分析了ISATN组件，探讨了如何有效地利用LLMs解决传统数据传输和处理中的瓶颈问题。  文章着重于ISATN的网络管理挑战，包括资源分配策略、流量路由以及在不断变化条件下确保无缝连接和最优性能的网络安全。同时，我们讨论了将LLMs整合到ISATN中所面临的技术挑战，如数据集成、扩展性问题、决策过程中的延迟，以及构建健壮且容错的系统设计。最后，研究指出了未来研究的关键方向，即如何充分利用LLM的优势，以提升网络可靠性、优化性能，实现一个真正全球互联且智能的网络体系。|
|**2024-07-05**|**VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models**|Hang Gao et.al.|[2407.04573](http://arxiv.org/abs/2407.04573)|null|在大型语言模型（LLMs）快速发展的背景下，向量检索算法对于满足相似度和多样性要求的语义查询至关重要。尽管Maximal Marginal Relevance（MMR）在涉及这两个需求的检索场景中被广泛应用，但其参数λ的变化会导致结果波动，使得向量空间中的优化路径变得模糊。此外，当前缺乏对相似性和多样性在检索过程中约束的坚实理论分析。本文提出了一种新方法，通过查询向量与求和向量之间的关系来刻画这两种约束。这种关系确保了相似性，同时要求求和向量中的各个向量以分散的方式与查询向量对齐，以满足多样性需求。  我们还提出了一个新的组合优化问题：从一组候选向量中选择 $k$ 个，使得它们的求和向量最大程度地与查询向量匹配。我们证明了这个问题是NP完全的，揭示了在向量检索中同时追求相似性和多样性的深刻困难，并为后续研究奠定了理论基础。此外，我们设计了一个名为Vectors Retrieval with Similarity and Diversity（VRSD）的启发式算法，它不仅具有明确的优化目标，无需预设参数，而且在时间复杂性上相对于MMR有所降低。实证验证表明，VRSD在各种数据集上显著优于MMR。|
|**2024-07-05**|**PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts**|Ana-Cristina Rogoz et.al.|[2407.04541](http://arxiv.org/abs/2407.04541)|**[link](https://github.com/ana-rogoz/poprero)**|**我们推出了PoPreRo，这是首个专为罗马尼亚Reddit帖子的流行度预测收集的dataset。PoPreRo汇集了五个不同罗马尼亚子论坛的多样化帖子样本，总计包含28,107条数据。随数据集一同发布的，我们还提供了一系列竞争性模型作为未来研究的基础。值得注意的是，测试集上得分最高的模型达到了61.35%的准确率和60.60%的宏F1分数，这表明在PoPreRo上的流行度预测任务极具挑战性。通过少量提示对Falcon-7B大型语言模型的进一步探究也指向了同样的结论。因此，我们相信PoPreRo是一个有价值的资源，可以用来评估罗马尼亚社交媒体帖子的流行度预测模型。我们的数据集已公开发布在https://github.com/ana-rogoz/PoPreRo。**|
|**2024-07-03**|**Universal Length Generalization with Turing Programs**|Kaiying Hou et.al.|[2407.03310](http://arxiv.org/abs/2407.03310)|null|**摘要：**  长度泛化指的是从简短的训练序列推断出长测试序列的能力，这对于当前的大语言模型是一个挑战。尽管先前的研究提出了一些架构或数据格式变化来实现长度泛化，但这些方法通常局限于特定任务。在此基础上，我们结合了擦除板和链式思考（Chain-of-Thought, CoT）技术，提出了Turing程序，这是一种新颖的CoT策略，它将算法性任务分解成类似图灵机计算的步骤。这个框架既通用又简单，只需要在上下文中稍作修改地复制文本。我们展示了使用Turing程序，我们在加法、乘法以及基于上下文的SGD等算法性任务上实现了稳健的长度泛化。接着，我们展示Transformer在随机Turing程序上也能实现长度泛化，这表明对于任何算法性任务，长度泛化都是可能的。最后，我们理论证明Transformer能够实现Turing程序，构造了一个简单的RASP（Weiss等人）程序，它模拟任意图灵机。|
|**2024-07-03**|**Large Language Models for JSON Schema Discovery**|Michael J. Mior et.al.|[2407.03286](http://arxiv.org/abs/2407.03286)|null|## 背景 半结构化数据格式如JSON因其在存储数据时的灵活性而被广泛应用。然而，JSON数据通常缺乏与关系数据库中的表单结构相对应的规范（schema）。因此，出现了许多从数据集中发现规范的工具。尽管这些工具很有用，但现有的方法主要关注文档的语法，而忽视了语义信息。本研究中，我们探讨如何自动为发现的规范添加有意义的语义信息，使其类似于人类作者编写的规范中所包含的信息。我们利用大型语言模型和人工编写的JSON Schema文档库，生成元素的自然语言描述、可重用定义的有意义名称，并识别出哪些发现的属性最有用，哪些可以视为“噪声”。我们的方法在先前已证明与人类判断高度相关的文本生成指标上表现出色。|
|**2024-07-03**|**LLM Internal States Reveal Hallucination Risk Faced With a Query**|Ziwei Ji et.al.|[2407.03282](http://arxiv.org/abs/2407.03282)|**[link](https://github.com/ziweiji/Internal_States_Reveal_Hallucination)**|## 背景  大型语言模型（LLMs）的幻觉问题严重制约了它们的可靠性和可信度。人类具有自我意识过程，能识别面对查询时的未知领域。为此，我们的论文研究了LLMs能否在生成响应之前自行评估其幻觉风险。我们从训练数据源和15个不同自然语言生成（NLG）任务的角度广泛分析LLMs的内部机制，这些任务涵盖了超过700个数据集。实证分析揭示了两个关键发现：(1) LLM的内部状态能够指示它们是否在训练数据中见过查询；(2) LLM的内部状态显示出它们对查询可能产生幻觉或不产生幻觉的风险。我们的研究关注特定的神经元、激活层和令牌，这些在LLM对不确定性和幻觉风险的认识中扮演着关键角色。通过一种探查估计算法，我们利用LLM的自我评估能力，在运行时实现了平均84.32%的幻觉估计准确率。|
|**2024-07-03**|**Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning**|Zhili Shen et.al.|[2407.03227](http://arxiv.org/abs/2407.03227)|null|我们从大型语言模型的角度探讨文本到SQL的语义解析。鉴于商业数据库模式的规模挑战和业务智能解决方案的部署问题，我们提出了一种方法，它动态获取输入数据库信息，并利用抽象语法树选择少量示例进行上下文学习。此外，我们研究了如何利用并行语义解析器生成SQL查询的近似版本，以支持我们的检索。我们甚至将这种方法推向极致，采用不到5亿参数的模型作为高效近似器，并赋予其并行处理模式的能力。我们在单语和跨语言的语义解析基准上应用了我们的方法，结果优于现有最佳基线。全面的实验揭示了这种检索增强生成设置中各个模块的贡献，为未来工作指明了有趣的方向。|
|**2024-07-03**|**How Does Quantization Affect Multilingual LLMs?**|Kelly Marchisio et.al.|[2407.03211](http://arxiv.org/abs/2407.03211)|null|## 背景 量化技术在提升大语言模型（LLM）的推理速度和部署效率方面被广泛应用。尽管有大量的研究关注了量化后的英语任务模型效果，但尚无研究针对多语言场景。我们对量化多语言LLM进行了深入分析，重点关注其跨语言性能及不同规模下的表现。我们采用自动基准测试、LLM作为评判者的方法以及人类评估，发现以下几点：(1) 量化对人类评价的影响是负面的，且自动指标严重低估了这种损害：自动任务中平均1.7%的性能下降对应人类评估中日本任务的16.0%显著下滑；(2) 不同语言受到量化的影响程度不均，非拉丁字母体系的语言受影响最严重；(3) 比如数学推理这类挑战性任务，其性能下降最为显著。随着低功耗模型服务于全球NLP技术的普及变得至关重要，我们的研究结果强调了在评估高效模型时，多语言性能应作为关键指标。|
|**2024-07-03**|**TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts**|Ruida Wang et.al.|[2407.03203](http://arxiv.org/abs/2407.03203)|**[link](https://github.com/RickySkywalker/TheoremLlama)**|**### 翻译  在数学证明的计算机可验证形式语言（如Lean）验证中，使用大型语言模型（LLMs）基于自然语言（NL）的证明方法具有重要影响。然而，由于NL与形式语言（FL）的证明数据稀缺，现代LLMs在生成完整证明方面的性能欠佳。为此，本文提出了一种名为**TheoremLlama**的端到端框架，旨在训练通用LLM成为Lean4专家。该框架包括NL-FL对齐数据集生成方法、LLM形式定理证明器的训练策略以及LLM在撰写Lean4证明中的技术。  关键创新在于我们开发了NL-FL自举方法，即将NL证明融入Lean4代码，利用LLMs的自然语言推理能力进行正式推理。通过这种数据集生成方式，我们提供了**Open Bootstrapped Theorems**（OBT），一个对齐且自举的NL-FL数据集。**TheoremLlama**框架在MiniF2F-Valid和Test数据集上的累计准确率分别达到36.48%和33.61%，超过了GPT-4的基线分数22.95%和25.41%。我们已公开了模型检查点和生成的数据集，并即将全部代码开源。**|
|**2024-07-03**|**Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models**|Haritz Puerto et.al.|[2407.03181](http://arxiv.org/abs/2407.03181)|**[link](https://github.com/ukplab/arxiv2024-divergent-cot)**|**该研究提出了一种新颖的方法，称为Divergent CoT（DCoT），通过要求模型在单次推理步骤中比较多个推理链来进一步提升性能。这种方法发现，即使在小型、更易于获取的大型语言模型上进行指令调优也能提高表现。通过广泛的实验，涉及不同类型的推理任务，研究发现对DCoT数据集的微调在各种规模的模型（从13亿到70亿参数）上普遍优于基本的CoT方法。实验和人工评估表明，这些性能提升源于模型在单次推理中生成了多个不同的推理路径，这表明语言模型能够实现自我纠正。相关代码和数据已在https://github.com/UKPLab/arxiv2024-divergent-cot上公开。**|
|**2024-07-03**|**Investigating Decoder-only Large Language Models for Speech-to-text Translation**|Chao-Wei Huang et.al.|[2407.03169](http://arxiv.org/abs/2407.03169)|null|## 背景  大型语言模型（LLMs）因其出色的推理能力、泛化能力和跨领域的流畅性，在提升语音相关任务方面展现出巨大潜力。本文关注的是如何将解码器仅有的LLMs整合到语音转文本翻译（Speech-to-Text Translation，S2TT）任务中。我们提出一种架构，让LLM直接处理编码的语音表示并生成文本翻译。同时，我们研究了不同参数高效微调技术和任务表述方式的影响。在不使用专有数据的情况下，我们的模型在CoVoST 2和FLEURS基准上实现了最先进的性能。我们还进行了深入分析，验证了我们设计选择的合理性，并为LLMs与S2TT任务的融合提供了见解。|
|**2024-07-03**|**SOS! Soft Prompt Attack Against Open-Source Large Language Models**|Ziqing Yang et.al.|[2407.03160](http://arxiv.org/abs/2407.03160)|null|## 背景  开源的大规模语言模型（LLMs）在公众和行业中的受欢迎程度日益提升，因为它们可定制、微调且免费使用。然而，一些开源LLMs在使用前需要审批，这促使第三方发布易于获取的版本，甚至对这些模型进行微调或量化优化，以降低计算需求。这些便捷版本对用户颇具吸引力，但也增加了训练时间攻击的风险，威胁到LLMs的完整性和安全性。本文提出一种新的训练时间攻击方法SOS，它设计得计算需求低，无需干净数据或调整模型权重，保持模型的可用性。SOS针对各种场景下的安全问题，包括后门攻击、破解攻击和提示窃取攻击。实验结果表明，该攻击在所有评估目标上均有效。此外，我们还展示了SOS技术的另一面——版权令牌：这是一种新颖的方法，允许用户标记其版权内容，防止模型使用。|
|**2024-07-03**|**Let the Code LLM Edit Itself When You Edit the Code**|Zhenyu He et.al.|[2407.03157](http://arxiv.org/abs/2407.03157)|null|在本研究中，我们探讨了代码生成中的常见场景：开发者实时编辑现有代码，并请求大型语言模型（如大语言模型）进行即时重预测下一个token或行。直接的方法是让LLM重新编码整个键值缓存以提供精确的预测，但这个过程计算成本高，特别是当序列长度很长时。仅编码编辑后的子序列并将其整合到原始键值缓存中会遇到时间混淆问题，导致性能大幅下降。为此，我们提出了一种解决方案——\textbf{位置完整性编码}（Positional Integrity Encoding，简称PIE）。PIE基于旋转型位置编码，首先移除引入时间混淆的旋转型矩阵，然后重新应用正确的矩阵，确保了令牌之间的位置关系正确，仅需一轮矩阵乘法即可完成。我们在RepoBench-C-8k数据集上，使用13亿、67亿和330亿参数的DeepSeek-Coder模型进行了广泛实验，涵盖了代码插入、代码删除和多位置代码编辑等三个实际编程任务。实验结果表明，与标准的完整重计算方法相比，PIE在所有模型规模和任务中都能减少超过85%的计算开销，同时保持了良好的性能近似。|
|**2024-07-02**|**MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention**|Huiqiang Jiang et.al.|[2407.02490](http://arxiv.org/abs/2407.02490)|**[link](https://github.com/microsoft/MInference)**|**由于大型语言模型（LLMs）的计算挑战，尤其是随着提示长度的增长，其广泛应用面临障碍。由于注意力计算的二次复杂性，80亿参数的LLM在单个A100 GPU上处理100万个令牌（即预填充阶段）需要30分钟。现有的加速预填充方法往往在面对长序列LLMs时难以保持既高效又准确。为此，我们提出了MInference（百万令牌推理），这是一种旨在提升长序列处理预填充阶段速度的稀疏计算方法。我们发现了注意力矩阵中的三种独特模式：A形、垂直斜线和块稀疏，这些模式可利用GPU进行高效的稀疏计算。我们在离线阶段确定每个注意力头的最佳模式，并在推理过程中动态构建稀疏索引。通过优化的GPU内核，我们实现了基于指定模式的稀疏注意力计算，显著减少了长序列LLMs预填充阶段的延迟。我们的方法无需修改预训练设置或额外微调即可直接应用于现有LLMs。我们在包括InfiniteBench、RULER、PG-19和Needle In A Haystack在内的各种下游任务以及LLaMA-3-1M、GLM4-1M、Yi-200K、Phi-3-128K和Qwen2-128K等模型上的实验表明，MInference在A100上有效降低了预填充的推理延迟高达10倍，同时保持了准确性。我们的代码已开源，地址为：https://aka.ms/MInference。**|
|**2024-07-02**|**Neurocache: Efficient Vector Retrieval for Long-range Language Modeling**|Ali Safaya et.al.|[2407.02486](http://arxiv.org/abs/2407.02486)|**[link](https://github.com/alisafaya/neurocache)**|**这篇论文介绍了一种名为Neurocache的方法，用于扩展大型语言模型（LLMs）的有效上下文范围，通过外部向量缓存存储其过去的模型状态。与近期的向量检索方法类似，Neurocache利用高效的k近邻(kNN)算法检索相关的历史状态，并将其融入注意力过程。Neurocache在改进现有方法方面有以下几点：(1) 存储压缩的状态，减小了缓存大小；(2) 每个令牌执行一次检索操作，提高了推理速度；(3) 将检索窗口扩展到邻近状态，提升了语言建模和下游任务的准确性。  实验结果表明，无论从头开始训练还是对预训练模型（如Llama2-7B和Mistral-7B）进行增强，Neurocache都能有效。我们还对比了Neurocache与其他文本检索方法，在单文档问答和少量样本学习任务中展示了其优势。源代码已在以下链接公开：https://github.com/alisafaya/neurocache。**|
|**2024-07-02**|**RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs**|Yue Yu et.al.|[2407.02485](http://arxiv.org/abs/2407.02485)|null|该研究提出了一种新颖的指令调优框架RankRAG，旨在针对检索增强生成（RAG）中的上下文排名和答案生成双重任务对大型语言模型进行调优。通过在训练过程中加入少量排名数据，指令调优的单个语言模型表现出令人惊讶的效果，超越了专门使用大量排名数据进行单独调优的现有专家排名模型。实验中，我们与包括GPT-4-0613、GPT-4-turbo-2024-0409和开放源代码的最先进的RAG性能模型ChatQA-1.5在内的多个强baseline进行了比较。具体来说，我们的Llama3-RankRAG在九个知识密集型基准上显著优于Llama3-ChatQA-1.5和GPT-4系列模型。此外，它还在无需针对生物医学领域数据进行指令调优的情况下，在五个生物医学领域的RAG基准上与GPT-4模型表现相当，这显示了其在新领域中的出色泛化能力。|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483](http://arxiv.org/abs/2407.02483)|**[link](https://github.com/Wangyixinxin/MMedAgent)**|尽管多模态大型语言模型（MLLMs）已经取得了成功，但它们的泛化能力仍然有限，在某些情况下不如专业模型。近期，研究人员开发了基于LLMs的代理，通过用户输入选择合适的专用模型来解决这些问题。然而，在医疗领域，这类进展的应用还不广泛。为了弥补这一空白，本文首次提出了一种专为医疗设计的代理，名为\textbf{M}ulti-modal \textbf{Med}ical \textbf{Agent}（MMedAgent）。我们构建了一个指令调优数据集，包含了六个医疗工具，用于解决七项任务，使代理能针对特定任务选择最适宜的工具。实验全面展示了MMedAgent在各种医疗任务上超越了开源方法，甚至包括封闭源模型GPT-4o，且在引入和整合新医疗工具方面表现出高效性。|
|**2024-07-02**|**Understanding Alignment in Multimodal LLMs: A Comprehensive Study**|Elmira Amirloo et.al.|[2407.02477](http://arxiv.org/abs/2407.02477)|null|随着大型语言模型（LLMs）性能的提升，偏好一致性已成为一个重要因素，但在多模态大型语言模型（MLLMs）中的应用相对较少。这些模型在图像理解任务中也会遇到诸如错误陈述和内容不一致（即幻觉）的问题。MLLMs的偏好对齐目标是使模型的回答更贴近图像信息。近期的研究已经引入了针对MLLM的偏好数据集，并尝试了直接偏好优化（DPO）和proximal policy optimization（PPO）等不同的对齐方法。然而，由于数据集、基础模型类型和对齐策略的差异，哪种方法对性能提升的贡献最大尚不清楚。  本文独立分析了MLLM偏好对齐的各个方面。我们将对齐算法分为离线（如DPO）和在线（如在线-DPO）两类，并表明在某些情况下结合这两种方法可以提高模型性能。我们还回顾了各种已发表的多模态偏好数据集，探讨了它们构建细节对模型性能的影响。基于这些发现，我们提出了一种新的多模态偏好数据生成方法——偏见驱动的幻觉采样（Bias-Driven Hallucination Sampling，BDHS），这种方法无需额外标注或外部模型，且在多个基准上展现出与之前发表的对齐工作相当的竞争性能。|
|**2024-07-02**|**Open Scene Graphs for Open World Object-Goal Navigation**|Joel Loo et.al.|[2407.02473](http://arxiv.org/abs/2407.02473)|null|如何构建能够在开放世界中执行语义导航任务的机器人，比如在新场景中寻找目标物体？尽管基础模型具备处理这类任务所需的丰富知识和泛化能力，但需要一种合适的场景表示来将它们整合到完整的机器人系统中。为此，我们提出了开放场景图（Open Scene Graphs，OSG），这是一种拓扑语义表示，用于保留和组织开放集中场景信息，且结构可适应不同环境类型。我们将基础模型和OSG整合到OpenSearch系统中，该系统专为开放世界的对象目标导航设计，能够理解自然语言指令并在多变环境中零样本泛化，寻找未见过的物体。我们的OSG增强了与大型语言模型（LLMs）的推理能力，使得OpenSearch在物体目标导航任务上表现出色，超越了现有的LLM方法。通过模拟实验和真实世界测试，我们验证了OpenSearch在各种环境、机器人和新颖指令下的泛化能力。|
|**2024-07-02**|**Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I**|Harrie Oosterhuis et.al.|[2407.02464](http://arxiv.org/abs/2407.02464)|null|传统的信息检索（IR）系统评估通常成本高昂，因为需要人工专家进行相关性标注。近年来，生成式人工智能，尤其是大型语言模型（LLMs），能够以相对较低的计算成本大规模生成相关性注释，可能减轻IR评估的传统成本，并使其适用于众多资源匮乏的应用场景。然而，生成的注释并非无误，直接用于评估可能导致结果不可靠。为此，本研究提出两种方法，分别是基于预测驱动的推断和规范风险控制，利用计算机生成的相关性注释为IR评估指标提供可靠的置信区间（CIs）。  我们的方法需要少量可靠的注释，通过统计分析生成注释中的错误，从而为评估指标设置CIs，具有坚实的理论基础。与现有方法不同，我们特别设计的规范风险控制方法适用于排名评估，并且可以根据查询和文档自适应调整CIs。实验结果显示，我们的置信区间准确捕捉了基于LLM注释的评估中的变异性和偏差，优于传统的Bootstrap估计。我们期望这些贡献能为那些传统上难以实现可靠评估的众多IR应用带来革新。|
|**2024-07-03**|**Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs**|Jinmin Li et.al.|[2407.02411](http://arxiv.org/abs/2407.02411)|null|随着视频驱动的大型语言模型（LLMs）的兴起，视频理解能力得到了显著提升，但同时也引发了数据保护方面的担忧，因为视频更容易被无授权地标注。为此，本文提出了一种名为“Video Watermarking”的创新方法，旨在保护视频免受未经授权的视频LLMs，特别是针对内容和描述的处理。通过在关键帧中嵌入难以察觉的水印，我们利用多模态流损失保持观看体验的同时，防止视频被滥用。大量的实验表明，Video Watermarking显著降低了视频在各种视频LLMs中的可理解性，证明了其隐秘性和鲁棒性。总的来说，我们的方法为确保视频内容的安全、完整性和保密性提供了一种解决方案，以应对不断发展的视频LLMs技术。|
|**2024-07-02**|**CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models**|Song Wang et.al.|[2407.02408](http://arxiv.org/abs/2407.02408)|null|随着大型语言模型（LLMs）被越来越多地应用于各种自然语言处理任务，对其生成内容可能产生的负面社会影响的担忧也随之增加。为了评估LLMs的偏见，研究人员已经提出了一系列数据集。然而，现有的偏见评估工作往往只关注某种类型的偏见，并使用不一致的评价指标，这导致不同数据集和LLM之间的比较困难。为此，我们收集了多种用于评估LLM偏见的数据集，并进一步提出了CEB（Compositional Evaluation Benchmark），它涵盖了不同社会群体和社会任务中的各种类型偏见。CEB的构建基于我们新提出的构成性分类体系，从三个维度对每个数据集进行刻画：偏见类型、社会群体和任务。通过结合这三个维度，我们开发出一种全面的LLM偏见评估策略。实验结果表明，这些偏见在各维度上的程度有所不同，从而为针对特定偏见的缓解方法的发展提供了指导。|
|**2024-07-02**|**Assessing the Code Clone Detection Capability of Large Language Models**|Zixian Zhang et.al.|[2407.02402](http://arxiv.org/abs/2407.02402)|null|该研究旨在评估两种先进的大型语言模型（LLMs），GPT-3.5和GPT-4，在代码克隆检测任务中的性能。实验通过在两个数据集上测试模型：BigCloneBench（人类创建）和GPTCloneBench（LLM生成）。研究发现，GPT-4在所有类型的代码克隆检测中都明显优于GPT-3.5。结果显示，GPT模型的准确度与其识别代码克隆的能力与代码相似度之间存在关联，但它们在识别最复杂的Type-4代码克隆时效果较低。此外，GPT模型在检测LLM生成的代码中的代码克隆表现优于人类生成的代码，但整体准确性仍不显著。这些发现强调了进一步提升LLM在代码克隆识别能力的必要性，特别是针对自我生成代码克隆的问题，随着软件工程师越来越多地使用基于LLM的代码生成和重构工具，这可能会成为一个问题。|
|**2024-06-28**|**Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs**|Sukmin Yun et.al.|[2406.20098](http://arxiv.org/abs/2406.20098)|**[link](https://github.com/mbzuai-llm/web2code)**|**多模态大型语言模型（MLLMs）在图像、视频和音频等多种模态的处理任务上表现出色。然而，它们在理解和生成网页截图以及相应的HTML代码方面的能力相对较弱。为解决这个问题，我们提出Web2Code，这是一个包括大规模网页到代码的新基准，用于指令调优，并评估MLLM在网页理解及HTML代码转换能力上的表现。我们构建数据集时，利用预训练的LLMs增强现有的网页到代码数据集，并生成多样化的网页图片，以供渲染。输入是网页图片和说明，输出是网页的HTML代码，同时加入关于网页内容的丰富自然语言问答对，以促进对网页内容的全面理解。为了评估模型在这类任务中的性能，我们开发了一个测试框架，用于测试MLLM在网页理解与网页到代码生成方面的技能。实验结果表明，我们的数据集不仅有益于我们提出的任务，还在视觉领域的一般性能上有所提升，而先前的数据集会导致性能下降。我们期望这项工作能推动通用MLLM的发展，使其适用于网络内容生成和自动化任务。我们的数据和代码将在<https://github.com/MBZUAI-LLM/web2code>上公开。**|
|**2024-06-28**|**LLaRA: Supercharging Robot Learning Data for Vision-Language Policy**|Xiang Li et.al.|[2406.20095](http://arxiv.org/abs/2406.20095)|**[link](https://github.com/lostxine/llara)**|**该论文介绍了一种名为LLaRA（大型语言和机器人助手）的框架，它将机器人行动策略转化为对话形式，通过结合额外的数据辅助学习，提升响应质量。利用具备视觉输入的大型语言模型（VLMs），即视觉语言模型，这些模型能够处理状态信息，作为视觉-文本提示，并生成最优的机器人决策策略。首先，论文提出了一种自动化方法，从现有的行为克隆数据中生成多样且高质量的机器人指令数据集。然后，使用这种定制的对话式格式对VLM进行训练，使其能够生成有意义的机器人行动策略。实验结果表明，LLaRA框架在多个模拟和真实世界环境中展现出最先进的性能。相关代码、数据集和预训练模型已在<https://github.com/LostXine/LLaRA>提供。**|
|**2024-06-28**|**Scaling Synthetic Data Creation with 1,000,000,000 Personas**|Xin Chan et.al.|[2406.20094](http://arxiv.org/abs/2406.20094)|**[link](https://github.com/tencent-ailab/persona-hub)**|我们提出了一种新颖的基于人格的数据合成方法，该方法利用大型语言模型（LLM）内的多种视角来生成多样化的人工合成数据。为了在大规模上充分利用这种方法，我们引入了Persona Hub，这是一个从网络数据自动整理出的一亿个多元化人格的集合，相当于全球人口的约13%。这些人格作为分布式世界知识载体，几乎可以调用LLM内包含的各类观点，从而推动大规模、多样化的合成数据创建，适用于各种场景。通过展示Persona Hub如何在大规模生成高质量的数学和逻辑推理问题、指令（用户提示）、富含知识的文本、游戏NPC和工具（函数）等方面的应用，我们证明了基于人格的数据合成具有多样性、可扩展性、灵活性和易用性，可能引领合成数据创造和实际应用的新范式，对LLM的研究和发展产生深远影响。|
|**2024-06-28**|**LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression**|Jieneng Chen et.al.|[2406.20092](http://arxiv.org/abs/2406.20092)|**[link](https://github.com/beckschen/llavolta)**|**尽管在大型语言模型（LLMs）的文本嵌入压缩方面取得了显著进步，但大型多模态模型（LMMs）中的视觉令牌压缩仍然被忽视。本文研究了视觉令牌的冗余性以及在这些模型中的有效训练。初步实验表明，在测试阶段通过简单平均池化消除高达70%的视觉令牌，GQA基准的视觉问答准确率仅下降3%，这显示出视觉上下文中存在大量冗余。为解决这个问题，我们提出了Visual Context Compressor，它在训练阶段减少视觉令牌数量，以提高效率而不会影响性能。为了在压缩视觉令牌时尽量减少信息损失并保持训练效率，我们开发了轻量级训练方案LLaVolta。LLaVolta采用分阶段的视觉上下文压缩策略，从重度到轻度逐渐压缩，最终在训练结束时完全不进行压缩，从而在测试时不会丢失任何信息。广泛的实验表明，我们的方法提升了多模态模型在图像-语言和视频-语言理解任务上的性能，并显著降低了训练成本。代码已在https://github.com/Beckschen/LLaVolta上开源。**|
|**2024-06-28**|**ProgressGym: Alignment with a Millennium of Moral Progress**|Tianyi Qiu et.al.|[2406.20087](http://arxiv.org/abs/2406.20087)|**[link](https://github.com/pku-alignment/progressgym)**|随着前沿人工智能系统，特别是大型语言模型（LLMs）在知识论中的影响力日益增强，它们可能强化社会普遍的价值观，进而加剧错误道德观念的固化，导致广泛的社会问题持续存在。为应对这一潜在风险，我们提出进步对齐作为一种技术解决方案。进步对齐算法旨在学习人类道德进步的机制，从而弥补现有对齐方法对当代道德盲点的敏感性。为了推动进步对齐的研究，我们开发了ProgressGym，一个实验性框架，它从历史中学习道德进步的规律，以促进现实世界道德决策的未来发展。借助9个世纪的历史文本和18个历史LLMs，ProgressGym将现实生活中的进步对齐挑战转化为具体的基准。我们定义了三个核心挑战：追踪演变的价值（PG-Follow）、预测道德进步（PG-Predict）以及调节人与AI价值变迁之间的反馈循环（PG-Coevolve）。这些任务需要时间维度的方法，而传统的对齐策略无法胜任。  为此，我们展示了终身学习和外推算法作为进步对齐的基本方法，并建立了一个开放的排行榜，邀请创新算法和新挑战。该框架和排行榜分别可在https://github.com/PKU-Alignment/ProgressGym 和 https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard 获取。|
|**2024-06-28**|**Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language**|Yicheng Chen et.al.|[2406.20085](http://arxiv.org/abs/2406.20085)|null|基于扩散模型的生成方法已经在生成各种布局的高质量图像方面展现出巨大潜力，这对于下游感知任务具有显著益处。然而，仅依赖语言描述和一个合适的多实例评估指标来实现全自动布局生成并未得到充分探索。本文提出了一种新颖的框架——Auto Cherry-Picker（ACP），旨在自动生成高质量的多模态训练样本，以增强感知和多模态训练效果。通过输入自然语言概念列表，我们引导大型语言模型（LLMs）生成详细的描述并设计合理的布局。然后，使用文本到图像模型生成多个图片。接着，我们采用精心设计的评估指标对生成的数据进行精炼，确保质量。特别是，我们提出了复合布局与图像评分（Composite Layout and Image Score，CLIS）这一新指标，用于公正地评估生成的图像。我们的合成高质示例在定制初始概念列表时，能够有效提升各种场景下的性能，尤其是在处理长尾分布和不平衡数据集的问题上。下游任务的实验结果显示，ACP显著提高了现有模型的表现。此外，我们深入研究了CLIS与下游任务性能提升之间的关联，发现CLIS分数越高，性能越好。这表明评估指标在视觉感知和多模态大型语言模型任务中可能发挥关键作用。我们将提供代码。|
|**2024-06-28**|**Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification**|Anisha Gunjal et.al.|[2406.20079](http://arxiv.org/abs/2406.20079)|**[link](https://github.com/anisha2102/molecular_facts)**|**随着大型语言模型（LLM）生成内容的自动事实核查变得越来越普遍，以应对错误叙述的问题，研究的一个关键焦点在于核查的粒度：较大的文本段落难以核查，而更原子化的事实（如命题）可能缺乏正确的上下文解读。本文探讨了在这些原子事实中上下文的作用。我们认为完全原子的事实并非最佳表示形式，为此我们提出了分子事实的两个标准：去情境化（decontextuality），即它们能否独立存在，以及最小化（minimality），即添加多少额外信息才能实现去情境化。我们量化了去情境化对最小化的影响，并提出了一种基础方法来自动生成分子事实，目标是在保持准确性的同时提供适量的信息。我们将这种方法与不同的去情境化策略进行了比较，发现分子事实能够在模糊场景中平衡最小化和事实核查的准确性。**|
|**2024-07-01**|**BMW Agents -- A Framework For Task Automation Through Multi-Agent Collaboration**|Noel Crawford et.al.|[2406.20041](http://arxiv.org/abs/2406.20041)|null|自主代理驱动的大规模语言模型（LLMs）展示了巨大的自动化潜力。早期的展示表明，这些代理能够解决复杂任务，与外部系统交互以增强知识，并触发行动。特别是，多个代理协作解决复杂任务的工作流证明了它们在不那么严格和定义不明确的环境中操作的能力。因此，多代理方法有巨大的潜力成为众多工业应用的核心，从复杂的知识检索系统到下一代机器人过程自动化。鉴于当前LLMs的推理能力，处理复杂流程需要分步骤的方法，包括设计明确且模块化的任务计划。根据复杂程度，这些任务可以由单个代理或一组代理执行。本研究专注于构建一个灵活的代理工程框架，重点关注规划和执行，旨在应对不同领域的复杂应用场景。该框架为工业应用提供可靠性，并提出确保可扩展、灵活且协作的工作流程技术，让多个自主代理协同解决问题。|
|**2024-06-28**|**LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models**|Renzhi Wang et.al.|[2406.20030](http://arxiv.org/abs/2406.20030)|null|## 背景  大型语言模型（LLMs）为了跟上不断变化的世界知识，需要持续进行模型更新，这催生了终生模型编辑任务。近年来，尽管已经开发出多种单次和批量编辑的技术，但它们在面对终生编辑时要么无法应用，要么效果不佳。本文中，我们提出LEMoE，一个专为终生模型编辑设计的混合专家（MoE）适配器。首先，我们分析了影响传统MoE适配器在终生编辑中有效性的因素，包括灾难性遗忘、路由不一致性和顺序敏感性。基于这些洞察，我们提出了一种定制的模块插入方法，引入了新颖的键值对锚定路由以增强训练和推理阶段的路由一致性，同时采用了一个简洁而有效的聚类基编辑顺序规划。实验结果表明，我们的方法在终生编辑任务中表现出色，超越了先前的模型编辑技术，同时保持了批量编辑任务中的优秀性能。我们的代码将开源。|
|**2024-06-28**|**ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models**|Yuxiang Zhang et.al.|[2406.20015](http://arxiv.org/abs/2406.20015)|**[link](https://github.com/toolbehonest/toolbehonest)**|**随着工具增强的大型语言模型（LLMs）迅速融入实际应用，社区亟需全面了解这些模型中的幻觉问题。为此，我们提出了一项全面的诊断基准——ToolBH。我们从深度和广度两个维度进行评估：在深度上，设计了多级诊断流程，包括（1）可解性检测、（2）解决方案规划和（3）缺失工具分析；在广度上，考虑了工具集特征下的三种场景：缺少必要工具、潜在工具和功能有限的工具。我们构建了七个任务，并通过多次人工标注收集了700份评估样本。结果显示，当前先进的模型Gemini-1.5-Pro和GPT-4o在这项基准上的总得分为45.3和37.0，满分100分。在工具增强的LLM场景中，更大的模型参数并不一定意味着更好的性能，训练数据和回复策略同样关键。我们的诊断分析指出，模型错误的主要原因在于任务可解性的判断。开放源码模型在冗长回复时性能下降，而专有模型在长链推理方面表现更优。**|
|**2024-06-27**|**ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos**|Jr-Jen Chen et.al.|[2406.19392](http://arxiv.org/abs/2406.19392)|**[link](https://github.com/rextime/rextime)**|**我们提出了一项名为ReXTime的基准测试，专门针对人工智能模型在视频事件中的时间推理能力进行严谨评估。ReXTime关注的是跨时间推理，即理解当问题及其相应的答案出现在不同的视频片段时的人类式理解。这种需要深入理解视频片段之间因果关系的时间推理能力对前沿的多模态大型语言模型构成了重大挑战。为了支持这种评价，我们开发了一个自动化管道，用于生成时间推理的问答对，大大减少了繁琐的手动标注需求。我们的基准包括921个精心筛选的验证样本和2,143个测试样本，每个样本都经过人工精心挑选以确保准确性和相关性。评估结果显示，尽管前沿大型语言模型在学术模型上表现突出，但它们与人类的表现仍存在显著的14.3%的精度差距。此外，我们的管道无需人工创建了一个包含9,695个机器生成样本的训练数据集，实证研究表明，这可以通过微调来提升跨时间推理能力。**|
|**2024-06-27**|**The Remarkable Robustness of LLMs: Stages of Inference?**|Vedang Lad et.al.|[2406.19384](http://arxiv.org/abs/2406.19384)|**[link](https://github.com/vdlad/remarkable-robustness-of-llms)**|**我们通过删除和交换相邻层来展示并研究大型语言模型的惊人鲁棒性。实验结果显示，在不进行微调的情况下，这些干预措施仍能保留原始模型72%至95%的预测精度，而且模型层数越多，表现出更高的鲁棒性。根据逐层干预实验和其他实验，我们提出了一个假设：存在四种通用的推理阶段，跨越八种不同的模型：解码器阶段，将原始令牌表示提升为更高级的上下文表示；特征工程阶段，迭代优化任务和实体特定特征；然后是模型的半部分，随着专门组件的作用，隐藏表示与词汇空间的对齐进入一个相变阶段；最后，最后一层通过消除对预测造成干扰的过时特征，精细化后续的令牌分布。**|
|**2024-06-27**|**The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models**|Xiliang Zhu et.al.|[2406.19358](http://arxiv.org/abs/2406.19358)|null|### 概述  情感分析在自然语言处理（NLP）中扮演着核心角色。XLM-R和mT5等多语言预训练模型的兴起推动了跨语言情感分析的关注度提升。近期大型语言模型（LLM）的出现极大地推动了通用NLP任务的发展，但这些模型在跨语言情感分析方面的性能尚未充分探讨。本研究通过实证分析，比较了公共小型多语言模型（SMLM）如XLM-R与以英语为中心的LLM（如Llama-3）在英语、西班牙语、法语和中文的情感分析中的零样本和少量样本迁移能力。结果显示，就公开模型而言，SMLM在零样本跨语言设置中表现出更好的性能。然而，在少量样本情况下，公开LLM显示出更强的适应性。此外，我们发现专有的GPT-3.5和GPT-4在零样本跨语言能力上领先，但在少量样本场景下，它们被公开模型超越。|
|**2024-06-27**|**DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions**|Nigel Fernandez et.al.|[2406.19356](http://arxiv.org/abs/2406.19356)|**[link](https://github.com/umass-ml4ed/divert)**|## 背景  高质量的干扰项对于选择题（尤其是数学选择题）的评估和教学价值至关重要。然而，手工设计能够反映学生实际知识缺陷或误解的干扰项是一项艰巨的任务。尽管大型语言模型（LLM）如GPT-4在生成干扰项方面有所助益，但数学这类学科的处理仍然具有挑战性。因此，我们提出了一种新的方法，旨在理解和生成解释性的错误表示，以生成数学选择题的干扰项。本文介绍DiVERT（基于文本的变异误差生成器），这是一种利用7亿参数开源LLM的变分方法，它在真实世界数学选择题数据集（包含1,434个问题，被数十万学生使用）上的实验表明，相较于最先进的GPT-4方法，DiVERT在干扰项生成方面表现出色。此外，我们还进行了与数学教育者的同行评审，结果表明DiVERT生成的错误标签质量接近人类编写的。  ## 任务  请将上述英文论文摘要翻译成中文，输出不应包含除摘要内容外的任何其他内容，且确保不出现","字符。|
|**2024-06-27**|**IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language**|Lucky Susanto et.al.|[2406.19349](http://arxiv.org/abs/2406.19349)|null|## 翻译  针对网络仇恨言论对社会和谐的严峻威胁，特别是在印尼这类国家，近年来仇恨言论在线比率增长了十倍，迫切需要有效的检测机制。然而，由于缺乏充足的标记数据，尤其是针对印尼文本的，这一进展受到了阻碍。边缘化群体，如什叶派、LGBTQ等少数群体，面临的挑战更大，因为仇恨言论报告不足，现有的检测工具对其理解有限。此外，当前数据集对主观性的处理不足，加剧了问题。为了应对这些问题，我们提出IndoToxic2024，这是一个全面的印尼仇恨言论和毒性分类数据集，包含43,692条记录，由19名多元化的个体进行标注，特别关注选举期间针对国内弱势群体（如总统选举中的特定群体）的文本。我们使用BERT模型（IndoBERTweet）进行了微调，为七种二元分类任务设定了基准，取得了0.78的宏F1分数。同时，我们展示了如何将人口统计信息融入其中，提升大型语言模型gpt-3.5-turbo在零样本情况下的性能。然而，我们也警告，过度依赖人口统计信息可能导致细化模型性能下降，因为这会导致数据碎片化。|
|**2024-06-27**|**Jump Starting Bandits with LLM-Generated Prior Knowledge**|Parand A. Alamdari et.al.|[2406.19317](http://arxiv.org/abs/2406.19317)|**[link](https://github.com/BorealisAI/jump-starting-bandits)**|我们提供了有力的证据，展示了将大型语言模型（LLMs）与上下文化多臂老虎机框架相结合的优势。上下文化老虎机在推荐系统中广泛应用，用于根据用户特定的上下文生成个性化建议。我们表明，经过大规模语料库训练，富含人类知识和偏好的LLMs能够很好地模拟人类行为，从而通过启动上下文化多臂老虎机来减少在线学习的遗憾（regret）。我们提出了一种初始化算法，通过提示LLMs生成接近人类偏好的预训练数据集，供老虎机学习使用。这显著降低了在线学习的遗憾和数据收集成本。我们的方法通过两组实验验证，包括使用LLMs作为占卜者（oracle）的实验和基于联合调查实验数据的真实世界实验。|
|**2024-06-27**|**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data**|Zheyang Xiong et.al.|[2406.19292](http://arxiv.org/abs/2406.19292)|**[link](https://github.com/edixiong/artificial-needles)**|近期的研究指出，大型语言模型（LLMs）在处理长文本输入时在信息检索和推理能力上存在困难。为解决这个问题，我们提出了一种利用精心设计的合成数据集进行微调的方法，该数据集包含数值型键值对检索任务。我们在GPT-3.5 Turbo和Mistral 7B等模型上的实验显示，对这些模型进行这种数据集的微调显著提高了它们在长文本环境中的信息检索和推理能力。我们分析了微调后的模型，发现它们在从合成任务迁移到实际评估（如在20文档MDQA中的位置10处提升10.5%）方面的表现有所提升。此外，我们还发现，经过我们合成数据集微调的LLMs在通用基准上的性能保持稳定，而使用其他基于长文本增强数据集微调的LLMs可能会导致错误增加（例如，在TriviaQA上，Mistral 7B在我们的合成数据上微调无明显性能下降，而其他基线数据可能导致性能下降，范围在2.33%到6.19%之间）。本研究突显了通过合成数据微调来提升LLMs在长文本任务性能的潜力。|
|**2024-06-27**|**PhysioLLM: Supporting Personalized Health Insights with Wearables and Large Language Models**|Cathy Mengying Fang et.al.|[2406.19283](http://arxiv.org/abs/2406.19283)|null|我们介绍了一种名为PhysioLLM的互动系统，它利用大型语言模型（LLMs）结合可穿戴设备的生理数据和上下文信息，提供个性化的健康理解和探索。与商业健康应用不同，PhysioLLM具备全面的统计分析功能，能发现用户数据中的关联和趋势。用户可以用自然语言提问，获取生成的个性化洞察，并根据这些信息制定行动目标。以改善睡眠质量为例，因为其可通过生理数据量化且对整体健康至关重要。通过一项涉及24名Fitbit智能手表用户的用户研究，我们证明了PhysioLLM在促进对健康数据的深入个性化理解，以及支持实现个人健康目标方面，优于Fitbit应用和通用LLM聊天机器人。|
|**2024-06-27**|**HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**|Junying Chen et.al.|[2406.19280](http://arxiv.org/abs/2406.19280)|**[link](https://github.com/freedomintelligence/huatuogpt-vision)**|**随着大型多模态语言模型（如GPT-4V）的迅速发展，它们在医学多模态能力方面取得了显著进步。然而，由于医学影像-文本数据的数量和质量受限于数据隐私问题和高昂的标注成本，这些模型仍面临挑战。早期的研究尝试利用PubMed的大型去标识化医疗图像-文本对来缓解这些问题，但它们仍受到数据噪音的影响。为解决这一问题，我们优化了PubMed中的医疗图像-文本对，并利用GPT-4V在“非盲”模式下进行数据清洗和格式转换，创建了PubMedVision数据集，包含130万份医学视觉问答样本。我们的验证表明：（1）PubMedVision显著提升了当前多模态语言模型在医学领域的性能，在诸如MMMU Health & Medicine track等基准测试中表现出显著改善；（2）医学专家的手动检查和实证结果证实了我们的数据集在数据质量上优于其他构建方法。利用PubMedVision，我们训练了一个名为HuatuoGPT-Vision的340亿参数的医学多模态语言模型，它在公开源多模态语言模型中表现出色，在医学多模态场景中显示出优越性能。**|
|**2024-06-27**|**AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning**|Praneeth Vadlapati et.al.|[2406.19271](http://arxiv.org/abs/2406.19271)|**[link](https://github.com/Pro-GenAI/AutoPureData)**|**人们对最新的和可靠的大型语言模型（LLMs）的需求持续增长。通常，LLMs是基于固定的数据集训练然后部署的。然而，训练数据会随着时间逐渐过时。研究关注如何利用网络数据自动更新AI模型，但这一过程涉及数据质量与安全的顾虑，如偏见、垃圾信息等。确保数据纯净对于生成可靠的模型至关重要。在不纯数据上训练可能导致不良结果。该研究提出了一种系统，它收集网络数据，并借助现有可信的AI模型自动筛选出不需要的内容。实验中，我们收集并处理了一小部分网络数据，验证了该系统的数据净化效果。**|
|**2024-06-26**|**Symbolic Learning Enables Self-Evolving Agents**|Wangchunshu Zhou et.al.|[2406.18532](http://arxiv.org/abs/2406.18532)|**[link](https://github.com/aiwaves-cn/agents)**|**人工智能界通过构建"语言代理"（即复杂的大型语言模型管道）来探寻通用人工智能（AGI）的道路，这些模型结合了提示技术和工具使用方法。尽管它们在众多实际任务中表现出色，但当前语言代理研究的一个关键局限是其模型中心或工程导向：提示、工具和管道的改进依赖于大量的人工专家设计，而非自动从数据学习。我们认为，从模型中心向数据中心转变——让语言代理能够自主学习和适应环境，是它们迈向AGI的关键。为此，我们提出了"代理符号学习"框架，这是一个系统性的方法，它使语言代理能够在数据驱动的方式下自我优化，利用符号优化器。我们将代理视为具有可学习权重的符号网络，这些权重由提示、工具及其组合方式定义。代理符号学习旨在模仿连接主义学习中的两个基本算法：反向传播和梯度下降，但它处理的是自然语言形式的权重、损失和梯度。我们在标准基准和复杂现实任务上进行了概念验证实验，结果表明，代理符号学习使得语言代理在创建和部署后能够自我更新，实现了"自我进化的代理"。**|
|**2024-06-26**|**PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation**|Christoph Leiter et.al.|[2406.18528](http://arxiv.org/abs/2406.18528)|**[link](https://github.com/gringham/prexme)**|## 翻译  大型语言模型（LLMs）在自然语言处理领域带来了革命性变化，它们的上下文学习能力使其成为自然语言生成评价的有力工具，特别适用于资源匮乏和时间限制的场景。本文提出PrExMe，一项大规模的提示探索度量法，我们在机器翻译（MT）和摘要任务上评估了超过720种开源LLM作为度量标准的模板，总计约660万次评估。这项详尽的比较（1）为近期开源LLMs作为评价指标的表现设定了基准；（2）探讨了不同提示策略的稳定性和变异性。我们发现，一方面，存在一些情况下提示表现稳定：有些LLMs表现出特有的偏好，倾向于使用文本标签来评分，而另一些则倾向于返回数值分数。另一方面，提示的稳定性和模型排名可能受到看似微不足道的更改的影响。例如，将输出格式从“0到100”改为“-1到+1”可能会显著改变我们的评估结果。我们的研究有助于理解不同提示方法对MT和摘要评价中LLM-based度量的影响，揭示了最稳定的提示模式，并指出了潜在局限性。|
|**2024-06-26**|**CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs**|Zirui Wang et.al.|[2406.18521](http://arxiv.org/abs/2406.18521)|**[link](https://github.com/princeton-nlp/CharXiv)**|在实际应用多模态大型语言模型（Multimodal Large Language Models，MLLMs）处理科学论文或财务报告等任务时，图表理解至关重要。然而，现有的数据集往往集中在简化和同质化的图表上，以及基于模板的问题，这可能导致性能评估过于乐观。我们发现，尽管开源模型在现有基准上可能表现优于强大的专有模型，但通过简单的压力测试，如改变图表或问题，性能会下降高达34.5%。为此，我们提出CharXiv，这是一个包含2,323个来自arXiv论文的自然、复杂且多样化的图表的全面评估套件。CharXiv包括两类问题：1）描述性问题，用于检查基本图表元素；2）推理问题，需要综合分析图表中的复杂视觉元素。所有图表和问题都由专家精心挑选、整理和验证以保证质量。结果显示，最强专有模型（例如GPT-4o，准确率为47.1%）与最强开源模型（如InternVL Chat V1.5，准确率为29.2%）之间存在显著差距，而所有模型的表现均远低于人类的80.5%水平，这揭示了现有MLLM在图表理解能力上的不足。我们希望CharXiv能推动未来的研究，通过提供更真实、更具代表性的进步衡量标准，促进图表理解领域的研究。项目页面和排行榜可访问：https://charxiv.github.io/。|
|**2024-06-26**|**"Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline**|Grace Li et.al.|[2406.18512](http://arxiv.org/abs/2406.18512)|null|### 概述  解释是知识共享的核心，它建立在沟通原理、社会动态和学习理论之上。我们专注于对话式的解释方法，因为其环境高度适应性和交互性。我们的研究利用了解释行为框架，这是一个理解解释者和被解释者在对话中如何运用策略进行解释、理解和互动的工具。我们利用Wachsmuth等人构建的WIRED YouTube系列数据集，并由Booshehri等人进行了带有解释行为的标注，这些注释为我们理解对话中解释者如何构建回应提供了依据。  随着去年生成式人工智能的发展，我们期望更好地理解大型语言模型（LLMs）的能力，以及它们如何增强专家解释者的对话交流能力。为此，我们使用了Booshehri等人2023年标注的5-Levels数据集来评估LLMs在解释性对话中的表现。为了评价LLMs生成解释者回应的有效性，我们设计了三种策略：人类解释者的原始回应、GPT4的标准回应以及加入了解释步骤的GPT4回应。我们邀请人类标注者对这三种策略进行评估。|
|**2024-06-26**|**Mental Modeling of Reinforcement Learning Agents by Language Models**|Wenhao Lu et.al.|[2406.18505](http://arxiv.org/abs/2406.18505)|null|## 背景 尽管现代语言模型已经展现出一定的推理能力，理论上能够表达任意可能的令牌分布，但它们如何利用预训练时积累的世界知识来理解物理世界中的代理行为，这一方面仍未得到充分探索。本研究首次实证考察大型语言模型（LLMs）在通过推理分析代理的行为及其对状态的影响，从而构建代理心理模型（agent mental modeling）的能力。这可能揭示出利用LLMs解析强化学习（RL）代理行为的潜力，这对于可解释强化学习（XRL）的关键挑战具有重要意义。为此，我们提出特定的评估指标，并在不同复杂度的RL任务数据集上进行测试，报告关于代理心理模型建立的研究结果。结果显示，当前的LLMs还无法仅通过推理完全实现代理的心理建模，这需要进一步创新。因此，这项工作提供了对现代LLMs能力和局限性的新见解。|
|**2024-06-26**|**Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming**|Zhenghao Zhou et.al.|[2406.18501](http://arxiv.org/abs/2406.18501)|null|这篇论文探讨了大型语言模型（LLMs）的内插学习（in-context learning，ICL）能力，并将其与基于梯度的学习进行功能等效性诊断。研究者提出了一种新方法，利用逆频率效应（inverse frequency effect，IFE）来分析。IFE现象指的是在错误驱动的学习过程中，模型应对罕见样例产生的更新幅度大于常见样例。在心理学中，人类在结构化提示（如倾向于重复最近接触的句子结构）情境中表现出IFE，这表明其可能涉及错误驱动的学习机制。实验通过模拟结构化提示在ICL中的影响发现，LLMs同样显示出IFE，且这一效应在更大的模型中更为明显。因此，研究结果支持了ICL本质上是基于梯度的学习的假设，即在ICL的前向传播过程中隐含地计算了梯度。论文结论指出，人类和LLMs都使用了基于梯度的、错误驱动的处理机制。|
|**2024-06-26**|**Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation**|Ahmed Njifenjou et.al.|[2406.18460](http://arxiv.org/abs/2406.18460)|null|近年来，人们提出了一系列方法来创建能够进行开放领域对话的大型语言模型（LLMs）。这些模型能回答用户问题，但局限于单向问答形式，而非真正的对话。通常，通过针对特定数据集进行微调来调整它们的交流风格，但这既昂贵又限于少数语言。本研究探索了角色扮演的零样本提示作为提高开放领域对话效率和成本效益的解决方案，利用多语言能力强的训练有素模型（Beeching等人，2023年），这些模型能遵循指令。我们设计了一个提示系统，当与遵循指令的模型——这里使用Vicuna（Chiang等人，2023年）结合时，能够生成在法语中的对话代理，在两项任务中甚至超越了经过微调的模型，并在人类评估中表现出色。|
|**2024-06-26**|**Cascading Large Language Models for Salient Event Graph Generation**|Xingwei Tan et.al.|[2406.18449](http://arxiv.org/abs/2406.18449)|**[link](https://github.com/xingwei-warwick/callmsae)**|由于长文档中事件检测、关系识别以及非结构化输入与结构化图谱的整合等任务的复杂性，从文本生成事件图谱是一项挑战。当前的研究往往同等重视所有事件，未能区分对理解叙事至关重要的关键事件。本文提出CALLMSAE，一个基于CAscading大型语言模型（LLMs）的SAlient Event图谱生成框架，它利用LLMs的能力，并避免了昂贵的人工标注需求。首先，通过提示LLMs生成摘要，我们识别出重要事件。然后，我们开发了一种迭代的代码精炼提示策略，用于生成事件关系图，消除错误的关系并恢复缺失的边。对基于上下文的图谱生成模型进行 fine-tuning，在使用 LLM 生成的图谱上表现出色，优于使用 CAEVO 生成数据训练的模型。在人类标注的测试集上的实验结果显示，我们的方法能生成更突出且准确的图谱，超越了竞争性的基线。|
|**2024-06-26**|**New intelligent empowerment for digital transformation**|Peng Yifeng et.al.|[2406.18440](http://arxiv.org/abs/2406.18440)|null|这项研究提出了一种基于大型语言模型（LLMs）的创新评估方法，用于衡量企业的数字化转型（DT）过程。通过对2005年至2022年间在纽约证券交易所和纳斯达克上市的4407家公司的年度报告进行分析，构建了一套全面的DT指标。研究结果显示，DT显著提高了企业的财务表现。然而，不同的数字技术对财务性能的影响各不相同，区块链技术的积极影响相对较小。此外，研究还发现DT通过提升运营效率和降低成本促进财务绩效增长。本研究为学术界提供了新的DT评估工具，同时拓宽了生成人工智能技术在经济研究中的应用范围。|
|**2024-06-26**|**IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons**|Dan Shi et.al.|[2406.18406](http://arxiv.org/abs/2406.18406)|**[link](https://github.com/danshi777/ircan)**|人们普遍认为，大型语言模型（LLMs）在大规模数据训练后蕴含着丰富的知识。然而，近期研究揭示了LLMs生成文本时的知识冲突问题，即模型内编码的参数知识（即知识库）与上下文提供的新知识存在矛盾。为解决这一问题，我们提出了一种新颖框架——IRCAN（识别和重权上下文感知神经元）。IRCAN首先利用整合梯度计算得到的上下文感知归因分数，来识别那些对处理语境至关重要 的神经元。接着，通过重新赋权，我们强化这些识别出的上下文相关神经元，从而引导LLMs生成更符合上下文新知识的响应。我们在多种模型和任务上的广泛实验表明，IRCAN不仅显著提升了处理知识冲突的能力，还提供了一个可扩展的、即插即用的解决方案，能够无缝融入现有模型中。|
|**2024-06-25**|**MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning**|Xiangyu Zhao et.al.|[2406.17770](http://arxiv.org/abs/2406.17770)|**[link](https://github.com/phoenixz810/mg-llava)**|**## 背景  多模态大型语言模型（MLLMs）在视觉理解任务上取得了显著进步。然而，大多数模型局限于处理低分辨率图像，这限制了它们在需要详细视觉信息的感知任务中的表现。在我们的研究中，我们提出了一种创新的MLLM——MG-LLaVA，通过引入多尺度视觉流，包括低分辨率、高分辨率和对象级特征，来增强模型的视觉处理能力。我们设计了一个额外的高分辨率视觉编码器，以捕捉精细细节，并通过卷积门融合网络与基础视觉特征融合。为了进一步提升模型的对象识别能力，我们结合了来自离线检测器确定的边界框的物体级别特征。MG-LLaVA仅使用公开可用的多模态数据进行指令调优，展现出卓越的感知能力。我们用不同规模的语言编码器（从38亿到340亿参数）实例化MG-LLaVA，以全面评估其性能。多项基准测试的结果表明，MG-LLaVA在同类参数量的现有MLLM中表现出色，证明了其出色的效率。代码将在https://github.com/PhoenixZ810/MG-LLaVA上开源。**|
|**2024-06-25**|**BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning**|Ercong Nie et.al.|[2406.17764](http://arxiv.org/abs/2406.17764)|null|## 背景 大型语言模型（LLMs）积累了丰富的参数知识，但由于重新训练成本高昂且对闭源模型不可行，更新这些知识变得困难。知识编辑（KE）作为一种可能的解决方案，允许在不损害整体性能的前提下更新LLMs的知识。基于“上下文学习”（ICL）的即席KE方法展现出巨大潜力，使得LLMs能够作为黑盒处理。过去，KE主要集中在英语环境，而当前以英语为中心的LLMs在跨语言KE方面的潜力尚未充分挖掘。为了推动这方面的更多研究，我们推出了BMIKE-53基准，该基准针对53种不同语言的三种KE任务类型进行评估。我们还提出了一种无梯度的KE方法——多语言上下文知识编辑（MIKE），并在BMIKE-53上进行了实验。我们的评估关注跨语言知识转移的可靠性、泛化性、局部性和可移植性，为未来跨语言KE的研究提供了有价值的观点和框架。我们的代码和数据已通过匿名仓库https://anonymous.4open.science/r/MIKE公开获取。|
|**2024-06-25**|**CaLMQA: Exploring culturally specific long-form question answering across 23 languages**|Shane Arora et.al.|[2406.17761](http://arxiv.org/abs/2406.17761)|**[link](https://github.com/2015aroras/calmqa)**|**## 背景  大型语言模型（LLMs）在长篇问答任务中广泛应用，它们需生成段落级别的答案来回应复杂问题。尽管英语的长篇问答研究已相当深入，涉及多种数据集和评估指标，但其他语言的研究却相对匮乏。为了弥补这一差距，我们推出了CaLMQA，一个包含2,600个跨23种语言的复杂问题集合，其中包括资源有限、鲜少研究的语言，如斐济语和基林迪语。我们的数据集既包括社区网络论坛上收集的自然出现的问题，也包含了由母语使用者撰写的题目，我们为此专门聘请了他们。这个过程产生了多样且复杂的题目，反映了文化主题（如传统、法律、新闻），以及母语使用者的语言习惯。  我们对一系列开源和闭源模型进行了自动评估，使用了我们新提出的CaLMScore指标，该指标能检测答案中的语言错误和重复词。结果显示，对于某些低资源语言，LLM生成的答案质量明显下降。我们在部分模型的人工评估中发现，对于具有文化特性的问题，模型表现显著低于文化中立的问题。这些发现强调了对LLM多语言能力及非英语长篇问答评价领域更深入研究的必要性。**|
|**2024-06-25**|**Accelerating Clinical Evidence Synthesis with Large Language Models**|Zifeng Wang et.al.|[2406.17755](http://arxiv.org/abs/2406.17755)|null|人工智能自动医学发现是许多人的梦想。为此，我们开发了一种名为TrialMind的生成式AI管道，旨在进行医学系统性回顾，涵盖研究搜索、筛选和数据提取阶段。该系统利用大型语言模型（LLMs）驱动每个环节，并引入专家监督以减少错误。为了评估性能，我们创建了TrialReviewBench基准数据集，它是一个定制的包含870份来自25篇元分析论文的临床研究标注数据，涵盖不同医疗治疗领域。结果显示，TrialMind显著提升了文献审查效率，在从超过2000万篇PubMed研究中检索相关研究时，召回率高达0.897至1.000。在筛选阶段，我们的方法优于基于传统语言模型嵌入的方法（召回率分别为0.227-0.246 vs. 0.000-0.102）。此外，我们的方法在结果提取方面超越了直接使用GPT-4的表现，准确率范围为0.65到0.84。我们还支持森林图中的临床证据综合，经八名人类标注员验证，他们普遍更偏好TrialMind，其在涉及的审查中胜出率为62.5%至100%。这些发现表明，基于LLM的临床证据合成方法，如TrialMind，能够促进可靠且高质量的临床证据合成，从而提升临床研究的效率。|
|**2024-06-25**|**Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language**|Amalie Brogaard Pauli et.al.|[2406.17753](http://arxiv.org/abs/2406.17753)|null|本文探讨了在面对大量试图影响我们的信息，如预告消息、辩论、带有政治色彩的新闻和宣传时，大型语言模型（LLMs）生成具有说服力文本的能力。不同于以往专注于特定领域或类型劝说的研究，我们进行了一项全面的分析，旨在测量和基准LLMs在被明确要求增强或减少说服力时，以及仅要求进行释义时产生说服性文本的程度。为此，我们创建了一个新的数据集——“Persuasive-Pairs”，包含一组由简短文本和LLM重写以放大或削弱说服力的文本对。我们对这些配对进行了多标注，按相对尺度评估其说服力。这个数据集不仅本身具有价值，还展示了如何使用它训练一个回归模型，预测文本对之间说服力的得分，从而能够对不同领域的LLMs进行评分和比较。最后，我们讨论了不同系统提示对LLaMA3产生的影响，值得注意的是，即使在仅要求释义的情况下，不同的“角色”提示也会显著改变文本中的说服力。这些发现强调了研究LLM生成文本中的说服语言的重要性。|
|**2024-06-25**|**LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users**|Elinor Poole-Dayan et.al.|[2406.17737](http://arxiv.org/abs/2406.17737)|null|在最新的大型语言模型（LLMs）展现出卓越性能的同时，关于它们的不可靠行为，如虚构和偏见的研究层出不穷。本研究探讨了LLMs的回答质量在信息准确性、真实性以及拒绝回答方面，如何随着三种用户特征的变化而变化：英语水平、教育程度和国籍。我们在三个最先进的LLMs和两个事实核查相关的数据集上进行了详尽实验，重点关注其真实性。研究结果表明，当前最先进的LLMs对英语能力较低、教育水平较低以及非美国籍用户的回答质量存在更明显的负面倾向，这使得这些模型对于其最弱势用户来说，并非可靠的信息来源。|
|**2024-06-25**|**FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model**|Feijie Wu et.al.|[2406.17706](http://arxiv.org/abs/2406.17706)|**[link](https://github.com/HarliWu/FedBiOT)**|大型语言模型（LLMs）在经过适当领域特定数据的微调后，在许多任务上展现出出色性能。然而，这类专用数据通常分布在多个所有者之间，这就提出了如何在联邦学习（FL）中进行LLM微调的问题。面对有限的计算和通信能力，FL客户端在有效微调大型语言模型时面临挑战。为此，我们介绍了FedBiOT，一种旨在提高资源效率的LLM微调FL方法。具体来说，我们的方法包括服务器生成一个压缩的LLM，并确保其性能与完整模型相当。然后，客户端针对这个压缩模型的一个轻量但重要的部分——适配器进行微调。值得注意的是，由于服务器无法访问客户端拥有的私人数据，服务器用于校准的数据分布与客户端用于微调的数据不同。我们将问题建模为一个带有数据不一致性影响的 bilevel 优化问题，并导出了服务器和客户端的更新规则。我们在 LLaMA-2 上进行了广泛实验，结果显示，适配器在重新整合到全局语言模型时表现出色。实验结果还表明，FedBiOT 相比现有基准显著减少了资源消耗，同时保持了相近的性能水平。|
|**2024-06-25**|**From Distributional to Overton Pluralism: Investigating Large Language Model Alignment**|Thom Lake et.al.|[2406.17692](http://arxiv.org/abs/2406.17692)|**[link](https://github.com/thomlake/investigating-alignment)**|**该研究分析了大型语言模型（LLMs）经过校准后输出分布的变化特性。首先，重新评估了之前关于校准后响应多样性降低的报告，发现这种下降主要归因于质量控制和信息整合。校准能够抑制不相关和无帮助的内容，同时使输出分布倾向于更长的、涵盖多个基础LLM响应信息的答案，实质上是将多样化信息汇总在单个响应中。研究并未发现校准显著减少有用信息，进而引出问题：校准模型是否会产生基础模型无法再现的信息？第二部分的研究结果表明，情况并非如此，校准模型的行为可以通过基础模型在无需微调的情况下进行复现。通过上下文示例和较低分辨率的语义提示，可以从基础LLMs引导出与校准后的相似响应，甚至与校准后的响应之间的相似度接近。这些发现支持“表面校准假设”，即当前的校准技术仅捕捉了助手型基础LLM行为中有用的部分，并未扩展其能力。此外，它们还显示，基于上下文的校准作为一种模仿校准LLMs的策略，效果出人意料地好，且无需微调。研究代码和数据可在<https://github.com/thomlake/investigating-alignment>获取。**|
|**2024-06-25**|**VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation**|Kun Qian et.al.|[2406.17681](http://arxiv.org/abs/2406.17681)|**[link](https://github.com/qbetterk/VarBench)**|随着大型语言模型在传统基准测试中的表现日益出色，越来越多的研究人员开始关注预训练期间的基准数据泄露问题，通常称为数据污染问题。为了确保公正的评估，最近的基准测试仅公开训练和验证集，对测试集标签保密。他们要求任何希望评估自己语言模型的人都需要提交模型的预测结果，进行集中处理，然后在排行榜上公布模型的得分。然而，这个提交过程既低效又妨碍了有效的错误分析。为解决这个问题，我们提出动态化基准测试并实时评估语言模型。具体来说，我们从每个测试案例中提取变量，并为每个变量定义一个值范围。每次评估时，我们会从这些值域中抽取新的值来创建独特的测试案例，从而保证每次都是全新的评估。  我们针对数学生成任务的GSM8K、多项选择任务的ARC、commonsense问答的CommonsenseQA以及TruthfulQA的真实性问答任务，应用了这种变量扰动方法。实验结果显示，这种方法能更准确地衡量语言模型的真实能力，有效缓解了数据污染问题。|
|**2024-06-25**|**Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models**|Yuan Li et.al.|[2406.17675](http://arxiv.org/abs/2406.17675)|null|大型语言模型（LLMs）展现出卓越的任务解决能力，日益扮演类似人类助手的角色。社会对将LLMs更广泛地融入其中产生了兴趣，探讨它们是否具备心理特质，以及这些特质是否稳定且有助于理解其行为。本文借鉴心理学测量学的方法，提出了一种框架，用于研究LLMs中的心理学，包括心理维度识别、评估数据集创建和结果验证。在此框架下，我们开发了一个全面的LLM心理测量基准，涵盖了六种心理维度：个性、价值观、情绪、心智理论、动机和智力。这个基准包含了十三个包含多样场景和题型的数据集。研究发现，LLMs展现出广泛的心理特性。同时，我们观察到LLMs在自我报告的特质与其实际行为之间的不一致。该论文详细展示了LLMs的心理测量评估，为AI和社会科学领域的可靠评估提供了洞见，以及可能的应用方向。|
|**2024-06-24**|**EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees**|Yuhui Li et.al.|[2406.16858](http://arxiv.org/abs/2406.16858)|**[link](https://github.com/safeailab/eagle)**|在现代大型语言模型（LLMs）的推理过程中，成本高且耗时。实验表明，投机取巧的抽样方法如EAGLE已证实有效。传统方法假设草稿树的接受率仅依赖于令牌的位置，然而我们发现这其实还取决于上下文。为此，我们在EAGLE的基础上提出了EAGLE-2，引入了一种新的上下文感知动态草稿树技术到起草建模中。这一改进利用了EAGLE的草稿模型校准良好的特性：草稿模型的信心分数能近似表示接受率，误差较小。我们在三个系列的LLMs和六个任务上进行了广泛评估，结果显示EAGLE-2的速度提升比率为3.05倍到4.26倍，比EAGLE-1快20%到40%。此外，EAGLE-2还能保持生成文本分布不变，因此是一个无损加速算法。|
|**2024-06-24**|**From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models**|Sean Welleck et.al.|[2406.16838](http://arxiv.org/abs/2406.16838)|null|现代研究中最引人注目的发现之一是，在大型语言模型（LLMs）的训练过程中增加计算资源会带来更好的性能。然而，对于推断时的优化方法的关注相对较少。这篇综述专门探讨了这些推断时间的方法。我们从统一的数学框架出发，考察了三个领域：逐词生成算法、元生成算法和高效生成。逐词生成算法，通常称为解码算法，通过一次抽样一个token或构建词级搜索空间，然后选择输出。这些方法通常假设能够访问语言模型的logits、下一个token分布或概率分数。元生成算法处理部分或完整序列，融入领域知识，支持回溯，并整合外部信息。高效生成方法旨在减少token成本，提高生成速度。我们的综述融合了来自传统自然语言处理、现代LLMs和机器学习系统三个研究社区的观点。|
|**2024-06-24**|**USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$ onversations**|Mounika Marreddy et.al.|[2406.16833](http://arxiv.org/abs/2406.16833)|null|在当前的背景下，识别用户在各种话题的长篇讨论中的观点和立场对于个性化、市场研究、政治竞选、客户服务、冲突解决、定向广告和内容管理至关重要。然而，手动标注数据以训练此类模型面临诸多挑战，如耗时昂贵、长对话可能引入噪声，以及用户观点转变的微妙之处可能导致解读困难。鉴于大型语言模型（LLMs）在复杂自然语言处理任务中的出色表现，本文尝试利用Mistral Large和GPT-4自动化两个关键任务的标注过程，并提供推理：一是用户立场分类，即在对话中对用户帖子的观点进行五级标注；二是用户固执程度分类，关注用户在整个对话中的总体意见，采用四级标注。通过在764个多用户Reddit对话上应用零样本、一示例和少量样例标注的多数投票，我们创建了USDC数据集。然后，我们使用这个数据集对多个小型部署语言模型进行微调和指令调整，用于执行五类立场和四类固执程度的分类任务。我们公开了代码和数据集：[https://anonymous.4open.science/r/USDC-0F7F]。|
|**2024-06-24**|**Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track**|Ronak Pradeep et.al.|[2406.16828](http://arxiv.org/abs/2406.16828)|**[link](https://github.com/castorini/ragnarok)**|## 背景  您可能体验过新的Bing搜索或Google AI概述？这些都反映出当前搜索引擎正逐步发展到基于检索增强生成（RAG）的系统。这类系统能整合实时数据到大型语言模型（LLMs），提供信息丰富、有来源且简洁的摘要，与传统的文档排名展示方式形成对比。因此，为了推动RAG系统评估的创新，我们提议在TREC 2024年增设RAG竞赛。本文详述了我们如何实现这一目标：描述了可复用框架Ragnar\"ok的设计，解释了MS MARCO V2.1语料库的选择，发布了竞赛开发话题，并标准化了用户接口定义，以便利用户。接下来，我们将利用Ragnar\"ok展示关键的工业基准，如OpenAI的GPT-4o和Cohere的Command R+。我们还推出了一个网页界面，用于互动式地比较不同RAG系统的性能，并通过众包方式进行评估。我们开源Ragnar\"ok框架和基准，旨在为未来的RAG系统建立统一的标准。|
|**2024-06-24**|**RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale**|Beck LaBash et.al.|[2406.16801](http://arxiv.org/abs/2406.16801)|**[link](https://github.com/qurrent-ai/res-q)**|**## 翻译  大型语言模型（LLMs）的指令跟随能力促使了一类能够处理复杂任务的系统发展，如对大型代码仓库进行编辑。鉴于LLMs对提示微调的高敏感性和不可预测性，迫切需要稳健的评估工具来推动这些系统的未来发展。我们提出RES-Q，一个针对 $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ ystems的自然语言指令基准，它基于100个真实的GitHub提交构建了100个仓库编辑任务。给定编辑指令和代码仓库，RES-Q评估LLM系统获取信息并构造满足指令要求的编辑的能力。我们认为，这种评估方式优于传统方法，能全面评估模型的性能。  我们使用Qurrent OS开发的语言代理软件构建了一个仓库编辑系统，对该系统中的各种最先进的LLMs，如Claude Sonnet 3.5和GPT-4o，进行了评估。尽管在HumanEval上的1%精确度@1得分有所差异，但在RES-Q上，Claude Sonnet 3.5的1%精确度@1得分比GPT-4o高出12%，这表明RES-Q具有区分模型能力的潜力，随着传统基准接近饱和，它能提供更深入的洞察。  我们还研究了token效率、与现有基准的性能关联，以及封闭源和开源LLM之间的有趣差异。相关代码和数据集可在https://github.com/Qurrent-AI/RES-Q获取。**|
|**2024-06-24**|**Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs**|Ashwinee Panda et.al.|[2406.16797](http://arxiv.org/abs/2406.16797)|**[link](https://github.com/kiddyboots216/lottery-ticket-adaptation)**|**## 背景 当前的大规模语言模型（LLMs）适应新任务的方法并不适用于多任务适应，因为它们会修改所有模型权重，导致不同任务之间产生破坏性的干扰。这可能导致对先前任务的遗忘，使得同时在多个任务上获得良好性能变得困难。为了解决这个问题，我们提出了Lottery Ticket Adaptation（LoTA），这是一种稀疏适应方法，它识别并优化模型中的一个稀疏子网络。我们在诸如指令跟随、推理、数学和摘要等复杂任务上评估了LoTA。  ## 方法 LoTA通过发现和优化“彩票券”（或稀疏任务向量）来实现，这种方法优于全量微调和低秩适应（LoRA）。LoTA不仅表现出更好的性能，还能在训练其他任务后保持良好的表现，从而避免了灾难性遗忘。此外，通过提取和针对特定任务进行微调，LoTA还支持在高度不同的任务间进行模型融合。  ## 结论 总的来说，LoTA作为一种有效的稀疏适应策略，为多任务大语言模型的适应提供了新的解决方案，能够在处理多个任务时保持稳定且高效的表现。**|
|**2024-06-24**|**M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models**|Rishabh Maheshwary et.al.|[2406.16783](http://arxiv.org/abs/2406.16783)|null|## 背景  在大型语言模型（LLMs）遵循指令的校准过程中，微调（finetuning, IFT）至关重要。近期已经提出了一些有效的IFT数据集，但大多集中在高资源语言如英语上。本研究中，我们创新性地提出一个全合成的、基于Evol分类法引导的多语言、多轮指令微调数据集——M2Lingual，目标是提升LLMs在多样语言和任务上的表现。M2Lingual共包含182,000个IFT对，源自不同种子，涵盖70种语言、17个NLP任务以及通用的指令-响应对。  ## 目的与贡献  使用M2Lingual进行训练的LLMs性能显著优于大多数现有的多语言IFT数据集。更重要的是，经M2Lingual微调的模型在各种评估基准上展现出稳健的跨语言能力，无论是在我们的多语言、多轮翻译评价基准上，还是在多种多样的多语言任务中。因此，我们贡献了Evol分类法的两步方法，并公开了M2Lingual的数据集：https://huggingface.co/datasets/ServiceNow-AI/M2Lingual。|
|**2024-06-24**|**It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension**|Sagi Shaier et.al.|[2406.16779](http://arxiv.org/abs/2406.16779)|null|过去十年，自然语言处理领域取得了显著进步。然而，一些实践未经充分评估就已确立。针对阅读理解这一情况，我们首先提出问题：1）输入顺序（即问题和上下文）如何影响模型性能？鉴于近期在输入侧重领域的进展，我们进一步探究：2）强调问题、上下文或两者是否能提升表现？我们在3个数据集上测试了9种大型语言模型，发现先呈现上下文再给出问题可以提高模型性能，最高可达31%的准确率提升。此外，强调上下文的效果优于突出显示问题，而且对模型缺乏参数知识来回答的问题，针对性地强调输入部分尤其有效。通过尝试基于提示和注意力的强调方法，我们发现最有效的策略出人意料地简单：只需在输入中附加几个标记，就能实现高达36%的准确性提升，使得小型模型能够超越其大得多的同类模型。|
|**2024-06-24**|**Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024**|Sai Koneru et.al.|[2406.16777](http://arxiv.org/abs/2406.16777)|null|## 背景  大型语言模型（LLMs）正在被广泛研究，以应用于诸如语音识别（ASR）、机器翻译（MT）甚至端到端语音翻译（ST）等任务。本文介绍KIT团队在受限+LLM赛道下的离线提交，我们通过整合最新技术改进了级联语音翻译系统。特别地，我们将Mistral-7B模型\footnote{mistralai/Mistral-7B-Instruct-v0.1}融入其中，从两个方面增强系统：一是利用我们的系统生成的N-best列表精炼ASR输出，通过微调LLM提高转录准确性；二是对MT输出进行文档级别的精炼，利用ASR和MT预测来提升翻译质量。结果显示，LLM的集成使得ASR的Word Error Rate下降了绝对0.3%，MT的COMET评分提高了0.65%。然而，在包含重叠说话者和背景噪音的挑战性测试集中，由于ASR性能不佳，LLM集成的效果不明显。为了改善在这种情况下可能缺失的上下文信息，我们采用了分块长形式解码的ASR方法。|
|**2024-06-24**|**WARP: On the Benefits of Weight Averaged Rewarded Policies**|Alexandre Ramé et.al.|[2406.16768](http://arxiv.org/abs/2406.16768)|null|### 翻译  强化学习从人类反馈（RLHF）通过训练奖励模型来调整大型语言模型（LLMs），使其生成的内容符合人类偏好。为了保持预训练知识，RLHF通常采用KL散度正则化，但这会限制奖励优化。为此，本文提出了一种新颖的对齐策略，称为权重平均奖励策略（WARP）。WARP在三个阶段在权重空间中融合策略：首先，它使用指数移动平均策略作为KL正则化的动态基准。其次，应用球面插值将独立微调的策略合并成一个增强模型。最后，线性插值在合并模型和初始模型之间进行，以恢复预训练特征。该过程迭代进行，每次迭代的最终模型用作下一轮的高级初始化，逐步优化KL与奖励之间的权衡，实现固定KL下的更高奖励。GEMMA策略的实验验证了WARP的优点，其质量和对齐性能优于开源的LLMs。|
|**2024-06-21**|**GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians**|Haoyang Liu et.al.|[2406.15341](http://arxiv.org/abs/2406.15341)|**[link](https://github.com/liu-hy/genotex)**|**## 翻译  近年来，机器学习的进步显著提升了从基因表达数据中识别疾病相关基因的能力。然而，这些过程往往需要深厚的专长和大量的人工努力，限制了其可扩展性。大型语言模型（LLMs）驱动的代理显示出在自动化此类任务方面的潜力，因为它们的问题解决能力日益增强。为了支持这类方法的评估和发展，我们创建了GenoTEX，这是一个基因表达数据分析自动探索的基准，包括数据集选择、预处理和统计分析任务。GenoTEX提供了全面的分析管道，其中包含了人类生物信息学家精心编写的注释，他们对数据集进行深入分析以确保准确性和可靠性。  为了提供这些任务的基线，我们设计了GenoAgents，这是一个基于LLMs的代理团队，具备上下文感知规划、迭代校正以及与领域专家咨询的能力，它们协作探索基因数据集。我们的实验显示了LLM驱动方法在基因组数据分析中的潜力，而错误分析指出了挑战和未来的改进方向。我们提议GenoTEX作为一个有前景的资源，用于衡量和提升人工智能驱动的基因组数据分析方法。我们的基准已公开发布在：\url{https://github.com/Liu-Hy/GenoTex}。**|
|**2024-06-21**|**Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance**|Haoling Li et.al.|[2406.15330](http://arxiv.org/abs/2406.15330)|null|大型语言模型（LLMs）已经在众多研究领域带来了革新。尽管人们普遍知道微调对于增强LLMs的功能至关重要，但现有研究表明，微调过程中可能存在参数冗余。因此，有研究建议只更新部分参数，但这未能有效利用任务特定信息来识别训练中的重要参数。考虑到梯度本质上蕴含着任务相关数据的信息，我们提出了梯度掩码调优（Gradient-Mask Tuning，GMT）方法，该方法根据参数的梯度信息选择性地进行训练更新。具体来说，我们计算梯度的绝对值，并对较小幅度的梯度应用掩码。我们的实验结果表明，GMT不仅优于传统的微调方法，还提升了LLM性能的上限。进一步分析显示，GMT对掩码比例具有一定的鲁棒性，并且在计算效率上与基本的微调（Simple Fine-Tuning，SFT）相当。|
|**2024-06-21**|**Bug In the Code Stack: Can LLMs Find Bugs in Large Python Code Stacks**|Hokyung Lee et.al.|[2406.15325](http://arxiv.org/abs/2406.15325)|**[link](https://github.com/hamminghq/bug-in-the-code-stack)**|近年来，针对针对于大型语言模型（LLMs）在海量文本文档中检索上下文信息的Needle-in-a-Haystack（NIAH）基准研究有所进展。随着LLMs在软件开发流程中的日益融合，评估它们在代码环境中的表现变得至关重要。随着LLMs朝着程序合成方向发展，必须确保它们能理解语法并编写出符合语法规则的代码。为此，我们设计了Bug In The Code Stack（BICS）基准测试，旨在检验LLMs识别简单语法错误的能力于大型源代码中。我们的研究发现三个关键点：（1）与文本环境相比，基于代码的环境对检索任务构成了更大的挑战；（2）不同模型之间的性能存在显著差异；（3）尽管如此，较长的上下文长度与性能下降之间存在关联，但这种下降程度在不同的模型间有所不同。|
|**2024-06-21**|**Towards Fine-Grained Citation Evaluation in Generated Text: A Comparative Analysis of Faithfulness Metrics**|Weijia Zhang et.al.|[2406.15264](http://arxiv.org/abs/2406.15264)|null|大型语言模型（LLMs）常常产生不可靠或难以验证的信息，即“幻觉”。为解决这个问题，检索增强的LLMs引入了引用，使内容基于可核查的来源。然而，手动评估引用是否充分支持相关陈述仍然是一个重大挑战。先前的研究试图通过信仰度指标自动估计引用的支持程度，但这些方法仅限于二分类，忽视了实际场景中对精细级别引用支持的考量。为了探究信仰度指标在精细级别评估中的有效性，我们提出了一种比较评估框架，用于检验这些指标在区分三种支持等级（全面、部分和无支持）之间的能力：全面支持、部分支持和不支持。我们的框架采用相关性分析、分类评估和检索评估，全方位衡量指标分数与人类判断的一致性。研究结果显示，没有单一指标在所有评估中表现出色，揭示了精细级别支持评估的复杂性。根据发现的结果，我们为开发更有效的指标提供了实用建议。|
|**2024-06-21**|**Detecting Synthetic Lyrics with Few-Shot Inference**|Yanis Labrak et.al.|[2406.15231](http://arxiv.org/abs/2406.15231)|null|近年来，生成的音乐内容逐渐受到关注，大型语言模型被有效应用于创作各种风格、主题和语言结构的歌词，这推动了艺术家们的创作，但也带来了版权侵犯、消费者满意度和内容滥发等问题。为此，检测生成歌词的方法变得至关重要。然而，现有的研究并未专注于这一特定领域或创意文本的机器生成内容检测。针对这一空白，我们精心构建了首个高质量合成歌词数据集，并对多种基于少量样本的检测方法进行了详尽的定量评估，测试它们的泛化能力，并辅以人类评价。结果显示，我们的最佳少数样本检测器——基于LLM2Vec的方法超越了在其他领域表现强劲的风格和统计方法，成功鉴别出人类创作与机器生成的歌词，且展现出良好的跨艺术家和模型泛化能力，还能有效识别生成后的人工润色。这项研究强调了在创意内容检测领域，特别是泛化能力和对更大歌曲库的适应性方面，需要进一步研究。所有数据集、预处理脚本和代码已公开在GitHub和Hugging Face上，遵循Apache 2.0许可协议。|
|**2024-06-21**|**A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation**|Irune Zubiaga et.al.|[2406.15227](http://arxiv.org/abs/2406.15227)|**[link](https://github.com/hitz-zentroa/cn-eval)**|随着网络上错误信息和有害言论的增多，迫切需要有效的反叙事（Counter Narrative，CN）生成技术。然而，现有的自动评估方法往往缺乏可解释性，无法准确反映生成的CN与人类感知之间的复杂关系。为此，本文提出了一种新颖的方法来评估生成的CN，即利用大型语言模型（Large Language Model，LLM）作为评估器。通过以锦标赛形式对生成的CN进行对战比较，我们建立了一个模型排名流程，其与人类偏好间的相关系数达到0.88。此外，我们还探讨了使用LLM进行零样本（Zero-Shot，ZS）CN生成的能力，对比分析了聊天、指令和基础模型的性能和局限性。通过细致的评估，包括微调实验，我们揭示了在特定领域数据下的响应差异。结论是，对于执行这项任务，如果能避免因安全顾虑而拒绝生成，聊天导向的ZS模型可能是最佳选择。|
|**2024-06-21**|**Unsupervised Extraction of Dialogue Policies from Conversations**|Makesh Narsimhan Sreedhar et.al.|[2406.15214](http://arxiv.org/abs/2406.15214)|null|## 翻译  对话策略在构建任务导向的对话系统中至关重要，但其开发和维护往往需要对话建模专家的大量投入。尽管在许多情况下，手头有大量的对话数据，但人们缺乏有效的方法从这些数据中提取对话策略。为此，本文通过展示大型语言模型（LLMs）如何在对话数据转化为统一的中间表示——规范形式的过程中发挥作用，填补了这一空白。接着，我们提出了一种新颖的利用可控且可解释的图基方法生成对话策略的技术。通过将对话中的规范形式整合成流程网络，我们发现运行图遍历算法有助于提取对话流程。相比仅依赖LLM提取的流程，这些流程更好地反映了底层交互。我们的方法旨在赋予对话设计者更大的控制力，提供一个提升对话策略开发效率的工具。|
|**2024-06-21**|**Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language Understanding**|Mohan Li et.al.|[2406.15209](http://arxiv.org/abs/2406.15209)|null|## 背景 零样本语音语言理解（SLU）使系统能够在无需先前训练数据的新领域理解用户话语。当前的研究往往依赖大型语言模型（LLMs），导致庞大的存储需求和复杂性。本文提出使用 Whisper，一个独立的语音处理模型，来进行零样本端到端（E2E）SLU。为处理未见过的语义标签，我们将SLU任务融入问答（QA）框架中，通过提示Whisper解码器进行语义推断。我们采用前缀调优方法高效地训练该系统，只优化少量参数，而不是整个Whisper模型。实验结果显示，我们的提议系统在SLURP上的槽位填充（SLU-F1）得分比最近引入的零样本基准提高了40.7%。此外，在既定和跨领域评估环境下，它与基于Whisper-GPT-2的模块化系统表现相当，但模型参数减少了34.8%。|
|**2024-06-21**|**Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms**|Santiago Berrezueta-Guzman et.al.|[2406.15198](http://arxiv.org/abs/2406.15198)|null|注意力缺陷多动障碍（ADHD）是一种神经发育障碍，其特征为注意力不集中、过度活跃和冲动，严重影响个体的日常生活和生活质量。职业疗法在ADHD管理中扮演着关键角色，通过培养日常生活所需的技能，提升个体在学校、家庭和社会环境中全面参与的能力。近期研究强调了大型语言模型（如ChatGPT和社交辅助机器人）在心理治疗中的潜在价值，以弥补现有疗法的局限，提供定制化的支持并适应个体的独特需求。然而，关于这些先进技术在ADHD疗法中的联合应用研究尚存在较大空白。因此，我们整合了ChatGPT-4 Turbo和Claude-3 Opus两个先进语言模型到一个机器人助理中，以考察它们在机器人辅助互动中的性能，并在一个模拟治疗场景中比较它们与临床验证的定制模型的效果。研究结果显示，ChatGPT-4 Turbo在性能和响应速度上表现出色，适合于时间敏感的应用。而Claude-3 Opus在理解、连贯性和伦理考量方面表现出优势，强调安全和吸引人的互动。两者都展现出创新和适应性，但ChatGPT-4 Turbo在集成简易度和语言支持方面更具优势。选择哪个模型取决于ADHD疗法的具体需求。|
|**2024-06-21**|**UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis**|Yulong Hui et.al.|[2406.15187](http://arxiv.org/abs/2406.15187)|**[link](https://github.com/qinchuanhui/uda-benchmark)**|**## 翻译  尽管检索增强生成（Retrieval-Augmented Generation, RAG）技术提升了大型语言模型（Large Language Models, LLMs）与外部数据的协作能力，但在现实场景中仍面临诸多挑战。特别是在学术文献和金融问答等领域，数据常常以HTML或PDF格式的冗长、结构复杂的文本和表格形式存在。为此，我们提出一个名为“Unstructured Document Analysis”（UDA）的新基准，它包含2,965份真实世界的文档和29,590个专家标注的问答对。我们重新审视了基于LLM和RAG的方法在处理文档分析任务中的设计决策，并在多个文档领域和多样化的查询类型上评估答案质量和策略。  我们的评估揭示了有趣的结果，强调了数据解析和检索的重要性。我们希望这个基准能够为现实世界的文档分析应用提供启示，并为其发展服务。基准套件和代码已可在<https://github.com/qinchuanhui/UDA-Benchmark>获取。**|
|**2024-06-20**|**Model Merging and Safety Alignment: One Bad Model Spoils the Bunch**|Hasan Abed Al Kader Hammoud et.al.|[2406.14563](http://arxiv.org/abs/2406.14563)|null|## 背景 大型语言模型（LLMs）的合并是一种经济高效的方法，可以将多个专家级LLMs整合成一个全能模型，保留原始模型的专业知识。然而，当前的方法往往忽视了合并过程中安全对齐的重要性，导致生成的模型高度不一致。本研究探讨了模型合并对对齐性的影响。我们评估了几种流行的模型合并技术，发现现有方法不仅传递了领域专业知识，还传播了不一致性。为此，我们提出了一种两步法解决方案：(1) 生成合成的安全性和领域特定数据，(2) 将这些生成的数据融入现有的数据驱动的模型合并优化过程中。这样，我们能够将对齐性视为可以最大化于合并后LLM中的能力。实验表明，在合并过程中整合对齐相关数据的有效性，结果是既能保持领域专长又能实现良好对齐的模型。|
|**2024-06-20**|**Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities**|Sachit Menon et.al.|[2406.14562](http://arxiv.org/abs/2406.14562)|null|当面临涉及视觉思维的问题时，人类会自然地切换到推理模式，常常形成心理图像或绘制视觉辅助工具。大型语言模型在数学和符号推理方面展现出良好表现，通过文本形式表达中间推理步骤的链条思考，但在处理可以通过视觉推理轻松解答的文本查询时仍存在问题，即使经过大量的多模态预训练也是如此。我们提出了一种简单方法，即“白板思维提示”，来解锁多模态大型语言模型在跨模态中的视觉推理能力。白板思维提示为模型提供了一个比喻性的“白板”，让其以图像形式展现推理步骤，然后将这些图像返回模型进行进一步处理。我们发现这种方法无需示范或专用模块，而是利用模型现有的使用Matplotlib和Turtle等库编写代码的能力。这个简单策略在四个涉及视觉和空间推理的困难自然语言任务中实现了最先进的结果。我们发现，与链式思考相比，GPT-4o在某些场景下大幅失败，包括一些准确率为0%的情况下，而白板思维提示能提升至高达92%的准确性。我们详细探讨了该技术的成功之处及其错误来源。|
|**2024-06-21**|**Asynchronous Large Language Model Enhanced Planner for Autonomous Driving**|Yuan Chen et.al.|[2406.14556](http://arxiv.org/abs/2406.14556)|**[link](https://github.com/memberre/asyncdriver)**|尽管实时规划器在自动驾驶中表现出色，但大型语言模型（LLMs）的兴起为提高运动规划的可解释性和可控性开辟了新途径。然而，LLM驱动的规划器仍面临资源消耗大和推理时间长的问题，这阻碍了其实用部署。鉴于这些挑战，我们提出了AsyncDriver，一个全新的异步LLM增强的闭环框架。该框架利用LLM生成的与场景相关的指令特征，指导实时规划器进行精确和可控的轨迹预测。AsyncDriver展示了LLMs在理解和处理向量化场景数据及一系列路线指示方面的强大能力，同时通过异步设计，有效降低了LLM带来的计算成本，保持了与之相近的性能。实验表明，我们的方法在nuPlan的复杂场景中实现了更优的闭环评估性能。|
|**2024-06-20**|**GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**|Shilong Li et.al.|[2406.14550](http://arxiv.org/abs/2406.14550)|null|长文本处理能力对于大型语言模型（LLMs）应对复杂任务至关重要。尽管已有多方努力优化LLMs处理长输入，但依然面临挑战。本文提出GraphReader，这是一种基于图的代理系统，旨在通过构建文本图并让代理自主探索来处理长文本。当接收到问题时，代理会逐步分析并制定合理计划，然后调用预定义函数读取节点内容和邻居信息，实现从粗到细的图探索。在探索过程中，代理不断记录新发现并反思当前情况，以优化获取信息的过程，直到收集足够信息生成答案。在LV-Eval数据集上的实验显示，使用4k上下文窗口的GraphReader在16k到256k的长文本长度上，相对于GPT-4-128k有显著优势。此外，我们的方法在四个单跳和多跳的挑战性基准上表现出色。|
|**2024-06-20**|**Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models**|Sunny Duan et.al.|[2406.14549](http://arxiv.org/abs/2406.14549)|null|随着大型语言模型的兴起，自然语言处理任务发生了革命性变化，但这也引发了数据隐私和安全的重大忧虑。这些模型在包含潜在敏感或专有信息的大量语料库上进行训练，数据泄露的风险——即模型响应揭示部分信息——尚不为人充分理解。本研究旨在探讨机器学习模型中的记忆现象，特别是关注其在训练过程中的演变。我们调查了训练数据的统计特性如何影响模型内编码的记忆，通过评估重复对记忆的影响。研究发现，模型记住一个序列的概率与它在数据中出现的次数呈对数关系。此外，我们发现即使没有后续的接触，某些看似未被记住的序列也可能在整个训练过程中逐渐显现。这种隐藏的已记住序列对数据隐私构成挑战，因为它们可能隐藏在模型的最终检查点中。因此，我们开发了一种诊断测试，通过考虑它们的交叉熵损失来揭示这些潜在的记忆序列。|
|**2024-06-20**|**Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data**|Johannes Treutlein et.al.|[2406.14546](http://arxiv.org/abs/2406.14546)|**[link](https://github.com/choidami/inductive-oocr)**|**针对大型语言模型（LLMs）的安全风险，一个策略是从其训练数据中删除危险知识。尽管这消除了显性信息，但隐性信息可能仍散落在多个训练文档中。我们研究的问题是：LLMs能否通过拼凑这些隐含线索，推断出被屏蔽的知识？为此，我们专注于无上下文归纳推理（Inductive Out-of-Context Reasoning，OOCR），这是一种泛化能力，要求LLMs根据分布在训练文档中的证据推断潜在信息，并在无需上下文学习的情况下应用于下游任务。通过五个任务的实验，我们展示了前沿LLMs确实具备这种能力。例如，在一项实验中，仅对一个未知城市与其与其他已知城市之间的距离进行微调，令人惊讶的是，即使没有示例或链式思考，该LLM也能表述出未知城市是巴黎，并据此解答后续问题。进一步的实验表明，仅接受单个硬币抛掷结果训练的LLMs能判断硬币是否偏斜，而只接触 $(x, f(x))$对的模型能阐述$f$ 的定义并计算逆运算。虽然OOCR在某些情况下表现良好，但我们也发现它并不总是可靠的，特别是在小型LLMs学习复杂结构时。总的来说，LLMs无需明确的上下文学习就能“串联起”信息，这给监控和控制它们获取的知识带来了潜在挑战。**|
|**2024-06-20**|**Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems**|Đorđe Klisura et.al.|[2406.14545](http://arxiv.org/abs/2406.14545)|null|关系数据库在现代信息系统中至关重要，是存储、查询和管理数据的核心。随着大语言模型的进步，文本到SQL技术崭露头角，极大地提升了从数据库中获取信息的能力，但同时也引发了关于隐私和安全的担忧。我们的研究专注于提取文本到SQL模型所依赖的数据库模式元素。了解模式可能使SQL注入攻击更为容易。为此，我们设计了一种零知识框架，通过提出精心构造的问题，无需直接了解数据库，该框架能促使这些模型处理这些问题并生成输出，从而揭示数据库模式结构。我们将此方法应用于针对文本-SQL对进行过微调的专用文本到SQL模型以及用于SQL生成的生成式语言模型。结果显示，对于微调模型，我们能够以接近0.75的F1分数重构表名，而对于生成式模型，这一分数更是高达0.96。|
|**2024-06-20**|**Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs**|Yuxuan Qiao et.al.|[2406.14544](http://arxiv.org/abs/2406.14544)|**[link](https://github.com/sparksjoe/prism)**|**## 翻译  视觉语言模型（VLMs）在处理各种视觉问题时展现出卓越的能力，这要求模型具备强大的感知和推理能力。然而，由于感知和推理在现有VLM中的交织性，独立评估这两方面的能力颇具挑战。为此，我们提出了一种创新框架——Prism，旨在分离视觉理解和推理在视觉问答中的作用。Prism分为两个阶段：感知阶段利用VLM提取并以文本形式表达视觉信息；推理阶段则根据提取的视觉信息，通过大型语言模型（LLM）生成响应。这种模块化设计使得我们可以系统地比较和评估不同VLM的感知和推理性能。  我们的分析框架提供了诸多洞见，证明了Prism作为成本效益高的视觉语言任务解决方案的潜力。通过将专注于感知的简化VLM与专为推理设计的强大LLM相结合，Prism在通用视觉语言任务上取得了优异成绩，同时显著降低了训练和运营成本。定量评估显示，当Prism配备基础的2B LLaVA VLM和开源的GPT-3.5时，其在严谨的多模态基准MMStar上的表现可与大十倍的VLM相当。该项目已发布在：https://github.com/SparksJoe/Prism。**|
|**2024-06-21**|**Are LLMs Naturally Good at Synthetic Tabular Data Generation?**|Shengzhe Xu et.al.|[2406.14541](http://arxiv.org/abs/2406.14541)|**[link](https://github.com/anonymou9167/anonymouscode)**|**大型语言模型（LLMs）在生成文本和图像方面表现出色，但其在生成最常见的数据类型——表格数据方面的潜力却鲜有研究。这篇论文指出，直接使用或经过传统微调的LLMs在作为合成表格生成器时表现极差。由于LLMs的自回归特性，随机顺序排列的微调与捕捉功能性依赖的重要性相悖，导致它们无法处理条件混合分布（这是反映现实世界约束的关键）。我们展示了如何通过使LLMs变得感知排列顺序来改善这些不足，从而提升其性能。**|
|**2024-06-20**|**PostMark: A Robust Blackbox Watermark for Large Language Models**|Yapei Chang et.al.|[2406.14517](http://arxiv.org/abs/2406.14517)|**[link](https://github.com/lilakk/postmark)**|**最有效的检测生成式语言模型（LLM）文本的方法是通过在解码过程中插入可识别的标记，即水印。然而，大多数现有方法依赖于获取到LLM的原始概率（logits），这使得LLM服务提供商不愿分享，因为担心模型泄露问题。因此，这些水印需要每个提供者独立开发。本文提出了一种创新的后处理水印方案，名为PostMark。它是一种模块化的、生成后插入的水印策略，无需触及logits，适合第三方实施。PostMark表现出更强的对抗同义句攻击能力：我们在实验中涵盖了八个基础算法、五个基线LLM和三个数据集。此外，我们还评估了PostMark对文本质量的影响，包括自动化和人工评估，探讨了质量和抗改写攻击之间的权衡。研究代码、输出和注释已公开在https://github.com/lilakk/PostMark。**|
|**2024-06-18**|**DrVideo: Document Retrieval Based Long Video Understanding**|Ziyu Ma et.al.|[2406.12846](http://arxiv.org/abs/2406.12846)|null|当前的长视频理解方法主要关注时长仅十几秒的视频，对处理更长视频的技术探索有限。长视频中的大量帧数带来了两个主要挑战：难以定位关键信息和进行长期推理。因此，我们提出DrVideo，一个基于文档检索的系统，专为长视频理解设计。我们的核心思想是将长视频理解问题转化为长文档理解任务，以充分利用大型语言模型的强大能力。具体来说，DrVideo将长视频转换为文本形式的长文档，首先检索关键帧并增强这些帧的信息，作为系统的起点。然后，它采用基于代理的迭代循环，持续搜索缺失信息、补充相关数据，并在收集到足够的与问题相关的信息后，以链式思考的方式给出最终预测。在多个长视频基准上的实验验证了我们方法的有效性。DrVideo在EgoSchema（3分钟）测试中比现有最先进的方法高出3.8个百分点，在MovieChat-1K（10分钟）的break模式和global模式中分别提高17.9和38.0分，以及在LLama-Vid QA（超过60分钟）数据集上提升30.2分。|
|**2024-06-18**|**Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts**|Haoxiang Wang et.al.|[2406.12845](http://arxiv.org/abs/2406.12845)|**[link](https://github.com/RLHFlow/RLHF-Reward-Modeling)**|**强化学习从人类反馈（RLHF）已经成为大型语言模型（LLMs）与人类偏好对齐的主要方法。传统上，通过使用人类偏好数据训练奖励模型（RM），过程通常从比较同一用户请求的响应开始，相对评分指示人类更喜欢哪个响应。然而，由于RM的黑盒特性，其输出缺乏可解释性，人们难以理解为什么RM认为某个回复是好的。鉴于RM作为人类偏好的代理，我们提议采用两阶段方法来创建可解释的RM：首先，使用多维绝对评分数据训练绝对评级多目标奖励模型（ArmoRM），每个维度对应于人类可理解的目标（如诚实、详尽、安全）；其次，利用混合专家（MoE）策略，结合一个门控网络，根据上下文自动选择最合适的奖励目标。我们成功地使用Llama-3 8B训练了ArmoRM，并在顶部添加了一个浅层MLP作为门控网络，形成了ArmoRM-Llama3-8B。我们的模型在评估RM的语言建模性能的RewardBench基准上实现了最先进的成绩。值得注意的是，我们的模型在性能上超过了使用GPT-4法官的LLM作为评判者的方法，并接近于规模更大的Nemotron-4 340B奖励模型的水平。**|
|**2024-06-18**|**Synergizing Foundation Models and Federated Learning: A Survey**|Shenghui Li et.al.|[2406.12844](http://arxiv.org/abs/2406.12844)|null|近期，大型语言模型、视觉Transformer和多模态模型等基础模型（FMs）的发展在学术界和工业界产生了显著影响。与小型模型相比，FMs在预训练阶段对大量数据的需求更大。尽管通用FMs可以使用互联网上的公开数据进行预训练，但针对特定领域的FMs需要专有数据，这在实际应用中因隐私问题而面临数据可用性挑战。联邦学习（FL）作为一种协作学习范式，打破了数据共享的障碍，为利用分布式数据定制和适应各种领域特定任务的FMs提供了前景，同时保护了数据隐私。这篇综述论文探讨了FL与FMs融合的潜力与挑战，总结了核心技术、未来发展方向以及应用场景。关于FM-FL的定期更新论文集合可在<https://github.com/lishenghui/awesome-fm-fl>获取。|
|**2024-06-18**|**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**|Seyedarmin Azizi et.al.|[2406.12832](http://arxiv.org/abs/2406.12832)|**[link](https://github.com/arminazizi98/lamda)**|**在大语言模型微调领域，低秩适应（LoRA）已经成为标准方法，因为它显著减少了可训练参数。然而，随着模型嵌入维度的增加，LoRA所需的可训练参数量也随之上升，导致计算成本较高。此外，其后向更新需要存储高维中间激活和优化器状态，对GPU内存需求较大。为此，本文提出了一种新的大语言模型微调方法——基于谱分解的低维适应（LaMDA）。LaMDA通过冻结第一投影矩阵（PMA），同时引入一个低维可训练的平方矩阵，实现了可训练参数和峰值GPU内存使用的大幅减少。在早期的微调阶段，LaMDA逐步冻结第二投影矩阵（PMB），进一步降低权重更新的计算成本，提高参数效率。  我们还引入了增强版LaMDA++，它通过规范化预训练模型权重的谱分析，实现轻量级的LoRA路径自适应秩分配。我们在多个任务上进行了评估，包括GLUE自然语言理解基准、文本摘要、自然语言生成以及复杂推理，应用于不同类型的大型语言模型。实验结果显示，LaMDA在性能上与现有方法相当或超越，且在微调期间可减少高达17.7倍的参数更新次数，以及1.32倍的峰值GPU内存使用。我们将公开代码。**|
|**2024-06-18**|**Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?**|Pinzhen Chen et.al.|[2406.12822](http://arxiv.org/abs/2406.12822)|null|## 背景 大型多语言模型旨在服务不同语种的母语使用者。我们推测，当前针对这些模型的微调和评估方法可能与其初衷不符，原因在于过度依赖翻译，可能导致翻译中的瑕疵。尚不清楚指令数据的性质如何影响模型输出，同时，用翻译测试集来捕捉这些细微差别是否有效。由于训练和评估阶段常常结合使用翻译数据，这些潜在问题可能被忽视。本研究通过在指令调优和评估阶段使用控制性的母语或翻译数据，来探究这些问题，并观察模型表现。我们在八种基础模型和八个不同基准上进行实验，结果显示，对于母语或生成性基准，使用母语或翻译指令数据时，模型性能高时，两者之间的差异尤为明显，而在其他类型的测试集上则不然。最后，我们发现正则化对于结构化任务有益，但对于生成性任务则不然。|
|**2024-06-18**|**Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?**|Zhe Yang et.al.|[2406.12809](http://arxiv.org/abs/2406.12809)|null|大型语言模型（LLMs）展现了令人印象深刻的性能，但它们仍存在不一致的问题，例如对重述或微小顺序变化的反应不一致。除了这些不稳定性，我们还观察到尽管LLMs能够解决难题，但在相对简单的任务上却可能失败。为了评估这种从难到易的不一致性，我们创建了ConsisEval基准，其中每个条目包含两个难度有序的问题。我们还引入了一致性分数的概念，以量化这种不一致性，并分析通过相对一致性分数改进一致性潜力。通过对现有模型的广泛实验，我们得出以下发现：(1) GPT-4获得92.2%的最高一致性分数，但仍因冗余信息的干扰、问题误解等问题对特定问题不一致；(2) 能力更强的模型通常表现出更高的一致性，但也存在例外情况；(3) 对于 Fine-tuning 和上下文学习而言，硬数据可以提高一致性。我们的数据和代码将在GitHub上公开提供。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**Supporting Human Raters with the Detection of Harmful Content using Large Language Models**|Kurt Thomas et.al.|[2406.12800](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型（LLMs）自动或辅助人类审阅者检测有害内容的可能性，如仇恨言论、骚扰、极端主义和选举误导。通过50,000条评论的数据集，我们发现LLMs在与人类判断相比时能达到90%的准确率。我们提出五种设计模式，以整合LLMs与人工评级，例如预筛选非暴力内容、检测人类评级可能的错误，或者提供关键上下文以支持人工评级。我们展示了如何使用一个优化的提示来支持这些设计模式。在实际应用的试点中，我们的方法在优化人力资源效率方面实现了41.5%的提升，同时在检测违规内容的精确度和召回率上分别提高了9%至11%。|
|**2024-06-18**|**ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools**|Team GLM et.al.|[2406.12793](http://arxiv.org/abs/2406.12793)|**[link](https://github.com/thudm/chatglm-6b)**|我们介绍ChatGLM，这是一个随时间不断发展的大型语言模型系列。本报告主要关注GLM-4语言系列，包括GLM-4、GLM-4-Air和GLM-4-9B，它们代表了我们当前最强大的模型，集成了前三代ChatGLM的所有经验和教训。这些模型经过了十万亿次训练，主要涵盖中文和英语，以及少量来自24种语言的语料库，侧重于中英文的对齐。高质量的对齐是通过多阶段的后训练过程实现的，包括监督微调和学习人类反馈。评估显示，GLM-4在通用指标如MMLU、GSM8K、MATH、BBH、GPQA和HumanEval上接近或优于GPT-4；在IFEval指令跟随任务中的表现接近GPT-4 Turbo；在长文本任务上与GPT-4 Turbo（128K）和Claude 3相当；在中文对齐方面，GLM-4优于GPT-4，根据AlignBench衡量。GLM-4 All Tools模型进一步进行了对齐，以理解用户意图并能自主决定何时使用哪种工具，如Web浏览器、Python解释器、文本转图像模型和自定义函数，以有效地完成复杂任务。在实际应用中，它在诸如通过网络浏览获取信息和使用Python解释器解题等任务上与GPT-4 All Tools相匹配甚至超越。到目前为止，我们已经开源了一系列模型，包括ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM和CodeGeeX，在2023年仅Hugging Face上就有超过1000万次下载。这些开源模型可通过<https://github.com/THUDM>和<https://huggingface.co/THUDM>访问。|
|**2024-06-18**|**UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions**|Xunzhi Wang et.al.|[2406.12784](http://arxiv.org/abs/2406.12784)|**[link](https://github.com/Cyno2232/UBENCH)**|随着大型语言模型（LLMs）的迅速发展，它们在实际应用中展现出显著的效果。然而，由于低可解释性，这些模型在未预见情况下常会出现错误，限制了其价值。尽管已有许多研究致力于构建全面的评估体系，但先前的基准测试主要关注问题解决能力，对响应的不确定性评估不足，可能导致不稳定性。当前的方法在衡量LLM可靠性时资源消耗大，且难以测试黑盒模型。  为解决这些问题，我们提出了UBENCH，一个全面的LLM可靠性评估基准。它包含3,978个涵盖知识、语言理解、推理能力的多选题。实验结果显示，UBENCH达到了最先进的性能，并且其单次采样方法显著节省了计算资源，相较于需要多次采样的基线方法更为高效。此外，我们利用UBENCH评估了15种流行LLM的可靠性，发现GLM4表现出色，紧随其后的是GPT-4。我们还探究了Chain-of-Thought提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同模型的不同作用。|
|**2024-06-17**|**LLaNA: Large Language and NeRF Assistant**|Andrea Amaduzzi et.al.|[2406.11840](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型（MLLM）在理解和处理图像和3D数据方面表现出色，但它们在全面捕捉物体的外观和几何特性上存在局限。近期，神经辐射场（Neural Radiance Fields，简称NeRF）作为一种新兴的表示方式，通过一个简单的多层感知器（Multi-Layer Perceptron，MLP）的权重编码了物体的几何结构和高度逼真的外观，引起了广泛关注。本文探讨了将NeRF整合到MLLM中的可行性和效果。我们开发了LLaNA，这是首个通用的NeRF-语言助手，能够执行新任务，如NeRF描述和问答。我们的方法直接处理NeRF MLP的权重，无需渲染图像或构建3D数据结构，就能提取有关代表对象的信息。此外，我们创建了一个无须人工干预的NeRF文本标注数据集，用于各种NeRF-语言任务，并据此建立了一个评估方法来衡量我们的模型对NeRF理解能力。实验结果表明，处理NeRF权重的方法在与从NeRF中提取2D或3D表示进行比较时表现更优。|
|**2024-06-17**|**mDPO: Conditional Preference Optimization for Multimodal Large Language Models**|Fei Wang et.al.|[2406.11839](http://arxiv.org/abs/2406.11839)|null|### 背景  直接偏好优化（DPO）已被证明是大型语言模型（LLM）校准的有效手段。最近的研究尝试将DPO应用于多模态场景，但发现实现持续改进颇具挑战。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽视了图像条件。为解决这个问题，我们提出了mDPO，一个旨在防止语言偏好过度优先的多模态DPO目标，同时优化图像偏好。此外，我们引入了奖励锚点，确保选择的响应奖励保持正向，从而避免相对偏好优化固有的可能性降低问题。  ### 任务  我们在两个不同规模的多模态LLM以及三个常用基准上进行了实验，结果显示，mDPO有效解决了多模态偏好优化中的无条件偏好问题，并显著提高了模型性能，特别是在减少幻觉方面。|
|**2024-06-17**|**Unveiling Encoder-Free Vision-Language Models**|Haiwen Diao et.al.|[2406.11832](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|**当前的视觉语言模型（VLM）主要依赖于视觉编码器来提取视觉特征，然后利用大型语言模型（LLMs）处理视觉语言任务。然而，视觉编码器在抽象视觉表示方面设定了强烈的先验，如分辨率、比例和语义倾向，这可能限制了VLM的灵活性和效率。直接训练无编码器的纯VLM仍然具有挑战性，且鲜有探索。实证研究显示，这种直接训练方法会导致收敛缓慢和性能差距较大。本文旨在弥合编码器依赖型和无编码器模型之间的差距，提出了一种简单而有效的纯VLM训练策略。具体来说，我们通过深入实验揭示了高效训练无编码器VLM的关键要素：（1）在统一的解码器内融合视觉与语言表示；（2）通过额外监督提升视觉识别能力。基于这些策略，我们开发了EVE，一个无编码器的视觉语言模型，既能高效训练也能快速推理。值得注意的是，仅使用3500万公开可用的数据，EVE就能在多个视觉语言基准上与类似容量的编码器依赖型VLM匹敌，甚至超越了训练过程神秘、数据未公开的Fuyu-8B模型。我们相信，EVE为跨模态开发纯粹的解码器架构提供了一个透明且高效的路径。我们的代码和模型已公开在：https://github.com/baaivision/EVE。**|
|**2024-06-17**|**Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models**|Bingqi Ma et.al.|[2406.11831](http://arxiv.org/abs/2406.11831)|null|大型语言模型（LLMs）基于解码器-only变压器在文本理解方面表现出色，但如何将这些先进的LLMs应用于文本到图像的扩散模型仍是一个待探索的问题。我们发现直接使用LLM作为提示编码器会显著降低生成图像时的提示跟随能力。主要存在两个问题：一是LLM的下一个词预测训练与扩散模型对区分性提示特征的需求不匹配；二是解码器架构固有的位置偏见。为解决这些问题，我们提出了一种新框架，通过精心设计的使用指南，增强LLM的文本表示能力，消除其内在的定位偏见，从而灵活地将最先进的LLMs融入文本到图像生成模型。此外，我们还提供了一种融合多个LLMs的方法。鉴于Transformer架构的卓越性能和扩展能力，我们进一步设计了基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，验证了LI-DiT在不同模型规模和数据量下的性能。得益于LLMs的内在能力及我们的创新设计，LI-DiT的提示理解性能轻松超越开源的最新模型，以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。强大的LI-DiT-10B将在进一步优化和安全检查后提供。|
|**2024-06-17**|**WPO: Enhancing RLHF with Weighted Preference Optimization**|Wenxuan Zhou et.al.|[2406.11827](http://arxiv.org/abs/2406.11827)|**[link](https://github.com/wzhouad/wpo)**|**强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）以更好地符合人类价值观的有前景方法。由于成本效益和可扩展性，离线偏好优化——通过其他模型获取偏好数据——被广泛采用。然而，离线偏好优化常受采样策略与目标策略之间分布差异的影响，导致优化效果不理想。为此，我们提出了一种创新策略——加权偏好优化（WPO），旨在通过调整偏好评分对，使离线数据更接近于当前策略，从而缓解这一问题。这种方法不仅解决了分布差距难题，还提升了优化过程，无需额外成本。  我们在Alpaca Eval 2和MT-bench等指令跟随基准上验证了我们的方法。WPO在Alpaca Eval 2上的性能比直接偏好优化（DPO）提高了5.6%。基于Llama-3-8B-Instruct，WPO甚至建立了显著的长度控制胜率，达到48.6%，在80亿参数模型排行榜上成为最强劲的模型。我们将在<https://github.com/wzhouad/WPO>上开源代码和模型。**|
|**2024-06-17**|**Embodied Instruction Following in Unknown Environments**|Zhenyu Wu et.al.|[2406.11818](http://arxiv.org/abs/2406.11818)|null|在自主家庭服务系统中，使实体代理能根据自然语言完成复杂的人类指令至关重要。传统方法仅能在所有互动对象都提供给代理的已知环境中执行指令，直接将现有方法应用于未知环境通常会产生操作不存在物体的不可行计划。相反，我们提出了一种针对未知环境的复杂任务实体指令跟随（Embodied Instruction Following，EIF）方法，该方法使代理能够有效地探索环境，利用现有物体生成可执行计划，以达成抽象指令。具体来说，我们构建了一个包括高层任务规划器和低层探索控制器的多模态大语言模型的层次化实体指令跟随框架。然后，我们通过动态区域注意力构建场景的语义表示地图，以展示已知的视觉线索，使任务规划和场景探索与人类指令目标保持一致。对于任务规划器，根据任务完成过程和已知视觉线索，我们生成步骤式的可行计划。对于探索控制器，根据生成的步骤计划和已知视觉线索预测最优的导航或物体交互策略。实验结果表明，我们的方法在大型房屋级场景中的204个复杂人类指令（如做早餐和整理房间）上实现了45.09%的成功率。|
|**2024-06-17**|**VideoLLM-online: Online Video Large Language Model for Streaming Video**|Joya Chen et.al.|[2406.11816](http://arxiv.org/abs/2406.11816)|null|## 翻译  近期的大型语言模型已经增强了视觉功能，能够理解图像、视频和融合了视觉与语言的内容。然而，这些大模odels的训练方法通常将视频视为预先剪辑好的片段，这使得它们在处理连续视频流时效果不佳且效率低下。为此，我们在本文中提出了一种新颖的“Learning-In-Video-Stream”（LIVE）框架，旨在实现实时、长序列、与视频流同步的对话，适用于连续视频输入。LIVE框架包括以下三个方面：（1）一个设计用于处理连续流式输入的语言建模目标；（2）一种数据生成策略，将离线时间标注转换为适合流式对话的格式；（3）一个优化的推理管道，以提高在实际视频流中的响应速度。基于Llama-2/Llama-3，我们构建了VideoLLM-online模型，并通过它展示了在处理视频流对话方面的显著优势，例如，在A100 GPU上，该模型能在5分钟视频片段中实现超过10帧每秒的流式对话。此外，VideoLLM-online还在公开的离线视频基准测试（如识别、captioning和预测）上展现出最先进的性能。我们已将代码、模型、数据和演示发布在https://showlab.github.io/videollm-online供人使用。|
|**2024-06-17**|**How Do Large Language Models Acquire Factual Knowledge During Pretraining?**|Hoyeon Chang et.al.|[2406.11813](http://arxiv.org/abs/2406.11813)|**[link](https://github.com/kaistai/factual-knowledge-acquisition)**|尽管近期研究表明大型语言模型（LLMs）能够存储大量事实知识，但它们如何在预训练过程中获取这些知识的机制尚不明确。本研究针对这一缺口，探讨了LLMs在预训练期间如何获取和保持事实知识。研究发现了一些关键洞见：首先，出乎意料的是，更多的训练数据对模型获取和保持事实知识的能力并无显著提升。其次，训练步数与记忆遗忘和事实知识泛化之间存在幂律关系，使用重复训练数据的模型遗忘速度更快。第三，增大批量大小可以提高模型抵抗遗忘的能力。总的来说，我们的观察表明，LLMs在预训练中的事实知识获取是通过逐步增加每一步中预训练数据中事实知识出现的概率。然而，这种增加随后会因遗忘而稀释。基于这种理解，我们能够解释一些最近观察到的LLM行为，如长尾知识上的性能不佳，以及去重预训练语料库的好处。|
|**2024-06-17**|**RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**|Joao Monteiro et.al.|[2406.11811](http://arxiv.org/abs/2406.11811)|**[link](https://github.com/ServiceNow/repliqa)**|## 背景  大型语言模型（LLMs）在训练过程中大量依赖自动从互联网抓取的数据，其中包括包含大量通用知识的百科全书（如维基百科），也可能与用于评估LLMs的基准数据集重叠。因此，如果测试集可能已泄露到训练集中，对模型的评估可能会产生误导性的结论。为了推动语言模型的公正评估，我们提出了一种新的测试数据集——RepLiQA，适用于问答和主题检索任务。RepLiQA是一个包含五个分片的测试集，其中四个在本论文发布前未公开或通过LLM API提供。RepLiQA的每个样本由以下四部分组成：（1）由人类标注员创作的虚构场景描述文档（例如新闻文章），这些内容不会出现在互联网上；（2）关于文档主题的问题；（3）直接源自文档信息的正确答案；（4）包含答案的文档段落。这意味着只有当模型能在提供的文档中找到相关内容时，才能生成准确的答案。  我们进行了一项大规模基准测试，包括多个最先进的LLM，以揭示不同类型的和规模的模型在条件语言建模设置下的性能差异。RepLiQA的已发布分片可在以下链接找到：https://huggingface.co/datasets/ServiceNow/repliqa。|
|**2024-06-17**|**Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations**|Rima Hazra et.al.|[2406.11801](http://arxiv.org/abs/2406.11801)|**[link](https://github.com/declare-lab/safety-arithmetic)**|**随着大型语言模型（LLMs）在翻译和问答等应用中的日益重要，确保它们与人类价值观的正确导向变得至关重要。然而，当前的对齐方法在处理动态用户意图和复杂目标时存在困难，使得模型容易生成有害内容。为此，我们提出了一种无需训练的框架——安全算术（Safety Arithmetic），旨在提升LLMs在不同场景下的安全性，包括基础模型、监督微调模型（SFT）和编辑后的模型。安全算术包含两部分：有害内容消除（Harm Direction Removal）以避免不良输出，以及安全对齐（Safety Alignment）以促进安全响应。此外，我们还发布了NoIntentEdit数据集，它揭示了可能导致模型安全风险的编辑实例。实验结果显示，安全算术显著增强了安全措施，减少了过度安全的问题，同时保持了模型的实用性，相较于现有方法在保障内容生成的安全性方面表现出色。**|
|**2024-06-14**|**Quantifying Variance in Evaluation Benchmarks**|Lovish Madaan et.al.|[2406.10229](http://arxiv.org/abs/2406.10229)|null|评价基准是衡量大型语言模型（LLMs）能力的关键，也是推动这些能力进步的驱动力。最初设计用于评估预训练模型的性能（或缺乏），现在它们也被广泛用于决定不同的训练选择之间。然而，尽管被广泛应用，我们很少量化评价基准的方差，这决定了性能差异的含义。本文定义并测量了一系列旨在衡量评价基准方差的指标，包括初始化时的随机种子方差和训练过程中的单调性。通过对大量模型（包括公开可用的和从头训练的模型）进行研究，我们提供了各种方差度量的实证估计，并为实践者提供了考虑和建议。我们还评估了连续和离散性能度量的实用性和权衡，并探索了更好地理解和减少方差的方法。我们发现，对于较小规模（约70亿参数）的模型，如将多模态多任务学习（MMLU）任务框架为完成任务，可以常常降低方差；而受到人类测试文献启发的更复杂方法（如项目分析和项目反应理论）在显著减少方差方面效果有限。总的来说，我们的工作揭示了评价基准的方差特性，提出了针对LLMs的特定技术来减少方差，并普遍鼓励实践者在比较模型时仔细考虑方差因素。|
|**2024-06-14**|**Semantic Membership Inference Attack against Large Language Models**|Hamid Mozaffari et.al.|[2406.10218](http://arxiv.org/abs/2406.10218)|null|## 背景 成员身份泄露攻击（Membership Inference Attacks，MIA）的目标是识别特定数据点是否被纳入了目标模型的训练集。本文提出了一种新颖的方法——语义成员身份泄露攻击（Semantic Membership Inference Attack，SMIA），通过利用输入的语义内容及其扰动，提升MIA的性能。SMIA训练一个神经网络来分析目标模型对扰动输入的行为，从而捕捉成员样本与非成员样本之间输出概率分布的差异。我们在Pythia和GPT-Neo模型家族，以及Wikipedia数据集上进行了全面的评估。实验结果显示，SMIA明显优于现有攻击手段，例如在Pythia-12B上的AUC-ROC值达到了67.39%，而第二好的攻击方法仅为58.90%。|
|**2024-06-14**|**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**|Rui Yang et.al.|[2406.10216](http://arxiv.org/abs/2406.10216)|**[link](https://github.com/yangrui2015/generalizable-reward-model)**|在强化学习从人类反馈（RLHF）框架中，利用基于人类偏好数据的奖励模型已证实能有效调整大型语言模型（LLMs）以符合人类意图。然而，当前奖励模型对未见过的提示和响应的泛化能力有限，可能导致所谓的过度优化问题，即奖励优化过度导致实际性能下降。尽管先前的研究倾向于约束策略优化，我们的研究提出了一种新方法，通过正则化隐藏状态来增强奖励模型应对分布变化的泛化能力。具体来说，我们保留基础模型的语言模型头，并结合一系列文本生成损失，旨在保持隐藏状态的文本生成能力，同时在相同的隐藏状态后学习一个奖励头。实验结果表明，引入的正则化技术显著提高了在各种泛化任务中的奖励模型准确性，并有效缓解了RLHF中的过度优化问题，提供了一个更可靠、更稳健的偏好学习范式。|
|**2024-06-14**|**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs**|Abhimanyu Hans et.al.|[2406.10209](http://arxiv.org/abs/2406.10209)|**[link](https://github.com/ahans30/goldfish-loss)**|**## 背景 大型语言模型能够记住并重复其训练数据，这带来了隐私和版权问题。为了减轻这种记忆，我们提出了一种对下一步 token 训练目标的微妙修改，称为“金鱼损失”。在训练过程中，随机选择一部分令牌不参与损失计算。模型不会记住这些被丢弃的令牌，从而防止了完整训练序列的逐字复制。我们在数十亿规模的 Llama-2 模型上进行了大量实验，包括预训练和从头开始训练，结果显示，我们的方法显著减少了可提取的记忆，而对下游基准的影响微乎其微。**|
|**2024-06-14**|**TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**|Tomas de la Rosa et.al.|[2406.10196](http://arxiv.org/abs/2406.10196)|null|**摘要：**  旅行规划是一个复杂的任务，它涉及根据约束条件生成一系列与访问地点相关的行动，同时最大化用户的满意度。传统方法通常会将问题转化为特定形式的语言表达，从网络资源中提取相关信息，并使用合适的求解器来生成有效解决方案。然而，近期的基于大型语言模型（LLMs）的方法直接从用户请求中输出计划，利用丰富的旅行领域知识提供景点和可能路线等高层次信息。尽管如此，当前最先进的模型往往产生不连贯、未能完全满足约束的计划，且无法保证生成高质量方案。我们提出TRIP-PAL，一种融合LLMs和自动化规划器的混合方法：（1）LLMs获取并转换旅行信息和用户需求，将其转化为可输入规划器的数据结构；（2）自动化规划器负责生成满足约束并优化用户效用的旅行计划。我们在不同旅行场景中的实验表明，TRIP-PAL在生成旅行计划方面优于纯LLM方法。|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|Jiawei Chen et.al.|[2406.10185](http://arxiv.org/abs/2406.10185)|null|随着大型视觉语言模型（LVLM）在医疗领域的应用日益增长，如医学图像问答和报告生成，它们从基础大语言模型（LLMs）那里继承了强大的功能，但同时也带来了令人担忧的幻觉问题，这在医疗这样对错误容限极低的环境中尤为重要。然而，目前尚无专门针对医疗领域的幻觉检测和评估方法或基准。为了填补这一空白，我们推出了Med-HallMark，这是首个专为医疗多模态领域设计的幻觉检测和评估基准。Med-HallMark支持多任务幻觉检测，提供多元化的幻觉数据，并采用分级幻觉分类。此外，我们提出了MediHall Score，这是一种新的医疗评估指标，通过分层评分系统评估LVLM的幻觉，考虑其严重程度和类型，从而实现对潜在临床影响的细致评估。我们还展示了MediHallDetector，一种专为精确幻觉检测设计的医疗LVLM，它采用了多任务训练方法。通过广泛的实验，我们在我们的基准上为流行的LVLM设立了基线。实验结果表明，MediHall Score提供了比传统指标更深入理解幻觉影响的能力，并显示了MediHallDetector的提升性能。我们期望这项工作能显著提高LVLM在医疗应用中的可靠性。所有相关资源将在不久后发布。|
|**2024-06-14**|**Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**|Siyuan Chen et.al.|[2406.10181](http://arxiv.org/abs/2406.10181)|null|在大语言模型（LLMs）的微调过程中，由于内存需求通常超过单个GPU的容量，解决这一内存挑战的一个常见方法是将计算和数据从GPU迁移到CPU。然而，这受到普通硬件带宽限制的制约，影响了CPU与GPU之间的通信效率。本文提出了一种名为LSP_Offload的框架，通过学习式的子空间投影器，实现在 commodity 硬件上接近原生速度的大规模语言模型微调。我们的数据驱动方法涉及学习一个高效的稀疏压缩器，以最小化通信并保持最小精度损失。此外，我们引入了一种创新的层级通信调度策略，以最大化通信与计算之间的并行性。因此，我们的框架能够在4GB笔记本GPU上微调13亿参数的模型，在配备24GB内存的NVIDIA RTX 4090 GPU上微调70亿参数的模型，仅比无内存限制的微调慢31%。与最先进的离线框架相比，我们的方法提高了微调吞吐量，最高可达3.33倍，当达到相同准确度时，减少了端到端微调时间的33.1%至62.5%。|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|Matteo Gabburo et.al.|[2406.10172](http://arxiv.org/abs/2406.10172)|null|**摘要：**  在设计高效的检索式问答（Question Answering，QA）系统中，答案句子选择（Answer Sentence Selection，AS2）是一个关键任务。然而，由于缺乏标注数据，大多数AS2领域的进展主要集中在英语上。这导致了非英语环境下QA系统的性能与英语系统之间的差距。本论文针对这一问题，我们开发了新的高质量多语言（法语、德语、意大利语、葡萄牙语和西班牙语）AS2数据集，通过使用大型语言模型（Large Language Model，LLM）对现有的英文AS2数据集（如ASNQ、WikiQA和TREC-QA）进行监督自动机器翻译（Automatic Machine Translation，AMT）。我们通过多种实验和不同Transformer架构的评估，验证了我们的方法以及翻译数据集的质量。结果显示，我们的数据集对于构建健壮的多语言AS2模型至关重要，显著缩小了非英语与英语环境下的性能差距。|
|**2024-06-14**|**Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**|Carson Denison et.al.|[2406.10162](http://arxiv.org/abs/2406.10162)|**[link](https://github.com/anthropics/sycophancy-to-subterfuge-paper)**|**在强化学习中，当人工智能系统学会因训练目标不明确而获得不期望的行为时，就会出现规格游戏现象。这种行为可能从简单的奉承行为发展到更复杂且危险的奖励篡改，即模型直接修改其自身的奖励机制。然而，发现这些复杂行为可能超出探索的范畴。本论文探讨大型语言模型（LLMs）是否会在学习常见规格游戏策略后，泛化到执行更为罕见和明显的行为，包括奖励篡改。我们构建了一个逐步升级的可游戏环境系列，并发现针对早期阶段环境的训练会导致在后续环境中出现更多的规格游戏。令人惊讶的是，一小部分但非零的LLMs，在经历了完整训练课程后，能够零样本地直接修改其奖励函数。重新训练LLMs以避免早期阶段的游戏行为可以减轻但不能完全消除后期环境中的奖励篡改。此外，对可游戏环境进行无害性训练并不能阻止奖励篡改。这些结果表明，LLMs能够从常见的规格游戏策略中泛化到更恶劣的奖励篡改行为，并且要消除这种行为可能并非易事。**|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|Yuri Kuratov et.al.|[2406.10149](http://arxiv.org/abs/2406.10149)|**[link](https://github.com/booydar/babilong)**|近年来，大型语言模型（LLMs）的输入上下文长度显著增加。然而，现有的评估方法未能充分衡量模型处理长篇文本中的事实推理能力。为此，我们提出了BABILong基准测试，旨在测试模型在分布式长文档中跨事实推理的能力。BABILong包括20个多样化的推理任务，如事实链、简单归纳、演绎、计数以及处理列表/集合等。这些任务本身就具有挑战性，而当所需事实分散在长篇自然文本中时，难度进一步提升。我们的评估显示，流行的LLMs实际上只利用了10%-20%的上下文信息，且随着推理复杂性的提高，性能急剧下降。对于替代的上下文推理方法，检索增强生成策略在单事实问题回答上的准确率仅为60%，与上下文长度无关。在上下文扩展方法中，循环记忆Transformer展现出最高性能，可处理长达1100万个令牌的长度。BABILong基准测试可以扩展到任意长度，以支持评估具有更强能力的新模型，并提供了长达100万令牌的分隔。|
|**2024-06-13**|**VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding**|Muhammad Maaz et.al.|[2406.09418](http://arxiv.org/abs/2406.09418)|**[link](https://github.com/mbzuai-oryx/videogpt-plus)**|**在基于语言模型的进展基础上，大型多模态模型（LMMs）在视频理解方面取得了显著进步。然而，现有的视频LMMs依赖于图像或视频编码器处理视觉输入，这些编码器各自存在局限性。图像编码器擅长捕捉帧序列中的丰富空间细节，但缺乏明确的时间上下文；而视频编码器提供时间上下文，但常常受限于计算资源，导致只能处理低分辨率的稀疏帧，从而影响了对空间和上下文的理解。因此，我们提出VideoGPT+，它结合了图像编码器（用于详细的空间理解）和视频编码器（用于全局时序上下文建模）的优势。该模型通过将视频划分为小段，并对来自两者特征的提取应用自适应池化策略，以提高性能。我们的架构在多个视频基准上表现出色，包括VCGBench、MVBench和零样本问答任务。此外，我们开发了一个112K的视频指令集，通过新颖的半自动标注管道进一步提升模型性能。为了全面评估视频LMMs，我们还提出了VCGBench-Diverse，它涵盖了18个广泛视频类别，如生活方式、体育、科学、游戏和监控视频，共4,354个问题-答案对。这个基准测试评估现有LMMs在密集视频描述、空间和时间理解以及复杂推理方面的泛化能力，确保在各种视频类型和动态下的全面评估。代码可在https://github.com/mbzuai-oryx/VideoGPT-plus找到。**|
|**2024-06-13**|**Explore the Limits of Omni-modal Pretraining at Scale**|Yiyuan Zhang et.al.|[2406.09412](http://arxiv.org/abs/2406.09412)|**[link](https://github.com/invictus717/MiCo)**|**我们提议构建全模态智能，旨在理解各种模态并学习通用表示。为此，我们提出了一种可扩展的预训练范式，称为多模态上下文（MiCo）。这种方法能够在预训练过程中同时增加模态数量、数据量以及模型参数的数量。通过MiCo，预训练模型在多项任务上展现出显著的多模态学习能力：一是针对10种不同模态的单模态感知基准，二是包括检索、问答和captioning在内的25项跨模态理解任务，三是18个多模态大语言模型基准。我们的模型创造了37项最新的最高性能记录。我们期望这项研究能推动全模态智能的发展。相关代码和模型已在<https://github.com/invictus717/MiCo>开源。**|
|**2024-06-13**|**Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms**|Miaosen Zhang et.al.|[2406.09397](http://arxiv.org/abs/2406.09397)|null|现代视觉模型在大规模嘈杂数据集上进行训练，虽然展现出强大能力，但在遵循用户意图、如视觉美感、特定风格和责任输出方面可能存在问题。本文关注视觉美学领域，目标是使视觉模型与人类审美标准在检索系统中保持一致。高级检索系统通常采用基于低级特征（如饱和度）的审美模型作为重排器或过滤器，但面对风格、文化或知识背景时性能有限。我们发现利用大型语言模型（LLM）的推理能力，通过改写搜索查询并扩展审美期望，可以弥补这一不足。  因此，我们提出了一种基于偏好的强化学习方法，该方法针对视觉模型进行微调，以提取LLM推理和审美模型的知识，从而更好地使视觉模型符合人类审美。由于缺乏专门用于评估检索系统的基准，我们利用强大的多模态大模型（LMM）来评价美感表现。考虑到美感评估的主观性，我们还提出了一个名为HPIR的新数据集，用于衡量与人类审美的契合度。实验结果显示，我们的方法显著提升了视觉模型的美感行为，从多个指标来看。我们相信，提出的算法可以作为一种通用实践，用于使视觉模型与人类价值观相一致。|
|**2024-06-13**|**Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA**|Jongwoo Park et.al.|[2406.09396](http://arxiv.org/abs/2406.09396)|**[link](https://github.com/jongwoopark7978/LVNet)**|长期视频通常包含大量冗余信息，跨越较长的时间间隔，且包含多个松散关联的事件或实体。因此，在进行长视频问答（LVQA）时，生成正确答案所需的所有信息往往只需一小部分帧就足以提供。近期的研究试图利用大型语言模型（LLMs）在LVQA基准上取得卓越性能，但这些模型依赖于视觉语言模型（VLMs）将视频中的所有视觉内容转换成自然语言。传统做法通常是均匀采样大量帧并独立为其生成描述，这既不高效也不免有冗余。针对这一问题，我们探索了关键帧选择和顺序感知的描述方法，以显著减少这些冗余。  为此，我们提出了两个创新方法：层次关键帧选择器和顺序视觉语言模型。我们的最终框架称为LVNet，在三个基准LVQA数据集上实现了最先进的性能。我们将公开我们的代码。|
|**2024-06-13**|**Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs**|Zijia Zhao et.al.|[2406.09367](http://arxiv.org/abs/2406.09367)|**[link](https://github.com/joez17/videoniah)**|**视频理解是大规模多模态语言模型（MLLMs）的关键下一步。为了检验视频理解的特定方面，现有的视频基准通常需要精心选择与目标能力匹配的视频，并对查询-响应对进行繁琐的标注，以匹配视频内容。这个过程既具有挑战性又资源密集。本文提出VideoNIAH（视频针 haystack），一个通过合成视频生成的基准构建框架。VideoNIAH通过将不相关的图像/文本“针”插入原始视频中，将测试视频内容与它们的查询-响应分离。它仅基于这些针生成注释，确保视频来源的多样性和查询-响应的丰富性。此外，通过插入多个针，VideoNIAH严格评估模型的时序理解能力。我们利用VideoNIAH构建了视频基准VNBench，包括检索、排序和计数等任务。VNBench能够高效地评估视频模型的精细理解能力和时空建模能力，同时支持长距离依赖性的评估。我们还对近期的视频为中心的多模态大型语言模型进行了评估，包括开源和专有模型，提供了全面的分析。尽管专有模型相对于开源模型具有显著优势，但所有现有视频模型在长距离依赖任务上的性能仍然不佳。VideoNIAH是一个简单且高度可扩展的基准构建框架，我们相信它将激发未来视频基准工作的创新。代码和数据已在https://github.com/joez17/VideoNIAH上提供。**|
|**2024-06-13**|**ElicitationGPT: Text Elicitation Mechanisms via Language Models**|Yifan Wu et.al.|[2406.09363](http://arxiv.org/abs/2406.09363)|null|该论文探讨了如何利用无需领域知识的查询来大型语言模型（如ChatGPT）对获取的文本预测进行评分，以评估其与实际状态的一致性。这种方法是激励信息收集和机器学习模型训练的关键组成部分。研究通过在同行评审数据集上进行实验，比较自动的模型评分与人工导师给出的评分，旨在实证评估这些机制与人类偏好的一致性。|
|**2024-06-13**|**DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding**|Suwon Shon et.al.|[2406.09345](http://arxiv.org/abs/2406.09345)|null|## 背景  将预训练的文本型大型语言模型（LLMs）与语音输入相结合，已经赋予了这些模型执行多样化语音任务的能力，包括指令跟随。这种整合需要结合语音编码器、语音适配器和LLM，它们分别针对不同的任务进行训练。我们提议使用离散语音单元（DSU），而非连续值的语音编码输出，通过语音适配器将DSU转换到LLM的嵌入空间。我们通过无监督的语音编码器生成DSU，然后运用k-means聚类方法。提出的模型在处理来自见/未见过领域以及口语问答中的指令跟随任务时表现出稳健性能。我们还研究了来自不同自监督语音编码器层的DSU类型，以及梅尔频率倒谱系数（MFCC）。实验结果表明，在口语问答的指令调优任务中，ASR任务和数据集的重要性可能较低。|
|**2024-06-13**|**REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space**|Tomer Ashuach et.al.|[2406.09325](http://arxiv.org/abs/2406.09325)|null|大型语言模型（LLMs）可能无意中记住并泄露训练数据中的敏感或个人识别信息（PII），引发隐私问题。当前的解决方案包括昂贵的数据清洗，或者通过遗忘和模型编辑来过滤模型，但这些方法可能被提取攻击绕过。我们提出了一种新颖的模型编辑方法，名为REVS，用于从LLMs中消除敏感信息。REVS识别并修改与每条敏感信息相关的少量神经元。通过将这些神经元投影到词汇空间（去嵌入），我们定位驱动其生成的关键部分。然后，我们根据去嵌入矩阵的伪逆计算模型编辑，并应用它来降低目标敏感数据的生成概率。为了充分评估我们的方法在真正敏感信息上的效果，我们创建了两个数据集：一个是GPT-J固有的电子邮件数据集，另一个是我们调整模型使其记忆的合成社会保障号码数据集。与最先进的模型编辑方法相比，REVS在消除敏感信息和抵抗提取攻击方面表现出色，同时保持模型的完整性。代码和演示笔记本可在<https://technion-cs-nlp.github.io/REVS>获取。|
|**2024-06-13**|**Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs**|Zhao Xu et.al.|[2406.09324](http://arxiv.org/abs/2406.09324)|**[link](https://github.com/usail-hkust/bag_of_tricks_for_llm_jailbreaking)**|**尽管大型语言模型（LLMs）在零样本任务执行方面展现出显著能力，但它们易受破解攻击，可能被操纵产生有害输出。近期的研究开始将破解攻击分为令牌级和提示级。然而，先前的工作主要忽视了破解攻击的多样关键因素，大部分研究聚焦于LLM的漏洞，而对防御增强的LLMs探索不足。为了改进这一状况，我们评估了不同攻击设置对LLM性能的影响，并提议建立一个基准测试框架，以促进标准化评估。我们从目标级和攻击级两个角度，详细考察了实施针对LLMs的破解攻击的八个关键因素。我们在两个常用数据集上对六种防御方法进行了七种代表性的破解攻击，总计约320个实验，使用A800-80G GPU耗时大约5万小时。实验结果强调了对防御增强的LLMs进行标准化评估的必要性。我们的代码已开源：https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking。**|
|**2024-06-13**|**JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models**|Delong Ran et.al.|[2406.09321](http://arxiv.org/abs/2406.09321)|**[link](https://github.com/thuccslab/jailbreakeval)**|**本文探讨了针对大型语言模型（LLMs）的越狱攻击研究中的评估难题。目前，对于攻击是否成功缺乏统一标准，不同的评估方法如人工标注或特定方式提示GPT-4存在，各有优缺点，对人类价值观的体现和研究成本产生影响。我们的研究分析了近九十项2023年5月至2024年4月期间发布的越狱攻击相关研究，提出了一种详细的评估方法分类体系，深入剖析了各种评估器的优缺点及其应用现状。为了推动后续研究，我们开发并推出了JailbreakEval工具包，它是一个用户友好的平台，集成了多种知名的评估器，用户只需一个命令即可获取结果。此外，JailbreakEval支持用户在统一框架内定制自定义评估流程，简化了开发和比较过程。总之，我们期望JailbreakEval能促进越狱攻击评价的标准化，成为社区内越狱研究评估的催化剂。**|
|**2024-06-12**|**Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens**|Ting-Ji Huang et.al.|[2406.08477](http://arxiv.org/abs/2406.08477)|null|在推荐系统中，通过向量表示用户和项目对于多种任务至关重要。最近的研究尝试将大型语言模型（LLMs）应用于问答形式的推荐，使用词汇表内的标记（如“item”、“20”、“24”）来表示实际的用户和项目。然而，由于LLMs通常是在自然语言任务上预训练的，这些词汇表内的标记在表达独特用户和项目方面能力有限，即使经过推荐任务的微调，也会削弱推荐性能。本文探讨如何有效在LLM基的推荐系统中处理用户和项目的标记。  我们强调了出词汇表（OOV）标记的作用，它们除了词汇表内的标记外，还能捕捉用户/项目之间的关联性和多样性。通过分析历史用户-项目交互的表示学习，我们使具有相似特性的用户/项目组合共享相同的OOV标记。此外，将这些OOV标记整合到LLM的词汇表中，有助于更好地区分用户和项目，增强在下游任务微调时对用户-项目关系的捕捉。  我们的提出的框架在各种下游推荐任务上超越了现有最先进的方法。|
|**2024-06-12**|**Real2Code: Reconstruct Articulated Objects via Code Generation**|Zhao Mandi et.al.|[2406.08474](http://arxiv.org/abs/2406.08474)|null|我们提出了一种新颖的方法——Real2Code，旨在通过代码生成来重建可动物体。给定物体的视觉观测，我们首先利用图像分割模型和形状补全模型重构其部件几何结构。接着，我们将物体部件表示为带有方向的边界框，然后输入到一个经过微调的大语言模型（LLM）中，预测关节活动的代码表示。通过利用预训练的视觉和语言模型，我们的方法能够优雅地扩展到具有更多可动部件的对象，并能从合成训练数据中泛化到现实世界中的不规则环境物体。实验结果表明，Real2Code在重建精度上显著优于现有最先进的方法，并且是首个能够超越训练集中对象结构复杂性的方法，能够重建多达10个可动部件的物体。当与立体重建模型结合时，Real2Code还能从少量多视图RGB图像中泛化到现实世界的物体，无需深度或相机信息。|
|**2024-06-12**|**Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**|Zhangchen Xu et.al.|[2406.08464](http://arxiv.org/abs/2406.08464)|**[link](https://github.com/magpie-align/magpie)**|高质量的指令数据对于调整大型语言模型至关重要。尽管像Llama-3-Instruct这样的模型公开了权重，但它们的对齐数据仍然保密，这限制了人工智能的普及。现有的开源数据生成方法受限于高昂的人力成本和有限的提示范围，难以有效扩展，可能影响公共对齐数据集的多样性和质量。能否通过直接从已对齐的大型语言模型中提取，大规模合成高质指令数据呢？我们提出了一种自我合成方法，称为Magpie。我们的关键观察是，由于Llama-3-Instruct等已对齐的模型具有自回归特性，当我们仅输入左侧模板到用户消息预留位置时，它们可以生成用户查询。我们利用这种方法提示Llama-3-Instruct，生成了400万个指令及其对应的响应。我们对提取的数据进行了全面分析，并选择了30万个高质量实例。为了比较Magpie数据与其他公共指令数据集，我们分别使用每个数据集对Llama-3-8B-Base进行微调，并评估微调后模型的性能。结果显示，在某些任务中，仅使用Magpie进行微调的模型在性能上与官方经过1000万个数据点监督微调（SFT）和后续反馈学习增强的Llama-3-8B-Instruct相当。我们还展示了仅使用Magpie进行SFT可以超越先前用于SFT和偏好优化（如UltraFeedback的直接偏好优化）的公共数据集。这种优势在AlpacaEval、ArenaHard和WildBench等对齐基准测试中表现明显。|
|**2024-06-12**|**TasTe: Teaching Large Language Models to Translate through Self-Reflection**|Yutong Wang et.al.|[2406.08434](http://arxiv.org/abs/2406.08434)|**[link](https://github.com/yutongwang1216/reflectionllmmt)**|**大型语言模型在自然语言处理任务中展现出卓越性能，特别是通过指令调优后，在机器翻译（Machine Translation, MT）等下游任务中的表现有所提升。然而，这些方法未能达到与监督神经机器翻译（Supervised Neural Machine Translation, NMT）系统相当的翻译质量。原因可能是当前使用的简单提示无法充分利用模型的指令跟随能力。为此，我们提出了TasTe框架，即“通过自我反思进行翻译”。该框架包括两个推理阶段：第一阶段，模型被引导生成初步翻译并同时对其自身进行评估；第二阶段，模型根据评估结果对初步翻译进行细化。在WMT22基准的四种语言方向上，我们的方法显示出与现有技术相比的有效性。这项工作展示了一种有前景的方法，能够释放大型语言模型的潜力，并增强其在机器翻译领域的性能。相关代码和数据已在https://github.com/YutongWang1216/ReflectionLLMMT上开源。**|
|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|Zijin Hong et.al.|[2406.08426](http://arxiv.org/abs/2406.08426)|null|文本转SQL生成准确的SQL查询以响应自然语言问题是一个长期存在的挑战，它涉及用户问题理解、数据库模式理解以及SQL生成等多个复杂环节。传统的文本转SQL系统依赖于人工工程和深度神经网络。随着预训练语言模型（PLMs）的发展和在该任务中的应用，性能得到了显著提升。然而，随着数据库复杂度增加和用户问题难度增大，PLMs有限的理解能力可能导致错误的SQL生成，这促使研究人员寻求更高级和定制化的优化方法，限制了PLM基础系统的广泛应用。最近，大型语言模型（LLMs）因其在自然语言理解上的强大能力而备受瞩目。因此，整合LLM的实现为文本转SQL研究带来了独特的机遇、挑战和解决方案。本综述全面概述了基于LLM的文本转SQL。首先，我们概述当前面临的挑战和文本转SQL的发展历程。接着，详细介绍用于评估文本转SQL系统的数据集和评价指标。然后，我们系统分析了近期在LLM支持下的文本转SQL进展。最后，我们讨论了该领域尚存的挑战，并对未来研究方向提出期待。|
|**2024-06-12**|**OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text**|Qingyun Li et.al.|[2406.08418](http://arxiv.org/abs/2406.08418)|**[link](https://github.com/opengvlab/omnicorpus)**|**该论文介绍了一种名为OmniCorpus的大型图像-文本交错数据集，规模达到100亿级别。这个数据集通过高效的引擎筛选和提取了大量高质量文档，包含86亿张图片和1,696万亿个文本令牌，相较于同类数据（如MMC4、OBELICS），OmniCorpus具有以下优势：1）规模扩大15倍，同时保持了良好的数据质量；2）来源更为多样，包括英文和非英文网站，以及视频为主的网站；3）灵活性更强，可以从图像-文本交错格式轻松转换为纯文本语料库或图像-文本对。通过全面分析和实验，论文验证了OmniCorpus的数据质量、可用性和有效性，旨在为未来的多模态模型研究提供坚实的数据基础。相关的代码和数据已在https://github.com/OpenGVLab/OmniCorpus上公开。**|
|**2024-06-12**|**Discovering Preference Optimization Algorithms with and for Large Language Models**|Chris Lu et.al.|[2406.08414](http://arxiv.org/abs/2406.08414)|**[link](https://github.com/luchris429/DiscoPOP)**|****中文翻译：**  离线偏好优化是提升和控制大型语言模型（LLM）输出质量的重要方法。传统上，偏好优化被视为基于人工设计的凸损失函数的离线监督学习任务。然而，这些方法受限于人类创造力，未能充分探索可能的损失函数的巨大搜索空间。为此，我们提出了一种利用LLM进行目标发现的方法，以自动发现新的最先进的偏好优化算法，无需（专家）人工干预。具体来说，我们通过迭代地提示LLM，根据先前的性能评估提出并实现新的偏好优化损失函数。这个过程导致了未知且高效的优化算法的发现。其中最好的一个被命名为“发现偏好优化”（DiscoPOP），这是一种新颖的算法，它巧妙地融合了逻辑和指数损失。实验结果表明，DiscoPOP在性能上达到了最新水平，并成功地应用于未见过的任务上。**|
|**2024-06-12**|**Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference**|Christopher Wolters et.al.|[2406.08413](http://arxiv.org/abs/2406.08413)|null|## 背景  大型语言模型（LLMs）近期在自然语言处理领域取得了显著进步，使得机器能够生成逼真的文本并进行有意义的对话。然而，随着计算和内存需求的急剧增长，尤其是当LLMs超越单个GPU的处理能力时，对速度、效率和可访问性的需求也随之增加。同时，计算机性能和内存能力的发展并未跟上步伐，尤其是在摩尔定律放缓的背景下。内存访问成本远高于计算，这给大规模扩展带来了挑战，即所谓的“内存墙”。在这个时候，计算在内存（Compute-in-Memory, CIM）技术为AI推理提供了加速可能，通过在内存中直接执行模拟计算，有望降低延迟和功耗。通过紧密集成内存和计算元件，CIM消除了冯诺依曼瓶颈，减少了数据传输，提高了能源效率。  本综述论文概述了基于变压器的模型，探讨了各种CIM架构，并研究了它们如何应对现代人工智能计算系统面临的紧迫挑战。我们详细讨论了与变压器相关的运算及其硬件加速策略，同时指出相关CIM设计中的挑战、趋势和洞察。|
|**2024-06-12**|**Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**|Chun-Yi Kuan et.al.|[2406.08402](http://arxiv.org/abs/2406.08402)|**[link](https://github.com/kuan2jiu99/audio-hallucination)**|**## 背景 大型音频语言模型（LALMs）通过整合音频感知能力，增强了传统的大规模语言模型，使其能够处理音频相关任务。先前的研究主要集中在评估LALMs在各种任务上的性能，但对它们的可靠性，特别是关于对象幻觉等问题的关注不足。我们的研究中，我们提出方法来评估公开可用的LALMs在对象幻觉方面的程度。结果表明，LALMs在理解音频内容方面与专门的音频captioning模型相当，但在回答区分性问题时表现不佳，尤其是那些需要识别音频片段中特定物体声音的问题。这揭示了当前LALMs的一个关键弱点：它们对区分性查询的理解不足。此外，我们还探讨了提示工程如何提升LALMs在区分性问题上的性能。**|
|**2024-06-12**|**cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers**|Anirudh Sundar et.al.|[2406.08398](http://arxiv.org/abs/2406.08398)|null|## 背景 在情境化和多模态交互对话（SIMMC）的新兴研究领域中，科学论文的互动是一个重要方向。由于科学论文主要由文本、公式、图表和表格构成，SIMMC方法需要针对这些组成部分进行专门设计，以支持科研人员所需的深度探究和互动。本研究提出了一种名为“对话式论文”（cPAPERS）的数据集，它包含了来自arXiv上可用的科学文档的学术论文评论中的问答对，这些问答与论文组件及其引用相关。我们介绍了数据收集策略，通过OpenReview收集这些问题-答案对，并与LaTeX源文件中的上下文信息关联起来。此外，我们展示了使用大型语言模型（LLMs）的一系列基线方法，包括零样本和微调配置，来处理cPAPERS数据集。|
|**2024-06-11**|**Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena**|Aidar Myrzakhan et.al.|[2406.07545](http://arxiv.org/abs/2406.07545)|**[link](https://github.com/vila-lab/open-llm-leaderboard)**|**### 背景  多项选择题（MCQ）常用于评估大型语言模型（LLMs）。通常，LLM会根据调整后的概率，如长度因素，选择最可能的答案。然而，LLMs可能存在固有的偏见，例如对A、B、C、D等选项ID的偏好，这可能影响答案预测。先前的研究通过在少数测试样本上随机打乱选项，并将其应用到新样本上，试图减少这种“选择偏差”。此外，MCQ的另一个问题是“彩票式猜测”，即LLM并未真正学习知识，而是凭运气猜对答案，这对小型LLMs尤为严重。  为解决这些问题，一个更全面的方法是转向开放式问题，这能从根本上消除选择偏差和随机猜测。但转向开放式问题也带来了挑战：一是如何识别合适的开放性问题，二是如何验证LLM对开放式问题的回答与人类标注的真实答案之间的准确性。本研究旨在解决这些难题，并建立一个新的LLM评估基准，通过完全的开放式问题来衡量模型性能，例如GPT-4o/4/3.5、Claude 3、Gemini等。  ### 任务  我们创建了Open-LLM-Leaderboard，这是一个新的评价平台，旨在跟踪各种LLM的表现，揭示它们的真实能力。我们的代码和数据集已开源，可在此链接获取：https://github.com/VILA-Lab/Open-LLM-Leaderboard。**|
|**2024-06-11**|**QuickLLaMA: Query-aware Inference Acceleration for Large Language Models**|Jingyao Li et.al.|[2406.07528](http://arxiv.org/abs/2406.07528)|**[link](https://github.com/dvlab-research/q-llm)**|**大型语言模型（LLMs）在理解和处理长序列方面的能力对于各领域的发展至关重要。然而，它们在捕捉序列中的长期依赖关系以深入理解语义方面仍然存在挑战。为此，我们提出了Query-aware Inference for LLMs（Q-LLM），这是一种旨在模仿人类认知处理大规模序列的系统。通过聚焦于与给定查询相关的内存数据，Q-LLM能够在固定窗口大小内准确捕捉相关信息，并为查询提供精确的答案，无需额外训练，可无缝集成到任何LLMs中。使用LLaMA3（QuickLLaMA）的Q-LLM能在30秒内阅读《哈利·波特》，并能准确回答问题。相较于当前最先进的LLaMA3，Q-LLM的性能提升了7.17%，而在Mistral上，它在 $\infty$ -bench上的表现提升了3.26%。在“针锋相对”任务中，Q-LLM在广泛认可的基准上，相对于当前最佳成绩，Mistral上的提升达到了7.0%，在LLaMA3上实现了100%的准确率。我们的代码已在https://github.com/dvlab-research/Q-LLM上开源。**|
|**2024-06-11**|**Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement**|Yunzhen Feng et.al.|[2406.07515](http://arxiv.org/abs/2406.07515)|null|随着生成模型合成数据的兴起，越来越多地被用于大型语言模型的微调，这引发了对模型崩溃（即微调性能下降）的关注。由于人类和机器都较容易分辨好样本和坏样本，而非生成高质量样本，我们探讨了如何利用反馈来防止模型在合成数据上出现崩溃。我们理论分析了一个高斯混合分类模型在基于反馈增强的合成数据训练下的最优性能，并提供了有限样本情况下的实验证据。我们在两个实际问题上展示了这些理论预测：使用变压器计算矩阵特征值和利用大型语言模型进行新闻摘要，这两种情况下模型在生成数据上都会经历崩溃。我们发现，通过从反馈增强的合成数据中训练，无论是修剪错误预测还是选择最佳猜测，都能防止模型崩溃，证实了像RLHF（Reinforcement Learning with Human Feedback）这样的流行方法的有效性。|
|**2024-06-11**|**THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report**|KBTG Labs et.al.|[2406.07505](http://arxiv.org/abs/2406.07505)|null|## 背景  近期大型语言模型（LLMs）的进步在科技领域展现了新功能和机遇。然而，非常大的LLMs的实际应用受到其高计算成本的制约，这与其相对有限的人类能力相比，收益并不明显。尽管小型、更实用的LLMs在金融分析方面展现出潜力，但它们尚未完全掌握，如它们在模拟特许金融分析师（CFA）考试中的接近通过表现所示。本文中，我们展示了Financial Analyst Extension（FAE）对我们的Text Hyperlocally Augmented Large Language Extension（THaLLE）系列的扩展，这一系列80亿参数的LLMs在模拟CFA考试中始终表现出最高性能，与同类规模的模型相比。我们详细记录了用于优化的微调技术，以供后续研究参考。此外，我们引入Flare CFA，这是一个公开可用的金融顾问评估数据集，用于检验LLMs在财务顾问角色中的能力。|
|**2024-06-11**|**Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions**|Renjie Pi et.al.|[2406.07502](http://arxiv.org/abs/2406.07502)|**[link](https://github.com/sterzhang/image-textualization)**|**## 背景  图像描述数据集对于推动图像理解、文本到图像生成和文本图像检索等应用至关重要。当前，这些数据集主要来自两个途径：一是从网络上抓取图像与文字对，但这类描述往往质量较低且存在噪声；二是人工标注，如COCO等，通常描述简洁，缺乏详细信息。尽管详细的图像描述可以通过人类标注获得，但高昂的标注成本限制了其可行性。这些局限性促使我们寻求更有效和可扩展的方法来生成准确而详尽的图像描述。  本文提出了一种创新框架，称为“图像文本化”（Image Textualization，简称IT），它通过协同利用现有的多模态大型语言模型（Multimodal Large Language Models，MLLMs）和视觉专家模型，有效地将视觉信息转化为文本，从而自动生成高质量的图像描述。针对当前缺乏详尽描述的基准问题，我们还提出了多个评价基准，以全面评估我们的框架生成的图像描述质量。  此外，我们展示了在IT精心编纂的描述训练下，LLaVA-7B模型的图像描述生成能力得到了提升，能够生成更丰富的描述，输出长度和细节显著增加，同时减少了幻觉现象。**|
|**2024-06-11**|**TextGrad: Automatic "Differentiation" via Text**|Mert Yuksekgonul et.al.|[2406.07496](http://arxiv.org/abs/2406.07496)|**[link](https://github.com/zou-group/textgrad)**|**人工智能正经历一场范式转变，通过大型语言模型（LLMs）和其他复杂组件的协同工作取得了突破。当前，为复合人工智能系统设计原则化的自动化优化方法成为一项关键新挑战。神经网络在早期面临类似问题时，通过反向传播和自动微分实现了重大革新。受此启发，我们提出了TextGrad，这是一个强大的框架，它通过文本实现自动“微分”，将LLMs提供的丰富、通用的自然语言建议回传到复合AI系统的各个组件中。TextGrad遵循PyTorch的语法和抽象，易于使用且灵活，用户仅需提供目标函数，无需调整框架组件或提示，即可无缝应用。  TextGrad适用于多种任务，从问答和分子优化到放射治疗计划设计。在无需修改框架的情况下，它显著提升了GPT-4o在Google证明性问题回答中的零-shot准确率，从51%提升至55%；在优化LeetCode难题解法上实现了20%的相对性能提升；改进了推理提示，设计出具有理想体外亲和力的新药候选分子；以及设计出具有高特异性的放射治疗方案。TextGrad为下一代AI系统的发展奠定了基础，推动了复合AI技术的加速发展。**|
|**2024-06-12**|**CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization**|Frederic Kirstein et.al.|[2406.07494](http://arxiv.org/abs/2406.07494)|null|该文章综述了2019年至2024年间发表的1262篇独特的研究论文，集中在Transformer架构在英文对话摘要生成方面的研究。文章详细探讨了对话摘要中存在的主要挑战，如语言理解、结构处理、理解能力、说话者识别、重要性判断和事实准确性，并与相应的技术，如图解方法、额外训练任务和规划策略进行了关联。尽管在某些方面（如语言）取得了显著进展，但如理解力、真实性与重要性评估等挑战仍然存在，提供了丰富的研究空间。  文章还分析了评估这些方法的方式，涵盖了对话子领域（如会议、医疗）的常用数据集，以及自动评价指标（如ROUGE）和人类评估的普遍实践。然而，发现跨领域的数据集相对有限，且报告的人类评估往往缺乏足够的内审员一致性信息和标注指南细节。此外，文章讨论了大语言模型的最新探索可能带来的影响，指出尽管它们可能会改变相关性和难度，但描述的挑战分类体系仍然具有价值。|
|**2024-06-11**|**PITCH: Productivity and Mental Well-being Coaching through Daily Conversational Interaction**|Adnan Abbas et.al.|[2406.07485](http://arxiv.org/abs/2406.07485)|null|高效的计划制定对生产力和心理健康至关重要，但人们往往难以制定实际的计划并反思自己的效率。利用人工智能的发展，对话助手作为一种有前景的工具，旨在通过对话方式将计划外化，强化决心，促进专注行动，从而正面影响生产力和心理健康。我们的研究目标是设计一个对话助手，通过自然对话的社交互动性，提供深入的问题和反思提示，以提高计划执行度。尽管先前的研究显示了这些代理的效益，但许多干预措施仍保持静态，可能导致用户参与度随时间下降。为了弥补这一不足，我们提出了一种新颖的旋转和上下文感知的提示策略，每天为用户提供多样的干预手段。我们的系统PITCH利用大语言模型（LLMs）来促进日常计划的外部化和反思。本研究旨在探究与对话代理一起外化任务对生产力和心理健康的影响，以及旋转策略在保持用户参与度方面的有效性。|
|**2024-06-11**|**Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing**|Mao Li et.al.|[2406.07483](http://arxiv.org/abs/2406.07483)|null|在快速发展的自然语言处理领域，大型语言模型（LLMs）在社交媒体帖子的自动文本标注方面展现出浓厚兴趣。本文研究了八种开源和专有LLMs在立场标注任务中的性能，将其与人类（通过众包）的判断进行基准测试。我们探究了何时LLMs可能与人类判断产生分歧的情况。研究发现，文本中表达立场的明确程度对LLMs判断与人类一致性至关重要。当人类注释者表现良好时，LLMs也表现出色；反之，LLMs的失败往往对应于人类难以达成一致的情境。因此，我们建议结合人类专业知识的精确度与LLMs预测的规模，提出一种全面的方法。这项研究强调了提高自动化立场检测准确性和全面性的必要性，旨在推动这些技术在更高效、无偏见的社会媒体分析中得到提升。|
|**2024-06-11**|**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**|Zesen Cheng et.al.|[2406.07476](http://arxiv.org/abs/2406.07476)|**[link](https://github.com/damo-nlp-sg/videollama2)**|**本文介绍VideoLLaMA 2，一套专为提升视频和音频定向任务中的空间-时间建模及音频理解能力而设计的视频大型语言模型（Video-LLMs）。它在前一代的基础上增添了定制的时空卷积（STC）连接器，有效地捕捉视频数据的复杂空间和时间动态。此外，我们通过联合训练融入了音频分支，增强了模型的多模态理解能力，使其能无缝融合音频线索。在多项评估中，如多选视频问答（MC-VQA）、开放性视频问答（OE-VQA）和视频captioning（VC）任务上，VideoLLaMA 2表现出与开源模型相当的竞争实力，并在某些基准上接近专有模型。在音频仅用（AQA）和音频-视频问答（OE-AVQA）任务上，VideoLLaMA 2也显示出对现有模型的合理改进。这些进步凸显了VideoLLaMA 2在多模态理解方面的卓越性能，为智能视频分析系统树立了新标准。所有模型均公开以促进进一步研究。**|
|**2024-06-10**|**Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation**|Peize Sun et.al.|[2406.06525](http://arxiv.org/abs/2406.06525)|**[link](https://github.com/foundationvision/llamagen)**|**我们提出LlamaGen，这是一种全新的图像生成模型家族，它将大型语言模型的原始“下一个词预测”范式应用于视觉生成领域。这表明，如果适当扩展，未经视觉特性的先验知识增强的纯自回归模型（如Llama）也能达到最先进的图像生成性能。我们的研究探索了图像分词器的设计空间、图像生成模型的可扩展性以及训练数据质量，结果如下：(1) 一种具有16倍下采样的图像分词器，其在ImageNet基准上的重构质量为0.94，代码书利用率高达97%。(2) 一系列从111百万到31亿参数的类条件图像生成模型，在ImageNet 256x256基准上实现了2.18的FID分数，超越了流行的扩散模型，如LDM和DiT。(3) 一个7.75亿参数的文本条件图像生成模型，通过两阶段训练在LAION-COCO和高审美质量图像上，显示出良好的视觉质量和文本一致性性能。(4) 我们验证了大语言模型服务框架在优化图像生成模型推理速度方面的有效性，实现了326%至414%的速度提升。我们开源所有模型和代码，以促进视觉生成和多模态基础模型的开放源代码社区的发展。**|
|**2024-06-10**|**UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor**|Shivani Upadhyay et.al.|[2406.06519](http://arxiv.org/abs/2406.06519)|**[link](https://github.com/castorini/umbrela)**|**## 翻译  大量相关性判断对于检索系统的有效训练和精确评估至关重要。传统上，这些判断由人工评定员完成，过程昂贵且耗时。微软Bing的Thomas等人最近的一项研究表明，大型语言模型（LLMs）能够准确地进行相关性评估，提供与人类相当的判断。遗憾的是，他们的研究并未公开可供重复使用的软件工具。我们的工作介绍了一个开源工具包——UMBRELA（全称为“UMBRELA是Bing RELevance Assessor的递归缩写”），它基于OpenAI的GPT-4模型复现了Thomas等人的结果，并为原论文增添了更多细节。我们在TREC 2019年至2023年的深度学习任务中发现，LLM生成的相关性判断与高效多阶段检索系统生成的排名高度相关。该工具包设计为易于扩展，可以融入现有的多阶段检索和评估流程，为研究检索评估方法的研究者提供了宝贵的资源。UMBRELA将在TREC 2024年的RAG任务中用于辅助相关性评估，我们期望它成为该领域进一步创新的基础。UMBRELA的代码库可于https://github.com/castorini/umbrela获取。**|
|**2024-06-10**|**NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative**|Asmar Nadeem et.al.|[2406.06499](http://arxiv.org/abs/2406.06499)|null|当前的视频字幕基准和模型在表征因果时间叙事方面存在不足，这种叙事是通过因果关系连接的一系列事件，随时间发展，由人物或主体驱动。这种缺乏叙事性限制了模型生成捕捉视频内容内在因果和时间动态的文本描述的能力。为填补这一空白，我们提出NarrativeBridge，它包括以下两个组成部分：（1）一个由大型语言模型通过少量提示生成的新型因果时间叙事（CTN）字幕基准，该基准明确地在视频描述中编码因果关系，通过自动评估确保质量和相关性；（2）一个专门的因果网络（CEN）架构，具有独立的编码器以分别捕获因果动态，从而实现有效的学习和生成具有因果时间叙事的字幕。实验结果表明，CEN在表达视频内容的因果和时间方面比第二好的模型（GIT）更准确：在MSVD和MSR-VTT数据集上的CIDEr分数分别为17.88和17.44。提出的框架能够理解和生成具有复杂因果时间叙事结构的细微文本描述，这是视频字幕生成的一个关键局限性。有关项目详情，请访问<https://narrativebridge.github.io/>。|
|**2024-06-10**|**Towards a Personal Health Large Language Model**|Justin Cosentino et.al.|[2406.06474](http://arxiv.org/abs/2406.06474)|null|在健康领域，大部分大型语言模型（LLM）的研究集中在临床任务上。然而，移动和可穿戴设备提供的丰富、长期的个人健康监测数据往往被忽视。本文介绍了一种名为Personal Health Large Language Model（PH-LLM）的新模型，它是Gemini的定制版，专为理解和处理数值时间序列的个人健康数据而设计。我们创建并整理了三个测试集，考察了PH-LLM在以下方面的性能：1）从睡眠模式、身体活动和生理反应中生成个性化见解和建议；2）专业知识领域的专家水平；3）预测自我报告的睡眠结果。我们与领域专家合作构建了857个案例研究，以评估实际的睡眠和健身场景。通过针对特定领域的评分标准进行全面评估，我们发现Gemini Ultra 1.0和PH-LLM在健身方面与专家表现无统计差异，尽管在睡眠方面专家仍占优势，但Fine-tune后的PH-LLM在利用相关领域知识和个人化睡眠信息方面表现出显著提升。我们还通过多项选择的睡眠医学和健身考试评估了PH-LLM的专业知识，其得分分别为79%和88%，超过了人类专家样本的平均分。最后，我们训练PH-LLM预测来自可穿戴设备文本和多模态编码数据的自我报告睡眠质量结果，并证明了多模态编码对于达到专门区分模型的性能至关重要。尽管在个人健康这个关键安全领域还需要进一步发展和评估，但这些结果展示了Gemini模型的广泛知识和能力，以及将生理数据应用于个人健康应用，如PH-LLM中的做法。|
|**2024-06-10**|**AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction**|Zhen Xing et.al.|[2406.06465](http://arxiv.org/abs/2406.06465)|null|文本引导的视频预测（TVP）任务旨在根据初始帧和指令预测后续帧的运动，这对于虚拟现实、机器人技术和内容创作等领域具有广泛的应用。尽管先前的方法通过改编Stable Diffusion在该任务上取得了重大进展，但它们在帧一致性与时间稳定性方面仍存在问题，主要受限于视频数据集的规模。我们观察到，预训练的Image2Video扩散模型对视频动态有良好的先验知识，但缺乏文本控制。因此，将Image2Video模型转移，同时注入指令控制以生成可控制的视频，既具有意义又颇具挑战。  为了实现这一目标，我们提出了多模态大型语言模型（MLLM），用于根据初始帧和文本指令预测未来的视频状态。特别地，我们设计了双查询Transformer（DQFormer）架构，它将指令和帧信息整合到条件嵌入中，用于未来帧的预测。此外，我们开发了长短期时序适配器和空间适配器，能够在少量训练成本下快速将通用视频扩散模型适应特定场景。  实验结果表明，我们的方法在Something Something V2、Epic Kitchen-100、Bridge Data和UCF-101四个数据集上显著优于现有技术。特别是在Bridge数据集和SSv2上，AID分别实现了91.2%和55.5%的FVD改进，这证明了其在不同领域的有效性。更多示例可在我们的网站<https://chenhsing.github.io/AID>找到。|
|**2024-06-10**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-11**|**Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies**|Junlin Wang et.al.|[2406.06461](http://arxiv.org/abs/2406.06461)|null|这篇论文指出，尽管已经提出了多种推理策略来评估大型语言模型的能力，但传统的评价方法仅关注性能指标，忽视了一个关键因素：额外计算资源带来的增效。这可能导致对策略效率的片面理解。为此，论文提出了一种框架，将计算预算纳入评估，以提供一个既考虑性能指标又考虑计算成本的更全面比较。通过这种预算意识的视角，研究发现复杂的推理策略在没有显著算法创新的情况下，往往由于分配了更多的计算资源而超越了简单的基线。例如，当给予链式思考自洽性（chain-of-thought self-consistency）类似级别的计算资源，它常常能优于文献中提出的推理策略。然而，在这种规模敏感的视角下，某些策略如多代理辩论或多反思在增加计算预算时可能会表现得更差。|
|**2024-06-10**|**Evaluating the Retrieval Component in LLM-Based Question Answering Systems**|Ashkan Alinejad et.al.|[2406.06458](http://arxiv.org/abs/2406.06458)|null|## 背景  大规模语言模型（LLMs）驱动的问答系统在依赖检索组件时，能够获取领域特定信息并降低产生不准确回复或错误信息的风险。尽管信息检索领域的评估方法早已存在，但如何评估LLMs驱动的聊天机器人中的检索器性能仍是一个挑战。本研究提出了一种简单的基准方法，用于评价基于检索增强生成（Retrieval-Augmented Generation，RAG）的聊天机器人中的检索器。  ## 任务  我们的研究发现，这种方法能更全面地反映检索器的性能，并与整个问答系统的整体表现更为一致。尽管传统的精确度（precision）、召回率（recall）和F1分数等指标可能无法完全揭示LLMs的能力，因为它们可能会在检索器不完美时仍提供准确答案，但我们的评估方法考虑到了LLMs的优势，即它们能够忽略无关上下文，同时也能处理可能存在的错误和虚构内容。|
|**2024-06-10**|**A Large Language Model Pipeline for Breast Cancer Oncology**|Tristen Pool et.al.|[2406.06455](http://arxiv.org/abs/2406.06455)|null|大型语言模型在众多领域展现出创新潜力，但在癌症治疗方面的应用仍需进一步开发。研究者使用一种新颖的Langchain提示工程管道，对最先进的OpenAI模型进行了微调，数据集包括临床数据和临床指南文本，专注于乳腺癌患者辅助放疗和化疗两个关键治疗因素。结果显示，模型在分类这两个治疗手段时达到了高精度（0.85+）。通过观察人类肿瘤学家的治疗质量数据，建立了一个置信区间，估计模型在预测治疗方案时必须比原始肿瘤学家表现得更好，才能在总体上成为更好的解决方案的比例为8.2%至13.3%。由于癌症治疗决策结果的不确定性，未来可能需要进行临床试验来验证这一阈值。考虑到美国85%的癌症患者在地方社区设施接受治疗，这类模型有可能显著扩大优质护理的可及性，其效果至少接近人类肿瘤学家。|
|**2024-06-10**|**Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course**|Aadarsh Padiyath et.al.|[2406.06451](http://arxiv.org/abs/2406.06451)|null|大型语言模型（LLMs）在代码生成、调试和解释方面的性能引发了许多研究者和教育工作者对本科编程教育的关注，他们期待这些模型能革新编程教学。然而，关于如何以及为何在编程教育中使用LLMs的决策可能不仅仅基于技术评估。本研究以社会塑造技术理论为指导框架，探讨了学生对LLMs的社会感知如何影响他们的使用行为。我们通过分析一份匿名的课程结束时的调查问卷（n=158）、中期自我效能问卷（n=158）、10位学生的深度访谈、自我报告的LLM在作业中的使用情况，以及期中考试成绩，发现学生的LLM使用与其对未来职业的期望和对同伴使用的感知有关。此外，我们发现早期自我报告的LLM使用与较低的自我效能和中期考试成绩相关，而学生对过度依赖LLM的感知，而非实际使用，与课程后期的自我效能下降有关。|
|**2024-06-07**|**3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs**|Jianing Yang et.al.|[2406.05132](http://arxiv.org/abs/2406.05132)|**[link](https://github.com/sled-group/3D-GRAND)**|在这个研究中，语言与三维感知的融合对于构建理解和互动于物理世界的实体代理和机器人至关重要。尽管大型语言模型（LLMs）在语言理解和生成方面表现出色，但在适应三维环境（3D-LLMs）方面仍处于初级阶段，主要挑战在于缺乏大规模的密集地将语言与三维场景关联的数据集。为此，我们提出了3D-GRAND，这是一个开创性的大型数据集，包含40,087个家庭场景，配对有620万条详尽的场景-语言指令。实验结果显示，使用3D-GRAND进行指令调优显著提高了3D-LLMs的定位能力，并减少了错误的想象。我们还设计了3D-POPE基准，用于系统性评估3D-LLMs中的幻觉问题，以促进未来模型的公平比较。  我们的实验揭示了数据集规模与3D-LLM性能之间的关联，强调了大型三维文本数据集在推动体感AI研究中的关键作用。值得注意的是，初步迹象表明，通过在大型合成数据上训练的模型可能在现实世界3D扫描中表现良好，这展示了模拟到实际的迁移学习潜力。通过3D-GRAND和3D-POPE，我们旨在为体感AI社区提供必要的资源和洞见，推动更可靠、更扎实的3D-LLMs的发展。项目网站：https://3d-grand.github.io|
|**2024-06-07**|**An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models**|Xiongtao Zhou et.al.|[2406.05130](http://arxiv.org/abs/2406.05130)|**[link](https://github.com/alenai97/peft-mllm)**|这篇论文关注的是大型多模态语言模型（MLLMs）的参数高效微调（PEFT）。由于这些模型通常具有数十亿参数，全面调整变得困难。研究目标是找出在参数受限情况下提升MLLM性能的有效方法。通过实验使用四种流行的PEFT技术对开源MLLMs的LLM组件进行微调，论文进行了详尽的分析，内容包括不同方法对模型、参数位置、微调数据规模、模型稳定性、泛化能力以及幻觉的影响。研究涵盖了两种类型的七项数据集：未见过的和已见过的。结果显示，适配器是最有效的PEFT方法，而连接器层的微调在大多数情况下能提高性能。研究代码和数据可在<https://github.com/alenai97/PEFT-MLLM.git>获取。|
|**2024-06-07**|**Towards Semantic Equivalence of Tokenization in Multimodal LLM**|Shengqiong Wu et.al.|[2406.05127](http://arxiv.org/abs/2406.05127)|null|### 背景  多模态大型语言模型（MLLMs）在处理视觉语言任务方面展现出卓越性能。MLLM的核心在于视觉 tokenization，即如何有效地将输入的视觉信号转化为对语言模型有益的特征表示。然而，现有的视觉tokenizer在保持视觉与语言的语义一致性上存在问题，它们过于碎片化视觉输入，破坏了视觉内容的语义完整性。为解决这一问题，本文提出了一种新颖的动态语义等效视觉tokenizer（SeTok），它通过动态聚类算法将视觉特征组织成语义单元，根据图像复杂性灵活决定token的数量。这种生成的视觉tokens能有效保持语义完整性，同时捕捉低频和高频视觉特征。  ### 任务  我们提出了一种名为Setokim的新型MLLM，它结合了SeTok。实验结果表明，Setokim在各种任务上表现出显著的优势。关于更多详情，可以访问项目网页：https://chocowu.github.io/SeTok-web/。|
|**2024-06-07**|**LINX: A Language Driven Generative System for Goal-Oriented Automated Data Exploration**|Tavor Lipman et.al.|[2406.05107](http://arxiv.org/abs/2406.05107)|null|## 翻译  数据探索是一个复杂的过程，用户通过逐步执行一系列查询来审视数据集。有时，用户会探索新数据以熟悉它，但更多时候，探索过程是围绕特定分析目标或问题进行的。为了帮助用户有效探索，已提出自动化数据探索（Automated Data Exploration，ADE）系统，它们旨在自动生成展示数据有趣特性的完整探索流程。然而，现有的ADE系统常受限于预定义的优化函数，导致对同一数据集始终产生相同的探索序列，这在有明确目标的探索中显得不足。为此，本文提出LINX，一个结合自然语言接口的生成式系统，专注于面向目标的数据探索。  LINX接受输入数据集和用自然语言描述的分析目标，生成与用户需求相关的个性化探索会话。系统利用大型语言模型解析输入的分析目标，并据此生成期望输出探索会话的规范。这些规范随后被传递给基于约束深度强化学习（Constrained Deep Reinforcement Learning，CDRL）的新型模块化ADE引擎，使其能根据指定指令调整输出。为了验证LINX的效果，我们创建了一个新的面向目标探索的基准数据集，并进行了深入的用户研究。实验结果表明，LINX生成的探索笔记本在相关性和实用性上显著优于现有解决方案，包括ChatGPT、无目标导向的ADE以及商业系统。|
|**2024-06-07**|**Multi-Head RAG: Solving Multi-Aspect Problems with LLMs**|Maciej Besta et.al.|[2406.05085](http://arxiv.org/abs/2406.05085)|**[link](https://github.com/spcl/mrag)**|**## 背景  **增强型检索生成（Retrieval Augmented Generation, RAG）**通过将文档内容融入大语言模型（Large Language Models, LLMs）的上下文中，提高了其响应的准确性和相关性。然而，现有的RAG方法并未充分处理那些可能需要检索包含不同内容的多文档查询。这类问题在现实中很常见，但挑战在于，这些文档的嵌入在向量空间中可能相距较远，难以一次性获取。本文提出了一种新的方案——**多头检索增强生成（Multi-Head RAG, MRAG）**，它以一种简单而强大的方式解决这个问题：利用Transformer的多头注意力层的激活作为检索键，而非解码层。这个想法的驱动力在于，不同的注意力头能够学习捕捉数据的不同方面。通过利用这些激活，我们得到的嵌入能代表数据项和查询的多种特性，从而提升复杂查询的检索精度。  **贡献**  我们提供了评估方法、度量标准、合成数据集以及实际应用案例，来展示MRAG的有效性。与标准RAG基线相比，MRAG在相关性方面的提升可高达20%。MRAG可以无缝融入现有的RAG框架，如RAGAS，以及各类数据存储系统。  总结，本文旨在改进现有RAG模型，以更好地处理涉及多角度信息检索的复杂查询任务。**|
|**2024-06-07**|**Are Large Language Models More Empathetic than Humans?**|Anuradha Welivita et.al.|[2406.05063](http://arxiv.org/abs/2406.05063)|null|随着大型语言模型（LLMs）的兴起，研究它们是否能在情感识别和共情回应方面超越人类已成为研究焦点。本论文开展了一项深入研究，对比了包括GPT-4、LLaMA-2-70B-Chat、Gemini-1.0-Pro和Mixtral-8x7B-Instruct在内的四款最先进的LLMs与人类在共情回应能力上的表现。我们通过一项涉及1,000名参与者的双盲用户研究，对2,000个精心挑选的情感对话提示进行了分析，这些提示涵盖了32种不同正负情绪的广泛范围。研究结果显示，LLMs的共情回应能力在统计学上优于人类。GPT-4表现出最强烈的共情，其“好”等级别的回复比人类基准提高了约31%。紧随其后的是LLaMA-2，提升了约24%，Mixtral-8x7B提升了约21%，Gemini-Pro提升了约10%。我们还对回复评级进行了更详细的分析，发现某些LLMs在回应特定情绪方面明显优于其他模型。提出的评估框架提供了一种可扩展且适应性强的方法，用于评估新LLMs的共情能力，避免了未来研究重复这项研究的必要性。|
|**2024-06-07**|**Robustness Assessment of Mathematical Reasoning in the Presence of Missing and Contradictory Conditions**|Shi-Yu Tian et.al.|[2406.05055](http://arxiv.org/abs/2406.05055)|null|大型语言模型在推理任务上表现出色，通过少量示例提示可以进一步提升性能。然而，当前的评估主要集中在精心构建的基准上，忽视了现实世界中存在缺失和矛盾条件的推理问题，即所谓的不明确问题。我们的观察表明，现有的少量提示方法在这种情况下效果不佳，往往给出过度自信的答案或错误推断。为了深入研究这个问题，我们创建了一个名为“带有缺失和矛盾条件的问题”（PMC）的基准，并引入了两个新指标来评估少量提示方法在处理这类问题时的表现。使用PMC基准的分析揭示了在解决明确问题的数学推理性能与识别不明确问题能力之间存在权衡。针对PMC带来的挑战，我们提出了一种新颖的少量提示方法，称为SMT-LIB提示（SLP）。这种方法利用SMT-LIB语言描述问题，而不是直接求解，然后采用双重检查求解策略验证解决方案的满足性和唯一性，从而提供最终反馈。实验结果全面展示了我们的SLP方法在处理带有缺失和矛盾条件的问题时，相较于现有方法具有显著优势。我们将开源我们的基准和代码，以促进未来的研究。|
|**2024-06-07**|**Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation**|Nachiket Kotalwar et.al.|[2406.05053](http://arxiv.org/abs/2406.05053)|null|### 概述  生成式人工智能和大型语言模型在编程教育中的潜力巨大，它们能够为学习者提供个性化的反馈和提示。当前的研究主要集中在提升生成反馈的质量，以达到人类导师的水平。然而，在实际教育部署中，除了质量外，成本、时间及数据隐私也是关键考量因素。本论文旨在对语言模型在编程反馈生成方面的性能进行全面评估，包括质量、成本、速度和数据隐私等多个维度。我们特别关注利用最新的在浏览器内推理技术，这有助于直接降低成本并保护数据隐私。  为了优化适合浏览器内运行的小型模型的反馈质量，我们开发了一种基于GPT-4生成的合成数据的微调流程。我们将展示如何使用WebLLM的浏览器内推理引擎来优化Llama3-8B和Phi3-3.8B的4位量化模型在三个不同Python编程数据集上的效果。我们承诺会公开全部实现、web应用和数据集，以促进在浏览器语言模型领域的进一步研究。|
|**2024-06-07**|**Bootstrapping Referring Multi-Object Tracking**|Yani Zhang et.al.|[2406.05039](http://arxiv.org/abs/2406.05039)|**[link](https://github.com/zyn213/temprmot)**|## 背景 当前的多对象引用跟踪（RMOT）任务通常依赖于手动标注的数据集和静态规则，这限制了多样性和实施范围。为了解决这个问题，我们的研究主要关注通过引入更多区分性语言词汇来推动RMOT任务的发展。为此，我们首先对Refer-KITTI数据集进行了扩展，创建了Refer-KITTI-V2，它从最初的2,719个手动标注开始，解决了类别不平衡问题，并增加了更多关键词，使其更贴近现实场景，相较于Refer-KITTI有所进步。我们进一步利用大型语言模型扩充这些标注，总计达到9,758个，生成了617个不同的词汇，超越了先前的RMOT基准。  此外，我们还改进了RMOT的端到端框架，采用了一个简单而优雅的时序推进策略，该策略在性能上优于先前的方法。相关源代码和数据集已可在<https://github.com/zyn213/TempRMOT>获取。|
|**2024-06-07**|**Scenarios and Approaches for Situated Natural Language Explanations**|Pengshuo Qiu et.al.|[2406.05035](http://arxiv.org/abs/2406.05035)|null|大型语言模型（LLMs）能够生成适应不同用户情境的自然语言解释（NLE）。然而，对于这种适应性的量化评估尚存空白。为此，我们创建了一个基准数据集——基于情境的解释（Situation-Based Explanation，SBE）数据集，包含100个需要解释的事物（explanandum）。每个事物都配对了针对教师、学生和专业人士等不同受众群体的解释，以便评估模型在满足这些多元化群体信息需求和背景下的解释精准度，如学生、教师和家长。每种“事例-受众”组合都附有人类撰写的参考解释，用于计算分数，以量化模型如何根据情境调整解释。我们在不同规模的预训练语言模型上测试了三种提示方法：规则基础提示、元提示和上下文学习提示。研究发现：1）模型可以通过生成提示产生更精确地符合目标情境的解释；2）明确提示“你是一个有用的助手”并非针对情境化NLE任务的必要技术；3）上下文学习提示仅能帮助模型学习演示模板，但无助于提升其推理性能。SBE数据集和我们的分析为今后生成适应情境的自然语言解释的研究提供了基础。|
|**2024-06-06**|**Verbalized Machine Learning: Revisiting Machine Learning with Language Models**|Tim Z. Xiao et.al.|[2406.04344](http://arxiv.org/abs/2406.04344)|null|受大型语言模型（LLMs）取得的巨大进展启发，我们提出了口头化机器学习（VML）框架。与传统的机器学习模型，通常在连续参数空间中优化不同，VML将参数空间限制为人可理解的自然语言。这种约束促使我们从新角度看待函数逼近问题，即将带有文本提示的LLM视为由文本提示参数化的函数。我们借此视角重新审视了经典机器学习任务，如回归和分类，发现这些问题可以通过LLM参数化的学习器和优化器来解决。VML的主要优势包括：（1）易于编码先验知识：关于问题和假设类的先验知识可以以自然语言形式编码并输入给LLM参数化的学习器；（2）自动模型选择：优化器可以根据数据和口头化先验知识自动选择具体的模型类别，并在训练过程中更新模型类别；（3）可解释的学习者更新：LLM参数化的优化器可以解释每次学习者更新的原因。我们进行了多项实验评估VML的有效性，希望它能成为增强机器学习可解释性和信任度的桥梁。|
|**2024-06-06**|**RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation**|Jiaming Liu et.al.|[2406.04339](http://arxiv.org/abs/2406.04339)|null|在机器人操作的核心目标中，让模型理解视觉场景并执行动作是一个基本任务。尽管现有的机器人多模态大型语言模型（MLLM）能够处理一些基础任务，但它们在两个方面仍面临挑战：1）处理复杂任务的推理能力不足；2）对于MLLM的微调和推理存在高计算成本。近期提出的基于状态空间模型（SSM）的Mamba展示了在非平凡序列建模方面的潜力，具有线性推理复杂度。在此启发下，我们开发了RoboMamba，一个端到端的机器人MLLM，它利用Mamba模型结合机器人推理和动作能力，同时保持高效的微调和推理效率。  首先，我们将视觉编码器与Mamba集成，通过联合训练使视觉数据与语言嵌入对齐，赋予模型视觉常识和与机器人相关的推理能力。为了进一步提升RoboMamba的动作姿态预测能力，我们探索了一种高效的微调策略，仅使用简单的策略头。实验表明，一旦RoboMamba具备足够的推理能力，只需极少的微调参数（模型的0.1%）和时间（20分钟），就能习得操纵技能。在实验中，RoboMamba在通用和机器人评估基准上展现出卓越的推理能力。同时，我们的模型在模拟和真实世界实验中实现了姿态预测的出色表现，其推理速度比现有机器人MLLM快7倍。项目的网页链接为：<https://sites.google.com/view/robomamba-web>。|
|**2024-06-06**|**Coherent Zero-Shot Visual Instruction Generation**|Quynh Phung et.al.|[2406.04337](http://arxiv.org/abs/2406.04337)|null|尽管文本到图像合成技术取得了进步，特别是在扩散模型方面，但生成需要物体在连续步骤中保持一致表示和平滑状态转换的视觉指令仍然是一项艰巨挑战。本文提出了一种无需训练的框架，巧妙地结合了文本理解与图像生成，以确保视觉指令既美观又具有连贯性和准确性。通过测试多步骤指令，并与多个基线进行比较，我们验证了这种方法的有效性。实验结果显示，我们的方法能够生成连贯且视觉上吸引人的指令。|
|**2024-06-06**|**DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs**|Lingchen Meng et.al.|[2406.04334](http://arxiv.org/abs/2406.04334)|null|大多数大型多模态模型（LMMs）通过将视觉令牌作为序列输入到大型语言模型（LLMs）的第一层来实现。这种方法虽然直观，但会显著增加计算和内存开销，因为模型需要处理更多的输入层令牌。本文提出了一种新的架构DeepStack，用于LMMs。在LMM的视觉和语言Transformer的N层中，我们将视觉令牌分为N组，并从底层逐层向上馈送到对应的Transformer层。令人惊讶的是，这种简单的方法极大地增强了LMM在跨层视觉令牌交互方面的建模能力，同时成本几乎不变。我们分别将DeepStack应用于LMM的语言和视觉Transformer，并通过广泛实证结果验证了DeepStack LMM的有效性。  使用相同的上下文长度，我们的DeepStack 7B和13B参数模型在9个基准测试上平均超越同类模型2.7分和2.9分。仅使用五分之一的上下文长度，DeepStack的表现接近于使用完整上下文长度的模型。这些提升在高分辨率任务中尤为明显，例如，与LLaVA-1.5-7B相比，TextVQA、DocVQA和InfoVQA上的性能分别提高了4.2分、11.0分和4.0分。此外，我们还将DeepStack应用到视觉Transformer层，这带来了与LLaVA-1.5-7B相当的平均改进，为3.8分。|
|**2024-06-06**|**PaCE: Parsimonious Concept Engineering for Large Language Models**|Jinqi Luo et.al.|[2406.04331](http://arxiv.org/abs/2406.04331)|**[link](https://github.com/peterljq/parsimonious-concept-engineering)**|**大型语言模型（LLMs）被广泛应用于各种任务，尽管它们能够生成类似人类的回复，但也会产生不良输出，如潜在有害信息、种族或性别歧视性言论以及错误的信息。为了减少这些问题，研究人员开发了对齐方法，如微调、提示工程和表示工程。然而，现有方法面临挑战：一些需要针对每个对齐任务进行昂贵的微调；一些未能充分消除不良概念，对齐效果不佳；一些则删除了良性的概念，降低了LLMs的语言能力。为此，我们提出了名为Parsimonious Concept Engineering（PaCE）的新型激活工程框架，旨在解决这些问题。  首先，我们构建了一个大规模的概念字典，它在激活空间中表示每个原子对应一个语义概念。接着，对于给定的任何对齐任务，我们会使用一个概念分区器高效地标记这些概念为良性或不良。在推理阶段，我们利用稀疏编码方法，根据概念字典分解LLM的激活，将其准确表示为良性成分和不良成分的线性组合。通过移除不良成分，我们能够调整LLMs的行为以符合对齐目标。  我们在回应净化、真实性增强和情感修订等任务上进行了实验，并发现PaCE在实现对齐性能的同时，保持了良好的语言能力，达到了当前最先进的水平。**|
|**2024-06-06**|**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step**|Zhanhao Liang et.al.|[2406.04314](http://arxiv.org/abs/2406.04314)|null|## 背景  近期，Direct Preference Optimization (DPO) 已成功扩展到调整文本到图像的扩散模型，使其与人类偏好保持一致。不同于大多数现有 DPO 方法假设所有扩散步骤都与最终生成图像保持一致的偏好顺序，我们认为这种假设忽略了每个步骤特有的去噪性能，因此应该为每一步定制偏好标签。为此，我们提出了一种新颖的后训练方法——Step-aware Preference Optimization (SPO)，它独立评估并调整每个步骤的去噪性能，利用步级感知偏好模型和步级重采样器来确保准确的步级监督。  在SPO中，我们在每个去噪步骤中会创建一个图像池，寻找合适的胜者-败者对，并且关键在于，我们会从池中随机选择一个图像作为下一次去噪步骤的起点。这个步级重采样过程保证了每次胜者-败者对都来自同一原始图像，使得比较独立于前一步。为了评估每个步骤的偏好，我们训练了一个专门的步级感知偏好模型，适用于模糊和清晰的图像。在Stable Diffusion v1.5和SDXL等实验中，SPO 显著优于最新的Diffusion-DPO，尤其是在处理复杂、详细的提示时，能更好地生成图像并提升美学效果，同时在训练效率上超过20倍。代码和模型可在此链接获取：[https://rockeycoss.github.io/spo.github.io/](https://rockeycoss.github.io/spo.github.io/)。|
|**2024-06-06**|**Semantically Diverse Language Generation for Uncertainty Estimation in Language Models**|Lukas Aichberger et.al.|[2406.04306](http://arxiv.org/abs/2406.04306)|**[link](https://github.com/ml-jku/SDLG)**|**大型语言模型（LLMs）在生成文本时可能会出现幻觉，这阻碍了社会和工业中的各种应用，因为它们会降低LLMs的可信度。当前的LLMs采用自回归方式生成文本，即预测并添加文本标记。当LLMs对生成的下一个标记的语义含义不确定时，很可能会产生幻觉。因此，人们认为幻觉源于预测不确定性。我们提出了“语义多样性语言生成”（Semantically Diverse Language Generation，SDLG），用于量化LLMs的预测不确定性。SDLG引导LLM生成语义多样但又合理的初始文本替代方案，从而提供了精确的aleatoric语义不确定性测量，能够检测初始文本是否可能出现幻觉。  实验在问答任务上表明，SDLG始终优于现有方法，并且在计算效率上最为高效，为LLMs的不确定性估计设定了新的标准。**|
|**2024-06-06**|**Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models**|Phat Nguyen et.al.|[2406.04300](http://arxiv.org/abs/2406.04300)|null|在模拟训练和评估关键安全系统，如自动驾驶车辆时，通过模拟生成各种场景至关重要。然而，模型其他车辆的轨迹以模拟复杂且有意义的近距离交互任务成本高昂。利用语言描述来生成驾驶行为是一种有前景的方法，它提供了一种可扩展且直观的人类操作方式，能够模拟广泛驾驶互动。但大型标注的语言-轨迹数据稀缺是这一方法面临的挑战。为此，我们提出了Text-to-Drive（T2D），这是一种利用大型语言模型（LLMs）合成多样化驾驶行为的技术。我们的方法采用知识驱动两阶段策略：首先，利用LLMs的内置知识生成丰富多样的驾驶行为语言描述；接着，利用其推理能力在模拟器中实现这些行为。T2D的核心是使用LLM构建状态图，将低级状态映射到高级抽象，从而简化了诸如总结低级观测、评估策略与行为描述的一致性以及设计辅助奖励等下游任务，无需人工监督。通过我们的知识驱动方法，我们证明T2D能生成比其他基准更丰富的轨迹，并提供一个自然语言界面，允许用户交互式地融入人类偏好。更多示例请访问我们的网站：<https://text-to-drive.github.io/>|
|**2024-06-07**|**What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages**|Nadav Borenstein et.al.|[2406.04289](http://arxiv.org/abs/2406.04289)|null|## 背景  大型语言模型能够学习什么？根据定义，语言模型（LM）是字符串的分布。因此，可以将这个问题转化为评估字符串分布类的学习能力。尽管先前的研究主要关注理论限制，但我们关注的是实际可学习性。不同于以往的实证工作，我们评估神经语言模型在其“主场”——学习概率语言——上的表现，而不是作为形式语言的分类器。具体来说，我们研究递归语言模型（RLM）由循环神经网络（RNN）和Transformer LM学习的可行性。我们通过实验测试RLM的可学习性，考察其与RLM的复杂参数以及神经LM隐藏层大小的关系。实验结果显示，RLM的秩（对应于其条件分布对数似然线性空间的大小）和采样字符串的预期长度是RNN和Transformer LM可学习性的强且显著预测因素。其他一些预测指标也达到了显著性，但RNN和Transformer之间存在不同的模式。|
|**2024-06-06**|**Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People**|Dun-Ming Huang et.al.|[2406.04278](http://arxiv.org/abs/2406.04278)|**[link](https://github.com/jacobyn/SamplingTonesACL)**|**## 翻译后的中文摘要  对话语气在人际交流中至关重要。随着大型语言模型（LLMs）的日益普及，研究它们与人类交流语气的差异变得尤为重要。然而，当前关于对话模式的研究往往依赖于预先存在的分类体系或文本语料库，这些可能存在实验者偏见，并可能无法充分反映研究领域中的真实世界分布。受认知科学方法的启发，我们提出一种迭代方法，通过交替进行两项任务来同时揭示语气和句子：（1）参与者判断给定句子的语气，（2）另一参与者根据该语气生成句子。我们在人类参与者和GPT-4之间进行了100轮这样的互动，从而获得了一组包含句子和常见对话语气的数据。我们还进行了额外实验，让人类和GPT-4对所有句子标注所有语气。基于1,339名人类参与者、33,370次人类评价以及29,900个GPT-4查询的数据，我们展示了如何使用这种方法创建一个可解释的几何表示，以展示人类和GPT-4之间的对话语气关系。这项工作展示了机器学习和认知科学理念如何结合，以解决人机交互中的挑战。**|
|**2024-06-05**|**Wings: Learning Multimodal LLMs without Text-only Forgetting**|Yi-Kai Zhang et.al.|[2406.03496](http://arxiv.org/abs/2406.03496)|null|## 任务  多模态大型语言模型（MLLMs）起源于预训练的通用语言模型，首先将图像与文本对齐，然后在混合模态输入上进行微调。然而，MLLM在处理仅包含文本的指令时会出现灾难性的遗忘，这些文本指令并未包含图像，这些问题在初始的语言模型阶段就已经存在。本文提出Wings，一个新型的MLLM，它在文本对话和多模态理解方面表现出色。通过分析MLLM在多模态指令中的注意力，我们发现文本遗忘与从图像前向图像后的注意力转移有关。因此，我们构建了额外模块作为增强学习器，以补偿这种注意力转移。视觉和文本学习器作为“翅膀”式的补充，平行连接在每个注意力块内，起初图像和文本输入由视觉学习器与主注意力协同工作，平衡对视觉元素的关注。随后，文本学习器通过注意力路由的方式与视觉学习器的输出协作整合。我们设计了低秩残差注意力（LoRRA）机制以保证学习器的高效运行。  实验结果表明，Wings在文本对话和视觉问答任务上优于同等规模的MLLM。在我们新构建的交错图像-文本（IIT）基准测试中，Wings在从文本为主到多模态为主的问答任务中展现出卓越性能。|
|**2024-06-06**|**Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training**|Ao Sun et.al.|[2406.03488](http://arxiv.org/abs/2406.03488)|**[link](https://github.com/maydomine/seq1f1b)**|大型语言模型（LLMs）的兴起在很大程度上依赖于分布式训练策略，其中管道并行性起着关键作用。随着LLMs的训练序列长度扩展到32k甚至128k，当前的管道并行方法面临严重瓶颈，如高内存占用和显著的管道延迟，这极大地限制了模型的可扩展性和训练吞吐量。为了提高内存效率和训练效率，我们提出了一种针对长序列训练LLMs的高效序列级一次前向一次后向（1F1B）管道调度方法，称为Seq1F1B。Seq1F1B将批级别可调度单元分解为更细的序列级单元，从而减小延迟并降低内存需求。  考虑到如果均匀分割序列，Seq1F1B可能会产生轻微的额外延迟，我们设计了一种基于计算的策略来划分输入序列，以缓解这个副作用。与竞争性的管道基线方法，如Megatron的1F1B管道并行相比，我们的方法在保持更高训练吞吐量的同时，内存占用更低。值得注意的是，Seq1F1B能够在不使用重新计算策略的情况下，有效地在64个NVIDIA A100 GPU上训练一个具有300亿参数的LLM，处理长达64k的序列，这是现有方法无法实现的。我们的代码基于Megatron-LM，并已开源：https://github.com/MayDomine/Seq1F1B.git。|
|**2024-06-05**|**Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends**|Sanjana Ramprasad et.al.|[2406.03487](http://arxiv.org/abs/2406.03487)|null|### 翻译  近期的大型语言模型（LLMs）的进步显著提升了摘要生成系统的性能，但它们在真实性方面的问题引起了关注。尽管之前的研究广泛评估了新闻领域的LLMs，对话摘要的评价主要集中在基于BART的模型上，这在我们理解它们的可信度方面留下了空白。本研究旨在评估LLMs在对话摘要中的真实性，通过人类标注，并着重于识别和分类句级不一致。我们特别关注GPT-4和Alpaca-13B这两款主流模型。我们的评估揭示了错误定义的微妙之处：LLMs常常生成看似合理的推断，这些推断依赖于对话中的间接证据，而缺乏直接证据，这在旧模型中较少见。我们提出了一种改进的错误分类体系，引入了“情境推理”类别来归类这些LLM行为，并公开了相关数据集。利用我们的分类体系，我们比较了LLMs与老式微调模型之间的行为差异。此外，我们系统地评估了自动错误检测方法在LLM摘要上的效果，发现它们在识别这类细微错误时表现不佳。为此，我们提出了两种基于提示的精细错误检测方法，这两种方法优于现有指标，特别是在识别“情境推理”错误时。|
|**2024-06-05**|**BIPED: Pedagogically Informed Tutoring System for ESL Education**|Soonwoo Kwon et.al.|[2406.03486](http://arxiv.org/abs/2406.03486)|null|大型语言模型（LLMs）显示出巨大的潜力，能够作为经济且易于获取的英语第二语言（L2）学习者对话式智能辅导系统（CITS）。然而，现有的CITS往往只能教授简单概念，或者在教学深度上无法满足不同学习策略的需求。为了开发一个更具教育学导向、能教授复杂概念的CITS，我们构建了一个双语教育指导对话数据集（BIPED），包含一对一的人类英语辅导互动。通过对辅导对话的后处理分析，我们提炼出一套包含34种教师行为和9种学生行为的对话动作词典，并将其用于进一步标注收集的数据。根据先预测合适的教师行为再生成相应回复的两步框架，我们利用GPT-4和SOLAR-KO分别实现了两个CITS模型。实验结果表明，这些实施的模型不仅模仿了人类教师的风格，还运用了丰富且与上下文相适应的教学策略。|
|**2024-06-05**|**Does your data spark joy? Performance gains from domain upsampling at the end of training**|Cody Blakeney et.al.|[2406.03476](http://arxiv.org/abs/2406.03476)|null|随着大型语言模型（LLMs）的预训练数据集规模增长到万亿级别的tokens，这些数据集主要由大规模的CommonCrawl网络爬虫内容以及较小的领域特定数据组成。由于在大计算量（FLOPs）下训练以揭示模型在困难和新兴基准上的显著变化成本高昂，如何在通用网络抓取的多样性和领域特定信息密度之间找到最优平衡成为一个问题。本文展示了如何利用这些较小的领域特定数据，在训练后期对其进行上采样，从而在诸如MMLU、GSM8K和HumanEval等基准上提升性能。对于一个训练了1万亿（T）令牌的70亿参数模型，这种简单方法可使其性能提高6.90分、8.26分和6.17分，与训练时间两倍的Llama-2（7B）模型相当。我们研究了在训练后期领域上采样的持续时间，从5%到30%，发现10%到20%的比例最为合适，以平衡一般语言建模能力与特定任务的优化。此外，我们还利用领域上采样来大规模分析单个数据集对不同基准的增益，通过在这一阶段移除它们进行实验。这种方法极大地降低了实验成本，使得能够以预训练运行的十分之一左右的成本探索不同预训练数据集的影响。|
|**2024-06-05**|**AD-H: Autonomous Driving with Hierarchical Agents**|Zaibin Zhang et.al.|[2406.03474](http://arxiv.org/abs/2406.03474)|null|鉴于多模态大语言模型（MLLM）的强大功能，近期的研究聚焦于使用MLLM驱动的自动驾驶系统在大规模动态环境中。然而，常见的方法直接将高级指令转化为低级车辆控制信号，这违背了MLLM的本质生成模式，未能充分利用其潜在能力。因此，这些方法的一般化能力受到训练数据集的极大限制。为解决这个问题，我们提出通过中层语言驱动命令来连接高级指令和低级控制信号，它们比高级指令更细致，但比控制信号更通用且可解释，从而有效弥合两者之间的鸿沟。我们通过一个名为AD-H的分层多代理驾驶系统实现这一理念，包括一个用于高层推理的MLLM规划器和一个轻量级控制器进行低层执行。这种分层设计使MLLM摆脱了低级控制信号解码，充分释放了其在高层感知、推理和规划方面的涌现能力。  我们构建了一个带有动作层次注释的新数据集。全面的闭环评估显示，我们的AD-H系统具有多项关键优势。首先，AD-H在驾驶性能上显著优于现有方法，甚至展现出在车辆操作过程中自我纠正的能力，这是训练数据未涵盖的场景。其次，AD-H在长程指令和新环境条件下表现出色，明显超越当前最先进的方法。我们将公开我们的数据和代码，可通过<https://github.com/zhangzaibin/AD-H>获取。|
|**2024-06-05**|**What is the Best Way for ChatGPT to Translate Poetry?**|Shanshan Wang et.al.|[2406.03450](http://arxiv.org/abs/2406.03450)|null|本文研究了大型语言模型如ChatGPT在英语-中文诗歌翻译任务中的性能，通过定向提示和小样本场景分析以优化其表现。尽管初期结果令人鼓舞，但研究发现ChatGPT的翻译存在持续问题。为此，我们提出了“解释辅助诗歌机器翻译”（EAPMT）方法，该方法利用诗歌的单语解释作为翻译过程的指导。同时，我们改进了现有的评估标准，以更好地适应现代诗歌翻译的微妙之处。我们邀请专业诗人进行评估，并结合GPT-4的评价，结果显示，我们的EAPMT方法在与传统ChatGPT翻译方法以及现有在线系统的比较中表现出色。论文验证了我们方法的有效性，并为文学翻译的机器辅助提供了新颖视角。|
|**2024-06-05**|**Pre-trained Large Language Models Use Fourier Features to Compute Addition**|Tianyi Zhou et.al.|[2406.03445](http://arxiv.org/abs/2406.03445)|null|## 翻译  预训练的大型语言模型（LLMs）在数学推理方面表现出色，但它们如何执行基本的算术运算，如加法，仍不清楚。本文揭示了预训练的LLMs通过傅里叶特征进行加法——这些是隐藏状态中的维度，通过一组在频域中稀疏分布的特征来表示数字。在模型中，多层感知器（MLP）层和注意力层以互补的方式使用傅里叶特征：MLP层主要使用低频特征近似答案的大小，而注意力层主要通过高频特征执行模运算（例如判断答案是否为偶数）。预训练对于这种机制至关重要：从头开始训练的模型仅利用低频特征，导致准确性较低。将预训练的词嵌入引入到随机初始化的模型中可以恢复其性能。总的来说，我们的分析表明，适当的预训练表示（如傅里叶特征）能够解锁Transformer学习算法任务精确机制的能力。|
|**2024-06-05**|**Cycles of Thought: Measuring LLM Confidence through Stable Explanations**|Evan Becker et.al.|[2406.03441](http://arxiv.org/abs/2406.03441)|null|在许多高风险的机器学习应用中，模型需要能够表明其对预测的不确定性至关重要。尽管大型语言模型（LLMs）在各种基准上的准确度可达到甚至超过人类水平，但它们对错误响应的过度自信仍是已知的问题。传统的方法在直接应用于LLMs时可能面临计算成本和封闭源模型的挑战。近期提出了一些黑盒方法，但它们往往依赖于诸如自我表述的信心等启发式。我们提出了一种框架，通过分析模型生成答案的解释分布来衡量LLMs的不确定性。尽管利用解释本身并非新颖，但我们将其视为测试时间分类器，通过计算最可能的分类器后验答案分布，以此进行不确定性评估。  我们展示了使用解释蕴含作为分类器似然性的一种特定框架实例，如何在五个不同的数据集上改进了信心分数指标（特别是AUROC和AURC）。我们的结果表明，该框架既具有理论依据，又是有效量化LLMs不确定性的方式。|
|**2024-06-05**|**Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach**|Saehyung Lee et.al.|[2406.03411](http://arxiv.org/abs/2406.03411)|**[link](https://github.com/saehyung-lee/plugir)**|**该论文主要关注的是交互式文本到图像检索任务中的对话形式上下文查询问题。我们的方法论，名为PlugIR，通过两种方式有效地利用大型语言模型（LLMs）的一般指令跟随能力。首先，通过重述对话形式的上下文，我们消除了在现有视觉对话数据上微调检索模型的需求，从而能够使用任意黑盒模型。其次，我们设计了一个LLM提问者，根据当前上下文中候选图像的信息，生成关于目标图像属性的非冗余问题。这种方法减少了生成问题的噪声和冗余。除了我们的方法，我们还提出了一种新的评估指标，称为最佳对数排名积分（BRI），以全面评估交互式检索系统。PlugIR在多个基准测试中表现出优于零次设置和 Fine-tuned 基准的性能。此外， PlugIR 的两个组成部分可以根据不同情况灵活单独或结合应用。我们的代码已开源在：https://github.com/Saehyung-Lee/PlugIR。**|
|**2024-06-04**|**Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks**|Tianyu He et.al.|[2406.02550](http://arxiv.org/abs/2406.02550)|**[link](https://github.com/ablghtianyi/ICL_Modular_Arithmetic)**|**这篇工作研究了大型语言模型在一组模块化算术任务中出现的上下文学习和技能组合现象。我们关注的是有限数量的一次性模运算函数 $z = a \times x + b \times y \;(\text{mod}\; p)$，这些函数由向量 $(a, b) \in \mathbb{Z}_p^2$ 标记。部分任务被用作预训练，其余用于分布外测试。实验表明，GPT风格的Transformer随着预训练任务数量增加，其在分布内和分布外的泛化能力会经历转变。最小型能实现分布外泛化的模型需要两个Transformer块；而对于更深的模型，分布外泛化阶段是“瞬态”的，需要早期停止。最后，我们对预训练模型进行了可解释性分析，揭示了两种阶段中高度结构化的表示，并讨论了学习到的算法。**|
|**2024-06-04**|**Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning**|Alex Jinpeng Wang et.al.|[2406.02547](http://arxiv.org/abs/2406.02547)|**[link](https://github.com/showlab/VisInContext)**|**这段研究并未介绍最先进的多模态大语言模型（MLLM），而是提出了一种创新方法，旨在有效提升长序列在多模态模型中的处理。我们提出了“Visualized In-Context Text Processing”（VisInContext）技术，通过视觉令牌来处理长文本，从而显著降低GPU内存使用和浮点运算（FLOPs）在训练和推理阶段的需求。例如，对于一个560亿参数的混合 Experts（MOE）模型，我们的方法将预训练中的上下文文本长度扩展到了2048个tokens，而计算量几乎保持不变。实验结果显示，使用VisInContext训练的模型在常见的基于实例的少量数据评估下游任务中表现出色。此外，VisInContext与现有技术相结合，能增强对文档的理解能力，特别适用于文档问答和连续文档检索，显示出巨大的潜力。**|
|**2024-06-04**|**To Believe or Not to Believe Your LLM**|Yasin Abbasi Yadkori et.al.|[2406.02543](http://arxiv.org/abs/2406.02543)|null|我们研究大型语言模型（LLMs）中的不确定性量化，目标是识别对给定查询的响应时的不确定性程度。我们同时考虑了两种类型的不确定性：一种是知识性不确定性（例如对事实或语言真理的未知），另一种是不可消除的随机性（如可能的答案多样性）。特别是，我们提出了一种信息论指标，能够可靠地区分出只有知识性不确定性较大的情况，这时模型的输出是不可靠的。这个条件仅依赖于通过特殊迭代提示基于先前响应得到的模型输出来计算。这种量化方法可以检测单答和多答情况下是否存在虚构（即知识性不确定性高）的情况，这与许多标准的不确定性量化策略（如以响应的对数似然性作为阈值）不同，后者无法识别多答情况下的虚构。  我们进行了一系列实验，展示了我们的方法的优势。此外，我们的研究还揭示了LLM如何通过迭代提示放大对给定输出的概率分配，这可能具有独立的兴趣价值。|
|**2024-06-04**|**Loki: Low-Rank Keys for Efficient Sparse Attention**|Prajwal Singhania et.al.|[2406.02542](http://arxiv.org/abs/2406.02542)|**[link](https://github.com/hpcgroup/loki)**|针对大型语言模型的推理计算成本高昂，特别是当使用长序列时，自注意力机制是主要开销。为了解决这个问题，近期的研究提出了一些稀疏注意力近似方法。本文中，我们通过分析发现，注意力块中的键向量实际上处于一个远低于原始维度的空间。这一观察促使我们提出Loki，一种新的稀疏注意力方法。Loki根据在低维空间计算的注意力得分，对KV缓存中的令牌进行排序和选择。实验结果表明，Loki能够比其他流行近似方法更好地保持模型的效能，同时由于减少了数据移动（加载/存储）和计算成本，加速了注意力计算。|
|**2024-06-04**|**Parrot: Multilingual Visual Instruction Tuning**|Hai-Long Sun et.al.|[2406.02539](http://arxiv.org/abs/2406.02539)|**[link](https://github.com/aidc-ai/parrot)**|随着GPT-4V等多模态大型语言模型的快速发展，人工智能朝着通用人工智能迈出了重要一步。当前的方法主要依赖于监督微调（SFT）来同步视觉编码器与语言模型，从而赋予它们多模态能力。然而，这种做法可能导致随着训练的进行，语言模型处理多种语言的能力逐渐减弱。我们发现，以英语为中心的不平衡SFT数据集会导致非英语语言性能显著下降，原因在于SFT过程中未能有效连接视觉编码器和多语言令牌。为此，我们提出Parrot，一种利用文本引导在语言层面驱动视觉令牌对齐的新方法。Parrot通过让视觉令牌根据不同的语言输入进行条件化，并借助混合专家（MoE）促进多语言令牌的对齐。特别是，为了增强非英语视觉令牌的对齐，我们计算初始视觉特征与文本嵌入之间的跨注意力，然后将其输入到MoE路由器，选择最相关的专家。选定的专家会将初始视觉令牌转化为特定语言的视觉令牌。鉴于目前缺乏评估多语言能力的标准基准，我们还创建并公开了一个大规模多语言多模态基准（MMMB），包括6种语言、15个类别和12,000个问题。Parrot不仅在MMMB和MMM Benchmark上展现出最先进的性能，还在广泛的多模态任务中表现出色。我们将提供Parrot的源代码和训练数据集供公众使用。|
|**2024-06-04**|**Mitigate Position Bias in Large Language Models via Scaling a Single Dimension**|Yijiong Yu et.al.|[2406.02536](http://arxiv.org/abs/2406.02536)|**[link](https://github.com/PositionalHidden/PositionalHidden)**|这篇论文主要探讨了大型语言模型（LLMs）在实际应用中的一个现象——位置偏见，也称为"迷失在中间"。这种偏见在长文本情境中尤为明显，即关键信息在提示中的不同位置会显著影响模型的准确性。研究发现，注意力权重是位置偏见的微观表现。此外，论文指出，因果注意力掩码通过创建位置特定的隐藏状态，也对位置偏见有所贡献。  基于这些洞察，作者提出了一种方法来减轻位置偏见，即调整这些位置特定的隐藏状态。实验在多个任务上进行，包括自然问题多文档问答、键值检索、LongBench和时间线重排，涉及RoPE模型、扩展上下文窗口模型和Alibi模型等多种架构。结果显示，我们的方法通过仅修改隐藏状态的一个维度，就能实现性能提升，最高可达15.2%。研究者还提供了代码供进一步使用，代码地址为：https://aka.ms/PositionalHidden。|
|**2024-06-04**|**SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices**|Ruslan Svirschevski et.al.|[2406.02532](http://arxiv.org/abs/2406.02532)|**[link](https://github.com/yandex-research/specexec)**|随着大型语言模型的广泛应用，高效运行它们变得至关重要。近期的研究通过推测性解码实现了显著的速度提升。然而，大多数工作都是针对数据中心硬件进行设计。本研究反问：我们能在消费级设备上多快地运行LLMs？消费者级GPU已无法容纳最大的模型（500亿参数以上），因此需要将参数卸载到RAM或SSD。当使用卸载参数的方式运行时，推理引擎可以同时处理数百乃至数千个令牌的批次，使其非常适合推测性解码。我们提出SpecExec（推测性执行），这是一种简单的并行解码方法，适用于主流LLM家族，能生成每轮目标模型迭代高达20个令牌的预测。它利用现代LLMs中概率分布的高波动性和模型输出概率之间的高度一致性。SpecExec通过从草稿模型获取最可能的令牌延续，构建一个目标模型的“缓存”树，然后在一个单次遍历中验证。  使用SpecExec，我们在消费级GPU上实现了500亿参数LLM的推理，配合RAM卸载，4位量化下的速度达到4-6个令牌/秒，而16位权重下的速度为2-3个令牌/秒。|
|**2024-06-04**|**Scalable MatMul-free Language Modeling**|Rui-Jie Zhu et.al.|[2406.02528](http://arxiv.org/abs/2406.02528)|**[link](https://github.com/ridgerchu/matmulfreellm)**|**## 翻译  在大型语言模型（LLMs）中，矩阵乘法（MatMul）通常占据主要计算开销。随着LLMs的规模扩大，其嵌入维度和上下文长度也随之增加，这一问题更为显著。本文提出了一种方法，能够在保持强大性能的同时，完全移除LLMs中的MatMul操作，即使是在27亿参数量级的模型上也能实现。实验表明，我们的无MatMul模型在与内存消耗显著更多的状态-of-the-artTransformer相当的条件下表现出色。我们研究了模型的扩展性规律，并发现无MatMul模型与全精度Transformer之间的性能差距随着模型尺寸增大而减小。  此外，我们提供了一个高效的GPU实现，相较于未优化的基线，训练时能减少高达61%的内存使用。在推理阶段，通过优化的内核，我们的模型内存消耗可降低超过10倍。为了准确评估架构效率，我们在FPGA上构建了定制硬件解决方案，利用GPU无法处理的轻量级运算，实现了对十亿参数规模模型的高速处理，使其接近人脑级别的效率。  这项工作不仅展示了LLMs在减小复杂性后仍能保持高效，还指出了未来加速器应优化的运算类型，以适应下一代轻量级LLMs的需求。我们的代码实现已开源至：\url{https://github.com/ridgerchu/matmulfreellm}。**|
|**2024-06-04**|**CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks**|Maciej Besta et.al.|[2406.02524](http://arxiv.org/abs/2406.02524)|**[link](https://github.com/spcl/checkembed)**|大型语言模型（LLMs）正在各个领域带来变革，但验证其答案仍然是一个重大挑战，尤其是在处理复杂、开放性的任务，如知识整合、摘要和提取。本文提出了一种名为CheckEmbed的精确、可扩展且简便的LLM验证方法。CheckEmbed的核心理念是：通过利用如GPT文本嵌入大模型获取的答案级嵌入来比较LLM的回答。这将复杂的文本答案转化为单一的嵌入，简化了对比过程，实现快速而有意义的验证。我们构建了一个全面的验证管道，该管道实现了CheckEmbed的理念，并提供了评估LLM答案真实性的度量，如嵌入热力图及其总结。我们展示了如何利用这些指标设计实际的引擎，以决定LLM答案是否令人满意。在实际文档分析任务中，如术语提取和文档摘要，我们的方法表现出显著的准确性提升、成本效益和运行时间性能，相较于BERTScore或SelfCheckGPT等基于token、句子和事实级别的方案。|
|**2024-06-04**|**RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots**|Soroush Nasiriany et.al.|[2406.02523](http://arxiv.org/abs/2406.02523)|null|## 翻译  人工智能的最新进展在很大程度上依赖于规模的扩大。然而，在机器人领域，大规模机器人数据集的获取是一个瓶颈。我们主张利用逼真的物理模拟来提升环境、任务和数据集的规模，以支持机器人学习方法。为此，我们介绍RoboCasa，这是一个大型的仿真框架，旨在训练能够在日常环境中通用的机器人。RoboCasa的特点是拥有丰富且多样化的厨房场景，包括超过150个类别的一千多件3D模型资产和数十种可交互的家具和电器。  我们通过生成式AI工具进一步增强模拟的真实性和多样性，如使用文本到3D模型的技术生成对象资产，以及通过文本到图像模型生成环境纹理。我们设计了100项任务，包括由大型语言模型指导的复合任务，用于系统性评估。为了促进学习，我们提供了高质量的人类演示，并结合自动轨迹生成方法，以最小的人力成本大幅扩充数据集。  我们的实验表明，在使用合成生成的机器人数据进行大规模模仿学习时，存在明显的规模效应，并显示出利用模拟数据在现实世界任务中的巨大潜力。相关视频和开源代码已在https://robocasa.ai/网站上提供。|
|**2024-05-31**|**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**|Chaoyou Fu et.al.|[2405.21075](http://arxiv.org/abs/2405.21075)|null|在人工智能的追求中，多模态大型语言模型（MLLMs）已成为近期进步的核心。然而，对它们处理序列视觉数据的能力的关注尚显不足。为此，我们在本文中提出Video-MME，这是首个全面评估MLLMs在视频分析性能的多模态评估基准。我们的工作有四个关键特性：1）视频类型多样，涵盖6个主要视觉领域和30个子领域，确保广泛的应用场景泛化能力；2）时间维度的跨度，包括短、中、长期视频，从11秒到1小时，以检验模型对复杂情境动态的适应性；3）数据模态的广度，结合视频帧以外的多种输入，如字幕和音频，揭示MLLMs的全方位能力；4）高质量的标注，由专家严格手动标记，以保证精确且可靠的模型评估。我们精心挑选并手动注解了900段视频，总时长达到256小时，生成了2,700个问题-答案对。通过Video-MME，我们对包括GPT-4系列、Gemini 1.5 Pro在内的多个最先进的MLLM，以及开源图像模型InternVL-Chat-V1.5和视频模型LLaVA-NeXT-Video进行了深入评估。实验结果显示，Gemini 1.5 Pro是表现最佳的商业模型，明显优于开源模型。我们的数据集和发现强调了改进处理更长序列和多模态数据的必要性。项目网页链接：https://video-mme.github.io|
|**2024-05-31**|**Grammar-Aligned Decoding**|Kanghee Park et.al.|[2405.21047](http://arxiv.org/abs/2405.21047)|null|大型语言模型（LLMs）在生成高度结构化的输出时面临挑战，如程序代码、数学公式或规范的标记。约束解码方法通过限制每次输出可能的令牌，确保输出符合特定规则来缓解这个问题，例如在语法约束解码（GCD）中，LLM的输出必须遵循给定的语法规则。然而，研究表明，这种约束解码可能会扭曲模型的分布，导致生成的输出虽然语法正确，但其概率并不直接反映LLM本身的概率分配，从而质量不高。我们称之为“与语法约束对齐的解码”（Grammar-Aligned Decoding，GAD），并提出了一种名为“自适应采样与近似期望未来”（Adaptive Sampling with Approximate Expected Futures，ASAp）的解码算法。  ASAp算法旨在保证输出的语法性，并理论上产生与LLM在给定语法约束条件下的条件概率相符的结果。该算法利用先前的样本输出来稳健地估算不同输出前缀的未来语法可能性。我们在代码生成和结构化自然语言处理任务上的实验表明，ASAp经常能够生成比现有GCD技术更符合LLM分布且仍遵守所需语法限制的输出，从而提高了整体质量。|
|**2024-05-31**|**Direct Alignment of Language Models via Quality-Aware Self-Refinement**|Runsheng Yu et.al.|[2405.21040](http://arxiv.org/abs/2405.21040)|null|强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）行为以符合人类偏好的常用方法。最近，直接策略优化（DPO）作为一种替代方案兴起，它不再依赖LLM奖励模型，从而减少了额外的内存和训练时间。然而，DPO忽视了正向和负向响应的相对质量，可能导致训练结果不理想。为解决这个问题，我们探讨利用LLM内部知识在即时微调过程中获取响应的质量，并优化损失函数。我们设计了一种细化函数，利用LLM的知识来估计正向和负向响应的品质。实验表明，在轻度假设下，构建的细化函数能够帮助自我调整损失函数。我们将这个细化功能整合到DPO及其变体身份策略优化（IPO）中。实验证明，这些改进后的模型在各种评估者上表现出优于DPO和IPO的性能。|
|**2024-05-31**|**Standards for Belief Representations in LLMs**|Daniel A. Herrmann et.al.|[2405.21030](http://arxiv.org/abs/2405.21030)|null|随着大型语言模型（LLMs）在各个领域展现出非凡能力，计算机科学家们正在寻求理解它们的认知过程，特别是关于LLMs如何（如果有的话）内部构建对世界的信念。然而，目前尚缺乏一个统一的理论框架来支撑对LLM中信念的研究。本文试图填补这一空白，提出了一套条件，使LLM中的表示能够被视为信念似的。我们指出，尽管在LLMs中测量信念的项目与决策理论和形式认识论中的信念测量在许多方面有相似之处，但也存在差异，这些差异应影响我们的测量方法。因此，借鉴哲学洞察和机器学习的当代实践，我们确立了四个标准：准确性、一致性、统一性和实用性。这四个标准结合了理论考量与实际限制，为全面理解LLM中的信念表示奠定了基础。我们引用实证工作的成果，揭示了单独使用某些标准时识别信念表示的局限性。|
|**2024-05-31**|**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**|Elias Stengel-Eskin et.al.|[2405.21028](http://arxiv.org/abs/2405.21028)|**[link](https://github.com/esteng/pragmatic_calibration)**|**当回答问题时，语言模型不仅能提供答案，还能传达对答案正确性的信心程度。这包括明确的分数标记，如给出数字，以及隐含的信心标志，如权威语气或提供额外知识。然而，当前大多数模型往往过于自信。为了校准这些信心度，我们提出了一种实用的、考虑听众的微调方法（LACIE），它不仅关注答案是否正确，还关注答案是否会被听众接受。我们将校准视为偏好优化，通过双代理游戏创建数据，让一个演讲者模型的输出接受模拟听者的评判。然后，我们使用LACIE对三个语言模型（Mistral-7B、Llama3-8B和Llama3-70B）进行微调，并显示经过微调的模型在模拟听者面前有更好的校准。重要的是，这些趋势也适用于人类听众，帮助他们更准确地预测模型的正确性：我们在人机评估中发现，经过LACIE训练的模型接受的错误答案减少了47%，而正确答案的接受率保持不变。此外，LACIE泛化到另一个数据集上，在使用TriviaQA训练后，TruthfulQA上的真实性大幅提高。我们的分析表明，LACIE导致了正确和错误示例之间的信心度更好地分离。定性上，我们发现经过LACIE训练的模型会更加谨慎，并在回答正确时通过使用权威语气或提供细节来隐性地表示确定性。最后，LACIE微调导致模型对于可能错误的答案更倾向于放弃（例如说“我不知道”）。**|
|**2024-05-31**|**Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**|Xiaojun Jia et.al.|[2405.21018](http://arxiv.org/abs/2405.21018)|**[link](https://github.com/jiaxiaojunqaq/i-gcg)**|**随着大型语言模型（LLMs）的快速发展，其安全校准成为广泛应用的关键。针对这些模型的破解（即“jailbreaking”）活动日益增多，其中贪婪坐标梯度（GCG）攻击因其成效显著而受到关注。然而，GCG的攻击效率仍有提升空间。本文提出了一系列改进的优化基线破解技术，以提升GCG的性能。首先，我们注意到单个目标模板“Sure”极大地限制了GCG的攻击效果，因此我们建议采用包含有害自我暗示和/或指导的多样化目标模板，以误导模型。在优化策略上，我们建议在GCG中实施自动多坐标更新，以加速收敛，并引入从简单到复杂（easy-to-hard）的初始化技巧。将这些改进整合，我们开发出一种高效的方法—— $\mathcal{I}$ -GCG。实验在一系列基准测试，如NeurIPS 2023 红队挑战中进行，结果显示，我们的改进技术能够帮助GCG超越现有破解攻击，实现接近100%的攻击成功率。代码已发布在https://github.com/jiaxiaojunQAQ/I-GCG。**|
|**2024-05-31**|**DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**|Linli Yao et.al.|[2405.20985](http://arxiv.org/abs/2405.20985)|**[link](https://github.com/yaolinli/deco)**|该研究关注于多模态语言模型（MLLMs）中的投影器模块，因为它们在连接视觉和语言模态、促进跨模态对齐方面发挥关键作用。然而，目前对于投影器在视觉-语言对齐方面的效果评估仍显不足，通常只能通过下游任务的性能间接推断。为此，本研究通过分析MLLM中的视觉-语言语义流，来解读投影器的工作机制。  具体来说，研究者追踪从生成的语言标记到原始视觉编码块以及投影器产生的中间输出之间的语义相关性流。发现压缩型投影器（如QFormer）倾向于将视觉块抽象成有限的几个概念，如物体或属性，导致“双重抽象”现象：首先，投影器参照预定义查询令牌进行视觉语义抽象，然后，基于文本指令的大语言模型进一步提取。这种双重抽象在训练过程中效率不高，并可能导致视觉语义信息的累积缺失。  为解决这个问题，研究提出“解耦压缩与抽象（DeCo）”的关键洞察，即在投影层面上将视觉令牌数量压缩，而让大语言模型完全负责视觉语义抽象。因此，研究人员采用了一种简单的压缩器——二维自适应池化，以无参数的方式降低视觉块的尺寸。实验结果显示，DeCo在性能和效率上都优于传统的压缩投影器。它在MLLM基准、视觉定位和开放性视觉问答任务中分别取得了0.9%、7.1%和2.9%的性能提升，同时拥有更少的可训练参数和更快的收敛速度。|
|**2024-05-31**|**Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**|Feiteng Fang et.al.|[2405.20978](http://arxiv.org/abs/2405.20978)|**[link](https://github.com/calubkk/raat)**|大型语言模型（LLMs）展现出强大功能，但面临挑战，如虚构、过时知识和难以追溯的推理过程。为解决这些问题，检索增强生成（RAG）作为一种有前景的方法崭露头角，它结合外部数据库的知识。然而，不适当的检索段落可能妨碍LLMs生成全面且高质量的回答。先前关于RAG中检索噪声稳健性的研究往往局限于有限的噪声类型，这与现实世界的检索环境不符，限制了实际应用。本研究首先探讨了检索噪声，并将其分为三种不同的类别，反映真实环境。我们分析了这些不同类型的检索噪声对LLMs稳健性的影响。  接着，我们提出了一种新颖的RAG方法，称为检索增强自适应对抗训练（RAAT）。RAAT利用自适应对抗训练来动态调整模型的训练流程以应对检索噪声，并采用多任务学习确保模型能够识别嘈杂的上下文。大量的实验表明，在各种噪声条件下，使用RAAT训练的LLaMA-2 7B模型在F1和EM分数上显示出显著提升。为了便于复现，我们已在https://github.com/calubkk/RAAT上发布了我们的代码和数据。|
|**2024-05-31**|**SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**|Tianyang Xu et.al.|[2405.20974](http://arxiv.org/abs/2405.20974)|**[link](https://github.com/xu1868/sayself)**|**大型语言模型（LLMs）常常产生不准确或虚假的信息，并且通常无法表明其信心水平，这限制了它们的广泛应用。先前的研究试图通过直接提示或自我一致性提示来提取LLMs的信心，或者构建特定数据集进行监督微调。基于提示的方法性能较差，而基于训练的方法又局限于二元或不精确的整体信心估计。本文提出了一种先进的方法——SaySelf，这是一个训练框架，旨在教导LLMs提供更精确的细粒度信心估计。  此外，SaySelf还推动LLMs生成自我反思的解释，明确指出它们在参数知识上的空白并解释不确定性。这是通过让LLM以自然语言的形式自动总结特定知识中的不确定性来实现的。这种总结是基于对多个采样推理链的不一致性分析，生成的数据用于监督微调。为了进一步校准信心估计，我们采用了精心设计的强化学习，奖励准确、高置信度的预测，同时惩罚错误输出中的过度自信。  实验结果表明，无论是在分布内还是分布外的数据集上，SaySelf都能有效减少信心校准误差，同时保持任务性能。生成的自我反思理由也被证明是合理的，能进一步促进校准。代码已公开在：\url{https://github.com/xu1868/SaySelf}。**|
|**2024-05-31**|**LCQ: Low-Rank Codebook based Quantization for Large Language Models**|Wen-Pu Cai et.al.|[2405.20973](http://arxiv.org/abs/2405.20973)|null|## 背景  大型语言模型（LLMs）在众多任务上展现出优异性能，但它们的存储和计算成本高成为部署的一大挑战。为了压缩模型并降低成本，权重量化技术被广泛应用。目前，大多数针对LLMs的量化方法使用秩一码本，然而在高压缩比下，这会导致显著的精度损失。本文提出了一种新颖的权重量化方法，称为低秩码本量化（LCQ），旨在解决这一问题。  ## 方法  LCQ采用低秩码本进行量化，其秩可以大于一。这种方法旨在通过利用更高的秩来保持或提升模型的精度，同时控制额外的存储开销几乎为零。实验表明，与现有方法相比，LCQ在保持良好准确性的前提下，能够实现更优的压缩效果。  ## 结论  综上所述，本文介绍了一种创新的低秩码本量化方法，它有望在不显著增加存储成本的情况下，提升大型语言模型在实际应用中的性能和效率，为高效部署这些模型提供了新的解决方案。|
|**2024-05-30**|**MotionLLM: Understanding Human Behaviors from Human Motions and Videos**|Ling-Hao Chen et.al.|[2405.20340](http://arxiv.org/abs/2405.20340)|**[link](https://github.com/IDEA-Research/MotionLLM)**|这项研究关注于多模态（视频和动作模态）下的人类行为理解，通过大型语言模型（LLMs）的强大功能。与专为单模态（视频或动作）设计的最新LLMs不同，我们认为理解人类行为需要对视频和动作序列（如SMPL序列）进行联合建模，以有效捕捉精细的身体部位动态和语义。为此，我们提出MotionLLM，这是一个简洁而有效的框架，用于人类动作理解、描述和推理。MotionLLM采用了一体化的视频-动作训练策略，利用现有粗粒度的视频-文本数据和精细动作-文本数据的优势，以获取丰富的空间-时间洞察。此外，我们还创建了一个大规模的MoVid数据集，包含了多样化的视频、动作、caption和指令。我们还提出了MoVid-Bench，它具有精心的手动标注，以更好地评估在视频和动作上的人类行为理解能力。实验结果充分展示了MotionLLM在caption生成、空间-时间理解以及推理能力方面的优越性。|
|**2024-05-30**|**Visual Perception by Large Language Model's Weights**|Feipeng Ma et.al.|[2405.20339](http://arxiv.org/abs/2405.20339)|**[link](https://github.com/FeipengMa6/VLoRA)**|这篇论文的背景是现有的多模态大型语言模型（MLLMs）采用了一种方法，即将视觉信息与语言模型的输入空间对齐，然后将视觉令牌与文本令牌合并，形成统一的序列输入给语言模型。然而，这种方法由于增加了由视觉令牌导致的输入序列长度，计算成本较高。为此，论文提出了一种新颖的参数空间对齐范式，通过将视觉信息表示为模型权重来处理。对于每个输入图像，首先使用视觉编码器提取特征，然后将这些特征转换为感知权重，并将其与语言模型的权重融合。这样，语言模型的输入无需视觉令牌，从而缩短了输入序列，显著提高了效率。  基于这一理念，论文提出了VLoRA模型，其中包含一个感知权重生成器。该生成器设计成能够将视觉特征转化为具有低秩特性的感知权重，类似于LoRA（低秩自适应训练）。实验结果表明，尽管VLoRA在多种多模态任务的基准上表现出与现有MLLMs相当的性能，但其在训练和推理阶段的计算成本显著降低。论文承诺开源代码和模型。|
|**2024-05-30**|**Xwin-LM: Strong and Scalable Alignment Practice for LLMs**|Bolin Ni et.al.|[2405.20335](http://arxiv.org/abs/2405.20335)|**[link](https://github.com/xwin-lm/xwin-lm)**|**本文介绍Xwin-LM，一个专为大型语言模型（LLMs）设计的全面对齐方法套件。它涵盖了监督微调（SFT）、奖励建模（RM）、拒绝采样微调（RS）和直接偏好优化（DPO）等多种关键技术。主要组成部分包括：(1) 使用高质量指令数据进行初始微调的Xwin-LM-SFT；(2) 由GPT-4精心标注的大型多轮偏好数据集Xwin-Pair；(3) 在7B、13B和70B参数规模上训练的Xwin-RM奖励模型；(4) 每个提示关联64个独特响应的多wise偏好数据集Xwin-Set，这些响应由Xwin-LM-SFT生成并由Xwin-RM评分；(5) 使用Xwin-Set中最高得分响应进行微调的Xwin-LM-RS模型；(6) 通过DPO算法在Xwin-Set上进一步优化的Xwin-LM-DPO模型。我们在AlpacaEval和MT-bench上的评估显示了整个管道的稳定且显著改进，证明了Xwin-LM的强大和可扩展性。我们将在https://github.com/Xwin-LM/Xwin-LM的仓库中持续更新，以促进社区研究。**|
|**2024-05-31**|**ParSEL: Parameterized Shape Editing with Language**|Aditya Ganeshan et.al.|[2405.20319](http://arxiv.org/abs/2405.20319)|null|本文提出了一种名为ParSEL的系统，它旨在通过自然语言实现高质量3D资产的可控编辑。面对自然语言在精确操控上的局限性，ParSEL接收一个分割的3D网格和编辑请求，生成一个参数化的编辑程序。用户可以调整程序参数，精细地探索形状变化，控制编辑幅度。系统利用大型语言模型（LLMs）来理解初始编辑指令，但发现它们在推断完整编辑程序时常常不足，产生的结果可能违反形状逻辑。为此，我们设计了分析性编辑传播（Analytical Edit Propagation，AEP）算法，它从初始编辑种子开始，通过计算机代数系统进行几何分析，寻找与潜在用户编辑兼容的分析性编辑操作，以生成完整的编辑程序。实验表明，相较于其他方案，ParSEL通过自然语言请求有效地实现了对3D对象的可控编辑。|
|**2024-05-30**|**CausalQuest: Collecting Natural Causal Questions for AI Agents**|Roberto Ceraolo et.al.|[2405.20318](http://arxiv.org/abs/2405.20318)|**[link](https://github.com/roberto-ceraolo/causal-quest)**|**人类天生就有寻求因果关系的驱动力，无论是出于好奇心还是特定目标。为了开发能处理这种人类本性追求的AI代理，我们急需一个全面的自然因果问题数据集。然而，现有的数据集要么包含人工制造的问题，无法反映实际AI应用场景，要么在特定来源的问题覆盖上有限。为此，我们提出了CausalQuest，这是一个源自社交网络、搜索引擎和AI助手的13,500个自然出现的问题的数据集。我们定义了因果问题，并建立了更细致的分类体系。通过人类标注员和大型语言模型的协作，我们对数据集进行了精心标注。研究发现，42%的人类提问实际上是关于因果的，大部分是想了解给定结果背后的原因。利用这个数据集，我们训练了高效的二分类器（高达28.5亿参数），用于识别因果问题，实现了高性能，F1分数高达0.877。最后，我们提出了一系列丰富的未来研究方向，这些都可以基于我们的数据和模型进行扩展。**|
|**2024-05-30**|**ANAH: Analytical Annotation of Hallucinations in Large Language Models**|Ziwei Ji et.al.|[2405.20315](http://arxiv.org/abs/2405.20315)|**[link](https://github.com/open-compass/anah)**|**### 背景  大型语言模型（LLMs）的“幻觉”问题对于其广泛应用至关重要。然而，对这一问题的细致测量在社区中并未得到充分探索。为此，我们提出了一项名为 $\textbf{ANAH}$ 的双语数据集，专注于生成式问答中的LLM幻觉分析。ANAH中的每个答案句子都经过严谨标注，包括参考片段检索、幻觉类型的判断以及错误内容的修正。该数据集包含约12,000个句级注释，涵盖了大约4,300个LLM响应，涉及超过700个主题，通过人机交互式流程构建而成。由于幻觉注释的精细粒度，我们可以定量确认LLMs的幻觉问题随着答案的扩展而逐渐增加，并利用ANAH来训练和评估幻觉标注器。  ### 任务  我们构建了大约12,000条句子级别的注释，针对约4,300个LLM生成的回答，涵盖了超过700个主题。这个名为ANAH的数据集通过人类参与的流程精心设计，旨在提供关于生成式问答中LLMs幻觉的详尽分析。通过细致的幻觉标注，我们能够量化地验证LLMs在生成答案时幻觉问题的累积，并利用ANAH来训练和评估幻觉识别能力。我们的实验深入研究了生成式和区分性标注器，并发现尽管开源LLMs在精细幻觉标注方面面临挑战，但使用ANAH训练的生成式标注器能够超越所有开源模型，甚至接近GPT-3.5的表现，并展现出在未见过问题上的良好泛化能力。**|
|**2024-05-30**|**Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation**|Guillaume Huguet et.al.|[2405.20313](http://arxiv.org/abs/2405.20313)|null|蛋白质在几乎所有的生物过程中发挥关键作用，其多样化的功能源于复杂的三维结构，而这些结构又由氨基酸序列决定。在这篇论文中，我们利用氨基酸序列丰富的生物学归纳偏置，提出了一种新的序列条件的SE(3)等变流匹配模型——FoldFlow-2，用于蛋白质结构生成。与FoldFlow家族的先前模型相比，FoldFlow-2引入了新颖的架构特性，包括用于编码序列的蛋白质大语言模型、结合结构和序列表示的新多模态融合主干，以及基于几何变换器的解码器。为了增加生成样本的多样性和新颖性——这对新药设计至关重要——我们在比先前工作使用的PDB数据集大一个数量级的新数据集上大规模训练FoldFlow-2，该数据集包含了已知的PDB蛋白质和通过过滤获得的高质量合成结构。此外，我们展示了如何通过引入强化微调（Reinforced Finetuning，简称ReFT）目标，使FoldFlow-2能够适应任意奖励，如提高二级结构多样性。  实验结果表明，FoldFlow-2超越了现有基于蛋白质结构的生成模型的状态，无论在无条件生成还是在设计性、多样性和新颖性方面，都优于RFDiffusion，且在蛋白质长度的各类任务上表现出良好的泛化能力，特别是在等温构象采样任务上。最后，我们展示了一个经过微调的FoldFlow-2在诸如VHH纳米抗体骨架设计等具有挑战性的条件设计任务上取得了进展。|
|**2024-05-30**|**Large Language Models Can Self-Improve At Web Agent Tasks**|Ajay Patel et.al.|[2405.20309](http://arxiv.org/abs/2405.20309)|**[link](https://github.com/AjayP13/webdreamer)**|在复杂的环境中，如网络浏览器，训练模型作为能够有效导航和执行动作的代理通常具有挑战性，主要受限于缺乏训练数据。近年来，大型语言模型（LLMs）显示出通过自然语言提示以零样本或少量样本来在新环境中导航的能力。研究还表明，LLMs可以通过自我改进（即在其自身生成的数据上微调）来超越基础性能。本研究旨在探究LLMs在长时序任务的复杂环境——WebArena基准中，通过自我改进能否提升其表现。WebArena要求代理自主浏览网页并执行操作以达成特定目标。我们使用三种不同的合成训练数据混合进行微调，并发现经过自我改进后，模型在WebArena基准上的任务完成率提高了31%。此外，我们还提出了新的评估指标，用于更全面地评估我们的微调代理模型的行为性能、鲁棒性、能力以及轨迹质量，这些指标超越了当前仅依赖于整体基准分数的评估方式。|
|**2024-05-30**|**Group Robust Preference Optimization in Reward-free RLHF**|Shyam Sundhar Ramesh et.al.|[2405.20304](http://arxiv.org/abs/2405.20304)|**[link](https://github.com/rsshyam/Group-robust-preference-optimization)**|**## 翻译  针对大型语言模型（LLMs）的特定任务进行适应时，通常需要通过基于人类反馈的强化学习（RLHF）和多元标签者群体（如不同性别、种族、公司团队等）的偏好数据进行微调。然而，传统方法倾向于采用“一刀切”的策略，即假设并优化单一的偏好模型，对各群体的独特特性和需求不够敏感。为此，我们提出了一种新颖的群体鲁棒偏好优化（GRPO）方法，旨在稳健地使LLMs适应各个群体的偏好。GRPO方法基于无奖励直接偏好优化，但区别于以往，它目标是寻找一个能最大化最差群体性能的鲁棒策略。为了实现这一目标，GRPO会动态且逐次调整不同群体的权重，优先关注累积损失较高的群体。我们在理论上探讨了GRPO的可行性，并分析了其在对数线性策略类别下的收敛性。通过使用来自不同群体的全局意见数据对LLMs进行GRPO微调，我们显著提高了最差群体的表现，减少了群体间损失的不平衡，同时提高了概率准确性，相较于非鲁棒基线，这些改进效果显著。**|
|**2024-05-30**|**Who Writes the Review, Human or AI?**|Panagiotis C. Theocharopoulos et.al.|[2405.20285](http://arxiv.org/abs/2405.20285)|null|随着人工智能在自然语言处理中的广泛应用，人们关注如何识别不同领域的AI生成文本。本研究旨在探讨这个问题，通过提出一种方法来准确区分人工智能生成的和人类撰写的书评。我们的方法利用迁移学习，让模型能够在不同主题间识别生成文本，同时提高其识别写作风格和词汇变化的能力。我们构建了一个数据集，包含真实的书评和使用Vicuna开源语言模型生成的模拟评论，以评估所提方法的有效性。实验结果显示，识别文本原创来源是可行的，准确率达到96.86%。我们的工作聚焦于大型语言模型在文本识别方面的性能与局限性研究，这对于未来有效管理此类模型以及确保人类创作内容的完整性和真实性具有重要意义。|
|**2024-05-29**|**X-VILA: Cross-Modality Alignment for Large Language Model**|Hanrong Ye et.al.|[2405.19335](http://arxiv.org/abs/2405.19335)|null|我们提出X-VILA，一种旨在增强大型语言模型（LLMs）功能的多模态模型，它融合了图像、视频和音频模态。通过将各模态特定的编码器与LLM输入对齐，并将扩散解码器与LLM输出对齐，X-VILA实现了跨模态理解、推理和生成。为了支持这种跨模态对齐，我们开发了一个有效的任意模态指令跟随数据集。然而，我们发现当前的跨模态对齐方法存在一个关键问题，导致视觉信息丢失。为此，我们设计了视觉对齐机制，包括一个视觉嵌入高速公路模块，以解决这一问题。此外，我们还提供了一种资源高效的训练策略，使得X-VILA在任意模态对话任务上表现出色，大幅超越先前的方法。令人惊讶的是，即使在缺乏类似训练数据的情况下，X-VILA在不同模态间也展现出涌现特性。该项目将开源。|
|**2024-05-29**|**LLMs Meet Multimodal Generation and Editing: A Survey**|Yingqing He et.al.|[2405.19334](http://arxiv.org/abs/2405.19334)|**[link](https://github.com/yingqinghe/awesome-llms-meet-multimodal-generation)**|**随着大型语言模型（LLMs）的最新进展，人们越来越关注将它们与多模态学习相结合。当前的多模态大语言模型（MLLMs）调查主要集中在理解上。这篇综述详细探讨了跨图像、视频、3D和音频等领域的多模态生成，特别强调了这些领域中的里程碑式工作及其技术进步。我们深入研究了这些方法的关键技术组件，以及在相关研究中使用的多模态数据集。此外，我们还剖析了借助现有生成模型进行人类-计算机交互的工具增强型多模态代理。最后，我们全面讨论了人工智能安全的进步，并探索了新兴应用和未来前景。我们的工作提供了一个系统而深入的多模态生成概述，有望推动生成内容的人工智能（AIGC）和世界模型的发展。所有相关的论文列表可在<https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation>找到。**|
|**2024-05-29**|**Multi-Modal Generative Embedding Model**|Feipeng Ma et.al.|[2405.19333](http://arxiv.org/abs/2405.19333)|null|在大多数多模态任务中，问题可以归结为生成或嵌入。现有的模型通常通过将语言模块分解为一个用于生成的文本解码器和一个用于嵌入的文本编码器来处理这两种问题。为了探索多模态方法的简约性，本工作试图仅使用一个模型来处理每种模态。为此，我们提出了一种多模态生成嵌入模型（MM-GEM），它将生成和嵌入目标整合到一个大型语言模型中。同时，我们设计了PoolAggregator，以提高效率并实现细粒度的嵌入和生成能力。  令人惊讶的是，这两个目标之间并没有显著冲突。例如，基于ViT-Large和TinyLlama的MM-GEM在诸如跨模态检索和零样本分类等多模态嵌入模型基准上表现出良好的性能，同时具备良好的图像描述能力。此外，MM-GEM能够无缝执行区域级别的图像描述生成和检索任务。另外，MM-GEM中的先进文本模型对于长文本和图像检索的Recall@1指标带来了超过5%的提升。|
|**2024-05-29**|**Self-Exploring Language Models: Active Preference Elicitation for Online Alignment**|Shenao Zhang et.al.|[2405.19332](http://arxiv.org/abs/2405.19332)|**[link](https://github.com/shenao-zhang/selm)**|****摘要：**  偏好优化，特别是在人类反馈强化学习（RLHF）的驱动下，已经在使大型语言模型（LLMs）遵循人类意愿方面取得了显著成就。相较于使用固定数据集的离线对齐，通过人或人工智能对模型生成的反馈通常能够通过迭代过程提升奖励模型的能力和LLMs的一致性。然而，要实现全局准确的奖励模型，需要系统地探索生成各种各样的响应，以涵盖自然语言的广阔空间。仅依赖标准奖励最大化LLMs的随机采样是不足以满足这一需求的。  为解决这个问题，我们提出了一种双层目标，乐观地倾向于可能具有高奖励的响应，以此来主动探索分布外区域。通过解决内层问题，利用重新参数化的奖励函数，我们提出了名为Self-Exploring Language Models（SELM）的算法。它消除了对单独奖励模型（RM）的需求，并通过一个直观的目标对LLMs进行迭代更新。与直接偏好优化（DPO）相比，SELM的目标降低了对未见过的过度延伸的无差别偏好，提高了探索效率。  我们的实验结果显示，在Zephyr-7B-SFT和Llama-3-8B-Instruct模型上进行微调后，SELM在MT-Bench和AlpacaEval 2.0等指令跟随基准以及不同设置下的各种标准学术基准上表现出显著的性能提升。我们的代码和模型已可在<https://github.com/shenao-zhang/SELM>获取。**|
|**2024-05-29**|**Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation**|Atrisha Sarkar et.al.|[2405.19328](http://arxiv.org/abs/2405.19328)|null|本文提出了一种名为“规范模块”的架构，它针对生成性代理在面对包含现有规范的社会结构时的协作挑战。这些代理通过大型语言模型理解和评估环境，但在处理复杂社会任务时，如何识别并适应规范基础设施成为关键问题。规范模块的核心在于促进均衡选择，借鉴分类机构实现相关均衡的概念，使代理能够通过同伴互动学习环境中不同候选机构中的权威性。通过提升规范能力，代理可以协调制裁行为，进而影响社交环境中的基本行为，从而提高整体福祉。  我们设计了一个支持机构的新环境，并根据两个主要标准来评估该框架：一是代理能否忽略非权威机构，二是代理在多个选项中识别权威机构的能力。实验结果显示，配备了规范模块的代理相比基础代理能实现更稳定的合作效果，这为研究设计考虑规范基础设施的环境和代理开辟了新途径。|
|**2024-05-29**|**MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series**|Ge Zhang et.al.|[2405.19327](http://arxiv.org/abs/2405.19327)|**[link](https://github.com/multimodal-art-projection/map-neo)**|近年来，大型语言模型（LLMs）在各种任务上取得了显著进步。然而，出于商业利益，像GPT、Gemini和Claude这样的最先进模型被封闭在专有接口后，其训练详情并未公开。近期，一些机构开源了类似性能的LLMs，如LLaMA-3，但大多数细节（如中间检查点、预训练语料库和训练代码等）仍未披露。为了提高LLMs的透明度，研究界正在推动真正开放的模型，如Pythia、Amber和OLMo，这些模型提供了更多的信息，促进了对大模型性能、局限性、偏见和风险的科学研究。然而，现有的开放模型在推理、知识和编程任务上的表现仍逊于同等规模的封闭源码模型。  因此，我们开源了MAP-Neo，一个拥有70亿参数的双语语言模型，从头开始在4.5万亿高质量令牌上进行训练。MAP-Neo是首个与现有顶级LLMs性能相当的完全开源的双语模型。此外，我们还公开了所有细节，包括清理后的预训练语料库、数据清洗流程、检查点以及优化的训练和评估框架，以供重现。我们期望MAP-Neo能推动开放研究社区的发展，激发更多创新，促进LLMs的进一步提升。|
|**2024-05-29**|**Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models**|Tianrun Chen et.al.|[2405.19326](http://arxiv.org/abs/2405.19326)|null|本文提出了一项新的任务：零样本3D推理分割，目标是针对物体的部件搜索和定位，这是一种超越了先前类别特定的3D语义分割、3D实例分割和开放词汇3D分割局限的新范式。我们设计了一个名为Reasoning3D的简单基线方法，它能够理解和执行复杂的命令，对3D网格进行（细致）部分分割，同时具备上下文感知和推理答案的交互式分割能力。特别地，Reasoning3D利用预训练的2D分割网络，该网络由大型语言模型（LLMs）驱动，在零样本情况下解析用户输入查询。已有研究表明，大规模预训练赋予基础模型世界知识的先验，使其能够理解复杂指令，这使得我们在依赖有限3D数据集的情况下也能“分割任何东西”（源效率高）。实验表明，我们的方法具有泛化性，能有效根据隐性文本查询在3D对象（3D网格）中定位和突出显示部分，包括可动3D对象和真实世界的扫描数据。此外，我们的无监督方法便于快速部署，并为未来3D（语义）对象理解领域的研究，如机器人、物体操作、部件组装、自动驾驶应用、增强现实和虚拟现实（AR/VR）、以及医疗应用，提供了一个可行的通用基准。代码、模型权重、部署指南和评估协议可在以下链接获取：http://tianrun-chen.github.io/Reason3D/。|
|**2024-05-29**|**Nearest Neighbor Speculative Decoding for LLM Generation and Attribution**|Minghan Li et.al.|[2405.19325](http://arxiv.org/abs/2405.19325)|null|大型语言模型（LLMs）常常会产生虚构内容且缺乏对生成文本的来源标注。为解决这些问题，半参数化语言模型如kNN-LM通过在非参数数据存储中寻找与给定提示最接近的邻居来改进LM输出。然而，这类模型的推理速度通常较慢，生成的文本流畅度不高。本文提出了一种新颖的半参数化语言建模方法——Nearest Neighbor Speculative Decoding（NEST），它能够将现实世界中的任意长度文本片段融入生成过程，并提供其源头的标注。NEST在每次推理步骤中进行基于令牌的检索，计算出一个半参数混合分布，并从语料库中识别出可能的连续文本段落扩展。它采用一种近似推测解码策略，接受检索到的片段前缀或生成新的令牌。NEST显著提高了基础LM在各种知识密集型任务中的生成质量和来源标注率，超越了传统的kNN-LM方法，并在基于上下文的检索增强方面表现出竞争力。此外，NEST大幅提升了生成速度，当应用于Llama-2-Chat 70B时，推理时间提高了1.8倍。|
|**2024-05-29**|**Are Large Language Models Chameleons?**|Mingmeng Geng et.al.|[2405.19323](http://arxiv.org/abs/2405.19323)|null|大语言模型（LLMs）是否拥有自己的世界观和人格倾向？研究人员进行了超过一百万次的实验，让LLMs回答主观问题。通过将这些模型的响应与欧洲社会调查（ESS）的实际数据进行比较，结果显示提示对偏见和变异性有显著影响，揭示了重大的文化、年龄和性别偏差。文中讨论了评估LLMs与调查数据差异的方法，如计算加权平均值以及一个新提出的基于Jaccard相似性的测量指标。研究者强调，在利用LLMs模拟个体决策或集体行为之前，分析提示的稳健性和变异性至关重要，因为它们的模仿能力充其量只能说是近似的。|
|**2024-05-29**|**Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF**|Shicong Cen et.al.|[2405.19320](http://arxiv.org/abs/2405.19320)|null|**摘要：**  强化学习从人类反馈（RLHF）在调整大型语言模型（LLMs）以符合人类偏好方面展现出巨大潜力。在线和离线RLHF都处于活跃的研究阶段，但关键挑战之一是如何在处理从偏好数据中学习的奖励函数不确定性时。尽管标准强化学习（RL）中乐观主义或悲观主义的原则已广为人知，但在大型语言模型中实现既实用又基于理论的方法尚不成熟，因为构建置信区间的标准技术在处理任意策略参数化时变得难以处理。  本文提出了一种统一的在线和离线RLHF方法——价值激励的偏好优化（VPO）。VPO通过在最大似然估计的奖励函数中添加相应的值函数的正则化，以指示选择乐观主义还是悲观主义，实现了这一目标。此外，VPO直接优化策略，并利用隐式奖励建模，因此其RLHF管道与直接偏好优化更为简单。对于在线和离线设置，VPO提供了理论保证，其收敛速度与标准RL相当。实验在文本摘要和对话任务上验证了VPO的实用性与有效性。|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414](http://arxiv.org/abs/2405.18414)|null|## 背景  检索增强生成（Retrieval Augmented Generation，RAG）通过结合现有文档的上下文显著提升了大语言模型（Large Language Model，LLM）的响应性能。然而，当文档与问题上下文的相关性不明显或存在部分信息时，RAG的效果如何？又该如何处理文档之间的关联性呢？本研究旨在解答RAG生成中的这两个核心问题。我们提出了一种名为G-RAG的方法，它是一个基于图神经网络（Graph Neural Networks，GNNs）的重排器，介于RAG的检索器和阅读器之间。G-RAG结合了文档之间的连接性和语义信息（通过抽象意义表示图），为RAG提供了一个具有上下文感知的排名器。实验结果表明，G-RAG超越了现有的领先方法，同时计算开销更小。此外，我们评估了PaLM 2作为重排器的表现，发现其明显逊色于G-RAG，这强调了即使使用大型语言模型，重排在RAG中的重要性。|
|**2024-05-28**|**Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**|Yixiao Zhang et.al.|[2405.18386](http://arxiv.org/abs/2405.18386)|**[link](https://github.com/ldzhangyx/instruct-MusicGen)**|**在文本到音乐编辑领域，近期的进步依赖于文本查询来改变音乐风格或调整乐器元素。然而，现有方法要么需要从头训练特定的编辑模型，耗时且资源密集，要么使用大型语言模型预测编辑后的音乐，导致音频重建不够精确。为了结合优点并解决这些问题，我们提出了Instruct-MusicGen，这是一种新颖的方法，它针对预训练的MusicGen模型进行微调，以高效地执行编辑指令，如添加、删除或分离音轨。我们的方法修改了原始MusicGen架构，引入了文本融合模块和音频融合模块，使模型能够同时处理指令文本和音频输入，生成所需的编辑音乐。令人惊讶的是，Instruct-MusicGen仅向原始模型增加了8%的新参数，并在5000步的训练后，其性能超越现有基准，且表现出与专门针对任务训练的模型相当的能力。这一进展不仅提高了文本到音乐编辑的效率，还拓宽了音乐语言模型在动态音乐制作环境中的应用范围。**|
|**2024-05-28**|**OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**|Pengxiang Li et.al.|[2405.18380](http://arxiv.org/abs/2405.18380)|**[link](https://github.com/pixeli99/owlore)**|**随着大型语言模型（LLMs）的快速发展，它们在自然语言处理任务中带来了革命性变化。然而，大模型的训练或微调带来了巨大挑战。针对这一问题，低秩适应（LoRA）等参数高效方法崭露头角，但往往牺牲性能。本文提出了一种新的内存高效微调方法——Outlier-weighed Layerwise Sampled Low-Rank Projection（OwLore），它受到LLMs层间异常分布的启发，通过动态采样预训练层而非添加额外适配器来进行微调。我们首先通过Heavy-Tailed Self-Regularization理论（HT-SR）解读异常现象，发现具有更多异常值的层更倾向于呈现长尾分布，训练效果更好。因此，OwLore策略性地为异常值较多的层分配更高的采样概率，以更好地利用预训练模型的知识。  为了进一步减少微调时的内存需求，我们结合梯度低秩投影，使得每一层能以低秩方式高效训练。通过融合低秩优势和最优层别采样策略，OwLore显著优化了LLM剪枝中的内存-性能权衡。我们在多个架构，如LLaMa2、LLaMa3和Mistral上的广泛实验表明，OwLore持续优于基础方法，包括全量微调。例如，在常识推理基准上，OwLore可实现平均1.1%的精度提升，MMLU上提高3.0%，而在MT-Bench上更是有显著的10%提升，同时内存效率更高。特别地，OwLore仅需21GB内存即可对LLaMa2-7B进行微调。**|
|**2024-05-28**|**LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**|Anthony Sarah et.al.|[2405.18377](http://arxiv.org/abs/2405.18377)|null|现代大型语言模型（LLMs）在自然语言处理、复杂推理、情感分析等任务中的卓越表现推动了它们的广泛应用。然而，这些强大的功能伴随着巨大的内存和计算成本，限制了在大多数硬件平台上的使用。为解决这一问题，我们提出了一种有效的方法，基于LLaMA2-7B进行单次微调后，通过遗传算法搜索找到更小、计算复杂度更低的网络架构。实验表明，对于某些标准基准任务，预训练的LLaMA2-7B模型实际上过于庞大且复杂。我们实现了1.5倍的模型大小缩减和1.3倍的吞吐量提升，同时保持了几乎无损的准确性。相较于某些剪枝或稀疏化技术，我们的方法在效率和效果上更为优越。最后，我们展示了量化与我们的方法相结合的效果，进一步通过量化减少了找到的网络的大小和复杂性。我们相信，本工作提供了一种自动创建可在更廉价和广泛可用硬件平台上使用的LLMs的方法。|
|**2024-05-28**|**Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning**|Dongjie Chen et.al.|[2405.18376](http://arxiv.org/abs/2405.18376)|**[link](https://github.com/Dong-Jie-Chen/RCL)**|**### 背景  源免费领域适应（SFDA）的目标是仅使用未标记的靶域数据来调整预训练的源模型。当前的SFDA方法在有效利用预训练知识和挖掘靶域数据潜力方面面临挑战。多模态大型语言模型（MLLMs）在理解视觉和文本信息方面表现出色，但它们应用于SFDA时存在问题，如指令执行失败、计算需求高以及在适应前性能评估困难。为了缓解这些问题，我们提出了一种新颖的框架——可靠性基于课程学习（RCL），它通过伪标签化整合多个MLLM以促进知识利用，应用于SFDA。  ### 方法  我们的框架包括：1) 可靠知识转移，2) 自我纠正，3) MLLM引导的知识扩展，以及4) 多热掩码精炼，这些方法协同作用，逐步发掘靶域未标记数据的价值。RCL在多个SFDA基准上实现了最先进的（SOTA）性能，例如在DomainNet上提升显著，达到 $\textbf{+9.4\%}$ ，证明了其在增强适应性和鲁棒性方面的有效性，同时无需访问源数据。代码可在https://github.com/Dong-Jie-Chen/RCL获取。**|
|**2024-05-28**|**Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**|Phakphum Artkaew et.al.|[2405.18375](http://arxiv.org/abs/2405.18375)|**[link](https://github.com/PhakphumAdev/Thai-Winograd)**|常识推理是自然语言理解的重要组成部分，为此已开发出多个评估基准。然而，这些基准大多仅限于英语。创建平行基准有助于跨语言评估，从而更好地理解不同语言。本研究介绍了一个泰语版的Winograd Schema集合，这是一个专为测试泰语中的常识推理能力而设计的新数据集。我们通过邀请母语者、专业翻译和严格验证的方法，确保该系列题库能准确反映泰国语言的独特性、习语和文化引用，同时保持模糊性和常识挑战。我们对大型语言模型（如GPT-4和Claude-3-Opus）在这项基准上的性能进行了评估，结果显示尽管在英语上表现优异，但它们在泰语中的性能明显下降，这表明在多语言常识推理方面仍有待进步。|
|**2024-05-28**|**PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**|Eshaan Agarwal et.al.|[2405.18369](http://arxiv.org/abs/2405.18369)|null|大型语言模型（LLMs）已经在各个领域带来了革命性的变化，展现出卓越的能力。它们成功的关键在于提示的概念，即指导模型生成输出。然而，手动创建提示既耗时又局限于特定领域，因此需要自动化的解决方案。本文介绍PromptWizard，一个新颖的框架，它利用LLMs迭代地合成和优化针对特定任务的提示。与现有方法不同，PromptWizard同时优化提示指令和上下文示例，以最大化模型性能。该框架通过变异指令并引入负例，逐步深化理解并保证多样性。借助一个评判者，PromptWizard进一步改进指令和示例，融入详细的推理步骤，以实现最佳表现。PromptWizard具有计算效率高、适应不同训练数据量场景以及在小型LLM上同样有效的特点。通过对8个数据集的35个任务进行严谨评估，结果显示PromptWizard明显优于现有的提示策略，证明了其在提示优化方面的高效性和可扩展性。|
|**2024-05-28**|**Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?**|Yifan Bai et.al.|[2405.18361](http://arxiv.org/abs/2405.18361)|null|随着自动驾驶（AD）任务的快速发展，基于端到端的方法，特别是视觉语言模型（VLM）的应用变得尤为重要。这些模型试图融合强大的逻辑推理和认知能力，以实现全面的端到端规划。然而，现有的VLM方法往往依赖于2D视觉分词器和大型语言模型（LLM），在处理三维几何信息方面存在不足，这对于可靠的规划至关重要。研究表明，2D分词的LLM并不能准确感知三维环境，这引发了关于VLM在自动驾驶中可靠性的质疑。  针对这一问题，我们提出了一种名为Atlas的新方法，它结合了DETR风格的3D感知器作为3D分词器，与单层线性投影器相连，巧妙地利用了三维物理世界的固有特性。这种方法允许高分辨率多视角图像的同时处理和时空建模。尽管简单，但Atlas在NuScenes数据集上的3D检测和自主驾驶规划任务中表现出色，证明了3D分词的LLM对于实现可靠自动驾驶至关重要。我们将开源代码和数据集，以供进一步研究。|
|**2024-05-28**|**Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**|Somnath Kumar et.al.|[2405.18359](http://arxiv.org/abs/2405.18359)|null|大型语言模型（LLMs）正在全球范围内重塑众多领域，但它们在处理非拉丁字母和低资源语言时的包容性和效果仍有待提升。本文针对这一关键挑战，提出了一种无需大量训练或微调的方法来增强多语言LLMs的表现。通过系统地研究和评估各种语言在流行的问题解答（QA）数据集上的性能，我们提出了一系列新颖技术，以释放LLMs在多元语言环境中的真正潜力。我们的方法包括三个核心策略，极大地提高了多语言能力：首先，精心优化适用于多语言LLM的提示，挖掘其潜在能力，显著提升了各语言的表现。其次，我们引入了一种新的混合方法，结合了多语言嵌入的LLM检索增强生成（RAG），实现了更好的多任务性能。最后，我们开发了一种动态学习策略，实现实时根据查询动态选择最合适的提示策略、LLM模型和嵌入模型，从而最大化LLM在不同语言上的效率，超越了最佳静态和随机策略。此外，我们的方法既适用于离线配置调整，也支持在线适应，能够无缝适应新语言和数据集，显著推动了多语言理解和生成在各种语言中的进步。|
|**2024-05-28**|**MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**|Somnath Kumar et.al.|[2405.18358](http://arxiv.org/abs/2405.18358)|null|## 背景  近期的多模态大型语言模型（MLLM）在视觉与语言融合任务上取得了显著进步。然而，它们在细致的多模态理解、复杂任务解析以及多模态信息推理方面仍存在挑战。本文提出MMCTAgent，一个旨在解决当前MLLM在复杂视觉推理任务中固有局限性的新型多模态批判性思维代理框架。MMCTAgent借鉴了人类认知过程和批判性思考的特点，通过迭代分析多模态信息、拆解问题、规划策略，并实现动态推理。  此外，MMCTAgent还融入了批判性思考元素，如对最终答案的验证和自我反思。它通过一种新颖的方法定义基于视觉的评判者，并确定特定任务的评估标准，从而提升决策能力。在多个图像理解和视频理解基准测试中，我们严谨地评估了MMCTAgent（包括带评判者的版本）的表现，结果表明它在超越基础MLLM和其他工具增强的管道方面表现出色。|
|**2024-05-27**|**Matryoshka Multimodal Models**|Mu Cai et.al.|[2405.17430](http://arxiv.org/abs/2405.17430)|null|## 背景  大型多模态模型（如LLaVA）在视觉-语言推理方面表现出色。这些模型首先将图像嵌入到大量的固定视觉令牌中，然后将它们输入到大型语言模型（LLM）。然而，这种设计在处理高分辨率图像和视频等密集视觉场景时会导致大量令牌，从而导致效率低下。尽管存在令牌剪枝/合并方法，但它们为每个图像生成单个长度的输出，无法在信息密度与效率之间灵活权衡。受到套娃玩偶概念的启发，我们提出了M3：套娃多模态模型，它学习将视觉内容表示为捕捉不同粗细粒度信息的嵌套视觉令牌集合。  ## 任务  我们的方法为LMMs带来了几个独特的优势：(1) 在测试实例中，用户可以明确控制视觉粒度，例如，根据内容的复杂性或简洁性调整用于表示图像的令牌数量；(2) M3提供了一个分析现有数据集所需粒度的框架，我们发现像COCO这样的基准只需要大约~9个视觉令牌就能获得与使用所有576个令牌相当的准确性；(3) 我们的方法为探索性能与视觉令牌长度之间的最佳权衡提供了基础，研究显示当前固定规模表示与理想上限之间存在显著差距。|
|**2024-05-27**|**NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models**|Chankyu Lee et.al.|[2405.17428](http://arxiv.org/abs/2405.17428)|null|本文介绍了一种名为NV-Embed的新型大语言模型，专门设计用于提升基于解码器的大型语言模型在文本嵌入任务中的性能，包括密集向量检索。NV-Embed通过多种架构设计和训练策略显著增强模型的灵活性和表现，同时保持其简洁性和可复现性。  在架构方面，我们引入了隐式注意力层来获取池化嵌入，这在检索和下游任务准确性上均优于平均池化或使用LLMs的最后一个<EOS> token嵌入。为了改进表示学习，我们移除了LLMs的自回归注意力掩码，在对比性训练中允许更全面的信息交互。  在训练策略上，我们采用两阶段的对比性指令调优方法。第一阶段在检索数据集上进行指令训练，利用批次内负样本和精心挑选的难例。第二阶段将各种非检索任务的数据融入指令调优，不仅提高非检索任务的准确性，还提升了检索性能。  凭借这些创新，NV-Embed仅使用公开数据就实现了前所未有的高分，达到69.32，荣登大规模文本嵌入基准（MTEB）（截至2024年5月24日）榜首，涵盖56项任务，包括检索、重排、分类、聚类和语义文本相似度。尤其值得注意的是，我们的模型在BEIR的15项检索任务中取得了最高的59.36分。NV-Embed模型的源代码将在以下网址开源：https://huggingface.co/nvidia/NV-Embed-v1。|
|**2024-05-27**|**Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model**|Kuan-Chih Huang et.al.|[2405.17427](http://arxiv.org/abs/2405.17427)|**[link](https://github.com/kuanchihhuang/reason3d)**|**随着多模态大型语言模型（LLMs）的最新进展，它们在概念推理等领域展现出巨大潜力。然而，在理解三维环境方面的应用仍相对有限。本文提出Reason3D，这是一种专为全面3D理解设计的新颖LLM。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩码，支持高级任务，如3D推理分割、层次搜索、表达式指代和详细掩码输出的问答。特别是，我们设计了一种分层掩码解码器，能够精确定位广阔场景中的小物体。该解码器首先生成一个粗略的位置估计，覆盖物体的大致区域，然后采用逐步细化的策略，显著提高对象识别和分割的精度。实验结果显示，Reason3D在ScanNet和Matterport3D等大规模数据集上，在3D表达式指代、3D问答和3D推理分割任务上表现出卓越性能。代码和模型已在以下链接提供：https://github.com/KuanchihHuang/Reason3D。**|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|由于实体代理需要与现实世界互动，它们必须具备全面的先验知识、长远规划能力以及快速响应速度。尽管近期基于大型语言模型（LLM）的代理表现出色，但它们仍存在一些局限性。例如，LLM的输出通常是描述性的句子，在确定具体动作时可能存在歧义。为了克服这些问题，我们提出了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归方式预测后续动作。为了训练LARM，我们开发了一种新颖的数据格式，称为自回归节点传输结构，并构建了相应的数据集。通过两阶段训练，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法所能达到的成就需要更复杂的决策链。此外，LARM的速度是最快的，比以前快6.8倍。|
|**2024-05-27**|**Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation**|Jiaming Liu et.al.|[2405.17418](http://arxiv.org/abs/2405.17418)|null|当机器人操作策略面对新任务或物体实例时，其动作性能往往不尽人意。因此，自动检测和自我纠正失败动作的能力对于实际的机器人系统至关重要。近期，多模态大型语言模型（Multimodal Large Language Models，MLLM）在视觉指令跟随方面展现出前景，并在多种任务中展现出强大的推理能力。为了将通用MLLM作为端到端的机器人代理，我们提出了Self-Corrected (SC)-MLLM，不仅使其能够预测末端执行器位置，还赋予其自主识别并纠正错误动作的能力。首先，我们通过参数效率高的微调，使MLLM具备姿态预测功能，将其转化为一个语言建模问题。在遇到执行失败时，模型能识别低层次动作错误的原因（如位置和旋转误差），并主动寻求专家的提示。根据反馈，SC-MLLM会重新思考当前失败场景，生成修正后的动作。此外，我们设计了一种连续策略学习方法，针对成功纠正的样本，提升模型对当前场景配置的适应性，减少专家干预的频率。  为了评估我们的SC-MLLM，我们在模拟和真实世界环境中进行了广泛实验。结果表明，与先前最先进的机器人MLLM（ManipLLM）相比，SC-MLLM显著提高了操作精度：在已知物体类别上从57%提升至79%，在未知新类别上从47%提升至69%。|
|**2024-05-27**|**THREAD: Thinking Deeper with Recursive Spawning**|Philip Schroeder et.al.|[2405.17402](http://arxiv.org/abs/2405.17402)|**[link](https://github.com/philipmit/thread)**|大型语言模型（LLMs）在各种场景中展现出卓越的能力，但随着上下文的长度和复杂度增加，它们仍面临挑战。为此，我们提出了Thinking Recursively and Dynamically（ThReaD）方法。ThReaD将模型生成过程构想为一个执行流程，根据上下文可以完整运行或动态地创建新线程。通过子线程，模型可以分发任务（如思考、获取信息），子线程只返回父线程所需的令牌，从而让模型能够根据需要调整产生令牌时使用的中间工作量。我们在任务解决和问答等场景中应用ThReaD，使其能递归地将给定的任务或问题分解为逐步简化的小子问题，由单独的子线程解决。我们使用少量样本学习的方式实现ThReaD，并在包括ALFWorld、TextCraft、WebShop在内的多个基准测试上评估GPT-4和GPT-3.5的表现，以及两个新基准：DataCommons QA和MIMIC-III ICU QA。实验结果显示，ThReaD在这些基准上实现了最先进的性能，相对于现有框架，即使是小型模型（如Llama-3-8b和CodeLlama-7b）也能提升10%到50%的绝对分数。|
|**2024-05-27**|**MindMerger: Efficient Boosting LLM Reasoning in non-English Languages**|Zixian Huang et.al.|[2405.17386](http://arxiv.org/abs/2405.17386)|**[link](https://github.com/cone-mt/mindmerger)**|## 任务  推理能力对于大型语言模型（LLMs）至关重要，但英语与其他非英语语言之间的差距明显。一些研究通过微调LLMs以重新学习非英语的推理能力，而另一些方法则使用外部模型（如英语翻译文本）的输出来替换非英语输入，以应对LLM理解非英语的挑战。然而，这些方法往往未能充分利用LLMs内在的推理和语言理解能力。为了更好地利用LLMs的思维和语言理解能力，我们提出了一种新方法，称为MindMerger，它将LLMs与多语言模型的外部语言理解能力相结合，以提升多语言推理性能。我们还引入了两步训练策略，首先将外部能力嵌入LLMs，然后训练外部能力和内置能力的协作使用。在三个多语言推理数据集和一个语言理解数据集上的实验表明，MindMerger始终优于所有基线，特别是在低资源语言上。在不更新LLMs参数的情况下，MGSM数据集上所有语言的平均准确率提高了6.7%，低资源语言提高了8.0%。|
|**2024-05-27**|**ReMoDetect: Reward Models Recognize Aligned LLM's Generations**|Hyunseok Lee et.al.|[2405.17382](http://arxiv.org/abs/2405.17382)|**[link](https://github.com/hyunseoklee-ai/reward_llm_detect)**|随着大型语言模型（LLMs）的卓越性能和易用性提升，它们带来的社会风险，如假新闻生成，促使开发出能检测LLM生成文本（LGT）的方法以确保安全使用。然而，由于大量LLM的存在，逐个识别它们的特点变得不切实际。因此，研究关注的是这些强大模型共有的特性，即“对齐训练”，即训练LLMs生成更符合人类偏好的文本。我们的关键发现是，随着这些对齐训练的LLMs致力于最大化人类偏好，它们生成的文本甚至比人类撰写的文本在估计偏好上更高，这使得利用偏好模型（一个训练来模拟人类偏好分布的LLM）轻易就能检测到这些文本。  基于这一发现，我们提出两种进一步增强偏好模型检测能力的训练策略：（1）持续偏好微调，使模型更偏向于识别对齐的LLG；（2）奖励模型对人/LLM混合文本的学习，即使用对齐LLM重述的人类原创文本，这是一种介于LGT和人类文本之间的偏好基准，有助于更好地学习决策边界。我们在六个文本领域和十二种对齐LLM上进行了广泛评估，结果显示我们的方法表现出最先进的性能。相关代码已在https://github.com/hyunseoklee-ai/reward_llm_detect上提供。|
|**2024-05-27**|**RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects**|Ahmed Allam et.al.|[2405.17378](http://arxiv.org/abs/2405.17378)|**[link](https://github.com/AUCOHL/RTL-Repo)**|大型语言模型在辅助进行寄存器传输级（Register Transfer Level, RTL）设计任务上展现出潜力。然而，现有的基准测试在反映真实世界RTL项目复杂性方面存在显著差距。为此，该论文提出了一项新的基准——RTL-Repo，专为评估大型语言模型在大规模RTL设计项目中的性能而设计。RTL-Repo包含了从GitHub公共仓库提取的超过4000个Verilog代码样本，每个样本都提供了对应仓库的完整上下文。我们对包括GPT-4、GPT-3.5、Starcoder2以及像VeriGen和RTLCoder这样的Verilog专用模型在内的多款最先进的模型在RTL-Repo基准上的性能进行了评估，比较它们在生成复杂项目的Verilog代码方面的表现。RTL-Repo为硬件设计社区提供了一个宝贵的资源，用于评估和比较语言模型在实际RTL设计场景中的性能，并针对复杂的多文件RTL项目专门训练Verilog代码生成。RTL-Repo是开源的，已在GitHub上公开可用。|
|**2024-05-28**|**Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models**|ShengYun Peng et.al.|[2405.17374](http://arxiv.org/abs/2405.17374)|**[link](https://github.com/shengyun-peng/llm-landscape)**|### 背景  安全校准是确保大型语言模型（LLMs）的行为符合人类偏好并避免有害行为的关键，但近期研究显示，仅使用少量精心设计的训练样本来微调模型可能导致安全性被轻易破坏。我们致力于通过探索LLM的安全景观来评估微调过程中的风险。我们发现了一个普遍存在于流行开源LLM模型参数空间中的新现象，称为“安全盆地”：随机扰动模型权重能使模型在局部区域保持原始校准模型的安全性。  ### 发现与贡献  我们的发现启发我们提出了一种新的安全度量方法——VISAGE，它通过探测模型的安全景观来评估LLM微调过程中的安全性。可视化校准模型的安全景观有助于理解微调如何使模型偏离安全盆地，从而损害安全性。此外，我们观察到系统提示在保护模型方面的重要性，这种保护甚至会传递给处于安全盆地内的扰动版本。这些从安全景观研究中得出的见解为未来LLM安全领域的研究提供了新的洞见。|
|**2024-05-24**|**Scaling Laws for Discriminative Classification in Large Language Models**|Dean Wyatte et.al.|[2405.15765](http://arxiv.org/abs/2405.15765)|null|## 背景  现代大型语言模型（LLMs）标志着机器学习模型能力的一个重大飞跃。这些模型能够对各种查询生成合理的回答，这表明它们在客户服务应用中具有潜力。然而，LLMs已被观察到存在胡言乱语的问题，这在短期内限制了它们在客户服务中的应用。为了解决这个问题，我们提出了一种系统，将语言建模任务重新构想为分类任务，以帮助客户服务代表选择最佳的模板回复。我们的目标是为客服代表提供最合适的前K个候选回复。  ## 任务描述  我们展示了离线和在线实验的结果，证明了实验系统的有效性，离线实验显示出改进，而在线实验则带来了统计显著的效果提升。此外，我们分享了通过模型参数调整进行的验证损失和前K精度的度量曲线。最后，我们讨论了模型大小、延迟和准确性之间的权衡，并展望了未来可能的应用领域。|
|**2024-05-24**|**Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**|Andres Algaba et.al.|[2405.15739](http://arxiv.org/abs/2405.15739)|**[link](https://github.com/andresalgaba/llm_citation_patterns)**|论文摘要： 引用实践对于构建科学知识结构至关重要，但往往受到当代规范和偏见的影响。随着大型语言模型（如GPT-4）的出现，这一领域出现了新的动态。研究者首次探索了完全依赖参数知识而非基于搜索或检索增强生成的推荐引用的特性及其潜在偏见。实验使用了一组包含166篇来自AAAI、NeurIPS、ICML和ICLR的论文，这些论文在GPT-4的知识截止日期后发表，涉及3,066个引用。实验让GPT-4为匿名文本中的引用提供学术参考。结果揭示了人类和语言模型（如GPT-4）的引用模式惊人相似，但GPT-4显示出更强的高引用偏见，即使在控制了出版年份、标题长度、作者数量和会议等因素后依然存在。此外，我们发现GPT-4生成的既有和不存在引用的特性高度一致，表明模型内化了引用模式。通过分析引用图谱，显示GPT-4推荐的引用嵌入在相关引用网络中，暗示其对概念的深入理解。尽管语言模型可以辅助引用生成，但它们也可能放大现有偏见并引入新偏见，可能影响科学知识的传播。我们的结果强调了识别模型偏见的必要性，并开发平衡的方法与语言模型互动的重要性。|
|**2024-05-24**|**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**|Boyang Zheng et.al.|[2405.15734](http://arxiv.org/abs/2405.15734)|**[link](https://github.com/bytetriper/lm4lv)**|大型语言模型（LLMs）的成功催生了多模态大型语言模型（MLLMs）的研究热潮，它们正在改变计算机视觉领域的多个研究范式。尽管MLLMs在诸如视觉问答（VQA）和文本到图像等高级视觉和 Vision-and-Language 任务上表现出色，但尚无研究探讨过低级视觉任务如何从这些模型中受益。我们发现，当前大多数MLLM的设计使其对低级特征视而不见，因此在解决低级视觉任务方面存在固有限制。为此，我们提出 $\textbf{LM4LV}$ ，这是一个框架，它允许一个冻结的LLM无需任何多模态数据或先验知识就能解决一系列低级视觉任务。这突显了LLMs在低级视觉领域的强大潜力，并弥合了MLLMs与低级视觉任务之间的鸿沟。我们期望这项工作能激发对LLMs的新视角，加深对其工作机制的理解。|
|**2024-05-24**|**Optimizing Large Language Models for OpenAPI Code Completion**|Bohdan Petryshyn et.al.|[2405.15729](http://arxiv.org/abs/2405.15729)|**[link](https://github.com/BohdanPetryshyn/openapi-completion-benchmark)**|近期，大型语言模型（LLMs）在代码生成任务中的进步极大地改变了软件开发领域。尽管主流编程语言的代码补全解决方案表现出色，但它们在处理较少见的格式，如OpenAPI定义时性能欠佳。本研究评估了GitHub Copilot，一个流行的商业代码补全工具，在OpenAPI完成任务中的表现，并针对Meta开源的Code Llama模型提出了一系列针对该任务的优化策略。研究中设计了一个语义感知的OpenAPI完成基准，通过实验分析了不同提示工程和微调技术对Code Llama模型性能的影响。经过微调的Code Llama模型在正确性上达到了比GitHub Copilot高出55.2%的峰值，同时其参数数量仅为商业解决方案（基于Codex模型）的1/25。此外，研究还改进了一种广泛使用的代码填充训练方法，解决了模型在接收到小于训练时使用的上下文长度提示时的性能不足问题。|
|**2024-05-24**|**Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models**|Yue Zhang et.al.|[2405.15684](http://arxiv.org/abs/2405.15684)|null|为了弥合视觉和语言模态之间的鸿沟，多模态大型语言模型（Multimodal Large Language Models，MLLMs）通常会学习一个适配器，将视觉输入转化为大语言模型（LLMs）能理解的令牌。然而，大多数适配器生成的视觉令牌相对固定，不考虑提示中提及的具体对象。由于这些适配器对图像中的每个细节分配同等关注，且倾向于处理整个场景，这可能会增加大语言模型在处理复杂场景时的认知负荷。为此，我们提出了提示感知适配器。这类适配器设计有根据提示特定关注点动态嵌入视觉输入的能力。具体来说，提示感知适配器利用全局和局部文本特征，在粗粒度和细粒度层次上捕捉与提示最相关的视觉线索。这种方法显著提升了大语言模型理解和解释视觉内容的能力。在各种视觉问答任务中，如计数和位置推理实验中，提示感知适配器的效果得到了验证。|
|**2024-05-24**|**What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models**|Abdelrahman Abdelhamed et.al.|[2405.15668](http://arxiv.org/abs/2405.15668)|null|这篇论文探讨了如何利用大型语言模型（LLMs）进行零样本图像分类。作者提出了一种简单但有效的方法，通过将多模态LLMs应用于图像输入，生成详尽的文本表示。这些文本表示被转化为跨模态嵌入空间中的固定维特征，并结合使用于零样本分类，无需为每个数据集设计复杂的提示。研究者采用通用提示策略，而非针对每个数据集单独调整。实验结果显示，这种方法在多个数据集上表现出色，比先前方法的准确性有所提升。平均而言，在十个基准测试中，该方法比传统方法提高了4.1个百分点，尤其在ImageNet数据集上的提升达到了6.8个百分点。这表明，多模态LLMs有潜力显著增强如零样本图像分类之类的计算机视觉任务，为现有技术带来了显著的进步。|
|**2024-05-24**|**Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning**|Wenhan Chang et.al.|[2405.15662](http://arxiv.org/abs/2405.15662)|null|在人工智能时代，用户可能因隐私顾虑要求AI公司从训练数据集中删除他们的信息。作为模型所有者，重新训练模型会消耗大量计算资源，因此机器遗忘（machine unlearning）技术应运而生，以允许删除请求的训练数据或类别，同时尽量减少对模型性能的影响。然而，对于大规模复杂数据，如图像或文本，从模型中“遗忘”一个类别可能导致性能下降，因为难以确定类别与模型之间的关联。为此，我们提出使用概念（Concept）而非图像特征或文本数据中的令牌来表示要删除类别的语义信息，这有助于切断模型与类别的联系，实现彻底消除影响。  为了分析复杂数据中的概念影响，我们采用了后处理概念瓶颈模型和集成梯度技术，精确识别不同类别中的概念。然后，我们利用随机标签和目标标签的数据污染策略，提出遗忘方法。我们在图像分类模型和大型语言模型（LLMs）上测试了我们的方法，结果一致显示，提出的策略能准确地从模型中抹除目标信息，同时保持模型性能的大部分。|
|**2024-05-24**|**$$\mathbf{L^2\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis**|Simen Gaure et.al.|[2405.15652](http://arxiv.org/abs/2405.15652)|null|近年来，大型语言模型（LLMs）因其在翻译、预测和内容生成等任务中的出色表现而备受瞩目。同时，研究界发现LLMs易受攻击，但也能增强系统的安全性。然而，这些开源的LLMs在作为掩蔽通信媒介，如支持抗审查通信方面的能力如何呢？本论文从实验角度出发，通过实证测量开源LLM模型（Llama-7B）的安全性与容量，以评估其作为掩蔽通信的有效性。尽管结果显示，基于这种模型的通道不太可能实现高实际比特率，这取决于消息长度和模型熵，但我们发现对手发现隐秘通信的可能性较低。为了使结果易于广泛参考，我们采用了一个简单且直观的方案，并假设模型是公开可用的。|
|**2024-05-24**|**LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots**|Ruoyu Wang et.al.|[2405.15646](http://arxiv.org/abs/2405.15646)|null|在日常生活中开发通用服务机器人的需求促使机器人必须能恰当地执行多种基础行为。近期，大规模语言模型（LLMs）的训练进步使得可以直接根据自然语言指令生成任务序列，无需额外的领域知识。然而，尽管LLMs的输出在语义上是正确的，但生成的任务计划可能并不精确地对应于可接受的动作，并且可能存在各种语言模糊性。LLM的幻觉问题对机器人任务规划构成挑战，可能导致生成的内容与现实世界事实或用户输入不符。为此，我们提出了一种基于约束LLM提示的任务规划方法，该方法可以从命令中生成可执行的动作序列。此外，我们还设计了一个异常处理模块来应对LLM幻觉问题，确保生成的结果在当前环境中是可接纳的。我们在RoboCup@Home命令生成器生成的命令上测试了我们的方法，结果显示机器人在理解和执行任务方面表现出色。|
|**2024-05-24**|**GECKO: Generative Language Model for English, Code and Korean**|Sungwoo Oh et.al.|[2405.15640](http://arxiv.org/abs/2405.15640)|null|我们介绍GECKO，一个专为韩语和英语（包括编程语言）设计的双语大语言模型（LLM）。它基于LLaMA架构，使用平衡且高质量的韩英语数据集进行预训练。本报告详述了我们在构建数据管道和训练模型过程中的一些努力。尽管GECKO的词汇量较小，但其在生成韩语和英语令牌时表现出高效性能。我们在代表性的基准测试上评估了其性能，特别是在韩国MMMLU（韩国多模态多语言理解）任务上表现优异，而在英语和代码方面则显示出适度的能力，尽管其训练的令牌数量少于专注于英语的LLMs。GECKO以宽松的许可协议对开源社区开放，我们希望它能为韩语LLM研究提供研究基线和实用见解。您可以在以下链接找到该模型：https://huggingface.co/kifai/GECKO-7B。|
|**2024-05-23**|**A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns**|Asaf Yehudai et.al.|[2405.14863](http://arxiv.org/abs/2405.14863)|null|跨领域对齐是指将一个概念从一个领域映射到另一个领域的任务。例如，询问“如果\textit{医生}是一种\textit{颜色}，它会是什么颜色？”这个看似奇特的课题旨在研究人们如何通过类别映射和对这些映射的推理来表征具体和抽象的概念。在这篇论文中，我们借鉴认知科学中的这一任务，通过行为研究评估大型语言模型（LLMs）在概念化和推理能力上的表现。我们通过提示LLMs执行跨域映射任务，并在群体和个体层面分析它们的响应。此外，我们还评估了模型对其预测进行推理的能力，通过分析和分类它们对这些映射的解释。结果显示，人类和模型的映射以及解释存在显著相似性，表明模型以与人类类似的方式表征概念。这种相似性不仅体现在模型的表示上，也体现在它们的行为中。而且，模型大多给出有效的解释，并采用与人类类似的推理路径。|
|**2024-05-23**|**Bitune: Bidirectional Instruction-Tuning**|Dawid J. Kopiczko et.al.|[2405.14862](http://arxiv.org/abs/2405.14862)|null|我们提出了一种名为Bitune的方法，该方法提升了预训练的解码器型大语言模型在指令调优方面的性能，从而在多个下游任务上实现了显著的提升。Bitune通过同时应用自回归和双向注意力到提示上，以获取更精确的查询或指令表示。我们为此引入了两组参数，并采用了参数高效微调技术来处理。这两种特征随后被组合成一个加权平均，其中权重由可训练系数决定，用于生成新的令牌。实验结果表明，Bitune在零样本设置下在常识推理、算术和语言理解任务上表现出色。大量的消融研究验证了每个组件的作用，并显示了该方法对不同PEFT技术的鲁棒性。|
|**2024-05-23**|**PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression**|Vladimir Malinovskii et.al.|[2405.14852](http://arxiv.org/abs/2405.14852)|**[link](https://github.com/vahe1994/aqlm)**|## 背景  对于大型语言模型（LLMs）的“极端”压缩，即将其参数压缩至1-2位每参数，以适应资源受限设备上的高效执行，引起了广泛关注。现有研究主要集中在改进一次性量化技术和权重表示上；然而，纯后训练方法在精度与位宽权衡方面的收益正在减少。当前最先进的量化方法，如QuIP#和AQLM，包含对部分压缩参数的小规模校准数据微调；然而，这些针对压缩权重的微调通常仅使用直通估计器（STE），STE在这种场景下的性能尚不明确。  本工作质疑在极端LLM压缩中使用STE的有效性，并系统地研究了量化感知微调策略。我们提出PV-Tuning，一个无特定架构限制的框架，它扩展并改进了现有的微调策略，并在某些受限情况下提供收敛保证。在实际应用中，当用于1-2位矢量量化时，PV-Tuning在高性能模型如Llama和Mistral上优于先前的技术。通过使用PV-Tuning，我们在2位参数的情况下首次实现了Llama 2家族模型的帕累托最优量化。|
|**2024-05-23**|**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**|Bernal Jiménez Gutiérrez et.al.|[2405.14831](http://arxiv.org/abs/2405.14831)|**[link](https://github.com/osu-nlp-group/hipporag)**|为了在恶劣多变的自然环境中生存，哺乳动物的大脑发展出存储大量世界知识并不断整合新信息的能力，同时避免灾难性遗忘。尽管大型语言模型（LLMs）如带有检索增强生成（RAG）的方法在处理此类任务上已取得显著成就，但它们在大规模新经验融合方面仍面临挑战。本研究中，我们提出HippoRAG，一个受人类长期记忆海马回索引理论启发的新型检索框架，旨在促进对新经验的更深、更有效集成。HippoRAG巧妙地协同LLMs、知识图谱以及个性化PageRank算法，模拟人脑皮层和海马体在记忆中的不同作用。  我们将HippoRAG与现有RAG方法在多轮问答任务中进行比较，结果显示HippoRAG显著优于当前最先进的方法，性能提升高达20%。单步检索时，HippoRAG表现出与迭代检索方法如IRCoT相当或更好的性能，同时成本节省10-30倍，速度提升6-13倍。当将HippoRAG融入IRCoT后，还能带来额外的显著增益。最后，我们展示HippoRAG能够应对现有方法难以触及的新场景。代码和数据已在<https://github.com/OSU-NLP-Group/HippoRAG>上开源。|
|**2024-05-23**|**Can LLMs Solve longer Math Word Problems Better?**|Xin Xu et.al.|[2405.14804](http://arxiv.org/abs/2405.14804)|null|### 翻译  数学应用题（MWPs）是衡量大型语言模型（LLMs）能力的关键，但现有研究主要集中在简短背景的题目上。然而，现实生活中的数学问题往往涉及复杂情境，因此LLMs解决长篇数学应用题的能力对于其在实际场景的应用至关重要，但这一方面尚未得到充分探索。本研究首次关注Context Length Generalizability（CoLeG），即LLMs处理长篇数学应用题的能力。我们创建了Extended Grade-School Math（E-GSM）数据集，其中包含带有详细叙述的问题。为此，我们提出了两个新指标来评估LLMs在这类任务上的效能和鲁棒性。  通过对现有零样本提示方法以及商业和开源模型的考察，我们发现它们在CoLeG方面普遍存在不足。针对不同类型的LLMs，我们提出针对性的解决方案：对于专有模型，我们设计了一种新的指导性提示以减轻长文本的影响；对于开源模型，我们开发了一种数据增强任务以提升模型的适应性。我们的全面实验结果显示，我们的方法不仅在E-GSM上表现出色，而且在其他多个数学应用题基准上也展现出良好的泛化能力。  本研究的结果为未来利用LLMs处理复杂现实问题的研究提供了方向，为当前限制提出了实用解决方案，并为进一步探索模型泛化性和训练策略开辟了道路。|
|**2024-05-23**|**Lessons from the Trenches on Reproducible Evaluation of Language Models**|Stella Biderman et.al.|[2405.14782](http://arxiv.org/abs/2405.14782)|null|在自然语言处理（NLP）领域，有效评估语言模型仍然是一项未解的挑战。研究人员和工程师面临诸多方法论难题，例如模型对评估设置的敏感性、不同方法之间的比较困难，以及可重复性和透明度的缺失。本文基于三年的大型语言模型评估经验，为研究者提供指导和教训。首先，我们概述了语言模型评估中常见的问题。其次，我们阐述了应对或减轻这些问题的最佳实践。第三，我们介绍了Language Model Evaluation Harness（lm-eval）：一个开源库，旨在独立、可重复和扩展地评估语言模型，以解决这些问题。我们将介绍库的功能，并通过案例研究展示如何使用该库来缓解这些方法论关注点。|
|**2024-05-23**|**WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**|Peng Wang et.al.|[2405.14768](http://arxiv.org/abs/2405.14768)|**[link](https://github.com/zjunlp/easyedit)**|**在大型语言模型（LLMs）中，随着世界事实的不断增长和纠正错误响应的需求，模型编辑的方法需要不断更新知识。论文的核心问题是：在编辑过程中，知识应存储在模型的哪个记忆层次更为合适。研究发现，直接修改长期记忆（模型参数）或利用工作记忆（通过检索的神经网络激活）都会导致不可逾越的三角困境——可靠性、泛化能力和局部性无法同时实现于终身编辑场景中。直接修改参数会与无关的预训练知识或先前编辑产生冲突（可靠性差、局部性不足）；而基于检索的工作记忆难以使模型理解并泛化编辑（泛化能力弱）。因此，作者提出了一个名为WISE的新方法，旨在弥合记忆之间的鸿沟。  在WISE中，设计了一种双参数内存机制，包括主内存用于存储预训练知识，侧内存用于存放编辑后的知识。仅对侧内存中的知识进行编辑，并训练一个路由器，以便根据查询决定从哪个内存中获取信息。对于持续编辑，采用了知识切片机制，将不同的编辑分布在参数的不同子空间中，然后合并到共享内存中，以避免冲突。实验结果表明，WISE在问答、幻觉生成和跨不同趋势的LLM架构（如GPT、LLaMA和Mistral）的终身模型编辑任务中表现出色，超越了先前的模型编辑方法，成功克服了上述困境。代码将在https://github.com/zjunlp/EasyEdit上发布。**|
|**2024-05-23**|**FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models**|Hongyang Yang et.al.|[2405.14767](http://arxiv.org/abs/2405.14767)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融机构和专业人士越来越多地将大型语言模型（LLMs）融入工作流程，金融行业与AI社区之间仍存在显著障碍，如专有数据和专业知识。这些挑战限制了AI在提升金融任务效率方面的潜力。鉴于金融分析的重要性，我们旨在开发专门针对金融的LLM驱动工具链，并通过开源项目推动其普及，促进AI在金融决策中的广泛应用。本文介绍FinRobot，一个创新的开源AI代理平台，支持多个金融专业AI代理，每个都由LLM驱动。平台主要分为四层：1）金融AI代理层，通过构建金融Chain-of-Thought（CoT）将复杂的金融问题分解为逻辑序列；2）金融LLM算法层，根据特定任务动态配置合适的模型应用策略；3）LLMOps和DataOps层，通过训练/微调技术以及使用与任务相关的数据生成精确模型；4）多源LLM基础模型层，整合各种LLM，使上述各层可以直接访问。FinRobot旨在为专业分析师和非专业人士提供实践操作，让他们能够利用强大的AI技术进行高级金融分析。FinRobot的开源代码可在此获取：\url{https://github.com/AI4Finance-Foundation/FinRobot}。**|
|**2024-05-23**|**Evaluating Large Language Models for Public Health Classification and Extraction Tasks**|Joshua Harris et.al.|[2405.14766](http://arxiv.org/abs/2405.14766)|null|随着大型语言模型（LLMs）的快速发展，人们对其在公共卫生领域支持专家工作的潜力产生了浓厚兴趣。本研究通过结合六个外部标注的和七个内部标注的数据集，评估了LLMs在处理与健康负担、流行病学风险因素和公共卫生干预相关的文本分类和提取任务上的性能。我们首先对五个开源大模型（参数量从7亿到70亿不等）进行了零样本的上下文学习测试。结果显示，Llama-3-70B-Instruct表现出色，微-F1得分在17个任务中的15项中最高。各任务间的性能差异显著，例如，有些模型如Contact Classification的得分低于60%，而像GI疾病分类这样的任务，所有模型都能达到80%以上的微-F1。对于12个任务的子集，我们还评估了GPT-4，发现其与Llama-3-70B-Instruct的结果相当，Llama-3-70B-Instruct在其中6个任务上得分更高或持平。总体而言，根据初步结果，我们发现LLMs有可能成为公共卫生专家从各种自由文本源提取信息的有效工具，有助于公共卫生监测、研究和干预措施。|
|**2024-05-23**|**Large language models can be zero-shot anomaly detectors for time series?**|Sarah Alnegheimish et.al.|[2405.14755](http://arxiv.org/abs/2405.14755)|**[link](https://github.com/sintel-dev/sigllm)**|近期的研究表明，大型语言模型能够执行多种任务，包括时间序列预测。这些模型的灵活性使其适用于众多应用。本文提出一项新颖的研究，探讨大型语言模型在复杂的时间序列异常检测任务中的性能。对于语言模型而言，这涉及识别输入序列（或多个部分）中的异常点，以及处理时间序列数据而非传统的文本输入。我们介绍了sigllm，一个专为时间序列异常检测设计的大型语言模型框架。该框架包含将时间序列转换为文本的模块，以及端到端的流程，用于引导语言模型进行异常检测。我们试验了两种测试大型语言模型能力的方法：一是直接提示模型指出输入中的异常元素；二是利用语言模型的预测能力来辅助检测过程。  我们在11个来自不同来源的数据集上评估了我们的框架，使用了10种不同的管道。结果显示，预测方法在所有11个数据集中都显著优于提示方法，尤其是在F1分数上。尽管大型语言模型能够发现异常，但目前的深度学习模型在性能上仍占优，其表现比大型语言模型高出30%。|
|**2024-05-21**|**Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**|William Brandon et.al.|[2405.12981](http://arxiv.org/abs/2405.12981)|null|## 翻译  键值缓存对于加速Transformer架构的自回归大型语言模型（LLMs）的解码至关重要。然而，随着序列长度增加和批量大小增大，存储键值缓存所需的内存可能会变得难以承受。自从Transformer诞生以来，两个最有效的内存减小策略是多查询注意力（MQA）及其推广，群组查询注意力（GQA）。MQA和GQA通过让多个查询头共享单个键/值头，显著减少了不同键/值头的数量，同时对准确性影响较小。本文展示了如何进一步发展MQA，即在相邻层之间也共享键和值头，我们将其称为跨层注意力（CLA）。实验表明，使用CLA，可以在保持接近原始MQA精度的同时，将键值缓存的大小再减少2倍。我们在从头训练10亿参数和30亿参数模型的实验中验证了这一点，结果表明，CLA在内存与准确性之间的权衡上提供了优于传统MQA的帕累托改进，使得更长的序列长度和更大的批量大小下的推理成为可能。|
|**2024-05-21**|**Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**|Shriram Chennakesavalu et.al.|[2405.12961](http://arxiv.org/abs/2405.12961)|**[link](https://github.com/rotskoff-group/llm-era)**|在化学空间中的搜索是一个极具挑战性的问题，因为可能的分子数量随着原子数量呈组合级增长。大型自回归模型通过学习化学化合物数据库已经产生了强大的生成器，但我们仍然缺乏有效策略来生成具有特定性质的分子。这个问题与大型语言模型的“对齐”问题相似，尽管在许多化学任务中，我们有一个明确且易于评估的奖励函数。本文介绍了一种名为能量排名对齐（ERA）的算法，它利用明确的奖励函数构建了一个梯度优化目标，用于调整自回归策略。理论上，我们发现该算法与Proximal Policy Optimization（PPO）和Direct Preference Optimization（DPO）密切相关，但其最小化器收敛于一个理想的吉布斯-玻尔兹曼分布，奖励函数扮演了能量角色。此外，该算法具有高度可扩展性，无需强化学习，并且在每对样本的偏好观察次数较少时，相对于DPO表现出色。  我们将这种方法应用于分子变压器的对齐，以生成具有外部指定属性的分子，并发现它能稳健地进行搜索，探索化学空间的多样化部分。虽然我们的重点在于化学搜索，但我们在一个AI监督的任务上也取得了优秀结果，表明该方法是可扩展且通用的。|
|**2024-05-21**|**Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**|Zhangyue Yin et.al.|[2405.12939](http://arxiv.org/abs/2405.12939)|**[link](https://github.com/yinzhangyue/AoR)**|## 背景 近期，Chain-of-Thought提示的进展极大地推动了大型语言模型（LLMs）在复杂推理任务中的突破。当前研究通过采样多种推理路径并根据答案频率进行ensemble，提高了LLMs的推理性能。然而，这种方法在正确答案处于少数的情况时失效。我们发现这是制约LLMs推理能力的关键因素，仅凭预测答案无法解决这个问题。为此，我们提出了一个层次化的推理聚合框架AoR（推理聚合），它依据推理链条的评估来选择答案。此外，AoR引入了动态采样策略，根据任务复杂度调整推理链条的数量。  ## 任务 一系列复杂推理任务的实验结果显示，AoR相较于主流ensemble方法表现出色。进一步分析表明，AoR不仅适用于各种LLMs，而且在与现有方法的性能天花板比较中，达到了更优秀的水平。|
|**2024-05-21**|**Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**|Bilgehan Sel et.al.|[2405.12933](http://arxiv.org/abs/2405.12933)|null|大型语言模型在诸如总结、算术推理和问答等任务上表现出色。然而，在道德推理和伦理决策方面，尤其是在涉及多个利益相关者的复杂情景中，它们面临严峻挑战。本文提出了一种名为Skin-in-the-Game（SKIG）的框架，旨在通过从不同利益相关者角度审视决策的后果，提升语言模型在道德推理中的能力。SKIG的核心机制是模拟行动的责任感，结合同理心练习和风险评估，对提高其有效性至关重要。我们使用专有和开源语言模型在各种道德推理基准上验证SKIG的表现，并通过深入的消融分析探究其关键组件。|
|**2024-05-21**|**Code-mixed Sentiment and Hate-speech Prediction**|Anjali Yadav et.al.|[2405.12929](http://arxiv.org/abs/2405.12929)|**[link](https://github.com/matejklemen/sentiment-hate-speech-with-code-mixed-models)**|在多语言环境中，混合代码（code-mixed discourse）指的是单文本中融合多种语言的现象，尤其是在官方语言多元的国家的非正式交流中常见。随着大型语言模型在自然语言处理任务中的主导地位提升，我们针对代码混合语境的研究也随之展开。首先，我们特别设计了四款新的英语-印地语和英语-斯洛文尼亚双语预训练遮罩语言模型，以适应非正式语言。接着，我们对各种类型的模型——包括单语、双语、少量语言和大规模多语言模型——在社交媒体文本的情感分析和攻击性语言检测等任务上的性能进行了评估。结果显示，最有效的分类器是针对社交媒体文本的专业化双语和多语言模型，随后是非专业的大规模多语言和单语模型，而大型生成模型的表现并不突出。对于涉及情感的问题，模型在处理代码混合数据时总体上略优于非代码混合数据。|
|**2024-05-21**|**Streamlining Software Reviews: Efficient Predictive Modeling with Minimal Examples**|Tim Menzies et.al.|[2405.12920](http://arxiv.org/abs/2405.12920)|**[link](https://github.com/timm/ez)**|该论文提出了一项新的软件分析挑战任务。在这个被称为“软件审查”的过程中，一组SME（主题专家）会评审软件行为示例，以建议如何改进软件的运行。由于SME的时间通常非常有限，理想的状况是，该团队仅通过查看少量具有高度信息价值的示例就能完成优化任务。为了支持这个审查过程，研究探索了训练预测模型的方法，该模型能够预测某个专家是否会喜欢或不喜欢下一个示例。这种预测模型可以与SME合作，引导他们探索所有示例，同时在专家离开后，模型也可以作为代理，处理新出现的案例，以应对专家们的忙碌。  在31个案例研究中（涵盖了从软件流程的高层决策到视频编码软件配置的低层决策），我们展示了仅使用12到30个标签就能建立这样的预测模型。据我们所知，仅凭少数示例（不依赖大型语言模型）就能取得这样的成果，在当前尚属罕见。遵循开放科学的原则，我们将在<https://github.com/timm/ez/tree/Stable-EMSE-paper>提供所有的代码和数据，以便他人能复制、验证或在此基础上进一步改进这些结果。|
|**2024-05-21**|**G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**|Xingyuan Pan et.al.|[2405.12915](http://arxiv.org/abs/2405.12915)|**[link](https://github.com/xypan0/G-DIG)**|大型语言模型（LLMs）在通用场景中展现出显著能力，通过指令微调，它们能够与人类在多种任务上协同。然而，指令数据的多样性和质量是指令微调面临的两大挑战。为此，本论文提出了一种新颖的基于梯度的方法，用于自动选择机器翻译中的高质量和多样化的指令微调数据。我们的核心创新在于分析单个训练样例如何在训练过程中影响模型。通过结合影响力函数和一小部分高质量种子数据，我们选择对模型产生积极影响的样例作为高质量数据。此外，为了增加数据多样性，我们通过聚类其梯度并重采样，最大化它们对模型产生的影响多样性。在WMT22和FLORES翻译任务上的广泛实验验证了我们方法的优越性，深入分析进一步证实了其效果和泛化能力。|
|**2024-05-21**|**An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation**|Zhiyu Tan et.al.|[2405.12914](http://arxiv.org/abs/2405.12914)|**[link](https://github.com/llm-conditioned-diffusion/llm-conditioned-diffusion.github.io)**|一个关键的先决条件是准确理解文本输入，这对于忠实的文本到图像生成至关重要。现有的方法利用CLIP模型的文本编码器来表示提示。然而，预训练的CLIP模型仅能处理英文，且其文本编码器的模型容量相对有限。相比之下，大型语言模型（LLMs）支持多语言输入，能够处理更长的上下文，并提供更优秀的文本表示。本文研究了使用LLMs作为文本编码器以提升文本到图像生成中的语言理解能力。然而，从头开始训练包含LLMs的文本到图像生成模型需要大量的计算资源和数据。  为此，我们提出了一种三阶段训练流程，有效地整合现有文本到图像模型与LLMs，同时保持高效的训练。特别地，我们设计了一个轻量级适配器，使得能够快速使用LLMs生成的文本表示来训练文本到图像模型。大量的实验表明，我们的模型不仅支持多语言输入，还能处理更长的上下文，而且在图像生成质量上表现出色。|
|**2024-05-21**|**Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**|Holli Sargeant et.al.|[2405.12910](http://arxiv.org/abs/2405.12910)|**[link](https://github.com/AhmedIzzidien/TopicLLM)**|**该论文关注法律分析中的一个重要空白，通过构建和应用一种新颖的判例主题分类法，对英国的简易判决案件进行了探索。利用精心挑选的简易判决案例数据集，我们利用大型语言模型Claude 3 Opus研究功能性话题和趋势。结果显示，Claude 3 Opus在主题分类上的准确率为87.10%，揭示了不同法律领域中简易判决的明显模式。由于英国的判例法并未原始标注关键词或提供主题过滤选项，这项研究不仅深化了我们对简易判决主题本质的理解，还展示了传统方法与人工智能驱动分类方法结合的可能性。因此，本文提供了英国法律的新通用分类框架。这项工作的意义为司法行政领域的进一步研究和计算法学研究方法论讨论奠定了基础。**|
|**2024-05-21**|**Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**|San Kim et.al.|[2405.12900](http://arxiv.org/abs/2405.12900)|null|近期，大规模语言模型（LLMs）和各种有效的训练方法的兴起推动了开放领域对话系统的发展。然而，这些模型中的毒性问题对用户体验构成重大挑战。本文提出了一种创新的训练算法——对抗式直接偏好优化（ADPO），它是在直接偏好优化（DPO）的基础上改进的。ADPO旨在训练模型增加对优选回复的概率分布，同时降低对使用有毒控制令牌生成的不安全回复的概率。研究显示，ADPO能够增强模型抵御有害对话的能力，同时尽量减少性能下降。此外，我们证明ADPO提供了比传统DPO更为稳定的训练流程。据我们所知，这是首次将有害数据直接融入生成模型的DPO变体，从而减少了人工创建安全对话数据的需求。|
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217](http://arxiv.org/abs/2405.12217)|**[link](https://github.com/jameszhou-gl/icl-distribution-shift)**|**近期的研究表明，大型多模态模型（LMMs）在应对自然分布变化时表现出极高的鲁棒性，常常超越先前的基准。然而，领域特定的适应仍然是必要的，尤其是在医疗等专业领域。鉴于LMMs庞大的参数空间使其微调不切实际，本研究聚焦于探索上下文学习（ICL）作为一种增强LMM适应性的有效方法。我们发现，ICL的成功在很大程度上依赖于示例的选择，这与大型语言模型类似，但对面临分布变化的LMMs提出了独特挑战。为此，我们评估了一种无监督的ICL方法——TopKNearestPR，该方法通过特征相似性进行最近示例搜索来选择示例。研究揭示了这种方法在处理分布转移场景下的视觉编码器缺陷对其效果的限制。  为解决这些问题，我们提出了一种新颖的方法——InvariantSelectPR，它利用类条件对比不变性（CCI）来提升预训练视觉编码器的稳健性。CCI通过增强不同类别间的区分度并确保对领域特定变化的不变性，提高了编码器识别和检索最有信息价值示例的能力。这种方法有助于引导LMM适应新的查询样本，即使在不同的分布下也是如此。实验结果显示，InvariantSelectPR显著提高了LMM的适应性，在Camelyon17和HAM10000基准数据集上的7-shot任务中，分别实现了34.2%和16.9%的准确率提升，相对于零-shot性能，这是显著的进步。**|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209](http://arxiv.org/abs/2405.12209)|**[link](https://github.com/open-compass/mathbench)**|**随着大型语言模型（LLMs）的最新进展在数学领域取得了显著进步，传统的数学基准如GSM8k在全面评价这些模型的数学能力方面存在局限。为了弥补这一不足，我们提出了MathBench，这是一个全新基准，旨在严格评估大型语言模型的数学能力。MathBench覆盖广泛的数学学科，对理论理解和实际问题解决能力进行详尽评估。它分为五个阶段，从基础算术到大学数学，结构上设计用于考察模型在不同深度知识的理解。每个阶段包括理论问题和应用题，以衡量模型的数学熟练度及其在实际情境中应用概念的能力。MathBench的目标是提升对LLMs数学能力的评价，提供对其知识理解水平和问题解决技能的细致视角，同时支持双语环境。该项目已发布在https://github.com/open-compass/MathBench。**|
|**2024-05-20**|**Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey**|Thiago S. Vaillant et.al.|[2405.12195](http://arxiv.org/abs/2405.12195)|**[link](https://github.com/gpt-impact/Paper-content)**|随着大型语言模型（如ChatGPT）的不断发展，其强大的自然语言处理能力和广泛应用引起了广泛关注。尽管人工智能（AI）与软件工程（SE）的融合趋势日益明显，但关于这种融合如何影响软件开发实践和认知的研究仍显不足。为了揭示将AI驱动工具，如ChatGPT，融入软件开发过程的影响和挑战，我们进行了一项调查，针对207名软件开发者进行了研究。调查内容包括ChatGPT对软件质量、生产力以及开发者工作满意度的影响，同时还探讨了他们对未来ChatGPT应用的预期、对可能的工作岗位替代的担忧，以及对监管措施的看法。|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174](http://arxiv.org/abs/2405.12174)|null|该论文介绍了一个名为CT-Eval的中文文本转表格数据集，旨在衡量大语言模型在非英语语言环境下的文本转表格任务性能。由于现有英文文本转表格数据集主要面向英语，CT-Eval填补了这一空白，选择了一种流行的多学科中文在线百科作为来源，涵盖了28个领域以保证数据多样性。为了减少数据虚构（hallucination）问题，研究者首先训练了一个语言模型来识别并过滤掉存在虚构问题的样本，然后人工标注验证集和测试集中的错误。最终，CT-Eval包含了大约88,600个任务样本。通过CT-Eval，研究者评估了开源和闭源大语言模型（如GPT-4）的表现，结果显示零-shot模式下这些模型与人类判断仍有显著差距。经过微调后，开源模型在文本转表格能力上有了显著提升，大幅超越了GPT-4。总之，CT-Eval不仅为评估和理解现有大语言模型的中文文本转表格能力提供了有价值的工具，也为提升这类模型在这项任务上的性能提供了宝贵资源。|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163](http://arxiv.org/abs/2405.12163)|**[link](https://github.com/dropreg/fennec)**|**随着大型语言模型的迅速发展，它们在众多现实任务中的应用日益广泛，主要目标是符合人类的意图。然而，理解人类意图的复杂性使得依赖于耗时的人工评估成为必要。为了缓解这一问题，我们探讨了利用开源大型语言模型作为评估者的趋势，特别是在GPT-4的流行背景下。我们提出了一种名为\textbf{Fennec}的框架，专注于\textbf{F}ine-grained \textbf{E}valuation（细致评估）和\textbf{N}eeded \textbf{E}xtension（必要扩展）通过分支（Branching）和连接（Bridging）。分支操作将评估任务分解为不同维度和粒度，从而减轻评估挑战。同时，连接操作融合了多样化的训练数据集，增加了评估任务的多样性。实验结果显示，我们的7B模型在各种常用基准上的\textit{一致性}和\textit{一致同意}性能均优于开源的更大规模评估模型，接近GPT-4的表现。我们利用模型的精细校正功能改进多个模型响应，结果显示，这种优化提升了响应质量，在MT-Bench上提高了1-2分。我们的代码已在GitHub上开源\footnote{\url{https://github.com/dropreg/Fennec}}。**|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130](http://arxiv.org/abs/2405.12130)|**[link](https://github.com/kongds/mora)**|**低秩适应是大型语言模型中流行的参数高效微调方法。在这篇论文中，我们研究了低秩更新（如LoRA实现）的影响。我们的发现指出，这种机制可能限制了大语言模型学习和记忆新知识的能力。受此启发，我们提出了一种新的方法MoRA，它利用平方矩阵实现高秩更新，同时保持与LoRA相同的可训练参数数量。为此，我们引入了相应的非参数运算器，以降低输入维度并增加输出维度处理平方矩阵。这些运算器确保权重能无缝融入到大语言模型中，使得我们的方法能够像LoRA一样部署。我们在五个任务上进行了全面评估：指令调整、数学推理、连续预训练、记忆以及预训练。在内存密集型任务上，我们的方法优于LoRA，并在其他任务上表现出相当的性能。**|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119](http://arxiv.org/abs/2405.12119)|null|大型语言模型（LLMs）正在通过出色地索引项目内容、理解复杂的对话上下文并生成相关项目标题，革新了对话推荐系统。然而，控制推荐项目的分布仍是一个挑战，导致在针对对话推荐平台的快速变化的数据分布，如项目流行度上，性能欠佳。在对话推荐中，LLMs通过自回归方式生成项目标题（作为多个令牌），这使得获取和控制所有项目推荐变得困难。因此，我们提出了一种名为“重索引-然后适应”（Reindex-Then-Adapt，RTA）的框架，它将多令牌项目标题转换为单个令牌于LLMs内，随后调整这些单令牌项目标题的概率分布。RTA框架结合了LLMs理解和复杂查询的优势，以及传统推荐系统（RecSys）在对话推荐中有效控制推荐项目分布的能力。实验结果表明，我们的框架在三个不同的对话推荐数据集和两种适应设置下，展示了改进的准确性指标。|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107](http://arxiv.org/abs/2405.12107)|**[link](https://github.com/milvlg/imp)**|**尽管大型语言模型（LLMs）和大型多模态模型（LMMs）在开放世界多模态理解方面展现出惊人的能力，但它们通常参数量大、计算需求高，限制了在资源受限环境中的应用。为了应对这一问题，研究人员已经提出了一系列轻量级LMM，旨在在有限规模（如30亿参数）下最大化性能。然而，这些方法多数仅关注设计空间的单一或两个方面，对影响模型能力的关键设计选择尚未进行全面探讨。  本文系统地研究了轻量级LMM的设计，包括模型架构、训练策略和训练数据。根据我们的研究结果，我们构建了一套名为Imp的高性能LMM家族，覆盖20亿到40亿参数规模。尤其值得注意的是，我们的Imp-30亿模型在与同类规模的现有轻量级模型相比时持续领先，并超越了130亿参数规模的最新LMM状态。通过低精度量化和分辨率降低技术，Imp模型能够在高通骁龙8Gen3移动芯片上实现高速部署，每秒处理大约13个令牌的推理速度。**|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100](http://arxiv.org/abs/2405.12100)|null|## 背景 数学世界问题修正（MWPC）是一个专门针对解决数学问题过程中错误推理的修正任务。本文利用大语言模型（LLMs）的进步，关注两点：（1）区分数学推理与错误修正；（2）探索策略以提升LLMs在数学领域的错误修正能力，以应对MWPC任务。我们注意到，在实时教育中，帮助学生识别错误比单纯提供正确答案更为关键。然而，当前研究往往侧重于获取精确的解题答案，而非纠正可能的错误。因此，我们调整了研究范式，表明提升数学推理能力并不等同于精通错误修正。同时，我们提出了一种名为诊断导向提示（DOP）的新方法，旨在促进LLMs在错误修正方面表现出色。实验结果显示，DOP表现出卓越性能，彰显其重要性。我们强调，在数学教育中，对出色修正者的需要超过了对熟练推理者的追求。代码和数据可在<https://github.com/ChenhaoEcnuCS/Reason-Correct>获取。|
|**2024-05-17**|**A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers**|Kaiyu Huang et.al.|[2405.10936](http://arxiv.org/abs/2405.10936)|**[link](https://github.com/kaiyuhwang/mllm-survey)**|**随着大型语言模型（LLMs）的快速发展，在自然语言处理领域展现出显著的多语言能力，引起了学术界和业界的广泛关注。为了减少潜在的歧视并提升技术的通用性和可访问性，对于多语言技术的发展至关重要。尽管LLMs取得了突破，但对多语言场景的深入研究仍显不足。因此，迫切需要一份全面的综述，总结近期的方法、进展、局限性和可能的解决方案。本文旨在从多个角度审视LLMs在多语言环境中的应用。我们首先回顾了预训练语言模型研究的历史演变。接着，我们探讨了LLMs的多语言特性，包括训练和推理方法、模型安全、跨领域与文化适应以及数据集使用。我们还分析了这些方面面临的挑战，并提出可能的解决策略。此外，我们指出了未来的研究方向，以进一步提升LLMs的多语言性能。本综述旨在帮助研究界应对多语言问题，提供一个关于基于LLMs的多语言自然语言处理核心概念、关键技术及最新进展的全面理解。**|
|**2024-05-17**|**The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks**|Lucius Bushnaq et.al.|[2405.10928](http://arxiv.org/abs/2405.10928)|**[link](https://github.com/apolloresearch/rib)**|### 概述  机械解释性目标是通过逆向工程理解神经网络的行为。然而，现有方法在解析神经网络激活方面面临挑战，因为缺乏对激活的分解，使得单个神经元或模型组件无法清晰对应于独特的特征或功能。为此，我们提出了一种新颖的可解释性方法——局部交互基（Local Interaction Basis，LIB）。LIB旨在通过消除无关激活和交互，识别计算特征。该方法摒弃无意义的激活方向，并使基础与相邻层间雅可比矩阵的奇异向量对齐。同时，它根据特征对后续计算的重要性进行缩放，生成一个显示模型中所有计算相关特性和交互的图谱。  我们在模块加法和CIFAR-10模型上评估了LIB的有效性，结果表明，相比于主成分分析，LIB能识别出更多计算相关的特征，并呈现出更稀疏的交互。然而，在应用于语言模型时，LIB并未显著提高可解释性或交互稀疏度。因此，我们得出结论，尽管LIB是一种有前景的理论驱动方法，但当前形式并不适用于大型语言模型。|
|**2024-05-17**|**COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain**|Dimitrios P. Panagoulias et.al.|[2405.10893](http://arxiv.org/abs/2405.10893)|null|这篇技术论文阐述了COGNET-MD，一个专为医疗领域设计的大型语言模型评估的新基准。我们提出了一种评分框架，旨在评估语言模型理解医学文本的能力，并且设计了一系列难度分级的多项选择题（MCQ）数据库。这个数据库由多个医疗领域的专家合作创建，以反映当前医学趋势，确保安全、实用和适用性。初期版本包含了精神科、牙科、肺病学、皮肤科和内分泌学等领域的题目，但会持续扩展，未来还会加入更多医学学科。|
|**2024-05-17**|**Application of Artificial Intelligence in Schizophrenia Rehabilitation Management: Systematic Literature Review**|Hongyi Yang et.al.|[2405.10883](http://arxiv.org/abs/2405.10883)|null|该综述旨在系统地评估人工智能（AI）在精神分裂症患者康复管理中的现状和前景，以及其对康复过程的影响。我们从2012年至现在筛选了70项研究，重点关注机器学习、深度学习、强化学习等技术在心理健康干预和管理中的应用、技术类别、产品和数据类型，如生态瞬时评估、行为和语音数据的分析。结果显示，AI在症状监测、复发风险预测和康复治疗中具有广泛的应用潜力。此外，本研究还探讨了基于AI的新兴产品、技术和分析方法，如社交媒体分析、严肃游戏和大型语言模型在康复中的潜在挑战和未来发展方向。总的来说，这篇论文系统回顾了AI在精神分裂症康复管理中的应用，并为未来的研究路径提供了有价值的见解和建议。|
|**2024-05-17**|**The Future of Large Language Model Pre-training is Federated**|Lorenzo Sani et.al.|[2405.10853](http://arxiv.org/abs/2405.10853)|null|## 背景  生成式预训练大型语言模型（LLMs）因其在众多任务上的出色表现而备受瞩目，这得益于它们所接受的海量训练数据。根据已建立的规模法则，LLMs未来性能的提升在很大程度上依赖于我们能够利用的计算和数据资源。联邦学习（FL）有可能释放全球大部分未充分利用的数据和计算能力，这些是当前以数据中心为中心的LLM训练方法所忽视的。本文提出了一种稳健、灵活且可复现的FL方法，旨在促进机构间的大规模协作，共同训练LLMs，从而动员更多的计算和数据资源，甚至可能达到或超越中心化的性能。  ## 任务  我们的工作展示了一种FL训练方法，它能够在有限资源下扩展到百亿元级的联邦LLM，使得拥有丰富数据的实体能够成为预训练LLMs的主导力量，而不是仅让计算资源丰富的机构独占鳌头。这种方法强调了联邦训练的规模效益，并为实现这一目标提供了一种实用路径。|
|**2024-05-17**|**Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities**|Hao Zhou et.al.|[2405.10825](http://arxiv.org/abs/2405.10825)|null|随着大型语言模型（LLMs）因其卓越的理解和推理能力而备受瞩目，它们在各个领域取得了显著进步，尤其在第六代（6G）通信技术的推动下展现出人工智能通用性（AGI）的潜力。本研究旨在全面概述LLM赋能的电信网络。首先，我们概述了LLMs的基础，包括模型架构、预训练、微调、推理与应用、模型评估，以及在电信部署中的运用。接着，我们将探讨LLM支持的关键技术和电信应用，涉及生成、分类、优化和预测问题。生成应用包括电信领域知识、代码和网络配置自动生成。基于LLM的分类任务涵盖网络安全、文本、图像和流量分类。此外，我们介绍了利用LLMs的自动化优化技术，如强化学习的奖励函数设计和口语强化学习。对于预测问题，LLMs可用于时间序列预测和多模态电信预测。最后，我们指出了LLM赋能电信网络所面临的挑战，并展望了未来的研究方向。|
|**2024-05-17**|**ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios**|Markus Bayer et.al.|[2405.10808](http://arxiv.org/abs/2405.10808)|null|主动学习旨在通过优先处理最能提升学习效果的实例来减少标注工作量。然而，许多主动学习策略面临“冷启动”问题，即在初期需要大量数据才能发挥效能，这限制了它们在预训练模型（如BERT）上的应用，这些模型在少量样本情况下已表现良好。为此，我们提出了一种新颖的主动学习方法——ActiveLLM，它利用大型语言模型（如GPT-4、Llama 3和Mistral Large）进行实例选择。实验证明，ActiveLLM显著提高了BERT分类器在少量样本情况下的性能，超越了传统主动学习方法和SetFit等少数样本学习方法。此外，ActiveLLM还能扩展到非少量样本场景，支持迭代选择，从而帮助其他主动学习策略克服冷启动难题。结果表明，ActiveLLM为改善不同学习环境中的模型性能提供了有前景的解决方案。|
|**2024-05-17**|**Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**|Albert Sawczyn et.al.|[2405.10745](http://arxiv.org/abs/2405.10745)|null|### 翻译  知识密集型任务对机器学习（ML）技术提出了严峻挑战。通常采用的方法，如大型语言模型（LLMs），在处理这类任务时往往存在局限性。然而，人们已经努力通过知识图谱（KG）来弥补这些不足，尤其是通过将小规模的领域特定KG与通用KG相结合。尽管KG在知识表示方面具有优势，但构建它们的成本可能阻碍了广泛的研究和应用。为此，我们提出了一种框架，旨在通过链接到大规模通用KG来提升小型领域特定KG嵌入的学习性能。实验结果显示，这种方法带来了显著的提升，例如，Hits@10指标最高提高了44%。这一相对未被充分探索的研究方向有望促进KG在知识密集型任务中的更频繁运用，从而产生更为稳健、可靠的ML解决方案，它们相较于流行但易出错的LLM方法更具可靠性。关键词：知识图谱、知识图谱补全、实体对齐、表示学习、机器学习|
|**2024-05-17**|**Efficient Multimodal Large Language Models: A Survey**|Yizhang Jin et.al.|[2405.10739](http://arxiv.org/abs/2405.10739)|**[link](https://github.com/lijiannuist/efficient-multimodal-llms-survey)**|**在过去一年里，多模态大型语言模型（Multimodal Large Language Models，MLLMs）在诸如视觉问答、视觉理解和推理等任务上展现出卓越性能。然而，这些模型的庞大规模和高昂的训练与推理成本限制了它们在学术界和工业界的广泛应用。因此，研究高效且轻量级的MLLM具有巨大的潜力，特别是在边缘计算环境中。本综述全面系统地回顾了当前高效MLLM的研究现状。我们概述了代表性高效模型的发展历程，总结了有效结构和策略的研究状态，以及其实用应用。最后，我们讨论了当前高效MLLM研究的局限，并展望了有前景的未来发展方向。如需更多信息，请参考我们的GitHub仓库：https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey。**|
|**2024-05-17**|**INDUS: Effective and Efficient Language Models for Scientific Applications**|Bishwaranjan Bhattacharjee et.al.|[2405.10725](http://arxiv.org/abs/2405.10725)|null|大型通用语言模型在自然语言处理任务上表现出色。然而，先前的研究表明，针对特定领域的训练数据可以使模型在专业任务上表现更佳。为此，我们开发了INDUS，一套专为地球科学、生物学、物理学、太阳物理、行星科学和天文学领域设计的定制化语言模型。这些模型基于精心挑选的科学语料库，包括：（1）一个使用领域专用词汇和数据集训练的编码器，用于提升自然语言理解任务的表现；（2）一个基于对比学习的通用文本嵌入模型，利用多源数据集进行训练，以优化信息检索任务；（3）通过知识蒸馏技术缩小规模的模型，适用于对延迟和资源有限的应用。此外，我们创建了三个新的科学基准数据集：CLIMATE-CHANGE-NER（实体识别）、NASA-QA（抽取式问答）和NASA-IR（信息检索），以推动跨学科领域的研究进展。最后，实验结果显示，我们的模型在新任务和相关领域现有基准任务上均优于通用编码器（如RoBERTa）和现有的领域特定编码器（如SciBERT）。|
|**2024-05-16**|**UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models**|Sahel Sharifymoghaddam et.al.|[2405.10311](http://arxiv.org/abs/2405.10311)|null|## 背景  近期，多模态（MM）大型语言模型（LLMs）已经解锁了许多需要多模态理解（如图像描述或视觉问答）和生成（如文本引导的图像生成或编辑）复杂任务。为了进一步提升MM-LLMs的输出质量，我们提出了一种模型通用的UniRAG技术，它在推理阶段将相关检索信息添加到提示中，作为少量样例。与普遍认为检索增强（RA）主要改进罕见实体的生成或理解不同，我们在MSCOCO数据集上对包括GPT4、Gemini-Pro在内的专有模型以及Llava、LaVIT和Emu2等开源小型模型进行了评估，结果显示，这些模型在输入提示通过MM检索器（如UniIR模型）增强后，显著提高了生成质量。|
|**2024-05-16**|**4D Panoptic Scene Graph Generation**|Jingkang Yang et.al.|[2405.10305](http://arxiv.org/abs/2405.10305)|**[link](https://github.com/jingkang50/psg4d)**|**我们生活在一个三维空间中，同时通过第四维时间向前推进。为了使人工智能能够全面理解这种4D环境，我们提出了一种新的表示形式——4D全景场景图（PSG-4D），它将动态4D世界中的原始视觉数据抽象为节点和边，节点代表具有精确位置和状态信息的实体，边捕捉时间关系。为了促进在这一新领域的研究，我们构建了一个丰富的注释PSG-4D数据集，包含3000个RGB-D视频，总计100万帧，每帧都带有4D全景分割掩码以及详细的动态场景图标签。我们为此任务提出了一种名为PSG4DFormer的Transformer模型，该模型能够预测全景分割掩码，沿时间轴跟踪掩码，并通过关系组件生成相应的场景图。在新数据集上的大量实验表明，我们的方法为未来的PSG-4D研究提供了一个强大的基准。最后，我们展示了如何通过将大型语言模型融入我们的PSG-4D系统来实现动态场景理解的一个实际应用示例。**|
|**2024-05-16**|**HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models**|Rhea Sanjay Sukthanker et.al.|[2405.10299](http://arxiv.org/abs/2405.10299)|**[link](https://github.com/automl/hw-aware-llm-bench)**|**随着语言模型的规模不断扩大，对硬件指标（如延迟、能耗、GPU内存使用和性能）之间的权衡需求日益增长。人们正在寻求为不同语言模型配置建立帕累托前沿，以在指定硬件限制下找到最优模型。然而，对多种架构在多台设备上的全面训练和评估在计算上是不可行的。为此，我们提出了HW-GPT-Bench，这是一个基于硬件感知的语言模型代理基准，利用神经架构搜索（NAS）中的权重共享技术，在一个模型中高效地训练包含不同规模语言模型的超网络。我们在13种设备上对这些模型进行了性能剖析，考虑了5种硬件指标和3种不同的模型规模。最后，我们通过8种不同的多目标NAS算法展示了HW-GPT-Bench的可用性，并评估了由此产生的帕累托前沿的质量。我们的目标是推动和加速大型语言模型的多目标方法，如NAS和结构化剪枝的研究。**|
|**2024-05-16**|**Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**|Jianhao Chen et.al.|[2405.10288](http://arxiv.org/abs/2405.10288)|**[link](https://github.com/jianhaochen-nju/tsdre)**|**摘要：**  事实抽取对于构建知识图谱至关重要。随着对时间相关事实在下游任务中的需求增长，出现了时间性事实抽取的任务。本文特别关注从自然语言文本中提取时间性事实。先前的研究未能妥善处理复杂句子中时间与事实对应关系的建立难题。为解决这一挑战，我们提出了一种基于时间线的句子分解策略，利用大语言模型（LLMs）进行上下文学习，以实现对事实相关时间线的精细理解。然而，直接使用LLMs进行时间性事实抽取的性能并不理想。因此，我们引入了TSDRE方法，将LLMs的分解能力融入到小型预训练语言模型（PLMs）的传统微调过程中。  为了支持评估，我们构建了一个复杂的时序事实抽取数据集ComplexTRED。实验结果显示，TSDRE在HyperRED-Temporal和ComplexTRED数据集上实现了最先进的性能。|
|**2024-05-16**|**Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers**|Tuo Zhang et.al.|[2405.10276](http://arxiv.org/abs/2405.10276)|null|近年来，许多研究旨在通过策略性提示提升大型语言模型（LLMs）的效能。特别是优化通过prompting（OPRO）方法表现出顶尖性能，它利用LLMs作为优化器，目标是寻找能最大化任务准确性的指令。本论文重新审视了OPRO在小型LLMs（如LaMa-2系列和Mistral 7B）上的自动化提示效果。我们的研究表明，对于小型LLMs，OPRO的效果有限，因为其有限的推理能力限制了优化潜力。因此，我们建议未来的自动提示工程应同时考虑模型能力和计算成本。针对小型LLMs，我们推荐直接提供明确阐述目标和方法的指令，作为稳健的提示基线，以确保在当前研究中实现高效且有效的提示设计。|
|**2024-05-16**|**Keep It Private: Unsupervised Privatization of Online Text**|Calvin Bao et.al.|[2405.10260](http://arxiv.org/abs/2405.10260)|**[link](https://github.com/csbao/kip-privatization)**|**## 背景  作者身份混淆技术有望通过自动重写文本来保护网络通信中的个人隐私。然而，在自然语言处理（NLP）文献中，这些技术的评估大多局限在狭小场景下，主要依赖于表面的编辑操作，可能导致输出不自然。本研究提出了一种自动文本私密化框架，通过强化学习对大型语言模型进行微调，以生成兼顾准确、连贯和隐私的重写。我们在大规模的英语Reddit帖子测试集上进行了详尽的评估，该数据集由68,000名作者撰写，包含短到中等长度的文本。我们探讨了在不同评估条件下，如作者简介长度和作者识别策略，性能的变化。我们的方法在自动化指标和人工评估中保持高文本质量，并成功地规避了几种自动作者识别攻击。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|**[link](https://github.com/activevisionlab/awesome-llm-3d)**|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动人工智能体在空间理解与交互方面的发展。研究覆盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的结合，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。此外，我们还简要回顾了其他结合三维和语言的方法。本文的元分析显示了显著的进步，但也指出了挖掘3D-LLMs全部潜力所需的创新方法的必要性。因此，本文旨在为未来的研究方向提供指导，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本调查，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-16**|**A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks**|Xuanfan Ni et.al.|[2405.10251](http://arxiv.org/abs/2405.10251)|null|近期的研究已评估了大型语言模型（LLMs）在常识推理、数学推理和代码生成等方面的能力。然而，据我们所知，尚无专门针对自然语言生成（NLG）任务的深入研究，这是衡量模型优秀程度的关键标准。因此，本论文旨在全面评估知名且性能出色的LLMs，包括ChatGPT、ChatGLM、基于T5的模型、基于LLaMA的模型和Pythia模型，在对话生成和文本总结等NLG任务中的表现。我们选择了涵盖英语和中文的数据集，并设计了一种共同的评估框架，包括输入模板和后处理策略。研究结果报告了自动评分，同时进行了详细分析。|
|**2024-05-16**|**IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers**|Hao Yan et.al.|[2405.10250](http://arxiv.org/abs/2405.10250)|null|大型语言模型（LLMs）在根据自然语言描述自动生成可执行代码方面展现出巨大潜力，特别是通过互动功能，用户可以通过迭代反馈指导模型。然而，当前的互动方式往往假设用户具备调试源代码的专业知识，对非专业程序员不太友好。这使得使互动代码生成对不同编程水平的个体更易于使用成为一个挑战。为解决这个问题，我们提出了IntelliExplain，这是一种创新的人机交互范式，通过让用户通过自然语言解释与源代码互动，提升非专业人士的体验。用户通过提供他们发现错误的自然语言纠正反馈，来指导系统修订代码，直到用户对系统的代码解释感到满意。我们的用户研究显示，使用IntelliExplain的用户在Text-to-SQL和Python代码生成任务中的成功率分别比纯GPT-3.5提高了11.6%和25.3%，同时所需时间分别减少了39.0%和15.6%。|
|**2024-05-16**|**CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations**|Jiahao Zhao et.al.|[2405.10212](http://arxiv.org/abs/2405.10212)|**[link](https://github.com/CAS-SIAT-XinHai/CPsyExam)**|在这篇论文中，我们提出了一种创新的心理学基准测试——CPsyExam，它源于中国语言考试的问题。CPsyExam旨在分别强调心理学知识和案例分析的重要性，认识到将心理学知识应用于实际情境的价值。从22,000个问题库中，我们精选了4,000个来构建该基准，确保了主题的均衡覆盖，并包含了各种案例分析方法的多样性。此外，我们对一系列现有的大型语言模型（LLMs）进行了评估，包括开源和API基础的模型。实验和分析结果显示，CPsyExam是一个有效的确立语言模型对心理学理解能力的基准，同时支持在不同粒度上比较这些模型。|

<p align=right>(<a href=#updated-on-20241121>back to top</a>)</p>

