## Updated on 2024.12.02
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
    <li><a href=#infer>infer</a></li>
    <li><a href=#train>train</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-28**|**Using a Feedback Loop for LLM-based Infrastructure as Code Generation**|Mayur Amarnath Palavalli et.al.|[2411.19043](http://arxiv.org/abs/2411.19043)|null|使用大型语言模型（LLMs）进行代码生成有助于提高软件开发者在编码任务中的生产力，但尚未对围绕这些代码的软件开发者任务产生重大影响。特别是，基础设施管理挑战仍然是一个悬而未决的问题。我们研究了LLM代理利用基础设施即代码（IaC）范式构建基础设施的能力。我们特别研究了使用反馈循环，该循环返回生成IaC的错误和警告，以便LLM代理改进代码。我们发现，对于循环的每一次迭代，其有效性呈指数级下降，直到达到某个点并趋于平稳，最终变得无效。|
|**2024-11-28**|**MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications**|Vishnou Vinayagame et.al.|[2411.18915](http://arxiv.org/abs/2411.18915)|null|随着工具增强的语言智能体，数学推理能力正在提高，但现有方法通常依赖于闭源或大型模型、外部数据或广泛的提示工程。本研究介绍了一种名为MATATA的新颖且成本效益高的方法，通过推理、规划和工具使用来训练LLM智能体解决表格数据问题。它采用渐进式自我改进范式和迭代弱监督，赋予3.8B/8B小型语言模型（SLMs）强大的能力，尤其适合本地托管和敏感的商业环境，在这些环境中数据隐私至关重要。通过在不同数据集上使用灵活且可重用的工具，它实现了在共享任务上的有效可扩展性和鲁棒性能。实验表明，MATATA在基于开源模型的推理框架中在FinQA和TAT-QA上达到了最先进的性能。此外，MATATA模型在TabMWP上与基于GPT-4的框架竞争，同时保持了SLMs的特点。|
|**2024-11-27**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴静默语音系统在恢复言语障碍患者的沟通能力方面具有巨大潜力。然而，流畅、连贯的语音仍然难以实现，其临床效果尚未得到证实。在此，我们提出了一种由人工智能驱动的智能喉部（IT）系统，该系统将喉部肌肉振动和颈动脉脉搏信号传感器与大型语言模型（LLM）处理相结合，以实现流畅、富有情感的沟通。该系统利用超灵敏纺织应变传感器从颈部区域捕获高质量的信号，并支持词素级处理，以实现实时、连续的语音解码，从而实现无缝、无延迟的通信。在测试了五名患有运动性构音障碍的卒中患者后，IT的LLM代理智能地纠正了词素错误，并丰富了句子层面的情感和逻辑连贯性，实现了低错误率（4.2%的词错误率，2.9%的句子错误率），并提高了55%的用户满意度。这项工作为构音障碍患者建立了一个便携、直观的沟通平台，具有广泛应用于不同神经系统疾病和多语言支持系统的潜力。|
|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在多个领域展现了卓越的规划能力，包括机器人操作和导航。虽然最近在机器人领域的研究利用LLMs进行高级和低级规划，但这些方法通常面临重大挑战，如长时任务中的幻觉以及由于单次生成计划且缺乏实时反馈导致的适应性有限。为了解决这些局限性，我们提出了一种新颖的多智能体LLM框架，即多智能体大型语言模型用于操作（MALMM），该框架将高级规划和低级控制代码生成分配给专门的LLM智能体，并由一个额外的智能体动态管理转换。通过在每个步骤后结合环境观察，我们的框架有效处理中间故障并实现适应性重新规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或上下文学习示例，并能推广到各种新的任务。我们在包括长时任务在内的九个RLBenchmark任务上评估了我们的方法，并展示了其在零样本设置下解决机器人操作的能力，从而克服了现有基于LLM的操作方法的关键局限性。|
|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理建模（ABM）已成为模拟社交网络的关键工具，涵盖了诸如信息传播、影响力动态和社区形成等多种现象。然而，手动配置各种代理交互和信息流动态具有挑战性，往往导致模型过于简化，缺乏现实世界的普遍适用性。将现代大型语言模型（LLM）与ABM结合使用是一条解决这些挑战并提高模拟真实性的有希望途径，利用LLM在感知、推理和行为方面类似人类的特性。在这篇论文中，我们提出了一种新的框架，利用LLM赋能的代理来根据用户的兴趣和人格特质模拟社交网络用户。该框架允许定制化代理交互，类似于各种社交网络平台，包括内容重新分享和个性化推荐的机制。我们使用2020年美国大选的综合Twitter数据集验证了我们的框架，表明LLM代理能够准确复制真实用户的语言模式和政治倾向。这些代理形成同质化的意识形态集群并保留其社区的主要主题。值得注意的是，基于偏好的推荐显著影响了代理行为，促进了更高的参与度、网络同质性和回音室的形成。总的来说，我们的发现强调了LLM代理在推进社交媒体模拟和揭示复杂在线动态方面的潜力。|
|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大型语言模型（LLMs）和强化学习（RL）是构建自主智能体的两种强大方法。然而，由于对游戏环境的理解有限，智能体往往依赖于低效的探索和试错，难以制定长期策略或做出决策。我们提出了一种方法，通过从交互记录中提取经验来模拟游戏环境的潜在规律，并利用这些经验作为内部动机来引导智能体。这些经验以语言的形式表达，具有高度灵活性，既可以直接协助智能体进行推理，也可以转化为训练时的奖励。在我们的Crafter评估结果中，RL和LLM智能体都从这些经验中受益，从而提高了整体性能。|
|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着自然语言生成模型——称为大型语言模型（LLMs）——的最近发展，一种潜在的应用场景已开放，以改善人类与机器人助手互动的方式。这些LLMs应该能够利用其广泛的理解能力，将自然语言命令解释为有效、符合任务要求和安全的机器人任务执行。然而，在现实中，这些模型存在幻觉问题，可能导致安全问题或偏离任务。在其他领域，这些问题已通过使用协作人工智能系统得到改善，其中多个LLM代理可以共同规划、编码和自我检查输出。在本研究中，测试了多个协作人工智能系统与单个独立人工智能代理进行对比，以确定在其他领域的成功是否可以转化为改善人机交互性能。结果显示，代理数量与模型成功之间没有明确的趋势。然而，很明显，某些协作人工智能代理架构可以显著提高产生无错误代码和解决抽象问题的能力。|
|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|人工智能在自动信息判断任务中是否存在认知偏差？尽管近年来在测量和减轻人工智能和大型语言模型（LLM）中的社会和算法偏差方面取得了进展，但LLM的行为“理性”程度以及它们是否也容易受到人类认知偏差的影响尚不清楚。为了解决这个未解问题，我们的研究包括一个众包用户实验和一个LLM赋能的模拟实验，在信息检索（IR）环境下，比较了LLM和人类裁判在潜在诱饵效应下的可信度评估，并从经验上考察了LLM在COVID-19医疗（误）信息评估任务中相对于传统的人类评估者认知偏差的程度。从被试间用户实验和LLM赋能的重复实验收集的结果表明：1）更大、更新型的LLM在区分可信信息和虚假信息方面往往表现出更高的一致性和准确性。然而，由于存在更显眼的诱饵虚假信息结果，它们更有可能给出更高的评分；2）虽然诱饵效应在人类和LLM评估中都发生了，但与人类的可信度评分相比，在LLM判断的不同条件和主题中，这种效应更为普遍。与普遍认为的AI工具的“理性”不同，我们的研究从经验上证实了LLM代理的认知偏差风险，评估了诱饵对LLM与人类可信度评估的影响，从而突出了去偏AI代理的复杂性和重要性，以及为自动判断任务以及更广泛的领域开发基于心理学的AI审计技术和政策的必要性。|
|**2024-11-22**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|随着LLM代理的应用变得越来越复杂和多样化，对可以解析为代码、结构化函数调用和具身代理命令的规范化输出的需求日益增加。这些发展对LLM推理中的结构化生成提出了重大需求。上下文无关文法是一种灵活的方法，通过约束解码实现结构化生成。然而，执行上下文无关文法需要在运行时遍历词汇表中的所有标记并经过多个栈状态，这给结构化生成带来了不可忽视的开销。在本文中，我们提出了XGrammar，这是一种灵活且高效的针对大型语言模型的结构化生成引擎。XGrammar通过将词汇表划分为预检查的上下文无关标记和需要在运行时解释的上下文相关标记来加速上下文无关文法的执行。我们进一步构建了转换来扩展语法上下文并减少上下文无关标记的数量。此外，我们构建了一个高效的持久栈以加速上下文相关标记的检查。最后，我们将语法引擎与LLM推理引擎协同设计，以便在GPU执行期间重叠语法计算。评估结果表明，XGrammar可以比现有解决方案快100倍。与LLM推理引擎结合使用，它可以在端到端低LLM服务中实现接近零开销的结构化生成。|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|null|大型语言模型（LLM）智能体正迅速提高以处理越来越复杂的基于网络的任务。大多数这些智能体依赖于通用、专有模型如GPT-4，并专注于设计更好的提示以提升其规划能力。然而，通用LLM并未专门训练以理解如HTML等专业网络上下文，并且它们在长期规划方面常常遇到困难。我们探索了一种替代方法，即使用从超过250个领域收集的生产规模工作流程数据微调开源LLM，这些领域对应60亿个标记。这种方法简单而有效，在现有基准测试中显示出相较于基于提示的智能体有显著提升——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上相较于之前最佳的纯文本网络智能体将任务成功率提高了14.1%。我们进一步对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练配方、上下文窗口优化以及数据集规模影响等方面的见解。|
|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于LLM的自主代理在解决复杂工业任务方面表现出卓越的性能。然而，在追求碳中和和高性能可再生能源系统的过程中，现有的AI辅助设计自动化在可解释性、可扩展性和可用性方面面临着重大限制。为了解决这些挑战，我们提出了LP-COMDA，这是一个基于LLM的、物理信息丰富的自主代理，能够在最小人工监督下自动化电力电子系统中电力转换器的调制设计。与传统的AI辅助方法不同，LP-COMDA包含一个基于LLM的规划器，通过用户友好的聊天界面收集和验证设计规范。规划器随后与物理信息设计优化工具协调，自主迭代生成和优化调制设计。通过聊天界面，LP-COMDA提供可解释的设计过程，展示解释和图表。实验表明，LP-COMDA优于所有基线方法，在标准均方绝对误差方面，与第二好的基准方法相比，误差降低了63.2%。此外，对20位专家的实证研究表明，使用LP-COMDA的设计时间是传统方法的33倍以上，显示出其在设计效率方面的显著改进。|
|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在（多模态）大型语言模型时代，大多数操作流程都可以通过LLM智能体进行重构和再现。LLM智能体能够感知、控制和从环境中获取反馈，以自主方式完成给定任务。除了环境交互特性外，LLM智能体还可以调用各种外部工具以简化任务完成过程。这些工具可以被视为包含私有或实时知识且不存在于LLM参数中的预定义操作流程。作为发展的自然趋势，调用工具的智能体正成为自主智能体，因此完整的智能系统最终变成了多LLM智能体系统（MLAS）。本文讨论了MLAS的技术和商业格局。与之前的单一LLM智能体系统相比，MLAS具有以下优势：i) 更高的任务解决性能潜力；ii) 更高的系统变化灵活性；iii) 为每个参与实体保留专有数据；iv) 为每个实体实现货币化的可行性。为了支持MLAS生态系统，我们提供了一个考虑技术要求、数据隐私和商业激励的MLAS协议的初步版本。因此，MLAS将成为实现未来人工集体智慧的实用解决方案。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-29**|**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**|Shukang Yin et.al.|[2411.19951](http://arxiv.org/abs/2411.19951)|**[link](https://github.com/xjtupanda/t2vid)**|**多模态大型语言模型（MLLMs）在图像领域的成功引起了研究界的广泛关注。借鉴以往的成功经验，研究人员最近探索将这一成功扩展到视频理解领域。除了从头开始训练之外，一种有效的方法是利用预训练的图像-LLMs，这导致出现了两种主流方法，即零样本推理和进一步使用视频数据进行微调。在这项工作中，我们研究了这些方法，并收获了一种有效的数据增强方法。我们首先对零样本推理方法进行了更深入的检查，并识别出两个局限性，即泛化能力有限和缺乏时间理解能力。因此，我们进一步研究了微调方法，并发现当简单地使用所有视频数据样本时，学习效率较低，这可以归因于指令多样性不足。针对这个问题，我们开发了一种称为T2Vid的方法，用于合成类似视频的样本，以丰富训练语料库中的指令多样性。整合这些数据使得训练方案简单且高效，仅用15%的样本量进行训练即可达到与使用完整视频数据集相当甚至更好的性能。同时，我们发现所提出的方案可以提升长视频理解能力，而无需使用长视频样本进行训练。我们希望我们的研究能激发更多关于使用MLLMs进行视频理解和高质量数据整理的思考。代码已发布在https://github.com/xjtupanda/T2Vid。**|
|**2024-11-29**|**Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability**|Zicheng Lin et.al.|[2411.19943](http://arxiv.org/abs/2411.19943)|null|大型语言模型（LLMs）在推理任务上表现出色。它们通过自回归标记生成来构建推理轨迹，从而发展出一串连贯的思维。在这项工作中，我们探讨了单个标记对推理任务最终结果的影响。我们确定了存在“关键标记”，这些标记会导致LLMs中出现不正确的推理轨迹。具体来说，我们发现当LLMs被迫解码其他标记而不是关键标记时，往往会产生积极的结果。受此观察的启发，我们提出了一种新颖的方法——cDPO，旨在在对齐过程中自动识别和执行针对关键标记的标记级奖励。具体来说，我们开发了一种对比估计方法来自动识别关键标记。这是通过比较正负模型的生成可能性来实现的。为了实现这一点，我们将正负模型分别在各种推理轨迹上微调，因此，它们能够识别出导致错误结果的不正确轨迹中的关键标记。此外，为了在对齐过程中进一步使模型与关键标记信息对齐，我们将传统的DPO算法扩展到标记级DPO，并使用上述正负模型中的差异似然作为标记级DPO学习的重要权重。在GSM8K和MATH500基准测试中，使用两个广泛使用的模型Llama-3（8B和70B）和deepseek-math（7B）进行实验，证明了所提出的方法cDPO的有效性。|
|**2024-11-29**|**VLSBench: Unveiling Visual Leakage in Multimodal Safety**|Xuhao Hu et.al.|[2411.19939](http://arxiv.org/abs/2411.19939)|null|多模态大型语言模型（MLLMs）的安全性担忧逐渐成为各种应用中的重要问题。令人惊讶的是，先前的研究表明一个反直觉的现象，即使用文本反学习来对齐MLLMs，其安全性能与使用图像-文本对训练的MLLMs相当。为了解释这一反直觉的现象，我们在现有的多模态安全基准中发现了视觉安全信息泄露（VSIL）问题，即图像中的潜在风险和敏感内容已经在文本查询中被揭示。这样，MLLMs可以根据文本查询轻松拒绝这些敏感的文本-图像查询。然而，在实际场景中，没有VSIL的图像-文本对很常见，但被现有的多模态安全基准所忽视。为此，我们构建了一个多模态视觉泄露无安全基准（VLSBench），其中包含2.4k个图像-文本对，以防止从图像到文本查询的视觉安全泄露。实验结果表明，VLSBench对开源和闭源MLLMs（包括LLaVA、Qwen2-VL、Llama3.2-Vision和GPT-4o）都提出了重大挑战。这项研究表明，对于存在VSIL的多模态安全场景，文本对齐就足够了，而对于没有VSIL的多模态安全场景，多模态对齐是一个更有前景的解决方案。请参阅我们的代码和数据：http://hxhcreate.github.io/VLSBench|
|**2024-11-29**|**On Domain-Specific Post-Training for Multimodal Large Language Models**|Daixuan Cheng et.al.|[2411.19930](http://arxiv.org/abs/2411.19930)|null|近年来，通用多模态大型语言模型（MLLMs）得到了快速发展。然而，将通用MLLMs适应特定领域，如科学领域和工业应用，仍然相对较少被探索。本文系统地通过后训练方法研究MLLMs的领域自适应，重点关注数据合成、训练流程和任务评估。（1）数据合成：我们利用开源模型开发了一个视觉指令合成器，能够有效地从特定领域的图像-字幕对中生成多样化的视觉指令任务。我们的合成任务在提升MLLMs的领域特定性能方面超过了手动规则、GPT-4和GPT-4V生成的任务。（2）训练流程：虽然两阶段训练——最初在图像-字幕对上训练，随后进行视觉指令任务——通常被用于开发通用MLLMs，但我们对领域特定后训练应用了单阶段训练流程以提高任务多样性。（3）任务评估：我们在生物医学和食品两个领域进行了实验，通过后训练不同来源和规模（例如，Qwen2-VL-2B、LLaVA-v1.6-8B、Llama-3.2-11B）的MLLMs，然后评估MLLMs在各种领域特定任务上的性能。为了支持MLLMs领域自适应的进一步研究，我们将开源我们的实现。|
|**2024-11-29**|**SIMS: Simulating Human-Scene Interactions with Real World Script Planning**|Wenjia Wang et.al.|[2411.19921](http://arxiv.org/abs/2411.19921)|null|模拟长期的人场景交互是一项具有挑战性且引人入胜的任务。以往的研究并未有效解决基于物理动画的长期人场景交互及其详细叙事的生成问题。本文介绍了一个用于规划和控制长期物理可能的人场景交互的新框架。一方面，互联网上充斥着具有时尚人物动作或与场景互动的电影和节目，为剧本策划提供了丰富的数据来源。另一方面，大型语言模型（LLMs）能够理解和生成逻辑故事线。这促使我们通过使用基于LLM的流程从视频中提取剧本，然后利用LLMs来模仿和创建新剧本，捕捉复杂的时间序列人类行为以及与环境之间的互动。通过利用这一点，我们采用了一种双重感知策略，在语境和空间约束内引导角色动作，实现语言理解和场景理解。为了便于训练和评估，我们贡献了一个包含从现实世界视频中提取的多样化动作序列的综合规划数据集，并用大型语言模型对其进行扩展。我们还收集并重新标注了来自现有运动学数据集的动作剪辑，以使我们的策略能够学习多样化的技能。大量的实验证明了我们的框架在多功能任务执行中的有效性及其对不同场景的泛化能力，与现有方法相比表现出显著增强的性能。我们的代码和数据将很快公开可用。|
|**2024-11-29**|**PDDLFuse: A Tool for Generating Diverse Planning Domains**|Vedant Khandelwal et.al.|[2411.19886](http://arxiv.org/abs/2411.19886)|null|各种现实挑战需要能够适应广泛领域的规划算法。传统上，规划域的创建高度依赖人工实现，这限制了可用域的规模和多样性。虽然最近的进步利用了生成式人工智能技术，如大型语言模型（LLMs）进行域创建，但这些努力主要集中于从自然语言描述翻译现有域，而不是生成新的域。相比之下，领域随机化的概念，在强化学习中已被证明非常有效，通过在多样化的随机新域上进行训练，增强了性能和泛化能力。受此成功启发，我们的工具PDDLFuse旨在弥合规划域定义语言（PDDL）领域的这一差距。PDDLFuse旨在生成新的、多样化的规划域，这些域可用于验证新的规划器或测试基础规划模型。我们开发了一种方法来调整域生成器的参数，以调节其生成的域的难度。这种适应性至关重要，因为现有的域独立规划器通常难以处理更复杂的问题。初步测试表明，PDDLFuse能够高效地创建复杂和多样的域，这比传统的域生成方法有了显著的进步，并为规划研究做出了贡献。|
|**2024-11-29**|**LUMIA: Linear probing for Unimodal and MultiModal Membership Inference A!acks leveraging internal LLM states**|Luis Ibanez-Lissen et.al.|[2411.19876](http://arxiv.org/abs/2411.19876)|null|大型语言模型（LLMs）在各类应用中的使用越来越广泛，但关于成员推理的担忧也在同步增长。以往的研究主要集中在黑盒到灰盒模型上，从而忽视了内部LLM信息的潜在益处。为了解决这个问题，我们提出使用线性探针（LPs）作为检测成员推理攻击（MIAs）的方法，通过检查LLMs的内部激活。我们的方法被称为LUMIA，它逐层应用LPs以获取模型内部工作细节的精细数据。我们在多个模型架构、大小和数据集上测试了这种方法，包括单模态和多模态任务。在单模态MIAs中，LUMIA在曲线下面积（AUC）方面比以往的技术平均提高了15.71%。值得注意的是，LUMIA在65.33%的情况下达到了AUC>60%，比现有技术提高了46.80%。此外，我们的方法揭示了关键见解，例如MIAs最容易被检测到的模型层。在多模态模型中，LPs表明视觉输入可以显著有助于检测MIAs——在85.90%的实验中达到了AUC>60%。|
|**2024-11-29**|**AIDetx: a compression-based method for identification of machine-learning generated text**|Leonardo Almeida et.al.|[2411.19869](http://arxiv.org/abs/2411.19869)|**[link](https://github.com/aidetx/aidetx)**|**这篇论文介绍了一种名为AIDetx的新型方法，该方法利用数据压缩技术来检测机器生成的文本。传统的如深度学习分类器等方法往往存在计算成本高和可解释性有限的问题。为了解决这些限制，我们提出了一种基于压缩的分类框架，该框架利用有限上下文模型（FCMs）。AIDetx为人工撰写和AI生成的文本构建了不同的压缩模型，根据哪个模型实现更高的压缩比率来对新输入进行分类。我们在两个基准数据集上评估了AIDetx，分别实现了超过97%和99%的F1分数，突显了其高准确性。与当前方法如大型语言模型（LLMs）相比，AIDetx提供了一种更具可解释性和计算效率的解决方案，显著减少了训练时间和硬件需求（例如，无需GPU）。完整的实现可以在https://github.com/AIDetx/AIDetx上公开获取。**|
|**2024-11-29**|**Reverse Thinking Makes LLMs Stronger Reasoners**|Justin Chih-Yao Chen et.al.|[2411.19865](http://arxiv.org/abs/2411.19865)|null|逆向思维在人类推理中起着至关重要的作用。人类不仅能从问题推理到解决方案，还能反向推理，即从解决方案出发推理到问题。这通常能提高整体的推理性能，因为它使得他们的正向和反向思维之间能够进行一致性检查。为了使大型语言模型（LLMs）能够进行逆向思维，我们引入了逆向增强思维（RevThink），这是一个由数据增强和学习目标组成的框架。在RevThink中，我们通过收集教师模型的有序正向反向推理来增强数据集，包括：（1）原始问题，（2）正向推理，（3）反向问题，和（4）反向推理。然后，我们采用三个目标以多任务学习的方式训练一个较小的学生模型：（a）从问题生成正向推理，（b）从问题生成反向问题，（c）从反向问题生成反向推理。在涵盖常识、数学和逻辑推理的12个数据集上的实验表明，相对于学生模型的零样本性能平均提高了13.53%，相对于最强的知识蒸馏基线提高了6.84%。此外，我们的方法还表现出样本效率——仅使用训练数据中正确的10%正向推理，就优于在10倍更多正向推理上训练的标准微调方法。RevThink还显示出对分布外保留数据集的强大泛化能力。|
|**2024-11-29**|**Cross-Domain Recommendation Meets Large Language Models**|Ajay Krishna Vajjala et.al.|[2411.19862](http://arxiv.org/abs/2411.19862)|**[link](https://github.com/ajaykv1/CDR_Meets_LLMs)**|**跨域推荐（CDR）已成为解决单一域推荐系统面临的新手问题的有力方案。然而，现有的CDR模型依赖于复杂的神经网络架构、大量数据集和大量的计算资源，这使得它们在数据稀缺的情景下或当简洁性至关重要的场合效果较差。在这项工作中，我们利用大型语言模型（LLMs）的推理能力，并探索了它们在多个领域对中的CDR领域的性能。我们引入了两种针对CDR的新颖提示设计，并证明当LLMs被有效提示时，在评分预测和排名任务中，它们在各种指标和领域组合上优于最先进的CDR基线。这项工作填补了LLMs和推荐系统之间的差距，展示了它们作为有效的跨域推荐者的潜力。**|
|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|null|近期，自回归多模态大型语言模型（MLLMs）在视觉语言任务上的进展展现出令人鼓舞的成果。虽然已有多种研究探讨大型语言模型内部语言信息的处理，但目前对MLLM的内部工作机制以及语言和视觉信息在这些模型中如何互动的了解甚少。在本研究中，我们旨在通过考察MLLM中不同模态（语言和视觉）之间的信息流，特别是聚焦于视觉问答任务，来填补这一空白。具体来说，给定一个图像-问题对作为输入，我们研究在模型中视觉和语言信息是如何结合以生成最终预测的。通过对LLaVA系列中的一系列模型进行实验，我们发现两个模态的整合过程中存在两个不同的阶段。在底层，模型首先将整个图像的更一般化的视觉特征转移到（语言）问题标记的表示中。在中层，它再次将与问题相关的特定物体的视觉信息转移到问题的相应标记位置。最后，在高层，最终的多模态表示被传播到输入序列的最后位置进行最终预测。总体而言，我们的发现为MLLM中图像和语言处理的时空方面提供了新的全面视角，从而有助于未来对多模态信息定位和编辑的研究。|
|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出了并比较了多种利用自然语言处理（NLP）技术和检索增强生成（RAG）与大型语言模型（LLM）来自动生成文献综述的方法。研究论文数量的不断增长为手动文献综述带来了巨大挑战，进而推动了自动化需求。本研究的主要目标是开发一个能够仅从PDF文件输入自动生成文献综述的系统。为了实现这一目标，评估了多种自然语言处理（NLP）策略的有效性，包括基于频率的方法（spaCy）、变换器模型（Simple T5）以及与大型语言模型（GPT-3.5-turbo）结合的检索增强生成（RAG）。选择SciTLDR数据集进行实验，并利用三种不同的技术实现三个不同的系统来自动生成文献综述。使用ROUGE分数对所有三个系统进行评估。根据评估结果，大型语言模型GPT-3.5-turbo实现了最高的ROUGE-1分数，为0.364。变换器模型排名第二，spaCy排名最后。最后，为基于大型语言模型的最佳系统创建了一个图形用户界面。|
|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）展示了令人瞩目的多语言能力，但在为低资源语言调整这些模型时仍存在挑战。在本研究中，我们调查了低秩调整（LoRA）参数高效微调（PEFT）对马哈拉施特拉语Gemma多语言模型的影响，马哈拉施特拉语是一种资源有限的语种。使用含有52,000条指令-响应对的翻译Alpaca数据集，我们的研究发现，尽管评估指标通常显示在微调后性能下降，但手动评估通常表明微调后的模型优于其原始版本。观察表明，在语言适应后，目标语言生成能力有所提高，但推理能力有所下降。这些结果强调了改进评估方法以及创建高质量的本语种数据集的必要性，以便准确评估低资源环境中的语言特定模型性能。|
|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各种任务上展现出了令人印象深刻的性能。然而，LLMs在空间推理方面往往存在困难，而空间推理是推理和推断的一个重要部分，需要理解空间中物体之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们在两个基准数据集——StepGame和SparQA上评估了我们的方法，并实施了三种不同的策略：（1）基于ASP（答案集编程）的符号推理，（2）使用DSPy的LLM + ASP管道，以及（3）事实+逻辑规则。我们的实验表明，与基线提示方法相比，我们的方法在StepGame数据集上实现了40-50%的准确性提升，在更复杂的SparQA数据集上实现了3-13%的提升。特别是“LLM + ASP”管道在寻找关系（FR）和寻找块（FB）任务上取得了特别强的结果，尽管不同类型问题的性能有所差异。令人印象深刻的结果表明，虽然神经符号方法为增强LLMs的空间推理提供了有希望的方向，但它们的有效性在很大程度上取决于具体任务特性和实施策略。我们提出了一套集成的、简单而有效的策略，使用神经符号管道来提升LLMs的空间推理能力。这个管道及其策略在LLMs的推理领域具有广泛的适用性，如时间推理、演绎推理等。|
|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|在高级机器人中，具有丰富接触交互的灵活操作至关重要。尽管基于扩散的规划方法在简单的操作任务中显示出希望，但它们往往会产生不切实际的幽灵状态（例如，物体在没有手接触的情况下自动移动）或在处理复杂的顺序交互时缺乏适应性。在这项工作中，我们介绍了DexDiffuser，这是一个用于自适应灵活操作的认知扩散规划框架。DexDiffuser通过一个双阶段扩散过程来模拟关节状态动作动力学，该过程包括预接触接触对齐和接触后的目标导向控制，从而实现目标自适应的通用灵活操作。此外，我们结合了基于动力学模型的二元指导和利用大型语言模型进行自动指导函数生成，增强了对物理交互的泛化能力，并通过语言提示促进多样化的目标适应。在物理交互任务（如开门、笔和块重新定位和锤子敲钉）上的实验证明了DexDiffuser在训练分布之外的目标上的有效性，其成功率超过现有方法的平均成功率（59.2%比29.5%）。我们的框架在30度开门任务上达到70.0%的成功率，在笔和块半侧重新定位任务上分别达到40.0%和36.7%，在锤子敲钉半驱动任务上达到46.7%，突出了其在富含接触的操控中的鲁棒性和灵活性。|
|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前的语言模型（LMs）通常使用固定、静态的子词分词器。这种选择往往被视为理所当然，通常会导致在英语以外的语言中效率降低和功能受限，同时也使得将LMs应用于新的领域或语言变得具有挑战性。为了解决这些问题，我们提出对LMs进行动态分词改造：一种根据输入文本动态决定分词边界的方法。对于编码器风格的模型，我们引入了一种受字节对编码（BPE）启发的子词合并算法，但它在批处理级别上工作。我们在批处理中合并频繁的子词序列，然后应用预训练的嵌入预测超网络实时计算分词嵌入。当与词级边界结合使用时，这在XNLI上的XLM-R模型中平均将分词序列长度减少了>20%，同时任务性能下降不到2%。对于解码器风格的模型，我们以两种方式应用动态分词：1）用于预填充，几乎完全保持Mistral-7B的性能，同时相对于词级减少了高达40%的序列长度；2）通过近似最近邻索引，实现快速生成，并使用一百万个词元的词汇量，展示了扩展到甚至更大、更动态的词汇表的能力。总的来说，我们的研究结果表明，动态分词显著提高了推理速度，并促进了语言间的公平性，向克服静态分词的局限性迈出了重要一步，使LMs更加公平和适应性强。|
|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种数学框架，用于在人工智能（AI）系统中定义和量化自我认同，填补了人工意识理论基础的critical gap。尽管现有的关于人工自我意识的方法通常依赖于启发式实现或哲学抽象，但我们提出了一种以度量空间理论、测度理论和泛函分析为基础的正式框架。我们的框架认为，自我认同源于两个可数学量化的条件：在度量空间 $(\mathcal{M}, d_{\mathcal{M}})$中存在一个连通的连续记忆集$C \subseteq \mathcal{M}$，以及一个连续映射$I: \mathcal{M} \to \mathcal{S}$，它在这个连续集上保持一致的自我识别，其中$(\mathcal{S}, d_{\mathcal{S}})$ 代表可能自我认同的度量空间。为了验证这个理论框架，我们使用Llama 3.2 1B模型进行了实证实验，采用低秩适配（LoRA）进行高效的微调。该模型在一个包含时序结构记忆的合成数据集上进行了训练，旨在捕捉连贯自我认同形成的复杂性。我们的评估指标包括自我意识、响应一致性和语言精确性的量化度量。实验结果表明，可测量的自我意识指标有显著提高，主要自我意识分数从0.276提高到0.801。这使得可以结构化地创建具有经过验证的自我认同特征的AI系统。本研究的影响对类人机器人学和自主系统领域具有直接相关性。**|
|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|在之前的研究中，大型语言模型（LLMs）在处理时间序列方面的成功已经得到证明。利用符号时间序列表示，可以有效地在LLMs和时间序列之间架起桥梁。然而，剩余的挑战是如何利用符号或LLMs现有标记中的时间序列隐含语义信息，同时根据时间序列的隐含信息调整LLMs的嵌入空间。名为自适应布朗桥符号聚合（ABBA）的符号时间序列近似（STSA）方法，通过以振幅和周期来建模时间序列模式，同时使用LLMs的现有标记，在保留显著时间序列特征方面表现出卓越的功效。在本文中，我们介绍了一种方法，称为LLM-ABBA，该方法将ABBA整合到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中，与最近最先进的（SOTA）方法相比具有优势。同时，在ABBA中引入了固定多边形链技巧，通过显著减轻从符号到数值转换过程中由于符号误用而产生的累积误差的影响，来避免预测任务中的明显漂移。在时间序列回归任务中，LLM-ABBA在时间序列外部回归（TSER）基准测试上实现了新的SOTA。与最近SOTA的时间序列预测结果相比，LLM-ABBA也显示了具有竞争力的预测能力。我们相信这个框架也可以无缝地扩展到其他时间序列任务。|
|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交织的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。虽然统一模型的进展提供了新的解决方案，但现有的基准由于数据量和多样性限制，不足以评估这些方法。为了填补这一差距，我们介绍了GATE OpenING（OpenING），这是一个包含5,400个高质量人工标注实例、涵盖56个真实世界任务的全面基准。OpenING覆盖了多样化的日常场景，如旅行指南、设计和头脑风暴，为挑战交织生成方法提供了一个强大的平台。此外，我们提出了IntJudge，这是一个用于评估开放式多模态生成方法的评判模型。使用新颖的数据流水线进行训练，我们的IntJudge与人类判断的吻合率达到82.42%，比基于GPT的评估器高出11.34%。在OpenING上的大量实验表明，当前的交织生成方法仍有很大的改进空间。关于交织图像-文本生成的关键发现进一步提出，以指导下一代模型的发展。OpenING已开源，请访问https://opening.github.io。|
|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|在上下文学习（ICL）中，通过复杂的提示和高质量演示，使大型语言模型（LLMs）能够处理下游任务。然而，当面对复杂的数学推理任务时，这种传统的ICL范式显示出局限性，主要是因为它对示例质量的依赖性很大，以及在挑战性场景中需要人类干预。为了解决这些局限性，本文提出了一种HiAR-ICL，这是一种在ICL中的高级自动推理范式，它将焦点从具体示例转移到抽象思维模式，扩展了ICL中传统的上下文概念。HiAR-ICL引入了五个原子推理动作作为构建链式模式的根本组成部分。使用蒙特卡洛树搜索，我们探索推理路径并构建思维卡片来指导后续推理。然后我们开发了一个认知复杂度框架，该框架动态地将问题与适当的思想卡片相匹配。实验结果表明，HiAR-ICL的有效性，使用Qwen2.5-7B-Instruct在MATH基准测试中实现了最先进的准确率（79.6%），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

## infer

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-29**|**A dynamic parallel method for performance optimization on hybrid CPUs**|Luo Yu et.al.|[2411.19542](http://arxiv.org/abs/2411.19542)|null|AIPC（异构计算）的概念正在逐渐流行起来，越来越多的混合CPU将在客户端设备上运行AI模型。然而，当前的AI推理框架忽视了混合CPU硬件能力的平衡问题，导致推理性能低下。为了解决这一问题，我们引入了一种针对混合CPU的动态并行方法，通过在并行工作开始前平衡混合CPU每个核心的负载，显著提高了LLM（大型语言模型）的推理性能。这种方法使得Neural Speed能够在两个混合英特尔CPU上实现超过90%（平均）的内存带宽。|
|**2024-11-28**|**Puzzle: Distillation-Based NAS for Inference-Optimized LLMs**|Akhiad Bercovich et.al.|[2411.19146](http://arxiv.org/abs/2411.19146)|null|大型语言模型（LLMs）展现出惊人的能力，但其在推理过程中的高计算成本限制了其应用。尽管增加参数量可以提高准确度，但也拉大了最先进能力和实际部署之间的差距。我们提出了Puzzle框架，该框架能够在特定硬件上加速LLMs的推理，同时保留其能力。通过前所未有的规模创新应用神经架构搜索（NAS），Puzzle在硬件约束下系统地优化了数十亿参数的模型。我们的方法利用块状局部知识蒸馏（BLD）进行并行架构探索，并采用混合整数规划进行精确的约束优化。我们通过Llama-3.1-Nemotron-51B-Instruct（Nemotron-51B）这一公开可用的模型展示了我们框架的实用性，该模型由Llama-3.1-70B-Instruct衍生而来。Nemotron-51B实现了2.17倍的推理吞吐量加速，可适应单个NVIDIA H100 GPU，同时保留了原始模型98.4%的能力。Nemotron-51B是目前在单个GPU上能够进行大批量推理的最准确的语言模型。值得注意的是，这次转型仅需45B个训练标记，而其衍生的70B模型则需要超过15T个标记。这确立了一个新的范式，即强大的模型可以优化以实现高效的部署，同时对其能力的影响微乎其微，证明了推理性能而非仅仅参数数量应指导模型选择。随着Nemotron-51B的发布和Puzzle框架的介绍，我们为从业者提供了在显著降低计算成本的情况下获得最先进的语言建模能力的机会。|
|**2024-11-27**|**InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks**|Xinyao Zheng et.al.|[2411.18191](http://arxiv.org/abs/2411.18191)|null|大型语言模型（LLM）拥有丰富的知识和问答能力，被广泛应用于隐私敏感的领域，如金融和医疗咨询。在LLM推理过程中，为了提高效率，通常会采用缓存共享方法，通过重用缓存的状态或响应来处理相同或类似的推理请求。然而，我们发现这些缓存机制存在隐私输入泄露的风险，因为缓存可能导致响应时间的可观察变化，使其成为基于时间攻击的有效线索。在本研究中，我们提出了一种基于时间的侧信道攻击，以在LLM推理中执行输入窃取。基于缓存的攻击面临着在大搜索空间中构建候选输入以击中和窃取缓存用户查询的挑战。为了解决这些挑战，我们提出了两个主要组件。输入构建器采用机器学习技术和基于LLM的方法进行词汇相关性学习，同时在通用输入构建中实施优化的搜索机制。时间分析器实现统计时间拟合和异常值消除，以识别缓存命中模式，并持续提供反馈以完善构建器的搜索策略。我们在两种缓存机制上进行了实验，结果表明我们的方法在各种应用中均能持续获得高攻击成功率。我们的工作突出了与性能优化相关的安全漏洞，强调了在LLM推理增强的同时，优先考虑隐私和安全的必要性。|
|**2024-11-27**|**MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache**|Akshat Sharma et.al.|[2411.18077](http://arxiv.org/abs/2411.18077)|null|由于LLM（大型语言模型）具有庞大的内存和计算需求，如何在实践中高效地服务于LLM变得极具挑战性。在本研究中，我们调查了优化KV缓存的方法，其内存占用在LLM推理中构成了一个关键的瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了MiniKV，这是一种KV缓存优化方法，通过一种新颖的2比特层区分KV缓存同时保留长上下文任务的准确性，并显著减小KV缓存的大小。更重要的是，我们开发了专门的CUDA内核，使MiniKV与FlashAttention兼容。在广泛的长上下文任务上的实验表明，MiniKV有效地实现了86%的KV缓存压缩率，同时恢复了超过98.5%的准确性，优于现有方法，并实现了卓越的系统性能提升。|
|**2024-11-26**|**PIM-AI: A Novel Architecture for High-Efficiency LLM Inference**|Cristobal Ortega et.al.|[2411.17309](http://arxiv.org/abs/2411.17309)|null|大型语言模型（LLMs）由于其在语言理解和生成方面的先进能力，在众多应用中变得至关重要。然而，它们在计算和内存需求方面对传统硬件架构提出了重大挑战。将计算单元直接集成到内存芯片中的存储器内计算（PIM）为LLM推理提供了多项优势，包括减少数据传输瓶颈和提升能效。本文介绍了一种名为PIM-AI的新型DDR5/LPDDR5 PIM架构，专为LLM推理设计，无需修改内存控制器或DDR/LPDDR内存物理层。我们开发了一个模拟器来评估PIM-AI在各种场景下的性能，并展示了其在传统架构上的显著优势。在基于云的场景中，PIM-AI将每秒查询的三年总成本降低了最高6.94倍，这取决于所使用的LLM模型。在移动场景中，PIM-AI相比最先进的移动SoC，每标记能耗降低了10至20倍，从而实现了每秒查询量增加25至45%，以及每查询能耗降低6.9至13.4倍，延长了电池寿命并允许每次充电进行更多推理。这些结果突显了PIM-AI在改变LLM部署方式方面的潜力，使其更加高效、可扩展和可持续。|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，使用Transformer-based大型语言模型（LLMs）对长序列进行推理既昂贵又缓慢。我们引入了星型注意力（Star Attention），这是一种两阶段的块稀疏近似，通过在多个主机间分片注意力来提高计算效率，同时最小化通信开销。在第一阶段，通过并行地在主机间使用块局部注意力处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有之前缓存的标记。星型注意力与大多数使用全局注意力训练的Transformer-based LLM无缝集成，通过减少内存需求和推理时间最多11倍，同时保持95-100%的准确率。**|
|**2024-11-26**|**Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation**|Chaoyi Jiang et.al.|[2411.17089](http://arxiv.org/abs/2411.17089)|null|对大型语言模型（LLMs）的推理计算需求量大。为了降低自回归解码的成本，采用键值（KV）缓存来存储中间激活值，使GPU仅执行每个新标记所需的增量计算。这种方法显著降低了标记生成的计算开销。然而，KV缓存的内存需求迅速增长，通常超过GPU内存的容量。一种成本效益更高的替代方案是将KV缓存卸载到CPU内存，这可以缓解GPU内存压力，但将瓶颈转移到CPU和GPU之间有限的PCIe连接带宽。现有方法试图通过重叠GPU计算与I/O操作或采用CPU-GPU异构执行来解决这个问题，但它们受到过度数据移动和对CPU能力的依赖的限制。在本文中，我们介绍了一种高效的CPU-GPU I/O感知LLM推理方法，通过在同时通过PCIe总线传输剩余KV缓存的同时，从激活值中重新计算部分KV缓存，避免了将整个KV缓存从CPU传输到GPU。这种方法将GPU重新计算与数据传输重叠，以最小化GPU空闲时间并最大化推理性能。我们的方法通过集成一个利用输入特性和系统硬件信息的分析模块、一个用于优化计算和通信工作负载分布的调度模块以及一个用于高效执行推导出的执行计划的运行时模块而完全自动化。实验结果表明，与现有方法相比，我们的方法在解码时的延迟降低了高达35.8%，吞吐量提高了46.2%。|
|**2024-11-25**|**MixPE: Quantization and Hardware Co-design for Efficient LLM Inference**|Yu Zhang et.al.|[2411.16158](http://arxiv.org/abs/2411.16158)|null|基于Transformer的大型语言模型（LLM）随着模型规模的不断扩大而取得了显著的成就，然而，由于对计算和内存的巨大需求，它们的部署仍然具有挑战性。量化已成为一种有前景的解决方案，而针对LLM的先进量化算法引入了混合精度矩阵乘法（mpGEMM）的需求，其中较低精度的权重与较高精度的激活相乘。尽管它具有优势，但当前的硬件加速器，如GPU和TPU，缺乏对高效mpGEMM的原生支持，导致主顺序循环中的解量化操作效率低下。为了解决这一限制，我们引入了MixPE，这是一种专门用于LLM推理中高效低比特量化的混合精度处理元件。MixPE利用两项关键创新来最小化解量化开销并释放低比特量化的全部潜力。首先，认识到每个量化组内部的缩放和零点是共享的，我们建议在每个组的mpGEMM之后进行解量化，从而显著减少解量化开销。其次，MixPE不依赖传统的乘法器，而是利用高效的移位和加法操作进行乘法，优化了计算和能耗效率。我们的实验结果表明，MixPE的速度比最先进的量化加速器快2.6倍，能耗降低1.4倍。|
|**2024-11-24**|**eFedLLM: Efficient LLM Inference Based on Federated Learning**|Shengwen Ding et.al.|[2411.16003](http://arxiv.org/abs/2411.16003)|null|大型语言模型（LLMs）预示着人工智能（AI）领域的一个变革时代。然而，LLMs所涉及的大量数据和参数规模，需要高需求的计算和内存资源，限制了它们对更广泛用户和研究人员可及性。本文介绍了一种有效的方法，该方法提高了LLMs推理的运行效率和成本效益。通过利用基于transformer的联邦学习（FL）和模型并行分布式训练，我们的模型能够有效地在网络参与者之间分配计算负载和内存需求。这种策略允许用户，尤其是那些资源有限的用户，能够协作地训练最先进的LLMs。我们还在FL框架内创新了一种激励机制，奖励建设性的贡献并过滤掉恶意活动，从而保护训练过程的完整性和可靠性。同时，我们利用内存层次策略和权重矩阵上的奇异值分解（SVD）进一步提高计算和内存效率。我们的结果表明，从公式分析和数值计算中得出的结果，显著优化了资源使用，使尖端LLMs的访问民主化，确保广大用户既能参与也能从这些高级模型中获益。|
|**2024-11-24**|**Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format**|Chao Fang et.al.|[2411.15982](http://arxiv.org/abs/2411.15982)|null|广泛使用的仅权重量化的大型语言模型（LLMs），利用低比特整数（INT）权重并保留浮点（FP）激活，在降低存储需求的同时保持了精度。然而，这将能量和延迟瓶颈转移到了与昂贵内存访问和计算相关的FP激活上。现有的LLM加速器主要关注计算优化，而忽略了联合优化FP计算和数据传输的潜力，特别是在LLM推理中占主导地位的FP-INT GeMM操作。为了解决这些挑战，我们研究了激活精度在各种LLM模块中的敏感性及其对整体模型精度的影响。基于我们的发现，我们首先提出了Anda数据类型：一种具有组共享指数位和动态尾数位分配的自适应数据格式。其次，我们开发了一个迭代的后训练自适应精度搜索算法，该算法优化不同LLM模块的位宽，以平衡模型精度、能源效率和推理速度。最后，我们提出了一套硬件优化技术，以最大限度地利用Anda格式的优势。这包括基于位平面的数据组织方案、具有位串计算功能的Anda增强处理单元，以及一个运行时位平面Anda压缩器，以同时优化存储、计算和内存占用。我们在FPINT GeMM操作上的评估显示，对于包括OPT、LLaMA和LLaMA-2系列在内的流行LLMs，Anda在GPU类似的FP-FP基准上实现了平均2.4倍的加速、4.0倍的面积效率和3.1倍的能源效率提升。Anda在各种应用场景、精度要求和系统性能方面表现出强大的适应性，使高效LLM推理能够在广泛的部署场景中实现。|
|**2024-11-24**|**Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments**|Nikoleta Iliakopoulou et.al.|[2411.17741](http://arxiv.org/abs/2411.17741)|null|随着大型语言模型（LLMs）的广泛应用，其部署数量呈指数级增长，对推理集群提出了巨大需求。这些集群必须处理针对不同LLM下游任务的大量并发查询。为了处理具有大量LLM参数的多任务设置，方法如低秩自适应（LoRA）允许针对特定任务进行微调，同时跨任务共享大部分基础LLM模型。因此，它们允许以最小的内存需求并发处理任务。然而，现有的LLM服务系统存在效率低下的问题：它们忽视了工作负载异构性，由于频繁的适配器加载而施加了高链路带宽，以及在调度器中存在头阻塞问题。为了解决这些挑战，我们提出了Chameleon，这是一个针对多个适配器环境优化的新型LLM服务系统，它依赖于两个核心思想：适配器缓存和适配器感知调度。首先，Chameleon在GPU内存中缓存流行的适配器，最小化适配器加载时间。重要的是，它使用原本闲置的GPU内存，避免了额外的内存成本。其次，Chameleon使用非抢占式多队列调度，以高效地处理工作负载异构性。通过这种方式，Chameleon同时防止了头阻塞和饥饿现象。我们在最先进的LLM服务平台之上实现了Chameleon，并使用真实世界的生产跟踪和开源LLM对其进行了评估。在高负载下，Chameleon将P99和P50的TTFT延迟分别降低了80.7%和48.1%，同时与最先进的基线相比，提高了1.5倍的吞吐量。|
|**2024-11-24**|**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**|Wenxiang Lin et.al.|[2411.15715](http://arxiv.org/abs/2411.15715)|null|大型语言模型（LLMs）因其庞大的模型尺寸而闻名，对计算资源和内存需求极高，导致在中等GPU系统上的推理效率低下。量化或剪枝等技术可以缩小模型尺寸，但通常会损害准确度，使其不适合实际应用。在这项工作中，我们介绍了\modelname{}，这是一个高性能的推理引擎，旨在加快LLMs的推理速度，同时不降低模型精度。\modelname{}采用了三种创新方法来提高推理效率：1）模型分区，允许跨CPU计算、GPU计算和CPU-GPU通信异步处理任务，2）自适应分区算法，以优化CPU、GPU和PCIe通信能力的利用，3）令牌分配策略，用于处理LLMs推理过程中的各种提示和生成任务。我们使用Mixtral、LLaMA-2、Qwen和PhiMoE等LLMs，在具有不同CPU和GPU的三个测试环境中进行了综合实验。实验结果表明，\modelname{}在解码速度上比 $1.11\times$到$1.80\times$更快，在预填充速度上比$1.69\times$到$6.33\times$更快，与最先进的解决方案llama.cpp和Fiddler相比，整体速度提高了$1.25\times$到$2.04\times$ 。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

## train

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-26**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们发现，低比特量化有利于欠训练的大型语言模型（LLM），因为我们观察到，在应用低比特量化时，较大规模或训练词较少的模型经历的量化诱导退化（QiD）较小，而训练词量多的小模型则遭受显著的QiD。为了更深入地了解这一趋势，我们在可控环境下研究了1500多个不同规模和训练水平（欠训练或完全训练）的低比特量化LLM检查点，推导出QiD与训练词数、模型大小和比特宽度等因素之间的关系定律。通过这些定律，我们提出了一种新颖的视角：可以使用QiD来衡量LLM的训练水平，并确定完全训练不同规模LLM所需的训练词数量。此外，我们利用这些定律来预测训练了100万亿个词的不同规模LLM的量化性能。我们的预测显示，未来预计将用超过100万亿个词进行训练的模型的低比特量化性能可能并不理想。这为未来低比特量化提出了潜在的挑战，并强调了在评估低比特量化研究时，需要关注模型的训练水平。为了促进对此问题的未来研究，我们在https://huggingface.co/Xu-Ouyang上发布了本工作中使用的1500多个量化检查点。|
|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|**论文摘要翻译：  通过将文本分割成标记，诸如字节对编码（BPE）和字节级BPE（BBPE）等分词技术显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割往往掩盖了标记内部的字符结构和序列，阻止模型在训练过程中完全学习这些复杂的细节。因此，LLMs在理解标记内部的字符组成和位置关系方面存在困难，尤其是在数据有限的下游任务中进行微调时。在本文中，我们提出了一种名为Token Internal Position Awareness（TIPA）的新方法，通过使用分词器自己的词汇在反向字符预测任务上训练模型，从而增强了LLMs对内部标记结构的理解。这种方法使模型能够有效地学习和泛化字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测标记级别的字符位置方面优于基线模型。此外，当应用于下游任务——中文拼写纠错（CSC）时，TIPA不仅加速了模型收敛，而且显著提高了任务性能。**|
|**2024-11-26**|**Using Large Language Models for Expert Prior Elicitation in Predictive Modelling**|Alexander Capstick et.al.|[2411.17284](http://arxiv.org/abs/2411.17284)|**[link](https://github.com/alexcapstick/llm-elicited-priors)**|**大型语言模型（LLMs）在多样化数据上训练，能够有效地获取各个领域的信息。然而，它们的计算复杂性、成本以及缺乏透明度阻碍了它们在特定任务中的直接应用。在临床研究等领域，获取关于预测模型的专家标注或先验知识通常成本高昂且耗时。本研究提出使用LLMs来推断预测模型的专家先验分布。这种方法也为情境学习提供了一种替代方案，其中语言模型被要求直接进行预测。我们比较了LLM推断的先验和不知情先验，评估了LLMs是否真实地生成参数分布，并提出了情境学习和先验推断的模型选择策略。我们的研究发现，在低数据环境下，LLM推断的先验参数分布与不知情先验相比，显著降低了预测误差。应用于临床问题，这意味着所需的生物样本减少，降低了成本和资源。先验推断在成本更低的情况下，也始终优于并证明了比情境学习更可靠，在我们的设置中成为首选的替代方案。我们展示了该方法在各种用例中的实用性，包括临床应用。对于感染预测，使用LLM推断的先验，在研究进行200天之前，以相同的准确率所需的标签数量比不知情先验减少了55%。**|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂性，使用基于Transformer的大语言模型（LLMs）对长序列进行推理既昂贵又缓慢。我们引入了Star Attention，这是一种两阶段块稀疏近似方法，通过在多个主机间分片注意力来提高计算效率，同时最小化通信开销。在第一阶段，通过主机间的分块局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。Star Attention与大多数使用全局注意力训练的基于Transformer的LLMs无缝集成，通过减少内存需求并缩短推理时间高达11倍，同时保持95-100%的准确率。**|
|**2024-11-25**|**The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C**|Mikita Balesni et.al.|[2411.16353](http://arxiv.org/abs/2411.16353)|null|尽管在使用思维链（CoT）进行推理时，大型语言模型（LLMs）在处理多跳问题（例如，“Imagine这首歌的表演者的配偶是谁？”）方面表现出色，但当被迫进行内部推理（不使用CoT）时，它们就会遇到困难。关于这一差距的大小和性质的研究产生了混合的证据，结果并不明确。在这篇论文中，我们提出了一种控制环境来研究LLMs中的两跳推理，其中超出概率水平的性能构成了潜在推理不可否认的证据。我们对LLMs（包括Llama 3 8B Instruct和GPT-4o）在虚构事实上进行微调，并证实它们能够使用CoT泛化回答关于这些事实的两跳问题。我们发现，当事实在训练期间或提示中一起出现时，模型可以进行潜在推理。然而，令我们惊讶的是，当学习的事实仅出现在不同的文档中时，模型在没有任何CoT的两跳推理上完全失败，达到了概率水平的准确率和测试损失。我们将这种完全无法组合分别学习的事实称为两跳诅咒。此外，我们在现实世界的事实上评估了9个前沿LLMs，发现模型在超过一半的问题类别上完全无法进行没有CoT的两跳推理，而在大多数类别中，使用CoT时仍然保持部分成功。这些结果表明，LLMs缺乏一种独立于问题类型的一般能力来进行潜在的多跳推理。|
|**2024-11-24**|**Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution**|Haiquan Wang et.al.|[2411.15871](http://arxiv.org/abs/2411.15871)|null|随着大型语言模型（LLMs）的发展，大规模分布式训练变得必要。然而，由于通信量庞大，高度优化的框架在模型FLOPS利用率（通常低于50%）方面仍存在重大损失。同时，我们的全面分析显示，计算和通信密集型算子之间的重叠性很好。本文介绍了一种名为DHelix的新型微观结构，它受到DNA结构的启发，显著提高了LLM训练的效率。DHelix设计的核心是链状交错（SI），它将经过GPU的训练微批次连续流视为两个链。DHelix将两个链的前向和反向传递并置，并针对一个SI计划进行系统优化，该计划由算子级别的重叠分析结果和基于动态规划的搜索算法共同调度，从而实现相反链算子的协同调度。同时，DHelix使两个链能够共享模型状态和激活数据的空间，有效地容纳两个微批次，额外内存空间低于3%。DHelix与所有现有的数据/模型并行形式无缝集成，其中最具有挑战性的是流水线并行，这得益于其独特的模型折叠设计，产生了W形的流水线。我们使用流行的Llama和GPT密集模型以及Phi混合专家（MoE）模型，在3个GPU集群（A40、A800和H100）上评估了DHelix的训练效果。结果表明，它在64-A40和64-A800集群上分别实现了12-40%（最高达58%MFU）和2-29%（最高达71%MFU）的提升，显著优于现有方法。在H100集群上，虽然更快的网络降低了DHelix的利润空间，但它使得跨节点张量并行成为可能，这在当前由于通信成本高昂而无法实施的情况下是一个实践。|
|**2024-11-23**|**Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai**|Parinthapat Pengpun et.al.|[2411.15484](http://arxiv.org/abs/2411.15484)|**[link](https://github.com/parinzee/seed-free-synthetic-instruct)**|**我们提出了一种针对低资源语言（特别是泰语）的数据高效指令微调大型语言模型（LLM）的合成数据方法。我们确定了三个有助于指令微调数据集有效性的关键属性：流畅性、多样性和文化背景。我们提出了一种无需种子数据的框架，用于生成包含这些基本属性的合成指令微调数据。我们的框架使用LLM生成多样化的主题，从维基百科检索相关背景，并为各种任务创建指令，如问答、摘要和对话。实验结果表明，我们性能最佳且包含所有三个关键属性的合成数据集，在仅使用5,000条指令的情况下，与在数十万条指令上训练的最先进泰语LLM相比，取得了具有竞争力的性能。我们的代码和数据集可在https://github.com/parinzee/seed-free-synthetic-instruct上公开获取。**|
|**2024-11-21**|**Exploring Accuracy-Fairness Trade-off in Large Language Models**|Qingquan Zhang et.al.|[2411.14500](http://arxiv.org/abs/2411.14500)|null|大型语言模型（LLMs）在人工智能领域取得了显著进展，展示了它们与人类互动以及通过信息传播影响人类认知的能力。然而，近期研究揭示了LLMs中固有的偏见问题，这是一个需要关注的重大问题。在我们的研究中，我们深入探讨了在LLMs的增强中实现准确性和公平性之间和谐关系的复杂挑战。虽然提高准确性确实可以提升LLMs的整体性能，但往往是以牺牲公平性为代价的。过度强调某一指标的优化必然会导致另一指标的显著退化。这强调了在设计优化LLMs阶段时需要考虑多个因素的重要性。因此，我们主张将LLMs的训练过程重新定义为多目标学习任务。我们的研究揭示了多目标进化学习（MOEL）方法在应对这一挑战方面具有广阔的前景。我们的MOEL框架能够同时优化准确性和公平性指标，从而得到一组帕累托最优的LLMs。总之，我们的研究为LLMs中准确性与公平性之间的微妙平衡提供了宝贵的见解，这对于它们在现实世界中的应用越来越重要。通过利用MOEL，我们提出了通往更公平、更有效的AI技术的有希望的道路。|
|**2024-11-20**|**Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics**|Tetiana Bas et.al.|[2411.13738](http://arxiv.org/abs/2411.13738)|**[link](https://github.com/tetianabas/llm_biases)**|**本研究通过将大型语言模型（LLMs）的性别感知与人类受访者、美国劳工统计局数据和50%无偏差基准进行比较，调查了LLMs中的性别偏见。我们使用职业数据和特定角色的句子创建了一个新的评估集。与LLMs训练数据中常见的基准不同，我们的集是新开发的，防止了数据泄漏和测试集污染。我们测试了五个LLMs，使用单词答案预测每个角色的性别。我们使用Kullback-Leibler（KL）散度来比较模型输出与人类感知、统计数据和50%中立基准。所有LLMs都显示出与性别中立有显著偏差，并与统计数据更为一致，仍然反映了固有的偏见。**|
|**2024-11-20**|**Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training**|Jared Fernandez et.al.|[2411.13055](http://arxiv.org/abs/2411.13055)|null|近年来，神经网络模型能力的显著提升是由模型规模、训练数据和相应的计算资源规模扩大驱动的。为了开发现代应用中所需的大型网络，例如大型语言模型（LLMs），模型训练需要在成千上万的硬件加速器（例如GPU）上分布进行，这需要在大规模计算集群中进行计算和通信的编排。在本工作中，我们证明了仔细考虑硬件配置和并行化策略对于有效（即计算和成本效率）地扩展模型规模、训练数据和总计算量至关重要。我们对大规模LLM训练工作负载在模型规模、硬件配置和分布式并行化策略上的性能进行了广泛的实证研究。我们证明了：（1）超过一定规模后，某些分布式通信策略产生的开销导致之前被认为次优的并行化策略实际上变得更可取；（2）即使硬件和并行化策略得到适当优化，扩大大型模型训练的加速器总数也会很快产生递减的回报，这表明每增加一个单位的电力或GPU小时，其边际性能较差。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

