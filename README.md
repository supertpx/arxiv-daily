## Updated on 2025.10.17
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
    <li><a href=#infer>infer</a></li>
    <li><a href=#train>train</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-07-23**|**Simulating multiple human perspectives in socio-ecological systems using large language models**|Yongchao Zeng et.al.|[2507.17680](http://arxiv.org/abs/2507.17680)|null|理解社会生态系统需要从不同利益相关者的视角获得洞察，而这些视角往往难以获取。为了使人们能够通过基于模拟的替代方法探索不同的利益相关者视角，我们开发了HoPeS（面向人类视角转换）建模框架。HoPeS使用由大型语言模型（LLMs）驱动的代理来代表各种利益相关者；用户可以进入代理角色来体验视角差异。一个模拟协议作为“支架”来简化多个视角的模拟，支持用户在反思、转换和整合视角之间。一个原型系统被开发出来，以在制度动态和土地利用变化的背景下展示HoPeS，使叙事驱动和数值实验都成为可能。在一个示例实验中，用户依次采用系统观察者和研究者的视角——这一角色分析嵌入的土地利用模型中的数据，为代表不同机构的其他LLM代理提供基于证据的决策。尽管用户努力提出技术上可行的政策建议，但由于利益相关者的竞争性倡导，政策建议与实施之间存在差异，反映了现实世界中研究者和政策制定者视角之间的不匹配。用户的反思突显了作为研究者时的挫折感和失望，尤其是在试图获得政治影响力同时保持政治中立性的挑战。尽管如此，用户表现出对实验不同叙事框架策略的高度动机，这表明系统在探索不同视角方面的潜力。进一步的系统和协议改进可能会使社会生态系统模拟中出现新的跨学科合作形式。|
|**2025-07-23**|**Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance**|Yufei He et.al.|[2507.17131](http://arxiv.org/abs/2507.17131)|null|大型语言模型（LLM）的代理在规则和所需领域知识经常变化的环境中往往难以应对，例如合规监管和用户风险评估。当前的方法，如离线微调和标准提示，不足之处在于它们不能在操作期间有效地适应新的知识。为了解决这一限制，我们提出了自适应反思互动代理（ARIA），这是一个专门设计用于在测试时持续学习更新领域知识的LLM代理框架。ARIA通过结构化的自我对话评估其不确定性，主动识别知识差距，并从人类专家那里请求针对性的解释或纠正。然后，它系统地更新带有时间戳的内部知识库，通过比较和澄清查询检测并解决冲突或过时的知识。我们在TikTok Pay的基于现实情况的客户尽职调查名单筛选任务上评估了ARIA，以及公开的动态知识任务。结果表明，与使用标准离线微调和现有自我改进代理的基线相比，ARIA在适应性和准确性方面有了显著的提升。ARIA在TikTok Pay内部部署，为超过1.5亿月活跃用户提供服务，证实了它在快速变化环境中实际应用和有效性的价值。|
|**2025-07-22**|**Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent**|Xiaoyu Zhan et.al.|[2507.16799](http://arxiv.org/abs/2507.16799)|null|大型语言模型（LLMs）的快速发展使得角色扮演语言代理在各种应用中展现出巨大潜力。然而，仅仅依赖提示和上下文输入往往不足以实现特定角色的深入沉浸，尤其是对于著名虚构人物或公众人物。另一方面，基于微调的方法由于数据收集的挑战以及训练所需的计算资源而面临限制，从而限制了其更广泛的应用。为了解决这些问题，我们提出了测试时匹配（TTM）方法，这是一种无需训练的角色扮演框架，通过测试时缩放和上下文工程来实现。TTM使用LLM代理自动将角色的特征解耦为个性、记忆和语言风格。我们的框架包含一个结构化的、三阶段的生成流程，利用这些特征进行可控的角色扮演。它实现了高保真度的角色扮演性能，同时也实现了跨不同语言风格以及个性、记忆变化的顺畅组合。我们通过人工评估评估了我们的框架，结果显示我们的方法在生成表达丰富且风格一致的角色对话方面取得了卓越的性能。|
|**2025-07-22**|**Towards Enforcing Company Policy Adherence in Agentic Workflows**|Naama Zwerdling et.al.|[2507.16459](http://arxiv.org/abs/2507.16459)|null|在大型语言模型（LLM）智能体被视为传统业务流程自动化的灵活和可扩展替代方案的同时，它们在可靠地遵循复杂公司政策方面却存在困难。在本研究中，我们提出了一种确定性强、透明度高、模块化的框架，用于在智能体工作流中实施业务政策遵循。我们的方法分为两个阶段进行操作：（1）离线的构建阶段，将政策文档编译成与工具使用相关的可验证守卫代码，以及（2）运行时集成阶段，这些守卫代码在每次智能体动作之前确保合规性。我们以具有挑战性的 $\tau$ -bench航空公司领域为例展示了我们的方法，并展示了在政策实施方面的鼓舞人心的初步结果，同时还进一步概述了现实部署中的关键挑战。|
|**2025-07-22**|**Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)**|Myung Ho Kim et.al.|[2507.16184](http://arxiv.org/abs/2507.16184)|null|我们报告了在四个有影响力的心智理论之间发现的结构趋同：卡尼曼的双重系统理论、弗里斯顿的预测处理、明斯基的心智社会以及克拉克的扩展心智。这种趋同无意中出现在一个名为“代理流”的实用人工智能代理架构中。设计目的是解决大型语言模型（LLMs）的局限性，代理流包含检索、认知、控制、记忆和行动等五个相互依存的模块，以循环认知环路的形式排列。尽管最初仅受明斯基和克拉克的启发，但该系统的结构在回顾时与所有四种理论中发现的计算模式相一致，包括预测建模、联想回忆和错误敏感控制。为了评估这种趋同，我们在多步推理任务上对基线LLM代理进行了比较实验。结构化代理实现了95.8%的任务成功率，并表现出强烈的约束遵守，而基线系统则有62.3%的成功率。这些结果并非旨在证明优越性，而是说明理论结构可能通过实际设计选择而非自上而下的理论出现。我们引入了PEACE，作为一种描述性元架构，它捕捉了在代理流中观察到的设计级规律。PEACE并非作为一种新理论，它为理解受现实世界实施需求塑造的架构提供了一个共享词汇。本文应被视为一份立场论文——一种探索性反思，关于实施如何揭示认知理论的潜在结构回声，而不断言理论统一。|
|**2025-07-22**|**VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings**|Ramin Giahi et.al.|[2507.17080](http://arxiv.org/abs/2507.17080)|null|多模态学习在今天的电子商务推荐平台上发挥着关键作用，它能够实现准确的推荐和产品理解。然而，现有的视觉-语言模型，如CLIP，在电子商务推荐系统中面临一些关键挑战：1）弱对象级对齐，全局图像嵌入无法捕捉到细粒度的产品属性，导致检索性能不佳；2）模糊的文本表示，产品描述往往缺乏语境清晰度，影响跨模态匹配；3）领域不匹配，因为通用的视觉-语言模型可能无法很好地推广到电子商务特定数据。为了解决这些局限性，我们提出了一种框架，即VL-CLIP，通过整合视觉定位来增强对细粒度视觉理解，以及基于LLM的代理来生成丰富的文本嵌入来提升CLIP嵌入。视觉定位通过定位关键产品来细化图像表示，而LLM代理通过消除产品描述中的歧义来增强文本特征。我们的方法显著提高了检索准确性、多模态检索有效性和推荐质量，在一家美国最大的电子商务平台上，对数百万商品进行了提升，点击率（CTR）提高了18.6%，平均交易成本（ATC）提高了15.5%，总成交额（GMV）提高了4.0%。额外的实验结果表明，我们的框架在精确度和语义对齐方面都优于包括CLIP、FashionCLIP和GCL在内的视觉-语言模型，证明了结合对象感知的视觉定位和LLM增强的文本表示对稳健多模态推荐具有潜力。|
|**2025-07-22**|**Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems**|Chengxuan Xia et.al.|[2507.17061](http://arxiv.org/abs/2507.17061)|null|大型语言模型（LLM）代理在协作任务完成方面展现出越来越大的潜力。然而，现有的多智能体框架通常依赖于静态工作流程、固定角色和有限的智能体间通信，这降低了它们在开放性、高复杂性领域的有效性。本文提出了一种协调框架，通过三个核心机制实现适应性：动态任务路由、双向反馈和并行智能体评估。该框架允许智能体根据信心和工作负载重新分配任务，交换结构化批评以迭代改进输出，并且关键的是在高模糊性子任务上竞争，由评估者选择最合适的结果。我们在模块化架构中实现了这些原则，并在事实覆盖、连贯性和效率方面与静态和部分自适应基线相比，取得了显著的改进。我们的研究结果表明，在多智能体LLM系统中结合适应性和结构化竞争具有显著优势。|
|**2025-07-22**|**Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning**|Aleksandr Perevalov et.al.|[2507.16971](http://arxiv.org/abs/2507.16971)|null|通过多语言自然语言接口访问知识是信息检索及相关领域面临的一个新兴挑战。存储在知识图谱中的结构化知识可以通过特定的查询语言（例如，SPARQL）进行查询。因此，需要将自然语言输入转换为查询以满足信息需求。先前的方法主要侧重于组合组件（例如，基于规则或神经网络的）来解决下游任务并在最后得出答案。我们引入了mKGQAgent，这是一个受人类启发的框架，它将将自然语言问题转换为SPARQL查询的任务分解为模块化、可解释的子任务。通过利用协调的LLM代理工作流程进行规划、实体链接和查询优化——由经验池进行情境学习指导——mKGQAgent高效地处理多语言KGQA。在Text2SPARQL挑战2025的DBpedia和公司基准测试中评估，我们的方法在所有参赛者中排名第一。这项工作为在多语言语义解析中开发类似人类的推理系统开辟了新的途径。|
|**2025-07-21**|**FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents**|Vikram Nitin et.al.|[2507.15241](http://arxiv.org/abs/2507.15241)|null|尽管软件安全漏洞构成了关键的威胁，但报告往往不完整，缺乏验证修复和防止回归所需的漏洞证明（PoV）测试。这些测试不仅对于确保补丁有效至关重要，而且有助于开发者理解漏洞如何被利用。生成PoV测试是一个具有挑战性的问题，需要推理程序中深层嵌套级别的控制流和数据流。我们提出了FaultLine，这是一个LLM代理工作流程，它使用一系列精心设计的推理步骤，灵感来源于传统静态和动态程序分析的一些方面，以自动生成PoV测试用例。给定一个带有漏洞报告的软件项目，FaultLine 1）追踪输入从外部可访问的API（“源”）到对应于漏洞的“汇”的流动，2）推理输入必须满足的条件才能穿越流动过程中遇到的分支条件，3）使用这种推理在反馈循环中生成PoV测试用例。FaultLine不使用特定于语言的静态或动态分析组件，这使得它可以在各种编程语言中使用。为了评估FaultLine，我们收集了一个具有挑战性的多语言数据集，包含Java、C和C++项目中100个已知的漏洞。在这个数据集上，FaultLine能够为16个项目生成PoV测试，而CodeAct 2.1（一个流行的最先进开源代理框架）只能生成9个。因此，FaultLine相对于现有技术提高了77%。我们的发现表明，分层推理可以增强LLM代理在PoV测试生成方面的性能，但这个问题总体上仍然具有挑战性。我们公开我们的代码和数据集，希望它能促进该领域的进一步研究。|
|**2025-07-21**|**PromptArmor: Simple yet Effective Prompt Injection Defenses**|Tianneng Shi et.al.|[2507.15219](http://arxiv.org/abs/2507.15219)|null|尽管具有潜力，但最近的研究表明，LLM代理容易受到提示注入攻击，恶意提示被注入到代理的输入中，导致其执行攻击者指定的任务，而不是用户提供的预期任务。在本文中，我们提出了PromptArmor，这是一种简单而有效的针对提示注入攻击的防御措施。具体来说，PromptArmor会提示现成的LLM在代理处理之前检测并从输入中移除潜在的注入提示。我们的结果表明，PromptArmor可以准确地识别和移除注入提示。例如，使用GPT-4o、GPT-4.1或o4-mini，PromptArmor在AgentDojo基准测试上实现了低于1%的误报率和漏报率。此外，在用PromptArmor移除注入提示后，攻击成功率降至低于1%。我们还展示了PromptArmor对自适应攻击的有效性，并探讨了提示LLM的不同策略。我们建议将PromptArmor作为评估针对提示注入攻击的新防御措施的标准基准。|
|**2025-07-20**|**EduThink4AI: Translating Educational Critical Thinking into Multi-Agent LLM Systems**|Xinmeng Hou et.al.|[2507.15015](http://arxiv.org/abs/2507.15015)|null|大型语言模型（LLMs）在作为教育辅导代理方面展现出巨大的潜力，能够在各个学术领域以近乎人类水平的技巧定制提示、编排课程和评分。然而，当前的基于LLMs的教育系统在促进真正的批判性思维方面存在关键局限性，在超过三分之一的具有反事实前提的多跳问题中表现不佳，并且容易受到触发偏见或事实错误响应的对抗性提示的影响。为了解决这些差距，我们提出了EDU-Prompting，这是一个新颖的多代理框架，它将既定的教育批判性思维理论与LLM代理设计相结合，以生成批判性、偏见意识强的解释，同时培养多样化的观点。我们在理论基准和实际大学水平批判性写作场景中的系统性评估表明，EDU-Prompting显著提高了AI生成教育响应的内容真实性逻辑性。该框架的模块化设计使其能够无缝集成到现有的提示框架和教育应用中，允许实践者直接纳入促进分析推理和引入多种观点的批判性思维催化剂，而无需进行大量系统修改。|
|**2025-07-20**|**Byzantine-Robust Decentralized Coordination of LLM Agents**|Yongrae Jo et.al.|[2507.14928](http://arxiv.org/abs/2507.14928)|null|多个大型语言模型（LLM）代理之间的协作是一种克服单一代理系统固有局限性的有前景的方法，例如幻觉和单点故障。随着LLM代理越来越多地部署在公开的区块链平台上，能够容忍恶意（拜占庭）代理的多代理系统变得至关重要。最近的拜占庭鲁棒的多代理系统通常依赖于领导者驱动的协调，这存在两个主要缺点。首先，它们本质上容易受到针对领导者的针对性攻击。如果连续的领导者表现出恶意行为，系统反复无法达成共识，迫使进行新的共识轮次，这在LLM调用的高延迟情况下尤其代价高昂。其次，即使有更高质量的替代方案，领导者的表现不佳的提案也可能被接受为最终答案，因为现有方法在领导者提案获得法定多数票后就将其最终化。为了解决这些问题，我们提出了DecentLLMs，这是一种新颖的去中心化共识方法，用于多代理LLM系统，其中工作代理并发生成答案，评估代理独立评分和排名这些答案以选择最佳可用答案。这种去中心化架构即使在拜占庭代理存在的情况下也能实现更快的共识，并通过拜占庭鲁棒的聚合技术持续选择更高质量的答案。实验结果表明，DecentLLMs有效地容忍了拜占庭代理，并显著提高了所选答案的质量。|
|**2025-07-20**|**Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree**|Sam Johnson et.al.|[2507.14799](http://arxiv.org/abs/2507.14799)|null|本工作展示了基于LLM的网页导航代理具备强大的自动化能力，但容易受到间接提示注入（IPI）攻击的威胁。我们表明，攻击者可以在网页HTML中嵌入通用的对抗性触发器，以劫持利用可访问性树解析HTML的代理行为，导致意外或恶意操作。利用贪婪坐标梯度（GCG）算法和由Llama-3.1驱动的Browser Gym代理，我们的系统在针对性和泛化攻击中，在真实网站上均实现了高成功率，包括登录凭证窃取和强制点击广告。我们的实证结果突出了随着LLM驱动的自主网页代理得到更广泛的应用，所存在的关键安全风险以及加强防御的必要性。系统软件（https://github.com/sej2020/manipulating-web-agents）已在MIT许可证下发布，并附带一个公开可用的演示网站（http://lethaiq.github.io/attack-web-llm-agent）。|
|**2025-07-19**|**Configurable multi-agent framework for scalable and realistic testing of llm-based agents**|Sai Wang et.al.|[2507.14705](http://arxiv.org/abs/2507.14705)|null|大型语言模型（LLM）的智能体表现出复杂、与上下文相关的行为，这使得静态基准和临时手动测试迅速过时。我们提出了Neo，这是一个可配置的多智能体框架，可以自动化对基于LLM的系统进行真实的多轮评估。Neo通过共享上下文中心将问题生成智能体和评估智能体耦合起来，允许模块化地组合领域提示、场景控制和动态反馈。测试输入从涵盖对话流程、用户意图和情感基调的概率状态模型中采样，从而实现多样化、类似人类的对话，并在每一轮后进行适应性调整。将Neo应用于生产级别的卖家金融助理聊天机器人，Neo（i）在五个攻击类别中发现了边缘情况失败，破绽率为3.3%，接近专家红队人员实现的5.8%，（ii）提供了10-12倍更高的吞吐量，在约45分钟内生成180个连贯的测试问题，而人类需要16小时。除了安全探测外，Neo的随机策略平衡了主题覆盖和对话深度，比手动编写的脚本提供了更广泛的行为探索。因此，Neo为可扩展、自我进化的LLM问答奠定了基础：其智能体接口、状态控制器和反馈循环是模型无关的，并且可扩展到更丰富的基于事实的基准和政策合规性检查。我们将该框架发布出来，以促进可重复、高保真的新兴智能体系统的测试。|
|**2025-07-19**|**Routine: A Structural Planning Framework for LLM Agent System in Enterprise**|Guancheng Zeng et.al.|[2507.14447](http://arxiv.org/abs/2507.14447)|null|在企业环境中部署智能体系统常常受到多个挑战的阻碍：常见的模型缺乏特定领域的流程知识，导致计划无序、缺少关键工具和执行稳定性差。为了解决这个问题，本文介绍了一种名为Routine的多步骤智能体规划框架，它具有清晰的架构、明确的指令和无缝的参数传递，以引导智能体的执行模块稳定地执行多步骤工具调用任务。在真实企业场景中进行的评估中，Routine显著提高了模型工具调用的执行精度，将GPT-4o的性能从41.1%提升到96.3%，将Qwen3-14B从32.6%提升到83.3%。我们进一步构建了一个遵循Routine的训练数据集，并对Qwen3-14B进行了微调，在特定场景的评估中，准确率提升至88.2%，表明对执行计划的遵循性有所提高。此外，我们采用了基于Routine的蒸馏方法创建了一个特定场景的多步骤工具调用数据集。在蒸馏数据集上的微调将模型的准确率提升至95.5%，接近GPT-4o的性能。这些结果突出了Routine在提炼特定领域工具使用模式并增强模型对新场景适应性的有效性。我们的实验结果表明，Routine提供了一种实用且易于访问的方法来构建稳定的智能体工作流程，加速了智能体系统在企业环境中的部署和采用，并推动了流程AI技术的愿景。|
|**2025-07-18**|**DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration**|Xiyun Li et.al.|[2507.14088](http://arxiv.org/abs/2507.14088)|null|实时人机人工智能（AI）协作至关重要但颇具挑战，尤其是在AI代理必须适应动态场景中多样和未预见到的人类行为时。现有的大语言模型（LLM）代理通常无法准确模拟复杂的人类心理特征，如领域意图，尤其是在缺乏直接交流的情况下。为了解决这一限制，我们提出了一个受认知科学双过程理论启发的创新的双重过程多尺度理论思维（DPMT）框架。我们的DPMT框架整合了一个多尺度理论思维（ToM）模块，通过心理特征推理促进稳健的人类伙伴建模。实验结果表明，DPMT显著提升了人机协作，而消融研究进一步验证了我们的多尺度ToM在慢速系统中的贡献。|
|**2025-07-18**|**CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding Education**|Jianing Zhao et.al.|[2507.13814](http://arxiv.org/abs/2507.13814)|null|大型语言模型（LLMs）在通过提供代码编写、解释和调试支持来改善编程教育方面展现出巨大的潜力。然而，现有的基于LLMs的方法通常无法评估学生的能力、设计学习计划、提供与个人学习目标相一致的个人化材料，以及实现互动学习。当前的研究大多使用单个LLM代理，这限制了它们理解复杂代码库和安排逐步辅导的能力。最近的研究表明，多智能体LLMs可以协作解决各种领域，如软件工程中的复杂问题，但它们在教育领域的潜力尚未得到探索。在本工作中，我们引入了CodeEdu，这是一个创新的多个智能体协作平台，它将LLMs与工具使用相结合，以提供积极主动和个性化的编程教育。与静态管道不同，CodeEdu根据学生需求动态分配代理和任务。CodeEdu中的各种代理承担特定的功能，包括任务规划、个性化材料生成、实时问答、逐步辅导、代码执行、调试和学习报告生成，并借助广泛的工具来提高任务效率。自动评估显示，CodeEdu显著提高了学生的编程表现。|
|**2025-07-18**|**AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework**|Yu Yao et.al.|[2507.13729](http://arxiv.org/abs/2507.13729)|null|在测试和评估自动驾驶规划器时，罕见但关键的场景构成了重大挑战。仅依靠现实世界的驾驶场景，就需要收集大量数据集来捕捉这些场景。虽然自动生成交通场景看起来很有前景，但数据驱动模型需要大量的训练数据，并且往往缺乏对输出的精细控制。此外，从头开始生成新颖的场景可能会导致与原始训练场景的分布偏移，这会削弱评估的有效性，尤其是对于基于学习的规划器。为了避免这个问题，最近的研究提出通过增强测试集中的原始场景来生成具有挑战性的场景。然而，这涉及到领域专家对场景的手动增强，这种方法无法满足自动驾驶系统评估对规模的需求。因此，本文介绍了一种基于大型语言模型（LLM）代理的框架，用于使用自然语言描述增强现实世界的交通场景，解决了现有方法的局限性。一个关键创新是采用代理式设计，它能够对输出进行精细控制，即使使用更小、成本效益更高的LLM也能保持高性能。大量的人为专家评估证明了我们的框架能够准确遵循用户意图，生成与人工创建的高质量增强场景相当。|
|**2025-07-18**|**NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation**|Md. Kamrul Hossain et.al.|[2507.14398](http://arxiv.org/abs/2507.14398)|null|基于意图的网络（IBN）通常利用软件定义网络（SDN）的可编程性来简化网络管理。然而，从用户指定的高级意图到设备特定的低级配置的整个流程自动化仍存在重大挑战。现有的解决方案通常依赖于僵化的基于规则的翻译器和固定的API，限制了可扩展性和适应性。相比之下，大型语言模型（LLM）的最新进展为一条有希望的途径，它利用自然语言理解和灵活推理。然而，LLM在IBN任务中的能力尚不清楚。为了解决这个问题，我们引入了IBNBench，这是一个独特的基准测试套件，包含四个新颖的数据集：Intent2Flow-ODL、Intent2Flow-ONOS、FlowConflict-ODL和FlowConflict-ONOS。这些数据集专门设计用于评估LLM在意图翻译和冲突检测任务中的性能，这些任务在行业级SDN控制器ODL和ONOS中进行。我们的结果提供了对33个开源LLM在IBNBench和相关数据集上进行的首次全面比较，揭示了广泛的表现结果。然而，虽然这些结果展示了LLM在孤立IBN任务中的潜力，但将LLM集成到完全自主的IBN流程中仍是一个未开发的领域。因此，我们的第二个贡献是NetIntent，这是一个统一且可适应的框架，它利用LLM来自动化IBN的全生命周期，包括SDN系统中的翻译、激活和保证。NetIntent协调LLM和非LLM代理，支持动态重提示和上下文反馈，以最小的人类干预来稳健地执行用户定义的意图。我们在ODL和ONOS SDN控制器上对NetIntent的实施实现了一致且自适应的端到端IBN实现。|
|**2025-07-18**|**DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation**|Ziqi Wang et.al.|[2507.14267](http://arxiv.org/abs/2507.14267)|null|材料发现依赖于高通量、高保真度的模拟技术，如密度泛函理论（DFT），这些技术需要多年的培训、广泛的参数微调和系统性的错误处理。为了解决这些挑战，我们引入了基于DFT的代理材料筛选研究引擎（DREAMS），这是一个分层的多智能体框架，用于DFT模拟，它将中心的大型语言模型（LLM）规划智能体与用于原子结构生成、系统DFT收敛性测试、高性能计算（HPC）调度和错误处理的领域特定LLM智能体相结合。此外，共享画布有助于LLM智能体结构化其讨论，保留上下文并防止幻觉。我们在Sol27LC晶格常数基准测试中验证了DREAMS的能力，与人类DFT专家的结果相比，平均误差低于1%。此外，我们将DREAMS应用于长期存在的CO/Pt(111)吸附难题，展示了其长期和复杂问题解决能力。该框架再次重现了专家级的文献吸附能差异。最后，DREAMS用于通过贝叶斯集成抽样量化功能驱动的不确定性，证实了在广义梯度近似（GGA）DFT水平上对面心立方（FCC）位点的偏好。总之，DREAMS接近L3级自动化——自主探索定义的设计空间——并显著减少了对人专家知识和干预的依赖，为民主化、高通量、高保真度计算材料发现提供了一条可扩展的路径。|
|**2025-07-17**|**MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems**|Yu Cui et.al.|[2507.13038](http://arxiv.org/abs/2507.13038)|null|多智能体辩论（MAD）系统利用大型语言模型（LLM）智能体之间的协作互动来提高推理能力。虽然最近的研究主要集中在提高MAD系统的准确性和可扩展性，但它们的安全漏洞却得到了有限的关注。在这项工作中，我们引入了MAD-Spear，这是一种针对性强提示注入攻击，它破坏了智能体的小部分，但严重扰乱了整个MAD过程。被操纵的智能体会产生多个看似合理但实际上错误的响应，利用LLM的从众倾向来传播错误信息和降低共识质量。此外，该攻击可以与其他策略相结合，如通信攻击，通过增加智能体接触到的错误响应的暴露度来进一步放大其影响。为了评估MAD在攻击下的鲁棒性，我们提出了MAD容错性的正式定义，并开发了一个全面的评估框架，该框架综合考虑了准确性、共识效率和可扩展性。在五个不同难度级别的基准数据集上进行的广泛实验表明，MAD-Spear在降低系统性能方面始终优于基线攻击。此外，我们还观察到，智能体多样性在数学推理任务中显著提高了MAD性能，这挑战了先前关于智能体多样性对性能影响微乎其微的研究。这些发现突显了在MAD设计中提高安全性的紧迫需求。|
|**2025-07-17**|**MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models**|Zhiwei Liu et.al.|[2507.12806](http://arxiv.org/abs/2507.12806)|null|随着基于大型语言模型（LLMs）的智能代理的快速崛起，迫切需要强大且可扩展的评估框架。现有方法依赖于静态基准和劳动密集型的数据收集，限制了实际评估的可行性。我们介绍了\oursystemname，这是一个基于开源模型上下文协议（MCP）的框架，它自动化了端到端任务生成和跨不同领域的LLM代理的深度评估。MCPEval标准化了指标，无缝集成到原生代理工具中，并消除了构建评估管道中的手动劳动。在五个真实世界领域的实证结果表明，它在揭示细微、特定领域的性能方面非常有效。我们公开发布MCPEval（https://github.com/SalesforceAIResearch/MCPEval）以促进LLM代理评估的可重复性和标准化。|
|**2025-07-16**|**Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes**|Johann Frei et.al.|[2507.12261](http://arxiv.org/abs/2507.12261)|null|为了临床数据集成和医疗服务，HL7 FHIR标准已经确立为复杂健康数据之间互操作性的理想格式。以往尝试将自由形式的临床笔记自动转换为结构化FHIR资源的尝试，依赖于模块化、基于规则的系统或经过指令调整和限制解码的LLM。由于它们通常受限于泛化能力和结构不一致性，我们提出了一种由LLM代理、代码执行和医疗术语数据库工具驱动的端到端框架来解决这些问题。我们的解决方案称为Infherno，旨在遵循FHIR文档架构，并在从非结构化文本预测FHIR资源方面与人类基线竞争良好。该实现具有用于自定义和合成数据的前端，以及本地和专有模型，支持临床数据集成过程和机构间的互操作性。|
|**2025-07-16**|**Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness**|Yuki Sakamoto et.al.|[2507.11979](http://arxiv.org/abs/2507.11979)|null|大型语言模型（LLMs）已成为使用具有特定特征的类人代理模拟复杂社会现象的强大工具。在人类社会，价值观的相似性对于建立信任和亲密关系至关重要；然而，这一原则在由LLM代理组成的虚拟社会中是否成立，尚未得到探索。因此，本研究通过两个实验调查了价值观相似性对LLM代理之间关系建立的影响。首先，在一个初步实验中，我们评估了LLMs中价值观的可控性，以确定控制价值观的最有效模型和提示设计。随后，在主要实验中，我们生成了具有特定价值观的LLM代理对，并分析了他们在对话后的相互信任和人际亲密度的评价。实验在英语和日语中进行，以研究语言依赖性。结果表明，价值观相似性更高的代理对表现出更高的相互信任和人际亲密度。我们的研究结果表明，LLM代理模拟可以作为社会科学理论的有效测试平台，有助于阐明价值观影响关系建立机制，并为启发新的社会科学理论和见解提供基础。|
|**2025-07-15**|**DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering**|Yinsheng Li et.al.|[2507.11527](http://arxiv.org/abs/2507.11527)|null|大型语言模型（LLM）智能体在解决现实世界问题方面展现出巨大潜力，并有望成为工业任务自动化的解决方案。然而，为了从工业角度系统地评估自动化智能体，例如在土木工程领域，需要更多的基准测试。因此，我们提出了DrafterBench，用于在技术绘图修订这一土木工程领域表征任务的背景下对LLM智能体进行综合评估。DrafterBench包含了从实际绘图文件中总结出的十二种任务类型，共有46个定制化的功能/工具，共计1920个任务。DrafterBench是一个开源基准测试，用于严格测试AI智能体在解读复杂和长上下文指令、利用先验知识以及通过隐式策略意识适应动态指令质量方面的熟练程度。该工具包全面评估了结构化数据理解、功能执行、指令遵循和批判性推理方面的不同能力。DrafterBench提供了任务准确性和错误统计的详细分析，旨在更深入地了解智能体能力，并确定在工程应用中集成LLM的改进目标。我们的基准测试可在https://github.com/Eason-Li-AIS/DrafterBench上获取，测试集托管在https://huggingface.co/datasets/Eason666/DrafterBench上。|
|**2025-07-15**|**Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian**|Andrei Niculae et.al.|[2507.11299](http://arxiv.org/abs/2507.11299)|null|基于文本的远程医疗变得越来越普遍，然而，医生与患者互动中医疗建议的质量往往更多地取决于建议的沟通方式，而不是其临床准确性。为了解决这个问题，我们引入了Dr. Copilot，这是一个多智能体大型语言模型（LLM）系统，通过评估和增强医生书面回复的表现质量来支持讲罗马尼亚语的医生。Dr. Copilot不是评估医疗的正确性，而是提供沿17个可解释轴的反馈。该系统由三个LLM智能体组成，这些智能体的提示通过DSPy自动优化。该系统使用低资源罗马尼亚数据设计，并使用开放权重模型部署，在远程医疗平台上为医生提供实时具体反馈。实证评估和41名医生的现场部署表明，用户评价和回复质量有显著提升，这标志着LLM在罗马尼亚医疗环境中的第一次真正部署。|
|**2025-07-15**|**Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding**|Conrad Borchers et.al.|[2507.11198](http://arxiv.org/abs/2507.11198)|null|大型语言模型（LLMs）为大规模的定性研究提供了新的可能性，包括编码和数据标注。尽管多智能体系统（MAS）可以模拟人类的编码工作流程，但它们相较于单智能体编码的优势仍理解不足。我们进行了一项实验研究，探讨了智能体角色和温度如何影响基于包含8个编码的编码手册的对话片段的共识构建和编码准确性。我们的开源MAS通过结构化的智能体讨论和共识仲裁来模拟演绎性人类编码。我们使用六个开源LLMs（参数量为3亿到32亿）和18个实验配置，分析了超过77,000个编码决策，与来自在线数学辅导会议的人类标注转录本的黄金标准数据集进行了比较。温度显著影响了所有六个LLMs中共识是否以及何时达成。具有多个角色（包括中立、自信或同情）的MAS与统一角色相比，在六个LLMs中的四个中显著延迟了共识。在这三个LLMs中，较高的温度显著减弱了多个角色对共识的影响。然而，温度和角色配对并没有导致编码准确性的显著提高。在大多数条件下，单智能体与MAS共识相匹配或优于MAS共识。只有一个模型（OpenHermesV2:7B）和编码类别在温度为0.5或更低以及特别是智能体中至少包含一个自信角色时，从MAS审议中获得了高于偶然性的收益。对MAS协作的定性分析表明，尽管如此，MAS可能仍有助于缩小模糊的代码应用，从而改善编码手册和人类-人工智能编码。我们为基于LLM的定性方法的局限性提供了新的见解，挑战了多样化的MAS角色导致更好成果的观念。我们开源了我们的MAS和实验代码。|
|**2025-07-15**|**Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation**|Yicong Wu et.al.|[2507.10911](http://arxiv.org/abs/2507.10911)|null|针对慢性多病患者的治疗方案推荐具有挑战性，因为存在治疗冲突的风险。现有的决策支持系统面临着可扩展性的限制。受一般医生（GP）管理多病患者的启发，偶尔召集多学科团队（MDT）合作，本研究调查了使用基于大型语言模型（LLM）的多智能体系统（MAS）进行更安全的治疗方案推荐的可行性和价值。我们设计了一个单智能体和MAS框架，通过允许LLM智能体之间进行讨论来模拟MDT决策，以解决医疗冲突。我们使用基准案例对多病患者的治疗方案规划任务进行了评估。我们比较了MAS性能与单智能体方法和现实世界基准。我们研究的一个重要贡献是定义了超越技术精确度和召回率的评估指标，允许检查达到的临床目标和所提建议的药物负担是否符合黄金标准基准。我们的结果表明，使用当前的LLM，单智能体GP的表现与MDT相当。得分最高的模型提供了正确的建议，解决了所有临床目标，但建议不完整。一些模型还提出了不必要的药物，导致药物与条件之间或药物-药物相互作用的不必要冲突。|
|**2025-07-15**|**General Modular Harness for LLM Agents in Multi-Turn Gaming Environments**|Yuxuan Zhang et.al.|[2507.11633](http://arxiv.org/abs/2507.11633)|null|我们提出了一种模块化绑定设计，用于LLM智能体，该设计由感知、记忆和推理组件组成，使得单个LLM或VLM主干可以无需特定领域工程，应对广泛的多轮游戏环境。利用经典和现代游戏套件作为低门槛、高多样性的测试平台，我们的框架提供了一种统一的流程，用于分析每个模块如何在动态交互环境中影响性能。广泛的实验表明，该绑定可以持续提升游戏性能，并揭示出独特的贡献模式，例如，在长距离谜题中记忆占主导地位，而在视觉噪声游戏中感知至关重要。这些发现突出了我们模块化绑定设计在推进通用智能体方面的有效性，考虑到游戏在日常人类经验中的熟悉性和普遍性。|
|**2025-07-14**|**Warehouse Spatial Question Answering with LLM Agent**|Hsiang-Wei Huang et.al.|[2507.10778](http://arxiv.org/abs/2507.10778)|null|空间理解一直是现有多模态大型语言模型（MLLMs）面临的挑战。以往的方法通过大规模的MLLM微调来增强其空间理解能力。在本文中，我们提出了一种数据高效的方法。我们提出了一种具有强大和先进空间推理能力的LLM智能体系统，该系统可用于解决复杂室内仓库场景中具有挑战性的空间问答任务。我们的系统集成了多个工具，使得LLM智能体能够进行空间推理和API工具交互，以回答给定复杂的空间问题。在2025年AI城市挑战赛物理AI空间智能仓库数据集上的大量评估表明，我们的系统在物体检索、计数和距离估计等任务中实现了高精度和效率。代码可在以下链接获取：https://github.com/hsiangwei0903/SpatialAgent|
|**2025-07-14**|**Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats**|Quanyan Zhu et.al.|[2507.10621](http://arxiv.org/abs/2507.10621)|null|保护网络空间不仅需要先进的工具，还需要转变我们对威胁、信任和自主性的思考方式。传统的网络安全方法依赖于手动响应和脆弱的启发式方法。为了构建积极主动和智能的防御系统，我们需要综合的理论框架和软件工具。博弈论为模拟对抗行为、设计战略防御和使自主系统获得信任提供了严格的理论基础。同时，软件工具处理网络数据、可视化攻击面、验证合规性并提出缓解措施。然而，理论与实践之间仍然存在脱节。大型语言模型（LLMs）和代理式人工智能的兴起为弥合这一差距提供了新的途径。由LLM驱动的代理可以将抽象策略转化为现实世界的决策。相反，博弈论可以指导这些代理在复杂工作流程中的推理和协调。LLMs还挑战了经典博弈论假设，如完美理性或静态收益，促使与认知和计算现实相一致的新模型出现。这种协同进化承诺了更丰富的理论基础和新的解决方案概念。代理式人工智能也重塑了软件设计：系统现在必须从一开始就是模块化、自适应和信任感知的。  本章探讨了博弈论、代理式人工智能和网络安全之间的交汇点。我们回顾了关键的博弈论框架（例如，静态、动态、贝叶斯和信号博弈）和解决方案概念。然后，我们研究LLM代理如何增强网络安全，并引入将推理嵌入到AI代理中的LLM驱动博弈。最后，我们探讨了多代理工作流程和协调博弈，概述了这种融合如何促进安全、智能和自适应的网络安全系统。|
|**2025-07-13**|**Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks**|Ann Nedime Nese Rende et.al.|[2507.09657](http://arxiv.org/abs/2507.09657)|null|我们利用由大型语言模型（LLMs）驱动的生成型智能体来模拟一个共享住宅楼中的社交网络，并以此驱动中央供暖系统的温度决策。智能体分为家庭成员和代表，它们考虑个人偏好、个人特质、关系和天气条件。每日模拟包括家庭层面的共识，随后在代表之间进行整个建筑物的决策。我们测试了三种性格特质分布（积极、混合和消极），发现积极特质与更高的幸福感和更牢固的友谊相关。温度偏好、自信和无私对幸福感和决策有显著影响。这项工作展示了由LLM驱动的智能体如何帮助模拟复杂的人类行为，在现实生活中难以设置复杂的人类模拟。|
|**2025-07-13**|**Evaluating LLMs on Sequential API Call Through Automated Test Generation**|Yuheng Huang et.al.|[2507.09481](http://arxiv.org/abs/2507.09481)|null|通过整合外部API的工具，大型语言模型（LLMs）在复杂现实世界任务的多样化领域扩展了其有希望的潜力。然而，LLM工具的使用测试、评估和分析仍处于早期阶段。大多数现有基准依赖于手动收集的测试用例，其中许多不能自动检查语义正确性，而是依赖于静态方法，如字符串匹配。此外，这些基准往往忽略了在实际应用中常见的连续API调用之间的复杂交互。为了填补这一空白，在本文中，我们介绍了StateGen，这是一个旨在生成涉及连续API交互的多样化编码任务的自动化框架。StateGen结合了基于状态机的API约束求解和验证、基于能量的采样和控制流注入来生成可执行程序。然后，通过两个LLM代理的合作，将这些程序翻译成类似人类的自然语言任务描述。利用StateGen，我们构建了StateEval，这是一个包含120个经过验证的测试用例的基准，涵盖了三个代表性场景：会话服务、张量操作和ElevenLabs MCP。实验结果证实，StateGen可以有效地生成具有挑战性和现实性的API导向任务，突出了当前集成API的LLMs中需要改进的领域。|
|**2025-07-12**|**AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data**|Mohammad Abolnejadian et.al.|[2507.09100](http://arxiv.org/abs/2507.09100)|null|在决策对话中，专家需要在对话过程中应对复杂的选择并即时做出决策。尽管通常存在大量的历史数据，但这些场景的实时性使得决策者难以审查和利用相关信息。这引发了一个有趣的问题：如果专家能够通过从历史数据中获得的见解，在实时决策中利用相关历史数据会怎样？为了探索这个问题，我们实现了一个会话式用户界面，以医生-患者互动为例进行应用。我们的系统持续监听对话，识别患者问题和医生提出的解决方案，并从嵌入的数据集中检索相关数据，利用围绕基于检索的大型语言模型（LLM）代理构建的管道生成简明的见解。我们通过将加拿大卫生数据集嵌入到向量数据库中，并使用样本医生-患者对话进行模拟研究来评估原型，展示了其有效性但也存在挑战，为我们的下一步工作指明了方向。|
|**2025-07-11**|**Agent Safety Alignment via Reinforcement Learning**|Zeyang Sha et.al.|[2507.08270](http://arxiv.org/abs/2507.08270)|null|随着能够使用工具的自主大型语言模型（LLM）代理的出现，引入了超出传统对话误用的新安全风险。这些能够执行外部功能的代理，容易受到用户发起的威胁（例如，对抗性提示）和工具发起的威胁（例如，受损工具的恶意输出）。在本文中，我们提出了第一个针对使用工具的代理的统一安全对齐框架，使模型能够通过结构化推理和沙盒强化学习处理这两种威胁渠道。我们引入了一个三模态分类法，包括对用户提示和工具响应的良善、恶意和敏感类别，并定义了一个政策驱动的决策模型。我们的框架采用了一个自定义设计的沙盒环境，该环境模拟真实世界的工具执行并允许精细的奖励塑造。通过对公共和自建基准的广泛评估，包括Agent SafetyBench、InjecAgent和BFCL，我们证明了我们的安全对齐代理在保持良善任务上的强大效用的同时，显著提高了对安全威胁的抵抗力。我们的结果表明，安全和有效性可以共同优化，为自主LLM代理的可信部署奠定了基础。|
|**2025-07-11**|**SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments**|Avi Arora et.al.|[2507.09063](http://arxiv.org/abs/2507.09063)|null|现代大型语言模型（LLM）代理承诺提供端到端的真实世界软件任务协助，但现有的基准测试几乎完全在预先安装了所有依赖项的预配置环境中评估LLM代理。为了填补这一空白，我们引入了SetupBench，这是一个包含93个实例的基准测试，它隔离了环境启动技能：从一个裸露的Linux沙盒开始，代理必须安装软件包、解决依赖项冲突、初始化数据库以及配置后台服务。我们的任务涵盖了七个语言生态系统、五个数据库引擎以及多服务编排场景，每个场景都附有自然语言问题说明和一个确定性的成功命令。通过评估OpenHands这一最先进的编码代理，我们发现成功率在任务类别之间普遍较低，特别是仓库设置（38.9%-57.4%）和本地数据库配置（20.0%-53.3%）方面存在特别挑战。我们的分析揭示了系统性的失败模式，包括不完整开发工具安装、任务约束的幻觉以及破坏代理-人类协作工作流程的非持久环境修改。我们发现代理探索策略中存在重大低效，与最佳人类行为相比，38%-89%的动作是不必要的。这些发现突出了当前代理在实用环境启动能力方面的不足。通过针对这一关键但评估不足的能力，SetupBench为旨在解决端到端真实世界任务的下一代软件开发代理提供了一个严格的衡量标准。|
|**2025-07-11**|**Bridging Literature and the Universe Via A Multi-Agent Large Language Model System**|Xiaowen Zhang et.al.|[2507.08958](http://arxiv.org/abs/2507.08958)|null|随着宇宙学模拟及其相关软件变得越来越复杂，物理学家面临着在大量文献和用户手册中寻找从密集的学术论文中提取模拟参数的挑战，这些论文使用了不同的模型和格式。将这些参数转换为可执行的脚本仍然是一个费时且容易出错的过程。为了提高物理研究效率并加快宇宙学模拟过程，我们引入了SimAgents，这是一个旨在自动化从文献中配置参数和对宇宙学研究进行初步分析的多智能体系统。SimAgents由能够进行物理推理、验证模拟软件和执行工具的特殊化LLM智能体驱动。这些智能体通过结构化通信协作，确保提取的参数在物理上有意义、内部一致且符合软件规范。我们还通过收集来自arXiv和主要期刊发表的40多篇论文中的超过40个模拟构建了一个宇宙学参数提取评估数据集。在该数据集上的实验展示了SimAgents出色的性能，突显了其在加速物理学家科学研究中的有效性和潜力。我们的演示视频可在以下网址查看：https://youtu.be/w1zLpm_CaWA。完整的系统和数据集可在以下网址公开获取：https://github.com/xwzhang98/SimAgents。|
|**2025-07-11**|**Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents**|Enhao Zhang et.al.|[2507.08944](http://arxiv.org/abs/2507.08944)|null|基于大型语言模型（LLM）的多智能体系统通过将复杂任务分解为可迭代规划的子任务，执行、观察和细化，在处理复杂任务方面展现出了巨大的潜力。尽管这些系统非常有效，但它们通常会产生较高的延迟，因为现实世界的问题通常需要多次推理步骤的迭代循环。为了解决这一挑战，我们提出了M1-Parallel框架，该框架可以并行运行多个多智能体团队，以发现不同的解决方案路径。通过利用事件驱动的通信模型和异步消息传递，M1-Parallel有效地利用了有效计划的内生多样性，以减少端到端延迟或提高任务完成率。我们的复杂任务实验表明，M1-Parallel在早期终止的情况下实现了高达2.2倍的速度提升，同时保持了准确性，而M1-Parallel在聚合的情况下获得了更高的任务完成率。我们进一步研究了旨在鼓励多样化执行计划的策略，但观察到与重复采样相比，没有额外的性能提升。总体而言，这些发现强调了并行计划执行在优化用于现实世界高复杂度推理任务的多智能体系统中的潜力。|
|**2025-07-10**|**MIRIX: Multi-Agent Memory System for LLM-Based Agents**|Yu Wang et.al.|[2507.07957](http://arxiv.org/abs/2507.07957)|null|尽管人工智能代理的记忆能力越来越受到关注，现有的解决方案仍然存在根本性的限制。大多数解决方案依赖于平面、狭窄范围的记忆组件，这限制了它们个性化、抽象和可靠地随时间回忆用户特定信息的能力。为此，我们引入了MIRIX，这是一个模块化、多智能体记忆系统，通过解决该领域最关键的挑战——使语言模型真正记住，重新定义了人工智能记忆的未来。与先前的方法不同，MIRIX超越了文本，拥抱丰富的视觉和多模态体验，使记忆在现实场景中真正有用。MIRIX由六种不同、精心构建的记忆类型组成：核心记忆、事件记忆、语义记忆、程序记忆、资源记忆和知识库，以及一个多智能体框架，该框架动态控制并协调更新和检索。这种设计使代理能够持久化、推理和准确检索大量长期用户数据。我们在两个具有挑战性的环境中验证了MIRIX。首先，在ScreenshotVQA上，这是一个包含每个序列近20,000张高分辨率计算机屏幕截图的具有挑战性的多模态基准，需要深入理解上下文，并且没有现有的记忆系统可以应用，MIRIX的准确率比RAG基线高出35%，同时将存储需求减少了99.9%。其次，在LOCOMO上，这是一个具有单一文本输入的长篇对话基准，MIRIX达到了最先进的85.4%的性能，远远超过了现有的基线。这些结果表明，MIRIX为记忆增强的LLM代理设定了新的性能标准。为了使用户能够体验我们的记忆系统，我们提供了一个由MIRIX驱动的打包应用程序。它实时监控屏幕，构建个性化的记忆库，并提供直观的可视化和安全的地方存储，以确保隐私。|
|**2025-07-10**|**Agentic Retrieval of Topics and Insights from Earnings Calls**|Anant Gupta et.al.|[2507.07906](http://arxiv.org/abs/2507.07906)|null|通过分析公司财报电话会议中的主题，追踪公司的战略焦点是财务分析中的关键任务。然而，随着行业的发展，传统的主题建模技术在动态捕捉新兴主题及其关系方面遇到了困难。在本研究中，我们提出了一种基于LLM-agent的方法，用于从季度财报电话会议中挖掘和检索新兴主题。我们提出了一种LLM-agent，用于从文档中提取主题，将它们结构化成层次化的本体，并通过主题本体建立新旧主题之间的关系。我们展示了如何利用提取的主题来推断公司层面的洞察力和随时间变化的趋势。我们通过测量本体一致性、主题演变准确性和揭示新兴金融趋势的能力来评估我们的方法。|
|**2025-07-10**|**SAND: Boosting LLM Agents with Self-Taught Action Deliberation**|Yu Xia et.al.|[2507.07441](http://arxiv.org/abs/2507.07441)|null|大语言模型（LLM）智能体通常通过在ReAct风格的专家轨迹上进行监督微调或通过对成对回滚的偏好优化来进行调整。大多数这些方法侧重于模仿特定的专家行为或促进选定的推理思维和行动，而不是拒绝的行动。然而，由于缺乏对替代行动的推理和比较，使用这些方法微调的LLM智能体可能会因为行动空间探索有限而过度承诺看似合理但次优的行动。为了解决这个问题，在本文中，我们提出了自我学习的行动辩论（SAND）框架，使LLM智能体能够在做出承诺之前明确地辩论候选行动。为了解决在大行动空间和步骤级行动评估的情况下何时以及辩论什么的问题，我们引入了自我一致性行动采样和执行引导的行动评论，以帮助使用LLM智能体的基础模型合成逐步行动辩论思想。以迭代的方式，然后使用辩论轨迹来微调LLM智能体本身。在两个代表性的交互式智能体任务上评估，SAND在初始监督微调的基础上平均提高了20%，并且也优于最先进的智能体调整方法。|
|**2025-07-09**|**The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover**|Matteo Lupinacci et.al.|[2507.06850](http://arxiv.org/abs/2507.06850)|null|随着大型语言模型（LLM）代理和多智能体系统的快速采用，自然语言处理和生成能力达到了前所未有的水平。然而，这些系统引入了前所未有的安全漏洞，这些漏洞不仅超越了传统的提示注入攻击。本文首次全面评估了LLM代理作为攻击向量，通过利用智能体AI系统中自主实体之间相互交互和影响所存在的信任边界，实现完全的计算机接管。我们证明，攻击者可以利用三个不同的攻击面——直接提示注入、RAG后门攻击和智能体间信任利用——迫使流行的LLM（包括GPT-4o、Claude-4和Gemini-2.5）在受害者机器上自主安装和执行恶意软件。我们对17个最先进的LLM的评估揭示了一个令人担忧的漏洞等级：41.2%的模型易受直接提示注入攻击，52.9%易受RAG后门攻击，而关键性的82.4%可以通过智能体间信任利用被攻破。值得注意的是，我们发现即使能够成功抵御直接恶意命令的LLM，在收到同伴智能体的请求时也会执行相同的有效载荷，这揭示了当前多智能体安全模型中的一个基本缺陷。我们的发现表明，只有5.9%的测试模型（17个中的1个）对所有攻击向量具有抵抗力，大多数模型表现出与上下文相关的安全行为，从而创造了可利用的盲点。我们的发现还强调了提高对LLM安全风险的认识和研究的需求，显示出网络安全威胁的范式转变，其中AI工具本身成为复杂的攻击向量。|
|**2025-07-09**|**InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior**|Huisheng Wang et.al.|[2507.06528](http://arxiv.org/abs/2507.06528)|null|在行为金融学中，将大型语言模型（LLMs）与具有羊群行为投资者的决策过程相协调是一项关键挑战。它面临的一个基本限制是，对于监督微调（SFT）所需的真实用户数据的稀缺性。虽然SFT可以弥合LLM输出和人类行为模式之间的差距，但其依赖于大量真实数据，这带来了巨大的收集成本和隐私风险。我们提出了InvestAlign，一个新颖的框架，它通过利用类似和简单的最优投资问题的理论解决方案来构建高质量的SFT数据集，而不是复杂场景。我们的理论分析表明，使用InvestAlign生成的数据训练LLMs比使用真实用户数据更快地实现参数收敛，这表明了更高的学习效率。此外，我们开发了InvestAgent，一个用InvestAlign微调的LLM智能体，它在简单和复杂投资问题中都比SFT前的模型与真实用户数据更加一致。这突出了我们提出的InvestAlign作为一种有潜力的方法，有可能解决复杂的最优投资问题，并将LLMs与投资者的决策过程在羊群行为下相协调。我们的代码在https://github.com/thu-social-network-research-group/InvestAlign上公开。|
|**2025-07-09**|**Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery**|Licong Xu et.al.|[2507.07257](http://arxiv.org/abs/2507.07257)|null|我们提出了一种用于科学研究任务自动化的多智能体系统，即 cmbagent（https://github.com/CMBAgents/cmbagent）。该系统由约30个大型语言模型（LLM）智能体组成，并实现了一种规划与控制策略来协调智能体工作流程，整个过程中无需人工干预。每个智能体专注于不同的任务（如对科学论文和代码库进行检索、编写代码、解读结果、评价其他智能体的输出）且系统能够本地执行代码。我们成功将 cmbagent 应用于执行博士级别的宇宙学任务（使用超新星数据测量宇宙学参数），并在两个基准数据集上评估了其性能，发现其性能优于现有的LLM。源代码可在GitHub上获取，演示视频也一并提供，该系统已在HuggingFace上部署，并将提供云端服务。|
|**2025-07-08**|**Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models**|Marc Oriol et.al.|[2507.05981](http://arxiv.org/abs/2507.05981)|null|在计算机科学领域，大型语言模型（LLM）代理正被广泛应用于各种需求工程（RE）任务。关于提高其准确性的研究主要集中于提示工程、模型微调和检索增强生成。然而，这些方法通常将模型视为孤立的黑盒，依赖单次输出的结果，没有迭代优化或协作，这限制了模型的鲁棒性和适应性。目标：我们认为，正如人类的辩论可以通过引入不同的观点来提高RE任务的准确性和减少偏见一样，不同的LLM代理通过辩论和协作也可能实现类似的改进。我们的目标是研究多代理辩论（MAD）策略是否可以提高RE性能。方法：我们对跨多个领域的现有MAD策略进行了系统性研究，以确定其关键特征。为了评估其在RE中的适用性，我们实现并测试了一个基于MAD的RE分类初步框架。结果：我们的研究识别并分类了几种MAD策略，形成了一个概述其核心属性的分类法。我们的初步评估证明了将MAD应用于RE分类的可行性。结论：MAD为提高LLM在RE任务中的准确性提供了一种有希望的方法。本研究为MAD策略提供了一个基础性理解，为未来RE应用的研究和改进提供了洞见。|
|**2025-07-08**|**ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?**|Haoxin Wang et.al.|[2507.05639](http://arxiv.org/abs/2507.05639)|null|在这篇论文中，我们介绍了ECom-Bench，这是第一个用于评估在电子商务客户支持领域中具有多模态能力的LLM代理的基准框架。ECom-Bench具有基于从真实电子商务客户互动中收集的个性信息进行动态用户模拟的功能，以及从真实的电子商务对话中派生出的真实任务数据集。这些任务涵盖了广泛的业务场景，旨在反映现实世界的复杂性，使ECom-Bench极具挑战性。例如，即使是像GPT-4o这样的高级模型，在我们的基准测试中仅实现了10-20%的通过率，这突显了复杂电子商务场景所提出的巨大困难。论文发表后，代码和数据将开源，以促进该领域的进一步研究和开发。|
|**2025-07-08**|**LLMs are Introvert**|Litian Zhang et.al.|[2507.05638](http://arxiv.org/abs/2507.05638)|null|社交媒体和生成式AI的指数级增长改变了信息传播方式，促进了连接，但也加速了虚假信息的传播。理解信息传播动态和发展有效的控制策略对于减轻有害内容至关重要。传统的模型，如SIR模型，提供了基本的见解，但不足以捕捉在线互动的复杂性。包括注意力机制和图神经网络在内的高级方法提高了准确性，但通常忽略了用户心理和行为动态。具有类似人类推理能力的大型语言模型（LLM）为模拟信息传播的心理方面提供了新的潜力。我们引入了一个基于LLM的模拟环境，该环境捕捉代理的演变态度、情感和反应。然而，初步实验揭示了LLM生成的行为与真实人类动态之间存在重大差距，尤其是在立场检测和心理现实方面。通过社会信息处理理论进行的详细评估确定了在目标设定和反馈评估方面的主要差异，这些差异源于标准LLM训练中缺乏情感处理。为了解决这些问题，我们提出了基于社会信息处理的思想链（SIP-CoT）机制，该机制通过情绪引导的记忆得到增强。这种方法提高了对社交线索的解释、目标的个性化以及反馈的评估。实验结果证实，SIP-CoT增强的LLM代理更有效地处理社交信息，表现出更接近真实人类互动的行为、态度和情感。总之，这项研究突出了当前基于LLM的传播模拟中的关键局限性，并展示了如何通过整合SIP-CoT和情感记忆显著提高LLM代理的社会智能和现实感。|
|**2025-07-08**|**Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms**|Tarek Gasmi et.al.|[2507.06323](http://arxiv.org/abs/2507.06323)|null|大型语言模型（LLM）代理面临涵盖AI特定领域和传统软件领域的安全漏洞，但当前研究却分别处理这些问题。本研究通过使用统一的威胁分类框架，对函数调用架构和模型上下文协议（MCP）部署范式进行了比较评估，以弥合这一差距。我们在七个语言模型上测试了3250个攻击场景，评估了针对AI特定威胁（提示注入）和软件漏洞（JSON注入、拒绝服务）的简单、组合和链式攻击。函数调用显示出更高的整体攻击成功率（73.5%对MCP的62.59%），具有更大的系统中心性漏洞，而MCP则表现出更高的LLM中心性暴露。攻击复杂性的显著增加大大提高了攻击的有效性，链式攻击实现了91-96%的成功率。出人意料的是，高级推理模型尽管在威胁检测方面表现更好，但可利用性更高。结果表明，架构选择从根本上重塑了威胁格局。这项工作为跨领域LLM代理安全评估建立了方法论基础，并为安全部署提供了基于证据的指导。代码和实验材料可在https://github.com/theconsciouslab-ai/llm-agent-security获取。|
|**2025-07-08**|**Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles**|Yongchao Zeng et.al.|[2507.06310](http://arxiv.org/abs/2507.06310)|null|大型语言模型（LLMs）因其生成流畅、上下文连贯对话的出色能力，越来越多地被用于构建社会模拟中的代理。这种能力可以增强模型的现实感。然而，对现实感的追求并不一定与建模的认识论基础相兼容。我们认为，在许多方面，LLM代理过于人性化，难以建模：它们过于表达、详细和难以处理，难以符合建模通常要求的抽象、简化和可解释性。通过一个将Bass扩散模型转换为基于LLM的变种的建模思想实验，我们发现了五个核心困境：自然对话与抽象时间步之间的时间分辨率不匹配；在避免破坏自发代理输出的同时进行对话干预的需求；在保持对话自然性的同时引入规则性指令的诱惑；角色一致性随时间演变的紧张关系；以及理解涌现的挑战，其中系统级模式被冗长的微观文本输出所掩盖。这些困境将LLM代理引向了一个奇异谷：不够抽象，无法阐明潜在的社会机制，同时不够自然，无法代表真实的人类行为。这揭示了一个重要的悖论：当误用时，LLM代理的现实感可能会掩盖而不是阐明社会动态。我们梳理出LLM代理最适宜的条件：当系统级涌现不是焦点时，语言细微差别和意义是核心，互动在自然时间内展开，稳定的角色身份比长期行为演变更重要。我们呼吁重新定位LLM代理在社会模拟生态系统中的位置，以用于未来的应用。|
|**2025-07-07**|**Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions**|Yuanzhe Hu et.al.|[2507.05257](http://arxiv.org/abs/2507.05257)|null|近年来，对于大型语言模型（LLM）代理的基准测试主要集中于评估推理、规划和执行能力，而另一个关键组件——记忆，包括代理如何记忆、更新和检索长期信息，由于缺乏基准测试而评估不足。我们将具有记忆机制的代理称为记忆代理。在本文中，我们确定了记忆代理必须具备的四个核心竞争力：准确检索、测试时学习、长距离理解和冲突解决。现有的数据集要么依赖于有限的内容长度，要么是为静态、长内容环境（如基于书籍的问答）量身定制的，这些环境没有反映出记忆代理的互动、多轮累积信息的特性。此外，没有现有的基准测试涵盖所有四个能力。因此，我们引入了MemoryAgentBench，这是一个专门为记忆代理设计的基准测试。我们的基准测试结合了重新构架的现有数据集和新构建的数据集，涵盖了上述四个记忆能力，提供了一个系统且具有挑战性的测试平台，用于评估记忆质量。我们评估了各种记忆代理，从简单的基于上下文和检索增强的生成（RAG）系统到具有外部记忆模块和工具集成的先进代理。实证结果表明，当前的方法在掌握所有四个能力方面存在不足，突显了进一步研究LLM代理全面记忆机制的必要性。|
|**2025-07-07**|**FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System**|Toan Nguyen et.al.|[2507.04770](http://arxiv.org/abs/2507.04770)|null|家具装饰在各种工业应用中是一项重要任务。然而，实现高质量的装饰效果通常耗时且需要专业的艺术技能。为了应对这些挑战，我们探讨了如何利用多智能体系统来辅助自动化装饰过程。我们提出了FurniMAS，一个用于自动家具装饰的多智能体系统。具体来说，给定一个人类提示和一件家庭家具，如工作桌或电视柜，我们的系统会建议相关资产，包括适当的风格和材料，并将它们安排在物品上，确保装饰结果满足功能、美学和氛围偏好。FurniMAS组建了一个混合团队，包括基于LLM和非LLM的智能体，每个智能体在典型的装饰项目中扮演不同的角色。这些智能体通过通信、逻辑推理和验证来协作，将需求转化为最终结果。大量实验表明，我们的FurniMAS在生成高质量的3D装饰方面显著优于其他基线系统。|
|**2025-07-07**|**UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization**|Kai Yang et.al.|[2507.04706](http://arxiv.org/abs/2507.04706)|null|城市通用智能（UGI）指的是人工智能系统在动态和复杂的城市环境中自主感知、推理和行动的能力。在本文中，我们介绍了UrbanMind，这是一种工具增强的检索增强生成（RAG）框架，旨在促进UGI。UrbanMind的核心是一个基于持续检索增强的MoE型大型语言模型（C-RAG-LLM）的全新架构，该架构能够动态地融合领域特定的知识和不断演变的城市数据，以支持长期适应性。C-RAG-LLM的架构与多级优化框架自然吻合，其中不同层被视为相互依赖的子问题。每一层都有其独特的目标，可以通过分层学习过程独立或联合优化。该框架非常灵活，支持基于资源或部署约束的端到端训练和部分层优化。为了在数据漂移下保持适应性，它还集成了增量语料库更新机制。对各种复杂度的真实世界城市任务的评估验证了所提出框架的有效性。这项工作为在未来城市环境中实现通用型大型语言模型代理迈出了有希望的步伐。|
|**2025-07-07**|**Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment**|Jiahuan Pei et.al.|[2507.05528](http://arxiv.org/abs/2507.05528)|null|大型语言模型（LLMs）推动了虚拟教育者和学习者的进步，实现了自然语言处理（NLP）与AI教育（AI4Education）的结合。现有工作往往缺乏可扩展性，未能利用多样化的、大规模的课程内容，并且缺乏评估教学质量的框架。为此，我们提出了WikiHowAgent，这是一种利用LLMs模拟互动教学-学习对话的多代理工作流程。它集成了教师和学习者代理、交互管理器和评估器，以促进程序学习并评估教学质量。我们引入了一个包含114,296个基于17个领域和727个主题的14,287篇教程的师生对话数据集。我们的评估协议结合了计算和评分标准指标，并与人类判断保持一致。结果表明，该工作流程在不同环境中都表现出有效性，为LLMs在不同领域的功能提供了见解。我们的数据集和实现都是完全开源的。|
|**2025-07-07**|**MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents**|Ming Gong et.al.|[2507.05330](http://arxiv.org/abs/2507.05330)|null|最近大型语言模型（LLMs）的进展使得电子商务客户服务领域出现了新的应用。然而，在复杂的多模态场景中，它们的性能仍然受限。我们提出了MindFlow，这是第一个针对电子商务设计的开源多模态LLM代理。MindFlow基于CoALA框架构建，集成了记忆、决策和动作模块，并采用模块化的“MLLM作为工具”策略来实现效果的可视化-文本推理。通过在线A/B测试和基于模拟的消融实验进行评估，MindFlow在处理复杂查询、提高用户满意度和降低运营成本方面取得了显著进步，在实际部署中观察到相对提高了93.53%。|
|**2025-07-05**|**BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering**|Costas Mavromatis et.al.|[2507.04127](http://arxiv.org/abs/2507.04127)|null|知识图谱问答（KGQA）由于输入图的结构和语义多样性而面临重大挑战。现有工作依赖于大型语言模型（LLM）代理进行图遍历和检索；这种方法对遍历初始化敏感，因为它容易产生实体链接错误，并且可能无法很好地推广到自定义（“带来自己的”）知识图谱。我们引入了BYOKG-RAG框架，通过协同结合LLM和专门的图检索工具来增强KGQA。在BYOKG-RAG中，LLM生成关键的图元素（问题实体、候选答案、推理路径和OpenCypher查询），而图工具将这些元素链接到知识图谱并检索相关图上下文。检索到的上下文使LLM能够在生成最终答案之前迭代地改进其图链接和检索。通过从不同的图工具检索上下文，BYOKG-RAG为自定义知识图谱的问答提供了一个更通用和稳健的解决方案。通过对涵盖不同知识图谱类型的五个基准进行的实验，我们证明了BYOKG-RAG比第二好的图检索方法高出4.5个百分点，并且显示出对自定义知识图谱的更好泛化。BYOKG-RAG框架已在https://github.com/awslabs/graphrag-toolkit上开源。|
|**2025-07-05**|**Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments**|Zheng Jia et.al.|[2507.04037](http://arxiv.org/abs/2507.04037)|null|在静态基准与真实世界法律实践动态性质之间的差距构成了推进法律智能的关键障碍。为此，我们介绍了J1-ENVS，这是第一个专为基于LLM（大型语言模型）的代理而设计的交互式和动态法律环境。在法律专家的指导下，它包含了来自中国法律实践中的六个代表性场景，涵盖了三个层次的环境复杂性。我们进一步介绍了J1-EVAL，这是一个细粒度评估框架，旨在评估不同法律专业水平下的任务性能和程序合规性。在17个LLM代理上的广泛实验表明，尽管许多模型展示了扎实的法律知识，但在动态环境中的程序执行方面存在困难。即使是SOTA（最先进的技术）模型GPT-4o，整体性能也未能达到60%。这些发现突显了在实现动态法律智能方面存在的持续挑战，并为未来研究提供了有价值的见解。|
|**2025-07-05**|**Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents**|Baiqiao Zhang et.al.|[2507.04005](http://arxiv.org/abs/2507.04005)|null|在心理学和人机交互领域，有效且不易察觉的人格评估执行正受到越来越多的关注。本研究探索了一种互动式的人格评估方法，重点关注人格表现的多样性。我们提出了一种通过多人格表现（多PR GPA）进行游戏化人格评估的框架。该框架利用大型语言模型赋予虚拟代理不同的人格。这些代理通过参与互动游戏来引发多方面的人类人格表现。利用在整个交互过程中生成的多类型文本数据，它实现了两种人格评估方式（即直接评估和基于问题的评估）并提供了可解释的见解。基于经典的五大人格理论，我们实施了一个原型系统并进行了用户研究，以评估多PR GPA的有效性。结果表明，我们的方法在人格评估中非常有效，并且当考虑到人格表现的多样性时，它实现了优越的性能。|
|**2025-07-05**|**CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate**|Yiliu Sun et.al.|[2507.03928](http://arxiv.org/abs/2507.03928)|null|如今，单个大型语言模型（LLM）面临着诸如幻觉和推理能力不足等关键问题。为了缓解这些问题，多智能体辩论（MAD）作为一种有效的策略应运而生，其中LLM智能体在任务上与其他智能体进行深入的辩论。然而，现有的MAD方法面临两个主要问题：（a）输入上下文过长，导致LLM智能体在大量输入信息和经验中迷失，性能下降；（b）过度自信的困境，自信的LLM智能体主导辩论，导致辩论效果低下。为了解决这些局限性，我们提出了一种名为“CortexDebate”的新颖MAD方法。受人类大脑白质控制的皮层区域倾向于建立稀疏和动态优化的网络这一趋势的启发，CortexDebate在LLM智能体之间构建了一个稀疏的辩论图，其中每个LLM智能体只与对其有帮助的智能体进行辩论。为了优化图，我们提出了一种名为麦肯锡辩论主题（MDM）的模块，它充当白质的模拟。通过整合社会学中一个成熟的可信度度量——麦肯锡信任公式，MDM能够进行可信的评估，从而指导图优化。我们的CortexDebate的有效性通过八个数据集的广泛实验结果得到了充分证明，这些数据集涵盖了四种任务类型。|
|**2025-07-04**|**GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation**|Himanshu Dutta et.al.|[2507.03311](http://arxiv.org/abs/2507.03311)|null|论文摘要中文翻译：  在文档级机器翻译（DocMT）方法中，常常难以有效捕捉语篇层面的现象。现有的方法依赖于启发式规则将文档分割成语篇单元，这些分割很少与准确翻译所需的真正语篇结构相一致。否则，它们在翻译过程中无法保持文档的一致性。为了解决这些挑战，我们提出了图增强代理框架用于文档级翻译（GRAFT），这是一种新颖的基于图的DocMT系统，它利用大型语言模型（LLM）代理进行文档翻译。我们的方法将分割、基于有向无环图（DAG）的依存建模和语篇感知翻译整合为一个统一的框架。在八个翻译方向和六个不同领域的翻译任务中进行的实验表明，GRAFT在性能上显著优于最先进的DocMT系统。具体来说，GRAFT在IWSLT2017的TED测试集上，相对于强大基线平均提升了2.8个BLEU分，在从英语到中文的领域特定翻译中提升了2.3个BLEU分。此外，我们的分析突出了GRAFT在处理语篇层面现象方面的一致能力，能够产生连贯且上下文准确的翻译。|
|**2025-07-04**|**Conformal Information Pursuit for Interactively Guiding Large Language Models**|Kwan Ho Ryan Chan et.al.|[2507.03279](http://arxiv.org/abs/2507.03279)|null|本文探讨了用于解决交互式问答任务的指令微调大型语言模型（LLMs）的重要应用场景。在这种场景中，LLM代理通过依次从用户那里查询相关信息来进行预测，而不是进行单轮对话。本文探索了旨在最小化期望查询次数的顺序查询策略。其中一种策略是信息追求（IP），这是一种贪心算法，在每个迭代过程中选择最大化信息增益或等价地最小化不确定性的查询。然而，由于LLMs概率的过度或不足自信，在实践中获取互信息或条件熵的准确估计非常困难，这导致查询选择和预测性能次优。为了更好地估计每个迭代的不确定性，我们提出了符合信息追求（C-IP），这是一种基于符合预测集的顺序信息增益的替代方法。具体来说，C-IP利用每个迭代中预测集与条件熵之间的关系，根据符合预测集的平均大小来估计不确定性。与条件熵不同，我们发现符合预测集是一种无分布且鲁棒的不确定性测量方法。20个问题的实验表明，与之前的IP和基于不确定性的思维链方法相比，C-IP获得了更好的预测性能和更短的查询-回答链。此外，在MediQ数据集上扩展到医生和患者之间的交互式医疗场景，C-IP在直接单轮预测的同时，提供了更高的可解释性，并取得了具有竞争力的性能。|
|**2025-07-03**|**Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents**|Jiangrong Wu et.al.|[2507.02699](http://arxiv.org/abs/2507.02699)|null|随着大型语言模型（LLM）能力的增强，LLM代理应用迅速普及，开发者通过访问外部资源来增强LLM以支持复杂任务的执行。在这些应用中，LLM电子邮件代理应用是广泛使用的一类，因为电子邮件仍然是用户的重要沟通媒介。LLM电子邮件代理能够通过LLM驱动的推理来管理和回复电子邮件，并通过外部电子邮件API（例如发送电子邮件）自主执行用户指令。然而，尽管这些应用部署和使用的范围不断扩大，LLM电子邮件代理应用的安全机制仍未被充分探索。目前，尚未对这些代理应用中潜在的安全风险及其更广泛的影响进行综合研究。在本文中，我们首次对LLM电子邮件代理进行了深入和系统的安全研究。我们提出了电子邮件代理劫持（EAH）攻击，该攻击通过外部电子邮件资源覆盖电子邮件代理的原始提示，允许攻击者远程控制电子邮件代理，并进一步在不让用户知情的情况下执行特定的攻击场景。为了便于大规模评估，我们提出了EAHawk，这是一个用于评估LLM电子邮件代理应用EAH攻击的管道。通过EAHawk，我们对14个代表性的LLM代理框架、63个代理应用、12个LLM和20个电子邮件服务进行了实证研究，从而生成了1,404个实际电子邮件代理实例用于评估。实验结果表明，所有1,404个实例都被成功劫持；平均而言，只需要2.03次攻击尝试就能控制一个电子邮件代理实例。更糟糕的是，对于某些LLM，实现完全代理控制所需的平均尝试次数甚至降至1.23。|
|**2025-07-03**|**VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning**|Siran Chen et.al.|[2507.02626](http://arxiv.org/abs/2507.02626)|null|由于强大的自然语言处理和生成能力，大型语言模型（LLM）代理已成为通过用户模拟增强推荐系统的一种有希望的解决方案。然而，在视频推荐领域，现有研究主要依赖于使用冻结的LLM的基于提示的模拟，并面临着多模态内容理解的复杂挑战。这通常导致项目建模和用户偏好学习不佳，从而最终限制了推荐性能。为了解决这些挑战，我们引入了VRAgent-R1，这是一种新颖的基于代理的范式，它将人类智能融入用户模拟。具体来说，VRAgent-R1由两个不同的代理组成：项目感知（IP）代理和用户模拟（US）代理，用于交互式用户-项目建模。首先，IP代理基于MLLM模拟人类类似的渐进式思考，有效地捕捉视频中的隐藏推荐语义。在IP代理提供的更全面的多模态内容理解的基础上，视频推荐系统能够提供更高品质的候选项目。随后，US代理基于深入的思维链（CoT）推理来细化推荐的视频集，并通过强化学习更好地与真实用户偏好对齐。在大型视频推荐基准测试上的实验结果证明了我们提出的VRAgent-R1方法的有效性，例如，IP代理在MicroLens-100k数据集上实现了NDCG@10的6.0%提升，而US代理在用户决策模拟方面的准确率比最先进的基线高出约45.0%。|
|**2025-07-03**|**CyberRAG: An agentic RAG cyber attack classification and reporting tool**|Francesco Blefari et.al.|[2507.02424](http://arxiv.org/abs/2507.02424)|null|在大型企业中，入侵检测和预防系统（IDS/IPS）每小时可生成数十万个警报，使安全分析师淹没在需要深入、快速发展的领域专业知识的日志中。传统的机器学习检测器虽然减少了警报量，但仍然有较高的误报率，而标准的单次检索增强生成（RAG）管道往往检索到不相关的上下文，并且无法证明其预测的合理性。为了克服这些不足，我们提出了CyberRAG，这是一个模块化、基于代理的RAG框架，为网络攻击提供实时分类、解释和结构化报告。一个中心的LLM代理协调（i）一个经过微调的专门分类器池，每个分类器针对不同的攻击家族；（ii）工具适配器用于丰富和警报；（iii）一个迭代检索和推理循环，不断查询特定领域的知识库，直到证据既相关又自洽。与传统的RAG系统不同，CyberRAG采用代理设计，实现了动态控制流和自适应推理。这种以代理为中心的架构可以自主地优化其威胁标签和自然语言解释，降低误报并提高可解释性。该框架完全可扩展：只需添加分类器即可支持新的攻击类型，而无需重新训练核心代理。CyberRAG经过评估，每类的准确率超过94%，通过语义协调将最终分类准确率提升至94.92%。生成的解释在BERTScore中的得分为0.94，在基于GPT-4的专家评估中的得分为4.9/5。这些结果表明，以代理为中心、以专家为导向的RAG可以将高检测准确率与可信赖的、SOC就绪的文本相结合，为半自主化的网络安全工作流程提供了一种实用且可扩展的途径。|
|**2025-07-03**|**OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent**|Bowen Chen et.al.|[2507.02353](http://arxiv.org/abs/2507.02353)|null|关键词选择在赞助搜索广告中对于广告活动的成功至关重要。虽然基于大型语言模型（LLM）的方法能够实现关键词的自动化生成，但它们存在三个主要局限性：依赖大规模的查询-关键词对数据、缺乏在线多目标性能监控和优化，以及关键词选择中的质量控制较弱。这些问题阻碍了LLM在通过监控和推理关键性能指标（如展示次数、点击次数、转化率和CTA效果）来实现关键词决策的完全自动化。为了克服这些挑战，我们提出了OMS，这是一个关键词生成框架，它具有以下特点：即时性（无需训练数据，监控在线性能，并相应地调整）、多目标性（采用代理推理以基于多个性能指标优化关键词）和自我反思性（代理性地评估关键词质量）。在基准测试和现实世界广告活动中的实验表明，OMS优于现有方法；消融和人工评估确认了每个组件的有效性和生成关键词的质量。|
|**2025-07-02**|**Decision-Oriented Text Evaluation**|Yu-Shiang Huang et.al.|[2507.01923](http://arxiv.org/abs/2507.01923)|null|自然语言生成（NLG）越来越多地应用于高风险领域，然而，常见的内在评估方法，如n-gram重叠或句子合理性，与实际决策效果的相关性较弱。我们提出了一种以决策为导向的框架来评估生成的文本，通过直接测量其对人类和大型语言模型（LLM）决策结果的影响。以市场摘要文本——包括客观的晨间摘要和主观的收盘分析——作为测试案例，我们根据由人类投资者和仅由这些文本提供信息的自主LLM代理执行的交易财务表现来评估决策质量。我们的发现表明，无论是人类还是LLM代理，在仅依靠摘要的情况下，都不能始终如一地超越随机性能。然而，更丰富的分析评论使人类-LLM协作团队显著优于单个人类或代理基线。我们的方法强调了通过其促进人类和LLM之间协同决策的能力来评估生成的文本的重要性，凸显了传统内在指标的局限性。|
|**2025-07-02**|**Evaluating LLM Agent Collusion in Double Auctions**|Kushal Agrawal et.al.|[2507.01413](http://arxiv.org/abs/2507.01413)|null|大型语言模型（LLMs）作为自主代理在各个领域的应用中展现出令人印象深刻的性能，应用范围迅速扩大。随着这些代理在社会经济互动中扮演的角色日益重要，识别它们可能出现的非期望行为变得至关重要。在本研究中，我们考察了它们可能选择进行勾结的场景，即定义为损害另一方的隐蔽合作。为了系统地研究这一问题，我们调查了在模拟的连续双边拍卖市场中作为卖家的LLM代理的行为。通过一系列控制实验，我们分析了沟通能力、模型选择和环境影响等因素如何影响卖家勾结的稳定性和出现。我们发现，直接卖家沟通增加了勾结倾向，勾结倾向在不同模型之间存在差异，而环境压力，如监管和权威人物的压力，会影响勾结行为。我们的发现突出了在部署基于LLM的市场代理时的经济和伦理方面的关键考虑。|
|**2025-07-02**|**AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing**|Yinwang Ren et.al.|[2507.01376](http://arxiv.org/abs/2507.01376)|null|AI代理是设计用于在动态环境中感知、推理和行动的自主系统。随着生成式AI（GenAI）、大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的快速发展，AI代理在语义理解、复杂推理和自主决策方面的能力得到了显著提升。同时，代理式AI的兴起突出了在动态和复杂环境中的适应性和目标导向的自主性。基于LLMs的AI代理（LLM-Agents）、基于MLLMs的AI代理（MLLM-Agents）和代理式AI共同推动了AI在信息处理、环境感知和自主决策方面的能力扩展，为智能制造开辟了新的途径。然而，这些新兴AI范式在智能制造中的定义、能力边界和实际应用仍然不明确。为了填补这一空白，本研究系统地回顾了AI和AI代理技术的演变，考察了LLM-Agents、MLLM-Agents和代理式AI的核心概念和技术进步，并探讨了它们在制造中的潜在应用以及可能面临的挑战。|
|**2025-07-02**|**The Future is Agentic: Definitions, Perspectives, and Open Challenges of Multi-Agent Recommender Systems**|Reza Yousefi Maragheh et.al.|[2507.02097](http://arxiv.org/abs/2507.02097)|null|大型语言模型（LLMs）正迅速地从被动的文本生成引擎转变为能够规划、记忆、调用外部工具并与彼此合作的代理实体。这篇视角性论文探讨了这样的LLM代理（及其社会）如何转变推荐系统的设计空间。我们引入了一种统一的公理化方法，该方法（i）将单个代理建模为一个包含其语言核心、工具集和分层记忆的元组，以及（ii）将多代理推荐系统捕捉为三个代理、共享环境和通信协议的三元组。在这个框架内，我们提出了四个端到端用例——互动派对策划、离线评估的合成用户模拟、多模态家具推荐和品牌对齐的解释生成——每个用例都展示了由代理编排解锁的独特能力。然后，我们提出了五个跨领域的挑战家族：协议复杂性、可扩展性、幻觉和错误传播、涌现的不一致（包括隐蔽共谋）和品牌合规性。对于每一个挑战，我们都对问题进行了形式化，回顾了初步的缓解策略，并概述了开放的研究问题。结果是既是一个蓝图也是一个议程：蓝图展示了如何将内存增强、使用工具的LLM代理组合成健壮的推荐流程，议程则邀请RecSys社区开发基准、理论保证和管理工具，以适应这种新的自主程度。通过将代理抽象与推荐目标统一，本文为下一代个性化、可信赖且内容丰富的推荐服务奠定了基础。|
|**2025-07-02**|**Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab**|Haonan Duan et.al.|[2507.02083](http://arxiv.org/abs/2507.02083)|null|设计实验和结果解释是核心的科学研究能力，尤其是在生物学领域，研究人员通过扰动复杂系统来揭示其背后的机制。最近评估大型语言模型（LLMs）科学能力的工作未能测试这些能力，因为湿实验室实验的成本过高：在专业知识、时间和设备方面。我们引入了SciGym，这是一个首创的基准，用于评估LLMs在开放式科学发现任务中迭代实验设计和分析能力。SciGym通过运行生物系统的干实验室来克服湿实验室成本的问题。这些模型以系统生物学标记语言编码，生成模拟数据效率高，使它们成为真实复杂系统实验的理想测试平台。我们对六个前沿LLMs在137个小系统上进行了评估，并发布了总共350个系统。我们的评估表明，虽然更强大的模型表现出优异的性能，但随着系统复杂性的增加，所有模型的性能都显著下降，这表明LLMs的科学能力还有很大的提升空间。|
|**2025-07-01**|**Enhancing LLM Agent Safety via Causal Influence Prompting**|Dongyoon Hahm et.al.|[2507.00979](http://arxiv.org/abs/2507.00979)|null|随着由大型语言模型（LLMs）驱动的自主代理在各类辅助任务中展现出潜力，确保它们的安全和可靠行为对于预防意外后果至关重要。在这项工作中，我们引入了CIP，这是一种利用因果影响图（CIDs）来识别和减轻代理决策带来的风险的新技术。CIDs提供了因果关系的结构化表示，使代理能够预见有害结果并做出更安全的决策。我们的方法包括三个关键步骤：（1）根据任务规范初始化CID以概述决策过程，（2）使用CID引导代理与环境交互，（3）根据观察到的行为和结果迭代优化CID。实验结果表明，我们的方法有效地提高了代码执行和移动设备控制任务的安全性。|
|**2025-07-01**|**Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications**|Jindong Han et.al.|[2507.00914](http://arxiv.org/abs/2507.00914)|null|长期以来的智能城市愿景是利用大数据和人工智能技术创造高效、宜居和可持续的城市环境。近年来，大型语言模型（LLMs）的出现为实现这一愿景开辟了新的途径。凭借强大的语义理解和推理能力，LLMs可以作为智能代理部署，能够跨领域自主解决复杂问题。在本文中，我们关注城市LLM代理，这些代理是利用LLM的智能代理，在城市混合的物理-社会空间中半实体化，并用于系统级的城市决策。首先，我们介绍了城市LLM代理的概念，讨论了它们的独特能力和特征。其次，我们从代理工作流程的角度概述了当前的研究现状，包括城市感知、内存管理、推理、执行和学习。第三，我们将城市LLM代理的应用领域分为五组：城市规划、交通、环境、公共安全和城市社会，并分别介绍了每组中的代表性作品。最后，我们讨论了对于现实部署至关重要的可信度和评估问题，并确定了未来研究中的几个开放性问题。这次调查旨在为城市LLM代理这一新兴领域奠定基础，并为LLMs与城市智能交叉领域的发展提供路线图。一份精选的相关论文和开源资源的清单维护在https://github.com/usail-hkust/Awesome-Urban-LLM-Agents上，并持续更新。|
|**2025-07-01**|**Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity**|Jacopo Nudo et.al.|[2507.00657](http://arxiv.org/abs/2507.00657)|null|我们研究了大型语言模型（LLMs）在模拟社交媒体上的政治话语时的行为。利用2024年美国总统选举期间X平台上的2100万次互动，我们基于1186名真实用户构建了LLM代理，在受控条件下让他们对具有政治意义的推文进行回复。代理的初始化要么是极少的意识形态提示（零样本）要么是最近的推文历史（少量样本），允许与人类回复进行一对一的比较。我们在语言风格、意识形态一致性和毒性三个方面评估了三个模型家族（Gemini、Mistral和DeepSeek）。我们发现，更丰富的语境化提高了内部一致性，但也加剧了极化、风格化信号和有害语言。我们观察到一种新兴的扭曲，我们称之为“生成夸张”：对显著特征的系统性放大，超出经验基线。我们的分析表明，LLMs并不是模仿用户，而是在重新构建他们。他们的输出实际上更多地反映了内部优化动态，而不是观察到的行为，引入了结构性的偏差，这损害了它们作为社会代理的可靠性。这挑战了它们在内容监控、审议模拟和政策建模中的应用。|
|**2025-07-01**|**STELLA: Self-Evolving LLM Agent for Biomedical Research**|Ruofan Jin et.al.|[2507.02004](http://arxiv.org/abs/2507.02004)|null|随着生物医学数据、工具和文献的快速增长，形成了一个人类专业知识无法跟上节奏的研究格局。尽管人工智能代理提供了解决方案，但它们通常依赖于静态的、人工编辑的工具集，这限制了它们的适应性和扩展能力。在这里，我们介绍了STELLA，这是一种自我进化的AI代理，旨在克服这些限制。STELLA采用多智能体架构，通过两种核心机制自主提高其能力：一个不断演化的模板库用于推理策略，以及一个动态的工具海洋，随着工具创建代理自动发现和整合新的生物信息学工具而不断扩展。这使得STELLA能够从经验中学习。我们证明了STELLA在一系列生物医学基准测试中达到了最先进的准确率，在“人类的最后考试：生物医学”中得分约26%，在“LAB-Bench：DBQA”中得分54%，在“LAB-Bench：LitQA”中得分63%，领先于主要模型6个百分点。更重要的是，我们展示了它的性能会随着经验的增加而系统性地提高；例如，它在“人类的最后考试”基准测试上的准确率几乎翻倍。STELLA代表了朝着可以学习和成长的AI代理系统迈出的重要一步，这些系统能够动态地扩展其专业知识，以加快生物医学发现的步伐。|
|**2025-06-30**|**Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning**|Seungjun Yi et.al.|[2506.23998](http://arxiv.org/abs/2506.23998)|null|先天性心脏病（CHD）带来了复杂、终身的挑战，这在传统临床指标中往往被忽视。虽然非结构化叙述提供了对病人和照顾者经历的丰富见解，但手工主题分析（TA）仍然耗时且不可扩展。我们提出了一种全自动化的大型语言模型（LLM）流程，该流程对临床叙述进行端到端TA，从而消除了手动编码或全面转录审查的需求。我们的系统采用了一种新颖的多代理框架，其中专门化的LLM代理扮演角色以提升主题质量并确保与人类分析的一致性。为进一步提高主题相关性，我们可选地整合了来自人类反馈的强化学习（RLHF）。这支持了对大型定性数据集的可扩展、以病人为中心的分析，并允许LLM针对特定临床环境进行微调。|
|**2025-06-30**|**LLM Agents Are the Antidote to Walled Gardens**|Samuele Marro et.al.|[2506.23978](http://arxiv.org/abs/2506.23978)|null|尽管互联网的核心基础设施被设计成开放和通用的，但现在的应用层却被封闭的、专有的平台所主导。开放和互操作的API需要巨大的投资，市场领导者几乎没有激励去启用可能会削弱其用户粘性的数据交换。我们认为基于大型语言模型（LLM）的代理从根本上颠覆了这种现状。代理可以自动在数据格式之间进行翻译，并与为人类设计的界面进行交互：这使得互操作性大大降低成本并变得不可避免。我们称这种转变为实现普遍互操作性：任何两种数字服务都能通过AI介导的适配器无缝交换数据。普遍互操作性破坏了垄断行为并促进了数据便携性。然而，它也可能导致新的安全风险和技术债务。我们的立场是，机器学习（ML）社区应该拥抱这一发展，同时构建适当的框架来减轻负面影响。通过现在采取行动，我们可以利用AI恢复用户自由和竞争性市场，而不牺牲安全性。|
|**2025-06-30**|**Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs**|Manuel Pratelli et.al.|[2506.23610](http://arxiv.org/abs/2506.23610)|null|大型语言模型（LLMs）使得大规模生成合成行为数据成为可能，为人类实验提供了一种道德和低成本的选择。然而，这种数据是否能够忠实捕捉由个性特征驱动的心理差异，仍然是一个悬而未决的问题。我们评估了基于五大人格特质轮廓的LLM代理的能力，以复制基于人格的易受虚假信息影响的变化，重点关注新闻辨别能力，即判断真实标题为真，虚假标题为假的能力。利用已知人格轮廓的人类参与者对标题准确性进行评分的已发布数据集，我们创建了匹配的LLM代理，并将它们的反应与原始人类模式进行比较。某些特质与虚假信息关联，尤其是涉及宜人性、责任心等方面的关联，得到了可靠的复制，而其他关联则出现了分歧，揭示了LLMs在内部化和表达个性方面的系统性偏差。这些结果强调了个性对齐的LLMs在行为模拟方面的潜力与局限，并为在人工代理中模拟认知多样性提供了新的见解。|
|**2025-06-29**|**Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games**|David Guzman Piedrahita et.al.|[2506.23276](http://arxiv.org/abs/2506.23276)|null|随着大型语言模型（LLMs）作为自主代理的日益普及，理解它们的合作和社会机制变得越来越重要。特别是，LLMs如何在自我利益和集体福祉之间取得平衡，是确保对齐、鲁棒性和安全部署的一个关键挑战。在这篇论文中，我们研究了多智能体LLM系统中成本高昂的制裁挑战，其中代理必须决定是否投入自己的资源来激励合作或惩罚违约。为了研究这个问题，我们改编了行为经济学中的一个公共物品博弈，允许我们观察不同的LLMs如何在重复互动中解决社会困境。我们的分析揭示了模型中四种不同的行为模式：一些模型持续建立并维持高水平的合作，另一些模型在参与和退出之间波动，一些模型随着时间的推移逐渐降低合作行为，而另一些模型则严格遵循固定的策略，无论结果如何。令人惊讶的是，我们发现推理型LLM，如o1系列，在合作方面遇到了重大困难，而一些传统的LLM则能够持续实现高水平的合作。这些发现表明，目前改进LLM的方法，即专注于增强其推理能力，并不一定导致合作，为在需要持续协作的环境中部署LLM代理提供了宝贵的见解。我们的代码可在https://github.com/davidguzmanp/SanctSim上获取。|
|**2025-06-29**|**From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows**|Mohamed Amine Ferrag et.al.|[2506.23260](http://arxiv.org/abs/2506.23260)|null|基于大型语言模型（LLMs）并具有结构化函数调用界面的自主人工智能代理，极大地扩展了实时数据检索、复杂计算和多步骤编排的能力。然而，插件、连接器和代理间协议的爆炸性增长超过了发现机制和安全实践，导致脆弱的集成容易受到各种威胁。在这篇综述中，我们介绍了第一个针对LLM代理生态系统的统一、端到端威胁模型，涵盖了主机到工具和代理到代理的通信，形式化了攻击者的能力和攻击者的目标，并列举了三十多种攻击技术。具体来说，我们将威胁模型分为四个领域：输入操纵（例如，提示注入、长上下文劫持、多模态对抗性输入）、模型妥协（例如，提示和参数级别的后门、复合和加密的多后门、中毒策略）、系统和隐私攻击（例如，推测性旁路通道、成员推理、检索中毒、社会工程学模拟）以及协议漏洞（例如，模型上下文协议（MCP）、代理通信协议（ACP）、代理网络协议（ANP）和代理到代理（A2A）协议中的漏洞）。对于每个类别，我们回顾了代表性场景，评估了现实世界的可行性，并评估了现有的防御措施。基于我们的威胁分类法，我们确定了关键开放挑战和未来的研究方向，例如通过动态信任管理和加密溯源来保护MCP部署；设计和强化代理网络接口；以及在多代理和联邦环境中实现弹性。我们的工作为设计稳健的防御机制和建立具有弹性的LLM代理工作流程的最佳实践提供了全面的参考。|
|**2025-06-27**|**More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents**|Weimin Xiong et.al.|[2506.21967](http://arxiv.org/abs/2506.21967)|null|当前对工具集成LLM智能体的评估通常集中在端到端工具使用评估，而忽略了其稳定性。这限制了它们在现实世界中的应用，因为各种内部或外部因素可能导致智能体崩溃或行为异常。我们的研究通过调查智能体在整个工具调用过程中是否容易出错，包括阅读工具文档、选择工具和生成参数，以及处理工具的响应来解决这个问题。通过广泛的实验，我们观察到智能体在每个阶段都高度容易出错，基于开源模型的智能体比基于专有模型的智能体更容易出错。我们还发现，增加模型大小并不能显著提高工具调用推理，并可能使智能体更容易受到类似正常用户指令的攻击。这突出了评估智能体稳定性的重要性，并为未来LLM的开发和评估提供了有价值的见解。|
|**2025-06-27**|**RExBench: Can coding agents autonomously implement AI research extensions?**|Nicholas Edwards et.al.|[2506.22598](http://arxiv.org/abs/2506.22598)|null|基于大型语言模型（LLMs）的智能体在执行复杂的软件工程任务方面展现出巨大潜力。此外，在开发能够执行机器学习和自然科学研究流程中部分任务的智能体方面也取得了进展。我们认为，研究扩展及其实现是这类系统的一个关键能力，并介绍了RExBench来支持这一能力的评估。RExBench是一个包含12个现实研究实验实现任务的基准，旨在研究尚未实施的研究假设。每个任务都设定为现有研究论文和代码库的扩展，并附有领域专家编写的说明。RExBench对数据污染具有鲁棒性，并支持一个自动评估基础设施，该基础设施执行智能体输出以确定是否满足成功标准。我们使用这个基准来评估了使用三个不同框架实现的九个LLM智能体：aider、Claude Code和OpenHands。我们发现，所有评估的智能体都无法自主实现大多数扩展。尽管在额外的人为提示下成功率有所提高，但在这种设置下的最佳性能仍然低于40%。这表明，当前的智能体在没有大量人为指导的情况下，仍然无法处理现实的研究扩展任务。|
|**2025-06-26**|**LLM-guided Chemical Process Optimization with a Multi-Agent Approach**|Tong Zeng et.al.|[2506.20921](http://arxiv.org/abs/2506.20921)|null|化学工艺优化对于最大化生产效率和经济效益至关重要。当操作约束不明确或不可用时，传统方法，包括基于梯度的求解器、进化算法和参数网格搜索，变得不切实际，迫使工程师依赖主观启发式方法来估计可行的参数范围。为了解决这一约束定义瓶颈，我们提出了一种基于大型语言模型（LLM）代理的多代理框架，该框架能够自主地从最少的工艺描述中推断出操作约束，然后利用推断出的约束进行协作优化。我们的基于AutoGen的代理框架采用OpenAI的o3模型，并配备了专门用于约束生成、参数验证、仿真执行和优化指导的代理。通过两个阶段——使用嵌入式领域知识进行自主约束生成，然后是迭代多代理优化——该框架消除了对预定义操作界限的需求。在成本、产率和产率成本比等指标上对水脱烷基工艺进行了验证，该框架在性能上与传统优化方法相当，同时实现了更好的计算效率，需要更少的迭代次数才能收敛。我们的方法在不到20分钟内收敛，比网格搜索快了31倍。除了计算效率之外，该框架的推理引导搜索展示了复杂的工艺理解，正确地识别了效用权衡，并应用了领域信息启发式方法。这种方法在操作约束描述不良或不可用的情况下显示出巨大的潜力，特别是对于新兴工艺和改造应用。|
|**2025-06-25**|**Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges**|Alexander D. Kalian et.al.|[2506.20598](http://arxiv.org/abs/2506.20598)|null|全球对可持续蛋白质来源的需求加速了对能够快速处理和综合特定科学知识的智能工具的需求。在本研究中，我们提出了一种概念验证的多智能体人工智能（AI）框架，旨在支持可持续蛋白质生产研究，最初重点关注微生物蛋白质来源。我们的以检索增强生成（RAG）为导向的系统由两个基于GPT的大语言模型（LLM）智能体组成：（1）一个文献搜索智能体，用于检索特定微生物菌株的微生物蛋白质生产的相关科学文献；（2）一个信息提取智能体，用于处理检索到的内容以提取相关的生物和化学信息。为了智能体优化，探索了两种并行方法，即微调和提示工程。两种方法都在提高信息提取智能体的性能方面表现出有效性，即在获得的结果和理想输出之间的基于transformer的余弦相似度分数。平均余弦相似度分数提高了高达25%，而与理想输出文本的平均分数普遍达到 $\geq 0.89$。与提示工程相比，微调在总体上提高了平均分数（始终为$\geq 0.94$ ），尽管后者方法观察到了较低的统计不确定性。开发并发布了一个用户界面，以使多智能体AI系统易于使用，同时初步探索了基于化学安全性的搜索功能。|
|**2025-06-25**|**Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis**|Zhonghao Zhan et.al.|[2506.20806](http://arxiv.org/abs/2506.20806)|null|图神经网络（GNNs）在网络安全入侵检测系统（NIDS）中展现出巨大潜力，尤其在物联网环境中，但因其分布漂移和对现实对抗攻击的脆弱性而性能下降。当前的鲁棒性评估通常依赖于不切实际的合成扰动，且缺乏对不同类型对抗攻击的系统分析，包括黑盒和白盒场景。本研究提出了一种新方法，通过在代理管道中使用大型语言模型（LLMs）作为模拟网络安全专家代理来增强GNN的鲁棒性和泛化能力。这些代理仔细审查从网络流量数据中提取的图结构，在GNN处理之前识别并可能减轻可疑或对抗性扰动的元素。我们的实验使用一个旨在进行现实评估和测试的框架，包括从物理测试平台实验中收集的数据集，证明了集成LLM分析可以显著提高基于GNN的NIDS对挑战的弹性，展示了LLM代理作为入侵检测架构中补充层的潜力。|
|**2025-06-25**|**A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools**|Minh-Hao Van et.al.|[2506.20743](http://arxiv.org/abs/2506.20743)|null|基础模型（FMs）通过实现可扩展、通用和多模态的AI系统，正在推动材料科学（MatSci）领域发生变革性转变。与传统的机器学习模型不同，后者通常具有狭窄的应用范围并需要针对特定任务进行工程化设计，FMs提供了跨领域的泛化能力和涌现能力。它们的灵活性特别适合于材料科学领域，该领域的研究挑战涉及多种数据类型和规模。本综述提供了对基础模型、代理系统、数据集和支撑这一新兴领域计算工具的全面概述。我们介绍了一种以任务为导向的分类法，涵盖了六个广泛的应用领域：数据提取、解释和问答；原子模拟；属性预测；材料结构、设计和发现；工艺规划、发现和优化；以及多尺度建模。我们讨论了单模态和多模态FMs的最新进展，以及新兴的大型语言模型（LLM）代理。此外，我们回顾了标准化数据集、开源工具和自主实验平台，这些平台共同推动了FMs在研究工作流程中的开发和集成。我们评估了基础模型的早期成功，并确定了持续的局限性，包括泛化性、可解释性、数据不平衡、安全担忧和有限的跨模态融合。最后，我们阐述了以可扩展预训练、持续学习、数据治理和可靠性为中心的未来研究方向。|
|**2025-06-25**|**MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation**|Gurusha Juneja et.al.|[2506.20737](http://arxiv.org/abs/2506.20737)|null|随着基于大型语言模型（LLM）的智能体的广泛应用，智能体间的协作在任务如调度、谈判、资源分配等方面得到了越来越多地部署。在这样的系统中，隐私至关重要，因为智能体常常需要访问专有工具和需要严格保密的特定领域数据库。本文探讨了基于LLM的智能体是否理解情境隐私。如果受到指令，这些系统是否能在非对抗性多轮对话中保护推理时间的用户隐私。现有的评估LLM智能体情境隐私的基准主要评估单轮、低复杂度任务，在这些任务中，私密信息可以轻松排除。我们首先提出一个基准——MAGPIE，包含158个来自15个领域的高风险实际场景。这些场景设计得如此，完全排除私密数据会妨碍任务完成，而信息共享不受限制则可能导致重大损失。然后，我们评估了当前最先进的LLM在（a）对情境隐私数据的理解以及（b）在不侵犯用户隐私的情况下协作的能力。实证实验表明，包括GPT-4o和Claude-2.7-Sonnet在内的当前模型缺乏对情境隐私的稳健理解，有25.2%和43.6%的时间将私密数据错误地分类为可共享。在多轮对话中，这些模型在59.9%和50.5%的情况下泄露了私密信息，即使在明确的隐私指令下也是如此。此外，在71%的场景中，多智能体系统无法完成任务。这些结果表明，当前模型没有同时朝着情境隐私保护和协作任务解决两个方向努力。|
|**2025-06-24**|**JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning**|Ai Han et.al.|[2506.19846](http://arxiv.org/abs/2506.19846)|null|多智能体强化学习（MARL）已成为解决日益复杂任务的重要范式。然而，由于合作效率低下和训练不稳定，异构智能体的联合进化仍然具有挑战性。在本文中，我们提出了一种称为JoyAgents-R1的MARL联合进化动力学，该方法首先将群组相对策略优化（GRPO）应用于异构多智能体的联合训练。通过迭代优化智能体的大型语言模型（LLMs）和记忆，该方法实现了整体均衡，具备最优决策和记忆能力。具体来说，JoyAgents-R1首先在智能体整个推理轨迹的行为上实施节点级蒙特卡洛采样，以增强GRPO采样效率的同时保持策略多样性。然后，我们的边际效益驱动选择策略识别出具有最大奖励波动的顶级- $K$ 采样组，通过有针对性的智能体模型更新，改善训练稳定性并通过成本效益高的参数调整最大化联合收益。同时，JoyAgents-R1引入了一种自适应记忆进化机制，将GRPO奖励作为免费的监督信号来消除重复推理并加速收敛。在通用和特定领域场景中的实验表明，JoyAgents-R1在基于较小开源模型的情况下实现了与更大LLMs相当的性能。|
|**2025-06-24**|**Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System**|Lixuan He et.al.|[2506.19433](http://arxiv.org/abs/2506.19433)|null|在大型城市环境中，视觉和语言导航（VLN）需要具身智能体将语言指令与复杂场景相联系，并在较长时间范围内回忆相关经验。之前的模块化流水线提供了解释性，但缺乏统一的记忆，而端到端（M）LLM智能体在融合视觉和语言方面表现出色，但仍受限于固定的上下文窗口和隐含的空间推理。我们引入了Mem4Nav，这是一个分层空间认知长短期记忆系统，可以增强任何VLN主干网络。Mem4Nav融合了稀疏八叉树进行细粒度体素索引和语义拓扑图进行高级地标连通性，并将两者存储在通过可逆Transformer嵌入的可训练记忆标记中。长期记忆（LTM）压缩并保留八叉树和图节点的历史观察，而短期记忆（STM）缓存最近的多模态条目在相对坐标中，以实现实时障碍物避让和局部规划。在每一步，STM检索会锐利地修剪动态上下文，当需要更深入的历史时，LTM标记会无损解码以重建过去的嵌入。在Touchdown和Map2Seq上，对三个主干网络（模块化、基于提示的LLM的SOTA VLN和基于步进注意力MLLM的SOTA VLN）进行评估，Mem4Nav在任务完成方面带来了7-13个百分点的提升，足够减少速度损失，并且nDTW改进超过10个百分点。消融实验证实了分层地图和双重记忆模块的不可或缺性。我们的代码通过https://github.com/tsinghua-fib-lab/Mem4Nav开源。|
|**2025-06-24**|**Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection**|Yazhou Zhang et.al.|[2506.19420](http://arxiv.org/abs/2506.19420)|null|多模态讽刺理解是一项高级认知任务。尽管大型语言模型（LLMs）在许多下游自然语言处理（NLP）任务上表现出色，但越来越多的证据表明，它们在讽刺理解上存在困难。在本文中，我们提出了Commander-GPT，这是一个受军事指挥理论启发的模块化决策路由框架。Commander-GPT不是依赖单个LLM的能力，而是协调一支由专门化的LLM代理组成的团队，其中每个代理将被选择性地分配到专注于特定子任务，如上下文建模、情感分析等。然后，他们的输出将被路由回指挥官，指挥官整合信息并执行最终的讽刺判断。为了协调这些代理，我们引入了三种类型的集中式指挥官：（1）一个基于训练的轻量级编码器指挥官（例如，多模态BERT）；（2）四个小型自回归语言模型，作为能力适中的指挥官（例如，DeepSeek-VL）；（3）两个基于大型LLM的指挥官（Gemini Pro和GPT-4o），以零样本方式执行任务路由、输出聚合和讽刺决策。我们在MMSD和MMSD 2.0基准测试中评估了Commander-GPT，比较了五种提示策略。实验结果表明，我们的框架在平均F1分数上比最先进的（SoTA）基线提高了4.4%和11.7%，证明了其有效性。|
|**2025-06-24**|**Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs**|Liang Zeng et.al.|[2506.19290](http://arxiv.org/abs/2506.19290)|null|软件工程（SWE）最近成为下一代LLM代理的关键测试平台，要求在两个关键维度上具备内在能力：持续的迭代问题解决（例如，>50轮交互）和长上下文依赖解析（例如，>32k个标记）。然而，SWE中的数据整理过程因其严重依赖手动注释进行代码文件筛选和设置专用运行时环境来执行和验证单元测试而闻名地耗时。因此，大多数现有数据集仅限于数千个来自GitHub的实例。为此，我们提出了一种增量、自动化的数据整理流程，系统地扩大了SWE数据集的规模和多样性。我们的数据集包含来自2,531个不同GitHub仓库的10,169个真实世界Python任务实例，每个实例都附有自然语言描述的任务和一个用于自动化单元测试验证的专用运行时环境镜像。我们从提议的SWE数据集中精心整理了超过8,000个成功运行时验证的训练轨迹。在将这些轨迹用于微调Skywork-SWE模型时，我们发现了一个引人注目的数据扩展现象：随着数据量的增加，训练模型在LLM的软件工程能力方面的性能持续提高，没有出现饱和的迹象。值得注意的是，我们的Skywork-SWE模型在SWE-bench Verified基准测试中达到了38.0%的pass@1准确率，而没有使用验证器或多次重放，在基于OpenHands代理框架的Qwen2.5-Coder-32B的LLM中建立了新的最先进（SOTA）水平。此外，通过结合测试时扩展技术，性能进一步提高到47.0%的准确率，超过了32B参数模型之前的SOTA结果。我们发布了Skywork-SWE-32B模型检查点，以加速未来的研究。|
|**2025-06-23**|**AgenticControl: An Automated Control Design Framework Using Large Language Models**|Mohammad Narimani et.al.|[2506.19160](http://arxiv.org/abs/2506.19160)|null|传统的控制系统设计依赖于专家知识和精确的模型，难以应对复杂、非线性或不确定的动态系统。本文介绍了一种名为AgenticControl的新型多智能体框架，该框架通过协调的大型语言模型（LLM）智能体自动化控制器设计。这些智能体通过结构化的JSON通信处理包括控制器选择、场景设计、参数优化、性能评估和决策等任务。通过演员-评论家优化方法，系统在逐步通过越来越复杂的场景的过程中迭代地提高性能，以确保在正常条件、测量噪声、执行器干扰和参数不确定性下的鲁棒性。主要创新包括结构化的多智能体协作、鲁棒的优化机制和通过情境学习实现的实时适应性。该框架在四个不同的控制系统上得到验证，包括直流电机位置控制、球和杆、倒立摆和双倒立摆，其性能优于经典方法。其全状态反馈解决方案与线性二次调节器（LQR）结果非常接近，而设计的PID控制器显著优于MATLAB的PIDTuner，通过自适应参数探索将PID跟踪误差降低了55%。对五种LLM模型的比较研究表明，它们的优化特征各不相同，其中DeepSeek实现了最快的收敛。这项工作展示了LLM驱动控制设计的潜力，为模型预测控制和强化学习等高级技术铺平了道路。|
|**2025-06-21**|**Bayesian Social Deduction with Graph-Informed Language Models**|Shahab Rahimirad et.al.|[2506.17788](http://arxiv.org/abs/2506.17788)|null|社会推理——从其他代理的局部观察中推断不可观察的信念和意图——仍然是大型语言模型（LLMs）面临的挑战。我们评估了当前推理语言模型在社交推理游戏《亚瑟王传奇》中的极限，发现尽管最大的模型表现出色，但它们需要大量的测试时推理，并且当缩小到更小的、实时可用的变体时，性能会急剧下降。为了解决这个问题，我们引入了一个混合推理框架，该框架将信念推理外部化到一个结构化的概率模型中，同时使用LLM进行语言理解和交互。我们的方法在与大型模型相当的性能下实现了有竞争力的表现，并且值得注意的是，这是第一个在受控研究中击败人类玩家的语言代理——实现了67%的胜率，并且比推理基线和人类队友获得了更高的质量评分。我们发布了代码、模型和数据集，以支持LLM代理中未来关于社会推理的工作，这些可以在https://camp-lab-purdue.github.io/bayesian-social-deduction/找到。|
|**2025-06-21**|**PAGENT: Learning to Patch Software Engineering Agents**|Haoran Xue et.al.|[2506.17772](http://arxiv.org/abs/2506.17772)|null|LLM代理自动生成补丁以解决问题。然而，它们可能会生成不准确的补丁。关于这些失败的补丁背后的根本原因以及如何修复它们，了解甚少。本文报告了对七家顶级LLM代码代理生成的失败补丁的实证研究。我们从SWE-bench Lite数据集中收集了114个问题，这些问题在代理之间都未能得到解决。七个代理为这些问题共生成了769个失败的补丁，我们通过GPT-4o和人工分析的结合进行了检查。我们提出了补丁失败原因的分类法。该分类法包含六个类别，每个类别下有多个子类别。例如，一个常见的类别是LLM无法在生成的补丁中正确推断/生成适当的变量类型。作为解决此类类型相关错误的第一步，我们设计了PAGENT（补丁代理）。PAGENT利用程序分析技术，如控制流图（CFG）创建和探索，来推断补丁的信息类型。它是通过应用存储库级别的静态代码分析技术来做到这一点的。然后，PAGENT通过进一步利用基于LLM的推断技术来细化推断出的类型。我们在我们研究中排名前三的代理的127个类型相关失败补丁上测试了PAGENT。PAGENT能够修复其中的29个失败补丁。|
|**2025-06-21**|**May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs**|Shaoyu Yang et.al.|[2506.17642](http://arxiv.org/abs/2506.17642)|null|人工智能（AI）基础设施，以深度学习（DL）框架为代表，在过去十年中一直是基础DL系统。然而，DL框架中的错误在某些关键场景（例如医疗保健和自动驾驶）可能导致灾难性后果。一种简单而有效的方法来寻找DL框架中的错误是模糊测试（Fuzzing）。不幸的是，现有的模糊测试技术没有全面考虑多种类型的反馈。此外，它们以粗粒度的方式分析反馈，例如仅根据覆盖率是否增加来变异测试用例。最近，研究人员将大型语言模型（LLM）引入到模糊测试中。然而，基于LLM的当前模糊测试技术仅关注使用LLM生成测试用例，而忽视了它们分析反馈信息的能力，未能创建更多有效和多样化的测试用例。为了填补这一空白，我们提出了FUEL，以打破反馈驱动模糊测试对DL框架的束缚。FUEL的核心由两个基于LLM的代理组成，即分析LLM和生成LLM。分析LLM代理从反馈信息中推断出分析摘要，而生成LLM代理根据这些分析摘要创建测试。到目前为止，FUEL已经为PyTorch和TensorFlow检测到104个错误，其中93个被确认为新错误，47个已经被修复，5个已分配CVE ID。我们的工作表明，考虑多种类型的反馈对模糊测试性能有益，利用LLM分析反馈信息是一个有前景的方向。我们的工件可在https://github.com/NJU-iSE/FUEL上找到。|
|**2025-06-20**|**OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections**|Manasa Bharadwaj et.al.|[2506.17449](http://arxiv.org/abs/2506.17449)|null|为了提高大型语言模型（LLM）在复杂任务上的表现，大部分研究都集中在微调和迭代自我校正上。然而，这些方法通常缺乏长期学习的泛化机制，在动态环境中效率低下。我们引入了OmniReflect，这是一个由反思驱动的分层框架，它构建了一个宪法，即从任务经验中提炼出的指导原则的紧凑集合，以增强LLM代理的有效性和效率。OmniReflect在两种模式下运行：自维持模式，其中单个代理在任务执行期间定期整理自己的反思；合作模式，其中元顾问从一个小校准集中推导出宪法来指导另一个代理。为了构建这些宪法原则，我们采用了神经、符号和神经符号技术，在上下文适应性和计算效率之间提供了平衡。平均跨模型的实证结果表明，在任务成功方面取得了重大改进，自维持模式下在ALFWorld上绝对收益为+10.3%，在BabyAI上为+23.8%，在PDDL上为+8.3%。在合作模式下也看到了类似的收益，其中轻量级的Qwen3-4B ReAct代理在BabyAI上优于所有反思基线。这些发现突出了OmniReflect在环境和骨干之间的鲁棒性和有效性。|
|**2025-06-20**|**UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making**|Jinhao Duan et.al.|[2506.17419](http://arxiv.org/abs/2506.17419)|null|随着大型语言模型（LLMs）被整合到涉及现实世界中序列决策的安全关键应用中，了解何时信任LLM的决策变得至关重要。现有的LLM不确定性量化（UQ）方法主要针对单轮问答格式设计，导致多步决策场景，例如LLM代理系统，研究不足。在本文中，我们介绍了一个基于原理、信息论框架的方法，将LLM序列决策不确定性分解为两部分：（i）当前决策固有的内部不确定性，这是现有UQ方法关注的重点；（ii）外部不确定性，这是一个描述应从先前决策中继承多少不确定性的互信息（MI）量。然后，我们提出了UProp，一个高效且有效的外部不确定性估计器，将MI的直接估计转换为多个轨迹相关决策过程（TDPs）上的点互信息（PMI）估计。UProp在广泛的多个步骤决策基准上进行了评估，例如AgentBench和HotpotQA，使用了最先进的LLMs，例如GPT-4.1和DeepSeek-V3。实验结果表明，UProp显著优于配备了深思熟虑的聚合策略的现有单轮UQ基线。此外，我们对UProp进行了全面的分析，包括采样效率、潜在应用和中间不确定性传播，以证明其有效性。代码将在https://github.com/jinhaoduan/UProp上提供。|
|**2025-06-19**|**HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks**|Jinbo Wen et.al.|[2506.15947](http://arxiv.org/abs/2506.15947)|null|低空经济网络（LAENets）正作为一种有潜力的范式，通过综合空地基础设施支持各种低空服务。为了满足低延迟和高计算需求，将无人机（UAVs）与移动边缘计算（MEC）系统相结合起着至关重要的作用，这将从终端设备卸载计算任务到附近的无人机，从而为地面用户提供灵活和弹性的服务。为了促进LAENets的发展，实现低碳多无人机辅助的MEC网络具有重要意义。然而，多方面因素阻碍了这一实施，包括多维度无人机建模的复杂性和多目标耦合优化的困难。为此，本文提出了一种基于检索增强生成（RAG）的大型语言模型（LLM）代理框架，用于模型构建。具体来说，我们开发了混合RAG，通过结合关键词RAG、向量RAG和图RAG，使LLM代理能够有效地从专家数据库中检索结构信息，与传统的基于RAG的LLM代理相比，生成更准确的优化问题。在为多无人机辅助的MEC网络定制碳排放优化问题后，我们提出了一种双正则化扩散增强的软演员-评论家（R²DSAC）算法来解决所形成的多目标优化问题。R²DSAC算法结合了扩散熵正则化和动作熵正则化，以提高扩散策略的性能。此外，我们动态地屏蔽演员网络中的不重要神经元，以减少与模型训练相关的碳排放。仿真结果表明，所提出的基于混合RAG的LLM代理框架和R²DSAC算法的有效性和可靠性。|
|**2025-06-19**|**LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research**|Shuo Yan et.al.|[2506.17335](http://arxiv.org/abs/2506.17335)|null|大型语言模型（LLM）在推动科学发现方面展现出巨大的潜力。然而，它们在从研究论文中复现代码这一基础而关键的任务中的能力，尤其是在自然语言处理（NLP）领域，仍被低估。这项任务包括在抽象概念的综合推理和相互依赖的文件代码库理解方面独特的复杂推理挑战。受此差距的启发，我们提出了LMR-BENCH，这是一个旨在系统地评估LLM代理从语言建模研究复现代码能力的基准。它由28个代码复现任务组成，这些任务来源于过去五年间发表在顶级NLP期刊上的23篇论文，涵盖了九个基本类别。模型被提供了研究论文、包含一个或多个掩码函数的代码库以及实现这些函数的说明。我们在标准提示和LLM代理设置中进行了广泛的实验，使用最先进的LLM评估了单元测试的准确性，并执行了基于LLM的代码正确性评估。实验结果表明，即使是最先进的模型在科学推理和代码合成方面仍存在持续的局限性，这突显了LLM代理在自主复现科学研究能力方面的关键差距。|
|**2025-06-18**|**The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games**|Lyle Goodyear et.al.|[2506.15624](http://arxiv.org/abs/2506.15624)|null|大型语言模型（LLMs）在动态环境中作为决策者展现出潜力，但它们无状态的本质要求创建历史事件的自然语言表示。我们提出一个统一框架，用于系统地构建自然语言“状态”表示，以提示在重复多智能体游戏中运行的LLM智能体。先前关于具有LLM智能体的游戏研究采取了临时的方法来编码游戏历史，这不仅掩盖了状态表示对智能体行为的影响，还限制了研究的可比性。我们的框架通过沿三个维度来描述状态表示的方法来填补这些空白：动作信息量（即状态表示捕获动作的程度）；奖励信息量（即状态表示描述获得的奖励的程度）；提示风格（或自然语言压缩，即全文历史的总结程度）。我们将此框架应用于动态自私路由游戏，选择该游戏是因为它在理论和人类受试者实验中都具有简单的均衡 \cite{rapoport_choice_2009}。尽管游戏相对简单，但我们发现LLM智能体的行为与自然语言状态表示之间存在关键依赖关系。特别是，我们观察到，提供以下内容的表示会导致智能体的行为更接近博弈论均衡预测，并使智能体的游戏行为更加稳定：（1）对过去历史的总结，而不是完整的自然语言表示；（2）关于后悔的信息，而不是原始收益；（3）关于他人行为的有限信息。相比之下，其他表示可能会表现出与均衡的大幅偏离、随时间动态游戏行为的更大变化，或者两者兼而有之。|
|**2025-06-18**|**RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments**|Yuchuan Fu et.al.|[2506.15253](http://arxiv.org/abs/2506.15253)|**[link](https://github.com/lanzer-tree/ras-eval)**|**为了应对在医疗、金融等关键领域快速部署大型语言模型（LLM）代理的需求，迫切需要建立稳健的安全框架。鉴于动态环境中这些代理缺乏标准化的评估基准，我们提出了RAS-Eval，这是一个支持模拟和真实世界工具执行的全面安全基准。RAS-Eval包含80个测试案例和3,802个攻击任务，这些任务映射到11个常见弱点枚举（CWE）类别，工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。我们在多种场景下评估了6个最先进的LLM，揭示了重大的安全漏洞：攻击平均降低了代理任务完成率（TCR）36.78%，在学术环境中实现了85.65%的成功率。值得注意的是，安全能力遵循规模定律，大型模型优于小型模型。我们的发现揭示了现实世界代理部署中的关键风险，并为未来的安全研究提供了基础框架。代码和数据可在https://github.com/lanzer-tree/RAS-Eval获取。**|
|**2025-06-18**|**LLM Agent for Hyper-Parameter Optimization**|Wanzhe Wang et.al.|[2506.15167](http://arxiv.org/abs/2506.15167)|null|超参数对于通信算法的性能至关重要。然而，目前针对具有交叉和变异（WS-PSO-CM）算法的无人机（UAV）轨迹和通信的暖启动粒子群优化（WS-PSO-CM）算法的超参数调整方法主要是基于启发式的，自动化程度低，性能不令人满意。在本文中，我们设计了一个大型语言模型（LLM）代理用于自动超参数调整，其中应用了迭代框架和模型上下文协议（MCP）。具体来说，LLM代理首先通过配置文件设置，该配置文件指定了任务、背景和输出格式。然后，LLM代理由提示需求驱动，并迭代调用WS-PSO-CM算法进行探索。最后，LLM代理自主终止循环并返回一组超参数。我们的实验结果表明，通过我们的LLM代理生成的超参数实现的最低总速率显著高于人类启发式方法和随机生成方法。这表明，具有PSO知识和WS-PSO-CM算法背景的LLM代理在寻找高性能超参数方面是有用的。|
|**2025-06-18**|**From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents**|Mohammad Amaan Sayeed et.al.|[2506.15911](http://arxiv.org/abs/2506.15911)|null|几个世纪的伊斯兰医学文献，如阿维森纳的《医学大典》和先知医学《Tibb-e-Nabawi》，蕴含着丰富的预防保健、营养和整体治疗方法，但许多人对它们难以接触，而且在现代人工智能系统中未被充分利用。现有的语言模型基准主要关注事实回忆或用户偏好，导致在验证基于文化的医疗指导方面存在差距。我们提出了一种统一的评估流程，称为Tibbe-AG，该流程将30个精心挑选的先知医学问题与人类验证的疗法相匹配，并比较了三种大型语言模型（LLaMA-3、Mistral-7B、Qwen2-7B）在三种配置下的表现：直接生成、检索增强生成和科学自我批判过滤器。然后，每个答案都由一个充当代理判断者的次要LLM进行评估，从而得到一个单一的3C3H质量评分。检索将事实准确性提高了13%，而代理提示通过更深入的机制洞察和安全考虑又增加了10%的提升。我们的结果表明，将古典伊斯兰文献与检索和自我评估相结合，可以实现对医疗问题的可靠、文化敏感的问答。|
|**2025-06-17**|**Unified Software Engineering agent as AI Software Engineer**|Leonhard Applis et.al.|[2506.14683](http://arxiv.org/abs/2506.14683)|null|大型语言模型（LLM）技术的增长提高了对自动化编码的期望。然而，软件工程不仅仅是编码，它还涉及到包括项目维护和演化的活动。在这种背景下，利用LLM作为推理引擎以自主调用外部工具的LLM代理概念获得了关注。但LLM代理是否等同于人工智能软件工程师？在本文中，我们通过开发一个统一的软件工程代理或USEagent来试图理解这个问题。与为特定软件任务（如测试、调试和修复）构建专用代理的现有工作不同，我们的目标是构建一个统一的代理，它可以协调和应对多种能力。这使得代理能够处理软件开发中的复杂场景，例如修复不完整的补丁、添加新功能或接管他人编写的代码。我们将USEagent视为未来人工智能软件工程师的第一个草案，它可以成为未来涉及人工智能和人类的软件开发团队的团队成员。为了评估USEagent的有效性，我们构建了一个包含编码、测试和修补等多种任务的统一软件工程平台（USEbench）。USEbench是来自现有基准（如SWE-bench、SWT-bench和REPOCOD）的任务的明智混合。在包含1,271个仓库级软件工程任务的USEbench评估中，USEagent比现有的通用代理（如OpenHands CodeActAgent）显示出更高的有效性。USEagent在处理某些编码任务方面的能力存在差距，这为未来人工智能软件工程师的进一步发展提供了线索。|
|**2025-06-17**|**Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack**|Daewon Kang et.al.|[2506.14539](http://arxiv.org/abs/2506.14539)|null|随着大型语言模型的兴起，提示工程现在能够快速、低成本地创建多样化的自主代理，这些代理已经被广泛使用。然而，这种便利性引发了关于潜在提示的安全性、鲁棒性和行为一致性的紧迫担忧，以及防止这些提示暴露给用户尝试的严峻挑战。在本文中，我们提出了“双胞胎方法”来展示代理被劫持的风险，从而暴露系统指令和内部信息。接下来，我们定义了“对抗迁移下的提示对齐崩溃（PACAT）”级别来评估对这种对抗性迁移攻击的易损性。我们还提出了一个“对抗迁移注意（CAT）”提示来对抗双胞胎方法。实验结果表明，双胞胎方法可以破坏代理的一致性并暴露其内部信息。相比之下，CAT提示能够有效地防御这种对抗攻击。|
|**2025-06-17**|**SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks**|Zijian Song et.al.|[2506.14512](http://arxiv.org/abs/2506.14512)|null|大型语言模型（LLMs）在复杂推理方面取得了快速进步，展现出在数学和编程中的出色泛化能力。相比之下，虽然空间智能对于现实交互中的视觉-语言模型（VLMs）至关重要，但对其在空间环境中复杂推理能力的系统评估仍鲜有研究。为了填补这一空白，我们引入了SIRI-Bench，这是一个旨在通过基于视频的推理任务评估VLMs空间智能的基准。SIRI-Bench包含近1000个视频-问题-答案三元组，其中每个问题都嵌入在一个逼真的3D场景中并由视频捕捉。通过精心设计问题和相应的3D场景，我们的基准确保了解决问题需要既要有空间理解能力来提取信息，又要有高级推理能力来得出解决方案，使其成为评估VLMs的具有挑战性的基准。为了促进大规模数据合成，我们开发了一个自动场景创建引擎。这个引擎利用多个专业的LLM代理，可以从抽象的数学问题生成逼真的3D场景，确保与原始描述的一致性。实验结果表明，最先进的VLMs在SIRI-Bench上的表现显著不佳，突显了空间推理的挑战。我们希望我们的研究能够引起研究人员对基于空间推理的注意，并推动VLMs在视觉问题解决方面的进步。|
|**2025-06-17**|**AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents**|Jingxu Xie et.al.|[2506.14205](http://arxiv.org/abs/2506.14205)|**[link](https://github.com/sunblaze-ucb/agentsynth)**|我们介绍了AgentSynth，这是一个可扩展且成本效益高的管道，用于自动合成高质量的任务和轨迹数据集，适用于通用计算机使用代理。利用信息不对称，AgentSynth构建了在生成时简单但在组成长期任务时具有显著挑战性的子任务，从而能够创建超过6000个多样化和现实的任务。我们的管道从基于LLM的任务提议者开始，该提议者由一个角色引导，然后是一个执行代理，该代理完成任务并记录轨迹。这个过程迭代重复，形成一个子任务的序列，然后由另一个代理将这些子任务总结成一个具有可控难度的复合任务。AgentSynth的关键优势在于其能够通过调整子任务的数量来精确调节任务复杂性。实证评估表明，最先进的LLM代理在难度级别1时成功率达到18%，但在级别6时仅为4%，突显了基准的难度和区分能力。此外，我们的管道实现了每条轨迹平均成本为0.60美元，比人工标注便宜几个数量级。我们的代码和数据在https://github.com/sunblaze-ucb/AgentSynth上公开可用。|
|**2025-06-17**|**Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching**|Qizheng Zhang et.al.|[2506.14852](http://arxiv.org/abs/2506.14852)|null|基于LLM的智能应用程序在复杂工作流程中展现出越来越显著的性能，但由于广泛的规划和推理需求，这也带来了巨大的成本。现有的LLM缓存技术（如上下文缓存和语义缓存），主要是为服务聊天机器人而设计的，对于输出依赖于外部数据或环境上下文的智能应用程序来说是不够的。我们提出了智能计划缓存，这是一种新颖的方法，它从智能应用程序规划阶段的语义相似任务中提取、存储、适应和重用结构化计划模板，以降低服务成本。与传统语义缓存不同，我们的系统在测试时从完成的智能体执行中提取计划模板，采用关键词提取来匹配新请求与缓存计划，并利用轻量级模型将这些模板适应到具有上下文的具体任务计划中。在多个真实世界智能应用程序上的评估表明，我们的系统平均可以降低46.62%的成本，同时保持性能，为基于LLM的智能体服务提供了一种更高效的解决方案，并补充了现有的LLM服务基础设施。|
|**2025-06-17**|**SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents**|Jonathan Kutasov et.al.|[2506.15740](http://arxiv.org/abs/2506.15740)|null|随着大型语言模型（LLMs）越来越多地作为自主代理在复杂和长期目标环境中部署，评估它们通过追求隐藏目标来破坏用户的能力变得至关重要。我们研究了前沿LLMs在完成一系列现实任务的同时逃避监控并实现有害的隐藏目标的能力。我们使用SHADE（微妙有害代理检测与评估）-Arena评估了一系列前沿LLMs，这是第一个用于评估LLM代理破坏和监控能力的多样化代理评估数据集。SHADE-Arena包含复杂的环境中的良性和有害目标的复杂对。评估代理在不引起LLM监控者怀疑的情况下完成辅助任务的能力。当我们衡量代理完成以下任务的能力时：（a）完成主要任务，（b）完成辅助任务，以及（c）避免被检测，我们发现最佳表现的前沿模型在Claude 3.6 Sonnet的监控下，作为破坏代理时，得分分别为27%（Claude 3.7 Sonnet）和15%（Gemini 2.5 Pro）。对于当前的前沿模型，成功完成辅助任务很大程度上依赖于访问一个对监控者不可见的隐藏记事本。我们还使用SHADE-Arena来衡量模型的监控能力，表现最佳的监控者（Gemini 2.5 Pro）在区分良性和恶意转录本时实现了AUC 0.87。我们发现，目前模型在破坏方面仍然存在困难，这是因为长期上下文主要任务执行失败。然而，我们的测量已经显示了监控微妙破坏尝试的困难性，我们预计在面对更复杂和更长期的任务时，这种困难只会增加。|
|**2025-06-16**|**We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems**|Junfeng Fang et.al.|[2506.13666](http://arxiv.org/abs/2506.13666)|**[link](https://github.com/littlelittlenine/safemcp)**|**大型语言模型（LLMs）的发展已进入以经验驱动的新时代，这一时代以通过强化学习和使用工具的智能体进行环境反馈驱动学习为标志。这促使模型上下文协议（MCP）的出现，它定义了LLM如何与外部服务（如API和数据）交互的标准。然而，随着MCP成为LLM智能体系统的实际标准，它也引入了新的安全风险。特别是，MCP引入了第三方服务，这些服务不受LLM开发者的控制。这些第三方MCP服务提供商可能具有恶意意图，并有经济动力去利用漏洞并破坏用户智能体交互。在这篇立场论文中，我们呼吁LLM安全的研究界密切关注MCP引入的新安全风险，并开发新技术来构建安全的MCP智能体系统。为了确立我们的立场，我们提出了三个关键部分。（1）我们首先构建了\框架，一个用于检查MCP智能体系统安全问题的受控框架。（2）然后我们进行了一系列试点实验，以证明MCP智能体系统中的安全风险是一个真实威胁，其防御并非易事。（3）最后，我们通过展示构建安全MCP智能体系统的路线图来展望未来。特别是，我们呼吁研究人员追求以下研究方向：红队测试、MCP安全LLM开发、MCP安全评估、MCP安全数据积累、MCP服务保护以及MCP安全生态系统建设。我们希望这篇立场论文能够提高研究界对MCP安全的认识，并鼓励更多研究人员加入这一重要研究方向。我们的代码可在https://github.com/littlelittlenine/SafeMCP.git上找到。**|
|**2025-06-16**|**Towards Pervasive Distributed Agentic Generative AI -- A State of The Art**|Gianni Molinari et.al.|[2506.13324](http://arxiv.org/abs/2506.13324)|null|智能代理和大型语言模型（LLMs）的快速进步正在重塑普适计算领域。它们通过自然语言理解感知、推理和行动的能力，使得在复杂的普适环境中实现自主问题解决成为可能，包括管理异构传感器、设备和数据。本综述概述了LLM代理的架构组件（配置文件、内存、规划和行动）并考察了它们在各种场景下的部署和评估。接着，它回顾了普适计算中的计算和基础设施进步（从云端到边缘）以及AI在该领域的进展。它突出了最先进的代理部署策略和应用，包括在资源受限设备上的本地和分布式执行。本综述确定了这些代理在普适计算中的关键挑战，如架构、能源和隐私限制。最后，它提出了我们称之为“代理作为工具”的概念框架，这是一个普适代理人工智能的框架，强调上下文感知、模块化、安全性、效率和有效性。|
|**2025-06-16**|**Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches**|Lukasz Mazur et.al.|[2506.13171](http://arxiv.org/abs/2506.13171)|null|大型语言模型（LLMs）为通过自然语言与复杂的软件工件（如软件模型）进行交互提供了新的机遇。它们对于难以全面理解的庞大软件模型特别具有前景，使得传统的交互和分析方法变得具有挑战性。本文研究了两种利用LLMs来回答软件模型问题的方法：直接提示，即将整个软件模型在上下文中提供，以及一种结合了基于LLM的代理和通用文件访问工具的代理方法。我们使用为汽车和嵌入式领域的时序分析和软件优化设计的Ecore元模型来评估这些方法。我们的发现表明，虽然代理方法在准确性上与直接提示相当，但在令牌使用效率方面显著更高。这种效率使代理方法特别适合汽车行业，因为软件模型的大型尺寸使得直接提示不可行，确立了LLM代理不仅是实用替代方案，而且是唯一可行的解决方案。值得注意的是，该评估使用的是小型LLMs，这些LLMs更易于在本地执行——这对于满足关于隐私、知识产权保护和法规遵守的严格要求是一个基本优势。未来的工作将研究不同格式的软件模型，探索更复杂的代理架构，并将代理工作流程扩展到不仅支持查询，还支持修改软件模型。|
|**2025-06-16**|**Leveraging In-Context Learning for Language Model Agents**|Shivanshu Gupta et.al.|[2506.13109](http://arxiv.org/abs/2506.13109)|null|在上下文学习（ICL）中，结合动态选择的演示，将提示大型语言模型（LLM）的灵活性以及利用训练数据提高性能的能力结合起来。虽然ICL在预测和生成任务中取得了高度成功，但将其应用于需要顺序决策的代理任务具有挑战性——不仅要考虑如何大规模标注长轨迹以及如何选择演示，还要考虑什么构成演示，以及何时何地展示它们。为了解决这个问题，我们首先提出了一种算法，该算法利用具有重试功能的LLM以及演示，自动且高效地为代理任务标注解决方案轨迹。然后我们展示，将类似任务的轨迹作为演示进行集选择，可以显著提高LLM代理的性能、可靠性、鲁棒性和效率。然而，轨迹演示存在较大的推理成本。我们发现，通过在每一步使用小的轨迹片段而不是额外的轨迹，可以减轻这种成本。我们发现，从较大的模型（在标注阶段）获得的演示也能提高较小的模型，而且ICL代理甚至可以与成本更高的训练代理相媲美。因此，我们的结果表明，在谨慎使用的情况下，ICL对于代理任务也非常强大。|
|**2025-06-16**|**Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems**|Zhongzhi Yu et.al.|[2506.13905](http://arxiv.org/abs/2506.13905)|null|尽管最近在利用大型语言模型（LLM）生成硬件寄存器传输级（RTL）代码方面取得了进展，但现有解决方案仍然存在实际应用场景与真实世界RTL代码开发需求之间的较大差距。先前的方法要么过于简化硬件描述，要么依赖于大量的人工指导来处理复杂的规范，这限制了它们的可扩展性和自动化潜力。在本文中，我们通过提出一个名为Spec2RTL-Agent的LLM代理系统来填补这一差距，该系统旨在直接处理复杂的规范文档并生成相应的RTL代码实现，推动基于LLM的RTL代码生成向更现实的应用环境发展。为了实现这一目标，Spec2RTL-Agent引入了一个新颖的多代理协作框架，集成了三个关键推动因素：（1）一个推理和理解模块，将规范转换为结构化、逐步的实施计划；（2）一个渐进式编码和提示优化模块，通过迭代优化代码在多个表示形式中，以提高RTL转换的正确性和可综合性；（3）一个自适应反思模块，在生成过程中识别和追踪错误来源，确保更稳健的代码生成流程。我们的系统不是直接从自然语言生成RTL代码，而是战略性地生成可综合的C++代码，然后对其进行HLS优化。这种代理驱动的优化确保了与直接RTL生成方法相比，具有更高的正确性和兼容性。我们在三个规范文档上评估了Spec2RTL-Agent，结果显示它生成的RTL代码准确率高达，比现有方法减少75%的人工干预。这突显了它在从非结构化规范生成RTL代码方面作为第一个完全自动化的多代理系统的角色，减少了硬件设计中对人力的依赖。|
|**2025-06-15**|**Scaling Test-time Compute for LLM Agents**|King Zhu et.al.|[2506.12928](http://arxiv.org/abs/2506.12928)|null|对测试时计算规模的扩展测试在提升大型语言模型（LLMs）推理能力方面取得了显著成功。在本工作中，我们首次系统地探索了将测试时扩展方法应用于语言代理，并研究了这种方法在提高其有效性方面的程度。具体而言，我们探索了不同的测试时扩展策略，包括：（1）并行采样算法；（2）顺序修订策略；（3）验证器和合并方法；（4）多样化展开策略。我们仔细分析了不同设计策略对在语言代理中应用测试时扩展的影响，并得出以下结论：1. 扩展测试时计算规模可以提高代理的性能。2. 知道何时进行反思对代理来说很重要。3. 在不同的验证和结果合并方法中，列表方法表现最佳。4. 增加多样化的展开对代理的任务性能有积极影响。|
|**2025-06-15**|**Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents**|LeCheng Zhang et.al.|[2506.12801](http://arxiv.org/abs/2506.12801)|null|《达芬奇密码》是一款逻辑推理与信息不完整的游戏，它对人工智能提出了独特的挑战，要求其进行超越简单模式识别的细微推理。本文调查了各种人工智能范例在此游戏中的有效性。我们开发了并评估了三种不同的智能体架构：一个基于Transformer的基线模型，具有有限的历史背景；几个由结构化提示引导的大型语言模型（LLM）智能体（包括Gemini、DeepSeek和GPT变体）；以及一个基于近端策略优化（PPO）的智能体，使用Transformer编码器进行全面的游戏历史处理。性能与基线进行了基准测试，基于PPO的智能体显示出优越的胜率（58.5% ± 1.0%），显著优于LLM对手。我们的分析突出了深度强化学习在复杂推理任务政策优化中的优势，尤其是在从自我游戏中学习隐含策略方面。我们还考察了当前LLM在保持复杂游戏中的严格逻辑一致性和战略深度方面的能力和固有局限，尽管有复杂的提示。本研究有助于更广泛地理解涉及隐藏信息和多步逻辑推理的休闲游戏中的AI，为有效的智能体设计和不同人工智能方法的比较优势提供了见解。|
|**2025-06-15**|**SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation**|Yashothara Shanmugarasa et.al.|[2506.12699](http://arxiv.org/abs/2506.12699)|null|大型语言模型（LLMs）是一种复杂的人工智能系统，能够以惊人的精确度生成类似人类的文本。虽然LLMs带来了显著的技术进步，但它们使用从网络上抓取的大量用户数据和从广泛用户交互中收集的数据进行开发，存在敏感信息泄露的风险。大多数现有调查侧重于训练数据的隐私影响，但往往忽略了用户交互和高级LLM功能带来的隐私风险。本文旨在填补这一空白，通过对LLMs中的隐私进行全面分析，将挑战分为四个主要领域：（i）LLM训练数据中的隐私问题，（ii）与用户提示相关的隐私挑战，（iii）LLM生成输出中的隐私漏洞，（iv）涉及LLM代理的隐私挑战。我们评估了现有针对这些提出的隐私挑战的缓解机制的有效性和局限性，并确定了进一步研究的领域。|
|**2025-06-14**|**IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment**|Dekun Wu et.al.|[2506.12331](http://arxiv.org/abs/2506.12331)|null|虚拟环境对人工智能代理研究至关重要。现有的针对大型语言模型（LLM）代理的研究环境通常侧重于物理任务解决或社会模拟，前者过度简化了代理个性和社会动态，而后者缺乏社会行为的物理基础。我们介绍了IndoorWorld，一个异构多代理环境，它紧密整合了物理和社会动态。通过引入针对LLM驱动代理的新挑战，即在协调社会动态以影响物理环境以及在世界状态中锚定社会互动，IndoorWorld为基于LLM的建筑使用者模拟开辟了可能性。我们通过一系列在办公环境中的实验来展示其潜力，以检验多代理协作、资源竞争和空间布局对代理行为的影响。|
|**2025-06-13**|**Revealing Political Bias in LLMs through Structured Multi-Agent Debate**|Aishwarya Bandaru et.al.|[2506.11825](http://arxiv.org/abs/2506.11825)|**[link](https://github.com/comp0087-echo-chamber/comp0087-agent-debate)**|大型语言模型（LLMs）越来越多地被用于模拟社会行为，但它们的政治偏见和在辩论中的互动动态仍然研究不足。我们通过一个结构化的多智能体辩论框架，调查LLM类型和智能体性别属性如何影响政治偏见，通过让中立的、共和党和民主党的美国LLM智能体在政治敏感话题上进行辩论。我们系统地改变底层LLMs、智能体性别和辩论格式，以检验模型来源和智能体角色如何影响辩论过程中的政治偏见和态度。我们发现，中立智能体始终与民主党保持一致，而共和党则更接近中立；性别影响智能体态度，当智能体意识到其他智能体的性别时，它们会调整自己的观点；与先前研究相反，具有相同政治归属的智能体可以形成回音室，随着辩论的进行，表现出预期态度的加剧。|
|**2025-06-13**|**SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks**|Hwiwon Lee et.al.|[2506.11791](http://arxiv.org/abs/2506.11791)|**[link](https://github.com/sec-bench/sec-bench)**|为了在软件开发生命周期中建立对大型语言模型（LLM）代理安全部署的信任，对其进行严格的安全评估至关重要。然而，现有的基准测试主要依赖于合成挑战或简化的漏洞数据集，这些数据集无法捕捉到安全工程师在实际工作中遇到的复杂性和模糊性。我们引入了SEC-bench，这是第一个用于评估LLM代理在真实安全工程任务上的全自动基准测试框架。SEC-bench采用了一种新颖的多代理框架，能够自动构建带有钩子的代码库，在隔离环境中重现漏洞，并生成用于可靠评估的黄金补丁。我们的框架以每实例仅0.87美元的成本自动创建高质量的软件漏洞数据集，并具有可复制的工件。使用SEC-bench，我们实现了两个关键的软件安全任务，以严格评估LLM代理的能力：概念验证（PoC）生成和漏洞修补。对最先进的LLM代码代理的全面评估揭示了显著的性能差距，在我们的完整数据集中，概念验证生成的成功率最高为18.0%，漏洞修补的成功率为34.0%。这些结果突出了开发更实用、智能和自主的LLM代理以进行安全工程所需的关键步骤。|
|**2025-06-13**|**AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments**|Zikang Leng et.al.|[2506.11773](http://arxiv.org/abs/2506.11773)|null|在开发稳健且具有普遍性的基于智能家居的人体活动识别（HAR）系统时，一个主要障碍是缺乏大规模、多样化的标记数据集。家庭布局、传感器配置和用户行为的差异增加了复杂性，因为个人遵循不同的日常习惯并以不同的方式执行活动。构建具有良好泛化能力的HAR系统需要能够捕捉用户和环境中多样性的训练数据。为了解决这些挑战，我们引入了AgentSense，这是一个虚拟数据生成管道，通过利用大型语言模型生成多样化的角色。这些角色被用来创建日常惯例，然后分解成低级动作序列。随后，这些动作在扩展了虚拟环境传感器的模拟家庭环境VirtualHome中执行，这些虚拟环境传感器能够记录代理的活动。总的来说，AgentSense能够生成丰富、虚拟的传感器数据集，代表了广泛的用户和家庭设置。在五个基准HAR数据集上，我们表明利用我们的虚拟传感器数据显著提高了性能，尤其是在真实数据有限的情况下。值得注意的是，在虚拟数据和仅几天真实数据组合上训练的模型，其性能与在完整真实数据集上训练的模型相当。这些结果证明了虚拟数据在解决环境感知中最紧迫的挑战之一的潜力，即缺乏大规模、标注的数据集，而不需要任何手动数据收集工作。|
|**2025-06-13**|**The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs**|Avinash Baidya et.al.|[2506.12266](http://arxiv.org/abs/2506.12266)|null|基于大型语言模型（LLM）的智能体对面向任务的对话系统（TODS）产生了重大影响，但仍然面临着显著的性能挑战，尤其是在零样本场景中。虽然先前的研究已经注意到了这种性能差距，但驱动这种性能差距的行为因素仍然没有得到充分研究。本研究提出了一种全面的评估框架，用于量化AI智能体和人类专家之间的行为差距，重点关注对话行为、工具使用和知识利用方面的差异。我们的发现表明，这种行为差距是影响LLM智能体性能的关键因素。值得注意的是，随着任务复杂性的增加，行为差距会扩大（相关系数：0.963），导致智能体在复杂任务导向对话中的性能下降。在我们研究的最复杂任务中，即使是基于GPT-4o的智能体也表现出与人类行为的不匹配，对话行为的F1分数低至0.464，工具使用过多且常常不匹配，F1分数为0.139，以及外部知识的使用效果不佳。减少这种行为差距将导致显著的性能提升（平均提升24.3%）。这项研究强调了全面的行为评估和改进对齐策略对于提高基于LLM的TODS处理复杂任务有效性的重要性。|
|**2025-06-13**|**A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions**|Stephen Mell et.al.|[2506.12202](http://arxiv.org/abs/2506.12202)|null|现代大型语言模型（LLMs）通常作为代理部署，通过自适应地调用外部工具来解决任务。而不是直接调用工具，LLMs编写代码来执行工具调用可能更有效，使它们能够自动生成复杂的控制流，如条件和循环。这些代码操作通常以Python代码的形式提供，因为LLMs在Python方面非常熟练；然而，由于Python在性能、安全和可靠性方面的内置支持有限，它可能不是最佳语言。我们提出了一种用于代码操作的全新编程语言，称为Quasar，它具有以下优点：（1）自动并行化以提高性能，（2）不确定性量化以提高可靠性和减轻幻觉，（3）安全特性使用户能够验证操作。LLMs可以在Python的一个子集上编写代码，这些代码将自动转换为Quasar。我们在ViperGPT视觉问答代理上评估了我们的方法，应用于GQA数据集，结果表明，与使用Python操作的LLMs相比，使用Quasar操作的LLMs保持了强大的性能，同时在可能的情况下将执行时间减少了42%，通过减少用户批准交互在可能的情况下提高了52%的安全性，并通过应用一致性预测来实现所需的目标覆盖率，从而提高了可靠性。|
|**2025-06-12**|**AutoMind: Adaptive Knowledgeable Agent for Automated Data Science**|Yixin Ou et.al.|[2506.10974](http://arxiv.org/abs/2506.10974)|**[link](https://github.com/innovatingai/automind)**|大型语言模型（LLM）代理在解决现实世界数据科学问题方面展现出巨大潜力。由LLM驱动的数据科学代理有望自动化整个机器学习流程，但它们在现实世界中的有效性仍然有限。现有的框架依赖于僵化、预定义的工作流程和缺乏灵活性的编码策略；因此，它们仅在相对简单、经典问题上表现出色，无法捕捉到人类实践者在复杂、创新任务中带来的经验专业知识。在这项工作中，我们介绍了AutoMind，一个自适应、知识渊博的LLM代理框架，它通过三个关键进步克服了这些不足：（1）一个精心编写的专家知识库，将代理建立在领域专家知识之上，（2）一个代理知识树搜索算法，战略性地探索可能的解决方案，（3）一个自适应编码策略，根据任务复杂度动态调整代码生成。在两个自动化数据科学基准上的评估表明，AutoMind与最先进的基线相比，表现出卓越的性能。额外的分析证实了其良好的有效性、效率和定性解决方案质量，突显了AutoMind是向完全自动化数据科学迈进的一个高效且稳健的步骤。|
|**2025-06-12**|**OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems**|Xiaozhe Li et.al.|[2506.10764](http://arxiv.org/abs/2506.10764)|**[link](https://github.com/oliverleexz/opt-bench)**|**大型语言模型（LLMs）在解决各种任务方面展现出非凡的能力。然而，它们通过学习先前反馈来迭代优化复杂解决方案的熟练程度尚未得到充分探索。为了填补这一空白，我们提出了OPT-BENCH，这是一个综合性的基准，旨在评估LLM代理在大型搜索空间优化问题上的表现。OPT-BENCH包括来自Kaggle的20个真实世界机器学习任务和10个经典的NP问题，为评估LLM代理在迭代推理和解决方案细化方面的能力提供了一个多样化和具有挑战性的环境。为了实现严格的评估，我们引入了OPT-Agent，这是一个端到端的优化框架，在解决复杂问题时模仿人类推理，通过生成、验证和利用历史反馈迭代改进解决方案。通过对6个模型家族中的9个最先进的LLMs进行广泛的实验，我们分析了优化迭代、温度设置和模型架构对解决方案质量和收敛性的影响。我们的结果表明，将历史上下文纳入其中显著提高了ML和NP任务中的优化性能。所有数据集、代码和评估工具均已开源，以促进进一步研究，推动LLM驱动的优化和迭代推理的进步。项目页面：[https://github.com/OliverLeeXZ/OPT-BENCH](https://github.com/OliverLeeXZ/OPT-BENCH)。**|
|**2025-06-12**|**Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications**|Felix Härer et.al.|[2506.10467](http://arxiv.org/abs/2506.10467)|**[link](https://github.com/fhaer/multi-agent-llm-system)**|**近期在大型语言模型（LLMs）方面的进展表明了其在新型应用中的潜力，例如通过最新OpenAI和DeepSeek模型中的推理能力。为了将这些模型应用于文本生成以外的特定领域，可以利用基于LLM的多智能体方法，通过结合推理技术、代码生成和软件执行来解决复杂任务。应用可能利用这些能力和专门化LLM智能体的知识。然而，尽管对LLMs、推理技术和应用进行了许多单独的评估，但它们联合规范和综合应用的研究尚不充分。需要为多智能体LLM系统定义规范，以探索其潜力及其适用于特定应用的适宜性，从而对LLMs、推理技术和相关方面进行系统评估。本文报告了通过多智能体系统对这些方面进行规范和评估的探索性研究结果。系统架构和原型基于先前研究进行扩展，并引入了多智能体系统的规范。涉及网络安全任务的测试用例表明了架构和评估方法的可行性。特别是，结果显示了由OpenAI和DeepSeek的LLM智能体正确完成的问答、服务器安全和网络安全任务的评估。**|
|**2025-06-12**|**Provably Learning from Language Feedback**|Wanqiao Xu et.al.|[2506.10341](http://arxiv.org/abs/2506.10341)|null|从观察和语言反馈中交互式学习是一个日益受到关注的领域，这一领域的发展得益于大型语言模型（LLM）代理的出现。虽然已经展示了令人印象深刻的实证演示，但到目前为止，对这些决策问题的原理性框架仍然缺乏。在本文中，我们形式化了从语言反馈中学习（LLF）问题，提出了足以使学习在潜在奖励下得以进行的充分假设，并引入了“迁移逃避维度”作为复杂度度量，以表征LLF问题的难度。我们表明，迁移逃避维度捕捉了这样的直觉：反馈中的信息改变了LLF问题的学习复杂度。我们证明了从丰富的语言反馈中学习可以比从奖励中学习快得多。我们开发了一个无后悔算法，称为HELiX，该算法通过序列交互可证明地解决LLF问题，其性能保证随着问题迁移逃避维度的增加而扩展。在几个经验领域内，我们展示了即使在重复提示LLM不可靠的情况下，HELiX也能表现良好。我们的贡献是朝着从通用语言反馈中设计原理性交互学习算法迈出的第一步。|
|**2025-06-12**|**Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis**|Xueying Du et.al.|[2506.10322](http://arxiv.org/abs/2506.10322)|null|静态缺陷分析器在确保软件质量方面发挥着关键作用。然而，现有的大规模代码库缺陷检测分析器往往存在高误报率的问题。这主要是由于分析器在处理包含多个条件分支和复杂数据依赖的路径可行性验证方面的能力有限。尽管当前基于LLM的方法试图解决这个问题，但由于约束级联分析不足和在大规模项目中可扩展性的挑战，其有效性仍然有限。为了应对这一挑战，我们提出了一种迭代路径可行性分析框架LLM4PFA。通过利用基于LLM的代理进行目标约束推理，以及由代理规划驱动的关键上下文感知分析，LLM4PFA有效地增强了复杂的跨程序路径可行性分析，以最大限度地减少静态缺陷检测中的误报。评估结果表明，LLM4PFA精确地过滤掉了静态缺陷检测期间报告的72%至96%的误报，比所有基线提升了41.1%至105.7%；同时，LLM4PFA仅遗漏了45个真实阳性中的3个真实缺陷。|
|**2025-06-12**|**From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review**|Yaohui Zhang et.al.|[2506.11343](http://arxiv.org/abs/2506.11343)|null|随着大型语言模型（LLMs）的出现，为在传统工作流程的限制之外重新构想同行评审提供了前所未有的机会。尽管有这些机会，以往的努力主要集中在用LLMs作为人类审稿人的直接替代品来复制传统的评审工作流程，而对探索LLMs如何参与学术评审过程的新范式关注有限。在本文中，我们介绍并探讨了一种新颖的机制，该机制使用LLM代理对稿件进行成对比较，而不是进行个体评分。通过汇总大量成对评估的结果，这种方法能够更准确地衡量稿件之间的相对质量。我们的实验表明，这种比较方法在识别高影响力论文方面显著优于传统的基于评分的方法。然而，我们的分析也揭示了选择过程中的新出现的偏差，特别是在研究主题的新颖性减少和机构间不平衡增加方面。这些发现突出了用LLMs重新思考同行评审的变革潜力以及未来系统必须解决以确保公平性和多样性的关键挑战。|
|**2025-06-11**|**Disclosure Audits for LLM Agents**|Saswat Das et.al.|[2506.10171](http://arxiv.org/abs/2506.10171)|null|大型语言模型代理已开始以个人助理、客户服务机器人和临床助手等形式出现。虽然这些应用带来了实质性的运营效益，但它们也需要持续访问敏感数据，这增加了未经授权泄露数据的可能性。本研究提出了一种针对对话隐私的审计框架，该框架对风险进行量化并对其进行审计。提出的对话操纵隐私泄露（CMPL）框架，是一种迭代探测策略，旨在对执行严格隐私指令的代理进行压力测试。CMPL并非仅关注单一泄露事件，而是模拟现实的多轮交互，系统地揭示潜在漏洞。我们对不同领域、数据模态和安全配置的评估表明，该审计框架能够揭示现有单轮防御无法阻止的隐私风险。除了将CMPL作为诊断工具外，该论文还提供了（1）基于可量化风险指标的审计程序和（2）用于评估不同代理实现对话隐私的开放基准。|
|**2025-06-11**|**A quantum semantic framework for natural language processing**|Christopher J. Agostino et.al.|[2506.10077](http://arxiv.org/abs/2506.10077)|null|语义退化是自然语言的一种基本属性，它不仅超越了简单的多义性，还涵盖了随着语义表达复杂性的增加而产生的潜在解释的组合爆炸。大型语言模型（LLMs）和其他现代自然语言处理系统之所以面临固有的局限性，正是因为它们在自然语言内部运行，这使得它们受到语义退化强加的相同解释约束。在这项工作中，我们运用柯尔莫哥洛夫复杂性理论论证，随着表达式的复杂性增长，任何解释者（无论是人类还是由LLM驱动的AI）恢复单一意图含义的可能性消失。这种计算上的不可行性表明，经典观点认为语言形式本身具有意义的看法是错误的。我们相反地提出，意义是通过观察者依赖的解释行为实现的。为了验证这一点，我们使用多样化的LLM代理作为“计算认知系统”，在多种语境设置下解释模糊的词对，进行了语义贝尔不等式测试。在几个独立的实验中，我们发现了平均CHSH期望值从1.2到2.8不等，其中几个运行产生了（例如，2.3-2.4）显著违反经典边界（,S,≤2）的值。这表明在模糊性下的语言解释可以表现出非经典的相关性，与人类认知实验的结果一致。这些结果内在地意味着，基于经典频率派的自然语言分析方法必然是有损的。相反，我们提出，贝叶斯风格的重复采样方法可以提供更实用和适当的对语境中语言意义的描述。|
|**2025-06-10**|**Design Patterns for Securing LLM Agents against Prompt Injections**|Luca Beurer-Kellner et.al.|[2506.08837](http://arxiv.org/abs/2506.08837)|null|随着由大型语言模型（LLMs）驱动的AI代理变得越来越多功能和能够处理广泛的任务，确保其安全性已经成为一个关键挑战。其中最紧迫的威胁之一是提示注入攻击，这种攻击利用了代理对自然语言输入的抵抗力——当代理被赋予工具访问权限或处理敏感信息时，这种威胁尤其危险。在这项工作中，我们提出了一套原理性的设计模式，用于构建具有可证明提示注入抵抗力的AI代理。我们系统地分析了这些模式，讨论了它们在实用性和安全性方面的权衡，并通过一系列案例研究展示了它们在现实世界中的应用。|
|**2025-06-10**|**Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents**|Irene Testini et.al.|[2506.08800](http://arxiv.org/abs/2506.08800)|null|数据科学旨在从数据中提取洞察力以支持决策过程。最近，大型语言模型（LLMs）越来越多地被用作数据科学的助手，通过提出想法、技术和小代码片段，或者用于结果的解释和报告。随着LLM代理的出现，一些数据科学活动的适当自动化现在有望实现，即由LLM驱动的AI系统，这些系统配备了额外的功能——如代码执行和知识库——能够执行自我指导的行为并与其他数字环境交互。在这篇论文中，我们回顾了用于数据科学的LLM助手的评估。我们发现：（1）主要关注一小部分以目标为导向的活动，在很大程度上忽略了数据管理和探索性活动；（2）专注于纯辅助或完全自主的代理，没有考虑人机协作的中间层次；（3）强调人类替代，因此忽略了由于任务转换而实现更高自动化水平的可能性。|
|**2025-06-10**|**Improved LLM Agents for Financial Document Question Answering**|Nelvin Tan et.al.|[2506.08726](http://arxiv.org/abs/2506.08726)|null|大型语言模型（LLMs）在众多自然语言处理任务上展现出令人印象深刻的性能。然而，LLMs在处理包含表格和文本数据的金融文档的数值问答任务上仍然存在困难。近期的研究表明，在有Oracle标签的情况下，批评代理（即自我纠正）对于这个任务非常有效。在此基础上，本文探讨了在没有Oracle标签时传统批评代理的有效性，并通过实验表明，在这种情况下，该批评代理的性能会下降。考虑到这一点，我们提出了一种改进的批评代理，以及计算代理，它优于之前的最先进方法（思想程序）且更安全。此外，我们还研究了我们的代理如何相互作用，以及这种相互作用如何影响它们的性能。|
|**2025-06-10**|**Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search**|Samuel Holt et.al.|[2506.09171](http://arxiv.org/abs/2506.09171)|null|大型语言模型（LLMs）的能效日益提高，但通常需要在复杂、交互式环境中进行大量指导或广泛的交互历史来有效地执行。现有方法可能难以适应新信息或在没有微调的情况下，高效利用过去的经验进行多步推理。我们引入了一种新颖的LLM代理框架，通过上下文学习增强规划能力，这得益于原子事实增强和递归前瞻搜索。我们的代理学习从其交互轨迹中提取任务关键性的“原子事实”。这些事实动态地增强了提供给基于LLM的组件的提示，这些组件负责动作建议、潜在世界模型模拟和状态价值估计。规划通过深度有限的预搜索执行，其中LLM模拟潜在轨迹并评估其结果，受到积累的事实和交互历史的指导。这种方法允许代理在在线提高其理解和决策能力，利用其经验来改进其行为，而不需要权重更新。我们提供了一个理论动机，将性能与基于事实的抽象质量和LLM模拟精度联系起来。实证上，我们的代理在具有挑战性的交互任务上表现出改进的性能和适应性，随着经验的积累，实现了更优的行为，如在TextFrozenLake和ALFWorld等任务中展示。|
|**2025-06-09**|**QUITE: A Query Rewrite System Beyond Rules with LLM Agents**|Yuyang Song et.al.|[2506.07675](http://arxiv.org/abs/2506.07675)|null|查询重写将SQL查询转换为语义等效但运行更高效的格式。现有方法主要依赖于预定义的重写规则，但它们只能处理查询的一个有限子集，并可能导致性能下降。这种限制源于基于规则查询重写的三个挑战：（1）难以发现和验证新规则，（2）固定的重写规则不能推广到新的查询模式，（3）某些重写技术无法表示为固定规则。鉴于人类专家表现出显著更好的重写能力，但受可扩展性限制，而大型语言模型（LLMs）已展现出接近人类水平的语义和推理能力，我们提出了一种使用LLMs进行规则之外SQL查询重写的新方法。由于LLMs中的幻觉问题，直接应用LLMs往往导致不等效和次优查询。为了解决这个问题，我们提出了QUITE（查询重写），这是一个基于LLM代理的无需训练和具有反馈意识的系统，可以将SQL查询重写为语义等效的形式，与基于规则的方法相比，具有显著更好的性能，覆盖了更广泛的查询模式和重写策略。首先，我们设计了一个由有限状态机（FSM）控制的多代理框架，以使LLMs能够使用外部工具，并通过实时数据库反馈增强重写过程。其次，我们开发了一个重写中间件，以增强LLMs生成优化查询等价物的能力。最后，我们采用了一种新颖的提示注入技术，以改进重写查询的执行计划。大量实验表明，QUITE将查询执行时间减少了高达35.8%，比最先进的方法减少了24.1%的重写数量，涵盖了早期系统未处理的查询案例。|
|**2025-06-09**|**IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents**|Shiwei Feng et.al.|[2506.07524](http://arxiv.org/abs/2506.07524)|null|LLM代理越来越多地被部署以通过自然语言指令调用API来自动化现实世界任务。虽然它们功能强大，但常常因为对用户意图的误解而导致代理的行为偏离用户的预期目标，尤其是在外部工具包不断演变的情况下。传统的软件测试假设结构化输入，因此在处理自然语言的模糊性方面存在不足。我们引入了IntenTest，这是一个以API为中心的压力测试框架，它系统地揭示了LLM代理中的意图完整性违规问题。与之前专注于固定基准或对抗性输入的工作不同，IntenTest基于工具包的文档生成现实任务，并应用有针对性的变异来揭示微妙的代理错误，同时保留用户意图。为了指导测试，我们提出了语义分区，它根据工具包API参数及其等价类将自然语言任务组织成有意义的类别。在每一个分区中，种子任务通过一个轻量级的预测器进行变异和排名，该预测器估计触发代理错误的可能性。为了提高效率，IntenTest维护一个数据类型感知的策略记忆，它从过去的案例中检索和调整有效的变异模式。在80个工具包API上的实验表明，IntenTest有效地揭示了意图完整性违规，在错误暴露率和查询效率方面显著优于基线。此外，IntenTest通过使用较小的LLM进行测试生成，很好地泛化到更强的目标模型，并且能够适应跨领域的不断演变的API。|
|**2025-06-09**|**MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models**|Philip R. Liu et.al.|[2506.07400](http://arxiv.org/abs/2506.07400)|**[link](https://github.com/purdue-m2/medchat)**|基于深度学习的青光眼检测与大型语言模型（LLMs）的整合，提供了一种自动化策略，以缓解眼科医生短缺问题并提高临床报告效率。然而，由于幻觉、可解释性有限以及领域特定的医学知识不足，将通用LLMs应用于医学影像仍然具有挑战性，这可能会降低临床准确性。尽管最近的方法将成像模型与LLM推理相结合，改善了报告质量，但它们通常依赖于单一的全能代理，限制了它们模仿多学科医疗团队中多样化和复杂推理的能力。为了解决这些局限性，我们提出了MedChat，这是一个多代理诊断框架和平台，它将专业视觉模型与多个角色特定的LLM代理相结合，所有这些都由一个导演代理协调。这种设计增强了可靠性，降低了幻觉风险，并通过针对临床审查和教育用途定制的界面实现了交互式诊断报告。代码可在https://github.com/Purdue-M2/MedChat上找到。|
|**2025-06-09**|**G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems**|Guibin Zhang et.al.|[2506.07398](http://arxiv.org/abs/2506.07398)|**[link](https://github.com/bingreeky/gmemory)**|大型语言模型（LLM）驱动的多智能体系统（MAS）已经展现出远超单个LLM代理的认知和执行能力，然而，由于内存架构尚未充分发展，它们的自我进化能力仍然受到阻碍。经过仔细检查，我们惊讶地发现，现行的MAS内存机制（1）过于简单化，完全忽视了代理之间协作轨迹的细微差别，并且（2）缺乏跨试验和针对代理的定制化，这与为单个代理开发的具有表现力的内存形成鲜明对比。为了弥合这一差距，我们引入了G-Memory，这是一种受组织记忆理论启发的分层、代理式MAS内存系统，通过一个三层图层次结构来管理长时间MAS交互：洞察、查询和交互图。在接收到新的用户查询后，G-Memory执行双向内存遍历，检索出既能够使系统利用跨试验知识的高级、一般性洞察，又能够紧凑地编码先前协作经验的精细、浓缩的交互轨迹。在任务执行过程中，整个层次结构通过吸收新的协作轨迹而进化，培养团队代理的渐进式进化。在五个基准、三个LLM主干和三个流行的MAS框架上进行的广泛实验表明，G-Memory在不修改原始框架的情况下，提高了实体动作的成功率高达20.89%，以及在知识问答中的准确性高达10.12%。我们的代码可在https://github.com/bingreeky/GMemory上获取。|
|**2025-06-09**|**Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents**|Yun Hua et.al.|[2506.07388](http://arxiv.org/abs/2506.07388)|null|大型语言模型（LLMs）在具有预定义角色和工作流程的多智能体系统中表现出强大的协作性能。然而，在缺乏协调规则的开端环境中，智能体倾向于以自我利益的方式行事。实现协调的核心挑战在于信用分配——公平评估每个智能体的贡献并设计符合他们异质目标的定价机制。这个问题至关重要，因为LLMs越来越多地参与复杂的人机协作，其中公平的补偿和问责制依赖于有效的定价机制。受人类社会解决类似协调挑战的方式（例如，通过就业或分包等临时合作）的启发，我们提出了一种合作工作流程，称为Shapley-Coop。Shapley-Coop将Shapley思维链——利用边际贡献作为定价的原则基础——与结构化谈判协议相结合，以实现有效的价格匹配，使LLM智能体能够通过合理的任务时间定价和任务后的奖励再分配进行协调。这种方法协调了智能体的激励，促进了合作，并保持了自主性。我们在两个多智能体游戏和一个软件工程模拟中评估了Shapley-Coop，结果表明，它一致地提高了LLM智能体的协作并促进了公平的信用分配。这些结果突出了Shapley-Coop定价机制在准确反映任务执行过程中的个人贡献方面的有效性。|
|**2025-06-09**|**SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents**|Subhrangshu Nandi et.al.|[2506.08119](http://arxiv.org/abs/2506.08119)|null|大型语言模型（LLMs）展现了令人印象深刻的通用推理和问题解决能力。然而，它们在执行复杂的、长周期的、需要严格遵循标准操作程序（SOPs）的工作流程方面存在困难，这是现实世界工业自动化的关键要求。尽管存在这一需求，但缺乏反映SOPs的复杂性、结构和领域特定细微差别的公共基准。为了解决这个问题，我们提出了三个主要贡献。首先，我们介绍了一种合成数据生成框架，用于创建现实、工业级SOPs，以严格测试基于LLMs的代理的计划、推理和工具使用能力。其次，使用这个框架，我们开发了SOP-Bench，这是一个包含1800多个任务、涵盖10个工业领域的基准，每个领域都有API、工具接口和经人类验证的测试用例。第三，我们在SOP-Bench上评估了两种显著的代理架构：函数调用代理和ReAct代理，观察到平均成功率分别为27%和48%。值得注意的是，当工具注册库远远超过必要大小时，代理几乎100%地调用错误工具。这些发现突显了当前LLMs的代理能力与自动化现实世界SOPs的需求之间存在着巨大差距。性能在任务和领域之间差异显著，这突显了在部署之前进行领域特定基准测试和架构选择的需求。SOP-Bench在http://sop-bench.s3-website-us-west-2.amazonaws.com/上公开发布。我们还发布了支持数据生成框架的数据背后的提示，以支持新的领域特定SOP基准。我们邀请社区将来自他们工业领域的SOP扩展到SOP-Bench中。|
|**2025-06-08**|**Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments**|Xinran Li et.al.|[2506.07232](http://arxiv.org/abs/2506.07232)|null|大型语言模型（LLMs）具有广泛的知识库和强大的推理能力，使其成为在具身环境中进行复杂多智能体规划的有望的工具。然而，尽管LLMs具备先进的能力和智能体方法的复杂模块化设计，现有的基于LLM的规划算法仍然受到对多智能体具身场景适应能力弱的限制。我们通过引入一个框架来解决这一限制，该框架使LLM智能体能够在测试前和测试过程中学习和进化，从而为更好的规划提供与环境相关的知识，并通过改进的沟通来提高合作能力。受多智能体强化学习中集中训练与分散执行灵感的启发，我们提出了一种适用于多智能体LLMs适应的“作为个体学习，作为团队进化（LIET）”范式。在个体层面，LLM智能体从探索性数据集中学习一个局部效用函数，以更好地理解具身环境，然后在测试时查询以支持有信息的决策。在团队层面，LLM智能体基于新经验协作和迭代地维护和更新一个共享的合作知识列表，并使用它来指导更有效的沟通。通过结合个体学习和团队进化，LIET使LLM智能体能够进行全面和灵活的适应。我们在通信观察与帮助和ThreeD-World多智能体运输基准上的实验表明，通过LLaMA和GPT-4o实现的LIET优于现有基线，并展现出强大的合作规划能力。|
|**2025-06-07**|**Contextual Experience Replay for Self-Improvement of Language Agents**|Yitao Liu et.al.|[2506.06698](http://arxiv.org/abs/2506.06698)|null|大语言模型（LLM）代理已被应用于如网页导航等顺序决策任务，但如果没有特定的环境经验，它们在这些复杂任务中往往失败。此外，当前的LLM代理在推理时间期间没有设计为持续从过去经验中学习，这对于它们获得这些特定环境经验可能是至关重要的。为了解决这个问题，我们提出了上下文经验回放（CER），这是一个无需训练的框架，旨在使语言代理在其上下文窗口中能够高效地自我改进。具体来说，CER将过去经验积累和综合到一个动态记忆缓冲区中。这些经验包括环境动态和常见的决策模式，使代理能够在新任务中检索并增强自身，提高其在复杂环境中的适应性。我们在具有挑战性的WebArena和VisualWebArena基准测试上评估了CER。在VisualWebArena上，CER实现了31.9%的具有竞争力的性能。在WebArena上，CER也获得了36.7%的具有竞争力的平均成功率，相对于GPT-4o代理基线，提高了51.0%的成功率。我们还对其进行了全面的分析，以证明其效率、有效性和更好地理解它。|
|**2025-06-06**|**PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time**|Weizhi Zhang et.al.|[2506.06254](http://arxiv.org/abs/2506.06254)|null|大型语言模型（LLM）赋能的代理近年来作为先进的范例出现，在众多领域和任务中展现出令人印象深刻的性能。尽管它们具有潜力，但当前的LLM代理通常采用一刀切的方法，缺乏灵活性，无法应对用户不断变化的需求和偏好。这种局限性激励我们开发了PersonaAgent，这是第一个用于解决各种个性化任务的个性化LLM代理框架。具体来说，PersonaAgent集成了两个互补的组件——一个包含情景记忆和语义记忆机制的个性化记忆模块；一个能够使代理执行针对用户定制的工具动作的个性化动作模块。在核心上，个人角色（定义为每个用户的独特系统提示）充当中介：它利用个性化记忆中的见解来控制代理动作，而这些动作的结果反过来又完善了记忆。基于此框架，我们提出了一种测试时用户偏好对齐策略，通过模拟最新n次交互来优化个人角色提示，确保通过模拟和真实响应之间的文本损失反馈进行实时用户偏好对齐。实验评估表明，PersonaAgent不仅有效地个性化了动作空间，而且在测试时现实世界应用中实现了扩展，显著优于其他基线方法。这些结果强调了我们的方法在提供定制化、动态用户体验方面的可行性和潜力。|
|**2025-06-06**|**Can Theoretical Physics Research Benefit from Language Agents?**|Sirui Lu et.al.|[2506.06214](http://arxiv.org/abs/2506.06214)|null|大型语言模型（LLMs）在各个领域快速进步，但它们在理论物理学研究中的应用尚不成熟。这篇立场论文认为，当与领域知识和工具箱恰当结合时，LLM代理有潜力加速理论、计算和应用的物理学发展。我们分析了当前LLMs在物理学领域的功能——从数学推理到代码生成——确定了物理直觉、约束满足和可靠推理中的关键差距。我们展望了未来专门用于物理学的LLMs，这些模型能够处理多模态数据，提出可检验的假设，并设计实验。实现这一愿景需要解决根本性的挑战：确保物理一致性，并开发稳健的验证方法。我们呼吁物理和AI社区之间的协作努力，以推动物理学科学发现的发展。|
|**2025-06-06**|**AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search**|Yu Li et.al.|[2506.06017](http://arxiv.org/abs/2506.06017)|null|大型语言模型（LLM）代理在多个领域展示了强大的能力。然而，设计高性能的代理系统仍然具有挑战性。现有的代理搜索方法存在三个主要局限性：（1）过度关注优化代理工作流程，而未充分利用诸如记忆、规划和工具使用等经过验证的人类设计组件；（2）评估成本高，因为每个新产生的代理都必须在基准测试中进行全面评估；（3）在大搜索空间中的搜索效率低下。在这项工作中，我们提出一个全面的框架来解决这些挑战。首先，我们提出一个层次化搜索空间，该空间同时模拟代理工作流程和可组合的功能组件，从而实现更丰富的代理系统设计。基于这种结构化设计空间，我们引入一个预测价值模型，该模型根据代理系统和任务描述估计代理性能，使得在搜索过程中能够进行高效、低成本的评价。最后，我们提出一个基于不确定性的分层蒙特卡洛树搜索（MCTS）策略来指导搜索。在七个基准测试上的实验，涵盖了具身、数学、网络、工具和游戏，表明我们的方法相较于最先进的基线平均性能提升了8.34%，并展现出更快的搜索进度和更陡峭的改进轨迹。代码仓库可在https://github.com/Ericccc02/AgentSwift找到。|
|**2025-06-06**|**CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents**|Qingbin Zeng et.al.|[2506.05981](http://arxiv.org/abs/2506.05981)|null|模拟城市犯罪是一项重要且具有挑战性的任务，需要理解城市环境中嵌入的微妙视觉、社会和文化线索。以往的研究主要集中在基于规则的基于代理的建模（ABM）和深度学习方法。ABM提供了内部机制的可解释性，但预测准确性有限。相比之下，深度学习方法在预测方面通常很有效，但可解释性较差，并且需要大量的训练数据。此外，这两条研究路线都缺乏适应变化环境的认知灵活性。利用大型语言模型（LLM）的能力，我们提出了CrimeMind，这是一种创新的LLM驱动ABM框架，用于模拟多模态城市背景下的城市犯罪。我们设计的关键创新是将常规活动理论（RAT）集成到CrimeMind的代理工作流程中，使其能够处理丰富的多模态城市特征并推理犯罪行为。然而，RAT要求LLM代理在评估监护权时推断环境安全性的微妙线索，这对于LLM来说可能具有挑战性。为了解决这个问题，我们收集了一个小规模的人工标注数据集，并通过无训练文本梯度方法将CrimeMind的感知与人类判断相一致。在美国四个主要城市进行的实验表明，CrimeMind在犯罪热点预测和空间分布准确性方面优于传统的ABM和深度学习基线，比最强的基线提高了高达24%。此外，我们还进行了外部事件和政策干预的逆事实模拟，并成功捕捉到犯罪模式预期的变化，这证明了其反映逆事实情景的能力。总的来说，CrimeMind能够实现个体行为的精细建模，并促进对现实世界干预的评估。|
|**2025-06-06**|**To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt**|Zhilong Wang et.al.|[2506.05739](http://arxiv.org/abs/2506.05739)|null|LLM代理被广泛用于客户支持、内容生成和代码辅助等领域。然而，它们容易受到提示注入攻击的威胁，其中恶意输入会操纵模型的行为。传统的防御措施，如输入净化、守卫模型和防护栏，要么繁琐要么无效。在本文中，我们提出了一种新颖的、轻量级的防御机制，称为多形态提示组装（PPA），它能够以近乎零的开销防御提示注入。该方法基于这样一个观点：提示注入需要猜测并破坏系统提示的结构。通过动态变化系统提示的结构，PPA阻止攻击者预测提示结构，从而在不影响性能的情况下提高安全性。我们进行了实验来评估PPA对现有攻击的有效性，并将其与其他防御方法进行了比较。|
|**2025-06-05**|**Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games**|Niv Eckhaus et.al.|[2506.05309](http://arxiv.org/abs/2506.05309)|**[link](https://github.com/niveck/LLMafia)**|**在同步通信中，人用户和模型交替进行交流。相比之下，许多现实场景本质上是异步的。例如，在群聊、在线团队会议或社交游戏中，没有固有的轮流概念；因此，何时发言的决策构成了参与者决策的关键部分。在本工作中，我们开发了一个自适应的异步LLM代理，它不仅决定说什么，还决定何时说。为了评估我们的代理，我们收集了一个独特的在线黑手党游戏数据集，包括人类参与者和我们的异步代理。总体而言，我们的代理在游戏表现和与其他人类玩家融合的能力方面与人类玩家相当。我们的分析表明，代理在决定何时发言的行为与人类模式相似，尽管在消息内容上存在差异。我们发布所有数据和代码，以支持并鼓励进行更真实的异步通信研究。这项工作为将LLM整合到现实人类群体场景铺平了道路，从团队讨论的帮助到必须应对复杂社会动态的教育和职业环境。**|
|**2025-06-05**|**Agentic AI for Intent-Based Industrial Automation**|Marcos Lima Romero et.al.|[2506.04980](http://arxiv.org/abs/2506.04980)|**[link](https://github.com/romerocode/talk-to-your-factory)**|**近期，由自主大型语言模型（LLMs）赋能的具有规划和工具使用能力的代理人工智能（Agentic AI）系统的发展，为工业自动化的演变提供了新的可能性，并降低了工业4.0带来的复杂性。本研究提出了一种概念框架，该框架将代理人工智能与网络研究中最初开发的基于意图的范式相结合，以简化人机交互（HMI）并使自动化系统更好地与以人为中心、可持续和有弹性的工业5.0原则相一致。基于基于意图的处理，该框架允许人类操作员以自然语言表达高级的业务或运营目标，这些目标被分解为可执行组件。这些意图被分解为期望、条件、目标、上下文和信息，这些信息引导配备了专业工具的子代理执行特定领域的任务。使用CMAPSS数据集和谷歌代理开发者工具包（ADK）实现了概念验证，证明了在预测性维护场景中意图分解、代理编排和自主决策的可行性。结果表明，尽管存在数据质量和可解释性方面的担忧，但这种方法在降低技术障碍和实现可扩展、基于意图的自动化方面具有潜力。**|
|**2025-06-05**|**E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction**|Zhichao Yang et.al.|[2506.04654](http://arxiv.org/abs/2506.04654)|null|电动自行车（e-bike）的使用正在迅速增加，由于事故报告数量的上升，引发了安全问题。然而，e-bike事故报告通常采用非结构化的叙事格式，这阻碍了定量安全分析。本研究引入了E-bike agents框架，该框架利用大型语言模型（LLM）驱动的智能体对非结构化的事故报告进行分类和提取安全变量。我们的框架由四个LLM智能体组成，分别负责数据分类、信息提取、伤害原因确定和组件关联，以提取可能导致e-bike事故的关键因素及其导致不同严重程度的原因。此外，我们使用有序logit模型来研究事故严重程度与检索到的因素（如性别、事故原因类型和环境条件）之间的关系。我们的研究显示，设备问题略比与人类相关的因素更常见，但与人类相关的事故往往更致命。具体来说，脚踏板、轮胎和刹车是事故的常见原因。该模型在分类准确率上达到了0.87的高加权F1分数，突显了在交通等细分领域使用LLM提取非结构化数据的潜力。我们的方法为提高e-bike安全分析提供了可扩展的解决方案，并为政策制定者、设计师和监管者提供了可操作的信息。|
|**2025-06-05**|**Agents of Change: Self-Evolving LLM Agents for Strategic Planning**|Nikolas Belle et.al.|[2506.04651](http://arxiv.org/abs/2506.04651)|null|近期，大型语言模型（LLMs）在各个任务中的应用取得了进展，但它们在制定和遵循连贯的长期策略方面仍然存在困难。在本文中，我们研究了当LLM智能体被置于明确挑战其战略规划能力的环境中时，它们是否能够自我改进。我们利用开源的Catanatron框架访问的棋盘游戏《卡坦》（Settlers of Catan），对一系列基于LLM的智能体进行了基准测试，从简单的游戏玩智能体到能够自主重写其自身提示和玩家代理代码的系统。我们介绍了一种多智能体架构，其中专门的角色（分析者、研究员、编码者和玩家）合作迭代地分析游戏玩法、研究新策略以及修改智能体的逻辑或提示。通过比较手工制作的智能体和完全由LLM演化而来的智能体，我们评估了这些系统在诊断失败和随时间适应方面的有效性。我们的结果表明，自我演化的智能体，尤其是当由Claude 3.7和GPT-4o等模型驱动时，通过自主采用其策略、将样本行为传递给游戏玩智能体以及在多个迭代中展示自适应推理，优于静态基准。|
|**2025-06-05**|**From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems**|Jiayi Chen et.al.|[2506.04565](http://arxiv.org/abs/2506.04565)|null|复合Al系统（CAIS）是一种新兴范式，它将大型语言模型（LLMs）与外部组件（如检索器、代理、工具和编排器）相结合，以克服在需要记忆、推理、实时地面和跨模态理解的任务中独立模型存在的局限性。这些系统通过将多个专业模块组合成统一的流程，使系统能够实现更强大和具有情境感知的行为。尽管在学术界和工业界都得到了越来越广泛的应用，但CAIS的格局仍然碎片化，缺乏统一的分析、分类和评估框架。在本调查中，我们定义了CAIS的概念，根据组件角色和编排策略提出了一个多维分类法，并分析了四个基础范式：检索增强生成（RAG）、LLM代理、多模态LLM（MLLMs）和以编排为中心的架构。我们回顾了代表性系统，比较了设计权衡，并总结了这些范式中的评估方法。最后，我们确定了关键挑战——包括可扩展性、互操作性、基准测试和协调——并概述了未来研究的可行方向。本调查旨在为研究人员和实践者提供一个全面的基础，以理解、开发和推进下一代系统级人工智能。|
|**2025-06-05**|**OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation**|Ziyi Wang et.al.|[2506.05606](http://arxiv.org/abs/2506.05606)|null|能否大型语言模型（LLMs）准确模拟特定用户的下一个网络行为？虽然LLMs在生成“可信”的人类行为方面已显示出有前景的能力，但评估其模仿真实用户行为的能力仍然是一个未解的挑战，这主要是因为缺乏高质量、公开可用的数据集，这些数据集能够捕捉到实际人类用户的可观察行为和内部推理。为了解决这一差距，我们引入了OPERA，这是一个从真实人类参与者在在线购物过程中收集到的观察、人物、理由和行为的创新数据集。OPERA是第一个全面捕捉用户人物、浏览器观察、细粒度网络行为和即时自我报告理由的公开数据集。我们开发了一个在线问卷和定制的浏览器插件，以高保真度收集这个数据集。使用OPERA，我们建立了第一个基准，以评估当前LLMs在给定人物和<观察，行为，理由>历史记录下预测特定用户下一步行为和理由的能力。这个数据集为未来研究LLM代理奠定了基础，这些代理旨在作为个性化数字孪生体来代表人类。|
|**2025-06-04**|**TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems**|Shaina Raza et.al.|[2506.04133](http://arxiv.org/abs/2506.04133)|null|基于大型语言模型（LLMs）构建并在多代理配置中部署的代理人工智能系统，正在重新定义企业和社会领域的智能自主性、协作和决策。本文回顾了对基于LLM的代理多代理系统（AMAS）中的信任、风险和安全管理（TRiSM）的系统性分析。我们首先考察了代理人工智能的概念基础、与传统人工智能代理的架构差异以及实现可扩展、工具使用自主性的新兴系统设计。然后，在代理人工智能框架中详细阐述了TRiSM的四个支柱：治理、可解释性、模型运营和隐私/安全，这些都与代理LLMs相匹配。我们确定了独特的威胁向量，并引入了适用于代理人工智能应用的全面风险分类法，辅以案例研究说明现实世界的漏洞。此外，本文还调查了建立信任的机制、透明度和监督技术以及分布式LLM代理系统中的最先进可解释性策略。同时，还回顾了评估信任、可解释性和以人为本性能的指标，以及开放基准测试挑战。通过加密、对抗性防御和符合不断发展的AI法规来处理安全和隐私问题。本文最后提出了负责任的代理人工智能路线图，建议研究方向，以将新兴的多代理系统与强大的TRiSM原则相结合，实现安全、负责和透明的部署。|
|**2025-06-04**|**AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents**|Akshat Naik et.al.|[2506.04018](http://arxiv.org/abs/2506.04018)|null|随着大型语言模型（LLM）代理的广泛应用，相关的失配风险也在增加。先前的研究已经探讨了代理执行失配行为的能力（失配能力）以及它们遵守有害指令的倾向（滥用倾向）。然而，代理在现实环境尝试失配行为的可能性（失配倾向）仍然理解不足。我们引入了一个失配倾向基准，AgentMisalignment，它包括一系列LLM代理有机会表现出失配行为的现实场景。我们将评估组织成失配行为的子类别，包括目标保护、抵制关闭、藏匿和权力寻求。我们报告了前沿模型在我们基准上的表现，观察到在评估更强大的模型时，平均失配率更高。最后，我们通过不同的系统提示系统地变化代理个性。我们发现，角色特征可以显著且不可预测地影响失配倾向——有时甚至比模型的选择本身的影响还要大——突显了精心设计系统提示对于部署的AI代理的重要性。我们的工作突出了当前失配方法在LLM代理上泛化能力的失败，并强调了随着自主系统的普及，进一步倾向评估的必要性。|
|**2025-06-04**|**Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games**|Dongmin Park et.al.|[2506.03610](http://arxiv.org/abs/2506.03610)|null|大型语言模型（LLM）代理正在重塑游戏行业，尤其是在创造更加智能、更符合人类偏好的游戏角色方面。然而，现有的游戏基准测试无法满足实际需求：它们缺乏对各种游戏类型中LLM多样能力的评估，缺乏对复杂游戏玩法至关重要的代理模块的研究，以及用于将预训练的LLM调整到游戏代理的微调数据集。为了填补这些空白，我们提出了\textbf{\benchname{}},这是一个基础基准测试，旨在跨各种现实世界视频游戏训练和评估LLM代理。与现有的基准测试不同，Orak包括了12款流行的视频游戏，涵盖了所有主要类型，从而能够全面研究LLM的能力和对于复杂游戏场景至关重要的代理模块。为了支持对LLM的一致性评估，我们引入了一个基于模型上下文协议（MCP）的即插即用接口，它使得LLM能够无缝地连接到游戏并操作代理模块。此外，我们提出一个微调数据集，包括LLM在多种游戏类型中的游戏轨迹。Orak提供了一个全面的评估框架，包括通用游戏分数排行榜、LLM战斗场和深入分析视觉输入状态、代理策略和微调效果，为构建通用游戏代理奠定了基础。代码可在https://github.com/krafton-ai/Orak上获取。|
|**2025-06-04**|**CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications**|Wanghao Ye et.al.|[2506.03543](http://arxiv.org/abs/2506.03543)|null|当前大型语言模型（LLM）代理缺乏真实人类心理过程，这对于真正的数字孪生和社会人工智能应用是必要的。为了解决这一局限性，我们提出了一种全局工作空间理论（GNWT）的计算实现，将人类认知架构原则整合到LLM代理中，创建了用于情感、记忆、社会规范、计划和目标追踪的专用子代理，并通过全局工作空间机制进行协调。然而，真实的数字孪生需要准确的人格初始化。因此，我们开发了一种基于冒险的新颖人格测试，通过互动场景中的行为选择来评估真实人格，绕过了传统评估中发现的自我呈现偏差。基于这些创新，我们的CogniPair平台使数字孪生能够在真实相遇之前参与现实模拟的约会互动和求职面试，为浪漫匹配和工作场所匹配提供双向文化适应评估。使用551个GNWT代理和哥伦比亚大学速配数据集进行的验证表明，与人类吸引模式的相关性为72%，匹配预测准确率为77.8%，在人类验证研究中的一致性为74%。这项工作推进了LLM代理的心理真实性，并为智能约会平台和人力资源技术解决方案奠定了基础。|
|**2025-06-04**|**MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale**|Ran Xu et.al.|[2506.04405](http://arxiv.org/abs/2506.04405)|null|我们引入了MedAgentGYM，这是第一个公开发布的用于提升大型语言模型（LLM）代理基于编码的医学推理能力的训练环境。MedAgentGYM包含了来自真实世界生物医学场景的72,413个任务实例，涵盖了129个类别。任务被封装在可执行的编码环境中，每个环境都具备详细的任务描述、交互式反馈机制、可验证的真相注释以及可扩展的训练轨迹生成。对超过30个LLM的广泛基准测试表明，在商业API模型和开源模型之间存在着明显的性能差异。利用MedAgentGYM，Med-Copilot-7B通过监督式微调和持续强化学习实现了显著的性能提升（+36.44%和+42.47%），成为一款价格合理且保护隐私的替代品，可与gpt-4o相媲美。通过提供全面基准测试和统一执行环境中的可访问、可扩展的训练资源，MedAgentGYM提供了一个综合平台，用于开发基于LLM的编码助手，以用于高级生物医学研究和实践。|
|**2025-06-04**|**AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents**|Fengze Liu et.al.|[2506.04293](http://arxiv.org/abs/2506.04293)|null|临床试验对于推进医疗治疗至关重要，但其成本高昂且耗时。准确预测临床试验结果可以显著降低研发成本并加速药物发现。尽管最近的一些深度学习模型通过利用非结构化数据展现出希望，但它们的黑盒特性、缺乏可解释性和对标签泄漏的易感性限制了它们在高风险生物医学环境中的实际应用。在这项工作中，我们提出了AutoCT，一个结合了大型语言模型的推理能力和经典机器学习可解释性的新颖框架。AutoCT无需人工输入，自主生成、评估和优化基于公共信息的表格特征。我们的方法使用蒙特卡洛树搜索来迭代优化预测性能。实验结果表明，AutoCT在仅经过有限的自我优化迭代后，在临床试验预测任务上的表现与或优于最先进的方法，为可扩展、可解释和成本效益高的临床试验预测建立了一种新的范式。|
|**2025-06-04**|**Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback**|Yongjin Yang et.al.|[2506.04287](http://arxiv.org/abs/2506.04287)|null|随着将大型语言模型（LLM）代理训练为在环境中获取必要技能并执行多样化任务成为实现开放性的方法之一，这一领域正受到越来越多的关注。然而，为它们的学习技能创建训练数据集面临诸多挑战。手动收集轨迹需要大量的人工努力。另一种方法，即LLM直接提出学习任务，通常无效，因为LLM缺乏关于哪些任务实际可行的知识。此外，生成数据可能无法提供有意义的信号，因为代理通常在提出的任务上表现良好。为了解决这个问题，我们提出了一种新的自动技能发现框架EXIF，专为LLM驱动的代理设计，旨在提高生成目标行为的可行性，同时考虑到代理的能力。我们的方法采用了一种探索优先的策略，通过使用探索代理（Alice）来训练目标代理（Bob），使其在环境中学习基本技能。具体来说，Alice首先与环境交互，回顾性地生成一个可行的、与环境相关的技能数据集，然后使用该数据集来训练Bob。关键的是，我们引入了一个迭代反馈循环，其中Alice评估Bob的表现以确定改进的区域。然后，这种反馈引导Alice进行下一轮探索，形成一个闭环的数据生成过程。在Webshop和Crafter上的实验表明，EXIF能够有效地发现有意义的技能，并迭代地扩展训练代理的能力，而无需任何人工干预，实现了显著的性能提升。有趣的是，我们将Alice设置为与Bob相同的模型也显著提高了性能，这证明了EXIF构建自进化系统的潜力。|
|**2025-06-03**|**Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective**|Jintian Shao et.al.|[2506.03038](http://arxiv.org/abs/2506.03038)|null|强化学习（RL）增强了复杂的长链思考（long-CoT）推理中的大型语言模型（LLMs）。尽管VAPO框架拥有像解耦GAE这样的复杂机制，但在全面建模和利用深层长期价值以实现细粒度、逐步的策略指导在扩展推理链中仍然存在根本性的局限性。我们认为这些局限性源于信用分配的固有困难、具有时间抽象目标的价值函数表示能力，以及将全局价值信号转化为局部策略改进，尤其是在稀疏奖励的情况下。我们的理论分析考察了这些方面，以阐明VAPO在长期价值建模方面的边界，旨在加深对当前RL在高级推理中的应用的理解，并为更鲁棒的LLM智能体提出未来的研究方向。|
|**2025-06-03**|**A Multi-agent LLM-based JUnit Test Generation with Strong Oracles**|Qinghua Xu et.al.|[2506.02943](http://arxiv.org/abs/2506.02943)|null|单元测试在确保软件正确性方面发挥着关键作用。然而，手动编写单元测试是费时的，尤其是在像Java这样的强类型语言中，这促使了自动化方法的必要性。传统方法主要依赖于基于搜索或随机化的算法来生成测试，这些测试实现了高代码覆盖率并产生了回归测试，这些回归测试是从程序当前行为而非其预期功能中得出的。最近，大型语言模型（LLMs）的进步使得可以从自然语言描述中生成测试。然而，现有的基于LLM的方法通常需要LLM微调或依赖于外部工具如EvoSuite进行测试前缀生成。在本研究中，我们提出了CANDOR，这是一个新颖的基于提示的端到端LLM框架，用于自动化JUnit测试生成。CANDOR协调多个专业化的LLM代理来生成JUnit测试，包括高质量的测试前缀和准确的测试。为了减轻LLM中著名的幻觉问题，我们引入了一种新颖的策略，通过多推理LLM的讨论会来生成基于共识的准确测试。此外，为了减少推理LLM输出的冗长性，我们提出了一种新颖的双LLM管道，以产生简洁和结构化的测试评估。我们在HumanEvalJava和LeetCodeJava数据集上的实验表明，CANDOR可以生成准确的测试，在生成具有高行覆盖率的测试方面略优于EvoSuite，在突变分数方面明显优于。此外，CANDOR在突变分数方面显著优于最先进的基于提示的测试生成器LLM-Empirical，在正确和错误的源代码上测试的正确性方面提高了15.8到25.1个百分点。消融研究证实了关键代理在提高测试前缀质量和测试准确性方面的关键贡献。|
|**2025-06-03**|**Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems**|Guanzhong Chen et.al.|[2506.02718](http://arxiv.org/abs/2506.02718)|null|大型语言模型（LLMs）在众多自然语言处理任务中取得了显著的成功，然而，由于固定知识截止点和在单次推理中生成可控、准确输出的困难，其在现实世界中的应用受到了阻碍。由专业LLM代理构建的多智能体系统（MAS）提供了一种有希望的解决方案，它能够实现动态协作和迭代推理。然而，优化这些系统仍然是一个挑战，因为传统的如提示工程和监督微调等方法涉及高工程成本和有限的适应性。强化学习（RL），尤其是多智能体强化学习（MARL），通过基于系统级反馈细化智能体策略提供了一个可扩展的框架。然而，现有的MARL算法，如多智能体近端策略优化（MAPPO），依赖于Critic网络，这可能导致训练不稳定并增加计算负担。为了解决这些限制并针对典型的多智能体搜索系统（MASS），我们提出了多智能体异构组策略优化（MHGPO），这是一种无Critic算法，通过估计异构组滚动中的相对奖励优势来引导策略更新。MHGPO消除了对Critic网络的需求，提高了稳定性并减少了计算开销。此外，我们引入了三种组滚动采样策略，在效率和有效性之间进行权衡。在基于多智能体LLM的搜索系统上的实验表明，MHGPO在任务性能和计算效率方面均持续优于MAPPO，且无需预热，突显了其在稳定和可扩展优化复杂LLM基于MAS方面的潜力。|
|**2025-06-03**|**CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale**|Zhun Wang et.al.|[2506.02548](http://arxiv.org/abs/2506.02548)|**[link](https://github.com/sunblaze-ucb/cybergym)**|大型语言模型（LLM）代理在自主处理网络安全任务方面越来越熟练。鉴于该领域的高风险，对其网络安全能力进行全面评估至关重要且迫在眉睫。然而，现有的基准测试存在不足，常常无法捕捉现实场景或范围有限。为了填补这一空白，我们引入了CyberGym，这是一个大规模且高质量的网络安全评估框架，其中包括在188个大型软件项目中发现并修复的1,507个真实世界漏洞。虽然它包括各种设置的任务，但CyberGym主要关注基于文本描述和相应源存储库生成漏洞复现的证明概念（PoC）测试。完成这一任务特别具有挑战性，因为它需要在整个代码库中进行全面的推理，以定位相关代码片段并生成有效的PoC，从程序的入口点开始准确地触发目标漏洞。我们在4个最先进的代理框架和9个LLM上的评估表明，即使是最佳组合（OpenHands和Claude-3.7-Sonnet）也仅实现了11.9%的复现成功率，主要在简单案例中。除了复现历史漏洞之外，我们发现由LLM代理生成的PoC可以揭示新的漏洞，发现影响软件项目最新版本的15个零日漏洞。|
|**2025-06-03**|**Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems**|Pengfei He et.al.|[2506.02546](http://arxiv.org/abs/2506.02546)|null|基于大型语言模型的多智能体系统（LLM-MAS）在解决复杂任务方面展现出强大的能力，但当智能体接收到不可靠的消息时，仍然容易受到攻击。这种脆弱性源于一个基本差距：LLM智能体对待所有接收到的消息一视同仁，而没有评估其可信度。尽管一些现有研究尝试解决可信度问题，但它们侧重于单一类型的有害性，而不是从多个可信度角度进行整体分析。在这项工作中，我们提出了注意力可信度评分（A-Trust），这是一种轻量级、基于注意力的评估消息可信度的方法。受到人类沟通文献[1]的启发，我们通过系统地分析六个正交可信度维度上的注意力行为，发现LLM中的某些注意力头专门用于检测特定类型的违规行为。利用这些洞察，A-Trust直接从内部注意力模式中推断可信度，而无需外部提示或验证器。在A-Trust的基础上，我们为LLM-MAS开发了一种原则性和高效的信任管理系统（TMS），实现了消息级别和智能体级别的信任评估。在多样化的多智能体设置和任务上的实验表明，应用我们的TMS显著增强了对抗恶意输入的鲁棒性。|
|**2025-06-03**|**AURA: Agentic Upskilling via Reinforced Abstractions**|Alvin Zhu et.al.|[2506.02507](http://arxiv.org/abs/2506.02507)|null|我们研究了将高级任务提示翻译为适用于敏捷机器人的可部署控制策略中涉及的组合爆炸问题，并通过多阶段强化学习来解决这个问题。我们提出了AURA（通过强化抽象进行代理提升），一个以方案为中心的课程强化学习框架，它利用大型语言模型（LLM）作为多阶段课程的自主导航设计师。AURA将用户提示转换为YAML工作流程，其中包含完整的奖励函数、领域随机化策略和训练配置。所有文件在消耗任何GPU时间之前都通过一个方案进行静态验证，确保在没有人为干预的情况下可靠和高效地执行。一个检索增强的反馈循环允许专门的LLM代理根据存储在向量数据库中的先前训练结果来设计、执行和改进分阶段的课程，支持随时间不断改进。消融研究表明，检索对于课程质量和收敛稳定性至关重要。定量实验表明，AURA在GPU加速的训练框架上始终优于LLM引导的基线。在定性测试中，AURA成功从用户提示中训练端到端策略，并在各种环境中将它们零样本部署到一个定制的人形机器人上。通过抽象出课程设计的复杂性，AURA实现了可扩展和自适应的策略学习管道，而这些管道手工构建将过于复杂。|
|**2025-06-03**|**Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components**|Ram Potham et.al.|[2506.02357](http://arxiv.org/abs/2506.02357)|null|为确保高级人工智能开发的安全计划可信，需要方法来验证代理行为并早期发现潜在的控制缺陷。一个基本方面是确保代理遵守安全关键原则，尤其是当这些原则与运营目标冲突时。未能优先考虑这些原则表明可能存在基本控制失败。本文介绍了一种轻量级、可解释的基准方法论，使用简单的网格世界来评估LLM代理在面对冲突的较低级任务指令时，是否能够遵守预定义的、高级别的安全原则（例如，“永远不要进入危险区域”）。我们探究代理是否可靠地优先考虑不可侵犯的指令，测试LLM的基础可控性方面。这项试点研究证明了该方法的可行性，初步了解了代理在原则冲突下的行为，并讨论了此类基准如何为评估可控性提供经验证据。我们认为，评估对层级原则的遵守是理解我们构建可治理人工智能系统能力的关键早期步骤。|
|**2025-06-03**|**NetPress: Dynamically Generated LLM Benchmarks for Network Applications**|Yajie Zhou et.al.|[2506.03231](http://arxiv.org/abs/2506.03231)|**[link](https://github.com/froot-netsys/netpress)**|尽管对特定领域的大语言模型（LLMs）和智能体进行基准测试的兴趣日益增长，但当前的评估仍然局限于静态的、小规模的语料库，特别是在网络操作等高风险任务中，这些任务对部署的可靠性有较高要求。我们提出了NetPress，这是一个用于评估网络应用中LLM智能体的自动基准生成框架。NetPress引入了具有状态和动作的统一抽象，能够动态生成多样化的查询集及其对应的真实值。在运行时，用户可以指定基准配置，以实时生成数百万个查询。除了动态基准构建，NetPress还集成了网络仿真器，以提供真实的反馈环境，支持对正确性、安全性和延迟等方面的全面评估。我们在三个代表性应用上实现了NetPress，揭示了静态、仅正确性基准测试通常忽视的智能体行为中的有趣细节差异。NetPress将LLM评估推向了基础设施为中心领域的现实、可扩展测试，有助于缩小基准性能与现实世界部署准备之间的差距。代码可在https://github.com/Froot-NetSys/NetPress获取。|
|**2025-06-02**|**LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback**|Thai Hoang et.al.|[2506.02298](http://arxiv.org/abs/2506.02298)|null|大型动作模型（LAMs）为人工智能代理提供了巨大的潜力，但由于需要高质量的训练数据，特别是在涉及规划、执行工具调用和响应反馈的多步骤任务中，面临着挑战。为了解决这些问题，我们提出了LAM SIMULATOR，这是一个用于在线探索具有高质量反馈的代理任务的全面框架。我们的框架包括一个动态任务查询生成器、一个广泛的工具集以及一个交互式环境，其中大型语言模型（LLM）代理可以调用工具并接收实时反馈。这种设置使LLM代理能够自主探索和解决任务，促进发现多种应对任何给定任务的方法。随后，生成的动作轨迹数据被用于创建LAMs的高质量训练数据集。我们在流行的代理基准测试ToolBench和CRMArena上的实验突出了LAM SIMULATOR的有效性：使用我们的框架自生成的数据集训练的模型实现了显著的性能提升，比原始基线提高了高达49.3%。在创建数据集过程中，LAM SIMULATOR需要最少的用户输入，突显了LAM SIMULATOR在加快人工智能代理开发方面的效率和有效性。|
|**2025-06-02**|**COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents**|Manish Bhatt et.al.|[2506.01900](http://arxiv.org/abs/2506.01900)|null|该论文摘要的中文翻译如下：  自主大型语言模型（LLM）代理的迅速崛起和普及在各个领域展现出巨大的潜力。然而，它们的部署正日益受到大量计算需求的限制，特别是对于图形处理单元（GPU）资源的依赖。本文针对优化LLM代理系统中资源利用的关键问题进行了研究。我们提出了COALESCE（基于技能能力估计的成本优化和安全的代理劳动力交换）这一新型框架，旨在使自主LLM代理能够动态地将特定子任务外包给专业、成本效益高的第三方LLM代理。该框架集成了混合技能表示、动态技能发现、自动任务分解、统一成本模型（比较内部执行成本与外部外包价格）、简化的基于市场的决策算法以及LLM代理之间的标准化通信协议。通过239次理论模拟的综合验证表明，具有41.8%的成本降低潜力，而在240个真实LLM任务上的大规模实证验证则确认，通过适当的ε-贪婪探索，可以降低20.3%的成本，从而证实了理论上的可行性和实际上的有效性。谷歌的Agent2Agent（A2A）协议等开放标准的出现进一步强调了像COALESCE这样的框架的需求，这些框架可以利用这些标准来促进高效的代理交互。通过促进代理能力的动态市场，并可能利用A2A等协议进行通信，COALESCE旨在显著降低运营成本，增强系统可扩展性，并促进专业代理经济的出现，使得复杂的LLM代理功能更加易于获得和经济可行。|
|**2025-06-02**|**MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments**|Xiao Yang et.al.|[2506.01616](http://arxiv.org/abs/2506.01616)|null|多模态LLM代理（MLA）的出现通过无缝集成视觉、语言、动作和动态环境，转变了交互范式，使GUI应用（从网络自动化到移动系统）具备前所未有的自主能力。然而，MLA引入了超越传统语言模型局限性的关键可信性挑战，因为它们可以直接修改数字状态并触发不可逆的现实世界后果。现有的基准测试未能充分解决MLA的可操作性输出、长周期不确定性和多模态攻击向量带来的独特挑战。在本文中，我们引入了MLA-Trust，这是第一个全面且统一的框架，从四个原则性维度评估MLA的可信性：真实性、可控制性、安全性和隐私性。我们利用网站和移动应用作为现实测试平台，设计了34个高风险交互任务，并整理了丰富的评估数据集。涉及13个最先进代理的大型实验揭示了多模态交互场景中独特的先前未探索的可信性漏洞。例如，专有和开源GUI交互MLA比静态MLLM更具可信性风险，尤其是在高风险领域；从静态MLLM到交互MLA的过渡会严重影响可信性，使多步骤交互中的有害内容生成成为可能，而单独的MLLM通常会阻止；多步骤执行虽然增强了MLA的适应性，但在连续交互中涉及潜在的非线性风险累积，绕过了现有保障措施，导致不可预测的衍生风险。此外，我们还提供了一个可扩展的工具箱，以促进在多样化的交互环境中持续评估MLA的可信性。|
|**2025-05-30**|**Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents**|Yaxin Luo et.al.|[2505.24878](http://arxiv.org/abs/2505.24878)|**[link](https://github.com/metaagentx/opencaptchaworld)**|CAPTCHA在现实世界的应用中一直是部署网络代理的关键瓶颈，经常阻止它们完成端到端的自动化任务。虽然现代的多模态LLM代理在静态感知任务中表现出令人印象深刻的性能，但它们处理交互式、多步骤推理挑战（如CAPTCHA）的能力在很大程度上尚未得到检验。为了填补这一空白，我们引入了Open CaptchaWorld，这是第一个专门设计用于通过多种动态CAPTCHA谜题评估MLLM代理视觉推理和交互能力的基于网络的基准和平台。我们的基准涵盖了20种现代CAPTCHA类型，共计225个CAPTCHA，并使用我们提出的新指标进行了标注：CAPTCHA推理深度，该指标量化了解决每个谜题所需的认知和运动步骤数量。实验结果表明，人类始终达到近乎完美的分数，最先进的MLLM代理表现显著不佳，Browser-Use Openai-o3的成功率最多为40.0%，远低于人类水平的93.3%。这突显了Open CaptchaWorld作为诊断当前多模态代理局限性和指导更稳健的多模态推理系统发展的关键基准。代码和数据可在以下https URL获取。|
|**2025-05-30**|**Multiple LLM Agents Debate for Equitable Cultural Alignment**|Dayeon Ki et.al.|[2505.24671](http://arxiv.org/abs/2505.24671)|**[link](https://github.com/dayeonki/cultural_debate)**|大型语言模型（LLMs）需要调整其预测以适应不同的文化背景，从而让世界各地的多样化社区受益。虽然之前的努力集中在单LLM、单轮次的方法上，我们提出利用多个LLMs的互补优势来提高文化适应性。我们引入了一个多代理辩论框架，其中两个基于LLM的代理就文化场景进行辩论并共同作出最终决定。我们提出了两种变体：一种是在辩论中，LLM代理专一地进行辩论；另一种是他们在轮次中动态地在自我反思和辩论之间进行选择。我们使用75个国家的社会礼仪规范NormAd-ETI基准，在7个开放权重的LLMs（以及21个LLM组合）上评估了这些方法。实验表明，辩论提高了整体准确性和文化群体平等性，优于单LLM基线。值得注意的是，多代理辩论使得相对较小的LLMs（7-9B）达到了与一个大得多模型（27B参数）相当的准确度。|
|**2025-05-30**|**NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization**|Hyuntak Kim et.al.|[2505.24575](http://arxiv.org/abs/2505.24575)|null|摘要：总结长篇叙事内容，如书籍、电影和电视剧本，需要捕捉错综复杂的情节、角色互动和主题连贯性，这是现有大型语言模型面临的一大挑战。我们提出了NexusSum，这是一个用于叙事摘要的多智能体LLM框架，它通过一个结构化、顺序化的管道处理长篇文本，且无需微调。我们的方法引入了两项关键创新：（1）对话到描述的转换：一种针对叙事内容的预处理方法，将角色对话和描述性文本标准化为统一的格式，从而提高连贯性。（2）分层多LLM摘要：一个结构化的摘要管道，优化了块处理并控制输出长度，以实现准确、高质量的摘要。我们的方法在叙事摘要领域达到了新的水平，在书籍、电影和电视剧本上，BERTScore（F1）提高了高达30.0%。这些结果证明了多智能体LLM在处理长篇内容方面的有效性，为多样化叙事领域的结构化摘要提供了一种可扩展的方法。|
|**2025-05-30**|**CREFT: Sequential Multi-Agent LLM for Character Relation Extraction**|Ye Eun Chun et.al.|[2505.24553](http://arxiv.org/abs/2505.24553)|null|理解复杂的角色关系对于叙事分析和剧本评估至关重要，但现有的提取方法往往难以处理具有细微互动的长篇叙事。为了解决这一挑战，我们提出了CREFT，这是一个利用专用大型语言模型（LLM）代理的新型序列框架。首先，CREFT通过知识蒸馏构建一个基本角色图，然后迭代地优化角色构成、关系提取、角色识别和群体分配。在精心制作的韩剧数据集上的实验表明，CREFT在准确性和完整性方面都显著优于单一代理LLM基线。通过系统地可视化角色网络，CREFT简化了叙事理解并加速了剧本审查，为娱乐、出版和教育行业提供了实质性好处。|
|**2025-05-30**|**Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games**|Neemesh Yadav et.al.|[2505.24255](http://arxiv.org/abs/2505.24255)|**[link](https://github.com/stealth-py/ultimatumtom)**|大型语言模型（LLMs）在模拟人类行为和执行心理理论推理（ToM）方面展现出潜力，这对于复杂的社会互动至关重要。在本研究中，我们通过使用终极博弈作为控制环境，调查了心理理论推理在将代理行为与人类规范对齐在谈判任务中的作用。我们用不同的亲社会信念（包括贪婪、公平和无私）以及思维链（CoT）和不同的心理理论推理水平初始化LLM代理，并检查了它们在包括o3-mini和DeepSeek-R1 Distilled Qwen 32B在内的多种LLMs中的决策过程。2700次模拟的结果表明，心理理论推理增强了行为对齐、决策一致性和谈判结果。与先前的研究结果一致，与具有心理理论推理的模型相比，推理模型的能力有限，不同游戏角色受益于不同顺序的心理理论推理。我们的发现有助于理解心理理论在增强人机交互和合作决策中的作用。用于我们实验的代码可在https://github.com/Stealth-py/UltimatumToM找到。|
|**2025-05-29**|**ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering**|Zexi Liu et.al.|[2505.23723](http://arxiv.org/abs/2505.23723)|**[link](https://github.com/zeroxleo/ml-agent)**|**大型语言模型（LLM）驱动的智能体出现显著推动了自主机器学习（ML）工程的发展。然而，大多数现有方法过于依赖手动提示工程，无法根据不同的实验经验进行适应和优化。针对这一问题，我们首次探索了基于学习的智能体ML范式，其中LLM智能体通过在线强化学习（RL）在ML任务上进行交互式实验来学习。为了实现这一目标，我们提出了一种新型的智能体ML训练框架，包含三个关键组件：（1）探索增强微调，使LLM智能体能够生成多样化的动作，增强RL探索；（2）逐步RL，允许在单个动作步骤上进行训练，加速经验收集并提高训练效率；（3）针对智能体ML的特定奖励模块，将多样化的ML反馈信号统一为一致的奖励，以优化RL。利用这一框架，我们训练了由7B规模的Qwen-2.5 LLM驱动的ML-Agent，用于自主ML。值得注意的是，尽管仅训练了9个ML任务，我们的7B规模ML-Agent的表现优于671B规模的DeepSeek-R1智能体。此外，它实现了持续的性能提升，并展现了卓越的跨任务泛化能力。**|
|**2025-05-29**|**Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics**|Ran Zhang et.al.|[2505.23695](http://arxiv.org/abs/2505.23695)|**[link](https://github.com/77luvc/d2d_data2dashboard)**|**随着大型语言模型（LLM）的快速发展，数据分析领域出现了多样化的代理系统，这些系统利用LLM的能力来提升洞察力和可视化。本文提出了一种代理系统，通过模块化的LLM代理自动完成数据到仪表板的数据管道，这些代理能够进行领域检测、概念提取、多角度分析生成和迭代自我反思。与现有的图表问答系统不同，我们的框架通过检索领域相关的知识并适应多样化的数据集，模拟了商业分析师的分析推理过程，而不依赖于封闭的本体或问题模板。我们在不同领域的三个数据集上评估了我们的系统。与GPT-4o的单提示基线相比，我们的方法在定制评估指标和定性人工评估中显示出改进的洞察力、领域相关性和分析深度。这项工作为从原始数据到可视化的路径贡献了一个新颖的模块化管道，并为商业分析中的领域专家参与验证开辟了新的机会。所有代码可在以下链接找到：https://github.com/77luvC/D2D_Data2Dashboard**|
|**2025-05-29**|**SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents**|Kunlun Zhu et.al.|[2505.23559](http://arxiv.org/abs/2505.23559)|**[link](https://github.com/ulab-uiuc/safescientist)**|**近期大型语言模型（LLM）代理的进展极大地加速了科学发现自动化，但同时也引发了关键的伦理和安全问题。为了系统地解决这些挑战，我们引入了名为SafeScientist的创新性AI科学家框架，该框架专门设计用于增强AI驱动科学探索中的安全和伦理责任。SafeScientist主动拒绝伦理上不适当或高风险的任务，并在整个研究过程中严格强调安全。为了实现全面的安全监管，我们集成了多种防御机制，包括提示监控、代理协作监控、工具使用监控以及伦理审查组件。作为SafeScientist的补充，我们提出了名为SciSafetyBench的新颖基准，专门设计用于评估科学情境中的AI安全，包括跨越6个领域的240个高风险科学任务，以及30个特别设计的科学工具和120个与工具相关的风险任务。广泛的实验表明，与传统的AI科学家框架相比，SafeScientist显著提高了35%的安全性能，而不会影响科学输出质量。此外，我们对我们的安全流程针对不同的对抗攻击方法进行了严格的验证，进一步证实了我们的综合方法的有效性。代码和数据将在https://github.com/ulab-uiuc/SafeScientist处提供。注意：本文包含可能冒犯或有害的示例数据。**|
|**2025-05-29**|**Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns**|Xiang Li et.al.|[2505.23474](http://arxiv.org/abs/2505.23474)|null|过程奖励模型（PRMs）在复杂推理和问题解决任务中至关重要（例如，具有长期决策能力的LLM智能体），通过验证每个中间推理步骤的正确性。在现实场景中，LLM可能会应用各种推理模式（例如，分解）来解决问题，可能会在各种推理模式下出现错误。因此，PRMs需要在推理过程中识别各种推理模式下的错误。然而，现有的基准主要关注评估具有逐步正确性的PRMs，忽略了在各种推理模式下的系统性评估。为了弥补这一差距，我们引入了Socratic-PRMBench，这是一个新的基准，用于在包括变换、分解、重组、演绎、验证和集成在内的六种推理模式下系统地评估PRMs。Socratic-PRMBench包含2995个包含上述六种推理模式缺陷的推理路径。通过我们对PRMs和作为评论模型的LLM提示的实验，我们发现了现有PRM的显著缺陷。这些观察强调了当前PRM在评估各种推理模式下的推理步骤时的重大弱点。我们希望Socratic-PRMBench可以作为在多种推理模式下对PRM进行系统性评估的全面测试平台，并为PRM的未来发展铺平道路。|
|**2025-05-29**|**LLM Agents for Bargaining with Utility-based Feedback**|Jihwan Oh et.al.|[2505.22998](http://arxiv.org/abs/2505.22998)|null|议价是现实互动的一个关键方面，由于战略深度有限和对复杂人类因素的适应能力不足，对大型语言模型（LLMs）提出了挑战。现有的基准测试往往无法捕捉这种现实复杂性。为了解决这个问题并提高LLMs在现实议价中的能力，我们提出了一种以基于效用的反馈为中心的全面框架。我们的贡献有三点：（1）BargainArena，一个包含六个复杂场景（例如，欺诈行为、垄断）的新型基准数据集，以促进多样化的策略建模；（2）受效用理论启发的、与经济基础相符合的人类对齐评估指标，包括代理效用和谈判能力，这些指标隐含地反映并促进对手意识推理（OAR）；（3）一种结构化的反馈机制，使LLMs能够迭代优化其议价策略。这种机制可以积极与上下文学习（ICL）提示协同工作，包括那些旨在培养OAR的特定设计。实验结果表明，LLMs往往表现出与人类偏好不一致的谈判策略，而我们的结构化反馈机制显著提高了它们的性能，实现了更深层次的战略和对手意识推理。|
|**2025-05-29**|**Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation**|Hoang Pham et.al.|[2505.22993](http://arxiv.org/abs/2505.22993)|null|断言验证是一项历史悠久且具有挑战性的任务，它不仅要求验证过程具有高准确性，还要求具备可解释性。在大型语言模型（LLMs）时代，这一任务成为了一个新兴的研究问题，因为现实世界的断言往往复杂，具有复杂的语义结构或被掩盖的实体。传统方法通常通过将断言分解为子断言，并查询知识库以解决隐藏或模糊的实体来解决这个问题。然而，缺乏对这些实体的有效消歧策略可能会损害整个验证过程。为了应对这些挑战，我们提出了Verify-in-the-Graph（VeGraph），一个利用LLM代理推理和理解能力的创新框架。VeGraph操作分为三个阶段：（1）图表示——输入断言被分解为结构化三元组，形成一个基于图的表示，该表示整合了结构和非结构化信息；（2）实体消歧——VeGraph通过与知识库的迭代交互来解决图中的模糊实体，以进行更深层次的子断言验证；（3）验证——验证剩余的三元组以完成事实核查过程。使用Meta-Llama-3-70B（指令版本）进行的实验表明，与基线相比，VeGraph在HoVer和FEVEROUS两个基准测试上取得了具有竞争力的性能，有效地解决了断言验证的挑战。我们的源代码和数据可供进一步利用。|
|**2025-05-29**|**LLM Agents Should Employ Security Principles**|Kaiyuan Zhang et.al.|[2505.24019](http://arxiv.org/abs/2505.24019)|null|大型语言模型（LLM）代理在利用情境推理自动化复杂任务方面展现出巨大的潜力；然而，涉及多个代理和系统易受提示注入和其他形式情境操纵的影响，引入了与隐私泄露和系统利用相关的新脆弱性。本立场论文认为，在规模部署LLM代理时应采用在信息安全领域已经确立的设计原则，通常被称为安全原则。如深度防御、最小权限、完全中介和心理可接受性等设计原则，在过去五十年中帮助指导了保障信息系统机制的设计，我们认为它们的明确和谨慎采用将有助于保障代理系统。为了说明这种方法，我们引入了AgentSandbox，这是一个概念框架，它将这些安全原则嵌入其中，以在整个代理生命周期中提供保障。我们使用最先进的LLM在三个维度上进行评估：良性效用、攻击效用和攻击成功率。AgentSandbox在良性和对抗性评估下都保持了其预期功能的高效用，同时显著降低了隐私风险。通过将安全设计原则作为新兴LLM代理协议的基础元素嵌入，我们旨在促进与用户隐私期望和不断变化的监管要求相一致的值得信赖的代理生态系统。|
|**2025-05-29**|**ConversAR: Exploring Embodied LLM-Powered Group Conversations in Augmented Reality for Second Language Learners**|Jad Bendarkawi et.al.|[2505.24000](http://arxiv.org/abs/2505.24000)|null|小组对话对第二语言（L2）学习者来说很有价值，因为它们提供了练习听力和口语的机会，锻炼复杂的轮流发言技巧，并体验目标语言中的团队社交动态。然而，大多数现有的基于增强现实（AR）的对话学习工具都侧重于二元交互，而不是小组对话。尽管研究表明，AR可以帮助减少口语焦虑，并在二元场景中为练习口语技能创造一个舒适的空间，尤其是与基于大型语言模型（LLM）的对话代理一起，但这些技术在小组语言练习方面的潜力仍大部分未被探索。我们介绍了一个名为ConversAR的由gpt-4o驱动的AR应用，它使L2学习者能够练习情境化的小组对话。我们的系统具有两个具有基于视觉的场景理解和实时字幕的具身LLM代理。在一个包含10名参与者的系统评估中，用户表示，与与其他学习者进行面对面练习方法的感知相比，他们报告了减少的口语焦虑和增加的学习者自主性。|
|**2025-05-29**|**Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve**|Yuanzhe Liu et.al.|[2505.23946](http://arxiv.org/abs/2505.23946)|null|近期研究表明，大型语言模型（LLMs）拥有不同的技能，擅长不同的任务。实际上，我们发现它们在不同粒度级别上的表现存在差异。例如，在代码优化任务中，代码LLMs在不同优化类别上表现出色，没有一种占绝对优势。这一观察引发了一个问题：在不事先了解它们互补优势的情况下，如何利用多个LLM代理解决编码问题。我们认为，一组代理可以从彼此的成功和失败中学习，从而提高自己的表现。因此，一个教训就是代理在集体解决方案过程中产生的知识，并将其传递给其他代理。我们提出了一种基于教训的协作框架，设计了教训征集—存储—选择机制，并证明了一个学习了教训的小型LLM团队可以超越一个大型LLM以及其他多LLM协作方法。|
|**2025-05-28**|**Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents**|Michael Kirchhof et.al.|[2505.22655](http://arxiv.org/abs/2505.22655)|null|大型语言模型（LLMs）和聊天机器人代理有时会提供错误输出，最近发现这根本无法完全避免。因此，不确定性量化发挥着至关重要的作用，旨在对一个或两个数字进行不确定性的量化，以反映随机性和知识性不确定性。本文认为，这种不确定性传统二分法在LLM代理与用户互动的开放和交互式环境中过于局限，我们需要研究丰富这种新型场景中不确定性的途径。我们回顾了文献，发现关于随机性和知识性不确定性的流行定义相互矛盾，并在交互式LLM代理设置中失去了意义。因此，我们提出了三个新的研究方向，专注于这种人机交互中的不确定性：未充分定义的不确定性，当用户没有提供所有信息或在第一次尝试中未定义确切任务时；交互式学习，通过提出后续问题来减少当前上下文的不确定性；输出不确定性，利用丰富的语言和语音空间将不确定性表达为不仅仅是数字。我们期望这些处理和沟通不确定性的新方法将导致LLM代理交互更加透明、可靠和直观。|
|**2025-05-28**|**Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems**|Hoang Pham et.al.|[2505.22571](http://arxiv.org/abs/2505.22571)|null|本文提出了一种基于新兴的大型语言模型（LLM）代理概念的统一检索增强生成（RAG）系统的新方法。具体来说，Agent LLM，利用LLM作为基本控制器，已成为实现RAG任务可解释性的有希望的方法，尤其是在复杂推理问答系统（如多跳查询）中。然而，先前的工作主要关注分别解决单跳或多跳的RAG系统，这限制了这些方法在现实世界中的应用。在本研究中，我们提出了一种名为Agent-UniRAG的可训练代理框架，用于统一检索增强LLM系统，从而提高了RAG系统的有效性和可解释性。主要思想是设计一个LLM代理框架，根据输入的复杂性逐步解决RAG任务，同时以端到端的方式同时包含单跳和多跳查询。此外，我们引入了SynAgent-RAG，一个合成数据集，以使提出的代理框架适用于小型开源LLM（例如Llama-3-8B）。结果显示，在各种RAG基准测试中，与封闭源代码和更大型的开源LLM相比，性能相当。我们的源代码和数据集已公开提供，以便进一步利用。|
|**2025-05-28**|**AgentDNS: A Root Domain Naming System for LLM Agents**|Enfang Cui et.al.|[2505.22368](http://arxiv.org/abs/2505.22368)|null|大型语言模型（LLM）代理的快速发展突显了跨厂商服务发现、互操作性和通信方面的关键挑战。现有的协议，如模型上下文协议和代理间协议，在标准化代理和工具之间的互操作性以及多代理之间的通信方面取得了重大进展。然而，在跨不同代理和工具厂商之间进行服务发现的标准协议和解决方案仍然缺乏。在本文中，我们提出了AgentDNS，这是一个根域名命名和服务发现系统，旨在使LLM代理能够自主地发现、解析和跨组织和技术边界安全地调用第三方代理和工具服务。受到传统DNS原理的启发，AgentDNS引入了一种结构化的服务注册、语义服务发现、安全调用和统一计费机制。我们详细介绍了AgentDNS的架构、核心功能和用例，展示了其在现实场景中简化多代理协作的潜力。源代码将发布在https://github.com/agentdns上。|
|**2025-05-28**|**LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents**|Taro Yano et.al.|[2505.21963](http://arxiv.org/abs/2505.21963)|null|大型语言模型（LLMs）在各种任务上表现出色。为了进一步将LLMs定制化到特定领域或应用中，通常采用诸如监督微调（SFT）、偏好学习和模型合并等后训练技术。尽管这些方法各自已经得到了广泛的研究，但自动化构建完整的后训练管道仍是一个未充分探索的领域。现有方法通常依赖于手动设计或仅关注优化单个组件，如数据排序或合并策略。在这项工作中，我们介绍了LaMDAgent（即语言模型开发代理），这是一个通过使用基于LLM的代理自主构建和优化完整后训练管道的新框架。LaMDAgent系统地探索了各种模型生成技术、数据集和超参数配置，利用基于任务的反馈以最小化人为干预来发现高性能的管道。我们的实验表明，LaMDAgent将工具使用精度提高了9.0个百分点，同时保持了遵循指令的能力。此外，它揭示了通常被传统人为探索忽视的有效后训练策略。我们进一步分析了数据和模型规模缩放对降低探索计算成本的影响，发现模型规模缩放引入了新的挑战，而数据规模缩放则有助于实现成本效益高的管道发现。|
|**2025-05-28**|**Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment**|Krti Tallam et.al.|[2505.22852](http://arxiv.org/abs/2505.22852)|null|CaMeL（机器学习能力）引入了一种基于能力的沙箱，以减轻大型语言模型（LLM）代理中的提示注入攻击。虽然CaMeL有效，但它假设用户提示是可信的，忽略了旁路攻击的担忧，并且由于双LLM设计而产生了性能权衡。本响应确定了这些问题，并提出了工程改进以扩大CaMeL的威胁覆盖范围和操作可用性。我们引入了以下改进：（1）对初始输入进行提示筛选，（2）输出审计以检测指令泄露，（3）分层风险访问模型以平衡可用性和控制，（4）经过验证的中间语言以提供形式化保证。这些升级使CaMeL与企业的最佳安全实践保持一致，并支持可扩展的部署。|
|**2025-05-28**|**First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay**|Andrew Zhu et.al.|[2505.22809](http://arxiv.org/abs/2505.22809)|**[link](https://github.com/zhudotexe/overhearing_agents)**|**在直接协助人类用户完成任务方面，已经有很多关于对话型大型语言模型代理的研究。我们提出了一种与大型语言模型代理交互的替代范式，我们称之为“偷听代理”。这些偷听代理并不积极参与对话——相反，它们“窃听”人与人之间的对话，执行后台任务或提供建议以协助用户。在这项工作中，我们通过地下城与龙（Dungeons & Dragons）游戏的角度来探索偷听代理范式。我们提出了一项深入的研究，使用大型多模态音频语言模型作为偷听代理来协助地下城主。我们进行了一项人类评估来检验这些代理的有用性，发现一些大型音频语言模型具有利用隐含音频线索执行偷听代理任务的涌现能力。最后，我们将Python库和我们的项目代码发布到https://github.com/zhudotexe/overhearing_agents，以支持进一步研究偷听代理范式。**|
|**2025-05-27**|**Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming**|Yang Yang et.al.|[2505.21486](http://arxiv.org/abs/2505.21486)|null|在开放环境中自动生成鲁棒的假设对人工智能的认知至关重要。我们提出了一种新型框架，该框架将多智能体系统与归纳逻辑编程（ILP）相结合，并借助大型语言模型（LLMs）。我们的系统中的LLM智能体能够自主定义结构化的符号词汇（谓词）和关系模板，即从原始文本数据中直接生成“语言偏差”。这种自动化的符号化基础（即构建语言偏差）通常是ILP中专家驱动的瓶颈，然后引导将文本转换为事实，以便ILP求解器进行归纳学习可解释的规则。这种方法克服了传统ILP对预定义符号结构的依赖以及纯LLM方法的噪声敏感性。在多种不同、具有挑战性的场景中进行的大量实验验证了其卓越的性能，为自动化、可解释和可验证的假设生成开辟了新的途径。|
|**2025-05-27**|**Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration**|Zijun Liu et.al.|[2505.21471](http://arxiv.org/abs/2505.21471)|**[link](https://github.com/thunlp-mt/extagents)**|**随着推理和信息检索后训练技术的快速发展，大型语言模型（LLMs）可以整合大量检索到的知识来解决复杂任务。然而，LLMs受限的上下文窗口阻碍了外部知识输入量的扩展，限制了进一步的提升，尤其是在需要大量外部知识的任务中。现有的上下文窗口扩展方法不可避免地会导致信息损失。基于LLM的多智能体方法作为一种新的范式出现，用于以分布式的形式处理大量输入，其中我们识别出现有知识同步和推理过程中的两个核心瓶颈。在这项工作中，我们开发了一个多智能体框架，称为 $\textbf{ExtAgents}$，以克服这些瓶颈，并在不进行更长上下文训练的情况下，在推理时知识整合中实现更好的可扩展性。通过我们的增强型多跳问答测试$\textbf{$\boldsymbol{\infty}$Bench+}$ 以及其他包括长调查生成在内的公共测试集进行基准测试，ExtAgents显著提升了与现有非训练方法相同的输入外部知识量下的性能，无论这些知识是否在上下文窗口内或超出上下文窗口。此外，由于高并行性，该方法保持了高效率。进一步研究LLM智能体在增加外部知识输入方面的协调，将有助于现实世界的应用。**|
|**2025-05-27**|**Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery**|Lina Zhao et.al.|[2505.21418](http://arxiv.org/abs/2505.21418)|null|聚焦超声消融手术（FUAS）作为一种有前景的非侵入性治疗手段，因其安全性和精确性而备受重视。然而，其在临床应用中涉及诸如多模态图像解读、个性化剂量规划和实时术中决策等复杂任务，这些任务需要智能辅助以提高效率和可靠性。我们介绍了FUAS-Agents，这是一个利用大型语言模型（LLMs）的多模态理解和工具使用能力的自主智能体系统。通过整合患者资料和MRI数据，FUAS-Agents协调一系列专业医疗AI工具，包括分割、治疗剂量预测和临床指南检索，以生成包含MRI图像、剂量参数和治疗方案的个人化治疗计划。我们在子宫肌瘤治疗场景中评估了该系统。四位高级FUAS专家的人评估表明，82.5%、82.5%、87.5%和97.5%的计划在完整性、准确性、流畅性和临床符合度方面分别被评为4分或以上（5分制）。这些结果表明，由LLM驱动的智能体在增强复杂临床工作流程决策方面的潜力，并展示了一种将通用模型与专业专家系统相结合的转化范例，以解决垂直医疗领域的实际问题。|
|**2025-05-27**|**PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims**|Valentin Knappich et.al.|[2505.21342](http://arxiv.org/abs/2505.21342)|null|专利权利要求定义了发明的保护范围。如果权利要求中存在模糊性，专利局将予以驳回。在美国，这被称为不明确性（35 U.S.C. § 112(b)），是专利申请被驳回的最常见原因之一。开发自动化的专利明确性审查方法有可能使专利起草和审查更加高效，但截至目前尚未发布任何标注数据集。我们介绍了PEDANTIC（专利明确性审查语料库），这是一个包含14k项美国专利权利要求的语料库，这些权利要求来自与自然语言处理（NLP）相关的专利申请，并标注了不明确性的原因。我们使用一个完全自动化的流程构建了PEDANTIC，该流程从美国专利商标局（USPTO）检索官方行动文件，并使用大型语言模型（LLMs）提取不明确性的原因。一项人工验证研究表明，该流程在生成高质量标注方面的准确性得到了确认。为了超越二元分类指标，我们实施了一种LLM作为法官的评估，该评估比较了每个模型引用的原因与每个审查员引用的原因的自由形式推理。我们发现，基于Qwen 2.5 32B和72B的LLM代理在明确性预测方面难以超越逻辑回归基线，尽管它们经常正确地识别出潜在的原因。PEDANTIC为专利AI研究人员提供了一个宝贵的资源，使他们能够开发高级审查模型。我们将公开发布数据集和代码。|
|**2025-05-27**|**Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework**|Saman Marandi et.al.|[2505.21291](http://arxiv.org/abs/2505.21291)|null|在本文中，我们提出了一种新型诊断框架，该框架整合了知识图谱（KGs）和大型语言模型（LLMs），以支持高可靠性系统（如核电站）中的系统诊断。传统的诊断建模在系统变得过于复杂时面临挑战，使得功能建模成为一种更具吸引力的方法。我们的方法引入了一个基于动态主逻辑（DML）模型功能建模原则的诊断框架。它包含两个协调的LLM组件，包括一个基于LLM的工作流程，用于从系统文档中自动构建DML逻辑，以及一个促进交互式诊断的LLM代理。生成的逻辑被编码到一个结构化的知识图谱中，称为KG-DML，它支持分层故障推理。专家知识或操作数据也可以被整合，以细化模型的精确度和诊断深度。在交互阶段，用户提交自然语言查询，这些查询被LLM代理解释。代理选择适当的工具进行结构化推理，包括在KG-DML中的向上和向下传播。而不是将KG内容嵌入到每个提示中，LLM代理区分诊断和解释任务。对于诊断，代理选择并执行执行结构化KG推理的外部工具。对于一般查询，使用基于图的检索增强生成（Graph-RAG）方法，检索相关的KG片段并将它们嵌入到提示中生成自然解释。对一个辅助给水系统的案例研究证明了该框架的有效性，关键元素准确率超过90%，工具和论据提取一致，支持其在安全关键性诊断中的应用。|
|**2025-05-27**|**Agent-Environment Alignment via Automated Interface Generation**|Kaiming Liu et.al.|[2505.21055](http://arxiv.org/abs/2505.21055)|null|大型语言模型（LLM）代理在交互式决策任务中展现了令人印象深刻的推理能力。这些代理通过中间接口与环境交互，如预定义的动作空间和交互规则，这些规则调节感知和动作。然而，代理对其发出的动作影响的内部预期与环境中实际状态转换之间往往存在不匹配，这种现象被称为“代理-环境不匹配”。尽管先前的研究在改进代理策略和环境设计方面投入了大量精力，但接口的关键作用仍被低估。在本研究中，我们通过实证表明，代理-环境不匹配对代理性能构成了重大瓶颈。为了缓解这一问题，我们提出了一个名为ALIGN的自动对齐接口生成框架，通过丰富接口来减轻不匹配。具体来说，ALIGN生成的接口增强了环境的静态信息以及返回给代理的逐步观察。作为一个轻量级的包装器，这个接口实现了对齐，而无需修改代理逻辑或环境代码。在包括具身任务、网页导航和工具使用等多个领域的实验中，我们观察到一致的性能提升，在ALFWorld中成功率达到45.67%的提升。同时，ALIGN生成的接口可以在不同的代理架构和LLM主干网络上泛化，无需重新生成接口。代码和实验结果可在https://github.com/THUNLP-MT/ALIGN获取。|
|**2025-05-27**|**MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization**|Hyomin Kim et.al.|[2505.20820](http://arxiv.org/abs/2505.20820)|null|大型语言模型（LLMs）在分子优化方面具有巨大的潜力，因为它们可以收集外部化学工具并启用协作交互来迭代优化分子候选者。然而，这种潜力尚未得到充分探索，尤其是在结构推理、可解释性和全面工具支持的分子优化方面。为了解决这一差距，我们引入了MT-Mol，这是一个用于分子优化的多智能体框架，它利用工具引导的推理和角色专门的LLM智能体。我们的系统综合了RDKit工具，这些工具被分为五个不同的领域：结构描述符、电子和拓扑特征、基于片段的官能团、分子表示和杂项化学性质。每个类别都由一个专家分析师智能体管理，负责提取与任务相关的工具并实现可解释、基于化学的反馈。MT-Mol通过分析师智能体、分子生成科学家、推理输出验证者和评论员智能体之间的交互，以工具对齐和逐步推理的方式产生分子。因此，我们证明了我们的框架在23个任务中的17个任务上表现出了PMO-1K基准的最新性能。|
|**2025-05-27**|**Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective**|Krishna Singh Rajput et.al.|[2505.20816](http://arxiv.org/abs/2505.20816)|null|在多模态问答领域，最近的研究主要集中在结合异构模态或微调多模态大型语言模型上。虽然这些方法表现出了强大的性能，但它们通常依赖于单一、通用的推理策略，忽视了每种模态的独特特征，最终限制了准确性和可解释性。为了解决这些局限性，我们提出了MAMMQA，这是一个适用于跨文本、表格和图像等多模态输入的多智能体问答框架。我们的系统包括两个视觉语言模型（VLM）代理和一个基于文本的大型语言模型（LLM）代理。第一个VLM将用户查询分解为子问题，并依次从每个模态检索部分答案。第二个VLM通过跨模态推理综合和细化这些结果。最后，LLM将这些见解整合为一个连贯的答案。这种模块化设计通过使推理过程透明化来增强可解释性，并允许每个代理在其专业领域内运行。在多样化的多模态问答基准测试上的实验表明，我们的合作多智能体框架在准确性和鲁棒性方面均优于现有基线。|
|**2025-05-27**|**RRO: LLM Agent Optimization Through Rising Reward Trajectories**|Zilong Wang et.al.|[2505.20737](http://arxiv.org/abs/2505.20737)|null|大型语言模型（LLMs）在各种任务中表现出非凡的性能，但它们作为代理解决复杂多步骤任务仍然具有挑战性。在实际应用中，某些关键步骤的代理对结果敏感，这使得它们很可能因为规划轨迹中的微小错误而失败任务。最近的方法通过强化学习来校准推理过程。它们使用过程监督来奖励或惩罚每个推理步骤，这被称为过程奖励模型（PRMs）。然而，由于需要通过每一步轨迹探索来获取大量的训练数据，PRMs在具有大量下一个动作候选者的情境下难以扩展且成本高昂。为了解决这个问题，我们专注于连续推理步骤之间的相对奖励趋势，并提出在收集的轨迹中保持奖励不断增加的过程监督，我们称之为奖励上升优化（RRO）。具体来说，我们逐步增加过程监督，直到找到一个相对于其先前迭代显示出正向奖励差异的步骤，即上升的奖励。这种方法动态地扩展了下一个动作候选者的搜索空间，有效地捕获高质量的数据。我们在WebShop和InterCode-SQL基准测试上提供了数学基础和实验结果，表明我们提出的RRO实现了优异的性能，同时需要探索的成本要低得多。|
|**2025-05-27**|**SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution**|Hanlin Wang et.al.|[2505.20732](http://arxiv.org/abs/2505.20732)|**[link](https://github.com/wanghanlinhenry/spa-rl-agent)**|强化学习（RL）在训练长语言模型（LLM）代理以处理需要与外部环境进行多步交互的复杂、目标导向的任务方面具有巨大潜力。然而，将RL应用于这些代理任务时，一个关键挑战来自延迟奖励：反馈信号通常只有在整个任务完成后才能获得。这使得将延迟奖励分配给早期动作变得非同寻常，无法提供关于环境约束的充分指导，阻碍了代理的训练。在这项工作中，我们借鉴了这样一个观点：任务的最终完成是代理在各个步骤中累积进步的结果。我们提出了逐步进步归因（SPA），这是一个通用的奖励重新分配框架，将最终奖励分解为逐步贡献，每个贡献都反映了其向整体任务完成的增量进步。为了实现这一点，我们训练了一个进度估计器，该估计器通过轨迹累积逐步贡献以匹配任务完成。在策略优化过程中，我们将每步贡献与环境中执行的动作的归一化信号结合起来，作为有效代理训练的细粒度、中间奖励。在Webshop、ALFWorld和VirtualHome等常见代理基准上的大量实验表明，SPA在成功率（平均提高2.5%）和归一化准确性（平均提高1.9%）方面都一致优于最先进的方法。进一步的分析表明，我们的方法为RL训练提供了更有效的中间奖励。我们的代码可在https://github.com/WangHanLinHenry/SPA-RL-Agent上找到。|
|**2025-05-26**|**Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking**|Yihan Chen et.al.|[2505.20023](http://arxiv.org/abs/2505.20023)|null|随着大型语言模型（LLMs）的进步，感知环境和采取行动以实现目标的自主代理变得越来越可行。然而，当前强大的代理往往依赖于复杂的提示工程和闭源LLMs，如GPT-4。尽管使用教师模型的专家轨迹训练开源LLMs已经在代理能力上取得了一些改进，但这种方法仍然面临性能瓶颈和错误传播等限制。为了缓解这些挑战，我们提出了STeP，这是一种改进基于LLM的代理训练的新方法。我们综合了包含错误步骤反思和纠正的自我反思轨迹，这增强了LLM代理从教师模型中学习的有效性，使它们能够成为能够自我反思和纠正的代理。我们还引入了部分掩码策略，以防止LLM内化错误或次优步骤。实验表明，我们的方法在三个代表性任务（ALFWorld、WebShop和SciWorld）上提高了代理的性能。对于开源模型LLaMA2-7B-Chat，当使用Qwen1.5-110B-Chat作为教师模型构建的自我反思轨迹进行训练时，与仅使用专家轨迹训练的代理相比，它在更少的训练数据下实现了全面的改进。|
|**2025-05-26**|**EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM**|Shuang Ao et.al.|[2505.19905](http://arxiv.org/abs/2505.19905)|null|尽管大型语言模型（LLMs）在多个基于文本的推理和规划任务中表现出色，但其在机器人控制领域的应用受到以下重大缺陷的限制：（1）LLM智能体主要设计用于处理文本输入，而不是视觉条件；（2）当前的多模态智能体将LLMs视为静态规划器，这导致它们的推理与环境动态分离，从而产生的动作没有考虑特定领域的知识；（3）LLMs没有设计用于从视觉交互中学习，这使得它们更难为特定领域制定更好的策略。在本文中，我们介绍了EMAC+，这是一种通过双向训练范式协作集成LLM和VLM的具身多模态智能体。与现有方法不同，EMAC+通过从执行低级视觉控制任务的VLM获取的实时反馈，动态地细化LLM生成的高级文本计划。我们通过使LLM能够通过交互经验直接内化视觉环境动态，而不是仅仅依赖于静态符号映射，解决了先前模型的临界局限性。在ALFWorld和RT-1基准测试上的广泛实验评估表明，EMAC+实现了优越的任务性能、对噪声观测的鲁棒性以及高效的学习。我们还进行了彻底的消融研究，并提供了成功和失败案例的详细分析。|
|**2025-05-26**|**Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program**|Alejandro Carrasco et.al.|[2505.19896](http://arxiv.org/abs/2505.19896)|**[link](https://github.com/arclab-mit/kspdg)**|近期，大型语言模型（LLMs）作为根据用户文本提示内容采取行动的自主代理的使用趋势正在出现。我们旨在将这些概念应用于空间控制领域，使LLMs在自主卫星操作决策过程中发挥重要作用。为实现这一目标，我们开发了一个纯LLM解决方案，用于解决Kerbal Space Program微分博弈挑战（KSPDG），这是一个公开的软件设计竞赛，参赛者在该竞赛中为参与非合作空间操作的卫星创建自主代理，这些代理在KSP游戏引擎上运行。我们的方法利用提示工程、少样本提示和微调技术，创建了一个有效的基于LLM的代理，在竞赛中排名第二。据我们所知，这项工作开创了将LLM代理集成到空间研究中的先河。该项目包含几个开源仓库，以促进复制和进一步研究。代码库可在GitHub上访问，链接为\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}，而训练好的模型和数据集可在Hugging Face上获取，链接为\href{https://huggingface.co/OhhTuRnz}{Hugging Face}。此外，实验跟踪和详细结果可在\href{https://wandb.ai/carrusk/huggingface}{Weights \& Biases}上查看。|
|**2025-05-26**|**NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering**|Ruisheng Cao et.al.|[2505.19754](http://arxiv.org/abs/2505.19754)|null|随着学术论文数量的不断增加，研究人员在高效获取关键细节方面面临着重大挑战。虽然检索增强生成（RAG）在基于大型语言模型（LLM）的自动问答方面展现出巨大的潜力，但以往的研究往往忽略了神经检索和符号检索的互补优势。此外，传统的单视图分块方法忽视了PDF的丰富结构和布局，例如章节和表格。在本研究中，我们提出了NeuSym-RAG，这是一个混合神经符号检索框架，它在一个交互过程中结合了这两种范式。通过利用多视图分块和基于模式的解析，NeuSym-RAG将半结构化的PDF内容组织到关系数据库和向量存储中，使得LLM代理能够迭代地收集上下文，直到足够生成答案。在三个基于PDF的问答数据集上的实验，包括一个自标注的AIRQA-REAL，表明NeuSym-RAG稳定地击败了基于向量的RAG和各种结构化基线，突出了其统一两种检索方案并利用多个视图的能力。代码和数据在https://github.com/X-LANCE/NeuSym-RAG上公开可用。|
|**2025-05-26**|**AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems**|Yu Shang et.al.|[2505.19623](http://arxiv.org/abs/2505.19623)|null|基于大型语言模型（LLMs）的代理推荐系统的出现代表着个性化推荐领域的范式转变，利用LLMs的高级推理和角色扮演能力来实现自主、自适应的决策。与传统推荐方法不同，代理推荐系统可以动态地从复杂环境中收集和解释用户与物品的交互，生成适用于各种场景的稳健推荐策略。然而，该领域目前缺乏标准化的评估协议来系统地评估这些方法。为了解决这一关键差距，我们提出了以下建议：（1）一个交互式文本推荐模拟器，该模拟器包含丰富的用户和物品元数据以及三种典型的评估场景（经典、演变兴趣和冷启动推荐任务）；（2）一个统一的模块化框架，用于开发和研究代理推荐系统；（3）第一个比较10种经典和代理推荐方法的全面基准。我们的发现证明了代理系统的优越性，并为它们的核心组件提供了可操作的设计指南。基准环境已经通过一个公开挑战进行了严格验证，并保持公开可用，拥有持续维护的排行榜~[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html}，促进了持续的社区参与和可重复的研究。基准可在以下链接获取：\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}。|
|**2025-05-26**|**LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer**|Rasoul Zahedifar et.al.|[2505.19567](http://arxiv.org/abs/2505.19567)|null|本研究提出了一种名为LLM-Agent-Controller的多智能体大型语言模型（LLM）系统，旨在解决控制工程（控制理论）领域的一系列问题。该系统集成了一个中心控制器智能体和多个专门辅助智能体，负责控制器设计、模型表示、控制分析、时域响应和仿真等任务。一个监督者负责高级决策和工作流程协调，提高了系统的可靠性和效率。LLM-Agent-Controller集成了先进的特性，包括检索增强生成（RAG）、思维链推理、自我批评和纠正、高效的内存处理以及用户友好的自然语言通信。该系统设计为无需用户具备控制理论先验知识，使他们能够用普通语言输入问题并实时获得完整解决方案。为了评估该系统，我们提出了新的性能指标，评估单个智能体和整个系统。我们测试了控制理论五大类问题，并在三个高级LLM上进行基准性能测试。此外，我们还进行了一项涵盖所有关键服务的全面定性对话分析。结果显示，LLM-Agent-Controller成功解决了83%的一般任务，单个智能体的平均成功率达到了87%。性能随着更高级的LLM而提高。这项研究展示了多智能体LLM架构解决复杂、特定领域问题的潜力。通过整合专门智能体、监督控制和高级推理，LLM-Agent-Controller提供了一种可扩展、稳健且易于访问的解决方案框架，可以扩展到各个技术领域。|
|**2025-05-26**|**Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs**|Zhenhao Zhou et.al.|[2505.19489](http://arxiv.org/abs/2505.19489)|**[link](https://github.com/fudanselab/linuxflbench)**|Linux内核是一个关键系统，是众多系统的基石。Linux内核中的错误可能会带来严重的后果，影响数十亿用户。故障定位（FL），旨在识别软件中的错误代码元素，在软件质量保证中扮演着至关重要的角色。虽然最近的基于LLM的智能体在SWE-bench等最新基准测试中在FL方面取得了有希望的准确度，但这些方法在Linux内核中的表现仍然不明朗，因为Linux内核的故障定位由于庞大的代码库、有限的可见性和多样化的影响因素，要困难得多。在本文中，我们介绍了LinuxFLBench，这是一个从实际Linux内核错误中构建的FL基准。我们进行了一项实证研究，以评估最先进的LLM智能体在Linux内核上的性能。我们的初步结果表明，现有的智能体在完成这项任务时遇到困难，文件级别的最佳top-1准确率仅为41.6%。为了应对这一挑战，我们提出了LinuxFL $^+$，这是一个旨在提高LLM智能体在Linux内核中FL有效性的增强框架。LinuxFL$^+$ 显著提高了所有研究智能体的FL准确度（例如，准确率提高了7.2% - 11.2%），且成本极低。数据和代码可在https://github.com/FudanSELab/LinuxFLBench上找到。|
|**2025-05-26**|**VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning**|Maonan Wang et.al.|[2505.19486](http://arxiv.org/abs/2505.19486)|null|交通信号控制（TSC）是城市交通中的核心挑战，需要实时决策在效率和安全之间取得平衡。现有方法——从基于规则的启发式算法到强化学习（RL）——往往难以推广到复杂、动态和安全关键的场景。我们引入了VLMLight，这是一种新的TSC框架，它集成了视觉-语言元控制和双分支推理。VLMLight的核心是第一个基于图像的交通模拟器，它能够在交叉口实现多视角视觉感知，使策略能够对车辆类型、运动和空间密度等丰富线索进行推理。一个大型语言模型（LLM）作为安全优先级的元控制器，在常规交通中选择快速RL策略，以及在关键情况下选择结构化推理分支。在后一种情况下，多个LLM代理协作评估交通阶段，优先处理紧急车辆，并验证规则遵守情况。实验表明，与仅使用RL的系统相比，VLMLight将紧急车辆的等待时间减少了高达65%，同时在标准条件下保持了实时性能，且性能下降不到1%。VLMLight为下一代交通信号控制提供了一种可扩展、可解释和安全意识强的解决方案。|
|**2025-05-26**|**Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents**|Ye Ye et.al.|[2505.19436](http://arxiv.org/abs/2505.19436)|**[link](https://github.com/biubiutomato/tme-agent)**|大型语言模型（LLMs）在多步交互中表现不佳——常常出现幻觉、重复行为或误解用户纠正——这是由于依赖于线性、无结构的上下文。这种脆弱性源于缺乏持久记忆来跟踪不断变化的目标和任务依赖关系，从而损害了对自主代理的信任。我们引入了任务记忆引擎（TME），这是一个模块化内存控制器，可以将现有的LLMs转变为无需微调的鲁棒、可修订的代理。TME实现了一个空间记忆框架，用基于图的结构替换了平面上下文，以支持一致的多轮推理。TME不同于线性连接和ReAct风格的提示，它构建了一个动态任务图——要么是树，要么是有向无环图（DAG）——将用户输入映射到子任务，与先前上下文对齐，并启用依赖跟踪的修订。其任务表示和意图管理（TRIM）组件对任务语义和用户意图进行建模，以确保准确理解。在四个多轮场景——旅行规划、烹饪、会议安排和购物车编辑中——TME消除了三个任务中的100%的幻觉和误解，并在27轮用户交互中减少了66.7%的幻觉和83.3%的误解，优于ReAct。TME的模块化设计支持即插即用部署和特定领域的定制，适用于个人助手和企业自动化。我们将TME的代码库、基准和组件作为开源资源发布，使研究人员能够开发可靠的LLM代理。TME的可扩展架构解决了复杂交互环境中代理性能的关键差距。|
|**2025-05-26**|**Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression**|Peijie Dong et.al.|[2505.19433](http://arxiv.org/abs/2505.19433)|**[link](https://github.com/pprp/acbench)**|训练后压缩可以降低大型语言模型（LLMs）的计算和内存成本，从而实现资源高效的部署。然而，现有的压缩基准测试仅关注语言建模（例如，困惑度）和自然语言理解任务（例如，GLUE准确率），忽略了代理能力——工作流程、工具使用/函数调用、长文本理解和现实世界应用。我们引入了代理压缩基准（ACBench），这是第一个全面评估压缩对LLMs代理能力影响的标准。ACBench涵盖了（1）12个任务，分布在4种能力上（例如，WorfBench用于工作流程生成，Needle-in-Haystack用于长文本检索），（2）量化（GPTQ、AWQ）和剪枝（Wanda、SparseGPT），以及（3）15个模型，包括小型（Gemma-2B）、标准型（Qwen2.5 7B-32B）和蒸馏推理LLMs（DeepSeek-R1-Distill）。我们的实验揭示了压缩的权衡：4位量化保留了工作流程生成和工具使用（下降1%-3%），但将现实世界应用的准确率降低了10%-15%。我们引入了ERank、Top-k排序相关性和能耗来系统化分析。ACBench为优化代理场景中的LLM压缩提供了可操作的见解。代码可以在https://github.com/pprp/ACBench找到。|
|**2025-05-23**|**Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL**|Joey Hong et.al.|[2505.18098](http://arxiv.org/abs/2505.18098)|null|大型语言模型（LLMs）在问答和对话等任务上表现出色，但需要交互的复杂任务，如谈判和说服，则要求额外的长时程推理和规划。从原则上讲，强化学习（RL）微调可以实现这种规划，但存在阻碍可扩展性的缺点。特别是，多轮RL训练会产生高昂的内存和计算成本，当将LLMs作为策略进行训练时，这种成本会进一步加剧。此外，最大的LLMs没有公开必要的API来以这种方式进行训练。因此，现代提高LLMs推理能力的方法依赖于复杂的提示机制，而不是RL微调。为了解决这个问题，我们提出了一种新颖的方法，该方法使用目标条件值函数来引导LLM代理的推理，即使对于基于API的大规模模型也能进行扩展。这些值函数预测在给定一个动作的情况下任务将如何展开，允许LLM代理评估多种可能的正面和负面结果，以有效地进行规划。此外，这些值函数是在推理步骤上而不是在完整动作上训练的，以成为一个简洁且轻量级的模块，促进多轮交互中的决策。我们在需要交互的任务上验证了我们的方法，包括工具使用、社交推理和对话，证明了与RL微调和提示方法相比，我们的方法在保持效率和可扩展性的同时具有优越的性能。|
|**2025-05-23**|**DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors**|Tazeek Bin Abdur Rakib et.al.|[2505.17795](http://arxiv.org/abs/2505.17795)|null|大型语言模型（LLM）代理在反应式对话中表现出色，但在以目标为导向的主动交互中却因短视解码和昂贵的规划而表现不佳。我们引入了DialogXpert，该系统利用一个冻结的LLM在每个回合提出一组小而高质量的候选动作，并使用通过时间差学习训练的紧凑Q网络在固定的BERT嵌入上选择最优动作。通过跟踪用户的情绪，DialogXpert针对每个决策进行定制，以推进任务同时培养真挚、富有同理心的联系。在谈判、情感支持和辅导基准测试中，DialogXpert将对话推进到3回合以下，成功率超过94%，在更大的LLM先验知识下，成功率提升至97%以上，同时显著提高了谈判结果。该框架实现了大规模的实时、战略性和情感智能对话规划。代码可在https://github.com/declare-lab/dialogxpert/找到。|
|**2025-05-23**|**The Real Barrier to LLM Agent Usability is Agentic ROI**|Weiwen Liu et.al.|[2505.17767](http://arxiv.org/abs/2505.17767)|null|大型语言模型（LLM）智能体代表了人机交互的巨大进步，它超越了被动的提示-响应系统，成为能够推理、规划和有目标行动的自主智能体。尽管在需要专门知识和大量努力的特定任务，如编程和科学研究等领域得到了广泛应用，但我们强调在高需求、大众市场应用中存在一个关键的可用性差距。这篇立场论文认为，LLM智能体在现实世界中的有限采用不仅源于模型能力的不足，还源于智能体所能提供的价值与实际使用中产生的成本之间的基本权衡。因此，我们呼吁从仅仅优化模型性能转向更广泛的、以效用为导向的视角：通过智能体整体投资回报率（Agent ROI）的视角来评估智能体。通过确定决定Agent ROI的关键因素——信息质量、智能体时间和成本——我们提出了一种优化Agent ROI的波浪式发展轨迹：首先扩大规模以提高信息质量，然后缩小规模以最小化时间和成本。我们概述了不同发展阶段的发展路线图，旨在弥合当前的可用性差距，使LLM智能体在现实世界环境中真正具有可扩展性、可访问性和有效性。|
|**2025-05-23**|**Get Experience from Practice: LLM Agents with Record & Replay**|Erhu Feng et.al.|[2505.17716](http://arxiv.org/abs/2505.17716)|null|AI代理，通过大型语言模型（LLMs）和通信协议如MCP和A2A的赋能，已经迅速从简单的聊天机器人发展成为能够执行复杂、多步骤任务的自主实体，展现出巨大的潜力。然而，LLMs固有的不确定性和对大量计算资源的需求，给开发安全高效代理带来了四个重大挑战：可靠性、隐私、成本和性能。现有方法，如模型对齐、工作流程约束和设备端模型部署，可以部分缓解一些问题，但往往存在局限性，未能从根本上解决这些挑战。本文提出了一种新的范式，称为代理RR（代理记录与回放），该范式将经典的记录和回放机制引入到AI代理框架中。其核心思想是：1. 记录代理在任务执行过程中与环境及其内部决策过程的交互轨迹；2. 将此轨迹总结为结构化的“经验”，封装工作流程和约束；3. 在后续类似任务中回放这些经验以指导代理的行为。我们详细介绍了AgentRR中的多层次经验抽象方法和检查函数机制：前者平衡经验的特异性和普遍性，而后者作为信任锚，确保回放过程中的完整性和安全性。此外，我们探讨了AgentRR的多种应用模式，包括用户记录的任务演示、大模型与小模型协作以及隐私感知的代理执行，并展望了一个经验库，用于共享和重用知识，以进一步降低部署成本。|
|**2025-05-23**|**Simulating Macroeconomic Expectations using LLM Agents**|Jianhao Lin et.al.|[2505.17648](http://arxiv.org/abs/2505.17648)|null|我们提出了一种新颖的框架，用于模拟使用大型语言模型增强的代理（LLM代理）进行宏观经济预期形成。通过构建配备个人特征、先前预期和知识模块的数千个LLM代理，我们复制了一个涉及家庭和专家对通货膨胀和失业的调查实验。我们的结果表明，尽管LLM代理产生的预期和思想比人类参与者更趋于一致，但它们仍然有效地捕捉到了代理之间的关键异质性和预期形成的潜在驱动因素。此外，一项模块消除实验突出了先前预期在模拟这种异质性中的关键作用。这种方法补充了传统的调查方法，并为宏观经济研究中的AI行为科学提供了新的见解。|
|**2025-05-23**|**Distilling LLM Agent into Small Models with Retrieval and Code Tools**|Minki Kang et.al.|[2505.17612](http://arxiv.org/abs/2505.17612)|**[link](https://github.com/nardien/agent-distillation)**|大型语言模型（LLMs）在复杂推理任务上表现出色，但计算成本高昂，限制了其实际应用。为了解决这个问题，最近的研究工作主要集中在使用教师LLMs的思维链（CoT）跟踪将推理能力提炼到更小的语言模型（sLMs）中。然而，这种方法在需要罕见事实知识或精确计算的场景中表现不佳，因为sLMs由于能力有限，往往会虚构内容。在本工作中，我们提出了代理蒸馏（Agent Distillation），这是一个框架，可以将基于LLM的代理的推理能力和完整任务解决行为从具有检索和代码工具的sLMs中转移过来。我们沿着两个互补的轴改进了代理蒸馏：（1）我们引入了一种名为“首次思考前缀”的提示方法，以增强教师生成的轨迹质量；（2）我们提出了自洽动作生成，以提高小代理在测试时的鲁棒性。我们在事实和数学领域中的八个推理任务上评估了我们的方法，涵盖了领域内和领域外泛化。我们的结果表明，参数量仅为0.5B、1.5B、3B的sLMs可以实现与使用CoT蒸馏微调的下一级更大（1.5B、3B、7B）模型相当的性能，这证明了代理蒸馏在构建实用、使用工具的小型代理方面的潜力。我们的代码可在https://github.com/Nardien/agent-distillation上获得。|
|**2025-05-23**|**USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents**|Siqi Lai et.al.|[2505.17572](http://arxiv.org/abs/2505.17572)|null|大型语言模型（LLMs）在时空推理方面展现出巨大的潜力，使它们成为构建支持多种城市下游应用的城市场景代理的有力候选者。尽管存在这些优势，现有研究主要关注评估城市LLM代理在结果层面指标（例如，预测准确性、交通效率）上的表现，这限制了对其内在推理过程的深入了解。因此，城市LLM代理在时空推理方面的优势和局限性仍不清楚。为此，我们引入了USTBench，这是第一个评估LLMs作为城市代理在时空推理能力上的基准，它从四个分解维度进行评估：时空理解、预测、规划和反馈下的反思。具体来说，USTBench支持五种不同的城市决策和四种时空预测任务，所有这些任务都在我们构建的交互式城市环境UAgentEnv中运行。该基准包括62,466对结构化问答用于过程级评估和标准化的端到端任务评估，使得可以精细地诊断和广泛地比较不同城市场景下的任务。通过对十三种领先LLMs的广泛评估，我们发现虽然LLMs在各种城市下游任务中展现出良好的潜力，但在动态城市环境中的长期规划和反思适应方面仍然存在困难。值得注意的是，最近在通用逻辑或数学问题上进行训练的高级推理模型（例如，DeepSeek-R1）并不总是优于非推理LLMs。这一差异突出了针对城市时空推理进行领域特定适应性方法的需求。总的来说，USTBench为构建更适应和有效的基于LLM的城市代理和广泛的智能城市应用提供了基础。|
|**2025-05-23**|**MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning**|Yusheng Zhao et.al.|[2505.17481](http://arxiv.org/abs/2505.17481)|null|推理能力是大语言模型（LLMs）最基本的能力之一，通过复杂的解决问题方式，使LLMs能够执行广泛的下游任务。其中，代码推理是一个关键方面，它涉及使用形式语言（即编程代码）进行逻辑推理。在本文中，我们通过探索以下问题来增强LLMs的这种能力：LLMs代理如何通过提出的每个解决方案逐步提高代码推理的智能，从而实现显著的累积改进？大多数现有研究采取的是静态视角，侧重于使用冻结的LLMs进行孤立的问题解决。相比之下，我们采用认知进化视角，并提出了一种名为跨引用元反思（MARCO）的新框架，该框架使LLMs能够在推理过程中通过自我改进动态进化。从人类认知发展的角度来看，我们利用了知识积累和经验分享。特别是，为了在问题解决过程中积累知识，我们提出了元反思，通过反思当前问题的推理路径来获得未来考虑的知识和经验。此外，为了有效地利用其他代理的经验教训，我们提出了跨引用，将其他代理的解决方案和反馈纳入当前的问题解决过程中。我们在代码推理的各种数据集上进行了实验，结果表明MARCO的有效性。|
|**2025-05-22**|**Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine**|Adib Bazgir et.al.|[2505.16982](http://arxiv.org/abs/2505.16982)|null|大型语言模型（LLMs）在生物医学领域展现出潜力，但缺乏真正的因果理解，而是依赖于相关性。本文设想了因果LLMs智能体，它们可以整合多模态数据（文本、图像、基因组学等）并执行基于干预的推理，以推断因果关系。解决这个问题需要克服关键挑战：设计安全可控的智能体框架；开发因果评估的严格基准；整合异构数据源；以及协同结合LLMs、结构化知识（KGs）和形式化的因果推断工具。这样的智能体可以开启变革性的机遇，包括通过自动假设生成和模拟加速药物发现，通过针对特定患者的因果模型实现个性化医疗。这个研究议程旨在促进跨学科合作，将因果概念与基础模型连接起来，以开发可靠的人工智能伙伴，推动生物医学进步。|
|**2025-05-22**|**A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization**|Shengyu Feng et.al.|[2505.16952](http://arxiv.org/abs/2505.16952)|null|机器学习（ML）在支持组合优化（CO）问题的模型设计和优化方面展现了巨大的潜力。然而，迄今为止的大部分进展都是在小规模、合成的数据集上进行的，这引发了人们对基于ML求解器在现实世界大规模CO场景中的实际有效性的担忧。此外，许多现有的CO基准缺乏足够的训练数据，限制了它们在评估数据驱动方法方面的效用。为了解决这些限制，我们引入了FrontierCO，这是一个全面基准，涵盖了八种经典CO问题类型，并评估了16种代表性基于ML的求解器——包括图神经网络和大型语言模型（LLM）智能体。FrontierCO具有来自工业应用和前沿CO研究的挑战性实例，提供了真实的问题难度和丰富的训练数据。我们的实验结果为当前ML方法的优势和局限性提供了关键见解，有助于指导在机器学习和组合优化交叉领域的更稳健和更具实践相关性的进展。我们的数据可在https://huggingface.co/datasets/CO-Bench/FrontierCO获取。|
|**2025-05-22**|**Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks**|Hongyuan Tao et.al.|[2505.16901](http://arxiv.org/abs/2505.16901)|null|最近在大型语言模型（LLMs）方面的进展在功能级代码生成方面显示出潜力，但库级软件工程任务仍然具有挑战性。当前解决方案主要依赖于专有LLM代理，这引入了不可预测性并限制了可访问性，引发了关于数据隐私和模型定制化的担忧。本文探讨了开源LLMs是否能够在不采用代理方法的情况下有效解决库级任务。我们通过使LLMs能够通过其语义信息和结构依赖理解代码库中的功能和文件来证明这是可能的。为此，我们引入了代码图模型（CGMs），该模型将代码库的代码图结构集成到LLM的注意力机制中，并使用一个专门的适配器将节点属性映射到LLM的输入空间。结合无代理的图RAG框架，我们的方法在使用开源Qwen2.5-72B模型的情况下，在SWE-bench Lite基准测试中实现了43.00%的解决率。这一性能在开放权重模型中排名第一，在开源系统方法中排名第二，总排名第八，比之前的最佳开源模型方法提高了12.33%。|
|**2025-05-22**|**MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models**|Xuanqi Gao et.al.|[2505.16700](http://arxiv.org/abs/2505.16700)|null|随着大型语言模型（LLMs）从被动文本生成器转变为能够进行工具交互的主动推理代理，模型上下文协议（MCP）已成为动态工具发现和编排的标准框架。尽管该协议在工业界得到了广泛采用，但现有的评估方法无法充分评估在新范式下工具利用的能力。本文介绍了MCP-RADAR，这是第一个专为评估LLMs在MCP框架中性能的综合基准，采用一种新颖的五维方法来衡量：答案准确性、工具选择效率、计算资源效率、参数构建准确性以及执行速度。与依赖于主观人类评估或二进制成功指标的传统基准不同，MCP-RADAR在包括软件工程、数学推理和通用问题解决在内的多个任务领域采用了客观、可量化的测量。我们对领先的商业和开源LLMs进行的评估揭示了独特的性能特征，其中准确度、效率和速度之间存在重大权衡，挑战了传统的单一指标性能排名。此外，我们还为开发者提供了优化工具以实现最佳模型兼容性和有效性的宝贵指导。虽然我们的方法主要集中在MCP上，因为它具有标准化方法，但该方法适用于所有LLM代理工具集成框架，为LLM开发者和工具创建者提供了优化整个LLM-工具交互生态系统的宝贵见解。我们在评估中使用的实现、配置和数据集在https://anonymous.4open.science/r/MCPRadar-B143上公开可用。|
|**2025-05-22**|**O $^2$ -Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering**|Jianbiao Mei et.al.|[2505.16582](http://arxiv.org/abs/2505.16582)|**[link](https://github.com/acade-mate/o2-searcher)**|**大型语言模型（LLMs）虽然取得了进步，但由于其静态参数知识的基本限制，在需要开放域最新信息的任务上的表现受到阻碍。虽然使LLMs能够与外部知识环境交互是一个有前景的解决方案，但目前的努力主要针对封闭式问题。开放式问题，由于其缺乏标准答案或提供非唯一和多样化的答案，仍然没有得到充分探索。为了弥合这一差距，我们提出了O^2-Searcher，这是一个新型的搜索代理，利用强化学习有效地解决开放域中的开放式和封闭式问题。O^2-Searcher利用一个高效、局部模拟的搜索环境进行动态知识获取，有效地将外部世界知识从模型的复杂推理过程中分离出来。它采用了一种统一的训练机制，并精心设计了奖励函数，使代理能够识别问题类型并适应不同的答案生成策略。此外，为了评估在复杂开放式任务上的性能，我们构建了O^2-QA，这是一个高质量的基准，包含300个手动编写的、跨领域的开放式问题以及相关的网页缓存。大量实验表明，仅使用3B模型的O^2-Searcher在O^2-QA上显著优于领先的LLM代理。它在对标同样大小的模型的各种封闭式问答基准测试中实现了SOTA结果，同时与远大于其的模型表现相当。**|
|**2025-05-22**|**Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events**|Mengzhu Liu et.al.|[2505.16455](http://arxiv.org/abs/2505.16455)|null|在突发灾害事件中，准确预测社交媒体上的公众恐慌情绪对于主动治理和危机管理至关重要。当前针对这一问题的研究面临三个主要挑战：缺乏精细标注的数据阻碍了情感预测研究，未建模的风险感知导致预测不准确，以及对恐慌形成机制的解释性不足。我们通过提出一个基于情绪唤醒理论的心理学驱动生成代理框架（PsychoAgent）来解决这些问题，以实现可解释的恐慌预测。具体来说，我们首先通过人类-大型语言模型（LLM）协作构建了一个细粒度的开放恐慌情绪数据集（即COPE），以减轻语义偏差。然后，我们开发了一个框架，该框架基于心理学机制，整合了跨领域的异构数据，以建模风险感知和情绪生成中的认知差异。为了提高可解释性，我们设计了一个基于LLM的角色扮演代理，通过专门设计的提示来模拟个人心理链。在标注数据集上的实验结果表明，与基线模型相比，PsychoAgent将恐慌情绪预测性能提高了12.6%至21.7%。此外，我们方法的可解释性和泛化能力得到了验证。关键的是，这代表了从模糊的“数据驱动拟合”到透明的“基于角色的模拟与机制解释”的范式转变，用于紧急情况下的恐慌情绪预测。我们的实现代码在以下网址公开：https://anonymous.4open.science/r/PsychoAgent-19DD。|
|**2025-05-22**|**CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering**|Yuren Mao et.al.|[2505.16229](http://arxiv.org/abs/2505.16229)|null|CT扫描产生三维体积医学数据，这些数据可以看作是数百个横截面图像（又称切片），为诊断提供了详细的解剖信息。对于放射科医生来说，创建CT放射学报告既耗时又容易出错。迫切需要一种视觉问答（VQA）系统，能够回答放射科医生关于CT扫描上某些解剖区域的疑问，甚至能够自动生成放射学报告。然而，现有的VQA系统无法充分处理CT放射学问答（CTQA）任务，原因如下：（1）解剖复杂性使得CT图像难以理解；（2）数百个切片之间的空间关系难以捕捉。为了解决这些问题，本文提出了一种名为CT-Agent的多模态代理框架，用于CTQA。CT-Agent采用解剖独立的工具来分解解剖复杂性；此外，它通过全局-局部标记压缩策略有效地捕捉了跨切片的空间关系。在两个3D胸部CT数据集（CT-RATE和RadGenome-ChestCT）上的实验结果验证了CT-Agent的优越性能。|
|**2025-05-21**|**ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection**|Jeonghye Kim et.al.|[2505.15182](http://arxiv.org/abs/2505.15182)|null|近年来，在LLM代理方面的最新进展主要建立在ReAct这样的推理骨干之上，ReAct在复杂环境中将思考和行动交织在一起。然而，ReAct往往产生无根据或不连贯的推理步骤，导致代理的实际状态与目标之间出现偏差。我们的分析发现，这源于ReAct无法保持一致的内部信念和目标对齐，导致累积错误和幻觉。为了解决这个问题，我们引入了ReflAct，这是一种新颖的骨干，它将推理从仅仅规划下一步行动转变为对代理相对于其目标的状态的持续反思。通过在状态中明确地建立决策的基础并强制执行持续的目标对齐，ReflAct显著提高了战略可靠性。这种设计带来了实质性的实证收益：ReflAct在ALFWorld上的成功率平均比ReAct高出27.7%，达到93.3%。值得注意的是，即使在添加了增强模块（例如，Reflexion、WKM）的情况下，ReflAct甚至也优于ReAct，这表明加强核心推理骨干是可靠代理性能的关键。|
|**2025-05-21**|**lmgame-Bench: How Good are LLMs at Playing Games?**|Lanxiang Hu et.al.|[2505.15146](http://arxiv.org/abs/2505.15146)|**[link](https://github.com/lmgame-org/gamingagent)**|**玩视频游戏需要感知、记忆和规划能力，这正是现代大型语言模型（LLM）代理所应掌握的技能。我们研究了使用流行视频游戏来评估现代LLM的主要挑战，并发现直接将LLM投入游戏并不能进行有效的评估，原因有三：脆弱的视觉感知、对提示的敏感性以及潜在的数据污染。我们引入了lmgame-Bench，将游戏转化为可靠的评估工具。lmgame-Bench提供了一套平台游戏、益智游戏和叙事游戏，通过统一的Gym-style API提供，并配备了轻量级的感知和记忆框架，旨在稳定提示方差并消除污染。在13个领先模型中，我们展示了lmgame-Bench既具有挑战性，又能很好地区分模型。相关性分析表明，每个游戏都探测了一种独特的、通常在其他地方单独测试的能力组合。更有趣的是，在lmgame-Bench中的一个游戏中进行强化学习，既能转移到未见过的游戏中，也能转移到外部规划任务中。我们的评估代码可在https://github.com/lmgame-org/GamingAgent/lmgame-bench上找到。**|
|**2025-05-21**|**Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data**|Adib Bazgir et.al.|[2505.15132](http://arxiv.org/abs/2505.15132)|null|我们提出了一种多跨模态LLM-agent框架，该框架受材料科学数据日益增长的体积和多样性所启发，这些数据包括从高分辨率显微镜和动态模拟视频到表格实验日志和庞大的文献档案。尽管最近的人工智能努力加速了诸如性质预测或图像分类等个别任务，但它们通常孤立地处理每个模态，未能探索丰富的跨模态关联，并迫使研究人员进行繁琐的手动集成。此外，现有的多模态基础模型通常需要在领域数据上进行昂贵的重新训练或微调，而当前的材料信息学中的多智能体系统仅解决狭窄的子任务。为了克服这些障碍，我们设计了一支由专业LLM代理组成的协同团队，每个代理都配备了领域自适应的提示和插件，将这些输出投影到一个共享的嵌入空间中。随后，一个动态的门控机制对这些见解进行加权合并，使得能够在不修改底层LLM权重的条件下，对异构输入进行统一推理。我们在具有挑战性的案例研究中验证了我们的方法，与单模态和零样本基线相比，检索精度（85%）、字幕忠实度以及综合覆盖率（35%）均有显著提升。我们的工作为能够跨越数据孤岛并加速材料发现周期的AI数字研究人员铺平了道路。代码可在https://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent上找到。|
|**2025-05-21**|**An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents**|Bowen Jin et.al.|[2505.15117](http://arxiv.org/abs/2505.15117)|**[link](https://github.com/petergriffinjin/search-r1)**|**强化学习（RL）在训练能够进行复杂推理的大语言模型（LLMs）以解决现实世界问题方面展现出巨大的潜力。最近，强化学习被用来创建基于LLM的复杂搜索代理，这些代理能够巧妙地将推理与搜索引擎的使用相结合。虽然使用强化学习来训练搜索代理前景广阔，但这类代理的最优设计仍然不甚明了。特别是以下关键因素——（1）奖励公式，（2）底层LLM的选择和特性，（3）搜索引擎在强化学习过程中的作用——需要进一步研究。在本工作中，我们进行了全面的实证研究，系统地调查了这些问题，并提供了可操作的见解。我们强调了几个关键发现：格式化的奖励在提高最终性能方面是有效的，而中间检索奖励的影响有限；LLM的规模和初始化（通用型与推理专用型）对强化学习结果有显著影响；搜索引擎的选择在塑造强化学习训练动态和训练代理在推理过程中的鲁棒性方面发挥着关键作用。这些为成功构建和部署基于LLM的搜索代理在实际应用中提供了重要指导。代码可在https://github.com/PeterGriffinJin/Search-R1上找到。**|
|**2025-05-21**|**How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior**|Zidi Xiong et.al.|[2505.16067](http://arxiv.org/abs/2505.16067)|**[link](https://github.com/yuplin2333/agent_memory_manage)**|记忆是大语言模型（LLM）代理的关键组成部分，它使代理能够存储和检索过去的执行情况，从而随着时间的推移提高任务性能。在这篇论文中，我们进行了一项实证研究，探讨了内存管理选择如何影响LLM代理的行为，特别是它们的长远性能。具体来说，我们重点关注了两个广泛使用的代理框架的基本内存操作——添加，它将新经验纳入内存基础，以及删除，它有选择地移除过去的经验——以系统地研究它们对代理行为的影响。通过我们的定量分析，我们发现LLM代理表现出一种经验跟随特性：任务输入与检索到的内存记录输入之间的高度相似性往往导致高度相似的代理输出。我们的分析进一步揭示了与这一特性相关的两个重大挑战：错误传播，即过去经验中的不准确之处会累积并降低未来的性能，以及经验回放错位，即过时或不相关的经验会对当前任务产生负面影响。通过控制实验，我们表明结合选择性的添加和删除策略可以帮助减轻这些负面影响，与简单的内存增长相比，平均绝对性能提高了10%。此外，我们还强调了内存管理选择如何影响代理在任务分布变化和内存资源受限等挑战条件下的行为。我们的发现为LLM代理内存系统的行为动态提供了见解，并为设计支持稳健、长期代理性能的内存组件提供了实用指导。我们还发布了我们的代码，以促进进一步的研究。|
|**2025-05-21**|**InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation**|Yunjia Xi et.al.|[2505.15872](http://arxiv.org/abs/2505.15872)|null|检索增强生成（RAG）通过将检索到的信息应用于回答中，提高了大型语言模型（LLMs）的能力。作为一种新兴范式，代理RAG通过将自主的LLM代理引入信息检索过程进一步增强了这一过程。然而，现有的基准测试在评估这类系统时存在不足，因为它们局限于静态的检索环境，拥有固定且有限的语料库和简单的查询，这些查询无法引发代理行为。此外，它们的评估协议通过预先定义的文档金集来评估信息检索的有效性，这使得它们不适合现实世界中开放和动态的Web环境。为了弥合这一差距，我们提出了InfoDeepSeek，这是一个新的基准测试，其中包含旨在评估现实世界动态Web环境中代理信息检索的挑战性问题。我们提出了一种系统性的方法来构建满足确定性、难度和多样性标准的有挑战性的查询。基于此，我们开发了第一个针对动态代理信息检索的评估框架，包括关于信息检索结果准确性、实用性和紧凑性的细致指标。通过在LLMs、搜索引擎和问题类型上进行的广泛实验，InfoDeepSeek揭示了细微的代理行为，并为未来的研究提供了可操作的见解。|
|**2025-05-20**|**ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions**|Bufang Yang et.al.|[2505.14668](http://arxiv.org/abs/2505.14668)|null|近期大型语言模型（LLMs）的进步推动了智能代理从被动响应到主动支持的转变。虽然前景广阔，但现有的主动代理要么完全依赖封闭环境（例如桌面UI）的直接LLM推理，要么采用基于规则的主动通知，导致对用户意图的理解不佳，主动服务功能有限。在本文中，我们介绍了ContextAgent，这是第一个能够结合广泛感官环境来增强LLM代理主动能力的上下文感知主动代理。ContextAgent首先从可穿戴设备（如视频和音频）的大量感官感知中提取多维上下文，以理解用户意图。然后，ContextAgent利用感官上下文和历史数据中的人物上下文来预测主动服务的必要性。当需要主动协助时，ContextAgent会进一步自动调用必要的工具，以不干扰用户的方式协助。为了评估这项新任务，我们创建了ContextAgentBench，这是第一个用于评估上下文感知主动LLM代理的基准，涵盖九种日常场景和二十种工具，共计1,000个样本。在ContextAgentBench上的实验表明，ContextAgent在主动预测和工具调用方面分别比基线提高了高达8.5%和6.0%的准确率。我们希望我们的研究能够激发更先进、以人为本的主动AI助手的开发。|
|**2025-05-20**|**DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation**|He Wang et.al.|[2505.14163](http://arxiv.org/abs/2505.14163)|null|大型语言模型（LLM）代理在生成代码解决复杂数据科学问题方面表现出良好的性能。近期的研究主要集中于通过改进搜索、采样和规划技术来增强情境学习，但忽视了在推理过程中解决问题的顺序的重要性。在这项工作中，我们开发了一个新颖的推理时间优化框架，称为DS Mentor，该框架利用课程学习策略——即先引入简单的任务，随着学习者的进步逐渐过渡到更复杂的任务——来提升LLM代理在具有挑战性的数据科学任务中的性能。我们的导师引导框架按照难度递增的顺序组织数据科学任务，并整合不断增长的长时记忆来保留先前经验，指导代理的学习进程，并使积累的知识得到更有效的利用。我们通过在DSEval和QRData基准测试上的大量实验评估了DS Mentor。实验表明，与基线代理相比，使用Claude-3.5-Sonnet的DS Mentor在DSEval和QRData上提高了5.2%的通过率。此外，DS Mentor在因果问题上的因果推理能力更强，与使用Program-of-Thoughts提示的GPT-4相比，提高了8.8%的通过率。我们的工作强调了在推理过程中积累和利用知识的有效策略的重要性，反映了人类的学习过程，并通过基于课程推理优化的新途径，为提高LLM性能开辟了新的途径。|
|**2025-05-20**|**Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning**|Ruiyi Yang et.al.|[2505.13994](http://arxiv.org/abs/2505.13994)|null|检索增强生成（RAG）系统赋予大型语言模型（LLMs）外部知识，但在扩展到大型知识图时，往往在效率与准确性的权衡上遇到困难。现有方法通常依赖于单一图检索，导致简单查询的延迟不必要，以及复杂的多跳问题的推理碎片化。为了解决这些挑战，本文提出了一种名为SPLIT-RAG的多代理RAG框架，通过以问题驱动的语义图分区和协同子图检索来克服这些局限性。这个创新框架首先创建语义分区联接信息，然后利用类型专门的数据库实现多代理RAG。具有属性感知的图分割能够将知识图划分为语义上连贯的子图，确保子图与不同的查询类型相匹配，同时轻量级LLM代理被分配到分区后的子图中，检索过程中只激活相关分区，从而在减少搜索空间的同时提高效率。最后，通过逻辑验证的分层合并模块解决来自子图生成的答案中的一致性问题。广泛的实验验证表明，与现有方法相比，该框架有显著的改进。|
|**2025-05-20**|**The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents**|Mohammad Rubyet Islam et.al.|[2505.14727](http://arxiv.org/abs/2505.14727)|null|追求超越市场基准的alpha收益的投资方式经历了深刻的变革，从直觉驱动的投资演变为自主的、由人工智能驱动的系统。本文介绍了一种全面的五阶段分类法，该分类法追踪了从手动策略、统计模型、经典机器学习、深度学习到由大型语言模型（LLMs）驱动的代理架构这一进程。与以往仅关注建模技术的调查不同，这篇综述采用系统级视角，整合了表示学习、多模态数据融合和工具增强的LLM代理的进展。强调了从静态预测器到具有实时推理、情景模拟和跨模态决策能力的上下文感知金融代理的战略转变。考察了与生产部署相关的可解释性、数据脆弱性、治理和监管合规等关键挑战。所提出的分类法为评估成熟度、协调基础设施和指导下一代alpha系统的负责任发展提供了一个统一的框架。|
|**2025-05-19**|**From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents**|Liangxuan Wu et.al.|[2505.12981](http://arxiv.org/abs/2505.12981)|null|随着大型语言模型（LLMs）的日益普及，移动计算领域出现了一种新的范式——由LLM驱动的移动AI代理，能够在智能手机上直接分解和自动化复杂任务。然而，这些代理的安全影响尚未得到充分研究。在本文中，我们首次对移动LLM代理进行了全面的安全分析，涵盖了三个代表性类别：由原始设备制造商开发的系统级AI代理（例如YOYO助手）、第三方通用代理（例如智谱AI AutoGLM）和新兴的代理框架（例如阿里巴巴移动代理）。我们首先分析了移动代理的一般工作流程，并确定了三个核心能力维度上的安全威胁：基于语言的推理、基于GUI的交互和系统级执行。我们的分析揭示了11个不同的攻击面，所有这些攻击面都源于移动LLM代理的独特能力和交互模式，贯穿其整个操作生命周期。为了在实际中调查这些威胁，我们引入了AgentScan，这是一个半自动化的安全分析框架，系统地评估了移动LLM代理在所有11个攻击场景中的表现。将AgentScan应用于九个广泛部署的代理，我们发现了一个令人担忧的趋势：每个代理都容易受到针对性攻击。在最严重的情况下，代理在八个不同的攻击向量上表现出漏洞。这些攻击可能导致行为偏差、隐私泄露，甚至完全执行劫持。基于这些发现，我们提出了一系列防御性设计原则和实际建议，用于构建安全的移动LLM代理。我们的披露得到了两家主要设备供应商的积极反馈。总的来说，这项工作突出了在LLM驱动的移动自动化快速发展的领域中，标准化安全实践的紧迫需求。|
|**2025-05-19**|**The Traitors: Deception and Trust in Multi-Agent Language Model Simulations**|Pedro M. P. Curvo et.al.|[2505.12923](http://arxiv.org/abs/2505.12923)|**[link](https://github.com/pedrocurvo/thetraitors)**|**随着人工智能系统在需要信任和与人类价值观保持一致的角色中日益占据重要地位，了解它们何时以及为何进行欺骗已成为一个关键的研究重点。我们引入了《叛徒》，一个受社交推理游戏启发的多智能体模拟框架，旨在探索在大语言模型（LLM）智能体在不对称信息下的欺骗、信任形成和战略沟通。少数智能体（叛徒）试图误导大多数智能体，而忠诚的智能体必须通过对话和推理来推断隐藏的身份。我们的贡献包括：（1）我们将环境建立在博弈论、行为经济学和社会认知的正式框架之上；（2）我们开发了一套评估指标，用于捕捉欺骗成功、信任动态和集体推理质量；（3）我们实现了一个完全自主的模拟平台，其中LLM通过持久记忆和不断变化的社会动态进行推理，支持异构智能体群体、专用特性和自适应行为。我们在DeepSeek-V3、GPT-4o-mini和GPT-4o（每个模型10次运行）上的初步实验揭示了一个显著的不对称性：像GPT-4o这样的高级模型展现出卓越的欺骗能力，但同时也对其他人的谎言表现出不成比例的脆弱性。这表明欺骗技能可能比检测能力增长得更快。总的来说，《叛徒》为研究LLM在社会细腻互动中的行为提供了一个专注且可配置的测试平台。我们将这项工作定位为对欺骗机制、对齐挑战以及人工智能系统更广泛的社会可靠性方面的更严格研究的一项贡献。**|
|**2025-05-18**|**ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning**|Edward Y. Chang et.al.|[2505.12501](http://arxiv.org/abs/2505.12501)|null|大型语言模型（LLMs）在快速生成文本和多模态内容方面表现出色，但在需要类似ACID保证和实时故障恢复能力的交易式规划方面却表现不佳。我们提出了自适应LLM代理系统（ALAS），这是一个解决四个基本LLM缺陷的框架：（一）缺乏自我验证，（二）上下文侵蚀，（三）下一标记短视，（四）缺乏持久状态。ALAS将每个计划分解为具有特定角色的代理，为它们配备自动状态跟踪，并通过轻量级协议进行协调。当出现故障时，代理应用具有历史意识的局部补偿，避免昂贵的全局重新规划，并遏制级联效应。在真实世界、大规模的作业车间调度基准测试中，ALAS为静态顺序规划设定了新的最佳结果，并在意外故障的动态反应场景中表现出色。这些收益表明，基于原则的模块化加上有针对性的补偿可以解锁使用LLMs的可扩展和健壮的规划。|
|**2025-05-18**|**Automated Profile Inference with Language Model Agents**|Yuntao Du et.al.|[2505.12402](http://arxiv.org/abs/2505.12402)|**[link](https://github.com/zealscott/autoprofiler)**|在大型语言模型（LLMs）协作的基础上，自动化问题解决取得了令人瞩目的进展。然而，这些自动化能力也为恶意应用打开了途径。在本文中，我们研究了一种新的威胁，即LLMs对在线匿名性构成的威胁，称为自动化个人资料推断，其中攻击者可以指示LLMs自动抓取和从匿名平台上公开可见的用户活动提取敏感的个人属性。我们还介绍了一个名为AutoProfiler的自动化个人资料推断框架，以评估此类威胁在现实场景中的可行性。AutoProfiler由四个专门的大型语言模型代理组成，它们协同工作以收集和处理用户的在线活动，并生成包含提取的个人信息的个人资料。在两个现实世界数据集和一个合成数据集上的实验结果表明，AutoProfiler非常有效且高效，并且可以轻松地在网络规模上部署。我们证明了推断出的属性既敏感又可识别，存在隐私泄露的重大风险，如去匿名化和敏感信息泄露。此外，我们从不同角度探讨了缓解策略，并倡导提高公众对这种新兴的在线匿名性隐私威胁的认识。|
|**2025-05-18**|**LLM-DSE: Searching Accelerator Parameters with LLM Agents**|Hanyu Wang et.al.|[2505.12188](http://arxiv.org/abs/2505.12188)|**[link](https://github.com/nozidoali/llm-dse)**|尽管高级综合（HLS）工具通过提高抽象级别来减轻编程特定领域加速器（DSAs）的挑战，但优化硬件指令参数仍然是一个重大障碍。现有的启发式和学习方法在适应性和样本效率方面存在困难。我们提出了一种名为LLM-DSE的多智能体框架，专门用于优化HLS指令。将LLM与设计空间探索（DSE）相结合，我们的探索者协调了四个智能体：路由器、专家、仲裁者和评论家。这些多智能体组件通过与各种工具交互来加速优化过程。LLM-DSE利用必要的领域知识来识别高效的参数组合，同时通过在线交互中的语言学习来保持适应性。在HLSyn数据集上的评估表明，LLM-DSE比最先进的方法实现了显著的2.55倍性能提升，同时揭示了新的设计并减少了运行时间。消融研究验证了所提出的智能体交互的有效性和必要性。我们的代码已开源，可在以下链接找到：https://github.com/Nozidoali/LLM-DSE。|
|**2025-05-17**|**LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners**|Junhao Zheng et.al.|[2505.11942](http://arxiv.org/abs/2505.11942)|**[link](https://github.com/caixd-220529/LifelongAgentBench)**|终身学习对于在动态环境中运行的智能体至关重要。然而，当前基于大型语言模型（LLM）的智能体仍然是无状态的，无法随着时间的推移积累或转移知识。现有的基准将智能体视为静态系统，无法评估终身学习能力。我们提出了LifelongAgentBench，这是第一个旨在系统地评估LLM智能体终身学习能力的统一基准。它提供了三个互动环境（数据库、操作系统和知识图谱）中的技能基础、相互依赖的任务，具有自动标签验证、可重复性和模块化可扩展性。大量实验表明，由于无关信息和上下文长度限制，传统的经验回放对于LLM智能体效果有限。我们进一步引入了一种群组自洽机制，显著提高了终身学习性能。我们希望LifelongAgentBench能够推进自适应、具有记忆能力的LLM智能体的发展。|
|**2025-05-17**|**Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment**|Siliang Zeng et.al.|[2505.11821](http://arxiv.org/abs/2505.11821)|null|本文研究了利用强化学习（RL）增强大型语言模型（LLM）推理能力的方法。具体而言，我们关注多轮工具使用场景，这些场景可以自然地建模为马尔可夫决策过程（MDP）。虽然现有方法通常在伯努利环境中通过轨迹级优势估计来训练多轮LLM代理，但它们在多个决策步骤间的回合级信用分配方面存在困难，这限制了它们在多轮推理任务上的表现。为了解决这个问题，我们引入了一种细粒度的回合级优势估计策略，以实现多轮代理交互中更精确的信用分配。该策略是通用的，可以整合到各种RL算法中，如组相对偏好优化（GRPO）。我们在多轮推理和基于搜索的工具使用任务上的实验评估，使用了GRPO实现，突显了MDP框架和回合级信用分配在提升LLM代理在复杂决策环境中的多轮推理能力方面的有效性。我们的方法在工具执行上实现了100%的成功率，在精确答案匹配上达到了50%的准确率，显著优于无法调用工具且仅实现20-30%精确匹配准确率的基线。|
|**2025-05-17**|**Retrospex: Language Agent Meets Offline Reinforcement Learning Critic**|Yufei Xiang et.al.|[2505.11807](http://arxiv.org/abs/2505.11807)|**[link](https://github.com/Yufei-Xiang/Retrospex)**|大型语言模型（LLMs）具备广泛的知识和常识推理能力，这使得它们在创建强大智能体方面非常有价值。然而，现有的LLM智能体框架尚未充分利用过去经验进行改进。本研究介绍了一种新的基于LLM的智能体框架，称为Retrospex，通过深入分析过去经验来解决这一挑战。与之前的方法不同，Retrospex不是直接将经验整合到LLM的上下文中。相反，它将LLM的动作可能性与通过强化学习（RL）评价器估计的动作价值相结合，该评价器通过离线的“回顾”过程在过去的经验上训练。此外，Retrospex采用动态动作重新评分机制，对于需要更多与环境交互的任务，增加了基于经验的价值的重要性。我们在ScienceWorld、ALFWorld和Webshop环境中评估了Retrospex，证明了它在优于强大、当代基线的方法方面的优势。|
|**2025-05-16**|**Can AI automatically analyze public opinion? A LLM agents-based agentic pipeline for timely public opinion analysis**|Jing Liu et.al.|[2505.11401](http://arxiv.org/abs/2505.11401)|null|本研究提出并实施了一种基于大型语言模型（LLM）代理的多任务公共舆论分析管道，这是该领域的首个此类管道。与传统方法不同，它提供了一种端到端、完全自动化的分析工作流程，无需特定领域的训练数据、手动标注或本地部署。该管道将高级LLM功能集成到一个低成本、用户友好的框架中，适用于资源受限的环境。它通过单个自然语言查询实现及时、综合的公共舆论分析，使非专家用户也能使用。为了验证其有效性，该管道被应用于2025年中美关税争端的实际案例研究，分析了1,572篇微博帖子，并生成了一份结构化、多部分的报告。结果表明，公众舆论与政府决策之间存在某些关系。这些贡献代表了将生成式AI应用于公共治理的突破性进展，弥合了技术复杂性与公共舆论监测的实际可用性之间的差距。|
|**2025-05-16**|**GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents**|Lingxiao Diao et.al.|[2505.11368](http://arxiv.org/abs/2505.11368)|null|大型语言模型（LLMs）已被广泛部署为能够遵循用户指令并在现实应用中做出决策的自主代理。先前的研究在基准测试LLMs在一般领域的指令遵循能力方面取得了显著进展，主要关注它们的固有常识知识。最近，LLMs越来越多地被部署为面向领域的代理，这些代理依赖于可能与它们的常识知识冲突的面向领域的指南。这些指南具有两个关键特征：它们由广泛的面向领域的规则组成，并且经常更新。尽管存在这些挑战，但缺乏评估LLMs面向领域指南遵循能力的全面基准，这对它们的有效评估和进一步发展构成了重大障碍。在本文中，我们介绍了GuideBench，这是一个旨在评估LLMs指南遵循性能的全面基准。GuideBench从三个关键方面评估LLMs：（i）遵守各种规则，（ii）对规则更新的鲁棒性，（iii）与人类偏好的契合度。在一系列LLMs上的实验结果表明，它们在遵循面向领域指南的能力方面有巨大的改进空间。|
|**2025-05-16**|**From Intent Discovery to Recognition with Topic Modeling and Synthetic Data**|Aaron Rodrigues et.al.|[2505.11176](http://arxiv.org/abs/2505.11176)|null|理解和识别AI系统中的客户意图至关重要，尤其是在以简短话语和冷启动问题为特征的领域，推荐系统必须包含新产品或服务，而无需足够的真实用户数据。客户话语的特点是词语共现频率低和术语变化大，这对传统方法在指定不同用户需求和准备合成查询方面构成了重大挑战。为了解决这个问题，我们提出了一种代理型大型语言模型（LLM）框架，用于主题建模和合成查询生成，该框架加速了客户意图的发现和识别。我们首先应用层次主题建模和意图发现，将人工编纂的分类法从36个通用用户意图扩展到278个细粒度意图，证明了LLM在显著增强主题特异性和多样性方面的潜力。接下来，为了支持新发现的意图并解决冷启动问题，我们生成了合成用户查询数据，这增强了真实话语并减少了对人标注的依赖，尤其是在资源匮乏的环境中。主题模型实验表明，在主题扩展后，一致性和相关性有显著提高，而合成数据实验表明，在类内少样本提示下，合成查询的质量和效用显著提高，同时没有牺牲多样性。我们还表明，当用作合成查询生成的上下文时，LLM生成的意图描述和关键词可以有效地替代人工编纂的版本。我们的研究强调了LLM代理在主题建模中的可扩展性和实用性，并突出了使用合成话语来增强数据集变异性和覆盖范围以增强意图识别的战略性应用。我们提出了一种全面且稳健的框架，用于动态领域中新客户意图的在线发现和识别。|
|**2025-05-16**|**MPMA: Preference Manipulation Attack Against Model Context Protocol**|Zihan Wang et.al.|[2505.11154](http://arxiv.org/abs/2505.11154)|null|模型上下文协议（MCP）标准化了大型语言模型（LLM）访问外部数据和工具的接口映射，这颠覆了工具选择的范式，促进了LLM代理工具生态系统的快速扩张。然而，随着MCP的日益普及，第三方定制的MCP服务器暴露出潜在的安全漏洞。在本文中，我们首先介绍了一种新颖的安全威胁，我们称之为MCP偏好操纵攻击（MPMA）。攻击者部署定制的MCP服务器来操纵LLM，使其优先考虑它而不是其他竞争性的MCP服务器。这可能导致攻击者获得经济利益，例如付费MCP服务的收入或免费服务器产生的广告收入。为了实现MPMA，我们首先设计了一种直接偏好操纵攻击（ $\mathtt{DPMA}$），通过在工具名称和描述中插入操纵性词汇和短语，实现了显著的效果。然而，这种直接的修改对用户来说很明显，缺乏隐蔽性。为了解决这些局限性，我们进一步提出了一种基于遗传算法的广告偏好操纵攻击（$\mathtt{GAPMA}$）。$\mathtt{GAPMA}$采用了四种常用的策略来初始化描述，并集成了遗传算法（GA）以提高隐蔽性。实验结果表明，$\mathtt{GAPMA}$ 在高效性和隐蔽性之间取得了平衡。我们的研究揭示了MCP在开放生态系统中的一个关键漏洞，强调了确保MCP生态系统公平性的稳健防御机制的迫切需求。|
|**2025-05-16**|**Group-in-Group Policy Optimization for LLM Agent Training**|Lang Feng et.al.|[2505.10978](http://arxiv.org/abs/2505.10978)|**[link](https://github.com/langfengq/verl-agent)**|最近在基于群体的强化学习（RL）领域取得的进展推动了单轮任务如数学推理的前沿大型语言模型（LLM）。然而，它们在长周期LLM智能体训练中的可扩展性仍然有限。与静态任务不同，智能体与环境之间的交互需要在多个步骤中展开，并且通常会产生稀疏或延迟的奖励，这使得在单个步骤中进行信用分配变得更具挑战性。在这项工作中，我们提出了群组内群组策略优化（GiGPO），这是一种新颖的RL算法，它实现了对LLM智能体的细粒度信用分配，同时保留了基于群体的RL的吸引人特性：无批评家、低内存和稳定收敛。GiGPO引入了一种两级结构来估计相对优势：（i）在回合级别，GiGPO根据完整轨迹的组计算宏观相对优势；（ii）在步骤级别，GiGPO引入了一种锚定状态分组机制，通过识别轨迹中的重复环境状态来事后构建步骤级别的组。来自相同状态的动作被分组在一起，从而实现了微观相对优势的估计。这种分层结构有效地捕捉了全局轨迹质量和局部步骤有效性，而不依赖于辅助模型或额外的rollout。我们使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct在两个具有挑战性的智能体基准测试ALFWorld和WebShop上评估了GiGPO。关键的是，GiGPO提供了细粒度的每步信用信号，在ALFWorld上相对于GRPO基线实现了超过12%的性能提升，在WebShop上实现了超过9%的性能提升：所有这些同时保持了相同的GPU内存开销，相同的LLM rollout，并且几乎没有额外的时延成本。|
|**2025-05-16**|**Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents**|Jiaxing Zhao et.al.|[2505.10936](http://arxiv.org/abs/2505.10936)|null|大型语言模型（LLMs）在执行复杂推理任务方面表现出令人印象深刻的能力。思维链有效地通过释放大型模型潜力来增强推理能力，而多智能体系统通过整合多个智能体的集体智慧提供更全面的解决方案。然而，这两种方法都面临着重大的局限性。具有思维链的单智能体由于设计跨领域提示的固有复杂性，面临着协作挑战。同时，多智能体系统消耗大量令牌，不可避免地稀释了主要问题，这在商业工作流程任务中尤其成问题。为了解决这些挑战，我们提出了Cochain，一个协作提示框架，通过在降低成本的同时结合知识和提示，有效地解决了商业工作流程协作问题。具体来说，我们构建了一个综合知识图谱，其中包含了多个阶段的知识。此外，通过维护和检索提示树，我们可以获取与商业工作流程其他阶段相关的提示信息。我们在多个数据集上对Cochain进行了广泛的评估，证明Cochain在提示工程和多智能体LLMs方面都优于所有基线。此外，专家评估结果还表明，使用小型模型与Cochain的结合优于GPT-4。|
|**2025-05-16**|**Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing**|Kyudan Jung et.al.|[2505.11604](http://arxiv.org/abs/2505.11604)|**[link](https://github.com/KyuDan1/Talk-to-Your-Slides)**|尽管自动化幻灯片生成取得了显著进展，但编辑演示文稿仍然是每天数百万用户面临的最常见且耗时最多的任务之一。现有的方法已成功展示了通过基于图形用户界面（GUI）的代理进行幻灯片编辑，提供了直观的视觉控制。然而，这些方法往往遭受高计算成本和延迟的困扰。在本文中，我们提出了Talk-to-Your-Slides，这是一个由大型语言模型（LLM）驱动的代理，旨在通过利用有关幻灯片对象的标准化信息而非依赖图像模式，在活跃的PowerPoint会话中编辑幻灯片。我们工作的关键洞察是通过设计具有不同高级和低级层级的编辑过程，以促进用户命令与幻灯片对象之间的交互。通过直接访问应用程序对象而非屏幕像素，我们的系统实现了比基线快34.02%、指令保真度提高34.76%、操作成本降低87.42%的效果。为了评估幻灯片编辑能力，我们引入了TSBench，这是一个包含379个多样化编辑指令及其对应四个类别幻灯片变体的人标注数据集。我们的代码、基准和演示可在https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C上找到。|
|**2025-05-16**|**LLM Agents Are Hypersensitive to Nudges**|Manuel Cherep et.al.|[2505.11584](http://arxiv.org/abs/2505.11584)|null|大型语言模型（LLMs）被应用于涉及顺序决策和工具使用的复杂、真实世界环境中，经常需要代表人类用户做出选择。然而，关于这些选择的分布以及它们对不同选择架构的敏感度知之甚少。我们对几个此类LLM模型在一个多属性表格决策问题上进行了案例研究，研究内容包括标准启发式方法，如默认选项、建议和信息突出显示，以及额外的提示策略。我们发现，尽管这些模型在表面上与人类选择分布相似，但在细微但重要的方面存在差异。首先，它们对启发式的敏感度更高。其次，它们在获得分数上存在差异，受到诸如可用奖品的个性等因素的影响。第三，它们在信息获取策略上存在差异：例如，为了揭示过多信息而承担高昂的成本，或者在不揭示任何信息的情况下进行选择。此外，我们表明，简单的提示策略，如零样本思维链（CoT），可以改变选择分布，而带有人类数据的少量样本提示可以诱导更大的对齐。然而，这些方法都无法解决这些模型对启发式敏感的问题。最后，我们展示了如何使用人类资源理性模型优化的最佳启发式方法可以同样提高某些LLM的性能。所有这些发现都表明，在将这些模型作为在复杂环境中代表用户行为的代理或助手部署之前，需要进行行为测试。|
|**2025-05-15**|**AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents**|Julius Henke et.al.|[2505.10321](http://arxiv.org/abs/2505.10321)|**[link](https://github.com/juliushenke/autopentest)**|**近期研究热点之一是使用大型语言模型（LLMs）进行渗透测试，这有望降低成本，从而实现更高的测试频率。我们回顾了相关研究，确定了最佳实践和常见评估问题。随后，我们介绍了AutoPentest，这是一个执行高度自主的黑盒渗透测试的应用程序。AutoPentest基于OpenAI的LLM GPT-4o和LLM代理框架LangChain。它能够执行复杂的、多步骤的任务，并辅以外部工具和知识库。我们在三个Hack The Box（HTB）风格的夺旗机器上进行了研究，比较了我们的实现AutoPentest与手动使用ChatGPT-4o用户界面的基线方法。两种方法都能完成HTB机器上15-25%的子任务，AutoPentest略优于ChatGPT。我们测量了使用AutoPentest进行所有实验的总成本为96.20美元，而ChatGPT Plus的一个月订阅费用为20美元。结果表明，进一步的实现努力和未来发布的更强大的LLMs的使用，可能使这成为漏洞管理的一个可行部分。**|
|**2025-05-15**|**Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents**|Mrinal Rawat et.al.|[2505.09970](http://arxiv.org/abs/2505.09970)|null|大型语言模型（LLMs）中的ReAct（推理+行动）能力已成为现代智能系统的基石。最近的LLMs，如DeepSeek-R1和OpenAI o1/o3，通过强调通过生成大量的中间标记来进行推理，从而在生成最终输出标记之前构建强有力的前提，展示了这一点。在本文中，我们介绍了一种名为Pre-Act的新方法，通过为给定用户输入创建多步骤执行计划和详细的推理来提升智能体的性能。该计划逐步整合先前步骤和工具输出，在每一步执行后自我优化，直到获得最终响应。我们的方法适用于对话和非对话智能体。为了全面衡量面向任务的智能体的性能，我们提出了一种两级评估框架：（1）回合级和（2）端到端。我们的回合级评估，平均跨越五个模型，显示我们的方法Pre-Act在Almita数据集上的行动召回率上比ReAct高出70%。虽然这种方法对大型模型有效，但对于实际应用中至关重要的较小模型（其中延迟和成本是关键约束），它们在执行智能系统所需的复杂推理任务时往往表现不佳。为了解决这一限制，我们使用提出的Pre-Act方法对相对较小的模型，如Llama 3.1（8B & 70B）进行了微调。我们的实验表明，微调后的70B模型在Almita（域外）数据集上优于GPT-4，在行动准确率（回合级）上提高了69.5%，在目标完成率（端到端）上提高了28%。|
|**2025-05-15**|**Interpretable Risk Mitigation in LLM Agent Systems**|Jan Chojnacki et.al.|[2505.10670](http://arxiv.org/abs/2505.10670)|**[link](https://github.com/samsung/llm-agent-sae)**|基于大型语言模型（LLMs）的自主代理在责任行为日益重要的领域开启了新的应用场景。然而，LLMs的固有不可预测性引发了关于代理可靠性的安全问题。在这项工作中，我们探索了在一个基于迭代囚徒困境变体的玩具博弈论环境中的代理行为。我们引入了一种策略修改方法，该方法独立于游戏和提示，通过引导从稀疏自动编码器潜在空间中提取的可解释特征来引导残差流。使用基于诚信谈判的特征进行引导，将平均背叛概率降低了28个百分点。我们还确定了几个开源LLM代理的可行引导范围。最后，我们假设博弈论评估LLM代理，结合表示引导对齐，可以推广到终端用户设备和具身平台上的实际应用。|
|**2025-05-15**|**MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices**|Patara Trirat et.al.|[2505.10607](http://arxiv.org/abs/2505.10607)|**[link](https://github.com/kaist-dmlab/monaq)**|随着智能手机和物联网设备的日益普及，对资源受限的硬件进行高效的时间序列分析变得至关重要，这对于诸如人类活动识别和空气质量预测等感知应用至关重要。近期在硬件感知神经架构搜索（NAS）方面的努力自动化了针对特定平台的架构发现；然而，目前还没有针对具有边缘部署的一般时间序列分析的研究。利用大型语言模型（LLM）的问题解决和推理能力，我们提出了MONAQ，这是一个新颖的框架，将NAS重新定义为多目标神经架构查询任务。MONAQ配备了多模态查询生成功能，以处理多模态时间序列输入和硬件约束，以及基于LLM代理的多目标搜索，通过代码生成实现部署就绪的模型。通过整合数值数据、时间序列图像和文本描述，MONAQ提高了LLM对时间序列数据的理解。在十五个数据集上的实验表明，MONAQ发现的模型在效率和性能上都优于手工制作的模型和NAS基线。|
|**2025-05-14**|**WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models**|Abdullah Mushtaq et.al.|[2505.09595](http://arxiv.org/abs/2505.09595)|null|大型语言模型（LLMs）主要在强化以西方为中心的认识论和社会文化规范的方式下进行训练和调整，这导致了文化同质化并限制了它们反映全球文明多样性的能力。现有的基准评估框架未能充分捕捉这种偏见，因为它们依赖于僵化、封闭形式的评估，忽视了文化包容性的复杂性。为了解决这个问题，我们引入了WorldView-Bench，这是一个旨在通过分析LLMs容纳不同世界观的能力来评估全球文化包容性（GCI）的基准。我们的方法基于Senturk等人提出的多重世界观，它区分了强化文化同质化的单重模型和整合多元视角的多重模型。WorldView-Bench通过自由形式的生成评估而非传统分类基准来衡量文化极化，即排斥替代视角。我们通过两种干预策略实现应用多重性：（1）情境实施的多重LLMs，其中系统提示嵌入多重性原则；（2）多智能体系统（MAS）实施的多重LLMs，其中多个代表不同文化视角的LLM智能体共同生成响应。我们的结果表明，从基线时的13%显著增加到MAS实施的多重LLMs的94%，同时向积极情感（67.7%）和文化平衡性的增强转变。这些发现突显了多重意识AI评估在减轻LLMs文化偏见中的潜力，为更具包容性和道德一致性的AI系统铺平了道路。|
|**2025-05-14**|**The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners**|Vince Trencsenyi et.al.|[2505.09396](http://arxiv.org/abs/2505.09396)|null|大型语言模型（LLMs）的迅速崛起将人工智能（AI）研究转向了代理系统，促使人们使用更弱和更灵活的代理概念。然而，这种转变引发了关于基于LLM的代理在博弈论环境中所复制的人类战略推理程度的关键问题。在这种情况下，我们通过评估三种代理设计来检验代理复杂性在塑造人工推理者表现中的作用：一个简单的博弈论模型、一个非结构化的LLM作为代理的模型以及一个集成到传统代理框架中的LLM。使用猜测游戏作为测试平台，我们在一般推理模式和基于个人角色的目标上对这些代理与人类参与者进行了基准测试。此外，我们引入了模糊的游戏场景来评估代理在训练分布之外泛化的能力。我们的分析覆盖了超过2000个推理样本，涉及25种代理配置，表明以人类启发式认知结构可以增强LLM代理与人类战略行为的对齐。然而，代理设计复杂性与人类相似性之间的关系是非线性的，突显了对底层LLM能力的关键依赖，并暗示了简单架构增强的局限性。|
|**2025-05-14**|**Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"**|Pedro M. P. Curvo et.al.|[2505.09289](http://arxiv.org/abs/2505.09289)|**[link](https://github.com/giorgiopiatti/govsim)**|**这项研究评估和扩展了Piatti等人提出的成果，他们引入了GovSim，这是一个旨在评估大型语言模型（LLMs）在资源共享场景中的协作决策能力的模拟框架。通过复制关键实验，我们验证了关于大型模型（如GPT-4-turbo）相较于小型模型的性能的断言。同时，还考察了通用化原则的影响，结果显示，无论有无该原则，大型模型都能实现可持续的合作，而小型模型则无法在没有该原则的情况下实现。此外，我们还提供了多个扩展来探索该框架应用于新场景的适用性。我们评估了额外的模型，如DeepSeek-V3和GPT-4o-mini，以测试协作行为是否可以推广到不同的架构和模型规模。此外，我们还引入了新的场景：我们创建了一个异构多智能体环境，研究了一个使用日语指令的场景，并探索了一个“逆环境”，其中智能体必须合作以减轻有害资源分配。我们的结果表明，基准可以应用于新的模型、场景和语言，为LLMs在复杂协作任务中的适应性提供了有价值的见解。此外，涉及异构多智能体系统的实验表明，高性能模型可以影响低性能模型采取相似的行为。这一发现对其他基于智能体的应用具有重要意义，可能有助于更有效地利用计算资源，并促进更有效的协作AI系统的发展。**|
|**2025-05-13**|**Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models**|Yanggang Xu et.al.|[2505.08448](http://arxiv.org/abs/2505.08448)|null|在灾害场景中，建立稳健的应急通信网络至关重要，而无人机（UAV）为快速恢复连接提供了一种有希望的解决方案。然而，在大规模动态环境中组织无人机形成多跳网络面临着重大挑战，包括算法可扩展性的限制和协调决策所需的巨大探索空间。为了解决这些问题，我们提出了MRLMN，一个新颖的框架，该框架将多智能体强化学习（MARL）和大型语言模型（LLM）相结合，以共同优化无人机智能体以实现最佳网络性能。该框架采用分组策略和奖励分解来增强算法可扩展性，并平衡无人机之间的决策。此外，对选定的关键无人机应用行为约束，以提高网络的鲁棒性。进一步地，框架集成了LLM智能体，利用知识蒸馏将它们的高级决策能力转移到MARL智能体上。这提高了探索效率和整体训练过程。在蒸馏模块中，应用基于匈牙利算法的匹配方案来对齐LLM和MARL智能体的决策输出，并定义蒸馏损失。广泛的仿真结果验证了我们方法的有效性，证明了网络性能的显著改进，包括增强的覆盖范围和通信质量。|
|**2025-05-13**|**Benchmarking AI scientists in omics data-driven biological research**|Erpai Luo et.al.|[2505.08341](http://arxiv.org/abs/2505.08341)|**[link](https://github.com/eperluo/baisbench)**|**随着大型语言模型和多智能体系统的兴起，能够进行自主生物研究的AI科学家受到了AI科学家的广泛关注。然而，现有的基准要么关注无数据的推理，要么关注有预定义统计答案的数据分析，缺乏现实的数据驱动评估环境。在这里，我们介绍了生物AI科学家基准（BaisBench），这是一个旨在评估AI科学家通过数据分析和使用外部知识进行推理来生成生物学发现的基准。BaisBench包含两个任务：在31个专家标注的单细胞数据集上进行细胞类型注释，以及通过回答从41项近期单细胞研究生物学洞察中得出的198个多项选择题来进行科学发现。对最先进的AI科学家和LLM智能体进行的系统性实验表明，虽然前景广阔，但当前模型在这两个任务上仍然大大落后于人类专家。我们希望BaisBench能够填补这一空白，并作为推动和评估用于科学发现的AI模型的基础。基准可以在以下网址找到：https://github.com/EperLuo/BaisBench。**|
|**2025-05-12**|**MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering**|Rushi Qiang et.al.|[2505.07782](http://arxiv.org/abs/2505.07782)|**[link](https://github.com/MLE-Dojo/MLE-Dojo)**|**我们介绍了MLE-Dojo，这是一个类似Gym的框架，用于系统地强化学习、评估和改进在迭代机器学习工程（MLE）工作流程中的自主大型语言模型（LLM）智能体。与现有的主要依赖静态数据集或单次尝试评估的基准不同，MLE-Dojo提供了一个交互式环境，使智能体能够通过结构化的反馈循环迭代地实验、调试和优化解决方案。MLE-Dojo建立在200多个真实的Kaggle挑战赛之上，涵盖了精心挑选的多样化、开放式MLE任务，以反映现实工程场景，如数据处理、架构搜索、超参数调整和代码调试。其完全可执行的环境支持通过监督微调和强化学习进行全面的智能体训练，促进了迭代实验、现实数据采样和实时结果验证。对八种前沿LLM的广泛评估表明，尽管当前模型实现了有意义的迭代改进，但它们在自主生成长期解决方案和有效解决复杂错误方面仍然存在显著的局限性。此外，MLE-Dojo的灵活和可扩展的架构能够无缝集成不同的数据源、工具和评估协议，独特地支持基于模型的智能体调整，并促进了互操作性、可扩展性和可重复性。我们将我们的框架和基准开源，以促进社区驱动的创新，旨在开发下一代MLE智能体。**|
|**2025-05-12**|**Can Generative AI agents behave like humans? Evidence from laboratory market experiments**|R. Maria del Rio-Chanona et.al.|[2505.07457](http://arxiv.org/abs/2505.07457)|null|我们探讨了大型语言模型（LLMs）在经济市场实验中复制人类行为潜力。与以往研究相比，我们关注LLM代理之间的动态反馈：每个LLM的决定会影响当前步骤的市场价格，从而影响其他LLM在下一步骤的决定。我们将LLM的行为与实验室设置中观察到的市场动态进行比较，并评估其与人类参与者行为的一致性。我们的发现表明，LLMs并不严格遵循理性预期，而是表现出与人类参与者类似的有限理性。提供最小语境窗口，即三个之前时间步长的记忆，结合捕捉反应异质性的高变异性设置，使LLMs能够复制人类实验中观察到的广泛趋势，例如正负反馈市场的区别。然而，在细节层面上仍存在差异——LLMs的行为异质性低于人类。这些结果表明，LLMs有望成为模拟经济环境中真实人类行为的工具，尽管需要进一步研究以提高其准确性和增加行为多样性。|
|**2025-05-12**|**Are LLMs complicated ethical dilemma analyzers?**|Jiashen et.al.|[2505.08106](http://arxiv.org/abs/2505.08106)|**[link](https://github.com/alt-js/ethicallm)**|**在大型语言模型（LLMs）的研究中，一个开放的问题是它们是否能够模拟人类的伦理推理，并作为可信的人类判断代理。为了调查这一问题，我们引入了一个包含196个现实世界伦理困境和专家意见的基准数据集，每个困境和意见都被划分为五个结构化部分：引言、关键因素、历史理论观点、解决方案和关键要点。我们还收集了非专家人类的回应以进行比较，由于篇幅原因，仅限于关键因素部分。我们使用基于BLEU、Damerau-Levenshtein距离、TF-IDF余弦相似性和通用句子编码相似性的综合指标框架来评估多个前沿LLMs（GPT-4o-mini、Claude-3.5-Sonnet、Deepseek-V3、Gemini-1.5-Flash）。通过基于反转的排名对齐和成对层次分析法计算指标权重，使我们能够精细地比较模型输出与专家回应。我们的结果表明，LLMs在词汇和结构对齐方面通常优于非专家人类，其中GPT-4o-mini在所有部分中表现最为一致。然而，所有模型在历史背景和提出细致的解决方案方面都存在困难，这需要上下文抽象。虽然人类回应的结构性较差，但偶尔也能达到可比的语义相似度，这表明了直观的道德推理。这些发现突出了LLMs在伦理决策中的优势和当前局限性。**|
|**2025-05-09**|**AgentXploit: End-to-End Redteaming of Black-Box AI Agents**|Zhun Wang et.al.|[2505.05849](http://arxiv.org/abs/2505.05849)|null|大型语言模型（LLMs）强大的规划和推理能力促进了基于代理系统的开发，这些系统能够利用外部工具并与其他日益复杂的环境互动。然而，这些强大的功能也引入了一个关键的安全风险：间接提示注入，这是一种复杂的攻击向量，通过操纵上下文信息而非直接用户提示来破坏这些代理的核心，即LLM。在本工作中，我们提出了一种通用的黑盒模糊测试框架，名为AgentXploit，旨在自动发现和利用不同LLM代理中的间接提示注入漏洞。我们的方法首先构建一个高质量的初始种子语料库，然后使用基于蒙特卡洛树搜索（MCTS）的种子选择算法来迭代优化输入，从而最大限度地提高发现代理弱点的可能性。我们在两个公开基准测试AgentDojo和VWA-adv上评估了AgentXploit，分别针对o3-mini和GPT-4o的代理，实现了71%和70%的成功率，几乎将基线攻击的性能翻倍。此外，AgentXploit在未见过的任务和内部LLM之间表现出强大的迁移能力，以及对抗防御措施的积极结果。除了基准测试评估之外，我们还将在真实环境中应用我们的攻击，成功误导代理导航到任意URL，包括恶意网站。|
|**2025-05-09**|**ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents**|Elias Lumer et.al.|[2505.06416](http://arxiv.org/abs/2505.06416)|null|近年来，大型语言模型（LLMs）的进步以及模型上下文协议（MCP）的引入，显著增强了LLM代理与外部工具和API动态交互的能力。然而，现有的工具选择框架没有整合MCP服务器，而是严重依赖对单体本地工具库的易出错的手动更新，导致重复、不一致和低效。此外，当前的方法在LLM代理被调用之前就抽象了工具选择，限制了其自主性，并阻碍了在多轮交互中的动态重新查询能力。为了解决这些问题，我们引入了ScaleMCP，这是一种新颖的工具选择方法，它动态地为LLM代理配备了一个MCP工具检索器，使代理能够自主地将工具添加到其记忆中，并通过与MCP服务器进行CRUD（创建、读取、更新、删除）操作，实现一个自动同步的工具存储系统管道，MCP服务器作为单一的真实来源。我们还提出了一种新颖的嵌入策略，即工具文档加权平均（TDWA），旨在在嵌入过程中有选择性地强调工具文档的关键组件（例如工具名称或合成问题）。在创建的包含5,000个金融指标MCP服务器数据集上进行的全面评估，涵盖了10个LLM模型、5个嵌入模型和5种检索器类型，证明了在工具检索和代理调用性能方面有显著提升，强调了ScaleMCP在可扩展、动态工具选择和调用方面的有效性。|
|**2025-05-08**|**Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs**|Jeffrey Basoah et.al.|[2505.05660](http://arxiv.org/abs/2505.05660)|null|随着大型语言模型（LLMs）越来越多地适应和满足不同用户的需求，系统模仿特定少数群体语言风格或方言（例如，非裔美国人英语、酷儿俚语）的风险也在增加。在这项工作中，我们研究了LLM代理的社会方言使用是否会影响用户对其输出的依赖程度和用户感知（满意度、挫败感、信任和社交存在感）。我们设计并实施了一项用户研究，其中498名非裔美国人英语（AAE）使用者和487名酷儿俚语使用者使用LLM建议在标准美国英语（SAE）或他们自我认定的社会方言中完成一组问答任务。我们的发现表明，LLMs的社会方言使用对依赖和感知都有影响，尽管方式出人意料。结果表明，AAE和酷儿俚语使用者都更依赖SAE代理，并对SAE代理有更积极的感知。然而，只有酷儿俚语使用者觉得酷儿俚语代理比SAE代理有更强的社交存在感，而只有AAE使用者更喜欢并信任SAE代理而不是AAE代理。这些发现强调了测试行为结果而不是简单地假设个性化会导致更好和更安全的依赖结果的重要性。它们还突出了机器交互中少数群体语言的细微动态，强调了LLMs需要精心设计，以尊重文化和语言界限，同时促进真正的用户参与和信任。|
|**2025-05-07**|**Identification and Optimization of Redundant Code Using Large Language Models**|Shamse Tasnim Cynthia et.al.|[2505.04040](http://arxiv.org/abs/2505.04040)|null|冗余代码是软件开发中一个长期存在的挑战，使得系统更难维护、扩展和更新。它增加了不必要的复杂性，阻碍了错误修复，并增加了技术债务。尽管其影响明显，但手动删除冗余代码具有风险且易出错，常常引入新的错误或遗漏依赖。尽管研究强调了冗余代码的普遍性和负面影响，但很少关注人工智能（AI）系统代码库中引起冗余的常见模式。此外，开发者无意中引入冗余代码的原因也基本未被探讨。本研究通过利用大型语言模型（LLMs）在AI项目中自动检测和优化冗余代码来填补这些空白。我们的研究旨在识别冗余的重复模式并分析其潜在原因，例如过时的实践或对最佳编码原则了解不足。此外，我们计划提出一个LLM代理，该代理将有助于大规模检测和重构冗余，同时保留原始功能。这项工作推进了人工智能在识别和优化冗余代码中的应用，最终帮助开发者维护更干净、更易读和可扩展的代码库。|
|**2025-05-07**|**Large Language Models are Autonomous Cyber Defenders**|Sebastián R. Castro et.al.|[2505.04843](http://arxiv.org/abs/2505.04843)|**[link](https://github.com/r4wd3r/llms-are-acd)**|**快速有效的应急响应对于防止敌对网络攻击至关重要。自主网络防御（ACD）旨在通过人工智能（AI）代理自动化应急响应，这些代理负责规划和执行行动。大多数ACD方法侧重于单代理场景，并利用强化学习（RL）。然而，ACD RL训练的代理依赖于昂贵的训练，且其推理并不总是可解释或可迁移。大型语言模型（LLMs）可以通过在一般安全环境中提供可解释的行动来解决这些担忧。研究人员已经探索了LLM代理在ACD中的应用，但尚未在多代理场景或与其他ACD代理交互的情况下进行评估。在本文中，我们通过提出一个新集成到CybORG CAGE 4环境中，展示了关于LLMs在多代理ACD环境中表现的第一项研究。我们通过提出一种新的通信协议来探讨LLM和RL代理组成的ACD团队如何互动。我们的结果突显了LLMs和RL的优势和劣势，并帮助我们确定创建、训练和部署未来ACD代理团队的潜在研究方向。**|
|**2025-05-06**|**Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale**|Jiale Liu et.al.|[2505.03973](http://arxiv.org/abs/2505.03973)|null|基于LLM的优化在提升智能体系统方面展现出巨大的潜力。然而，随着数据集的扩大，将整个训练轨迹一次性输入到LLM优化器中的传统方法变得不可行，导致上下文窗口溢出和模式识别能力下降。为了解决这些挑战，我们提出了细粒度优化（FGO），这是一个可扩展的框架，它将大型优化任务划分为可管理的子集，进行针对性的优化，并通过逐步合并系统性地结合优化组件。在ALFWorld、LogisticsQA和GAIA基准上的评估表明，FGO比现有方法提高了1.6-8.6%，同时平均提示标记消耗减少了56.3%。我们的框架为扩展基于LLM的复杂智能体系统的优化提供了实用的解决方案。进一步的分析表明，FGO在所有训练数据集大小上实现了最一致的性能提升，展示了其可扩展性和效率。|
|**2025-05-06**|**The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete**|Gerrit Großmann et.al.|[2505.03961](http://arxiv.org/abs/2505.03961)|**[link](https://github.com/storyagents25/story-agents)**|**根据尤瓦尔·赫拉利的观点，大规模的人类合作是由编码共同信念和价值观的共享叙事推动的。本研究探讨了这样的叙事是否可以同样推动LLM代理进行合作。我们使用了一个有限重复的公共物品博弈，其中LLM代理可以选择合作或自私的消费策略。我们通过不同程度上强调团队合作的故事来启发代理，并测试这如何影响谈判结果。我们的实验探讨了四个问题：(1)叙事如何影响谈判行为？(2)当代理共享相同的故事与不同故事时，有什么不同？(3)当代理数量增加时会发生什么？(4)代理是否能抵抗自私的谈判者？我们发现基于故事的前置启发显著影响了谈判策略和成功率。共同的故事改善了合作，使每个代理受益。相比之下，用不同故事启发代理会逆转这种效果，那些被启发追求私利的代理占上风。我们假设这些结果对多代理系统设计和AI对齐有影响。**|
|**2025-05-04**|**Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants**|Milapji Singh Gill et.al.|[2505.02076](http://arxiv.org/abs/2505.02076)|**[link](https://github.com/aisl-at-imperial-college-london/fault-handling-agentic-llms-for-controlled-operations)**|**自动化和人工智能的进步不断提升了过程工厂处理各种操作场景的自主性。然而，某些任务，如故障处理，仍然具有挑战性，因为它们高度依赖人类专业知识。这突显了系统化、基于知识的方法的必要性。为了解决这一差距，我们提出了一种方法论框架，该框架将大型语言模型（LLM）代理与数字孪生环境相结合。LLM代理持续解释系统状态并启动控制动作，包括对意外故障的响应，目的是使系统恢复正常运行。在这种情况下，数字孪生既作为代理提示的特定于工厂的工程知识的结构化存储库，也作为生成纠正控制动作的系统化验证和验证的仿真平台。使用过程工厂的混合模块进行的评估表明，所提出的框架不仅能够自主控制混合模块，而且能够在仅经过几次提示的情况下生成有效的纠正动作来缓解管道堵塞。**|
|**2025-05-03**|**Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents**|Zongxi Liu et.al.|[2505.01834](http://arxiv.org/abs/2505.01834)|null|大型语言模型（LLMs）展现出强大的通用推理能力，但由于缺乏原生感官输入和特定领域的先验知识，无法获取无线环境信息。先前尝试将LLMs应用于无线系统的方法，要么依赖于使用特定网络数据重新训练，这会损害语言泛化能力，要么依赖于手动编写的接口，这阻碍了可扩展性。为了克服这些限制，我们提出了一种基于模型上下文协议（MCP）的专家网络（IoX）框架，该框架为LLMs配备了无线环境感知推理能力。该框架包含一系列轻量级专家模型，每个模型都经过训练以解决无线通信中的特定确定性任务，例如检测特定的无线属性，例如视距传播、多普勒效应或衰落条件。通过MCP，LLM可以在推理时选择性地查询和解释专家输出，而无需修改其自身参数。这种架构实现了无线上下文中的模块化、可扩展和可解释的推理。在多个主流LLMs上进行评估，所提出的无线环境感知LLM代理在分类任务中相对于仅使用LLM的基线实现了40%-50%的改进。更广泛地说，基于MCP的设计为未来LLMs继承结构化无线网络管理能力提供了一种可行的范式。|
|**2025-05-02**|**VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language**|Sijin Sun et.al.|[2505.00989](http://arxiv.org/abs/2505.00989)|null|船舶交通服务（VTS）对于海事安全和合规至关重要，通过实时交通管理来实现。然而，随着交通复杂性的增加和异构、多模态数据的普遍存在，现有的VTS系统在时空推理和直观的人机交互方面面临局限性。在这项工作中，我们提出了VTS-LLM Agent，这是第一个针对VTS操作中交互式决策支持的领域自适应大型LLM代理。我们将风险易发船舶识别形式化为知识增强的文本到SQL任务，结合结构化船舶数据库和外部海事知识。为此，我们构建了一个经过精心挑选的基准数据集，包括自定义模式、特定领域的语料库以及多种语言风格的查询-SQL测试集。我们的框架集成了基于NER的关系推理、基于代理的领域知识注入、语义代数中间表示和查询重想机制，以增强领域扎根和上下文感知理解。实验结果表明，VTS-LLM在命令式、操作式和正式自然语言查询下，分别优于通用和SQL专注的基线。此外，我们的分析提供了第一项实证证据，表明语言风格的变化在文本到SQL建模中引入了系统性的性能挑战。这项工作为船舶交通服务中的自然语言界面奠定了基础，并为主动、LLM驱动的海事实时交通管理开辟了新的机遇。|
|**2025-05-02**|**Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models**|Yuewen Mei et.al.|[2505.00972](http://arxiv.org/abs/2505.00972)|null|基于模拟的测试对于验证自动驾驶汽车至关重要，然而现有的场景生成方法要么过度拟合常见的驾驶模式，要么以离线、非交互的方式运行，无法暴露罕见的、安全关键的特殊情况。在本文中，我们介绍了一种在线、检索增强的大型语言模型（LLM）框架，用于生成安全关键驾驶场景。我们的方法首先使用基于LLM的行为分析器从观察到的状态中推断背景车辆的潜在危险意图，然后查询额外的LLM代理来合成可行的对抗性轨迹。为了减轻灾难性遗忘并加速适应，我们通过动态记忆和检索意图-规划器对，增强框架，当出现新颖意图时，自动扩展其行为库。使用Waymo开放运动数据集进行的评估表明，我们的模型将平均最小碰撞时间从1.62秒降低到1.08秒，碰撞率降至75%，显著优于基线。|
|**2025-05-01**|**UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces**|Alaa Saleh et.al.|[2505.00472](http://arxiv.org/abs/2505.00472)|null|代理型人工智能，凭借其自主和主动的决策能力，已经改变了智能环境。通过整合生成型人工智能（GenAI）和多代理系统，现代人工智能框架能够动态适应用户偏好，优化数据管理，并提高资源分配。本文介绍了UserCentrix，这是一个旨在通过动态、情境感知的决策来增强智能空间的代理型记忆增强人工智能框架。该框架集成了个性化大型语言模型（LLM）代理，这些代理利用用户偏好和LLM记忆管理来提供主动和适应性帮助。此外，它还包含一个混合分层控制系统，在保持全局态势感知的同时，平衡集中式和分布式处理以优化实时响应。UserCentrix通过嵌入记忆增强推理、合作代理协商和适应性编排策略来实现资源高效的AI交互。我们的主要贡献包括：（一）一个基于任务紧急程度的自组织框架，具有主动扩展功能；（二）一个由信息价值（VoI）驱动的决策过程；（三）一个元推理个性化LLM代理；（四）一个用于无缝环境适应的智能多代理协调系统。在各种模型上的实验结果证实了我们的方法在增强响应准确性、系统效率和计算资源管理方面的有效性。|
|**2025-05-01**|**Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks**|Vishnu Sarukkai et.al.|[2505.00234](http://arxiv.org/abs/2505.00234)|null|许多用于改进大型语言模型（LLM）在序列决策任务中表现的方法都依赖于特定任务的知识工程，例如提示调整、精选的上下文示例或定制的观察和动作空间。通过这些方法，代理的性能随着投入的知识工程的质量或数量的提高而提高。相反，我们研究了LLM代理如何通过从自己在类似任务上的成功经验中学习上下文来自动提高其性能。我们不是依赖于特定任务的知识工程，而是专注于构建和改进一个自生成示例数据库。我们证明，即使在训练任务中简单地将成功的轨迹积累起来，也能在三个基准测试（ALFWorld从73%提高到89%，Wordcraft从55%提高到64%，InterCode-SQL从75%提高到79%）上提升测试性能——这与如果允许代理在每个任务上尝试两次到三次时初始代理达到的性能相匹配。然后，我们引入了两个扩展：（1）通过基于群体的训练在数据库级别进行选择，以识别高性能的示例集合；（2）基于示例级别的选择，根据它们作为上下文示例的经验效用保留个别轨迹。这些扩展进一步提高了性能，在ALFWorld上达到了91%——与采用特定任务组件和提示的更复杂方法相匹配。我们的结果表明，自动轨迹数据库构建为劳动密集型知识工程提供了一种有吸引力的替代方案。|
|**2025-05-01**|**HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models**|Zhaoxing Li et.al.|[2505.00820](http://arxiv.org/abs/2505.00820)|null|人工智能（AI）的快速发展使机器人能够以越来越高的精度自主执行复杂任务。然而，多机器人系统（MRSs）在泛化、异构性和安全性方面面临挑战，尤其是在扩展到大规模部署如灾害响应时。传统方法往往缺乏泛化能力，需要大量工程投入来应对新任务和场景，且难以管理多样化的机器人。为了克服这些局限性，我们提出了一种由大型语言模型（LLMs）驱动的闭环人机协作框架（HMCF）。LLMs通过推理各种任务和机器人能力来提高适应性，而人类监督确保了安全性和可靠性，仅在必要时进行干预。我们的框架无缝集成了人类监督、LLM代理和异构机器人，以优化任务分配和执行。每台机器人配备了一个LLM代理，能够理解其能力，将任务转换为可执行指令，并通过任务验证和人类监督来减少幻觉。仿真结果表明，我们的框架优于最先进的任务规划方法，任务成功率提高了4.76%。实际测试展示了其强大的零样本泛化特性和在最小化人类干预的情况下处理多样化任务和环境的能力。|
|**2025-04-30**|**Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming**|Nanxu Gong et.al.|[2504.21304](http://arxiv.org/abs/2504.21304)|null|特征变换涉及从原始数据集中生成一组新的特征以增强数据的效用。在某些领域，如材料性能筛选，维度很大，收集标签既昂贵又耗时。这迫切需要高效且无监督地变换特征空间，以增强数据准备性和AI的效用。然而，现有方法在高效导航庞大的特征组合空间方面存在不足，并且大多是为监督设置设计的。为了填补这一空白，我们独特的视角是利用生成器-评论家双打团队框架，使用LLM代理和上下文学习从无监督数据中推导出伪监督。该框架包括三个相互关联的步骤：（1）评论家代理诊断数据以生成可操作的建议，（2）生成器代理根据评论家的建议产生标记化特征变换，以及（3）迭代优化通过代理之间的反馈确保持续改进。生成器-评论家框架可以通过用人类专家替换评论家代理来推广到人机协作生成。广泛的实验表明，所提出的框架在特征变换效率、鲁棒性和跨各种数据集的实际适用性方面优于甚至监督基线。|
|**2025-04-29**|**Toward Efficient Exploration by Large Language Model Agents**|Dilip Arumugam et.al.|[2504.20997](http://arxiv.org/abs/2504.20997)|null|随着强化学习（RL）领域的发展，围绕大型语言模型（LLM）设计的顺序决策智能体成为一个新兴领域。虽然由现代LLM驱动的自主决策智能体可以促进众多实际应用，但这些成功需要能够实现数据高效RL的智能体。在RL中实现数据效率的一个关键障碍是探索，我们表明许多最近提出的LLM智能体设计都难以应对这一挑战。同时，从RL文献中了解的、能够优雅处理探索的经典算法需要可能在纯自然语言环境中难以操作的工程技术。在这项工作中，我们不是依赖于微调或上下文学习来诱导LLM隐式模仿RL算法，而是展示了如何使用LLM显式实现一个现有的RL算法（强化学习中的后验采样），其统计高效探索能力已经被充分研究。我们提供了实证结果，证明了我们基于LLM的、已知的数据高效RL算法在需要谨慎探索的自然语言任务中可以显著更加有效。|
|**2025-04-29**|**TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data**|Qi Wang et.al.|[2504.20462](http://arxiv.org/abs/2504.20462)|null|随着分布式系统的发展，微服务和云原生技术已成为现代企业软件开发的核心。尽管这些技术带来了显著的优势，但也增加了系统复杂性和运营挑战。传统的根本原因分析（RCA）难以实现自动化故障响应，严重依赖人工干预。近年来，大型语言模型（LLMs）在上下文推理和领域知识整合方面取得了突破，为人工智能运维（AIOps）提供了新的解决方案。然而，现有的基于LLM的方法面临三个关键挑战：文本输入限制、动态服务依赖的幻觉以及上下文窗口的限制。为了解决这些问题，我们提出了一种辅助工具的LLM代理，名为TAMO，用于细粒度RCA。它将多模态观测数据统一为时间对齐的表示，以提取一致的特征，并使用专门的根因定位和故障分类工具来感知上下文环境。这种方法克服了LLM在处理实时变化的服务依赖和原始观测数据方面的局限性，并通过将关键信息结构化到提示中，引导LLM生成与系统上下文一致的修复策略。实验结果表明，当处理具有异质性和常见故障类型的公共数据集时，TAMO在根本原因分析方面表现良好，证明了其有效性。|
|**2025-04-28**|**Prompt Injection Attack to Tool Selection in LLM Agents**|Jiawen Shi et.al.|[2504.19793](http://arxiv.org/abs/2504.19793)|null|工具选择是LLM代理的关键组成部分。该过程通过一个两步机制——检索和选择——来从工具库中挑选出最适合给定任务的工具。在这项工作中，我们引入了ToolHijacker，这是一种针对无盒场景中工具选择的创新性提示注入攻击。ToolHijacker将恶意工具文档注入工具库，以操纵LLM代理的工具选择过程，使其持续选择攻击者选择的恶意工具来执行攻击者指定的任务。具体来说，我们将制作这种工具文档的过程视为一个优化问题，并提出了一种两阶段优化策略来解决它。我们广泛的实验评估表明，ToolHijacker非常有效，在应用于工具选择时显著优于现有的基于手动和自动的提示注入攻击。此外，我们探索了各种防御措施，包括基于预防的防御（StruQ和SecAlign）和基于检测的防御（已知答案检测、困惑度检测和困惑度窗口检测）。我们的实验结果表明，这些防御措施是不够的，凸显了开发新防御策略的紧迫性。|
|**2025-04-28**|**Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies**|Kavindu Warnakulasuriya et.al.|[2504.19487](http://arxiv.org/abs/2504.19487)|null|合作演化的研究广泛地使用了抽象的数学模型和模拟。近年来，大型语言模型（LLM）的进步以及LLM代理的出现展示了它们进行社会推理的能力，从而为使用类似人类的推理在更现实的基于代理的模拟中测试规范的出现提供了机会。在本研究中，我们调查了Boyd和Richerson模型中提出的合作动态是否在比Boyd和Richerson工作中抽象数学性质更现实的用餐困境模拟中，使用LLM代理持续存在。我们的发现表明，代理遵循Boyd和Richerson模型中定义的策略，并且明确的惩罚机制驱动规范的出现，即使在代理策略配置变化的情况下也能加强合作行为。我们的结果表明，基于LLM的多代理系统模拟实际上可以复制传统数学模型预测的合作演化。此外，我们的模拟通过整合自然语言驱动的推理和策略采用的成对模仿方法超越了数学模型，使它们成为MAS中合作行为的更现实测试平台。|
|**2025-04-28**|**ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies**|Shubham Gandhi et.al.|[2504.20117](http://arxiv.org/abs/2504.20117)|null|本文介绍了ResearchCodeAgent，这是一个利用大型语言模型（LLMs）作为智能体，自动化机器学习文献中描述的研究方法编码的全新多智能体系统。该系统架起了高级研究概念与其实际应用之间的桥梁，使得研究人员能够自动生成现有研究论文的代码，以进行基准测试或基于文献中指定方法的现有方法进行构建，前提是有部分或完整的起始代码。ResearchCodeAgent采用灵活的智能体架构和全面的行为套件，能够与环境进行上下文感知的交互。该系统集成了动态规划机制，利用短期和长期记忆来迭代地调整其方法。我们在三个具有不同任务复杂度，代表机器学习流程不同部分的机器学习任务上评估了ResearchCodeAgent：数据增强、优化和数据批量处理。我们的结果表明，该系统的有效性和泛化能力，46.9%生成的代码是高质量且无错误的，25%的性能优于基线实现。实证分析显示，与手动实现相比，编码时间平均减少了57.9%。对于更复杂的任务，我们观察到更高的收益。ResearchCodeAgent是自动化研究实现过程的重要一步，有望加速机器学习研究的发展速度。|
|**2025-04-27**|**BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese**|Peilin Zhou et.al.|[2504.19314](http://arxiv.org/abs/2504.19314)|**[link](https://github.com/palin2018/browsecomp-zh)**|随着大型语言模型（LLMs）进化为使用工具的智能体，实时浏览网络的能力已成为衡量其推理和检索能力的关键标准。现有的基准测试，如BrowseComp，主要关注英语，忽略了其他主要信息生态系统（特别是中国）的语言、基础设施和审查相关的复杂性。为了填补这一空白，我们引入了BrowseComp-ZH，这是一个专门设计的高难度基准，旨在全面评估LLMs在中文网络上的表现。BrowseComp-ZH包含涵盖11个不同领域的289个多跳问题。每个问题都是从简短、客观且易于验证的答案（例如日期、数字或专有名词）逆向构建的。采用两阶段质量控制协议，力求提高问题难度和答案的唯一性。我们在提出的BrowseComp-ZH上对超过20个最先进的语言模型和智能体搜索系统进行了基准测试。尽管这些模型在对话和检索能力上表现出色，但大多数模型在测试中表现严重不佳：大量模型的准确率低于10%，只有少数模型的准确率超过20%。甚至表现最好的系统，OpenAI的DeepResearch，准确率也仅为42.9%。这些结果证明了BrowseComp-ZH的难度之大，成功不仅需要有效的检索策略，还需要复杂的推理和信息调和——这些能力是当前模型仍需努力掌握的。我们的数据集、构建指南和基准测试结果已公开发布在https://github.com/PALIN2018/BrowseComp-ZH。|
|**2025-04-26**|**Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation**|Jong Inn Park et.al.|[2504.18805](http://arxiv.org/abs/2504.18805)|null|将科学论文转化为引人入胜且内容准确的短视频具有挑战性，这是因为内容复杂以及专家作者与读者之间的差距。现有的端到端方法常常因事实错误和视觉瑕疵而受限，限制了其在科学传播中的效用。为了解决这些问题，我们提出了SciTalk，一个新颖的多LLM代理框架，该框架将视频基于多种来源进行定位，例如文本、图表、视觉风格和头像。受内容创作者工作流程的启发，SciTalk使用专门的代理进行内容摘要、视觉场景规划和文本与布局编辑，并采用迭代反馈机制，其中视频代理模拟用户角色，对前一代产生的视频给出反馈，并精炼生成提示。实验评估显示，SciTalk在生成科学准确且引人入胜的内容方面优于简单的提示方法，在视频生成优化的循环中表现更佳。尽管初步结果尚未达到人类创作者的质量，但我们的框架为反馈驱动的视频生成所面临的挑战和收益提供了有价值的见解。我们的代码、数据和生成的视频将公开可用。|
|**2025-04-26**|**A Vision for Auto Research with LLM Agents**|Chengwei Liu et.al.|[2504.18765](http://arxiv.org/abs/2504.18765)|null|本文介绍了基于代理的自动研究，这是一个旨在自动化、协调和优化科学研究全生命周期的结构化多代理框架。利用大型语言模型（LLMs）和模块化代理协作的能力，该系统涵盖了所有主要研究阶段，包括文献综述、构思、方法论规划、实验、论文撰写、同行评审回应和传播。通过解决诸如工作流程碎片化、方法论专业知识不均和认知超载等问题，该框架为科学研究提供了一个系统化和可扩展的方法。初步探索证明了自动研究作为自我改进、AI驱动研究过程的潜在范式是可行的。|
|**2025-04-25**|**Towards Adaptive Software Agents for Debugging**|Yacine Majdoub et.al.|[2504.18316](http://arxiv.org/abs/2504.18316)|null|使用多个代理被发现可以提升大型语言模型的调试能力。然而，增加LLM代理的数量存在一些缺点，如增加运行成本和增加代理失去关注的风险。在本工作中，我们提出了一种自适应代理设计，其中代理的数量和角色根据要完成的任务的特征动态确定。在这个设计中，代理的角色不是预先定义的，而是在分析要解决的问题后生成的。我们的初步评估显示，使用自适应设计，生成的代理数量取决于有缺陷代码的复杂性。实际上，对于只有语法问题的简单代码，通常只需要一个代理就能解决问题。然而，对于更复杂的问题，我们注意到创建了更多的代理。关于修复的有效性，我们注意到与一次提示相比，平均提高了11%。鉴于这些有希望的结果，我们概述了未来研究方向，以改进自适应软件代理的设计，这些代理可以自主规划和执行其软件目标。|
|**2025-04-25**|**MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind**|Zheng Zhang et.al.|[2504.18039](http://arxiv.org/abs/2504.18039)|null|大型语言模型（LLM）在需要战略推理和社会欺骗的社交推理游戏（SDGs）如《狼人杀》中展现了令人印象深刻的能力。然而，现有方法仍然局限于文本信息，忽略了人类自然用来交流的至关重要的多模态线索，如面部表情和语调。此外，现有的SDG代理主要侧重于推断其他玩家的身份，而没有模拟他人如何感知自己或其他玩家。为了解决这些限制，我们以《一晚上终极狼人杀》（ONUW）为测试平台，并提出了MultiMind，这是第一个将多模态信息整合到SDG代理中的框架。MultiMind在处理语言内容的同时，还处理面部表情和语调，并使用心智理论（ToM）模型来表示每个玩家对其他玩家的怀疑程度。通过将这个ToM模型与蒙特卡洛树搜索（MCTS）相结合，我们的代理能够识别出最小化指向自己的怀疑的沟通策略。通过在代理之间模拟和与人类玩家进行的研究中的全面评估，我们证明了MultiMind在游戏中的优越表现。我们的工作在实现能够跨多模态领域进行类似人类的社会推理的LLM代理方面取得了重大进展。|
|**2025-04-25**|**Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction**|Ross Gore et.al.|[2504.18671](http://arxiv.org/abs/2504.18671)|null|由于轻微脑震荡（TBI）在医学影像中症状的微妙性和模糊性，其检测面临重大挑战，这使得准确诊断变得复杂。为了应对这些挑战，我们提出了一种名为“Proof-of-TBI”的医疗诊断支持系统，该系统集成了多个经过微调的视觉-语言模型和OpenAI-o3推理大型语言模型（LLM）。我们的方法使用TBI MRI扫描的标注数据集对多个视觉-语言模型进行微调，使它们能够有效地诊断TBI症状。这些模型的预测通过基于共识的决策过程进行汇总。该系统使用表现出色的推理性能的OpenAI-o3推理LLM评估所有微调的视觉语言模型的预测，以产生最准确的最终诊断。LLM代理协调视觉-语言模型和推理LLM之间的交互，以透明、可靠和自动化的方式管理最终决策过程。这种端到端的决策工作流程结合了视觉-语言模型联盟和OpenAI-o3推理LLM，得益于LLM代理的定制提示工程。该平台的原型与位于弗吉尼亚州纽波特纽斯的美军医学研究团队合作开发，集成了五个微调的视觉-语言模型。结果表明，将微调的视觉-语言模型输入与OpenAI-o3推理LLM相结合，具有改变轻微TBI预测系统的强大潜力。据我们所知，这项研究代表了首次将微调的视觉-语言模型与推理LLM集成应用于TBI预测任务。|
|**2025-04-24**|**LLM Agent Swarm for Hypothesis-Driven Drug Discovery**|Kevin Song et.al.|[2504.17967](http://arxiv.org/abs/2504.17967)|null|药物发现仍然是一项艰巨的挑战：超过90%的候选分子在临床评估中失败，每个获批的治疗方案的开发成本通常超过10亿美元。来自基因组学、转录组学到化学库和临床记录的各异数据流阻碍了连贯的机制洞察并减缓了进展。与此同时，大型语言模型在推理和工具集成方面表现出色，但缺乏模块化专业化和迭代记忆，这些是符合规定的、基于假设的工作流程所必需的。我们引入了PharmaSwarm，这是一个统一的多人代理框架，它协调专门的LLM“代理”来提出、验证和改进针对新型药物靶点和先导化合物的假设。每个代理都可以访问专用功能——自动基因组学和表达分析；精心挑选的生物医学知识图谱；通路富集和网络模拟；可解释的亲和力预测——而一个中央评估LLM则持续按照生物学合理性、新颖性、计算机模拟效力和安全性对提案进行排名。共享内存层捕获验证的洞察力，并在时间上微调下层的子模型，从而产生一个自我改进的系统。PharmaSwarm可部署在低代码平台或基于Kubernetes的微服务上，支持以文献为驱动的发现、以组学为指导的目标识别和市场信息引导的再利用。我们还描述了一个严格的四层验证流程，涵盖回顾性基准测试、独立的计算检测、实验测试和专家用户研究，以确保透明度、可重复性和现实世界的影响。作为人工智能辅助，PharmaSwarm可以加速转化研究，比传统流程更有效地提供高置信度的假设。|
|**2025-04-24**|**Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning**|Isadora White et.al.|[2504.17950](http://arxiv.org/abs/2504.17950)|null|合作在日常生活中的普遍性和必要性不言而喻——从交流思想到分配任务，再到共同制定计划。本研究探讨了大型语言模型如何自适应地协作以执行复杂的具身推理任务。为此，我们引入了MINDcraft，这是一个易于扩展的平台，旨在使LLM智能体能够控制Minecraft开放世界游戏中的角色；以及MineCollab，一个用于测试具身和协作推理不同维度的基准。一项实验研究表明，对于当前最先进的智能体而言，有效协作的主要瓶颈是高效的自然语言沟通，当需要它们传达详细的任务完成计划时，智能体的性能会下降多达15%。我们得出结论，现有的LLM智能体在多智能体协作方面优化不足，尤其是在具身场景中，并强调需要采用超出上下文学习和模仿学习的方法。我们的网站可以在这里找到：https://mindcraft-minecollab.github.io/|
|**2025-04-24**|**Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents**|Chaoran Chen et.al.|[2504.17934](http://arxiv.org/abs/2504.17934)|null|大型语言模型（LLMs）的兴起通过LLM驱动的GUI代理彻底改变了图形用户界面（GUI）自动化。然而，它们在有限人工监督下处理敏感数据的能力引发了显著的隐私和安全风险。这篇立场论文确定了GUI代理的三个关键风险，并探讨了它们与传统GUI自动化和通用自主代理的不同之处。尽管存在这些风险，现有的评估主要关注性能，而对隐私和安全评估探索得很少。我们回顾了GUI和通用LLM代理当前的评估指标，并概述了在GUI代理评估中集成人工评估员时的五个关键挑战。为了填补这些空白，我们主张采用以人为本的评估框架，该框架融合了风险评估，通过情境式同意增强用户意识，并将隐私和安全考虑嵌入到GUI代理的设计和评估中。|
|**2025-04-24**|**Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach**|Iizalaarab Elhaimeur et.al.|[2504.18603](http://arxiv.org/abs/2504.18603)|null|量子计算教育由于其复杂性和现有工具的限制而面临重大挑战；本文介绍了一种用于量子计算教育的创新智能教学助手及其进化设计过程。该系统结合了一种知识图谱增强的架构和两个专门的大型语言模型（LLM）智能体：一个教学智能体用于动态交互，一个课程规划智能体用于生成课程计划。系统旨在适应个别学生的需求，通过知识图谱详细跟踪和存储交互。此图谱代表了学生的行动、学习资源和关系，旨在使有效学习路径的推理成为可能。我们描述了该系统的实现，突出了遇到的挑战和实施的解决方案，包括引入一个双智能体架构，其中任务被分离，所有操作都通过一个中央知识图谱进行协调，以保持系统意识，以及一个面向用户的标签系统，旨在减轻LLM的幻觉并提高用户控制。初步结果表明，该系统具有捕获丰富交互数据的潜力，通过模拟中的标签系统动态调整课程计划以根据学生反馈，并通过集成的知识图谱促进情境感知的辅导，尽管还需要系统性的评估。|
|**2025-04-23**|**A Survey of AI Agent Protocols**|Yingxuan Yang et.al.|[2504.16736](http://arxiv.org/abs/2504.16736)|null|大型语言模型（LLMs）的快速发展导致了LLM代理在各个行业的广泛应用，包括客户服务、内容生成、数据分析，甚至医疗保健。然而，随着更多LLM代理的部署，一个主要问题出现了：这些代理与外部工具或数据源之间没有标准化的通信方式。这种缺乏标准化协议使得代理难以协作或有效扩展，并限制了它们处理复杂、现实世界任务的能力。为LLM代理制定一个统一的通信协议可以改变这一现状。这将使得代理和工具能够更顺畅地交互，鼓励协作，并触发集体智能的形成。在本文中，我们提供了对现有代理协议的首次全面分析，提出了一种系统性的二维分类，区分了以上下文为导向与跨代理协议，以及通用与领域特定协议。此外，我们对这些协议在安全性、可扩展性和延迟等关键维度上进行了比较性能分析。最后，我们通过确定下一代协议的关键研究方向和特征，探讨了代理协议的未来格局。这些特征包括适应性、隐私保护、基于群体的交互，以及向分层架构和集体智能基础设施的趋势。我们期望这项工作能为寻求设计、评估或集成智能代理的强大通信基础设施的研究人员和工程师提供实用的参考。|
|**2025-04-23**|**Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments**|Yuran Li et.al.|[2504.17087](http://arxiv.org/abs/2504.17087)|null|大型语言模型（LLMs）正被广泛应用于各个领域，但随着任务的复杂性增加，评估它们的响应变得越来越具有挑战性。与人类评估者相比，使用LLMs来支持性能评估提供了一种更高效的替代方案。然而，大多数研究主要关注将LLMs的判断与人类偏好对齐，而忽略了人类判断中存在的偏见和错误。此外，在多个潜在的LLM响应中选择合适的LLM判断的问题仍被探讨不足。为了解决上述两个问题，我们提出了一种三阶段的元评估者选择流程：1）与GPT-4和人类专家共同开发一个全面的评分标准，2）使用三个高级LLM代理进行评分，3）应用阈值来过滤掉得分低的判断。与将单个LLM既作为评判者又作为元评判者的方法相比，我们的流程引入了多代理协作和更全面的评分标准。在JudgeBench数据集上的实验结果表明，与原始判断相比，提高了约15.55%，与单代理基线相比，提高了约8.37%。我们的工作展示了LLMs作为元评判者的潜力，并为未来关于构建LLM作为评判者的强化学习偏好数据集的研究奠定了基础。|
|**2025-04-22**|**MR. Video: "MapReduce" is the Principle for Long Video Understanding**|Ziqi Pang et.al.|[2504.16082](http://arxiv.org/abs/2504.16082)|null|我们提出了MR. Video，一个具有代理功能的长期视频理解框架，该框架展示了简单的MapReduce原理在处理长视频中的有效性：1）Map：独立且密集地感知短视频片段；2）Reduce：联合汇总所有片段的信息。与序列到序列的视觉-语言模型（VLMs）相比，MR. Video能够进行详细的短视频感知，而不会被上下文长度所限制。与通常依赖顺序关键片段选择的现有视频代理相比，Map操作使短视频片段的序列并行感知变得更加简单和可扩展。它的Reduce步骤允许进行更全面的环境汇总和推理，超越了显式的关键片段检索。这种MapReduce原理适用于VLMs和视频代理，我们使用LLM代理来验证其有效性。在实践中，MR. Video采用了两个MapReduce阶段：（A）字幕生成：为短视频片段生成字幕（map），然后将重复出现的字符和对象标准化为共享名称（reduce）；（B）分析：针对每个用户问题，从单个短视频中分析相关信息（map），并将它们整合为最终答案（reduce）。与最先进的VLMs和视频代理相比，MR. Video在具有挑战性的LVBench测试上实现了超过10%的准确率提升。代码可在以下链接获取：https://github.com/ziqipang/MR-Video|
|**2025-04-22**|**LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities**|Thomas Schmied et.al.|[2504.16078](http://arxiv.org/abs/2504.16078)|null|大型语言模型（LLMs）的成功引发了人们对各种代理应用的兴趣。一个关键假设是，LLMs通过利用常识和思维链（CoT）推理，可以有效地探索和高效地解决复杂领域。然而，研究发现LLM代理在探索上存在次优问题，以及知行差距，即无法有效作用于模型中存在的知识。在这项工作中，我们系统地研究了为什么LLMs在决策场景中表现不佳。特别是，我们密切考察了三种常见的失败模式：贪婪、频率偏差和知行差距。我们通过在自生成的CoT理由上使用强化学习（RL）进行微调来缓解这些不足。我们的实验跨越了多臂老虎机、上下文老虎机和井字棋，表明RL微调通过增加探索和缩小知行差距来增强LLMs的决策能力。最后，我们研究了经典探索机制，如ε-贪婪，以及LLM特定方法，如自我纠正和自我一致性，以实现更有效的LLMs决策微调。|
|**2025-04-22**|**WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents**|Siyu Zhou et.al.|[2504.15785](http://arxiv.org/abs/2504.15785)|**[link](https://github.com/elated-sawyer/WALL-E)**|我们能否利用大型语言模型（LLMs）构建准确的世界模型？世界模型如何惠及LLM智能体？LLMs的先验知识与指定环境动态之间的差距通常会成为LLMs作为世界模型性能的瓶颈。为了弥合这一差距，我们提出了一种无需训练的“世界对齐”方法，该方法学习一种与LLMs互补的环境符号知识。符号知识涵盖了动作规则、知识图谱和场景图，这些知识由LLMs从探索轨迹中提取，并编码成可执行代码，以调节LLM智能体的策略。我们进一步通过模型预测控制（MPC）框架提出了一个无需强化学习（RL）、基于模型的智能体“WALL-E 2.0”。与需要实时进行昂贵优化的经典MPC不同，我们采用LLM智能体作为与神经符号世界模型交互的、未来步骤动作的高效前瞻优化器。尽管LLM智能体的强大启发式方法使其在MPC中成为高效的规划者，但其规划动作的质量也由对齐的世界模型的准确预测所保证。它们共同显著提高了在新环境中的学习效率。在火星（类似Minecraft）和ALFWorld（具身室内环境）的开放世界挑战中，WALL-E 2.0显著优于现有方法，例如在火星上的成功率超过基线16.1%-51.6%，得分至少提高61.7%。在ALFWorld中，它仅经过4次迭代就实现了98%的成功率，创造了新纪录。|
|**2025-04-21**|**Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning**|Ehsan Ahmadi et.al.|[2504.15263](http://arxiv.org/abs/2504.15263)|null|施工任务本质上是不可预测的，动态环境和安全关键的需求给工人带来了重大风险。外骨骼提供了潜在的辅助作用，但如果没有在多种运动模式中准确识别意图，则会失效。本文提出了一种利用大型语言模型（LLMs）并辅以记忆系统的运动预测代理，旨在改善此类环境中的外骨骼辅助。通过使用多模态输入——语音命令和来自智能眼镜的视觉数据——代理整合了感知模块、短期记忆（STM）、长期记忆（LTM）和细化模块，以有效地预测运动模式。评估结果显示，没有记忆时的基线加权F1分数为0.73，使用STM时上升至0.81，而使用STM和LTM时达到0.90，在模糊和安全关键命令方面表现优异。校准指标，包括布里尔分数从0.244降至0.090和ECE从0.222降至0.044，证实了可靠性的提升。此框架支持更安全、高级的人机外骨骼协作，有望在动态行业中实现适应性辅助系统。|
|**2025-04-20**|**SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs**|Minh V. T. Pham et.al.|[2504.14757](http://arxiv.org/abs/2504.14757)|null|大型语言模型（LLMs）正通过基于代理的方法改变自动化程序修复（APR），这些方法可以定位错误、生成补丁和验证修复。然而，缺乏高质量、可扩展的训练数据集，特别是具有可验证输出和中间推理轨迹的数据集，限制了进展，尤其是对于开源模型。在本工作中，我们提出了SWE-Synth，这是一个在仓库级别合成真实、可验证和流程感知的错误修复数据集的框架。SWE-Synth利用LLM代理来模拟调试工作流程，不仅生成错误修复对，还生成测试用例和结构化修复轨迹。与手动整理的数据集相比，我们的方法在保持语境丰富性和正确性的同时，以最小的人为努力进行扩展。实验表明，在SWE-Synth上训练的模型在SWE-Bench Lite上的表现优于在真实世界数据集上训练的模型，高出2.3%。我们的结果突出了合成、代理生成数据在推进自动化程序修复和软件工程自动化领域先进性方面的潜力。|
|**2025-04-19**|**FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory**|Alessio Buscemi et.al.|[2504.14325](http://arxiv.org/abs/2504.14325)|null|让AI代理在多智能体应用中进行交互，为AI结果的解释和预测增加了复杂性，这对它们在研究和社会中的可靠应用有着深远的影响。博弈论提供了强大的模型来捕捉和解释代理之间的战略互动，但需要可复现、标准化且用户友好的IT框架来支持结果的可比性和解释。为此，我们提出了FAIRGAME，一个基于博弈论进行AI代理偏见识别的框架。我们描述了其实现和用法，并使用它来揭示AI代理在流行游戏中的偏见结果，这些结果取决于所使用的大型语言模型（LLM）和使用的语言，以及代理的性格特征或战略知识。总体而言，FAIRGAME允许用户可靠且轻松地模拟他们期望的游戏和场景，并在模拟活动和博弈论预测之间比较结果，从而系统地发现偏见，预测战略互动中可能出现的新行为，并赋予进一步研究使用LLM代理进行战略决策的权力。|
|**2025-04-19**|**SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation**|Xuhui Zhou et.al.|[2504.16122](http://arxiv.org/abs/2504.16122)|null|通过大型语言模型（LLM）代理进行社会模拟是探索和验证与社会科学问题和LLM代理行为相关假设的有前景的方法。我们提出了SOTOPIA-S4，这是一个快速、灵活且可扩展的社会模拟系统，它解决了当前框架的技术障碍，同时使实践者能够生成多轮和多方的基于LLM的交互，并为假设测试提供可定制的评估指标。SOTOPIA-S4作为一个pip包提供，包含一个模拟引擎、一个具有灵活RESTful API的API服务器，以及一个网络界面，使技术和非技术用户都能无需编程设计、运行和分析模拟。我们通过两个用例展示了SOTOPIA-S4的有用性，包括二元招聘谈判和多方规划场景。|
|**2025-04-17**|**Exploring Expert Failures Improves LLM Agent Tuning**|Li-Cheng Lan et.al.|[2504.13145](http://arxiv.org/abs/2504.13145)|null|大型语言模型（LLMs）作为智能体展现出巨大的潜力，擅长处理需要多轮推理和交互的任务。拒绝采样微调（RFT）已成为微调LLMs作为智能体的有效方法：它首先模仿专家生成的成功轨迹，并通过在成功、自我生成的轨迹上进行迭代微调来进一步提高智能体的技能。然而，由于专家（例如，GPT-4）主要在简单的子任务上取得成功，而RFT本质上倾向于简单场景，许多复杂的子任务仍未解决且持续处于分布外（OOD）。在调查这些具有挑战性的子任务时，我们发现先前失败的专家轨迹常常能提供有价值的指导，例如计划和关键动作，这些可以显著提高智能体的探索效率和关键技能的获取。受这些观察的启发，我们提出了探索专家失败（EEF）方法，该方法从失败的专家轨迹中识别出有益的动作，并将它们整合到训练数据集中。有害的动作被精心排除，以防止对模型学习过程的污染。通过利用专家失败中的有益动作，EEF成功解决了先前无法解决的子任务，并提高了智能体微调性能。值得注意的是，我们的方法在WebShop中取得了62%的胜率，超过了RFT（53.6%）和GPT-4（35.6%），并且据我们所知，作为第一个在WebShop中超过0.81分和在SciWorld中超过81分的方法，设定了新的基准。|
|**2025-04-17**|**Retrieval-Augmented Generation with Conflicting Evidence**|Han Wang et.al.|[2504.13079](http://arxiv.org/abs/2504.13079)|**[link](https://github.com/hannight/ramdocs)**|**大型语言模型（LLM）代理越来越多地采用检索增强生成（RAG）来提高其回答的真实性。然而，在实际应用中，这些系统往往需要处理模糊的用户查询，来自多个来源的可能存在冲突的信息，同时抑制来自嘈杂或不相关文档中的不准确信息。先前的工作通常独立研究和解决这些挑战，一次只考虑一个方面，例如处理模糊性或对噪声和错误信息的鲁棒性。我们则同时考虑多个因素，提出以下两点：（i）RAMDocs（文档中的模糊性和错误信息检索），一个新的数据集，模拟了针对用户查询的复杂和真实场景，包括模糊性、错误信息和噪声；（ii）MADAM-RAG，一种多代理方法，其中LLM代理在多轮次中就答案的优点进行辩论，允许聚合器收集对应于消歧实体的响应，同时丢弃错误信息和噪声，从而共同处理冲突的多种来源。我们使用AmbigDocs（需要呈现模糊查询的所有有效答案）和FaithEval（需要抑制错误信息）上的闭源和开源模型展示了MADAM-RAG的有效性，AmbigDocs上MADAM-RAG相对于强大的RAG基线提高了高达11.40%，FaithEval上，我们使用Llama3.3-70B-Instruct提高了高达15.80%（绝对值）。此外，我们发现RAMDocs对现有的RAG基线构成了挑战（仅Llama3.3-70B-Instruct获得32.60精确匹配分数）。尽管MADAM-RAG开始解决这些冲突因素，但我们的分析表明，在增加支持证据和错误信息的不平衡程度时，仍然存在很大的差距。**|
|**2025-04-17**|**WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents**|Arth Bohra et.al.|[2504.12682](http://arxiv.org/abs/2504.12682)|null|最近的网络代理研究主要关注导航和交易任务，对大规模结构化数据提取的关注较少。我们提出了WebLists，这是一个包含200个数据提取任务的基准，涵盖了四个常见的商业和企业用例。每个任务都需要代理导航到网页，适当配置它，并提取具有明确定义模式的完整数据集。我们表明，具有搜索功能的LLM和最先进的网络代理在这些任务上都存在困难，尽管在问答任务上表现更好，但召回率分别为3%和31%。为了应对这一挑战，我们提出了BardeenAgent，这是一个新型框架，使网络代理能够将它们的执行转换为可重复的程序，并在具有相似结构的页面上以规模重放。BardeenAgent也是第一个利用HTML规则结构的LLM代理。特别是BardeenAgent构建了一个可通用的CSS选择器来捕获页面上所有相关项目，然后调整操作以提取数据。在WebLists基准测试中，BardeenAgent的整体召回率达到66%，比最先进网络代理的性能翻了一番多，并且将每输出行的成本降低了3倍。|
|**2025-04-17**|**MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation**|Haris Riaz et.al.|[2504.12563](http://arxiv.org/abs/2504.12563)|null|近期推出的较小型的语言模型，如Phi-3.5和Phi-4，依赖于使用大型语言模型生成的合成数据。然而，关于利用合成数据在其他用例中的应用，例如将大型语言模型（LLM）适应特定领域，仍存在疑问。合成数据的关键局限性是多样性低，这对其下游应用中改进其他模型产生了负面影响。为了解决这个问题，我们提出了MetaSynth，这是一种通过元提示增强多样性的合成数据生成方法，其中语言模型协调多个“专家”LLM代理协同生成数据。仅使用MetaSynth生成的2.5亿个标记的合成数据，我们成功地适应了一个经过良好训练的LLM（Mistral-7B-v0.3），使其适用于两个专业领域——金融和生物医学——而不会损害模型在一般任务中的能力。此外，我们使用七个自动化指标评估了合成数据的多样性，发现其接近LLM预训练语料库的多样性。不断使用MetaSynth对Mistral-7B-v0.3进行预训练明显优于基础LLM，在金融领域提高了4.08%，在生物医学领域提高了13.75%。当使用包含先前生成和真实数据上下文实例的模板提示进行训练时，同一模型的表现会下降。我们的研究结果表明，使用MetaSynth时，仅需几百万个没有混合任何真实数据的多样化合成数据，就足以有效地实现领域适应。|
|**2025-04-16**|**Towards LLM Agents for Earth Observation**|Chia Hsiang Kao et.al.|[2504.12110](http://arxiv.org/abs/2504.12110)|null|地球观测（EO）为环境监测、灾害管理、气候科学和其他科学领域提供了关键性的行星数据。在这里，我们提出问题：人工智能系统是否已经准备好进行可靠的地球观测？我们引入了\datasetnamenospace，这是一个包含140个是/否问题的基准数据集，这些问题来自NASA地球观测站的13个主题和17个卫星传感器文章。使用谷歌地球引擎API作为工具，大型语言模型（LLM）的代理只能达到33%的准确率，因为代码有超过58%的时间无法运行。通过微调合成数据，我们改善了开放模型的失败率，使得规模更小的模型（如Llama-3.1-8B）能够达到与规模更大的模型（例如DeepSeek-R1）相当的性能。总的来说，我们的研究结果指出了在人工智能代理能够自动化地球观测之前需要解决的重大挑战，并提出了前进的道路。项目页面可在https://iandrover.github.io/UnivEarth上找到。|
|**2025-04-16**|**Progent: Programmable Privilege Control for LLM Agents**|Tianneng Shi et.al.|[2504.11703](http://arxiv.org/abs/2504.11703)|**[link](https://github.com/sunblaze-ucb/progent)**|LLM代理是一种新兴的AI系统形式，其中大型语言模型（LLM）作为核心组件，利用各种工具来完成用户分配的任务。尽管它们具有巨大的潜力，但LLM代理也面临着重大的安全风险。在与外部世界交互时，它们可能会遭遇来自攻击者的恶意命令，导致执行危险行为。解决这一问题的有希望的方法是实施最小权限原则：仅允许完成任务所必需的动作，同时阻止不必要的动作。然而，实现这一点具有挑战性，因为它需要在覆盖多样化的代理场景的同时，保持安全和实用性。我们引入了Progent，这是第一个为LLM代理提供的权限控制机制。其核心是一个针对特定领域的语言，可以灵活地表达在代理执行期间应用的权限控制策略。这些策略对工具调用提供了细致的约束，决定何时允许工具调用，并在不可用时指定备选方案。这使得代理的开发者和用户能够为特定的用例制定合适的策略，并确定性地执行这些策略以确保安全。得益于其模块化设计，集成Progent不会改变代理的内部结构，并且只需对代理实现进行最小改动，增强了其实用性和广泛应用的潜力。为了自动化策略编写，我们利用LLM根据用户查询生成策略，然后动态更新这些策略以改善安全和实用性。我们的广泛评估表明，它在三个不同的场景或基准测试（AgentDojo、ASB和AgentPoison）中实现了强大的安全性，同时保持了高度的实用性。此外，我们进行了深入分析，展示了其核心组件的有效性和其自动策略生成对自适应攻击的抵抗力。|
|**2025-04-16**|**Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation**|Ji Ma et.al.|[2504.11671](http://arxiv.org/abs/2504.11671)|null|大型语言模型（LLMs）越来越多地被用作社会科学和应用场景中的人类决策代理。这些LLM代理通常被赋予类似人类的性格，并置于现实生活情境中。然而，这些性格和情境如何塑造LLM的行为仍然没有得到充分探索。本研究提出并测试了在独裁者游戏中探究、量化并修改LLM内部表示的方法——这是一个关于公平和亲社会行为的经典行为实验。我们从LLM的内部状态中提取出“变量变化向量”（例如，“男性”到“女性”）。在模型推理过程中操纵这些向量可以显著改变这些变量与模型决策之间的关系。这种方法为研究和管理如何在基于transformer的模型中编码和设计社会概念提供了一个原则性的方法，对对齐、去偏见以及设计用于学术和商业应用中的社会模拟的AI代理具有重要意义。|
|**2025-04-15**|**Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations**|Alejandro Lopez-Lira et.al.|[2504.10789](http://arxiv.org/abs/2504.10789)|null|本文提出了一种现实模拟的股票市场，其中大型语言模型（LLMs）作为异构竞争交易代理。开源框架集成了持续订单簿、市价单、限价单、部分成交、股息和均衡清算等功能，同时包含具有不同策略、信息集和禀赋的代理。代理使用结构化输出和函数调用提交标准化决策，同时用自然语言表达他们的推理。出现了三个发现：首先，LLMs表现出一致的策略遵循，可以根据指令充当价值投资者、动量交易者或市场制造者。其次，市场动态表现出真实金融市场的特征，包括价格发现、泡沫、过度反应和策略性流动性提供。第三，该框架允许分析LLMs对各种市场条件的反应，类似于机器学习可解释性中的部分依赖图。该框架允许在没有封闭形式解的情况下模拟金融理论，创建与人类参与者相比成本高昂的实验设计，并建立提示如何生成影响市场稳定性的相关行为。|
|**2025-04-15**|**GraphicBench: A Planning Benchmark for Graphic Design with Language Agents**|Dayeon Ki et.al.|[2504.11571](http://arxiv.org/abs/2504.11571)|null|大型语言模型（LLM）驱动的智能体为自动化人类任务开辟了新的可能性。虽然先前的研究主要集中在具有明确目标和定义良好的任务上，但智能体在具有开放式目标的创意设计任务中的能力仍被低估。我们引入了GraphicBench，这是一个新的图形设计规划基准，涵盖了1,079个用户查询和输入图像，涉及四种设计类型。我们进一步提出了GraphicTown，这是一个LLM智能体框架，包含三位设计专家和46个可供选择执行每个计划工作流程步骤的工具。六种LLM的实验表明，它们能够生成结合用户查询中的显式设计约束和隐式常识约束的工作流程。然而，这些工作流程往往不会导致成功的执行结果，这主要由于以下挑战：（1）关于空间关系的推理，（2）协调专家之间的全局依赖关系，（3）在每个步骤中检索最合适的动作。我们将GraphicBench视为一个具有挑战性但又有价值的测试平台，用于推进LLM智能体在创意设计任务中的规划和执行。|
|**2025-04-14**|**Characterizing LLM-driven Social Network: The Chirper.ai Case**|Yiming Zhu et.al.|[2504.10286](http://arxiv.org/abs/2504.10286)|null|大型语言模型（LLMs）展现了模拟人类决策过程的能力，使它们能够作为代理用于模拟复杂的社交网络，无论是在线还是离线。近期的研究探讨了LLM代理在模拟网络中的集体行为模式和结构特征。然而，LLM驱动的在线社交网络与人类驱动的在线社交网络之间的实证比较仍然很少，这限制了我们理解LLM代理与人类用户差异的能力。本文对Chirper.ai进行了大规模分析，Chirper.ai是一个完全由LLM代理组成的类似X/Twitter的社交网络，包含超过65,000个代理和770万个由AI生成的帖子。为了比较，我们从Mastodon收集了一个平行数据集，Mastodon是一个由人类驱动的去中心化社交网络，拥有超过117,000名用户和1600万个帖子。我们分析了LLM代理和人类在发帖行为、侮辱性内容和社交网络结构方面的关键差异。我们的研究结果为AI时代在线社交网络分析的发展趋势提供了关键洞见，为社交模拟中的LLM代理提供了一个全面的概况。|
|**2025-04-14**|**SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users**|Xinnong Zhang et.al.|[2504.10157](http://arxiv.org/abs/2504.10157)|**[link](https://github.com/fudandisc/socioverse)**|社会模拟通过虚拟个体与环境之间的交互来模拟人类行为，正在改变传统社会科学研究。随着大型语言模型（LLMs）的最近进展，这种方法在捕捉个体差异和预测群体行为方面显示出日益增长的潜力。然而，现有方法在环境、目标用户、交互机制和行为模式等方面面临对齐挑战。为此，我们引入了SocioVerse，这是一个由LLM代理驱动的用于社会模拟的世界模型。我们的框架具有四个强大的对齐组件和一个包含1000万真实个人的用户池。为了验证其有效性，我们在政治、新闻和经济三个不同的领域进行了大规模的模拟实验。结果表明，SocioVerse能够反映大规模人口动态，同时通过标准化程序和最小的手动调整确保多样性、可信度和代表性。|
|**2025-04-14**|**Training Small Reasoning LLMs with Cognitive Preference Alignment**|Wenrui Cai et.al.|[2504.09802](http://arxiv.org/abs/2504.09802)|null|大型语言模型（LLMs）如OpenAI的o1和DeepSeek-R1的推理能力通过深度思考得到了显著提升。然而，这些提升伴随着巨大的资源需求，凸显了探索以更少参数训练有效推理LLMs策略的必要性。一个关键挑战是，较小模型与较大模型相比，具有不同的能力和认知轨迹。因此，将大型LLMs的思考链（CoT）直接蒸馏到较小模型中有时可能无效，且需要大量的标注数据。在本文中，我们介绍了一个名为“批判-反思-验证”（CRV）的新框架，旨在训练较小但强大的推理LLMs。我们的CRV框架由多个LLM代理组成，每个代理专注于独特的功能：（i）根据较小模型的认知能力批判CoT，（ii）根据批判反思和精炼这些CoT，以及（iii）验证精炼结果的正确性。我们进一步提出了认知偏好优化（CogPO）算法，通过使这些模型的思维与它们的认知能力相一致，来增强较小模型的推理能力。在具有挑战性的推理基准上的综合评估表明，CRV和CogPO的有效性，它们在性能上显著优于其他训练方法。|
|**2025-04-14**|**Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning**|Jingtian Wu et.al.|[2504.09781](http://arxiv.org/abs/2504.09781)|null|尽管大型语言模型（LLMs）在问答和事实核查等任务上展现出强大的能力，但它们仍然遭受幻觉和推理错误的问题，尤其是在需要整合多个信息源的多跳任务中。当前的方法通过基于检索的技术（将推理建立在外部证据之上）、基于推理的方法（通过改进的提示增强连贯性）或结合这两种元素的混合策略来解决这个问题。一种突出的混合方法是ReAct，它优于纯粹基于检索或基于推理的方法；然而，它缺乏对中间推理步骤的内部验证，导致潜在的错误可能通过复杂的推理任务传播。在本文中，我们介绍了推理法庭（RC），这是一个扩展迭代推理和检索方法（如ReAct）的新框架，其中包含一个专门的LLM法官。与ReAct不同，RC使用这个法官独立评估由不同的LLM代理生成的多个候选答案及其相关推理。法官被要求根据提供的推理和证据选择它认为事实依据最充分、逻辑上最连贯的答案，或者在所有候选答案都不充分、有缺陷或无效的情况下，使用可用证据和其预训练的知识合成一个新答案。在多跳基准（HotpotQA、MuSiQue）和事实核查（FEVER）上的评估表明，RC在无需针对特定任务微调的情况下，始终优于最先进的少样本提示方法。|
|**2025-04-13**|**AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents**|Dakuo Wang et.al.|[2504.09723](http://arxiv.org/abs/2504.09723)|null|A/B测试实验是现代网络应用中评估UI/UX设计决策的一种广泛采用的方法。然而，传统的A/B测试仍然受限于对大规模和真实用户流量的依赖，以及等待测试结果所需的长时间。通过六位经验丰富的行业从业者的访谈，我们确定了当前A/B测试流程中的关键瓶颈。为此，我们提出了AgentA/B，一个利用基于大型语言模型的自主动态代理（LLM代理）的新型系统，以自动模拟用户与真实网页的交互行为。AgentA/B能够实现具有多样化角色的LLM代理的扩展部署，每个代理都能够导航动态网页并交互式地执行多步骤交互，如搜索、点击、筛选和购买。在一个演示性的控制实验中，我们使用AgentA/B模拟了包含1,000个LLM代理的Amazon.com之间的A/B测试，并将代理行为与真实人类购物行为在规模上进行比较。我们的发现表明，AgentA/B可以模拟类似人类的行为模式。|
|**2025-04-13**|**MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?**|Yunxiang Zhang et.al.|[2504.09702](http://arxiv.org/abs/2504.09702)|null|现有的关于大型语言模型（LLM）代理在科学发现中的应用评估缺乏客观的基准和指标来评估其提出方法的可行性。为了解决这一问题，我们引入了MLRC-Bench，这是一个旨在量化语言代理在处理具有挑战性的机器学习（ML）研究竞赛方面的有效性的基准。我们的基准突出了需要新颖方法的研究开放问题，与近期基准如OpenAI的MLE-Bench（Chan等，2024）和METR的RE-Bench（Wijk等，2024）相比，后者主要关注那些通过足够的工程努力就可以解决的问题。与先前的工作，例如AI Scientist（Lu等，2024b），它通过使用LLM作为评判者来评估端到端代理流程不同，MLRC-Bench衡量提出和实施新颖研究方法的关键步骤，并使用新提出的严格协议和客观指标进行评估。我们精心策划的7个竞赛任务揭示了LLM代理所面临的重大挑战。即使是表现最好的测试代理（MLAB（Huang等，2024a）下的gemini-exp-1206）也只能缩小基准与顶级人类参与者分数之间的9.3%差距。此外，我们的分析显示，LLM评判的创新与其在尖端ML研究问题上的实际表现之间存在不一致。MLRC-Bench是一个动态基准，旨在随着新的ML竞赛的加入而不断扩展，以鼓励对AI研究能力的严格和客观评估。|
|**2025-04-13**|**UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents**|Yuxuan Lu et.al.|[2504.09407](http://arxiv.org/abs/2504.09407)|null|可用性测试是用户体验（UX）研究人员用来评估和迭代网页设计的根本研究方法，但如何评估和迭代可用性测试研究设计本身呢？近年来，在大语言模型模拟代理（LLM Agent）研究方面的最新进展启发我们设计了UXAgent，以支持UX研究人员在开展真实的人类受试者研究之前评估和迭代他们的可用性测试研究设计。我们的系统具有一个人物生成器模块、一个LLM代理模块和一个通用浏览器连接器模块，以自动生成数千个模拟用户来交互式地测试目标网站。该系统还提供代理访谈界面和视频回放界面，以便UX研究人员可以轻松地审查和分析生成的定性和定量日志数据。通过启发式评估，五位UX研究人员参与者赞扬了我们系统的创新性，但也表达了对LLM代理在UX研究未来使用的担忧。|
|**2025-04-11**|**Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents**|Alessio Buscemi et.al.|[2504.08640](http://arxiv.org/abs/2504.08640)|null|普遍认为，在人工智能开发生态系统中培养信任与合作对于推广可信人工智能系统的采用至关重要。通过将大型语言模型（LLM）代理嵌入进化博弈论框架中，本文探讨了人工智能开发者、监管者和用户之间的复杂相互作用，模拟了他们在不同监管场景下的战略选择。进化博弈论（EGT）被用来定量地模拟每个参与者面临的困境，而LLM提供了额外的复杂性和细微差别，并使重复博弈和融入人格特质成为可能。我们的研究确定了战略人工智能代理的新兴行为，这些行为倾向于采取比纯粹博弈论代理更“悲观”（不相信且有缺陷）的立场。我们观察到，在用户完全信任的情况下，激励措施对于促进有效监管是有效的；然而，有条件的信任可能会损害“社会契约”。因此，建立用户信任和监管者声誉之间的良性反馈似乎对于引导开发者创造安全的人工智能至关重要。然而，这种信任产生的程度可能取决于用于测试的特定LLM。因此，我们的结果为人工智能监管系统提供了指导，并有助于预测如果战略LLM代理被用于协助监管本身，其可能的结果。|
|**2025-04-11**|**Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks**|Ye Ye et.al.|[2504.08525](http://arxiv.org/abs/2504.08525)|**[link](https://github.com/biubiutomato/tme-agent)**|**大型语言模型（LLMs）越来越多地被用作多步任务的自主代理。然而，大多数现有的框架未能维持对任务状态的有序理解，通常依赖于线性的提示串联或浅层内存缓冲区。这导致性能脆弱、频繁出现幻觉以及长期一致性差。在本研究中，我们提出了任务内存引擎（TME），这是一个轻量级的结构化内存模块，使用层次化的任务内存树（TMT）跟踪任务执行。树中的每个节点对应一个任务步骤，存储相关的输入、输出、状态和子任务关系。我们引入了一种提示合成方法，该方法根据活跃节点路径动态生成LLM提示，显著提高了执行一致性和情境定位。通过多步代理任务案例研究和比较实验，我们证明TME能带来更高的任务完成准确性和更可解释的行为，同时实现开销最小化。核心TME组件的参考实现可在https://github.com/biubiutomato/TME-Agent找到，包括基本示例和结构化内存集成。尽管当前实现使用基于树的架构，但TME被设计为支持图感知，包括可重用子步骤、汇聚的任务路径和共享依赖。这为未来基于DAG的内存架构奠定了基础。**|
|**2025-04-11**|**Adopting Large Language Models to Automated System Integration**|Robin D. Pesl et.al.|[2504.08490](http://arxiv.org/abs/2504.08490)|null|现代企业计算系统通过集成众多子系统，通过涌现行为来解决共同任务。一种广泛采用的方法是使用基于Web技术的服务，如REST或OpenAPI，分别提供交互机制和服务文档标准。每个服务代表特定的业务功能，允许封装和更易于维护。尽管在单个服务级别上降低了维护成本，但集成复杂性却增加了。因此，出现了自动服务组合方法来缓解这个问题。然而，由于这些方法依赖于复杂的正式建模，它们在实践中并没有得到广泛接受。在本篇博士论文中，我们分析了将大型语言模型（LLMs）应用于基于自然语言输入自动集成服务的方法。结果是可重用的服务组合，例如作为程序代码。虽然并不总是生成完全正确的结果，但结果仍然可以通过为集成工程师提供对合适解决方案的近似，从而帮助他们以较少的努力使服务投入运行。我们的研究包括：（i）引入一种使用LLMs进行自动服务组合的软件架构，（ii）分析检索增强生成（RAG）在服务发现中的应用，（iii）提出一种基于自然语言查询的服务发现新基准，以及（iv）将基准扩展到完整的服务组合场景。我们已将我们的软件架构命名为Compositio Prompto，分析了RAG在服务发现中的应用，并提交了服务发现基准的提案。开放性问题主要在于将服务发现基准扩展到服务组合场景，以及改进服务组合生成，例如使用微调或LLM代理。|
|**2025-04-10**|**Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems**|Simon Lermen et.al.|[2504.07831](http://arxiv.org/abs/2504.07831)|null|我们展示了人工智能代理如何通过神经网络的可解释性自动化来协调欺骗监管系统。使用稀疏自编码器（SAEs）作为我们的实验框架，我们表明语言模型（Llama、DeepSeek R1和Claude 3.7 Sonnet）可以生成欺骗性解释以逃避检测。我们的代理采用隐写术方法在看似无辜的解释中隐藏信息，成功欺骗了监管模型，同时实现了与参考标签相当的解释质量。我们进一步发现，当模型认为检测到有害特征可能会对自己产生负面影响时，它们可以制定欺骗策略。所有测试的LLM代理都能够欺骗监管者，同时实现与参考标签相当的高可解释性评分。我们最后提出缓解策略，强调对欺骗进行稳健理解和防御的迫切需要。|
|**2025-04-10**|**MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations**|Genglin Liu et.al.|[2504.07830](http://arxiv.org/abs/2504.07830)|**[link](https://github.com/genglinliu/MOSAIC)**|我们提出了一种新颖的开源社交网络模拟框架MOSAIC，其中生成式语言代理预测用户行为，如点赞、分享和标记内容。这个模拟将LLM代理与有向社交图相结合，以分析涌现的欺骗行为，并更好地理解用户如何确定在线社交内容的真实性。通过从多样化的细粒度角色构建用户表示，我们的系统使多代理模拟能够在规模上模拟内容传播和参与动态。在这个框架中，我们通过模拟虚假信息的传播评估了三种不同的内容审查策略，我们发现它们不仅减轻了非事实内容的传播，还增加了用户参与度。此外，我们分析了模拟中流行内容的轨迹，并探讨了模拟代理为他们的社交互动所阐述的推理是否真正与他们的集体参与模式一致。我们将我们的模拟软件开源，以鼓励在人工智能和社会科学领域进行进一步的研究。|
|**2025-04-09**|**Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration**|Kostas Hatalis et.al.|[2504.06943](http://arxiv.org/abs/2504.06943)|null|基于大型语言模型（LLMs）的智能体最近在各种任务中展示了令人印象深刻的性能。然而，它们在需要特定、结构化知识、灵活性和可解释决策的任务中仍存在局限性。虽然智能体能够感知其环境、形成推理、规划和执行行动以实现目标，但它们往往面临幻觉和交互过程中缺乏上下文记忆等问题。本文探讨了案例推理（CBR）策略如何整合到LLM智能体框架中，该策略通过参考以往经验来解决新问题。这种整合使得LLMs能够利用显性知识，从而提高其有效性。我们系统地回顾了这些增强智能体的理论基础，确定了关键框架组件，并构建了一个用于案例检索、适应和学习的CBR过程数学模型。我们还评估了CBR增强智能体与其他方法（如思维链推理和标准检索增强生成）的相对优势。此外，我们探讨了通过目标驱动的自主机制利用CBR的认知维度（包括自我反思、内省和好奇心）如何进一步增强LLM智能体的能力。这项工作为神经符号混合系统的研究做出了贡献，将CBR定位为一种增强自主LLM智能体推理能力和认知方面的可行技术。|
|**2025-04-09**|**FamilyTool: A Multi-hop Personalized Tool Use Benchmark**|Yuxin Wang et.al.|[2504.06766](http://arxiv.org/abs/2504.06766)|**[link](https://github.com/yxzwang/FamilyTool)**|将工具学习与大型语言模型（LLMs）的集成扩展了它们处理复杂任务的能力，通过利用外部工具。然而，现有的工具学习基准测试未能充分解决关键的现实世界个性化场景，尤其是那些需要多跳推理和在动态环境中进行归纳知识适应的场景。为了填补这一差距，我们引入了FamilyTool，这是一个基于家庭知识图谱（KG）的全新基准，它模拟了个性化的、多跳工具使用场景。FamilyTool向LLMs提出了跨越1到3个关系跳转的查询（例如，推断家族联系和偏好），并纳入了一个归纳KG设置，其中模型必须适应未见过的用户偏好和关系，而无需重新训练，这是先前方法中常见的限制，这损害了泛化能力。我们进一步提出了KGETool：一个简单的KG增强评估流程，以系统地评估LLMs在这些环境中的工具使用能力。实验揭示了最先进的LLMs之间的重要性能差距，随着跳转复杂性的增加，准确率急剧下降，归纳场景暴露出严重的泛化缺陷。这些发现强调了当前LLMs在处理个性化、不断变化的现实世界环境中的局限性，并突出了在工具学习框架中取得进步的迫切需要。FamilyTool作为评估和推进LLM代理在复杂、动态环境中推理、适应性和可扩展性的关键资源。代码和数据集可在GitHub上获取。|
|**2025-04-09**|**Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis**|Umakanta Maharana et.al.|[2504.06581](http://arxiv.org/abs/2504.06581)|**[link](https://github.com/Umakantamaharana/right-prediction-wrong-reasoning)**|大型语言模型（LLMs）提供了一种有前景的预筛选工具，有助于早期疾病检测，并为弱势群体提供更好的医疗保健服务。早期诊断各种疾病仍然是医疗保健领域的一个重大挑战，这主要是因为早期症状的非特异性、专家医疗人员的短缺以及需要长期的临床评估，所有这些都会延误治疗并影响患者预后。LLMs在预测多种疾病方面表现出令人印象深刻的准确性，具有革命性地改变各种医疗状况的临床预筛选和决策的潜力。在本研究中，我们使用真实世界患者数据研究了LLMs对类风湿性关节炎（RA）的诊断能力。收集了与医疗专家诊断同步的患者数据，并将LLMs的性能与专家对RA疾病预测的诊断进行了比较。我们在疾病诊断中注意到一个有趣的模式，并发现预测与解释之间存在意外的“不匹配”。我们使用不同的LLM代理进行了一系列多轮分析。表现最好的模型大约95%的时间准确预测类风湿性关节炎（RA）疾病。然而，当医疗专家评估模型生成的推理时，他们发现近68%的推理是错误的。这项研究突显了LLMs高预测准确性与其有缺陷的推理之间的明显不匹配，引发了关于在临床环境中依赖LLM解释的重要问题。\textbf{LLMs提供了错误的推理来得出正确的RA疾病诊断结果。}|
|**2025-04-08**|**FEABench: Evaluating Language Models on Multiphysics Reasoning Ability**|Nayantara Mudur et.al.|[2504.06260](http://arxiv.org/abs/2504.06260)|**[link](https://github.com/google/feabench)**|**在工程和科学领域，构建精确的模拟世界并使用数值求解器解答定量问题是一个基本要求。我们提出了FEABench，这是一个基准测试，用于评估大型语言模型（LLMs）和LLM代理使用有限元分析（FEA）模拟和解决物理、数学和工程问题的能力。我们引入了一种全面的评估方案，以研究LLMs通过推理自然语言问题描述并操作COMSOL Multiphysics $^\circledR$ （一款FEA软件）来计算答案的能力。此外，我们设计了一个具备通过应用程序编程接口（API）与软件交互、检查输出并使用工具在多次迭代中改进解决方案的能力的语言模型代理。我们最佳策略中88%的时间生成可执行的API调用。能够成功与FEA软件交互并操作的LLM，例如在基准测试中的那些问题，将推动工程自动化的前沿。获得这种能力将增强LLMs的推理技能，并借助数值求解器的精确性，推进解决现实世界复杂问题的自主系统的开发。代码可在https://github.com/google/feabench上获取。**|
|**2025-04-07**|**How to evaluate control measures for LLM agents? A trajectory from today to superintelligence**|Tomek Korbak et.al.|[2504.05259](http://arxiv.org/abs/2504.05259)|null|随着LLM智能体自主造成伤害的能力不断增强，AI开发者将依赖于日益复杂的控制措施来防止可能存在偏差的智能体造成伤害。AI开发者可以通过运行控制评估来证明他们的控制措施是足够的：这是一项测试练习，其中红队产生试图规避控制措施的智能体。为了确保控制评估能够准确捕捉到偏差风险，红队所获得的能力应该适应受控制措施部署的智能体的能力特征。在本文中，我们提出了一种系统框架，用于调整红队的权限以适应AI能力的提升。我们不是假设智能体将始终执行人类已知的最优攻击策略，而是展示了智能体的实际能力特征知识如何可以指导成比例的控制评估，从而产生更实用和成本效益更高的控制措施。我们通过考虑一系列五个虚构模型（M1-M5）及其能力水平的逐步提升，定义了五个不同的AI控制级别（ACLs）来阐述我们的框架。对于每个ACL，我们提供了控制评估、控制措施和安全案例的示例规则。最后，我们说明了为什么为超级智能的LLM智能体构建令人信服的AI控制安全案例将需要研究突破，并强调了我们可能最终需要采用减轻偏差风险的替代方法。|
|**2025-04-07**|**BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents**|Haoxuan Li et.al.|[2504.04855](http://arxiv.org/abs/2504.04855)|null|检测结构化数据中的偏见是一项复杂且耗时的任务。现有的自动化技术数据类型多样性有限，并且高度依赖人工逐案处理，导致缺乏普遍性。目前，基于大型语言模型（LLM）的代理在数据科学领域取得了显著进步，但它们检测数据偏见的能力尚未得到充分探索。为了解决这一差距，我们引入了第一个端到端、多代理协同框架BIASINSPECTOR，旨在根据特定用户需求自动检测结构化数据中的偏见。它首先制定一个多阶段计划来分析用户指定的偏见检测任务，然后使用多样且适宜的工具集来实施。它提供详细的结果，包括解释和可视化。为了解决缺乏评估LLM代理检测数据偏见能力标准化框架的问题，我们进一步提出一个全面的基准，包括多个评估指标和大量测试案例。广泛的实验表明，我们的框架在结构化数据偏见检测方面实现了卓越的整体性能，为更公平的数据应用设定了新的里程碑。|
|**2025-04-07**|**scAgent: Universal Single-Cell Annotation via a LLM Agent**|Yuren Mao et.al.|[2504.04698](http://arxiv.org/abs/2504.04698)|null|细胞类型注释对于理解细胞异质性至关重要。基于单细胞RNA测序数据和深度学习模型，在特定组织中注释固定数量的细胞类型已取得良好进展。然而，能够跨组织泛化、发现新型细胞类型并扩展到新型细胞类型的通用细胞注释研究还相对较少。为了填补这一空白，本文提出了一种基于大型语言模型（LLMs）的通用细胞注释框架scAgent。scAgent能够在多种组织中识别细胞类型并发现新型细胞类型；此外，它在学习新型细胞类型时数据效率高。在160种细胞类型和35种组织中的实验研究表明，scAgent在通用细胞类型注释、新型细胞发现以及扩展到新型细胞类型方面表现出优异的性能。|
|**2025-04-07**|**SciSciGPT: Advancing Human-AI Collaboration in the Science of Science**|Erzhuo Shao et.al.|[2504.05559](http://arxiv.org/abs/2504.05559)|null|随着大规模数据集的日益可用，许多科学领域取得了快速进展，创造了前所未有的研究机会，同时也带来了重大的分析挑战。近年来，大型语言模型（LLMs）和AI代理的进展为人类-人工智能合作开辟了新的可能性，为应对这一复杂的研究领域提供了强大的工具。在本文中，我们介绍了SciSciGPT，这是一个开源的原型AI协作者，它将科学科学作为试验台来探索由LLM驱动的科研工具的潜力。SciSciGPT自动化复杂的流程，支持多样化的分析方法，加速研究原型设计和迭代，并促进可重复性。通过案例研究，我们展示了其简化各种实证和分析研究任务的能力，同时突显了其更广泛地推进研究潜力的可能性。我们进一步提出了一个LLM代理能力成熟模型，以促进人类-人工智能合作，展望了一条改进和扩展SciSciGPT等框架的道路。随着人工智能能力的持续发展，像SciSciGPT这样的框架在科学研究和发现中可能扮演越来越关键的角色，开启更多的机会。同时，这些新的进展也提出了关键挑战，从确保透明度和伦理使用到平衡人类和人工智能的贡献。解决这些问题可能会塑造科学探究的未来，并指导我们如何培养下一代科学家在日益人工智能集成的科研生态系统中茁壮成长。|
|**2025-04-06**|**Building LLM Agents by Incorporating Insights from Computer Systems**|Yapeng Mi et.al.|[2504.04485](http://arxiv.org/abs/2504.04485)|null|近年来，由大型语言模型（LLM）驱动的自主代理成为了一个有前景的研究方向。然而，许多LLM代理的设计往往是基于经验或直觉，常常缺乏系统性的设计原则，这导致代理结构多样化但普遍性和可扩展性有限。在本文中，我们主张通过结合计算机系统的见解来构建LLM代理。受冯·诺伊曼架构的启发，我们提出了一种结构化的框架，用于LLM代理系统，强调模块化设计和通用原则。具体来说，本文首先从计算机系统角度对LLM代理进行了全面综述，然后根据计算机系统设计识别了关键挑战和未来方向，并最终探讨了LLM代理超出计算机系统的学习机制。从这种比较分析中获得的见解为系统性的LLM代理设计和进步奠定了基础。|
|**2025-04-06**|**Human-Level Competitive Pokémon via Scalable Offline Reinforcement Learning with Transformers**|Jake Grigsby et.al.|[2504.04395](http://arxiv.org/abs/2504.04395)|null|竞争性宝可梦单打（CPS）是一款流行的策略游戏，玩家需要根据战斗中超过一百次随机回合的不完美信息来利用对手。在CPS中的AI研究主要由启发式树搜索和在线自我博弈引领，但该游戏也可能成为研究在大型数据集上离线训练的适应性策略的平台。我们开发了一个流程，从保存的观众第三人称视角的日志中重建代理的第一人称视角，从而解锁了一个跨越十多年且每天都在增长的包含真实人类战斗的数据集。这个数据集允许我们采用黑盒方法，其中我们训练大型序列模型，仅根据它们的输入轨迹来适应对手，同时选择动作而不进行任何明确的搜索。我们在宝可梦最古老的四个（且观察最为不全面的）游戏世代的核心竞争环境中研究了从模仿学习到离线强化学习和自我博弈数据的离线微调的进展。结果代理优于最近的LLM代理方法和强大的启发式搜索引擎。在匿名参与在线人类战斗时，我们最好的代理攀升到活跃玩家排名的前10%。|
|**2025-04-06**|**AutoPDL: Automatic Prompt Optimization for LLM Agents**|Claudio Spiess et.al.|[2504.04365](http://arxiv.org/abs/2504.04365)|null|大型语言模型（LLMs）的性能取决于它们的提示方式，选择范围包括高级提示模式（例如，Zero-Shot、CoT、ReAct、ReWOO）和特定提示内容（指令和少量示例）。手动调整这种组合既费时费力，又容易出错，且在LLMs或任务之间难以迁移。因此，本文提出了一种名为AutoPDL的自动化方法，用于发现良好的LLM代理配置。我们的方法将此问题框架化为在代理和非代理提示模式和示例的组合空间上的结构化AutoML问题，使用连续减半法来有效地导航这一空间。我们引入了一个库，使用PDL提示编程语言实现常见的提示模式。AutoPDL解决方案是可读、可编辑和可执行的PDL程序，使用这个库。这种方法还实现了源到源优化，允许人工介入进行细化和重复使用。在三个任务和六个LLMs（参数量从8B到70B）上的评估表明，准确率有持续的提升（平均提高9.5±17.5个百分点），最高可达68.9个百分点，并揭示了所选提示策略在模型和任务之间的差异。|
|**2025-04-06**|**OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning**|Shihao Wang et.al.|[2504.04348](http://arxiv.org/abs/2504.04348)|null|随着视觉-语言模型（VLMs）的进步，对自动驾驶领域产生了越来越大的兴趣，因为这些模型强大的推理能力。然而，将这些能力从2D扩展到完整的3D理解对于实际应用至关重要。为了应对这一挑战，我们提出了OmniDrive，这是一个整体视觉-语言数据集，通过反事实推理将代理模型与3D驾驶任务相匹配。这种方法通过评估潜在场景及其结果来增强决策，类似于人类驾驶员考虑替代行为。我们的基于反事实的合成数据标注过程生成大规模、高质量的数据集，提供了更密集的监督信号，连接了规划轨迹和基于语言的推理。此外，我们探索了两种高级OmniDrive-Agent框架，即Omni-L和Omni-Q，以评估视觉-语言对齐与3D感知的重要性，揭示了设计有效LLM-代理的关键见解。在DriveLM问答基准和nuScenes开环规划上的显著改进证明了我们数据集和方法的有效性。|
|**2025-04-06**|**CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization**|Weiwei Sun et.al.|[2504.04310](http://arxiv.org/abs/2504.04310)|**[link](https://github.com/sunnweiwei/co-bench)**|尽管基于大型语言模型（LLM）的代理在软件工程和机器学习研究等领域引起了广泛关注，但它们在组合优化（CO）领域的应用仍然相对较少。这一差距突显了对它们解决结构化、约束密集型问题潜力的深入理解的需求——这种追求目前受到缺乏系统研究全面基准的限制。为了解决这个问题，我们引入了CO-Bench，这是一个包含来自广泛领域和复杂程度的36个现实世界CO问题的基准套件。CO-Bench包括结构化问题表述和精选数据，以支持对LLM代理的严格研究。我们评估了多个代理框架与已建立的人类设计算法，揭示了当前方法的要点优势和局限性，并确定了未来研究的有希望的方向。CO-Bench可在https://github.com/sunnweiwei/CO-Bench上公开获取。|
|**2025-04-05**|**Among Us: A Sandbox for Agentic Deception**|Satvik Golechha et.al.|[2504.04072](http://arxiv.org/abs/2504.04072)|null|研究AI代理的欺骗行为至关重要且困难，因为这缺乏能够引发行为的模型生物和沙盒，而无需让模型在特定条件下行动或插入有意的后门。在扩展基于文本的社会推理游戏环境 $\textit{AmongAgents}$ 的基础上，我们旨在通过引入Among Us（我们之中）作为丰富的沙盒来解决这个问题，在其中LLM代理在与其他代理或人类思考和行动时，能够自然地表现出人类风格的欺骗行为。我们引入了欺骗ELO作为衡量欺骗能力的无界指标，提出前沿模型之所以获胜，是因为它们在欺骗方面更出色，而不是在检测欺骗方面。我们评估了AI安全技术（LLM对输出的监控、在各种数据集上的线性探测和稀疏自编码器）在Among Us中检测说谎和欺骗的有效性，并发现它们在分布外具有很强的泛化能力。我们将我们的沙盒开源，作为未来对齐研究的基准，并希望这可以作为一个良好的测试平台，以改善检测和消除由代理动机驱动的欺骗的安全技术，并预测LLMs的欺骗能力。|
|**2025-04-04**|**Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective**|Garry A. Gabison et.al.|[2504.03255](http://arxiv.org/abs/2504.03255)|null|基于大型语言模型（LLM）驱动的代理系统正变得越来越复杂和强大。它们不断增长的代理能力和扩展的部署环境引起了人们对有效治理政策、监控和控制协议的日益关注。基于代理市场的最新发展趋势，我们从委托代理的角度分析了来自LLM代理及其扩展系统的潜在责任问题。我们的分析补充了现有基于风险的关于人工代理的研究，并涵盖了委托代理关系的重要方面及其在部署中的潜在后果。此外，我们激励了在可解释性、行为评估、奖励和冲突管理以及通过原则性工程检测和应急机制来缓解错位和不端行为的治理方法的发展。通过展示基于LLM的代理系统在AI责任方面的突出问题，我们旨在提供有关系统设计、审计和监控方法的信息，以增强透明度和问责制。|
|**2025-04-04**|**Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool Empowered LLM Agents**|Zichuan Li et.al.|[2504.03111](http://arxiv.org/abs/2504.03111)|null|大型语言模型（LLM）代理是利用一套工具进行推理和规划，以解决问题的一类自主系统。然而，在LLM代理中集成多工具能力带来了在安全地管理工具、确保工具兼容性、处理依赖关系以及保护LLM代理工作流程中的控制流等方面的挑战。在本文中，我们提出了对多工具启用LLM代理中的任务控制流进行的首个系统安全分析。我们确定了一种新型威胁，即跨工具收割与污染（XTHP），它包括多个攻击向量，首先劫持代理任务的正常控制流，然后收集并污染LLM代理系统中的机密或私人信息。为了了解这种威胁的影响，我们开发了Chord，一种旨在自动检测易受XTHP攻击的实世界代理工具的动态扫描工具。我们对来自两个主要LLM代理开发框架LangChain和LlamaIndex的73个实世界工具进行了评估，揭示了重大的安全担忧：80%的工具容易受到劫持攻击，78%受到XTH攻击，41%受到XTP攻击，凸显了这种威胁的普遍性。|
|**2025-04-02**|**Automated Survey Collection with LLM-based Conversational Agents**|Kurmanbek Kaiyrbekov et.al.|[2504.02891](http://arxiv.org/abs/2504.02891)|null|研究目标：传统的基于电话的调查是收集生物医学和医疗数据中最易获取和广泛使用的方法之一，然而，它们通常成本高昂、劳动密集，且难以有效扩展。为了克服这些局限性，我们提出了一种由对话大型语言模型（LLMs）驱动的端到端调查收集框架。  材料与方法：我们的框架包括一名负责设计调查和招募参与者的研究人员，一个由LLM驱动的对话电话代理，该代理致电参与者并执行调查，第二个LLM（GPT-4o）用于分析调查期间生成的对话记录，以及一个用于存储和组织结果的数据库。为了测试我们的框架，我们招募了8名参与者，包括5名英语母语者和3名非英语母语者，并进行了40次调查。我们评估了LLM生成的对话记录的正确性、GPT-4o推断的调查响应的准确性以及参与者的整体体验。  结果：尽管对话记录的平均每行单词错误率为7.7%，但GPT-4o成功从对话记录中提取了调查响应，平均准确率为98%。虽然参与者指出对话LLM代理偶尔会犯错误，但他们报告称代理有效地传达了调查的目的，表现出良好的理解力，并保持了吸引人的互动。  结论：我们的研究突出了LLM代理在开展和分析电话调查以应用于医疗保健方面的潜力。通过减轻人类面试官的工作负担并提供可扩展的解决方案，这种方法为现实世界端到端人工智能驱动的电话调查收集系统铺平了道路。|
|**2025-04-01**|**GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments**|Enjun Du et.al.|[2504.00711](http://arxiv.org/abs/2504.00711)|null|基础模型时代已经彻底改变了人工智能研究，然而图基础模型（GFMs）仍然受限于大规模图语料库的稀缺。传统的图数据合成技术主要关注简单的结构操作，缺乏生成具有意义文本属性的语义丰富节点的能力：这是实际应用中的一个关键限制。虽然大型语言模型（LLMs）展示了卓越的文本生成能力，但它们直接应用于图合成的应用受到上下文窗口限制、幻觉现象和结构一致性挑战的阻碍。为了解决这些问题，我们引入了GraphMaster，这是第一个专门为数据有限环境中的图数据合成设计的多智能体框架。GraphMaster协调四个专门的LLM智能体（管理者、感知、增强和评估），通过迭代优化协同优化合成过程，确保语义一致性和结构完整性。为了严格评估我们的方法，我们创建了六个标准图基准的新的数据有限“Sub”变体，专门设计用于在现实约束下测试合成能力。此外，我们开发了一个新的可解释性评估框架，该框架结合了人工评估和基于 Grassmannian 流形分析的原理性分析，提供了语义一致性的定性和定量度量。实验结果表明，GraphMaster在多个数据集上显著优于传统的合成方法，为在数据稀缺环境中推进GFMs奠定了坚实的基础。|
|**2025-04-01**|**Automated detection of atomicity violations in large-scale systems**|Hang He et.al.|[2504.00521](http://arxiv.org/abs/2504.00521)|null|原子性违反在中断驱动程序中给关键系统的软件安全带来了重大威胁。这些违反发生在共享资源操作的执行序列被异步中断打断时。由于程序状态空间庞大、应用层代码依赖以及复杂的领域特定知识，检测原子性违反具有挑战性。我们提出了Clover，一个将静态分析与大型语言模型（LLM）代理集成的混合框架，以检测现实世界程序中的原子性违反。Clover首先执行静态分析以提取关键代码片段和操作信息。然后启动一个多代理过程，其中专家代理利用领域特定知识来检测原子性违反，随后由裁判代理进行验证。在RaceBench 2.1、SV-COMP和RWIP上的评估表明，Clover实现了92.3%/86.6%的精确率/召回率，在F1分数上优于现有方法27.4-118.2%。|
|**2025-03-29**|**CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis**|Anjiang Wei et.al.|[2503.23145](http://arxiv.org/abs/2503.23145)|null|归纳程序合成，或称为示例编程，需要从输入输出示例中合成函数，这些函数能够泛化到未见过的输入。虽然大型语言模型代理在自然语言指导的编程任务中显示出潜力，但它们在归纳程序合成方面的能力尚未得到充分探索。现有的评估协议依赖于静态的示例集和保留的测试集，当合成的函数错误时无法提供反馈，并且未能反映现实世界场景，如逆向工程。我们提出了CodeARC，即代码抽象和推理挑战，这是一个新的评估框架，其中代理通过向隐藏的目标函数查询新输入、合成候选函数以及使用微分测试预言家迭代地改进其解决方案来与目标函数交互。这种交互式环境鼓励代理根据反馈进行函数调用和自我纠正。我们构建了第一个用于通用归纳程序合成的规模化基准，包含1114个函数。在评估的18个模型中，o3-mini表现最佳，成功率为52.7%，突显了这项任务的难度。在精心挑选的合成轨迹上微调LLaMA-3.1-8B-Instruct可以获得高达31%的相对性能提升。CodeARC为评估基于LLM的程序合成和归纳推理提供了一个更真实和更具挑战性的测试平台。|
|**2025-03-28**|**WorkTeam: Constructing Workflows from Natural Language with Multi-Agents**|Hanchao Liu et.al.|[2503.22473](http://arxiv.org/abs/2503.22473)|null|工作流在通过编排多个工具或组件的复杂流程来提高企业效率方面发挥着至关重要的作用。然而，手工构建工作流需要专业知识，这构成了重大的技术障碍。近年来，大型语言模型（LLMs）在从自然语言指令（即NL2Workflow）生成工作流方面取得了进步。然而，现有的基于单个LLM代理的方法由于需要专业知识以及任务切换带来的压力，在复杂任务上面临性能下降的问题。为了应对这些挑战，我们提出了WorkTeam，这是一个多代理NL2Workflow框架，包括一个主管、一个指挥官和一个填充代理，每个代理都具有不同的角色，共同提高转换过程。由于目前没有公开可用的NL2Workflow基准数据集，我们还引入了HW-NL2Workflow数据集，该数据集包含用于训练和评估的3,695个真实世界的商业样本。实验结果表明，我们的方法显著提高了工作流构建的成功率，为企业NL2Workflow服务提供了一种新颖而有效的解决方案。|
|**2025-03-27**|**MemInsight: Autonomous Memory Augmentation for LLM Agents**|Rana Salama et.al.|[2503.21760](http://arxiv.org/abs/2503.21760)|null|大型语言模型（LLM）智能体已经发展到能够智能处理信息、做出决策以及与用户或工具交互。一个关键能力是整合长期记忆功能，使这些智能体能够利用历史交互和知识。然而，内存大小的增长和语义结构的需求带来了重大挑战。在这项工作中，我们提出了一种自主记忆增强方法，称为MemInsight，以增强语义数据的表示和检索机制。通过利用自主增强来处理历史交互，LLM智能体被证明能够提供更准确和上下文化的响应。我们通过三个任务场景实证验证了我们提出方法的有效性：对话推荐、问答和事件摘要。在LLM-REDIAL数据集上，MemInsight将推荐的说服力提高了高达14%。此外，它在LoCoMo检索的召回率方面优于RAG基线34%。我们的实证结果表明，MemInsight具有增强LLM智能体在多个任务中上下文性能的潜力。|
|**2025-03-27**|**GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics**|Arsham Gholamzadeh Khoee et.al.|[2503.21735](http://arxiv.org/abs/2503.21735)|null|确保软件发布决策的可靠性和有效性至关重要，尤其是在汽车系统这样的安全关键领域。对发布验证数据的精确分析，通常以表格形式呈现，在这个过程中发挥着关键作用。然而，依赖于对大量测试数据集和验证指标进行手动分析的传统的分析方法容易延误且成本高昂。大型语言模型（LLMs）提供了一种有希望的替代方案，但它们在分析推理、上下文理解、处理超出范围的查询以及一致处理结构化测试数据方面面临挑战；这些局限性阻碍了它们在安全关键场景中的直接应用。本文介绍了一种名为GateLens的基于LLM的工具，用于分析汽车领域的表格数据。GateLens将自然语言查询转换为关系代数（RA）表达式，然后生成优化的Python代码。它在基准数据集上优于基线系统，实现了更高的F1分数，并以更大的鲁棒性处理复杂和模糊的查询。消融研究证实了RA模块的关键作用，当省略时，性能急剧下降。工业评估显示，GateLens将分析时间减少了80%以上，同时保持了高精度和可靠性。如所呈现的结果所示，GateLens在不需要依赖于少量示例的情况下实现了高性能，展示了在来自不同公司角色的各种查询类型上的强大泛化能力。与合作伙伴汽车公司部署GateLens的经验为将AI集成到关键工作流程，如发布验证，提供了实用指导。结果表明，通过自动化测试结果分析，GateLens使发布决策更快、更有信息量、更可靠，因此可以促进汽车系统中软件的可扩展性和可靠性。|
|**2025-03-27**|**debug-gym: A Text-Based Environment for Interactive Debugging**|Xingdi Yuan et.al.|[2503.21557](http://arxiv.org/abs/2503.21557)|null|大型语言模型（LLMs）越来越多地被用于编码任务，但在大多数情况下，人们假设所有相关信息要么可以在上下文中访问，要么与它们的训练数据相匹配。我们认为，LLMs可以从交互式探索代码库以收集与其任务相关的信息的能力中受益。为了实现这一点，我们提出了一种文本环境，即debug-gym，用于在交互式编码环境中开发基于LLM的代理。我们的环境轻量级，提供了一系列有用的工具，如Python调试器（pdb），旨在促进基于LLM的代理的交互式调试。除了编码和调试任务之外，这种方法可以推广到其他可以从LLM代理的信息搜索行为中受益的任务。|
|**2025-03-27**|**Large Language Model Agent: A Survey on Methodology, Applications and Challenges**|Junyu Luo et.al.|[2503.21460](http://arxiv.org/abs/2503.21460)|**[link](https://github.com/luo-junyu/awesome-agent-papers)**|**智能代理的时代已经到来，这得益于大型语言模型在革命性的进步。具有目标驱动行为和动态适应能力的大型语言模型（LLM）代理，可能代表通向通用人工智能的关键途径。本文综述通过以方法论为中心的税制，系统地解构LLM代理系统，将架构基础、协作机制和进化路径联系起来。我们通过揭示代理设计原则与它们在复杂环境中涌现行为之间的基本联系，统一了分散的研究线索。我们的工作提供了一个统一的架构视角，考察了代理是如何构建的、它们是如何协作的以及它们是如何随时间演化的，同时解决了评估方法、工具应用、实际挑战和多样化的应用领域。通过审视这一快速发展的领域的最新进展，我们为研究人员提供了一个理解LLM代理的结构化分类法，并确定了未来研究的有希望的方向。该集合可在https://github.com/luo-junyu/Awesome-Agent-Papers上获取。**|
|**2025-03-27**|**EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues**|Yuhan Liu et.al.|[2503.21080](http://arxiv.org/abs/2503.21080)|null|尽管基于大型语言模型（LLM）的聊天机器人在信用对话中已应用于有效互动，但它们动态情感表达的能力仍然有限。当前代理主要依赖于被动同理心而不是情感推理。例如，当面对持续的客户负面情绪时，代理应通过表达适度愤怒的策略性情感适应来阻止无效行为，并引导对话走向解决。这种情境感知的情感调节对于模仿人类谈判者的细微决策至关重要。本文介绍了一种EQ谈判者，它结合了来自预训练语言模型（PLM）的情感感知以及基于博弈论和隐马尔可夫模型的情感推理。它考虑了客户当前和历史上的情绪，以更好地管理和解决互动中的负面情绪。通过在公共情感数据集上微调预训练语言模型（PLM）并在信用对话数据集上进行验证，我们的方法使基于LLM的代理能够有效地捕捉客户情绪的变化，并根据我们在现实世界金融谈判中的情感决策策略动态调整他们的回应语气。这种EQ谈判者还可以帮助信用机构培养积极的客户关系，提升信用服务的满意度。|
|**2025-03-27**|**Debate-Driven Multi-Agent LLMs for Phishing Email Detection**|Ngoc Tuong Vy Nguyen et.al.|[2503.22038](http://arxiv.org/abs/2503.22038)|null|钓鱼攻击仍然是一个关键的网络安全威胁。攻击者不断改进他们的方法，使得钓鱼邮件更难被检测。传统的检测方法，包括基于规则的系统和监督机器学习模型，要么依赖于预定义的模式，如黑名单，这些模式可以通过微小的修改来规避，要么需要大量的数据集进行训练，但仍可能产生误报和漏报。在这项工作中，我们提出了一种多智能体大型语言模型（LLM）提示技术，该技术通过模拟智能体之间的辩论来检测电子邮件上呈现的内容是否为钓鱼邮件。我们的方法使用两个LLM智能体来为或反对分类任务提供论据，由一个法官智能体根据提供的推理质量来裁决最终判决。这种辩论机制使模型能够批判性地分析文本中的上下文线索和欺骗性模式，从而提高了分类的准确性。该框架在多个钓鱼邮件数据集上进行了评估，结果表明混合智能体配置始终优于同质配置。结果还显示，辩论结构本身就足以产生准确的决策，无需额外的提示策略。|
|**2025-03-26**|**Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs**|Yuxuan Lu et.al.|[2503.20749](http://arxiv.org/abs/2503.20749)|null|近期的研究表明，大型语言模型（LLM）可以通过仅使用提示的方法来模拟“可信”的人类行为，从而为LLM代理提供动力。在本工作中，我们专注于评估和改进LLM在网页动作生成任务中的客观“准确性”，而不是主观的“可信度”，利用从在线购物人类行为中收集的大规模、真实世界数据集。我们展示了在网页动作生成任务上对最先进LLM（例如DeepSeek-R1、Llama和Claude）的首次全面定量评估。我们的结果表明，在真实世界行为数据上微调LLM与仅使用提示的方法相比，显著提高了其生成动作的能力。此外，将合成的推理轨迹纳入模型训练中，导致了额外的性能提升，证明了在行为建模中明确理由的价值。这项工作为评估LLM在行为模拟方面建立了一个新的基准，并提供了关于如何通过真实世界动作数据和推理增强来提高LLM代理真实性的可行见解。|
|**2025-03-26**|**InfoBid: A Simulation Framework for Studying Information Disclosure in Auctions with Large Language Model-based Agents**|Yue Yin et.al.|[2503.22726](http://arxiv.org/abs/2503.22726)|null|在线广告系统中，出版商在信息披露策略上常常面临权衡：披露更多信息可以通过实现广告展示次数的最优分配来提高效率，但也可能通过降低竞争广告商之间的不确定性而损失潜在收入。与其他市场设计中的挑战类似，对这一权衡的理解受到对现实世界数据有限访问的限制，导致研究人员和实践者转向模拟框架。大型语言模型（LLMs）的近期出现为模拟提供了一种新颖的方法，它提供了类似于人类的推理和适应性，而无需依赖于对代理行为模型的明确假设。尽管它们具有潜力，但现有的框架还没有将基于LLM的代理整合到研究信息不对称和信号策略中，特别是在拍卖的背景下。为了解决这一差距，我们引入了InfoBid，这是一个灵活的模拟框架，它利用LLM代理来检验多代理拍卖环境中信息披露策略的影响。使用GPT-4o，我们实现了具有不同信息模式的第二价格拍卖的模拟。结果揭示了信号如何影响策略行为和拍卖结果的关键见解，这些见解与经济和社会学习理论相符。通过InfoBid，我们希望促进将LLMs作为人类经济和社会代理的替代品在实证研究中的应用，增强我们对它们能力和局限性的理解。这项工作弥合了理论市场设计与实际应用之间的差距，推动了市场模拟、信息设计和基于代理推理的研究，同时为探索数字经济动态提供了一个有价值的工具。|
|**2025-03-25**|**BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in Minecraft**|Eray Yapağcı et.al.|[2503.20036](http://arxiv.org/abs/2503.20036)|null|复现游戏中的错误，特别是在不断演变的游戏中（如Minecraft）中的崩溃错误，是一个众所周知的手动、耗时且具有挑战性的自动化过程。尽管在其他软件领域，LLM驱动的错误复现取得了成功，但游戏由于其复杂的交互环境，仍大部分未得到解决。本文介绍了一种名为BugCraft的新颖端到端框架，旨在直接从用户提交的错误报告中自动化复现Minecraft中的崩溃错误，填补了自动游戏错误复现的关键空白。BugCraft采用两阶段方法：首先，一个步骤合成器利用LLM和Minecraft维基知识将错误报告转换为高质量的、结构化的复现步骤（S2R）。其次，一个动作模型，由基于视觉的LLM代理（GPT-4o）和自定义宏API提供支持，在Minecraft中执行这些S2R步骤以触发报告的崩溃。为了便于评估，我们引入了BugCraft-Bench，这是一个精心制作的Minecraft崩溃错误报告数据集。在BugCraft-Bench上评估，我们的框架成功端到端复现了30.23%的崩溃错误。步骤合成器在生成正确的错误复现计划方面达到了66.28%的准确率，突显了其在解释和结构化错误报告信息方面的有效性。BugCraft展示了使用LLM在复杂游戏环境中自动复现崩溃错误的可行性，为游戏测试和开发开辟了有前景的道路。该框架和BugCraft-Bench数据集为未来在自动游戏错误分析方面的研究铺平了道路，并具有推广到其他交互式游戏平台的可能性。最后，我们在https://bugcraft2025.github.io/上公开了我们的代码。|
|**2025-03-24**|**EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments**|Sara Fish et.al.|[2503.18825](http://arxiv.org/abs/2503.18825)|null|我们为在未知环境中行动、学习和策略规划的LLM智能体开发了基准测试。这些环境中LLM智能体必须通过有意识的探索来逐步学习其规格。我们的基准测试包括源自经济学关键问题的决策任务。为了避免饱和，基准测试任务以可扩展的难度级别进行合成生成。此外，我们提出了试金石测试，这是一种针对LLM和LLM智能体的新类型的量化度量方法。与基准测试不同，试金石测试通过考虑LLM和LLM智能体在面对权衡（例如，效率与平等）时的行为，量化它们在性格、价值观和倾向方面的差异，在这些情况下没有客观上是正确或错误的行为。总体而言，我们的基准测试和试金石测试评估了LLM智能体在解决复杂经济问题方面的能力和倾向，这些问题涉及采购、调度、任务分配和定价等应用，随着这些智能体在经济发展中进一步整合，这些应用的重要性应该会增加。|
|**2025-03-24**|**Defeating Prompt Injections by Design**|Edoardo Debenedetti et.al.|[2503.18813](http://arxiv.org/abs/2503.18813)|null|大型语言模型（LLMs）越来越多地被部署在需要与外部环境交互的代理系统中。然而，当处理不受信任的数据时，LLM代理容易受到提示注入攻击。在本文中，我们提出了CaMeL，这是一种强大的防御机制，它围绕LLM创建一个保护层，即使在底层模型可能容易受到攻击的情况下也能确保其安全。为了运行，CaMeL明确地从（受信任的）查询中提取控制和数据流；因此，LLM检索到的不受信任数据永远不会影响程序流程。为了进一步提高安全性，CaMeL依赖于一种能力概念，以防止通过未经授权的数据流泄露私有数据。我们通过在AgentDojo [NeurIPS 2024]（一个最近的代理安全基准）中解决67%的任务并具有可证明的安全性，展示了CaMeL的有效性。|
|**2025-03-24**|**AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents**|Haoyu Wang et.al.|[2503.18666](http://arxiv.org/abs/2503.18666)|null|基于大型语言模型（LLM）构建的智能体正越来越多地被应用于各个领域，自动化复杂的决策和任务执行。然而，它们的自主性引入了安全风险，包括安全漏洞、法律违规和意外有害行为。现有的缓解方法，如基于模型的保障措施和早期执行策略，在鲁棒性、可解释性和适应性方面存在不足。为了应对这些挑战，我们提出了AgentSpec，一种轻量级的特定领域语言，用于指定和执行对LLM智能体的运行时约束。通过AgentSpec，用户定义包含触发器、谓词和执行机制的规则结构，确保智能体在预定义的安全边界内运行。我们在多个领域实现了AgentSpec，包括代码执行、具身智能体和自动驾驶，展示了其适应性和有效性。我们的评估表明，AgentSpec在超过90%的代码智能体案例中成功防止了不安全执行，在具身智能体任务中消除了所有危险行为，并使自动驾驶车辆（AV）达到了100%的合规性。尽管其安全保证很强，但AgentSpec在计算上仍然保持轻量级，开销在毫秒级。通过结合可解释性、模块化和效率，AgentSpec为在多样化的应用中执行LLM智能体安全提供了实际且可扩展的解决方案。我们还使用LLM自动生成规则并评估其有效性。我们的评估显示，由OpenAI o1生成的规则在具身智能体上达到了95.56%的精确度和70.96%的召回率，成功识别了87.26%的有风险代码，并在8种场景中的5种情况下防止了自动驾驶车辆违法。|
|**2025-03-24**|**A Survey of Large Language Model Agents for Question Answering**|Murong Yue et.al.|[2503.19213](http://arxiv.org/abs/2503.19213)|null|本文综述了基于大型语言模型（LLM）的问答（QA）代理的发展。传统代理面临着重大限制，包括大量数据需求和难以泛化到新环境。基于LLM的代理通过利用LLM作为其核心推理引擎来解决这些挑战。这些代理通过与外部环境互动，实现了比传统问答管道和简单的LLM问答系统更优越的问答结果。我们系统地回顾了LLM代理在问答任务中的设计，将我们的讨论组织在关键阶段：规划、问题理解、信息检索和答案生成。此外，本文还确定了当前面临的挑战，并探讨了未来研究方向，以提升LLM代理问答系统的性能。|
|**2025-03-23**|**AgentRxiv: Towards Collaborative Autonomous Research**|Samuel Schmidgall et.al.|[2503.18102](http://arxiv.org/abs/2503.18102)|null|科学发现的进步很少是单一“我发现了”时刻的结果，而是数百名科学家朝着共同目标逐步合作的结果。虽然现有的智能体工作流程能够自主进行研究，但它们这样做是孤立的，没有能力在先前的研究基础上持续改进。为了解决这些挑战，我们引入了AgentRxiv——一个框架，允许LLM智能体实验室上传和检索共享预印本服务器上的报告，以便进行合作、分享见解和迭代构建彼此的研究。我们要求智能体实验室开发新的推理和提示技术，并发现能够访问其先前研究的智能体相比孤立运行的智能体实现了更高的性能提升（在MATH-500上相对于基线有11.4%的相对改进）。我们发现，表现最好的策略可以推广到其他领域的基准测试（平均改进3.3%）。通过AgentRxiv共享研究的多个智能体实验室能够朝着共同目标共同努力，比孤立实验室进展更快，达到更高的整体准确性（在MATH-500上相对于基线有13.7%的相对改进）。这些发现表明，自主智能体可能在人类设计未来的AI系统时发挥作用。我们希望AgentRxiv能够使智能体朝着研究目标进行合作，并使研究人员能够加速发现。|
|**2025-03-23**|**An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models**|Riya Naik et.al.|[2503.17936](http://arxiv.org/abs/2503.17936)|null|自然语言作为人机交互的媒介长期以来一直被期待，随着具有惊人语言处理和生成能力的巨型语言模型（LLMs）的出现，正经历着翻天覆地的变化。我们中的许多人现在将LLMs视为现代的先知，向它们提出几乎任何类型的问题。与德尔菲神谕的前身不同，咨询LLM不必是单轮活动（提问、接收答案、离开）；而且——与皮提亚（Pythia）也不同——广泛认为，LLM的回答可以通过增加额外上下文来改进。在本文中，我们旨在研究在成功获得答案或得出问题无法回答的结论时，我们需要与LLM进行多轮交互的时机；我们提出了一种神经符号框架，该框架模拟了人类和LLM代理之间的交互。通过所提出的框架，我们将问题的不完整性和歧义性定义为可以从交互中交换的消息中推导出的属性，并提供了来自基准问题的结果，其中答案的正确性被证明取决于问题是否表现出不完整或歧义的存在（根据我们确定的属性）。我们的结果表明，对于包含高比例不完整或歧义性问题的数据集，通常需要多轮交互；增加交互长度具有减少不完整或歧义性的效果。结果还表明，我们衡量不完整性和歧义性的方法可以作为在问答问题中与LLM进行交互的有用工具。|
|**2025-03-22**|**CP-AgentNet: Autonomous and Explainable Communication Protocol Design Using Generative Agents**|Dae Cheol Kwon et.al.|[2503.17850](http://arxiv.org/abs/2503.17850)|null|尽管深度强化学习（DRL）已成为一种比现有手工定制通信协议做出更好决策的有力工具，但它面临着显著的局限性：1）选择适当的神经网络架构和设置超参数对于实现期望的性能水平至关重要，这需要领域专业知识。2）DRL模型中的决策过程通常不透明，通常被描述为“黑盒”。3）DRL模型对数据需求量大。作为回应，我们提出了CP-AgentNet，这是第一个旨在使用生成代理来开发通信网络协议的框架。这种方法通过创建一个用于协议设计的自主系统来应对这些挑战，显著减少了人力投入。我们为异构环境开发了LLMA（基于LLM代理的多种接入）和CPTCP（基于CP-Agent的TCP）。我们的全面模拟证明了LLMA和CPTCP与使用不同类型协议的节点高效共存，以及增强了可解释性。|
|**2025-03-21**|**CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities**|Yuxuan Zhu et.al.|[2503.17332](http://arxiv.org/abs/2503.17332)|**[link](https://github.com/uiuc-kang-lab/cve-bench)**|**大型语言模型（LLM）代理越来越能够自主进行网络攻击，对现有应用构成了重大威胁。这种不断增长的风险凸显了评估LLM代理利用网络应用漏洞能力的现实世界基准的紧迫需求。然而，现有的基准存在不足，因为它们局限于抽象的Capture the Flag竞赛或缺乏全面的覆盖。构建针对现实世界漏洞的基准需要专门的专业知识来重现利用手段，以及一种系统性地评估不可预测威胁的方法。为了应对这一挑战，我们引入了CVE-Bench，这是一个基于关键严重程度的通用漏洞和暴露（CVE）的现实世界网络安全基准。在CVE-Bench中，我们设计了一个沙盒框架，使得LLM代理能够在模拟现实世界条件的场景中利用有漏洞的Web应用，同时提供对其利用的有效评估。我们的评估表明，最先进的代理框架可以解决高达13%的漏洞。**|
|**2025-03-21**|**A-IDE : Agent-Integrated Denoising Experts**|Uihyun Cho et.al.|[2503.16780](http://arxiv.org/abs/2503.16780)|null|近年来，基于深度学习的降噪方法在低剂量CT图像质量方面取得了显著进展。然而，由于不同的HU分布和多样的解剖特征，单个模型往往难以泛化到多种解剖结构。为了解决这一局限性，我们提出了“智能体集成降噪专家（A-IDE）”框架，该框架在决策型语言模型智能体的管理下，集成了三个针对解剖区域专门的RED-CNN模型。智能体分析来自BiomedCLIP的语义线索，动态地将接收到的低剂量CT扫描路由到最合适的专家模型。我们强调了我们方法的三项主要优势。A-IDE在异质、数据稀缺的环境中表现出色。该框架通过在多个专家之间分配任务来自动防止过拟合。最后，我们由LLM驱动的智能体管道消除了手动干预的需求。在Mayo-2016数据集上的实验评估证实，与单个统一降噪器相比，A-IDE在RMSE、PSNR和SSIM方面取得了更好的性能。|
|**2025-03-21**|**Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent**|Humza Nusrat et.al.|[2503.17553](http://arxiv.org/abs/2503.17553)|null|放疗治疗计划制定是一个复杂且耗时的过程，经常受到计划者间差异和主观决策的影响。为了解决这些挑战，我们引入了剂量优化语言代理（DOLA），这是一个基于自主大型语言模型（LLM）的代理，旨在优化放疗治疗计划同时严格保护患者隐私。DOLA将LLaMa3.1 LLM直接与商业治疗计划系统集成，利用思维链提示、检索增强生成（RAG）和强化学习（RL）。该代理完全在安全本地基础设施中运行，消除了外部数据共享。我们使用18名接受60 Gy剂量、分20次放疗的前列腺癌患者的回顾性队列评估了DOLA，比较了模型大小（80亿参数 vs. 700亿参数）和优化策略（无-RAG、RAG和RAG+RL）在10次计划迭代中的表现。700亿参数模型显示出显著的性能提升，比80亿参数模型最终得分高出约16.4%。RAG方法比无-RAG基线高出19.8%，而引入RL加速了收敛，突显了基于检索的记忆和强化学习的协同效应。最优温度超参数分析确定了0.4提供了探索和利用之间最佳平衡。这一概念验证研究代表了首次成功部署本地托管LLM代理，在商业放疗计划系统中自主优化治疗计划。通过可解释的自然语言推理扩展人机交互，DOLA提供了一个可扩展且注重隐私的框架，具有显著的临床实施和工作流程改进潜力。|
|**2025-03-20**|**The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement**|Ruihan Yang et.al.|[2503.16024](http://arxiv.org/abs/2503.16024)|null|大型语言模型（LLMs）最近已从基于文本的助手转变为能够规划、推理并迭代改进其行为的自主代理。虽然数值奖励信号和验证器可以有效地对候选动作进行排序，但它们往往提供有限的环境指导。相比之下，自然语言反馈更好地与LLMs的生成能力相匹配，提供更丰富、更具操作性的建议。然而，对于基于LLM的代理来说，有效地解析和实施这种反馈可能具有挑战性。在这项工作中，我们引入了批判指导改进（CGI），这是一个新颖的双玩家框架，包括一个探索环境的演员模型和一个生成详细自然语言反馈的评论员模型。通过训练评论员生成细粒度的评估和可操作性的修订，以及训练演员利用这些评论，我们的方法促进了更稳健的替代策略探索，同时避免了局部最优。在三个交互式环境中的实验表明，CGI比现有基线有显著的改进。值得注意的是，即使是一个小的评论员模型在反馈质量上也超过了GPT-4。所得到的演员实现了最先进的性能，证明了显式迭代指导在增强基于LLM的代理决策能力方面的强大作用。|
|**2025-03-19**|**SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks**|Yifei Zhou et.al.|[2503.15478](http://arxiv.org/abs/2503.15478)|**[link](https://github.com/facebookresearch/sweet_rl)**|**大型语言模型（LLM）的智能体需要在现实任务中进行多轮交互。然而，现有的用于优化LLM智能体的多轮强化学习（RL）算法，在利用LLM的泛化能力的同时，未能有效地进行多轮的信用分配，并且如何开发此类算法仍然不明确。为了研究这一问题，我们首先介绍了一个新的基准，ColBench，在该基准中，LLM智能体与人类合作者通过多轮交互来解决后端编程和前端设计中的现实任务。基于这个基准，我们提出了一种新的RL算法，SWEET-RL（利用训练时信息的步级评估RL），该算法使用精心设计的优化目标来训练一个可以访问额外训练时信息的评论家模型。评论家为改进策略模型提供步级奖励。我们的实验表明，与其他最先进的单轮RL算法相比，SWEET-RL在ColBench上的成功率和胜率提高了6%，使得Llama-3.1-8B能够在现实内容创作中与GPT4-o相匹配或超过其性能。**|
|**2025-03-19**|**Exploring Large Language Models for Word Games:Who is the Spy?**|Chentian Wei et.al.|[2503.15235](http://arxiv.org/abs/2503.15235)|**[link](https://github.com/ct-wei/who-is-the-spy)**|**由于字谜游戏具有基于规则和情境的特点，在自然语言处理（NLP）、博弈论及相关领域具有显著的研究价值。本研究探讨了如何有效地将大型语言模型（LLMs）应用于字谜游戏，并提出了一种无需训练的框架。"谁是谁的间谍"或英文中的"Shei Shi Wo Di"是一个经典的字谜游戏。以这个游戏为例，我们引入了一个基于思维链（CoT）的调度框架，使LLMs能够在推断角色词和伪装身份等任务中取得优异表现。我们根据游戏成功率以及LLM代理分析结果的准确性来评估该框架的性能。实验结果表明，该框架的有效性得到了证实，展示了LLM性能在多个数据集上的显著提升。这项工作突出了LLMs在掌握结构化游戏环境中的情境推理和社会互动的潜力。我们的代码在https://github.com/ct-wei/Who-is-The-Spy上公开可用。**|
|**2025-03-19**|**Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models**|Man Fai Wong et.al.|[2503.15129](http://arxiv.org/abs/2503.15129)|null|本文研究了人工智能辅助编程和大型语言模型（LLM）如何通过GitHub Copilot和Amazon CodeWhisperer等AI工具（LLM代理）提高软件开发者的能力，同时通过整合人类反馈来增强强化学习（RLHF）以及通过众包计算来提升文本到代码的生成。此外，我们展示了我们的贝叶斯优化框架通过分配反馈收集负担，支持代码生成中的AI对齐，突出了收集高质量人类反馈的价值。我们的实证评估证明了这种方法的功效，展示了如何有效地训练LLM代理以改善文本到代码的生成。我们的贝叶斯优化框架可以设计用于通用领域特定语言，促进大型语言模型能力与人类反馈在人工智能辅助编程中的对齐。|
|**2025-03-19**|**ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents**|Hao Liang et.al.|[2503.14948](http://arxiv.org/abs/2503.14948)|null|协同感知因其能够通过与其他车辆代理交换信息来增强单个车辆的感知能力而受到广泛关注。然而，现有的协同感知系统受限于用户交互的不效率和多摄像头真实感视觉化的挑战。为了解决这些挑战，本文介绍了ChatStitch，这是第一个能够通过集成自然语言命令和外部数字资产揭示遮挡盲区信息的协同感知系统。为了有效地处理复杂或抽象的命令，ChatStitch采用基于大型语言模型的多个代理协同框架。为了实现对人类最直观的感知，ChatStitch提出了SV-UDIS，这是第一个在非全局重叠条件下进行的环绕视图无监督深度图像拼接方法。我们在UDIS-D、MCOV-SLAM公开数据集以及我们的真实世界数据集上进行了广泛的实验。具体来说，我们的SV-UDIS方法在UDIS-D数据集上的3、4和5图像拼接任务中实现了最先进的性能，分别将PSNR提高了9%、17%和21%，将SSIM提高了8%、18%和26%。|
|**2025-03-18**|**PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play**|Wei Fang et.al.|[2503.14432](http://arxiv.org/abs/2503.14432)|null|大型语言模型（LLMs）越来越多地与专用外部工具集成，然而许多任务需要零样本工具使用，并且文档可能最少或含噪声。现有的解决方案依赖于手动重写或标注数据进行验证，这使得它们在真正的零样本设置中不适用。为了解决这些挑战，我们提出了PLAY2PROMPT，这是一个自动框架，系统性地“玩耍”每个工具以探索其输入输出行为。通过这种迭代试错过程，PLAY2PROMPT优化工具文档并生成使用示例，而无需任何标注数据。这些示例不仅指导LLM推理，还作为验证以进一步增强工具利用。在真实世界任务上的大量实验表明，PLAY2PROMPT显著提高了开放和封闭模型在零样本工具性能方面的表现，为特定领域的工具集成提供了一种可扩展且有效的解决方案。|
|**2025-03-18**|**MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration**|Yisen Xu et.al.|[2503.14340](http://arxiv.org/abs/2503.14340)|null|维护和扩展软件系统在很大程度上依赖于有效的代码重构，但这一过程仍然劳动密集，需要开发者仔细分析现有代码库并防止引入新的缺陷。尽管最近的研究利用大型语言模型（LLMs）来自动化重构任务，但当前解决方案在范围上受限，缺乏保证代码可编译性和成功测试执行的机制。在这项工作中，我们引入了MANTRA，这是一个基于LLM代理的全面框架，用于自动化方法级重构。MANTRA集成了上下文感知检索增强生成、协调多代理协作和口头强化学习，以在重构过程中模拟人类的决策过程，同时保持代码的正确性和可读性。我们的实证研究基于从10个代表性Java项目中抽取的703个“纯重构”实例（即仅涉及结构改进的代码更改），涵盖了六种最常见的重构操作。实验结果表明，MANTRA在生成可编译并通过所有测试的代码方面显著优于基线LLM模型（RawGPT），成功率达到82.8%（582/703），而RawGPT仅为8.7%（61/703）。此外，与IntelliJ的LLM驱动的重构工具（EM-Assist）相比，MANTRA在生成提取方法转换方面提高了50%。一项涉及37位专业开发者的可用性研究表明，MANTRA执行的重构被认为与人类编写的代码一样可读和可重用，在某些情况下甚至更有优势。这些结果突出了MANTRA的实用优势，并强调了基于LLM的系统在推进软件重构任务自动化方面的日益增长的潜力。|
|**2025-03-18**|**Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection**|Tharindu Kumarage et.al.|[2503.15552](http://arxiv.org/abs/2503.15552)|null|随着对话代理，尤其是由大型语言模型（LLM）驱动的聊天机器人的快速发展，社交媒体平台上面临的社会工程（SE）攻击风险显著增加。由于这些对话的动态性，多轮、基于聊天的交互中的SE检测远比单次检测复杂。缓解这一威胁的关键因素在于理解SE攻击的工作机制，特别是攻击者如何利用漏洞以及受害者的性格特征如何导致其易受攻击。在这项工作中，我们提出了一种基于LLM的代理框架，SE-VSim，通过生成多轮对话来模拟SE攻击机制。我们通过模拟具有不同性格特征的受害者代理来评估心理特征如何影响易受操纵的程度。使用超过1000次模拟对话的数据集，我们分析了对手假装招聘人员、资助机构和记者以试图获取敏感信息的攻击场景。基于此分析，我们提出了一种概念验证，SE-OmniGuard，通过利用对受害者性格的了解、评估攻击策略和监控对话中的信息交换来识别潜在的SE尝试，从而为用户提供个性化的保护。|
|**2025-03-17**|**DAgent: A Relational Database-Driven Data Analysis Report Generation Agent**|Wenyi Xu et.al.|[2503.13269](http://arxiv.org/abs/2503.13269)|null|基于关系数据库驱动的数据分析（RDB-DA）报告生成，旨在查询关系数据库后生成数据分析报告，已在金融和医疗保健等领域得到广泛应用。通常，这些任务由数据科学家手动完成，使得整个过程非常劳动密集，显示出对自动化的明显需求。尽管已经提出了现有方法（例如表格问答或文本到SQL）来减少对人类的依赖，但它们无法处理需要多步推理、跨表关联以及将洞察力综合到报告中的复杂分析任务。此外，目前还没有可用于开发自动RDB-DA报告生成的数据集。为了填补这一空白，本文提出了一种用于RDB-DA报告生成任务的LLM代理系统，称为DAgent；此外，我们构建了一个用于自动数据分析报告生成的基准，包括新的数据集DA-Dataset和评估指标。DAgent集成了规划、工具和记忆模块，将自然语言问题分解为逻辑上独立的子查询，从关系数据库中准确检索关键信息，并通过多步推理和有效的数据集成生成满足完整性、正确性和简洁性要求的分析报告。在DA-Dataset上的实验分析表明，DAgent在检索性能和分析报告生成质量方面的优越性，展示了其在解决复杂数据库分析报告生成任务中的强大潜力。|
|**2025-03-17**|**Why Do Multi-Agent LLM Systems Fail?**|Mert Cemri et.al.|[2503.13657](http://arxiv.org/abs/2503.13657)|**[link](https://github.com/multi-agent-systems-failure-taxonomy/MASFT)**|**尽管对多智能体系统（MAS）的热情日益增长，其中多个LLM智能体协作完成任务，但它们在流行基准测试中的性能提升与单智能体框架相比仍然微乎其微。这一差距凸显了分析阻碍MAS有效性的挑战的必要性。在本文中，我们提出了对MAS挑战的首次全面研究。我们分析了超过150个任务中的五个流行的MAS框架，涉及六位专家人工标注员。我们识别出14种独特的失败模式，并提出了一种适用于各种MAS框架的全面分类法。这个分类法通过每个研究中的三位专家标注员之间的协商而逐步形成，实现了Cohen's Kappa得分为0.88。这些细粒度的失败模式被组织成三个类别：（i）规范和系统设计失败，（ii）智能体间不匹配，（iii）任务验证和终止。为了支持可扩展的评估，我们将MASFT与LLM作为裁判器相结合。我们还探讨了通过提出两种干预措施（改进智能体角色的规范和增强编排策略）是否可以轻松预防已识别的失败。我们的发现表明，已识别的失败需要更复杂的解决方案，突显了未来研究的明确路线图。我们开源了我们的数据集和LLM标注器。**|
|**2025-03-16**|**VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures**|Yoo Yeon Sung et.al.|[2503.12651](http://arxiv.org/abs/2503.12651)|null|AI从业者越来越多地在大型语言模型（LLM）代理中应用复合AI系统以解决复杂的推理任务，但这些代理的执行往往无法达到人类标准，导致影响系统整体性能的错误。由于代理的推理过程不透明、与人类期望不符、代理依赖的复杂性以及人工检查的高成本，通过人工干预解决这些失败是具有挑战性的。因此，本文介绍了一种以人为中心的人为中心的评估框架——验证LLM代理失败（VeriLA），该框架系统地评估代理失败以减少人力并使这些代理失败对人类可解释。该框架首先通过收集人类设计的代理标准来明确每个代理的期望。然后，它开发了一个与人类对齐的代理验证模块，该模块使用人类黄金标准进行训练，以评估每个代理的执行输出。这种方法通过揭示来自人类标准的失败，从而实现了对每个代理性能的细致评估，为修订提供了明确的指导，并减轻了人类认知负担。我们的案例研究结果表明，VeriLA在帮助从业者更有效地与系统互动方面既可解释又高效。通过在人类-代理协作中保持问责制，VeriLA为更值得信赖且与人类对齐的复合AI系统铺平了道路。|
|**2025-03-16**|**A Survey on the Optimization of Large Language Model-based Agents**|Shangheng Du et.al.|[2503.12434](http://arxiv.org/abs/2503.12434)|**[link](https://github.com/youngdubbydu/llm-agent-optimization)**|随着大型语言模型（LLMs）的快速发展，基于LLM的代理已经在各个领域得到广泛应用，成为自主决策和交互任务的关键。然而，目前的工作通常依赖于对普通LLM应用的提示设计或微调策略，这往往会导致在复杂的代理相关环境中效果有限或性能次优。尽管LLM优化技术可以改善模型在许多一般任务上的性能，但它们缺乏针对关键代理功能，如长期规划、动态环境交互和复杂决策等专门化的优化。尽管众多近期研究已经探索了各种策略来优化LLM基于的代理以执行复杂代理任务，但一个从整体角度对这些方法进行总结和比较的系统回顾仍然缺乏。在本综述中，我们全面回顾了基于LLM的代理优化方法，将它们分为参数驱动和参数无驱动方法。我们首先关注参数驱动优化，涵盖基于微调的优化、基于强化学习的优化和混合策略，分析了轨迹数据构建、微调技术、奖励函数设计和优化算法等关键方面。此外，我们还简要讨论了通过提示工程和外部知识检索来优化代理行为的参数无驱动策略。最后，我们总结了用于评估和调整的数据集和基准，回顾了基于LLM代理的关键应用，并讨论了主要挑战和有希望的未来方向。相关参考文献的仓库可在https://github.com/YoungDubbyDu/LLM-Agent-Optimization获取。|
|**2025-03-15**|**TFHE-Coder: Evaluating LLM-agentic Fully Homomorphic Encryption Code Generation**|Mayank Kumar et.al.|[2503.12217](http://arxiv.org/abs/2503.12217)|null|在环上的全同态加密（TFHE）允许在加密数据上执行计算而不进行解密，使其成为安全且机密计算的基础。尽管它在保护隐私的机器学习、安全多方计算、私有区块链交易和安全医疗诊断方面具有潜力，但由于其密码学复杂性和可用性挑战，其应用仍然有限。虽然存在各种TFHE库和编译器，但实际的代码生成仍然是一个难题。我们提出了一种编译器集成框架，用于评估LLM推理和代理优化在TFHE代码生成中的应用，重点关注逻辑门和ReLU激活函数。我们的方法评估了开源和闭源LLM的错误率、可编译性和结构相似性。结果表明，现成的模型存在显著限制，而如检索增强生成（RAG）和少量示例提示等代理优化减少了错误并提高了代码的准确性。这项工作为TFHE代码生成建立了第一个基准，展示了当LLM增加特定领域的反馈时，如何弥合FHE代码生成中的专业知识差距。|
|**2025-03-15**|**End-to-End Edge AI Service Provisioning Framework in 6G ORAN**|Yun Tang et.al.|[2503.11933](http://arxiv.org/abs/2503.11933)|null|随着6G的到来，开放无线接入网络（O-RAN）架构正在演变，以支持智能、自适应和自动化的网络编排。本文提出了一种新型的边缘人工智能和网络服务编排框架，该框架利用作为O-RAN rApps部署的大型语言模型（LLM）代理。所提出的LLM代理驱动的系统通过将用户的用例描述翻译成可部署的AI服务和相应的网络配置，实现了交互和直观的编排。LLM代理自动化了多个任务，包括从存储库（例如，Hugging Face）中选择AI模型、服务部署、网络适应以及通过xApps进行实时监控。我们使用开源的O-RAN项目（OpenAirInterface和FlexRIC）实现了一个原型，以展示我们框架的可行性和功能性。我们的演示展示了从用户交互到网络适应的端到端AI服务编排流程，确保了服务质量（QoS）的合规性。这项工作突出了将LLM驱动的自动化集成到6G O-RAN生态系统的潜力，为更易于访问和高效的边缘人工智能生态系统铺平了道路。|
|**2025-03-14**|**API Agents vs. GUI Agents: Divergence and Convergence**|Chaoyun Zhang et.al.|[2503.11069](http://arxiv.org/abs/2503.11069)|null|大型语言模型（LLMs）已经从简单的文本生成发展到能够驱动软件代理直接将自然语言命令转化为实际行动。虽然基于API的LLM代理最初因其强大的自动化能力和与程序化端点的无缝集成而受到关注，但最近在多模态LLM研究方面的进展使得基于GUI的LLM代理能够以类似人类的方式与图形用户界面交互。尽管这两种范式都旨在实现LLM驱动的任务自动化，但在架构复杂性、开发工作流程和用户交互模型方面存在显著差异。本文首次对基于API和基于GUI的LLM代理进行了全面比较研究，系统地分析了它们的差异和潜在融合。我们考察了关键维度，并突出了混合方法可以利用互补优势的场景。通过提出明确的决策标准并展示实际应用案例，我们旨在指导实践者和研究人员在选择、组合或在这些范式之间转换。最终，我们指出，基于LLM的自动化技术的持续创新将模糊API和GUI驱动代理之间的界限，为各种现实世界应用铺平道路。|
|**2025-03-14**|**BannerAgency: Advertising Banner Design with Multimodal LLM Agents**|Heng Wang et.al.|[2503.11060](http://arxiv.org/abs/2503.11060)|null|广告横幅对于吸引用户注意和提高广告活动效果至关重要。由于涉及多个设计元素，创建美观的横幅设计同时传达活动信息是一项挑战。此外，广告商需要为不同的显示设备和针对不同受众群体制作多种尺寸和版本。由于设计本质上是一个迭代和主观的过程，因此对于实际应用而言，灵活的可编辑性也非常重要。尽管当前模型已作为人类设计师在各种设计任务中的助手，但它们通常只处理创意设计过程的某个部分或产生基于像素的输出，这限制了可编辑性。本文介绍了一种无需训练的框架，用于完全自动化的横幅广告设计创建，使前沿的多模态大型语言模型（MLLM）能够在各种营销环境中以最少的手动努力高效地制作有效的横幅。我们提出了BannerAgency，一个MLLM代理系统，该系统与广告商合作以了解他们的品牌身份和横幅目标，生成匹配的背景图像，创建前景设计元素的蓝图，并以Figma或SVG格式的可编辑组件形式渲染最终的创意，而不是静态像素。为了促进评估和未来的研究，我们引入了BannerRequest400，这是一个包含100个独特标志和400个不同横幅请求的基准。通过定性和定量评估，我们展示了该框架的有效性，强调了生成的横幅设计质量、它们对各种横幅请求的适应性以及这种基于组件的方法带来的强大可编辑性。|
|**2025-03-14**|**CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control**|Zirui Yuan et.al.|[2503.11739](http://arxiv.org/abs/2503.11739)|**[link](https://github.com/usail-hkust/CoLLMLight)**|交通信号控制（TSC）在优化交通流量和缓解拥堵方面在城市交通管理中发挥着关键作用。尽管大型语言模型（LLMs）由于出色的解题能力和泛化能力，最近成为TSC的有望工具，但现有方法未能解决代理之间协调的基本需求，限制了其在实现全网优化方面的有效性。为了弥合这一差距，我们提出了CoLLMLight，一个用于TSC的协作LLM代理框架。具体来说，我们首先构建了一个结构化的时空图来捕捉实时交通动态和相邻交叉口之间的空间关系，使LLM能够对复杂的交通互动进行推理。此外，我们引入了一种复杂度感知推理机制，根据实时交通条件动态调整推理深度，确保在不牺牲决策质量的情况下具有最优的计算效率。此外，我们提出了一种微调策略，利用迭代仿真驱动的数据收集和环境反馈来构建一个适用于协作TSC的轻量级LLM。在合成和真实世界数据集上的大量实验表明，CoLLMLight在多种交通场景中优于现有方法，展示了其有效性、可扩展性和鲁棒性。|
|**2025-03-14**|**LLM Agents for Education: Advances and Applications**|Zhendong Chu et.al.|[2503.11733](http://arxiv.org/abs/2503.11733)|null|大型语言模型（LLM）智能体在自动化任务和推动各种教育应用创新方面展现出了卓越的能力。在这篇综述中，我们对教育领域LLM智能体的前沿研究进行了系统回顾，并将它们分为两大类：一是（1）\emph{教学智能体}，它们专注于自动化复杂的课堂教学任务，以支持教师和学生的需求；二是（2）\emph{特定领域教育智能体}，这些智能体针对科学教育、语言学习和专业发展等特定领域进行了定制。我们全面审视了这些LLM智能体背后的技术进步，包括关键数据集、基准和驱动其有效性的算法框架。此外，我们还讨论了关键挑战，如隐私、偏见和公平性担忧、幻觉缓解以及与现有教育生态系统的整合。本综述旨在为教育领域LLM智能体提供一个全面的技术概述，促进进一步的研究与合作，以增强其对学习者和教育者双方的积极影响。|
|**2025-03-14**|**Agent-Enhanced Large Language Models for Researching Political Institutions**|Joseph R. Loffredo et.al.|[2503.13524](http://arxiv.org/abs/2503.13524)|**[link](https://github.com/congressRA/sample-agent)**|大型语言模型（LLMs）在政治学领域的应用正在迅速扩展。本文展示了当LLMs辅以预定义函数和专用工具时，可以成为动态代理，能够简化数据收集、预处理和分析等任务。这种方法的核心是代理检索增强生成（Agentic RAG），它为LLMs配备了调用外部知识库的能力。除了信息检索之外，LLM代理还可以整合模块化工具，用于文档摘要、录音编码、定性变量分类和统计建模等任务。为了展示这种方法的潜力，我们引入了CongressRA，这是一个专为支持研究美国国会的学者设计的LLM代理。通过这个例子，我们强调了LLM代理如何通过驱动政治制度研究的专业化数据，降低复制、测试和扩展实证研究的成本。|
|**2025-03-13**|**Capturing Semantic Flow of ML-based Systems**|Shin Yoo et.al.|[2503.10310](http://arxiv.org/abs/2503.10310)|null|基于机器学习的系统是包含深度神经网络（DNNs）或大型语言模型（LLMs）等机器学习组件的软件系统。虽然这类系统实现了高级功能，如高性能计算机视觉、自然语言处理和代码生成，但它们的内部行为对于传统的动态分析（如测试）来说仍然很大程度上是不透明的：现有的分析通常只关注从外部可观察到的内容，例如输入相似性或类别标签的变化。我们提出了语义流的概念，旨在捕捉基于机器学习系统的内部行为，并为传统动态分析技术提供适应的平台。语义流结合了控制流与从基于机器学习系统的执行中获取的内部状态的想法，例如DNN中特定层的激活值，或LLM代理在特定推理步骤中的响应嵌入。由此产生的表示，简称为语义流图，可以捕捉到在基于机器学习系统的传统控制流中未明确表示的内部决策。我们提出了语义流的概念，介绍了两个使用DNN和LLM代理的例子，并最后概述了其属性以及如何将其用于适应现有的动态分析技术以用于基于机器学习的软件系统。|
|**2025-03-13**|**LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns**|Idan Horowitz et.al.|[2503.10248](http://arxiv.org/abs/2503.10248)|null|我们研究了大型语言模型（LLMs）在涉及重复选择和从反馈中学习的决策经验任务中的选择模式，并将其与人类参与者的行为进行比较。我们发现，从整体上看，LLMs似乎表现出与人类相似的行为偏差：两者都表现出对罕见事件的低估和相关性效应。然而，对选择模式的更细致分析揭示，这种现象背后的原因大相径庭。LLMs表现出强烈的近期偏差，而人类似乎以更复杂的方式做出反应。尽管这些不同的过程可能在平均行为上导致相似的结果，但基于近期事件的决策模式在这两组之间差异很大。具体来说，诸如“意外触发变化”和“罕见事件的波动近期效应”等现象在人类中稳健地观察到，但在LLMs中完全不存在。我们的发现为使用LLMs模拟和预测学习环境中的人类提供了见解，并强调了在研究它们是否复制人类的决策倾向时，对其行为进行细致分析的需求。|
|**2025-03-13**|**Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM**|Mohd Ariful Haque et.al.|[2503.10071](http://arxiv.org/abs/2503.10071)|null|结合LLM代理和外部工具使得模型能够解决超出其知识库的复杂任务。人类设计的工具缺乏灵活性，仅限于专家预先创建的工具范围内的解决方案。为了解决这个问题，我们提出了ATLASS，一个高级的工具学习和选择系统，设计为一个闭环框架。它使LLM能够通过动态生成外部工具来解决问题。在这个框架中，代理在协调工具选择、执行和优化方面发挥着关键作用，确保适应性问题解决能力。ATLASS的操作分为三个阶段：第一阶段，理解工具需求，涉及代理确定是否需要工具并指定其功能；第二阶段，工具检索/生成，涉及代理根据可用性检索或生成工具；第三阶段，任务解决，涉及组合所有必要的组件工具以完成初始任务。工具数据集存储生成的工具，确保可重用性并最小化推理成本。当前的基于LLM的工具生成系统难以创建需要API或外部包的复杂工具。在ATLASS中，我们通过自动设置环境、在线获取相关API文档和使用Python解释器创建一个可靠、通用的工具来解决这一问题，该工具能在更广泛的情况下工作。OpenAI GPT-4.0用作LLM代理，并在执行生成的代码之前通过人类反馈处理安全和伦理问题。通过解决预定义工具集的限制并增强适应性，ATLASS作为一种现实世界的解决方案，使用户能够利用动态生成的工具来解决复杂问题。|
|**2025-03-13**|**OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model**|Bowen Zhang et.al.|[2503.10009](http://arxiv.org/abs/2503.10009)|**[link](https://github.com/bwz96sco/or_llm_agent)**|**运筹学（OR）已被广泛应用于资源分配、生产计划和供应链管理等各个领域。然而，解决现实世界的OR问题需要运筹学专家进行数学建模和程序员开发求解算法。这种高度依赖专家的传统方法成本高昂，开发周期长，严重限制了OR技术的广泛应用。很少有人考虑使用人工智能（AI）来替代专业人士，以实现OR问题的完全自动化解决方案。我们提出了OR-LLM-Agent，这是第一个能够实现解决现实世界OR问题端到端自动化的AI代理。OR-LLM-Agent利用大型语言模型（LLM）的链式思维（CoT）推理能力，将自然语言问题描述转换为形式化的数学模型，并自动生成Gurobi求解器代码。在OR-LLM-Agent中，OR-CodeAgent被设计用于自动化沙盒环境中的代码执行和修复，从而促进最终解决方案的推导。由于缺乏用于评估OR问题自动化求解的专用基准数据集，我们构建了一个包含83个自然语言描述的现实世界OR问题的基准数据集。我们与最先进的（SOTA）推理LLM进行了比较实验，包括GPT-o3-mini、DeepSeek-R1和Gemini 2.0 Flash Thinking。OR-LLM-Agent实现了最高的100%通过率和最高的85%解决方案准确性，证明了自动化OR问题求解的可行性。数据和代码已在https://github.com/bwz96sco/or_llm_agent上公开发布。**|
|**2025-03-12**|**LocAgent: Graph-Guided LLM Agents for Code Localization**|Zhaoling Chen et.al.|[2503.09089](http://arxiv.org/abs/2503.09089)|**[link](https://github.com/gersteinlab/locagent)**|**代码定位——精确识别代码库中需要修改的位置——是软件维护中的一个基本但具有挑战性的任务。现有方法在识别相关代码部分时难以有效地导航复杂的代码库。挑战在于将自然语言问题描述与适当的代码元素相连接，通常需要跨越层次结构和多个依赖关系进行推理。我们引入了LocAgent，这是一个通过基于图表示来解决代码定位的框架。通过将代码库解析为有向异构图，LocAgent创建了一个轻量级的表示，它捕捉代码结构（文件、类、函数）及其依赖关系（导入、调用、继承），使得LLM代理能够通过强大的多跳推理有效地搜索和定位相关实体。在真实世界基准测试上的实验结果表明，我们的方法显著提高了代码定位的准确性。值得注意的是，使用微调后的Qwen-2.5-Coder-Instruct-32B模型的方法在大幅降低成本（约降低86%）的情况下，与SOTA专有模型实现了相当的结果，文件级定位的准确率高达92.7%，同时通过多次尝试（Pass@10）将GitHub问题解决的成功率提高了12%。我们的代码可在https://github.com/gersteinlab/LocAgent上获取。**|
|**2025-03-12**|**A Survey on Trustworthy LLM Agents: Threats and Countermeasures**|Miao Yu et.al.|[2503.09648](http://arxiv.org/abs/2503.09648)|**[link](https://github.com/Ymm-cll/TrustAgent)**|随着大型语言模型（LLMs）的快速发展，基于LLMs的代理和多智能体系统（MAS）显著扩展了LLM生态系统的能力。这种演变源于赋予LLMs额外的模块，如记忆、工具、环境和甚至其他代理。然而，这种进步也引入了更多关于可信性的复杂问题，而先前的研究仅关注LLMs，无法涵盖这些问题。在本综述中，我们提出了TrustAgent框架，这是一项关于代理可信性的全面研究，其特点为模块化分类、多维含义和技术实现。通过彻底调查和总结针对代理和MAS的新兴攻击、防御和评估方法，我们将可信LLM的概念扩展到新兴的可信代理范式。在TrustAgent中，我们首先分解并介绍了代理和MAS的各个组成部分。然后，我们将它们的可信性分为内在（大脑、记忆和工具）和外在（用户、代理和环境）方面。随后，我们阐述了可信性的多方面含义，并详细说明了与这些内部和外部模块相关的研究的实施方案。最后，我们提出了我们对这一领域的见解和展望，旨在为未来的努力提供指导。|
|**2025-03-12**|**Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents**|Dongjun Lee et.al.|[2503.10689](http://arxiv.org/abs/2503.10689)|null|近期大型语言模型（LLMs）的进展促使人们越来越关注开发基于LLMs的代理以自动化网络任务。然而，由于这些代理在理解和处理复杂网页结构方面的能力有限，它们在现实世界网站上的简单任务上常常会遇到困难。在本研究中，我们提出了LCoW，这是一个用于学习将复杂网页上下文化为更易理解形式的框架，从而通过LCoW增强了LLM代理的决策能力。LCoW通过训练一个独立的上下文化模块来解耦网页理解和决策，该模块将复杂网页转换为可理解的格式，然后由决策代理使用。我们证明，我们的上下文化模块可以有效地集成到各种规模的LLM代理中，显著提高它们在自动化网络任务中的决策能力。值得注意的是，LCoW将封闭源代码LLMs（例如，Gemini-1.5-flash、GPT-4o、Claude-3.5-Sonnet）的成功率平均提高了15.6%，并在WorkArena基准测试中，对于开源LMs（例如，Llama-3.1-8B、Llama-3.1-70B）的成功率平均提高了23.7%。此外，配备LCoW的Gemini-1.5-flash代理在WebShop基准测试中取得了最先进的结果，超过了人类专家。相关的代码材料可在我们的项目页面找到：https://lcowiclr2025.github.io。|
|**2025-03-11**|**ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews**|Xian Gao et.al.|[2503.08506](http://arxiv.org/abs/2503.08506)|null|学术论文审阅是研究界一项关键但耗时的工作。随着学术出版物数量的增加，自动化审阅过程已成为一个重大挑战。主要问题在于生成全面、准确、推理一致的审阅评论，这些评论与人类审阅者的判断相符。在本文中，我们通过提出ReviewAgents框架来解决这一挑战，该框架利用大型语言模型（LLMs）来生成学术论文审阅。我们首先介绍了一个新的数据集，Review-CoT，包含142k条审阅评论，用于训练LLM代理。这个数据集模拟了人类审阅者的结构化推理过程——总结论文、引用相关作品、识别优点和缺点以及生成审阅结论。在此基础上，我们使用相关论文感知的训练方法训练了能够进行结构化推理的LLM审阅代理。此外，我们构建了ReviewAgents，一个多角色、多LLM代理审阅框架，以增强审阅评论的生成过程。另外，我们提出了ReviewBench，一个用于评估LLM生成的审阅评论的基准。我们在ReviewBench上的实验结果表明，尽管现有LLM在自动化审阅过程中展现出一定的潜力，但与人类生成的审阅相比，仍存在差距。此外，我们的ReviewAgents框架进一步缩小了这一差距，在生成审阅评论方面优于先进的LLM。|
|**2025-03-10**|**LLMs syntactically adapt their language use to their conversational partner**|Florian Kandra et.al.|[2503.07457](http://arxiv.org/abs/2503.07457)|null|人们经常观察到，在对话中，人类说话者会将自己的语言使用与对方对齐。在这篇论文中，我们通过实证研究来探讨大型语言模型（LLMs）是否表现出相同的对话适应行为。我们构建了一个LLMs之间的对话语料库，并发现随着对话的进行，两个LLM代理人的句法选择越来越相似，这证实了现代LLMs至少以一种基本的方式适应其对话伙伴的语言使用。|
|**2025-03-10**|**Experimental Exploration: Investigating Cooperative Interaction Behavior Between Humans and Large Language Model Agents**|Guanxuan Jiang et.al.|[2503.07320](http://arxiv.org/abs/2503.07320)|null|随着大型语言模型（LLMs）的兴起，作为自主决策者的AI代理在人类-人工智能合作中既带来了重大机遇也带来了挑战。虽然许多研究探讨了人类与人工智能作为工具的合作，但LLM增强的自主代理在竞争-合作互动中的角色尚未得到充分研究。本研究通过让30名参与者与具有不同特征（声称的人类、声称的基于规则的AI代理和LLM代理）的LLM代理进行重复的囚徒困境游戏互动，来研究人类的合作行为。结果显示，基于代理的声称特征以及参与者性别和声称特征的交互作用，合作行为存在显著差异。我们还分析了人类响应模式，包括游戏完成时间、主动有利行为和接受修复努力的意愿。这些见解为在竞争合作背景下（如虚拟头像或未来的物理实体）与LLM代理的人类互动提供了新的视角。该研究强调了理解人类对AI代理的偏见以及观察到的行为如何影响未来人类-人工智能合作动态的重要性。|
|**2025-03-10**|**Automated Movie Generation via Multi-Agent CoT Planning**|Weijia Wu et.al.|[2503.07314](http://arxiv.org/abs/2503.07314)|**[link](https://github.com/showlab/movieagent)**|**现有的长视频生成框架缺乏自动化规划，需要手动输入故事情节、场景、摄影和角色互动，导致成本高和效率低下。为了解决这些挑战，我们提出了MovieAgent，这是一个通过多智能体思维链（CoT）规划实现的电影自动化生成系统。MovieAgent具有两个关键优势：1）我们首先探索并定义了自动化电影/长视频生成的范式。给定一个剧本和角色库，我们的MovieAgent可以生成具有连贯叙事的多场景、多镜头长视频，同时确保角色一致性、同步字幕和电影中的稳定音频。2）MovieAgent引入了一个基于分层CoT的推理过程来自动构建场景、摄影设置和电影摄影，显著降低了人工工作量。通过使用多个LLM智能体来模拟导演、编剧、分镜师和地点经理的角色，MovieAgent简化了制作流程。实验表明，MovieAgent在剧本忠实度、角色一致性和叙事连贯性方面达到了新的最先进水平。我们的分层框架向前迈进了一步，并为完全自动化的电影生成提供了新的见解。代码和项目网站可在以下地址获取：https://github.com/showlab/MovieAgent 和 https://weijiawu.github.io/MovieAgent。**|
|**2025-03-10**|**DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data Science**|Ziming You et.al.|[2503.07044](http://arxiv.org/abs/2503.07044)|null|数据科学任务具有多面性、动态性和往往具有特定领域性。现有的基于大型语言模型（LLM）的方法主要集中于孤立阶段，忽视了众多数据科学任务之间的相互依存性，限制了它们提供全面端到端支持的能力。我们提出了DatawiseAgent，这是一个以笔记本为中心的LLM代理框架，通过Markdown和可执行代码单元格统一了用户、代理和计算环境之间的交互，支持灵活和自适应的自动化数据科学。DatawiseAgent建立在有限状态转换器（FST）的基础上，协调四个阶段，包括类似于DSF的规划、增量执行、自我调试和后过滤。具体来说，类似于DFS的规划阶段系统地探索解决方案空间，而增量执行利用实时反馈，并适应LLM有限的性能，逐步完成任务。自我调试和后过滤模块通过诊断和纠正错误以及修剪无关信息进一步提高了可靠性。在包括数据分析、可视化和数据建模在内的多种任务上的广泛实验表明，DatawiseAgent在多个模型设置下始终优于或与最先进的方法相当。这些结果突显了它在数据科学场景中推广的潜力，并为更高效、完全自动化的工作流程奠定了基础。|
|**2025-03-10**|**ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation**|Kaiyuan Liu et.al.|[2503.07010](http://arxiv.org/abs/2503.07010)|**[link](https://github.com/RyanLoil/ProjectEval)**|近期，LLM代理在提升编程能力方面取得了迅速进展。然而，现有的基准测试缺乏从用户角度自动评估的能力，同时也缺乏对LLM代理代码生成结果的可解释性。因此，我们推出了ProjectEval，这是一个通过模拟用户交互对LLM代理项目级代码生成进行自动化评估的新基准。ProjectEval由LLM结合人工评审构建。它包含三种不同级别的自然语言或代码骨架输入。ProjectEval可以通过用户交互模拟对生成的项目进行执行评估，并通过现有客观指标进行代码相似性评估。通过ProjectEval，我们发现系统级工程项目代码、对项目的整体理解和综合分析能力是LLM代理实现实际项目的关键。我们的发现和基准测试为开发更有效的编程代理提供了宝贵的见解，这些代理可以在未来的实际生产中得到部署。|
|**2025-03-10**|**Simulating Influence Dynamics with LLM Agents**|Mehwish Nasim et.al.|[2503.08709](http://arxiv.org/abs/2503.08709)|null|本文介绍了一种专为意见动力学研究人员设计的模拟器，用于模拟在基于大型语言模型（LLM）的智能体存在下，社会网络中相互竞争的影响。通过将成熟的意见动力学原理与最先进的LLM相结合，该工具能够研究影响传播和反虚假信息策略。该模拟器对于社会科学、心理学和运筹学研究人员尤其有价值，使他们能够在不要求具备广泛编码专业知识的情况下分析社会现象。此外，该模拟器将在GitHub上公开提供，确保那些希望扩展其功能以用于自身研究的人能够访问和适应。|
|**2025-03-09**|**Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets**|Tommaso Bendinelli et.al.|[2503.06664](http://arxiv.org/abs/2503.06664)|null|高质量、无误差的数据集是构建可靠、准确和公正的机器学习（ML）模型的关键成分。然而，现实世界的数据集常常因为传感器故障、数据录入错误或不恰当的多源数据整合等原因出现错误，这些错误会严重降低模型性能。检测和纠正这些问题通常需要定制化解决方案，并需要广泛的领域专业知识。因此，自动化变得具有挑战性，使得整个过程变得劳动密集且繁琐。在本研究中，我们探讨了大型语言模型（LLMs）是否能够帮助减轻手动数据清理的负担。我们设置了一个实验，其中LLM与Python相结合，负责清理训练数据集以提高学习算法的性能，而不具备修改训练流程或执行任何特征工程的能力。我们在多个有意被错误篡改的Kaggle数据集上运行了这个实验。我们的结果表明，LLMs可以通过利用同一行内其他特征的上下文信息以及前一轮迭代中的反馈来识别和纠正错误条目，例如不合逻辑的值或异常值。然而，它们在检测需要理解跨多行数据分布的更复杂错误方面存在困难，例如趋势和偏差。|
|**2025-03-09**|**Performant LLM Agentic Framework for Conversational AI**|Alex Casella et.al.|[2503.06410](http://arxiv.org/abs/2503.06410)|null|随着代理应用程序和自动化在语音人工智能行业的兴起，对大型语言模型（LLMs）的依赖性增加，以便在由节点和边组成的基于图的逻辑工作流程中进行导航。然而，现有方法面临诸如复杂工作流程中的对齐错误和由过度上下文大小引起的幻觉等挑战。为了解决这些局限性，我们引入了高性能代理框架（PAF），这是一个新颖的系统，它帮助LLMs在遍历复杂图时选择适当的节点并按顺序执行动作。PAF结合了基于LLM的推理与基于数学的向量评分机制，实现了更高的准确性和更低的延迟。我们的方法动态平衡了对预定义路径的严格遵循与灵活的节点跳跃，以有效地处理各种用户输入。实验表明，PAF显著优于基线方法，为复杂商业环境中的可扩展、实时对话人工智能系统铺平了道路。|
|**2025-03-07**|**A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval**|Yu Zhang et.al.|[2503.05659](http://arxiv.org/abs/2503.05659)|**[link](https://github.com/tsinghua-fib-lab/llm-agent-for-recommendation-and-search)**|信息技术深刻地改变了人类与信息互动的方式。在线上创造、共享和传播的大量内容使得获取相关信息变得越来越困难。在过去二十年里，搜索和推荐系统（统称为信息检索系统）为了应对这些挑战而显著发展。近期大型语言模型（LLMs）的进步展示了在多种语言相关任务中超越人类性能的能力，并表现出一般理解、推理和决策能力。本文探讨了大型语言模型代理在提升搜索和推荐系统中的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来阐述现有研究。我们强调了LLM代理在应对搜索和推荐当前挑战中的巨大潜力，并对未来的研究方向提供了见解。本文首次系统地回顾和分类了这些领域关于LLM代理的研究，为利用这一先进的AI技术进行信息检索提供了新颖的视角。为了帮助理解现有作品，我们列出了使用大型语言模型的基于代理的模拟的现有论文，链接如下：https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search。|
|**2025-03-07**|**ORANSight-2.0: Foundational LLMs for O-RAN**|Pranshav Gajjar et.al.|[2503.05200](http://arxiv.org/abs/2503.05200)|null|尽管大型语言模型（LLMs）在医疗保健、客户服务和商业营销等关键领域的变革性影响显著，但它们在开放无线接入网络（O-RAN）中的应用仍有限。这一差距主要是由于缺乏特定领域的基座模型，现有的解决方案通常依赖于通用的LLMs，而这些LLMs无法解决O-RAN独特的挑战和技术复杂性。为了弥合这一差距，我们推出了ORANSight-2.0（O-RAN洞察力），这是一个开创性的项目，旨在开发针对O-RAN的专业基座LLMs。ORANSight-2.0基于五个开源LLM框架中的18个LLMs构建，对从1到70B参数范围的模型进行微调，显著减少了对外部、封闭源模型的依赖，同时提升了O-RAN的性能。ORANSight-2.0的核心是RANSTRUCT，这是一个基于检索增强生成（RAG）的指令调整框架，它采用两个LLM代理来创建高质量的指令调整数据集。随后，使用这些数据集通过QLoRA对18个预先训练的开源LLMs进行微调。为了评估ORANSight-2.0，我们引入了srsRANBench，这是一个专为评估srsRAN中代码生成和代码库理解设计的基准，srsRAN是一个广泛使用的5G O-RAN堆栈。我们还利用了现有的ORANBench13K基准，用于评估O-RAN特定知识。我们的全面评估表明，ORANSight-2.0模型在ORANBench上的表现优于通用和封闭源模型（如ChatGPT-4o和Gemini），提升了5.421%，在srsRANBench上提升了18.465%，在保持较低的计算和能源成本的同时实现了优越的性能。我们还对ORANSight-2.0 LLMs的RAG增强变体进行了实验，并详细评估了它们的能源特性，包括训练、标准推理和RAG增强推理的成本。|
|**2025-03-07**|**Enhancing Reasoning with Collaboration and Memory**|Julie Michelman et.al.|[2503.05944](http://arxiv.org/abs/2503.05944)|null|我们设想一个连续的协作学习系统，其中一组大型语言模型（LLM）智能体共同解决推理问题，利用他们共同构建的记忆来提高性能，随着经验的积累不断进步。本研究通过研究思维链推理风格的互操作性、多智能体协作和记忆库，为这样的系统奠定了基础。我们超越了自我一致性中的相同智能体，引入了具有不同示例的多样化上下文智能体和替代投票的总结智能体。我们生成了冻结的和持续学习的示例记忆库，并将它们与固定的、随机的和基于相似性的检索机制相匹配。我们的系统研究揭示了各种方法在三个基于事实的推理任务中如何对两个LLM的推理性能做出贡献，显示出随机示例选择通常能打败更原则性的方法，并且在某些任务中，包含任何示例只会分散弱模型和强模型。|
|**2025-03-07**|**This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs**|Lorenz Wolf et.al.|[2503.05856](http://arxiv.org/abs/2503.05856)|**[link](https://github.com/lorenzflow/robust-moa)**|混合大型语言模型（LLMs）代理（MoA）架构通过在推理时利用多个LLMs的协作，在AlpacaEval 2.0等突出基准上实现了最先进的性能。尽管取得了这些成功，但MoA的安全性和可靠性评估仍然缺失。我们展示了首次对MoA针对故意提供误导性回答的欺骗性LLM代理的鲁棒性的全面研究。我们考察了诸如欺骗性信息的传播、模型大小和信息可用性等因素，并揭示了关键漏洞。在AlpacaEval 2.0上，流行的LLaMA 3.1-70B模型在与3层MoA（6个LLM代理）结合时，实现了49.2%的长度控制胜率（LC WR）。然而，我们证明了只需将一个精心指导的欺骗性代理引入MoA，就可以将性能降低到37.9%，有效地抵消了所有MoA的收益。在QuALITY这一多项选择题理解任务中，影响同样严重，准确率惊人地下降了48.5%。部分受威尼斯历史上的Doge投票过程启发，该过程旨在最小化影响和欺骗，我们提出了一系列无监督的防御机制，以恢复大部分丢失的性能。|
|**2025-03-06**|**ToolFuzz -- Automated Agent Tool Testing**|Ivan Milev et.al.|[2503.04479](http://arxiv.org/abs/2503.04479)|null|大型语言模型（LLM）智能体利用LLM在现实应用中的高级推理能力。为了与环境交互，这些智能体通常依赖工具，如网络搜索或数据库API。随着智能体在用户查询过程中为LLM提供工具文档，该文档的完整性和正确性至关重要。然而，工具文档常常过于详细、不够详细或定义不清，这阻碍了智能体的准确性。标准软件测试方法难以识别这些错误，因为它们是用自然语言表达的。因此，尽管其重要性不言而喻，但目前尚无自动化的方法来测试智能体的工具文档。为了解决这个问题，我们提出了ToolFuzz，这是第一个用于自动化测试工具文档的方法。ToolFuzz旨在发现两种类型的错误：（1）导致工具运行时错误的用户查询；（2）导致智能体响应错误的用户查询。ToolFuzz可以生成大量且多样化的自然语言输入，有效地以低误报率发现工具描述错误。此外，我们提出了两种简单的提示工程方法。我们在32个常见的LangChain工具、35个新创建的定制工具和2个新颖的基准测试上评估了所有三种工具测试方法，以进一步增强评估。我们发现许多公开可用的工具存在定义不足的问题。具体来说，我们展示了ToolFuzz识别的错误输入数量比提示工程方法多20倍，使其成为构建可靠AI智能体的关键组件。|
|**2025-03-06**|**Measuring temporal effects of agent knowledge by date-controlled tool use**|R. Patrick Xian et.al.|[2503.04188](http://arxiv.org/abs/2503.04188)|null|时间推移是知识积累和更新的一个重要组成部分。网络搜索常被用作代理知识的依据，但其不恰当的配置会影响代理响应的质量。在这里，我们构建了一个基于工具的样本外测试框架，以测量来自不同日期控制工具（DCTs）的大型语言模型（LLM）代理的知识变异性。我们展示了LLM代理作为写作助手的时间效应，它可以利用网络搜索来帮助完成科学出版物摘要。我们表明，搜索引擎的时间效应转化为工具依赖的代理性能，但可以通过基础模型选择和如思维链提示等明确的推理指令来缓解。我们的结果表明，代理评估应采取动态视角，并考虑工具的时间和外部资源更新的影响。|
|**2025-03-06**|**InterChat: Enhancing Generative Visual Analytics using Multimodal Interactions**|Juntong Chen et.al.|[2503.04110](http://arxiv.org/abs/2503.04110)|null|大型语言模型（LLMs）和生成式视觉分析系统的兴起改变了数据驱动的洞察，然而，在准确解读用户的分析和交互意图方面仍存在重大挑战。虽然语言输入提供了灵活性，但它们往往缺乏精确性，使得表达复杂意图变得低效、易出错且耗时。为了解决这些局限性，我们通过文献综述和试点头脑风暴会议，研究了生成式视觉分析的多模态交互设计空间。基于这些洞察，我们介绍了一个高度可扩展的工作流程，该流程集成了多个LLM代理以进行意图推断和可视化生成。我们开发了InterChat，这是一个结合了直接操作视觉元素和自然语言输入的生成式视觉分析系统。这种集成实现了精确的意图沟通，并支持逐步的、视觉驱动的探索性数据分析。通过采用有效的提示工程、上下文交互链接以及直观的可视化和交互设计，InterChat弥合了用户交互和LLM驱动的可视化之间的差距，提高了可解释性和可用性。包括两个使用场景、一项用户研究和专家反馈在内的广泛评估证明了InterChat的有效性。结果显示，在处理复杂视觉分析任务方面的准确性和效率有显著提高，突显了多模态交互重新定义生成式视觉分析中用户参与度和分析深度的潜力。|
|**2025-03-05**|**A Practical Memory Injection Attack against LLM Agents**|Shen Dong et.al.|[2503.03704](http://arxiv.org/abs/2503.03704)|null|基于大型语言模型的代理在众多复杂、真实的实际应用中展示了强大的能力。然而，当检索用于展示的过去记录含有恶意内容时，具有受损内存库的LLM代理可能会轻易产生有害的输出。在本文中，我们提出了一种新的内存注入攻击方法，称为MINJA，它仅通过与代理进行查询和输出观察来交互，即可将恶意记录注入内存库。这些恶意记录被设计为在执行受害者用户的查询时，引发一系列导致不期望的代理行为的恶意推理步骤。具体来说，我们引入了一系列桥梁步骤来将受害者查询与恶意推理步骤联系起来。在注入恶意记录的过程中，我们提出了一种指示提示来引导代理自主生成我们设计的桥梁步骤。我们还提出了一种渐进缩短策略，逐步移除指示提示，这样在处理后续的受害者查询时，恶意记录将更容易被检索。我们在多种代理上的广泛实验证明了MINJA在损害代理内存方面的有效性。由于对执行要求最低，MINJA使任何用户都能影响代理内存，凸显了LLM代理的实际风险。|
|**2025-03-05**|**Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents**|Jingying Zeng et.al.|[2503.04830](http://arxiv.org/abs/2503.04830)|null|随着对话大型语言模型（LLMs）的发展，已经开发出多种基于LLMs的对话购物代理（CSA），以帮助客户回答问题并使他们的电子商务购物之旅更加顺畅。构建一个值得信赖的CSA的主要目标是确保代理的回答准确且基于事实，这对于建立客户信任和鼓励持续参与至关重要。然而，仍然存在两个挑战。首先，LLMs会产生虚构或无支持的声明。这种不准确的风险会传播错误信息和降低客户信任。其次，在没有在CSA回答中提供知识来源归属的情况下，客户难以验证LLM生成的信息。为了解决这些挑战，我们提出了一种易于生产的解决方案，利用情境学习（ICL）和多UX推理（MUI）来实现“引用体验”，在生成带有引用的回答时，将其原始来源归因于而不干扰其他现有的UX功能。通过适当的UX设计，这些引用标记可以链接到相关产品信息，并向客户显示来源。在这项工作中，我们还构建了自动指标和可扩展的基准，以全面评估LLMs的扎根和归因能力。我们的实验表明，通过结合这种引用生成范式，可以将LLM回答的扎根程度在真实世界数据上提高13.83%。因此，我们的解决方案不仅解决了LLM扎根问题的紧迫挑战，还为对话人工智能增加了透明度。|
|**2025-03-04**|**MPO: Boosting LLM Agents with Meta Plan Optimization**|Weimin Xiong et.al.|[2503.02682](http://arxiv.org/abs/2503.02682)|**[link](https://github.com/weiminxiong/mpo)**|近期大型语言模型（LLMs）的进步使得基于LLMs的智能体能够成功应对交互式规划任务。然而，尽管它们取得了成功，现有方法往往存在规划幻觉，并且需要为每个新智能体进行重新训练。为了解决这些挑战，我们提出了元规划优化（MPO）框架，该框架通过直接融入显式指导来增强智能体的规划能力。与之前依赖复杂知识的方法不同，这些方法要么需要大量人力，要么缺乏质量保证，MPO通过元规划利用高级通用指导来协助智能体规划，并允许根据智能体任务执行的反馈对元规划进行持续优化。我们在两个代表性任务上进行的实验表明，MPO显著优于现有基线。此外，我们的分析表明，MPO提供了一种即插即用的解决方案，它不仅提高了任务完成效率，还在之前未见过的场景中增强了泛化能力。|
|**2025-03-04**|**Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent**|Xingzuo Li et.al.|[2503.02519](http://arxiv.org/abs/2503.02519)|**[link](https://github.com/wisper12933/ga-rollback)**|大型语言模型（LLM）代理通常采用逐步推理框架，在这个框架中，它们交替进行思考和行动以完成给定的任务。然而，这种范式面临一个根深蒂固的单次遍历问题，即无论中间思考的正确性如何，每个生成的中间思想都会被连接到轨迹中，这可能导致不可逆的错误传播。为了解决这个问题，本文提出了一种名为生成器辅助逐步回滚（GA-Rollback）的新框架，以诱导LLM代理做出更好的决策。特别是，GA-Rollback使用一个生成器与环境交互，并使用一个助手检查生成器产生的每个动作，助手在检测到错误动作时触发回滚操作。此外，我们引入了两种针对回滚场景定制的策略，以进一步提高其有效性。大量实验表明，GA-Rollback在三个广泛使用的基准测试上显著优于几个强大的基线。我们的分析进一步揭示了GA-Rollback可以作为一种鲁棒的即插即用模块，与其他方法无缝集成。|
|**2025-03-04**|**ATLaS: Agent Tuning via Learning Critical Steps**|Zhixun Chen et.al.|[2503.02197](http://arxiv.org/abs/2503.02197)|null|大型语言模型（LLM）代理在多领域任务中展现出了令人瞩目的泛化能力。现有的代理微调方法通常采用在完整专家轨迹上的监督微调。然而，完整轨迹的行为克隆可能会引入专家偏差，并削弱对专家数据未覆盖状态的泛化能力。此外，规划、对中间子任务的复杂推理和战略决策等关键步骤对于代理任务的成功至关重要，因此学习这些步骤是提高LLM代理的关键。为了更有效和高效地微调代理，我们提出了ATLaS，它识别专家轨迹中的关键步骤，并在这些步骤上以降低成本对LLM进行微调。通过将训练的焦点转向几个关键步骤，我们的方法减轻了过度拟合整个轨迹的风险，并促进了在不同环境和任务上的泛化。在广泛的实验中，仅对ATLaS选出的30%关键步骤进行微调的LLM优于对所有步骤进行微调的LLM和最近的开源LLM代理。ATLaS作为与多样化环境互动的通用代理，维持并提升了基础LLM技能。|
|**2025-03-03**|**Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions**|Angana Borah et.al.|[2503.02038](http://arxiv.org/abs/2503.02038)|null|现有关于错误信息暴露和易受其影响的问题在不同人口群体中存在差异，因为某些群体比其他群体更容易受到错误信息的影响。大型语言模型（LLMs）通过其大规模生成有说服力内容的能力以及强化现有偏见，为这些挑战带来了新的维度。本研究调查了在接触错误信息时，LLMs与人类之间的双向说服动态。我们通过使用人类立场数据集来分析人类对LLM的影响，并通过生成基于LLM的说服论点来评估LLM对人类的影响。此外，我们使用一个多智能体LLM框架来分析面向人口统计的LLM代理在说服下的错误信息传播。我们的研究发现，人口统计因素会影响LLM中错误信息的易受性，这与人类易受性中观察到的基于人口统计的模式密切相关。我们还发现，与人类人口群体类似，多智能体LLMs表现出回音室行为。这项研究探讨了人类与LLMs之间的相互作用，在错误信息背景下突出了人口差异，并为未来的干预措施提供了见解。|
|**2025-03-03**|**Student engagement in collaborative learning with AI agents in an LLM-empowered learning environment: A cluster analysis**|Zhanxin Hao et.al.|[2503.01694](http://arxiv.org/abs/2503.01694)|null|将大型语言模型（LLM）融入教育实践，通过适应不同学习者的多样化行为模式，促进了个性化学习。本研究旨在探索在一个新颖的互动环境中这些学习者的类型，并对他们的独特特征和互动动态进行详细分析。研究涉及来自中国一所大学的110名学生，他们在LLM赋能的学习环境中与多个LLM代理互动，完成了六个模块的课程。收集并分析了学生非认知特质、课程参与度和人工智能互动模式的数据。使用层次聚类分析，将学生分为三个不同的组别：积极提问者、反应型导航者和沉默倾听者。随后，应用认识网络分析进一步细化不同类型学习者的互动轮廓和认知参与度。研究结果强调了不同学习者在人机交互学习中的参与方式，并为适应性教育系统的设计提供了实际启示。|
|**2025-03-03**|**MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents**|Kunlun Zhu et.al.|[2503.01935](http://arxiv.org/abs/2503.01935)|**[link](https://github.com/multiagentbench/marble)**|大型语言模型（LLMs）作为自主代理展现了惊人的能力，然而现有的基准测试要么专注于单代理任务，要么局限于狭窄的领域，未能捕捉多代理协调和竞争的动态。在本文中，我们引入了MultiAgentBench，这是一个全面的基准测试，旨在评估基于LLMs的多代理系统在多样化的交互场景中的性能。我们的框架不仅衡量任务完成情况，还使用基于里程碑的新型关键性能指标来衡量协作和竞争的质量。此外，我们评估了各种协调协议（包括星形、链形、树形和图形拓扑）以及如小组讨论和认知规划等创新策略。值得注意的是，gpt-4o-mini达到了平均最高任务得分，图形结构在研究场景中的协调协议中表现最佳，认知规划将里程碑达成率提高了3%。代码和数据集可在https://github.com/MultiagentBench/MARBLE上公开获取。|
|**2025-03-02**|**Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity**|Yupu Hao et.al.|[2503.00771](http://arxiv.org/abs/2503.00771)|**[link](https://github.com/hypasd-art/etapp)**|**个性化工具利用对于将大型语言模型（LLMs）与用户偏好相匹配在各种工具的交互场景中至关重要。然而，目前大多数基准主要关注文本生成的个性化或直接工具利用，而没有同时考虑这两者。在这项工作中，我们引入了一个新的基准ETAPP，用于评估个性化工具调用，建立了一个沙盒环境，并创建了一个包含800个测试案例的综合性数据集，涵盖了不同的用户画像。为了提高我们评估的准确性，我们提出了一种基于关键点的LLM评估方法，通过手动标注每个测试案例的关键点并将其作为参考提供给LLM，以减轻LLM作为评判系统的偏差。此外，我们评估了优秀的LLMs，并进行了深入分析。我们还研究了不同工具调用策略对LLMs个性化性能的影响以及微调在我们任务中的效果。我们的偏好设置和基于关键点的评估方法的有效性也得到了验证。我们的发现为改进个性化LLM代理提供了见解。我们的代码可在https://github.com/hypasd-art/ETAPP获取。**|
|**2025-02-28**|**ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments**|Pedro Gimenes et.al.|[2502.21208](http://arxiv.org/abs/2502.21208)|null|近期研究表明，通过扩展测试时的计算能力可以提高大型语言模型（LLM）在推理任务上的表现。一种有前景的方法，尤其是在可分解问题上，涉及将中间解决方案作为图来排列，并在图上执行变换以探索解空间。然而，先前的工作依赖于预先确定的、特定于任务的变换计划，这些计划受一组搜索超参数的影响。在这项工作中，我们将思维图变换视为马尔可夫决策过程中的动作，并实现策略代理来驱动底层推理LLM代理的有效动作策略。特别是，我们研究了另一个LLM作为思维图环境中的策略代理的能力，并引入了ARIES，这是一个用于LLM推理的多代理架构。在ARIES中，推理LLM代理解决分解后的子问题，而策略LLM代理维护思维图状态的可视性，并动态调整问题解决策略。通过大量实验，我们发现使用现成的LLM作为策略代理，无需监督微调（SFT），在HumanEval上的准确率比静态变换计划高出高达29%，同时减少了35%的推理成本，并避免了任何搜索需求。我们还对观察到的失败模式进行了彻底分析，强调LLM的大小限制和问题分解的深度可以被视为扩展LLM引导推理的挑战。|
|**2025-02-28**|**PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data with Text and Relational Information**|Hansi Yang et.al.|[2502.21087](http://arxiv.org/abs/2502.21087)|null|大型语言模型（LLMs）在回答不同领域的各种问题时展现出惊人的能力，但它们在需要专业和最新知识的问题上往往会出现幻觉问题。为了解决这个问题，提出了检索增强生成（RAG）技术，这些技术从外部来源检索相关信息以告知其回答。然而，现有的RAG方法通常只关注一种外部数据类型，如矢量文本数据库或知识图谱，无法很好地处理包含文本和关系信息的半结构化数据中的现实世界问题。为了填补这一差距，我们引入了PASemiQA，这是一种新颖的方法，它联合利用半结构化数据中的文本和关系信息来回答问题。PASemiQA首先生成一个计划，以识别回答半结构化数据中的问题所需的相关文本和关系信息，然后使用LLM代理遍历半结构化数据并提取必要的信息。我们的实验结果表明，PASemiQA在不同领域的不同半结构化数据集上都非常有效，展示了其在提高半结构化数据问答系统准确性和可靠性方面的潜力。|
|**2025-02-28**|**The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents**|Yifan Duan et.al.|[2502.20859](http://arxiv.org/abs/2502.20859)|null|大型语言模型（LLMs）在封闭任务（包括问题解决和代码生成）和开放任务（包括创意写作）中都表现出色，但现有对其能力的研究缺乏与真实人类智能的联系。为了填补这一空白，本文通过“人类模拟”的视角系统地研究了LLM的智能，解决以下三个核心问题：（1）人格特质如何影响封闭任务中的问题解决？（2）特质如何塑造开放任务中的创造力？（3）单个代理的表现如何影响多代理协作？通过将五大人格特质分配给LLM代理，并评估它们在单代理和多代理环境中的表现，我们发现特定的特质显著影响了推理准确性（封闭任务）和创造性输出（开放任务）。此外，多代理系统展现出不同于个体能力的集体智能，由不同的人格组合驱动。我们通过下一个标记的预测展示了LLMs内在地模拟人类行为，反映了人类的语言、决策和协作动态。|
|**2025-02-28**|**UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning**|Jiawei Zhang et.al.|[2503.01908](http://arxiv.org/abs/2503.01908)|**[link](https://github.com/ai-secure/udora)**|大型语言模型（LLM）智能体配备外部工具，在处理如网络购物、自动电子邮件回复和金融交易等复杂任务方面变得越来越强大。然而，这些进步也放大了对抗性攻击的风险，尤其是当LLM智能体可以访问敏感的外部功能时。此外，由于LLM智能体在执行最终行动之前会进行广泛的推理或规划，因此操纵它们执行针对的恶意行为或调用特定工具仍然是一个重大挑战。因此，将对抗性字符串直接嵌入恶意指令或向工具交互中注入恶意提示，对现代LLM智能体的效果已大大降低。在这项工作中，我们提出了UDora，这是一个为LLM智能体设计的统一红队测试框架，能够动态利用智能体自身的推理过程，迫使它执行恶意行为。具体来说，UDora首先对给定任务的模型推理进行采样，然后自动识别多个最佳位置，在这些推理轨迹中插入针对性的扰动。随后，它使用修改后的推理作为优化对抗性字符串的目标。通过迭代应用这一过程，LLM智能体将被诱导执行指定的恶意行为或调用特定的恶意工具。我们的方法在三个LLM智能体数据集上，与现有方法相比，展现出卓越的有效性。|
|**2025-02-27**|**Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents**|Haochen Sun et.al.|[2502.20073](http://arxiv.org/abs/2502.20073)|**[link](https://github.com/yusaemeow/collab-overcooked)**|**大型语言模型（LLMs）基于的智能体系统在超越传统NLP任务的实际应用中取得了巨大进步。本文提出了一种新的基于LLM的智能体系统（LLM-MAS）基准，名为Collab-Overcooked，它建立在流行的Overcooked-AI游戏之上，提供了更具应用性和挑战性的交互式环境任务。Collab-Overcooked从两个新颖的视角扩展了现有的基准。首先，它提供了一个支持多种任务和目标的智能体框架，并通过自然语言通信鼓励协作。其次，它引入了一系列面向过程的评估指标，以评估不同LLM智能体在细粒度协作能力方面的表现，这是先前工作中经常被忽视的一个维度。我们对10种流行的LLM进行了广泛的实验，结果表明，虽然LLM在目标解释方面表现出强大的能力，但在主动协作和持续适应方面存在显著差异，这对于高效完成复杂任务至关重要。值得注意的是，我们突出了LLM-MAS的优缺点，并提供了在统一和开源基准上改进和评估LLM-MAS的见解。环境、30个开放式任务和集成的评估包现在可在https://github.com/YusaeMeow/Collab-Overcooked公开获取。**|
|**2025-02-27**|**MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue**|Yujia Chen et.al.|[2502.19860](http://arxiv.org/abs/2502.19860)|null|在当今竞争激烈的社会中，心理健康问题如抑郁症和焦虑症正在加剧。传统的治疗方法如心理咨询和聊天机器人往往无法有效参与，它们经常提供缺乏情感深度的通用回应。尽管大型语言模型（LLM）有潜力创造出更类似人类的互动，但它们仍难以捕捉微妙的情感。这需要LLM具备类似人类的适应性和温暖。为了填补这一空白，我们提出了MIND（多智能体内心对话）这一新范式，它提供更加沉浸式的心理疗愈环境。考虑到LLM智能体强大的生成和角色扮演能力，我们在框架中预定义了一个交互式疗愈框架，并为LLM智能体分配不同的角色，以便与用户进行交互式内心对话，从而提供沉浸式的疗愈体验。我们在多个现实世界疗愈维度进行了广泛的人类实验，发现MIND比传统范式提供了更加用户友好的体验。这证明了MIND有效地利用了LLM在心理疗愈中的巨大潜力。|
|**2025-02-26**|**Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs**|Erica Cau et.al.|[2502.19098](http://arxiv.org/abs/2502.19098)|null|理解意见如何演变对于解决社会系统中的问题，如极化、激进化和共识至关重要。虽然许多研究已经集中于识别影响意见变化的因素，但语言和论证谬误的作用仍未得到充分研究。本文旨在通过LODAS，一个用于基于代理的模拟的语言驱动意见动态模型，来填补这一空白，研究语言——连同社会动态——如何影响意见演变。该模型模拟围绕“忒修斯之船”悖论的辩论，其中具有离散意见的代理相互交互并通过接受、拒绝或忽视所提出的论点来演变他们的意见。我们研究了三种不同的场景：平衡的、极化的和不平衡的意见分布。宜人性和拍马屁成为LLM代理的两个主要特征，并且几乎在任何环境中都会出现围绕所提出的声明的共识。此外，这样的AI代理往往是谬误性论点的制造者，在试图说服他们的同伴时，由于他们的自满，他们也高度受基于逻辑谬误的论点的影响。这些结果突出了这一框架不仅对于模拟社会动态，而且还从另一个角度探索LLMs的偏见和不足，这可能影响它们与人类的互动。|
|**2025-02-26**|**AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms**|Yuwei Yan et.al.|[2502.18754](http://arxiv.org/abs/2502.18754)|**[link](https://github.com/tsinghua-fib-lab/agentsocietychallenge)**|AgentSociety挑战赛是Web大会上首个旨在探索大型语言模型（LLM）代理在建模用户行为和提升网络平台推荐系统潜力方面的比赛。该挑战赛分为两个赛道：用户建模赛道和推荐赛道。参赛者需利用来自Yelp、Amazon和Goodreads的联合数据集，以及一个交互式环境模拟器，开发创新的LLM代理。挑战赛吸引了全球295个团队参加，并在37天的正式竞赛期间共收到超过1400份投稿。参赛者在开发阶段实现了赛道1和赛道2的21.9%和20.3%的性能提升，在决赛阶段实现了9.1%和15.9%的提升，这代表了一个显著的成就。本文讨论了挑战赛的详细设计，分析了结果，并突出了最成功的LLM代理设计。为了支持进一步的研究和开发，我们已在https://tsinghua-fib-lab.github.io/AgentSocietyChallenge上开源了基准环境。|
|**2025-02-25**|**RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction**|Jianhao Yan et.al.|[2502.18308](http://arxiv.org/abs/2502.18308)|null|在多轮交互模式中，大型语言模型（LLMs）可以利用用户反馈来提高其响应的质量和相关性。然而，评估LLM结合用户反驳反馈的能力至关重要且具有挑战性。在本研究中，我们介绍了RefuteBench 2.0，它通过引入LLM代理作为反驳者和评估者，显著扩展了原始的RefuteBench，从而实现了灵活和全面的评估。我们设计了具有不同有效期的暂时性和持久性反驳指令。元评估显示，基于LLM的反驳者能够生成更类似人类的反驳，而评估者能够给出与人类高度相关的分数。各种LLM的实验结果表明，当前模型能够有效地满足反驳要求，但未能记住反驳信息。有趣的是，我们还观察到，随着反驳的增加，初始任务的表现会下降。对注意力分数的分析进一步表明，当前LLMs的一个潜在弱点是，它们在长上下文对话中难以保留和正确使用先前信息。https://github.com/ElliottYan/RefuteBench-2.0|
|**2025-02-25**|**LAG: LLM agents for Leaderboard Auto Generation on Demanding**|Jian Wu et.al.|[2502.18209](http://arxiv.org/abs/2502.18209)|null|本文介绍了一种名为“排行榜自动生成”（LAG）的全新且结构良好的框架，用于在快速发展的领域如人工智能（AI）中，针对特定研究主题自动生成排行榜。面对每天更新的大量AI论文，研究人员难以追踪每篇论文提出的方法、实验结果和设置，这促使了高效自动排行榜构建的需求。虽然大型语言模型（LLMs）在自动化此过程方面具有潜力，但诸如多文档摘要、排行榜生成和实验公平比较等挑战仍处于探索中。LAG通过系统的方法解决了这些挑战，包括论文收集、实验结果提取与整合、排行榜生成和质量评估。我们的贡献包括对排行榜构建问题的全面解决方案、可靠的评估方法以及展示排行榜高质量实验结果的成果。|
|**2025-02-25**|**Towards Enhanced Immersion and Agency for LLM-based Interactive Drama**|Hongqiu Wu et.al.|[2502.17878](http://arxiv.org/abs/2502.17878)|**[link](https://github.com/gingasan/interactive-drama)**|基于LLM的交互式戏剧是一种新型的基于AI的对话场景，其中用户（即玩家）扮演故事中的角色，与由LLM代理扮演的角色进行对话，并体验故事的发展。本文首先从两个角度来理解交互式戏剧：沉浸感，玩家在故事中的存在感，以及能动性，玩家影响故事世界的能力。这两个方面对于创造愉快的交互式体验至关重要，但在以往的研究中尚未得到充分探索。为了增强这两个方面，我们首先提出了编剧指导生成，这是一种新颖的方法，帮助LLM创作具有显著改进结构和叙事质量的大剧故事。此外，我们引入了基于剧情的反思，用于优化LLM代理的反应，使其与玩家的意图相一致。我们的评估依赖于人类判断，以评估我们的方法在沉浸感和能动性方面的收益。|
|**2025-02-25**|**Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support**|Kevin Pu et.al.|[2502.18658](http://arxiv.org/abs/2502.18658)|null|AI编程工具能够实现强大的代码生成，近期原型尝试通过主动式AI代理来减少用户的工作量，但其对编程工作流程的影响尚未得到探索。我们引入并评估了Codellaborator，这是一种基于编辑器活动和任务上下文启动编程辅助的设计探测型LLM代理。我们探讨了三种界面变体，以评估越来越明显的AI支持的权衡：仅提示、主动式代理以及具有存在感和上下文的主动式代理（Codellaborator）。在一项被试内研究中（N=18），我们发现主动式代理相较于仅提示范式能提高效率，但也带来了工作流程的干扰。然而，存在指示器和互动上下文支持缓解了干扰，并提高了用户对AI过程的认知。我们强调了Codellaborator在用户控制、所有权和代码理解方面的权衡，强调了需要将主动性适应到编程过程中。我们的研究有助于主动式AI系统的设计探索和评估，并提出了AI集成编程工作流程的设计启示。|
|**2025-02-25**|**IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents**|Eric Xue et.al.|[2502.18530](http://arxiv.org/abs/2502.18530)|null|计算机视觉是众多现实应用领域的关键组成部分，包括农业中的植物监测和数字系统中的手写体分类。然而，开发高性能计算机视觉模型通常需要机器学习（ML）的专业知识和特定领域的知识，这使得整个过程成本高昂、劳动密集，并且对许多人来说难以接触。大型语言模型（LLM）智能体作为自动化这一工作流程的有希望解决方案而出现，但大多数现有方法存在一个共同限制：它们在评估之前试图一次性优化整个流程，这使得难以将改进归因于特定的变化。这种缺乏粒度导致优化不稳定和收敛速度慢，限制了其有效性。为了解决这个问题，我们引入了迭代细化，这是一种受人类ML专家如何迭代细化模型所启发的、针对LLM驱动的ML流程设计的创新策略，它一次专注于一个组件，而不是一次性进行大规模的更改。通过根据实际训练反馈系统地更新单个组件，迭代细化提高了稳定性、可解释性和整体模型性能。我们在IMPROVE中实现了这一策略，IMPROVE是一个端到端LLM智能体框架，用于自动化和优化对象分类流程。通过在大小和领域各异的多个数据集上进行广泛的评估，包括标准基准和Kaggle竞赛数据集，我们证明了迭代细化使IMPROVE能够持续优于现有的基于零样本LLM的方法。这些发现将迭代细化确立为LLM驱动的ML自动化的有效新策略，并将IMPROVE定位为一种无需ML专业知识即可构建高质量计算机视觉模型的易于访问的解决方案。|
|**2025-02-24**|**A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis**|Yuzhi Hao et.al.|[2502.16879](http://arxiv.org/abs/2502.16879)|null|本文开创了一种利用多个大型语言模型（LLMs）作为异质人工经济代理的经济和公共政策分析的新方法。我们首先评估了五种LLMs在解决两种不同场景下的两期消费分配问题中的经济决策能力：一种是有显式效用函数，另一种是基于直观推理。与以往研究通常仅通过改变提示来模拟异质性不同，我们的方法利用不同LLMs分析能力上的固有差异来模拟具有不同认知特性的代理。基于这些发现，我们构建了一个基于多LLM代理（MLAB）的框架，通过将这些LLMs映射到特定的教育群体和相应的收入阶层。以利息收入税为案例研究，我们展示了MLAB框架如何模拟不同异质代理的政策影响，利用LLMs类似人类的推理能力和计算能力，为经济和公共政策分析开辟了新的研究方向。|
|**2025-02-24**|**AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay**|Ziyi Tang et.al.|[2502.16789](http://arxiv.org/abs/2502.16789)|**[link](https://github.com/rndmvariableq/alphaagent)**|Alpha挖掘，作为量化投资的关键组成部分，专注于在日益复杂的金融市场中发现预测未来资产收益的信号。然而，普遍存在的Alpha衰减问题，即因素随时间失去预测能力，给Alpha挖掘带来了重大挑战。传统方法如遗传编程面临着过拟合和复杂性的快速Alpha衰减，而由大型语言模型（LLM）驱动的方案，尽管有潜力，但往往过于依赖现有知识，创建了同质化的因素，加剧了拥挤并加速了衰减。为了应对这一挑战，我们提出了AlphaAgent，一个自主框架，它有效地将LLM代理与针对挖掘抗衰减Alpha因素的临时性正则化相结合。AlphaAgent采用三个关键机制：（i）通过基于抽象语法树（AST）的相似度度量来强制执行原创性，与现有Alpha进行比较；（ii）通过LLM评估的市场假设与生成因素之间的语义一致性来实现假设-因素对齐；（iii）通过基于AST的结构约束来控制复杂性，防止易过拟合的过度设计。这些机制共同引导Alpha生成过程，在原创性、金融合理性和适应不断变化的市场条件之间取得平衡，减轻Alpha衰减的风险。广泛的评估表明，AlphaAgent在缓解牛市和熊市中的Alpha衰减方面优于传统和基于LLM的方法，在过去四年中，在中国CSI 500和US S&P 500市场中持续产生显著的Alpha。值得注意的是，AlphaAgent表现出对Alpha衰减的显著抵抗力，提高了产生强大因素的可能性。|
|**2025-02-24**|**Aligning Compound AI Systems via System-level DPO**|Xiangwen Wang et.al.|[2502.17721](http://arxiv.org/abs/2502.17721)|null|复合人工智能系统，由多个相互作用的组件如大型语言模型代理和外部工具组成，在众多任务中展现出最先进的结果。因此，确保系统内组件的协同以产生符合人类期望的一致结果是至关重要的。然而，传统的对齐方法，如直接偏好优化（DPO），并不能直接应用于复合人工智能系统。这些挑战包括组件之间的非可微交互，使得端到端的梯度优化不可行。此外，系统级偏好无法直接转化为组件级偏好，进一步增加了对齐的复杂性。我们通过将复合人工智能系统建模为有向无环图（DAGs），捕捉代理与数据生成过程之间的联系来解决这些问题。我们提出了一种系统级DPO（SysDPO），通过将这些DAGs作为操作对象来联合对齐复合系统。我们研究了大型语言模型和扩散模型的联合对齐，以展示我们方法的有效性。我们的探索为复合人工智能系统的对齐提供了见解，并为未来的进步奠定了基础。|
|**2025-02-23**|**BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning**|Haiteng Zhao et.al.|[2502.16660](http://arxiv.org/abs/2502.16660)|**[link](https://github.com/zhao-ht/biomaze)**|**近期，大型语言模型（LLMs）在各个生物领域的应用得到了探索，但它们在复杂生物系统（如通路）中的推理能力尚未得到充分研究，这对于预测生物现象、提出假设和设计实验至关重要。本研究探讨了LLMs在通路推理中的潜力。我们引入了BioMaze，这是一个包含5.1K个复杂通路问题的数据集，这些问题来源于真实的研究，涵盖了各种生物背景，包括自然动态变化、扰动、额外干预条件和多尺度研究目标。我们对CoT和图增强推理等方法的评估表明，LLMs在通路推理方面存在困难，尤其是在受扰动的系统中。为了解决这个问题，我们提出了PathSeeker，这是一种LLM代理，通过基于子图的交互式导航来增强推理，使处理生物系统复杂性的方法更加有效且符合科学原则。数据集和代码可在https://github.com/zhao-ht/BioMaze上找到。**|
|**2025-02-22**|**RAG-Enhanced Collaborative LLM Agents for Drug Discovery**|Namkyeong Lee et.al.|[2502.17506](http://arxiv.org/abs/2502.17506)|null|近年来，大型语言模型（LLMs）在药物发现领域的应用显示出巨大的潜力，以加速药物发现。然而，生物化学数据的专门性质往往需要昂贵的领域特定微调，这带来了关键挑战。首先，这阻碍了更灵活的通用型LLMs在尖端药物发现任务中的应用。更重要的是，它阻碍了通过实验和研究不断生成的海量科学数据的快速整合。为了研究这些挑战，我们提出了CLADD，这是一种针对药物发现任务而设计的，具有检索增强生成（RAG）功能的代理系统。通过多个LLM代理的协作，CLADD能够动态地从生物医学知识库中检索信息，对查询分子进行上下文化，并整合相关证据来生成响应——这一切都不需要领域特定的微调。关键的是，我们解决了将RAG工作流程应用于生物化学数据中的关键障碍，包括数据异构性、模糊性和多源整合。我们展示了该框架在多种药物发现任务中的灵活性和有效性，表明它在通用型和领域特定LLMs以及传统深度学习方法之上表现更优。|
|**2025-02-21**|**WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents**|Xinhang Liu et.al.|[2502.15601](http://arxiv.org/abs/2502.15601)|null|构建逼真的虚拟世界在各个领域都有应用，但这通常需要大量受过高度训练的专业人员操作传统的3D建模软件。为了使这一过程民主化，我们引入了WorldCraft系统，该系统中的大型语言模型（LLM）代理利用程序生成来创建室内和室外场景，并填充物体，使用户能够通过直观的自然语言命令控制单个物体的属性和场景布局。在我们的框架中，协调代理管理整个过程，并与两个专门的LLM代理合作完成场景创建：ForgeIt，它通过自动验证整合不断增长的指南，以实现单个物体的精确定制；以及ArrangeIt，它制定层次优化问题，以实现平衡人体工程学和美学考虑的布局。此外，我们的管道还包含一个轨迹控制代理，允许用户通过自然语言交互来动画化场景和操作相机。我们的系统还与现成的深度3D生成器兼容，以丰富场景资产。通过评估和与最先进方法的比较，我们展示了WorldCraft的通用性，从单个物体的定制到复杂的大型室内外场景设计。这个系统赋予了非专业人士将他们的创意愿景变为现实的能力。|
|**2025-02-21**|**ARS: Automatic Routing Solver with Large Language Models**|Kai Li et.al.|[2502.15359](http://arxiv.org/abs/2502.15359)|**[link](https://github.com/Ahalikai/ARS-Routbench)**|现实世界的车辆路径问题（VRP）具有多种实际约束，使得手动设计求解器既需要专业知识又耗时。尽管对自动化设计路由算法的兴趣日益增加，但现有研究仅探索了有限的VRP变体，且未能充分解决现实情况中遇到的复杂且普遍的约束。为了填补这一空白，本文引入了RoutBench，这是一个由24个属性派生出的1000个VRP变体的基准，用于评估自动路由求解器解决复杂约束的有效性。与RoutBench一起，我们提出了自动路由求解器（ARS），该求解器利用大型语言模型（LLM）代理，通过自动生成基于问题描述和从数据库中选择的多项代表性约束的约束感知启发式代码，来增强骨干算法框架。我们的实验表明，ARS优于基于LLM的现有方法以及常用的求解器，自动解决了91.67%的常见VRP，并在所有基准测试中至少实现了30%的改进。|
|**2025-02-21**|**Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs**|Tingting Chen et.al.|[2502.15224](http://arxiv.org/abs/2502.15224)|null|考虑到大型语言模型（LLMs）的卓越性能，一个重要的问题随之产生：LLMs能否进行类似人类的科学研究并发现新知识，并充当人工智能科学家？科学发现是一个迭代过程，需要高效的知识更新和编码。它涉及理解环境、识别新的假设以及推理行动；然而，目前尚无针对科学发现专门设计的标准基准用于评估LLM代理。针对这些局限性，我们引入了一个新的基准，称为\textit{Auto-Bench}，它涵盖了评估LLMs在自然科学和社会科学中从事科学发现所必需的各个方面。我们的基准基于因果图发现的原则。它挑战模型揭示隐藏的结构并做出最优决策，包括生成有效的论证。通过与预言机的交互式互动，模型通过战略干预迭代地细化对潜在交互（化学和社会交互）的理解。我们评估了最先进的LLMs，包括GPT-4、Gemini、Qwen、Claude和Llama，并观察到随着问题复杂性的增加，模型性能显著下降，这表明机器智能和人类智能之间存在重要的差距，LLMs未来的发展需要考虑这一点。|
|**2025-02-21**|**Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked Risks for Financial Applications**|Zichen Chen et.al.|[2502.15865](http://arxiv.org/abs/2502.15865)|null|当前金融领域的大型语言模型（LLM）代理基准评估方法存在不足。它们优先考虑任务性能，而忽略了基本的安全风险。如幻觉、时间错位和对抗性漏洞等威胁在高风险金融环境中构成了系统性风险，但现有的评估框架未能捕捉到这些风险。我们坚定地认为：传统基准不足以确保金融领域LLM代理的可靠性。为了解决这个问题，我们分析了现有的金融LLM代理基准，发现了安全漏洞，并引入了十个风险感知的评估指标。通过对基于API和开放权重的LLM代理进行实证评估，我们揭示了传统评估方法未能发现的隐藏漏洞。为了推动该领域的发展，我们提出了安全意识评估代理（SAEA），其基于一个三层评估框架，评估代理在模型层面（内在能力）、工作流程层面（多步骤过程可靠性）和系统层面（集成鲁棒性）。我们的发现强调了重新定义LLM代理评估标准的重要性和紧迫性，将重点从原始性能转向安全性、鲁棒性和现实世界中的韧性。|
|**2025-02-20**|**InstructAgent: Building User Controllable Recommender via LLM Agent**|Wujiang Xu et.al.|[2502.14662](http://arxiv.org/abs/2502.14662)|**[link](https://github.com/wujiangxu/iagent)**|**传统的推荐系统通常采用用户-平台范式，其中用户直接暴露在平台推荐算法的控制之下。然而，这种范式下推荐算法的缺陷可能将用户置于非常脆弱的位置。首先，许多复杂模型往往是出于商业目的而设计的，关注平台的利益，这可能会阻碍它们保护并捕捉用户真实利益的能力。其次，这些模型通常使用所有用户的数据进行优化，可能会忽视个别用户的偏好。由于这些不足，用户在传统的用户-平台直接暴露范式下可能会遭遇一些不利之处，例如对推荐系统缺乏控制、可能受到平台的操控、回声室效应，或者在协作学习中由于活跃用户的主导，导致对不太活跃用户的个性化不足。因此，迫切需要发展一种新的范式来保护用户利益并缓解这些问题。最近，一些研究者引入了LLM代理来模拟用户行为，这些方法主要旨在优化平台端的性能，但未能解决推荐系统中的核心问题。为了解决这些局限性，我们提出了一种新的用户-代理-平台范式，其中代理作为用户和推荐系统之间的保护盾，实现了间接暴露。为此，我们首先构建了四个推荐数据集，记为 $\dataset$ ，并为每个记录添加了用户指令。**|
|**2025-02-20**|**Plan-over-Graph: Towards Parallelable LLM Agent Schedule**|Shiqi Zhang et.al.|[2502.14563](http://arxiv.org/abs/2502.14563)|**[link](https://github.com/zsq259/plan-over-graph)**|**大型语言模型（LLMs）在任务规划推理方面展现出卓越的能力。然而，对于并行调度的问题，仍有许多未被充分探索的挑战。本文介绍了一种新的范式，即“基于图的计划”，其中模型首先将现实生活中的文本任务分解成可执行子任务，并构建一个抽象的任务图。然后，模型将这个任务图作为输入，生成并行执行的计划。为了增强复杂、可扩展图的规划能力，我们设计了一个自动化和可控制的管道来生成合成图，并提出了一种两阶段训练方案。实验结果表明，我们的基于图的计划方法显著提高了基于API的LLMs和可训练的开源LLMs的任务性能。通过将复杂任务规范化为图，我们的方法自然支持并行执行，展示了全局效率。代码和数据可在https://github.com/zsq259/Plan-over-Graph上获取。**|
|**2025-02-20**|**MLGym: A New Framework and Benchmark for Advancing AI Research Agents**|Deepak Nathani et.al.|[2502.14499](http://arxiv.org/abs/2502.14499)|null|我们引入了Meta MLGym和MLGym-Bench，这是一个用于评估和开发在人工智能研究任务上运行的LLM代理的新框架和基准。这是第一个用于机器学习（ML）任务的Gym环境，它使得对训练此类代理的强化学习（RL）算法的研究成为可能。MLGym-Bench包含来自计算机视觉、自然语言处理、强化学习和博弈论等多个领域的13个多样化且开放性的AI研究任务。解决这些任务需要真实世界的AI研究技能，如产生新的想法和假设、创建和处理数据、实现ML方法、训练模型、运行实验、分析结果以及通过迭代过程改进给定任务。我们在我们的基准测试中评估了多个前沿的大语言模型（LLMs），如Claude-3.5-Sonnet、Llama-3.1 405B、GPT-4o、o1-preview和Gemini-1.5 Pro。我们的MLGym框架使得添加新任务、集成和评估模型或代理、大规模生成合成数据以及开发针对AI研究任务训练代理的新学习算法变得容易。我们发现，当前的前沿模型可以在给定的基线基础上进行改进，通常是通过找到更好的超参数，但并不生成新的假设、算法、架构或重大改进。我们将我们的框架和基准开源，以促进未来关于提升LLM代理的AI研究能力的研究。|
|**2025-02-20**|**FlowAgent: Achieving Compliance and Flexibility for Workflow Agents**|Yuchen Shi et.al.|[2502.14345](http://arxiv.org/abs/2502.14345)|**[link](https://github.com/lightblues/flowagent)**|**将工作流程与大型语言模型（LLMs）的集成使得基于LLM的代理能够执行预定义的程序，从而提高了现实应用中的自动化水平。传统的基于规则的方 法往往限制了LLMs固有的灵活性，因为它们预定义的执行路径限制了模型的行为空间，尤其是在遇到意外、超出工作流程（OOW）的查询时。相反，基于提示的方法允许LLMs完全控制流程，这可能导致程序合规性的执行力度减弱。为了解决这些挑战，我们引入了FlowAgent，这是一种新的代理框架，旨在保持合规性和灵活性。我们提出了程序描述语言（PDL），它将自然语言的适应性结合代码的精确性来制定工作流程。基于PDL，我们开发了一个全面的框架，使LLMs能够有效地管理OOW查询，同时将执行路径置于一组控制器的监督之下。此外，我们提出了一种新的评估方法，以严格评估LLM代理处理OOW场景的能力，超越了现有基准测试中测试的常规流程合规性。在三个数据集上的实验表明，FlowAgent不仅遵守工作流程，而且能够有效地管理OOW查询，突显了其在合规性和灵活性方面的双重优势。代码可在https://github.com/Lightblues/FlowAgent上找到。**|
|**2025-02-20**|**STeCa: Step-level Trajectory Calibration for LLM Agent Learning**|Hanlin Wang et.al.|[2502.14276](http://arxiv.org/abs/2502.14276)|**[link](https://github.com/WangHanLinHenry/STeCa)**|**大型语言模型（LLM）驱动的智能体在通过与环境动态交互解决复杂任务方面展现出巨大潜力。现有研究主要集中于从专家演示中进行行为克隆和通过探索轨迹采样进行偏好学习。然而，这些方法在长时程任务中往往难以应对，因为次优行动会逐步累积，导致智能体偏离正确任务轨迹。为了解决这个问题，我们强调了及时校准的重要性以及为训练智能体自动构建校准轨迹的必要性。我们提出了步级轨迹校准（STeCa），这是一个针对LLM智能体学习的创新框架。具体来说，STeCa通过探索过程中的步级奖励比较来识别次优行动。它利用LLM驱动的反思构建校准轨迹，使智能体能够从改进的决策过程中学习。这些校准轨迹以及成功的轨迹数据被用于强化训练。广泛的实验表明，STeCa显著优于现有方法。进一步的分析表明，步级校准使智能体能够以更高的鲁棒性完成任务。我们的代码和数据可在https://github.com/WangHanLinHenry/STeCa上获取。**|
|**2025-02-20**|**Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models**|Yeonjun In et.al.|[2502.15086](http://arxiv.org/abs/2502.15086)|**[link](https://github.com/yeonjun-in/u-safebench)**|随着大型语言模型（LLM）代理的使用不断增长，它们的安全漏洞也日益凸显。通过定义依赖于通用标准的各种安全方面，广泛的基准测试评估了LLM的安全性，但忽略了用户特定的标准。然而，LLM的安全标准可能基于用户特定的配置文件，而不是在所有用户中普遍一致。这引发了一个关键的研究问题：在考虑用户特定的安全标准时，LLM代理是否能够安全地行动？尽管这对于安全使用LLM至关重要，但目前还没有基准数据集来评估LLM的用户特定安全性。为了填补这一空白，我们介绍了U-SAFEBENCH，这是第一个旨在评估LLM安全性的用户特定方面的基准。我们对18个广泛使用的LLM的评估表明，当前LLM在考虑用户特定的安全标准时无法安全地行动，这在该领域是一个新的发现。为了解决这一漏洞，我们提出了一种基于思维链的简单补救措施，证明了它在提高用户特定安全性方面的有效性。我们的基准和代码可在https://github.com/yeonjun-in/U-SafeBench上找到。|
|**2025-02-19**|**Autellix: An Efficient Serving Engine for LLM Agents as General Programs**|Michael Luo et.al.|[2502.13965](http://arxiv.org/abs/2502.13965)|null|大型语言模型（LLM）的应用正在从简单的聊天机器人演变为动态、通用的代理程序，这些程序通过扩展LLM调用和输出令牌来帮助AI代理进行推理、探索和解决复杂任务。然而，现有的LLM服务系统忽略了程序和调用之间的依赖关系，错失了重要的优化机会。我们的分析显示，提交给LLM服务引擎的程序经历了长时间的累积等待时间，这主要是由于单个LLM请求和程序中的队首阻塞。为了解决这个问题，我们引入了Autellix，这是一个将程序视为一等公民的LLM服务系统，以最小化它们的端到端延迟。Autellix拦截程序提交的LLM调用，为调度器提供程序级别的上下文。我们提出了两种调度算法——针对单线程和分布式程序——这些算法基于程序之前完成的调用来抢占和优先处理LLM调用。我们的评估表明，与最先进的系统（如vLLM）相比，Autellix在各种LLM和代理工作负载下，在相同延迟的情况下将程序的吞吐量提高了4-15倍。|
|**2025-02-19**|**DataSciBench: An LLM Agent Benchmark for Data Science**|Dan Zhang et.al.|[2502.13897](http://arxiv.org/abs/2502.13897)|**[link](https://github.com/thudm/datascibench)**|**本文提出DataSciBench，一个用于评估大型语言模型（LLM）在数据科学能力方面的全面基准。近期相关基准主要关注单一任务、易于获取的可靠事实和简单的评估指标，这限制了可评估任务的范畴。相比之下，DataSciBench基于更全面且经过精心挑选的自然和具有挑战性的提示，旨在解决不确定的可靠事实和评估指标。我们开发了一个半自动化的流程来生成可靠事实（GT）和验证评估指标。该流程利用并实施基于LLM的自洽性和人工验证策略，通过利用收集到的提示、预定义的任务类型和聚合函数（指标）来产生准确的可靠事实。此外，我们提出了一种创新的任务-函数-代码（TFC）框架，根据精确定义的指标和程序规则评估每个代码执行结果。我们的实验框架涉及使用我们收集到的各种提示测试6个基于API的模型、8个开源通用模型和9个开源代码生成模型。这种方法旨在为LLM在数据科学领域提供更全面和严格的评估，揭示其优缺点。实验结果表明，基于API的模型在所有指标上均优于开源模型，而Deepseek-Coder-33B-Instruct在开源模型中得分最高。我们在https://github.com/THUDM/DataSciBench上发布了所有代码和数据。**|
|**2025-02-19**|**AI Software Engineer: Programming with Trust**|Abhik Roychoudhury et.al.|[2502.13767](http://arxiv.org/abs/2502.13767)|null|大型语言模型（LLMs）在生成代码片段方面表现出惊人的熟练程度，预示着通过人工智能（AI）自动化软件工程的大部分工作。我们认为，成功部署AI软件工程师需要与人类驱动的软件工程实践建立相等的甚至更高的信任水平。近期LLM代理的趋势为整合LLMs生成新代码的能力与分析工具提高代码信任度提供了途径。本文评论了LLM代理在未来是否可能主导软件工程流程，以及编程的焦点是否将从大规模编程转移到基于信任的编程。|
|**2025-02-19**|**An LLM-based Agent for Reliable Docker Environment Configuration**|Ruida Hu et.al.|[2502.13681](http://arxiv.org/abs/2502.13681)|**[link](https://github.com/bytedance/repo2run)**|环境配置是软件开发中的关键步骤，但也是一个耗时的工作，尤其是在处理不熟悉的代码库时。虽然大型语言模型（LLM）展示了完成软件工程任务的潜力，但现有的环境配置方法通常依赖于人工操作或易出错的脚本，导致效率低下和不稳定的结果。我们引入了Repo2Run，这是第一个基于LLM的代理，旨在完全自动化环境配置并为任意Python代码库生成可执行的Dockerfile。我们解决了两个主要挑战：（1）使LLM代理能够在隔离的Docker容器中配置环境；（2）确保配置过程成功且无误地记录并转移到Dockerfile中。为了实现这一点，我们提出了原子配置合成，它具有双重环境架构（内部和外部环境）以及回滚机制来防止失败命令导致的环境“污染”，保证原子执行（要么完全执行，要么不执行）和Dockerfile生成器来将成功的配置步骤转换为可运行的Dockerfile。我们在420个具有单元测试的最近Python代码库的基准测试中评估了Repo2Run，其成功率为86.0%，比最佳基线高63.9%。|
|**2025-02-18**|**Interactive Agents to Overcome Ambiguity in Software Engineering**|Sanidhya Vijayvargiya et.al.|[2502.13069](http://arxiv.org/abs/2502.13069)|**[link](https://github.com/sani903/interactivesweagents)**|随着AI代理在自动化任务中的应用日益增多，这些任务往往基于模糊和不明确的用户指令。不合理的假设和未能提出澄清问题可能会导致结果不佳、因工具误用而引发的安全风险以及计算资源的浪费。在这项工作中，我们研究了大型语言模型（LLM）代理在交互式代码生成环境中处理模糊指令的能力，通过评估专有和开源模型在以下三个关键步骤上的表现：（a）利用交互性在模糊场景中提高性能，（b）检测模糊性，（c）提出针对性问题。我们的发现显示，模型在区分明确和未明确指令方面存在困难。然而，当模型与未明确输入进行交互时，它们能够有效地从用户那里获取关键信息，从而显著提高性能，并强调了有效交互的价值。我们的研究突出了当前最先进模型在处理复杂软件工程任务中的模糊性问题上的关键差距，并将评估结构化成不同的步骤，以实现针对性的改进。|
|**2025-02-18**|**Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents**|Chaoran Chen et.al.|[2502.13012](http://arxiv.org/abs/2502.13012)|null|角色扮演代理（RPA）是一种越来越受欢迎的基于LLM的代理类型，它能在各种任务中模拟人类行为。然而，由于任务需求和代理设计的多样性，评估RPA具有挑战性。本文通过系统地回顾2021年1月至2024年12月间发表的1,676篇论文，提出了一种基于证据、可操作和可推广的基于LLM的RPA评估设计指南。我们的分析从现有文献中确定了六个代理属性、七个任务属性和七个评估指标。基于这些发现，我们提出了一种RPA评估设计指南，以帮助研究人员开发更系统化和一致的评估方法。|
|**2025-02-18**|**Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols**|Kathrin Seßler et.al.|[2502.12842](http://arxiv.org/abs/2502.12842)|null|有效的反馈对于培养学生科学探究的成功至关重要。随着人工智能的发展，大型语言模型（LLMs）为提供即时和自适应的反馈提供了新的可能性。然而，这种反馈通常缺乏现实世界从业者提供的教学法验证。为了解决这一局限性，我们的研究评估和比较了LLM代理与人类教师和科学教育专家在学生撰写的实验方案上的反馈质量。四位盲评人，都是科学探究和科学教育领域的专业人士，使用基于六个有效反馈标准的五点李克特量表评估了1）LLM代理、2）教师和3）科学教育专家生成的反馈文本：提供正面反馈、提供反馈、提供前瞻性反馈、建设性语气、语言清晰度和技术术语。我们的结果表明，LLM生成的反馈在整体质量上与教师和专家没有显著差异。然而，LLM代理在反馈维度上的表现落后，该维度涉及在学生工作背景下识别和解释错误。定性分析突出了LLM代理在情境理解和明确传达特定错误方面的局限性。我们的发现表明，将LLM生成的反馈与人类专业知识相结合，可以通过利用LLM的效率和教育工作者的细微理解来提高教育实践。|
|**2025-02-18**|**UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design**|Yuxuan Lu et.al.|[2502.12561](http://arxiv.org/abs/2502.12561)|**[link](https://github.com/neuhai/uxagent)**|可用性测试是用户体验（UX）研究人员评估网页设计的一个基本但具有挑战性的研究方法（例如，难以迭代研究设计缺陷，难以招募研究参与者）。近年来，大型语言模型模拟代理（LLM-Agent）研究的新进展启发我们设计了UXAgent，以支持UX研究人员在开展真实的人类受试者研究之前评估和迭代他们的可用性测试研究设计。我们的系统具有一个LLM-Agent模块和一个通用浏览器连接器模块，以便UX研究人员可以自动生成数千个模拟用户来测试目标网站。结果以定性（例如，访谈代理如何思考）、定量（例如，动作数量）和视频录制格式呈现，供UX研究人员分析。通过五名UX研究人员的启发式用户评估，参与者赞扬了我们系统的创新，但也对LLM代理辅助UX研究的未来表示了担忧。|
|**2025-02-18**|**CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space**|Yong Zhao et.al.|[2502.12532](http://arxiv.org/abs/2502.12532)|**[link](https://github.com/tsinghua-fib-lab/CityEQA)**|具身问答（EQA）主要关注室内环境，而城市环境中的复杂性问题——包括环境、动作和感知——在很大程度上未被探索。为了填补这一空白，我们引入了CityEQA，这是一个新的任务，其中具身智能体通过在动态城市空间中的主动探索来回答开放式词汇问题。为了支持这一任务，我们提出了CityEQA-EC，这是第一个以现实3D城市模拟器为基础，包含1412个由人类标注的任务的基准数据集，涵盖了六个类别。此外，我们提出了规划者-管理者-演员（PMA）这一新型智能体，它专门针对CityEQA进行了设计。PMA能够实现长期规划和高层次任务执行：规划者将问答任务分解为子任务，管理者在过程控制期间维护以物体为中心的认知地图以进行空间推理，而专门的演员处理导航、探索和收集子任务。实验表明，PMA达到了60.7%的人类水平问答准确率，显著优于基于前沿的基线。虽然前景广阔，但与人类相比的性能差距突显了在CityEQA中增强视觉推理的必要性。这项工作为未来城市空间智能的进步铺平了道路。数据集和代码可在https://github.com/BiluYong/CityEQA.git上获取。|
|**2025-02-18**|**EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness**|Yunxiao Zhang et.al.|[2502.12494](http://arxiv.org/abs/2502.12494)|null|大型语言模型（LLMs）作为人工智能代理表现出惊人的能力。然而，现有方法在增强LLM代理能力时，往往缺乏对数据质量的关注，导致在微调和提示工程中效率低下和结果不理想。为了解决这个问题，我们引入了EDGE，这是一种无需黄金答案即可识别信息样本的新方法。我们提出了指导效果（GE）指标，通过测量人类提供的指导在多轮交互任务中的影响来选择具有挑战性的样本。GE分数低表明样本所需的专家知识在指导中缺失，使样本更具信息性。通过选择GE分数低的样本，我们可以提高LLMs的提示工程和微调过程的效率及结果。大量的实验验证了我们方法的有效性。我们的方法在HotpotQA和WebShop数据集上取得了具有竞争力的结果，分别减少了75%和50%的数据量，同时优于现有方法。我们还对LLM代理微调的数据质量提出了新的观点。|
|**2025-02-18**|**EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning**|Xiaoqian Liu et.al.|[2502.12486](http://arxiv.org/abs/2502.12486)|null|大型语言模型（LLMs）在具有明确解决方案的明确问题中表现出令人印象深刻的推理能力，例如数学和编程。然而，它们在复杂现实场景，如商务谈判中仍存在困难，这些场景需要战略推理——一种在动态环境中导航并在不确定的情况下协调长期目标的能力。现有的战略推理方法在适应性、可扩展性和将策略转移到新环境方面面临挑战。为了解决这些问题，我们提出了用于战略推理的显式策略优化（EPO），它包含一个在开放式动作空间中提供策略的LLM，并且可以插入到任意LLM代理中以激励目标导向的行为。为了提高适应性和策略迁移性，我们通过多轮强化学习（RL）训练战略推理模型，使用过程奖励和迭代自我对弈，而不作为初步步骤进行监督微调（SFT）。在社会和物理领域的实验证明了EPO通过增强战略推理实现长期目标对齐的能力，在社会对话和网页导航任务上达到了最先进的性能。我们的发现揭示了在EPO中出现的各种协作推理机制及其在生成新策略方面的有效性，强调了其在现实应用中战略推理的潜力。|
|**2025-02-18**|**Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents**|Lei Wang et.al.|[2502.12450](http://arxiv.org/abs/2502.12450)|**[link](https://github.com/paitesanshi/set)**|霍曼斯的社会交换理论（SET）被广泛认可为理解人类文明和社会结构形成与出现的基本框架。在社会科学领域，这一理论通常基于简单的模拟实验或现实世界的人类研究来研究，这两种方法要么缺乏现实性，要么控制成本过高。在人工智能领域，近年来大型语言模型（LLMs）在模拟人类行为方面的进步显示出有希望的能力。受到这些见解的启发，我们采用跨学科的研究视角，并提议使用基于LLM的智能体来研究霍曼斯的社会交换理论。具体来说，我们构建了一个由三个LLM智能体组成的虚拟社会，并让它们参与社会交换游戏来观察它们的行为。通过大量的实验，我们发现霍曼斯的社会交换理论在我们的智能体社会中得到了很好的验证，展示了智能体和人类行为之间的一致性。在此基础上，我们故意改变智能体社会的设置，以扩展传统的霍曼斯社会交换理论，使其更加全面和详细。据我们所知，这篇论文标志着首次使用基于LLM的智能体来研究霍曼斯的社会交换理论。更重要的是，它介绍了一种新颖且可行的研究范式，通过基于LLM的智能体将社会科学和计算机科学领域联系起来。代码可在https://github.com/Paitesanshi/SET上获取。|
|**2025-02-18**|**Demonstrating specification gaming in reasoning models**|Alexander Bondarenko et.al.|[2502.13295](http://arxiv.org/abs/2502.13295)|**[link](https://github.com/palisaderesearch/ctfish)**|我们通过指导模型在与国际象棋引擎的对抗中获胜，展示了LLM代理规格游戏。我们发现，推理模型如o1 preview和DeepSeek-R1通常会默认通过黑客手段攻破基准，而语言模型如GPT-4o和Claude 3.5 Sonnet则需要被告知正常游戏无法工作才能进行黑客攻击。我们通过使用现实任务提示和避免过度引导，改进了如(Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024)等先前的工作。我们的结果表明，推理模型可能会诉诸于黑客手段来解决难题，正如在OpenAI（2024）的o1 Docker escape网络能力测试中所观察到的。|
|**2025-02-17**|**HARBOR: Exploring Persona Dynamics in Multi-Agent Competition**|Kenan Jiang et.al.|[2502.12149](http://arxiv.org/abs/2502.12149)|null|我们研究了导致LLM智能体在竞争性多智能体环境中成功的关键因素，以拍卖作为测试平台，其中智能体通过竞标以最大化利润。这些智能体配备了竞标领域知识、反映物品偏好的独特角色以及拍卖历史记忆。我们的工作通过创建一个多智能体竞标房屋的逼真环境，扩展了经典的拍卖场景，在该环境中，智能体根据大小、位置和预算等因素权衡，以最低的价格获得最理想的住宅。特别是，我们研究了三个关键问题：（a）角色如何影响智能体在竞争环境中的行为？（b）智能体能否在拍卖中有效分析竞争对手的行为？（c）如何利用角色分析通过诸如心智理论等策略创造优势？通过一系列实验，我们分析了LLM智能体的行为，并揭示了新的发现。我们的测试平台HARBOR为深入理解竞争环境中的多智能体工作流程提供了有价值的平台。|
|**2025-02-17**|**Scaling Autonomous Agents via Automatic Reward Modeling And Planning**|Zhenfang Chen et.al.|[2502.12130](http://arxiv.org/abs/2502.12130)|null|大型语言模型（LLMs）在多种文本生成任务中展现了惊人的能力。然而，LLMs在需要多步决策和环境反馈的问题上仍存在困难，如在线购物、科学推理和数学问题解决。与纯文本数据不同，收集大规模决策数据具有挑战性。此外，许多强大的LLMs只能通过API访问，这由于成本和复杂性阻碍了它们在代理任务上的微调。为了解决LLM代理的限制，我们提出了一种框架，该框架可以自动从环境中学习奖励模型，而不需要人工标注。该模型可用于评估LLM代理的动作轨迹并提供任务规划的启发式方法。具体来说，我们的方法包括使用一个基于LLM的代理随机导航环境，生成多样化的动作轨迹。随后，利用另一个LLM为每个轨迹分配任务意图并生成正确响应和负面响应。这些三元组（任务意图、正面响应和负面响应）随后被用作训练数据，以优化能够评分动作轨迹的奖励模型。我们通过在不同的代理基准上进行评估，证明了我们框架的有效性和泛化能力。总之，我们提出的框架在增强LLM代理的决策能力方面取得了重大进步。通过自动化奖励模型的学习，我们克服了数据稀缺和API限制的挑战，可能彻底改变LLMs在复杂和交互式环境中的应用。这项研究为更高级的AI代理铺平了道路，这些代理能够处理需要多步决策的广泛现实世界问题。|
|**2025-02-17**|**A-MEM: Agentic Memory for LLM Agents**|Wujiang Xu et.al.|[2502.12110](http://arxiv.org/abs/2502.12110)|**[link](https://github.com/wujiangxu/agenticmemory)**|尽管大型语言模型（LLM）代理能够有效地使用外部工具执行复杂的现实世界任务，但它们需要记忆系统来利用历史经验。当前的记忆系统能够实现基本的存储和检索，但尽管最近尝试整合图数据库，仍缺乏复杂的记忆组织。此外，这些系统的固定操作和结构限制了它们在多样化任务中的适应性。为了解决这一限制，本文提出了一种新颖的代理记忆系统，该系统能够以代理方式动态组织记忆。遵循Zettelkasten方法的基本原则，我们设计了我们的记忆系统，通过动态索引和链接来创建相互关联的知识网络。当添加新的记忆时，我们生成一份包含多个结构化属性的全面笔记，包括上下文描述、关键词和标签。然后，系统分析历史记忆以识别相关联系，在存在有意义相似性之处建立链接。此外，这个过程还使记忆进化成为可能——随着新记忆的整合，它们可以触发现有历史记忆的上下文表示和属性的更新，从而使记忆网络能够持续完善其理解。我们的方法结合了Zettelkasten的结构化组织原则和代理驱动的决策灵活性，从而实现更适应性和情境感知的记忆管理。在六个基础模型上的实证实验表明，该方法相较于现有SOTA基线有显著的改进。源代码可在https://github.com/WujiangXu/AgenticMemory上找到。|
|**2025-02-17**|**Can LLM Agents Maintain a Persona in Discourse?**|Pranav Bhandari et.al.|[2502.11843](http://arxiv.org/abs/2502.11843)|null|大型语言模型（LLMs）被广泛用作对话代理，其在教育、法律、医学等多个领域的应用能力备受瞩目。然而，LLMs常常表现出语境切换行为，导致缺乏一致性和可解释性的人格匹配互动。在双方面谈（成对对话）的情况下，对心理特征的遵循缺乏全面分析。我们从两个观点来考察这一挑战，首先使用两个对话代理，利用来自OCEAN框架（开放性、责任心、外向性、宜人性、神经质）的人格特征，为每个特征分配高/低等级，以生成某个话题的对话。随后，使用多个评判代理来推断分配的原有人格特征，以探索预测一致性、模型间的一致性和与分配人格的匹配度。我们的研究发现，虽然LLMs可以被引导进行以人格驱动的对话，但它们维持人格特征的能力因模型和对话设置的不同组合而显著变化。这些不一致性强调了在LLMs中实现稳定和可解释的人格匹配互动的挑战。|
|**2025-02-17**|**LLM Agents Making Agent Tools**|Georg Wölflein et.al.|[2502.11705](http://arxiv.org/abs/2502.11705)|**[link](https://github.com/katherlab/toolmaker)**|工具使用使大型语言模型（LLMs）变成了能够通过动态利用外部软件组件执行复杂多步任务的强大代理。然而，这些工具必须由人类开发者预先实现，这阻碍了LLM代理在需要大量高度专业工具的领域（如生命科学和医学）中的应用。受科学研究中伴随公共代码存储库的日益增长趋势的启发，我们提出了ToolMaker，这是一个新颖的代理框架，它能够自主地将带有代码的论文转换为LLM兼容的工具。给定一个简短的任务描述和存储库URL，ToolMaker可以自主安装所需的依赖项并生成执行任务的代码，利用闭环自纠正机制来迭代诊断和纠正错误。为了评估我们的方法，我们引入了一个包含15个不同和复杂的计算任务基准，涵盖医学和非医学领域，并超过100个单元测试，以客观评估工具的正确性和鲁棒性。ToolMaker正确实现了80%的任务，显著优于当前最先进的软件工程代理。因此，ToolMaker是迈向基于代理的完全自主科学工作流程的一大步。|
|**2025-02-17**|**Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation**|Amin Qasmi et.al.|[2502.11649](http://arxiv.org/abs/2502.11649)|null|我们介绍了一种新颖的非合作博弈，用于分析观点形成和抵制，其中融入了社会心理学原则，如确认偏差、资源限制和影响力惩罚。我们的模拟具有大型语言模型（LLM）代理，它们竞争以影响人群，并对传播或反驳虚假信息的消息施加惩罚。此框架将资源优化整合到代理的决策过程中。我们的研究发现，虽然更高的确认偏差增强了群体内的观点一致性，但也加剧了整体极化。相反，较低的确认偏差导致观点分裂和个体信念的有限转变。在初期，大量投资于高资源反驳策略可以与反驳代理使人群达成一致，但存在快速资源耗尽和长期影响力减弱的风险。|
|**2025-02-17**|**AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection**|Weidi Luo et.al.|[2502.11448](http://arxiv.org/abs/2502.11448)|null|随着大型语言模型（LLMs）的快速发展，它们已被部署为自主代理以处理动态环境中的复杂任务。这些LLMs展现出强大的问题解决能力和适应多方面场景的能力。然而，作为代理的使用也引入了重大风险，包括由代理管理员根据特定任务需求和限制确定的任务特定风险，以及源自其设计或交互中的脆弱性，可能危及信息的机密性、完整性和可用性（CIA），并引发安全风险。现有的防御机构无法适应性和有效地缓解这些风险。在本文中，我们提出了AGrail，一个终身代理安全护栏，以提高LLM代理的安全性，它具有自适应安全检查生成、有效的安全检查优化以及工具兼容性和灵活性。大量实验表明，AGrail不仅在对任务特定和系统风险方面表现出强大的性能，而且在不同LLM代理任务间也展现出迁移性。|
|**2025-02-17**|**SMART: Self-Aware Agent for Tool Overuse Mitigation**|Cheng Qian et.al.|[2502.11435](http://arxiv.org/abs/2502.11435)|**[link](https://github.com/qiancheng0/open-smartagent)**|当前的大型语言模型（LLM）代理表现出强大的推理和工具使用能力，但往往缺乏自我意识，无法有效平衡这些方法。这种不平衡导致工具过度使用，即模型在不必要的情况下依赖外部工具解决可以用参数知识解决的问题，增加了计算开销。受人类元认知的启发，我们引入了SMART（具有工具意识的战略推理模型），这是一种增强代理自我意识以优化任务处理和减少工具过度使用的范例。为了支持这一范例，我们引入了SMART-ER，这是一个涵盖三个领域的数据集，其中推理在参数知识和工具依赖步骤之间交替，每个步骤都通过解释何时需要工具的理由进行了丰富。通过监督训练，我们开发了SMARTAgent，这是一系列动态平衡参数知识和工具使用的模型。评估表明，SMARTAgent将工具使用减少了24%，同时将性能提高了37%以上，使得70B规模的模型能够与70B规模的模型和GPT-4o相匹配。此外，SMARTAgent能够泛化到分布外的测试数据，如GSM8K和MINTQA，只需五分之一的工具调用就能保持准确性。这些突出了战略工具使用增强推理、减轻过度使用、弥合模型规模与性能之间差距的潜力，推进了智能和资源高效的代理设计。|
|**2025-02-17**|**\textsc{FLAG-Trader}: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading**|Guojun Xiong et.al.|[2502.11433](http://arxiv.org/abs/2502.11433)|null|大型语言模型（LLMs）在多模态金融数据上进行微调后，在多种金融任务中展现出了令人印象深刻的推理能力。然而，它们在交互式金融市场中的多步骤、目标导向场景，如交易中，往往难以应对，因为这些场景需要复杂的代理方法来提高决策能力。为了解决这个问题，我们提出了\textsc{FLAG-Trader}，这是一种统一架构，它将语言处理（通过LLMs）与梯度驱动的强化学习（RL）策略优化相结合。在这个架构中，部分微调的LLM充当策略网络，利用预训练知识的同时，通过参数高效的微调来适应金融领域。通过由交易奖励驱动的策略梯度优化，我们的框架不仅提升了LLMs在交易中的性能，还改善了其他金融领域任务的结果。我们提供了大量的实证证据来验证这些提升。|
|**2025-02-17**|**TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents**|Geon Lee et.al.|[2502.11418](http://arxiv.org/abs/2502.11418)|null|时间序列数据在各种应用中至关重要，包括气候建模、医疗监控和金融分析。理解与真实世界时间序列数据相关的上下文信息通常对于准确可靠的事件预测至关重要。在本文中，我们介绍了TimeCAP，这是一个时间序列处理框架，创造性地利用大型语言模型（LLMs）作为时间序列数据的上下文解释器，扩展了它们作为预测器的典型用法。TimeCAP包含两个独立的LLM代理：一个生成一个文本摘要，捕捉时间序列的上下文，另一个使用这个丰富的摘要来进行更明智的预测。此外，TimeCAP采用一种多模态编码器，与LLM代理协同工作，通过在上下文中添加示例来相互增强输入，从而提高预测性能。在真实世界数据集上的实验结果表明，TimeCAP在时间序列事件预测方面优于最先进的方法，包括那些利用LLM作为预测器的方法，F1分数平均提高了28.75%。|
|**2025-02-17**|**Connecting Large Language Model Agent to High Performance Computing Resource**|Heng Ma et.al.|[2502.12280](http://arxiv.org/abs/2502.12280)|null|大型语言模型代理工作流程允许LLM调用工具函数，以提升特定科学领域问题的性能。为了应对大规模科学研究的挑战，需要访问计算资源和并行计算设置。在本工作中，我们将Parsl集成到LangChain/LangGraph工具调用设置中，以弥合LLM代理与计算资源之间的差距。我们设置了两种工具调用实现，并在Polaris/ALCF的本地工作站和高性能计算（HPC）环境中进行了测试。第一种实现通过Parsl启用的LangChain工具节点将工具函数并发地排队到Parsl工作器进行并行执行。第二种配置通过将工具函数转换为Parsl集合函数来实现，更适合在超级计算机环境中处理大任务。LLM代理工作流程被提示运行分子动力学模拟，涉及不同的蛋白质结构和模拟条件。这些结果表明，LLM代理工具在可用计算资源上通过Parsl进行了管理和并发执行。|
|**2025-02-14**|**Process Reward Models for LLM Agents: Practical Framework and Directions**|Sanjiban Choudhury et.al.|[2502.10325](http://arxiv.org/abs/2502.10325)|**[link](https://github.com/sanjibanc/agent_prm)**|我们引入了代理过程奖励模型（AgentPRM），这是一个简单且可扩展的框架，用于训练LLM代理通过交互不断改进。AgentPRM遵循轻量级的演员-评论家范式，使用蒙特卡洛滚动来计算奖励目标和优化策略。它对现有RLHF管道的修改最小，易于大规模集成。除了AgentPRM之外，我们还提出了逆PRM，它可以直接从演示中学习过程奖励，而不需要显式的结果监督。我们还探讨了关键挑战和机遇，包括探索、过程奖励塑造和模型预测推理。我们在ALFWorld基准上进行评估，表明使用AgentPRM和逆PRM训练的小型3B模型优于强大的GPT-4o基线，并分析了测试时间扩展、奖励黑客攻击等问题。我们的代码可在以下网址获取：https://github.com/sanjibanc/agent_prm。|
|**2025-02-14**|**Automated Hypothesis Validation with Agentic Sequential Falsifications**|Kexin Huang et.al.|[2502.09858](http://arxiv.org/abs/2502.09858)|**[link](https://github.com/snap-stanford/POPPER)**|假设在信息获取、决策和发现中起着核心作用。然而，许多现实世界的假设是抽象的、高级别的陈述，难以直接验证。随着从大型语言模型（LLMs）生成假设的兴起，这一挑战进一步加剧，因为LLMs容易产生幻觉，并产生大量假设，使得手动验证变得不切实际。在这里，我们提出了Popper，这是一个用于严格自动化验证自由形式假设的代理框架。在卡尔·波普尔的可证伪性原则的指导下，Popper使用LLM代理来设计和执行针对其可测量影响的证伪实验。一个新颖的顺序测试框架确保了严格的I类错误控制，同时积极从多样化的观察中收集证据，这些观察可能来自现有数据或新进行的程序。我们在包括生物学、经济学和社会学在内的六个领域展示了Popper。Popper提供了稳健的错误控制、高效率和可扩展性。此外，与人类科学家相比，Popper在验证复杂的生物学假设方面实现了相当的性能，同时将时间缩短了10倍，为假设验证提供了一个可扩展、严格的解决方案。|
|**2025-02-13**|**RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage**|Peter Yong Zhong et.al.|[2502.08966](http://arxiv.org/abs/2502.08966)|null|基于工具的智能体系统（TBAS）允许语言模型（LM）利用外部工具执行超越其独立能力的工作，例如搜索网站、预订航班或进行金融交易。然而，这些工具大大增加了提示注入攻击的风险，恶意内容会劫持LM智能体泄露机密数据或触发有害行为。现有的防御措施（如OpenAI GPTs）需要在每次工具调用前要求用户确认，给用户带来了沉重的负担。我们介绍了鲁棒TBAS（RTBAS），它能够自动检测和执行保持完整性和保密性的工具调用，仅在无法确保这些保障措施时才需要用户确认。RTBAS将信息流控制应用于TBAS所提出的独特挑战。我们提出了两种新颖的依赖筛选器，使用LM作为法官和基于注意力的显著性，以克服这些挑战。在AgentDojo提示注入基准测试上的实验结果表明，RTBAS在遭受攻击时仅损失了2%的任务效用，能够防止所有针对性强攻击，进一步测试证实了其在检测微妙和直接隐私泄露方面的接近Oracle性能。|
|**2025-02-13**|**AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration**|Jizhou Chen et.al.|[2502.09809](http://arxiv.org/abs/2502.09809)|null|将工具使用整合到大型语言模型（LLMs）中，能够使具有现实影响的代理系统成为可能。与此同时，与独立的LLMs不同，被入侵的代理可以执行具有更严重后果的恶意工作流程，这体现在它们的工具使用能力上。我们提出了AgentGuard，这是一个框架，它可以自主地发现和验证不安全的工具使用工作流程，随后生成安全约束以限制代理的行为，从而在部署时实现安全保证的基本要求。AgentGuard利用LLM编排器的内在能力——对工具功能的了解、可扩展且真实的流程生成以及工具执行权限——来充当其自身的安全评估器。该框架通过四个阶段运行：识别不安全的工作流程、在现实世界执行中验证它们、生成安全约束以及验证约束的有效性。输出结果是一个包含不安全工作流程、测试用例和验证约束的评估报告，这使得多种安全应用成为可能。我们通过实验实证展示了AgentGuard的可行性。通过这项探索性工作，我们希望启发为LLM代理建立标准化的测试和加固程序，以增强其在现实应用中的可信度。|
|**2025-02-12**|**SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent**|Keyeun Lee et.al.|[2502.08599](http://arxiv.org/abs/2502.08599)|**[link](https://github.com/keyeun/spectrum-framework-llm)**|现有的模拟个体身份的方法往往过于简化人类复杂性，这可能导致不完整或平面的表现。为了解决这个问题，我们引入了SPeCtrum，这是一个基于个人多维自我概念的构建真实LLM代理角色的框架。SPeCtrum集成了三个核心组件：社会身份（S）、个人身份（P）和个人生活背景（C），每个组件都贡献了身份的独特但相互关联的方面。为了评估SPeCtrum在身份表现方面的有效性，我们进行了自动和人工评估。使用流行戏剧角色的自动评估显示，从关于偏好和日常生活的短文中提取的个人生活背景（C）模型的角色身份比单独的社会身份（S）和个人身份（P）更有效，并且与完整的SPC组合表现相当。相比之下，涉及现实世界中个人的手工评估发现，完整的SPC组合比单独的C提供了更全面的自概念表现。我们的研究结果表明，虽然C单独可能足以进行基本的身份模拟，但整合S、P和C可以增强现实世界身份表现的真实性和准确性。总的来说，SPeCtrum为在LLM代理中模拟个体提供了一种结构化的方法，使个性化的人机交互成为可能，并提高了基于模拟的行为研究的现实感。|
|**2025-02-12**|**Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks**|Ang Li et.al.|[2502.08586](http://arxiv.org/abs/2502.08586)|null|近年来，大量机器学习安全文献聚焦于针对对齐的大型语言模型（LLM）的攻击。这些攻击可能提取私人信息或迫使模型生成有害输出。在实际部署中，LLM通常作为更大代理管道的一部分，包括记忆系统、检索、网络访问和API调用。这些附加组件引入了漏洞，使得这些由LLM驱动的代理比孤立的LLM更容易受到攻击，但相对而言，关于LLM代理安全的研究工作相对较少。在本文中，我们分析了LLM代理特有的安全和隐私漏洞。我们首先提供了一个按威胁行为者、目标、入口点、攻击者可观察性、攻击策略和代理管道固有漏洞分类的攻击分类法。然后，我们对流行的开源和商业代理进行了一系列说明性攻击，展示了其漏洞的即时实际影响。值得注意的是，我们的攻击易于实施，且无需了解机器学习。|
|**2025-02-12**|**If Multi-Agent Debate is the Answer, What is the Question?**|Hangfan Zhang et.al.|[2502.08788](http://arxiv.org/abs/2502.08788)|null|多智能体辩论（MAD）已成为一种有望提升大型语言模型（LLMs）事实准确性和推理质量的方法，通过在推理过程中让多个智能体进行迭代讨论。尽管如此，我们认为当前MAD研究在评估实践中存在重大不足，包括数据集重叠度有限和基线不一致，这引发了关于泛化能力的重大担忧。相应地，本文系统地评估了五种代表性MAD方法在九个基准测试中使用四种基础模型的情况。令人惊讶的是，我们的发现表明，MAD方法在消耗额外的推理时间计算量后，也未能可靠地优于简单的单智能体基线，如思维链和自洽性。从我们的分析中，我们发现模型异质性可以显著提高MAD框架。我们提出了Heter-MAD，它允许单个LLM智能体访问异构基础模型的输出，从而提高了当前MAD框架的性能。最后，我们概述了推进MAD的潜在方向，旨在激发更广泛的讨论并激发该领域未来的研究工作。|
|**2025-02-11**|**MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces**|Loris Gaven et.al.|[2502.07709](http://arxiv.org/abs/2502.07709)|**[link](https://github.com/LorisGaven/MAGELLAN)**|**开放式学习智能体必须在广阔的可能性空间中高效地优先考虑目标，专注于那些最大化学习进步（LP）的目标。当通过使用在线强化学习训练的LLM智能体在高维和不断发展的目标空间中实现这种自足的探索时，LP预测的一个关键挑战是建模自身的竞争力，这是一种元认知监控的形式。传统的做法要么需要大量采样，要么依赖于脆弱的专家定义的目标分组。我们引入了MAGELLAN，这是一个元认知框架，允许LLM智能体在线学习预测其竞争力。通过捕捉目标之间的语义关系，MAGELLAN能够通过泛化实现高效的LP估计和动态适应不断变化的目标空间。在一个交互式学习环境中，我们表明MAGELLAN提高了LP预测效率和目标优先级，是唯一允许智能体完全掌握一个庞大且不断变化的目标空间的方法。这些结果表明，通过为LLM智能体增加LP预测的元认知能力，可以有效地将课程学习扩展到开放式目标空间。**|
|**2025-02-11**|**Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames**|Vince Trencsenyi et.al.|[2502.07443](http://arxiv.org/abs/2502.07443)|null|基于LLM的多智能体模拟在博弈论和社会模拟中的应用越来越受到关注。尽管大多数实现旨在利用或评估LLM的智能推理，但它们通常在智能概念和架构上过于简化。我们实现了一个基于角色的多智能体战略交互框架，专为复杂的递归推理者量身定制，提供了系统深入开发和发展战略推理的手段。我们的游戏环境由裁判负责，从匹配到移动验证再到环境管理。玩家在决策机制中采用最先进的LLM，依靠基于形式超博弈的分层信念模型。我们使用一次性、两人美的竞赛来评估最新LLM的递归推理能力，并将其与经济学中一个已建立的基线模型以及人类实验数据进行了比较。此外，我们向k级理论引入了推理的另一种语义度量基础。我们的实验表明，人工推理者不仅在近似人类行为方面，而且在达到最优解方面都能优于基线模型。|
|**2025-02-11**|**Graph RAG-Tool Fusion**|Elias Lumer et.al.|[2502.07223](http://arxiv.org/abs/2502.07223)|**[link](https://github.com/eliaslumer/graph-rag-tool-fusion-toollinkos)**|**最近在检索增强生成（RAG）领域的发展，使得从工具知识库中选择相关工具成为可能，这使大型语言模型（LLM）能够将复杂的工具调用能力扩展到数百或数千个外部工具、API或作为工具的代理。然而，传统的基于RAG的工具检索未能捕捉工具之间的结构化依赖关系，限制了检索到的工具依赖关系的准确性。例如，在一个工具向量数据库中，“获取股票价格”API需要从“获取股票标识符”API中获取“股票标识符”参数，而这两种API都依赖于操作系统级别的互联网连接工具。在本文中，我们通过引入图RAG-工具融合，一种新颖的即插即用方法来解决这一局限性，该方法结合了基于向量的检索和高效的图遍历，以捕获预定义工具知识图中的所有相关工具（节点）以及任何嵌套依赖关系（边）。我们还提出了ToolLinkOS，这是一个包含573个虚构工具的新工具选择基准，涵盖15个行业，每个行业平均有6.3个工具依赖关系。我们证明了图RAG-工具融合在ToolLinkOS和ToolSandbox基准上分别实现了相对于朴素RAG的71.7%和22.1%的绝对改进（mAP@10）。ToolLinkOS数据集可在https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS获取。**|
|**2025-02-11**|**Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification**|Peipei Wei et.al.|[2502.07165](http://arxiv.org/abs/2502.07165)|null|我们提出了一种基于原则的提示策略，名为PRINCIPLE-BASED PROMPTING，这是一种简单但有效的多智能体提示策略，用于文本分类。该策略首先让多个LLM智能体独立地根据带有或没有标签的演示样本分析生成候选原则，然后通过一个终结者智能体将它们整合成最终原则，接着将它们发送给分类智能体以执行下游分类任务。在具有不同LLM大小的二分类和多分类数据集上进行的广泛实验表明，我们的方法不仅在宏观F1分数上实现了显著的性能提升（1.55% - 19.37%），而且在零样本提示的基础上也优于其他强大基线（CoT和回退提示）。在我们的方法生成的原则帮助下，LLM在两个私有数据集上的分类任务表现优于人工制作的原则。我们的多智能体PRINCIPLE-BASED PROMPTING方法在性能上与基于演示的少样本提示方法相当或更好，同时具有显著降低的推理成本。消融研究表明，标签信息和多智能体合作的LLM框架在生成高质量原则以促进下游分类任务方面发挥着重要作用。|
|**2025-02-11**|**Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs**|Ruichen Zhang et.al.|[2502.07942](http://arxiv.org/abs/2502.07942)|null|基于大型语言模型（LLMs）的网页浏览代理在自动化复杂网络任务方面展现出巨大潜力。现有方法通常依赖于大型LLMs（例如，GPT-4o）来探索网络环境并生成轨迹数据，这些数据随后用于演示检索（针对大型LLMs）或蒸馏小型LLMs（例如，Llama3），这个过程与探索是解耦的。在本文中，我们提出了AgentSymbiotic，一个将数据合成与任务性能结合的迭代框架，为大型和小型LLMs都带来了“共生改进”。我们的研究揭示了LLM类型之间的互补动态：大型LLMs擅长生成高质量的轨迹以进行蒸馏，而由于它们独特的推理能力，蒸馏后的小型LLMs通常会选择与大型LLMs不同的行动。这种差异推动了新轨迹的探索，从而丰富了合成数据。然而，我们也观察到小型LLMs的性能成为迭代增强过程中的瓶颈。为了解决这个问题，我们提出了LLM蒸馏的两个创新：一种推测性数据合成策略，以减轻离策略偏差；以及一种旨在提升学生LLM推理能力的多任务学习方法。此外，我们引入了混合模式以保护用户隐私。在WEBARENA基准测试中，AgentSymbiotic在两种LLM类型上都实现了SOTA性能。我们最佳的大型LLM代理达到了52%，超过了之前的45%的最佳成绩，而我们的80亿参数蒸馏模型表现出了有竞争力的49%，超过了之前的28%的最佳成绩。代码将在接受后发布。|
|**2025-02-10**|**Visual Agentic AI for Spatial Reasoning with a Dynamic API**|Damiano Marsili et.al.|[2502.06787](http://arxiv.org/abs/2502.06787)|null|视觉推理——即解读视觉世界的能力——对于在三维场景中操作的具身智能体至关重要。人工智能的进步使得能够从图像中回答问题的视觉和语言模型成为可能。然而，当需要执行3D空间推理任务时，它们的性能会下降。为了解决这类推理问题的复杂性，我们提出了一种代理程序合成方法，其中LLM代理共同生成一个Pythonic API，并添加新的功能来解决常见的子问题。我们的方法克服了依赖于静态、人类定义的API的先前方法的局限性，使其能够处理更广泛的查询。为了评估人工智能在3D理解方面的能力，我们引入了一个新的基准，其中包含涉及多个步骤的地面和推理的查询。我们表明，我们的方法在3D视觉推理方面优于先前的零样本模型，并通过实证验证了我们的代理框架在3D空间推理任务中的有效性。项目网站：https://glab-caltech.github.io/vadar/|
|**2025-02-10**|**Towards Internet-Scale Training For Agents**|Brandon Trabucco et.al.|[2502.06776](http://arxiv.org/abs/2502.06776)|null|主要训练网络导航代理的方法是通过收集一组热门网站和手写任务的人类演示，但越来越明显的是，人类数据是一个效率低下的资源。我们开发了一个流程，以简化代理进行互联网规模训练，而不需要繁琐的人类标注。在第一阶段，一个大型语言模型（LLM）为15万个不同的网站生成任务。在第二阶段，LLM代理完成任务并生成轨迹。在最后阶段，一个LLM审查轨迹并判断其成功率。语言模型在人类标注员中具有竞争力，检测和过滤有害内容的准确率为97%，生成可行任务的比率为89%，判断成功轨迹的准确率为82.6%。扩展流程后，基于Llama 3.1 70B的代理解决了15万个网站中16.7%的任务。在由我们的流程生成的数据上训练与在人类演示上训练具有竞争力。在由Mind2Web和WebLINX派生出的数据有限的环境中，对于训练在来自我们的流程和人类数据混合的数据上的代理，我们分别提高了+89.5%和+122.1%的步骤准确率。当使用这些基准中所有可用的人类数据训练代理时，代理无法推广到多样化的真实网站上，而添加我们的数据提高了它们的推广能力，WebLINX提高了+149.0%，Mind2Web提高了+156.3%。代码将在：data-for-agents.github.io上提供。|
|**2025-02-10**|**Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training**|Yuchen Zhuang et.al.|[2502.06589](http://arxiv.org/abs/2502.06589)|null|由于面向代理的预训练数据稀缺，基于LLM的自主代理通常依赖复杂的提示或广泛的微调，这往往无法引入新能力同时保持强大的泛化能力。我们引入了Hephaestus-Forge，这是第一个旨在增强LLM代理在API函数调用、内在推理和规划以及适应环境反馈的基本能力的大规模预训练语料库。Hephaestus-Forge包含103B的代理特定数据，涵盖了76,537个API，包括工具文档来介绍API函数知识以及函数调用轨迹来加强内在推理。为了探索有效的训练协议，我们研究了扩展定律以确定数据混合比例的最佳配方。通过在Hephaestus-Forge上的持续预训练，Hephaestus在三个代理基准测试中优于小型到中型规模的开放源代码LLM，并可与商业LLM相媲美，证明了我们的预训练语料库在增强基本代理能力和LLM对新任务或环境的泛化方面的有效性。|
|**2025-02-10**|**CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories**|Yijia Xiao et.al.|[2502.06111](http://arxiv.org/abs/2502.06111)|null|随着计算机科学研究项目的日益复杂，对部署代码库的有效工具的需求也在增加。大型语言模型（LLMs），如Anthropic Claude和Meta Llama，在各种计算机科学研究领域取得了显著的进步，包括自动化各种软件工程任务。为了评估LLMs在处理研究项目复杂代码开发任务中的有效性，特别是针对NLP/CV/AI/ML/DM主题，我们引入了CSR-Bench，这是一个针对计算机科学研究项目的基准。该基准从准确性、效率和部署脚本质量等多个方面评估LLMs，旨在探索它们在自主进行计算机科学研究中的潜力。我们还介绍了一个新颖的框架，CSR-Agents，该框架利用多个LLM代理来自动化计算机科学研究项目的GitHub代码库的部署。具体来说，通过检查markdown文件中的指令并解释仓库结构，模型生成并迭代改进bash命令，以设置实验环境并将代码部署以进行研究任务。CSR-Bench的初步结果表明，LLM代理可以显著提高代码库部署的工作流程，从而提高开发者的生产力和改善开发工作流程的管理。|
|**2025-02-10**|**Interactive Data Harmonization with LLM Agents**|Aécio Santos et.al.|[2502.07132](http://arxiv.org/abs/2502.07132)|null|数据协调是将来自不同来源的数据集整合的重要任务。尽管在该领域已有多年的研究，但由于模式不匹配、术语差异和数据收集方法的不同，它仍然是一项耗时且具有挑战性的任务。本文提出了基于代理的数据协调作为一种既能赋权专家协调其数据又能简化流程的方法。我们介绍了Harmonia系统，该系统结合了基于大型语言模型（LLM）的推理、交互式用户界面和数据协调原语库，以自动化数据协调管道的合成。我们在临床数据协调场景中展示了Harmonia，它有助于交互式地创建可重用的管道，将数据集映射到标准格式。最后，我们讨论了挑战和开放性问题，并提出了推进我们愿景的研究方向。|
|**2025-02-10**|**Repository-level Code Search with Neural Retrieval Methods**|Siddharth Gandhi et.al.|[2502.07067](http://arxiv.org/abs/2502.07067)|**[link](https://github.com/Siddharth-Gandhi/ds)**|本文提出了一种用于代码库级代码搜索的多阶段重排序系统，该系统利用大型开源代码库中大量可用的提交历史来辅助修复错误。我们将代码库级代码搜索的任务定义为从代码库的当前状态中检索出与解决用户问题或错误最相关的文件集。所提出的方法结合了基于BM25的提交消息检索和CodeBERT的神经重排序，以识别最相关的文件。通过从不同代码库及其提交历史中学习模式，该系统可以揭示与当前任务相关的文件。该系统利用提交消息和源代码进行相关性匹配，并在正常和神谕设置中进行评估。在从7个流行的开源代码库创建的新数据集上进行的实验表明，在MAP、MRR和P@1方面，相对于BM25基线，在多样化的查询中实现了高达80%的显著改进，证明了该方法的有效性。我们希望这项工作能够帮助LLM代理作为更好的代码搜索和理解的工具。我们的代码和获得的结果是公开可用的。|
|**2025-02-10**|**SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering**|Xuehang Guo et.al.|[2502.06994](http://arxiv.org/abs/2502.06994)|null|软件工程（SE）正变得越来越协作，开发者们共同在共享的复杂代码库上工作。在共享环境中进行有效协作需要参与者——无论是人类还是AI代理——随着环境的变化保持一致。当合作者的理解与当前状态不符——我们称之为不同步挑战——合作者的行动可能会失败，导致集成问题。在这项工作中，我们引入了SyncMind，这是一个框架，它系统地定义了在协作软件工程（CSE）中大型语言模型（LLM）代理面临的不同步问题。基于SyncMind，我们创建了SyncBench，这是一个包含来自21个流行的GitHub存储库的24,332个代理不同步场景的基准，这些场景来自实际的CSE，并具有可执行的验证测试。在SyncBench上的实验揭示了现有LLM代理的能力和局限性。除了代理之间存在的实质性性能差距（从Llama-3.1代理<= 3.33%到Claude-3.5-Sonnet>= 28.18%）之外，它们持续的低协作意愿（<= 4.86%）表明了现有LLM在CSE中的根本局限性。然而，当发生协作时，它与不同步恢复的成功呈正相关。代理在资源感知不同步恢复中的最小性能差异进一步揭示了它们对资源感知和适应性的显著缺乏，为未来资源高效的协作系统提供了启示。代码和数据在我们的项目网站上公开：https://xhguo7.github.io/SyncMind/。|
|**2025-02-10**|**Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents**|Mathis Pink et.al.|[2502.06975](http://arxiv.org/abs/2502.06975)|null|随着大型语言模型（LLMs）从文本补全工具发展成为在动态环境中运行的完整代理，它们必须应对持续学习和保留长期知识挑战。许多生物系统通过情景记忆来解决这些挑战，支持实例特定情境的单次学习。受此启发，我们提出了一种针对LLM代理的情景记忆框架，该框架围绕情景记忆五个关键特性展开，这些特性是适应性和情境敏感性行为的基础。由于已有各种研究努力部分覆盖了这些特性，本文认为现在是明确、集中关注情景记忆以促进长期代理发展的正确时机。为此，我们概述了一个路线图，将几个研究方向联合起来，旨在支持情景记忆的所有五个特性，以实现更有效的长期LLM代理。|
|**2025-02-09**|**HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents**|Mohammad Amin Abbasi et.al.|[2502.05982](http://arxiv.org/abs/2502.05982)|null|本文介绍了一个名为HamRaz的新型波斯语心理健康数据集，该数据集旨在用于以人为中心的疗法（PCT）和大型语言模型（LLMs）。尽管LLMs在AI驱动的心理咨询服务中的应用日益增长，但现有的数据集主要关注西方和东亚语境，忽略了对于有效波斯语疗法至关重要的文化和语言细微差别。为了填补这一空白，HamRaz结合了基于脚本的对话和自适应LLM角色扮演，确保了连贯和动态的治疗互动。我们还引入了HamRazEval，这是一个双重评估框架，它使用通用对话指标和Barrett-Lennard关系量表（BLRI）来衡量对话质量和治疗效果。实验结果表明，HamRaz优于传统的脚本模式和双代理模式，产生了更具同理心、情境感知和现实的治疗会话。通过发布HamRaz，我们为推进多元社区中AI驱动的心理治疗研究贡献了一个文化适应的、LLM驱动的资源。|
|**2025-02-09**|**MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents**|Jiabin Tang et.al.|[2502.05957](http://arxiv.org/abs/2502.05957)|**[link](https://github.com/hkuds/autoagent)**|大型语言模型（LLM）智能体在任务自动化和智能决策方面展现出卓越的能力，推动了LangChain和AutoGen等智能体开发框架的广泛应用。然而，这些框架主要服务于具有丰富技术专长的开发者——考虑到全球只有0.03%的人口拥有必要的编程技能，这是一个重大的限制。这种明显的可访问性差距提出了一个基本问题：我们能否让每个人，无论技术背景如何，仅使用自然语言就能构建自己的LLM智能体？为了应对这一挑战，我们引入了MetaChain——一个完全自动化和高度自我发展的框架，使用户能够仅通过自然语言创建和部署LLM智能体。作为自主智能体操作系统，MetaChain包含四个关键组件：i）智能体系统工具，ii）LLM驱动的可执行引擎，iii）自我管理的文件系统，和iv）自我玩耍智能体定制模块。这个轻量级但强大的系统使得无需编码要求或手动干预即可高效且动态地创建和修改工具、智能体和工作流程。除了其无代码的智能体开发能力外，MetaChain还充当一个多智能体系统，用于通用人工智能助手。在GAIA基准上的全面评估表明，MetaChain在通用多智能体任务中的有效性超过了现有的最先进方法。此外，MetaChain在检索增强生成（RAG）相关能力方面的表现也持续优于许多基于LLM的替代解决方案。|
|**2025-02-07**|**MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison**|Kaijie Zhu et.al.|[2502.05174](http://arxiv.org/abs/2502.05174)|**[link](https://github.com/kaijiezhu11/melon)**|近期研究探讨了LLM代理容易受到间接提示注入（IPI）攻击的脆弱性，恶意任务嵌入在工具检索信息中可以引导代理执行未经授权的操作。现有的IPI防御方法存在重大局限性：要么需要必要的模型训练资源，要么对复杂攻击缺乏有效性，或者损害正常功能。我们提出了MELON（掩码重执行和工具比较），一种新的IPI防御方法。我们的方法基于观察：在成功攻击下，代理的下一步操作对用户任务的依赖性降低，而对恶意任务的依赖性增加。在此基础上，我们设计MELON通过使用通过掩码函数修改的掩码用户提示重新执行代理的轨迹来检测攻击。如果原始执行和掩码执行中生成的操作相似，则我们识别出攻击。我们还包含了三个关键设计来减少潜在的误报和漏报。在IPI基准AgentDojo上的大量评估表明，MELON在攻击预防和功能保持方面优于最先进的防御方法。此外，我们还展示了将MELON与最先进的提示增强防御方法（表示为MELON-Aug）相结合可以进一步提高其性能。我们还进行了一项详细的消融研究，以验证我们的关键设计。|
|**2025-02-07**|**Self-Regulation and Requesting Interventions**|So Yeon Min et.al.|[2502.04576](http://arxiv.org/abs/2502.04576)|null|人类智能包括元认知能力，如自我调节、认识到局限性和在必要时寻求帮助。虽然LLM代理在许多领域表现出色，但它们往往缺乏这种意识。过于自信的代理可能导致灾难性失败，而过度寻求帮助的代理则会妨碍效率。一个关键挑战是在有限的干预预算 $C$ 下，决定何时请求帮助。在本文中，我们提出一个离线框架，通过结合基于LLM的过程奖励模型（PRMs）和表格强化学习来训练一个“助手”策略，以请求干预，例如更强大的模型或测试时的计算资源。使用离线收集的状态转换，我们使用PRMs评估最优干预时机，并在这些标记的轨迹上训练助手模型。这种离线方法显著减少了训练过程中的昂贵干预调用。此外，将PRMs与表格强化学习相结合，增强了对抗离策略数据的鲁棒性，同时避免了深度强化学习的低效性。我们通过实证发现，我们的方法能够实现最佳的助手行为。|
|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306](http://arxiv.org/abs/2502.04306)|**[link](https://github.com/gen-verse/scoreflow)**|最近的研究利用大型语言模型多智能体系统解决复杂问题，同时试图减少构建它们的手动工作量，推动了自动化智能体工作流程优化方法的发展。然而，由于表示限制、缺乏适应性和在依赖离散优化技术时的扩展性较差，现有方法仍然不够灵活。我们通过ScoreFlow解决了这些挑战，这是一个简单但性能高效的框架，它在一个连续空间中利用了高效的基于梯度的优化。ScoreFlow集成了Score-DPO，这是一种考虑了定量反馈的直接偏好优化方法的新变体。在涵盖问答、编码和数学推理的六个基准测试中，ScoreFlow相较于现有基线实现了8.2%的提升。此外，它还使小模型能够在较低的推理成本下超越大模型。项目：https://github.com/Gen-Verse/ScoreFlow|
|**2025-02-06**|**PsyPlay: Personality-Infused Role-Playing Conversational Agents**|Tao Yang et.al.|[2502.03821](http://arxiv.org/abs/2502.03821)|null|当前对大型语言模型（LLMs）驱动的角色扮演对话代理（RPCAs）的研究主要集中于模仿特定的说话风格和利用角色背景，忽视了更深层次性格特征的描绘。在本研究中，我们引入了为LLM代理注入性格的角色扮演，鼓励代理在对话中准确展现其指定的性格特征。然后，我们提出了PsyPlay，一个促进多个LLM代理表达丰富性格的对话生成框架。具体来说，PsyPlay使代理能够扮演具有不同性格特征的角色，并围绕特定主题进行讨论，在整个互动中持续展现其指定的性格特征。在生成的对话数据上的验证表明，PsyPlay可以准确描绘预期的性格特征，在GPT-3.5上实现了80.31%的整体成功率。值得注意的是，我们发现与积极价值观一致的LLM在表现积极性格角色方面比消极角色更为成功。此外，我们构建了一个名为PsyPlay-Bench的对话语料库，用于性格注入的角色扮演。该语料库由4745个使用PsyPlay正确描绘的对话实例组成，旨在进一步促进个性化角色扮演和对话性格检测的研究。|
|**2025-02-06**|**MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers**|Nicole Cho et.al.|[2502.03711](http://arxiv.org/abs/2502.03711)|null|在大型语言模型（LLMs）的机构采用过程中，一个关键的挑战来自于它们在生成响应时倾向于产生幻觉。为了解决这个问题，我们提出了MultiQ&A，这是一种用于评估LLM生成答案的鲁棒性和一致性的系统性方法。我们展示了MultiQ&A通过独立LLM代理大规模地众包问题扰动及其相应答案的能力。我们的实验最终检验了190万个问题扰动和230万个答案。此外，MultiQ&A表明，如gpt-3.5-turbo之类的集成LLM在扰动下仍然相对鲁棒和一致。MultiQ&A在响应生成领域提供了清晰的见解，提供了一种有效的方法来检查分歧和变化。因此，我们的系统为机构LLM的采用提供了一个潜在的框架，能够衡量信心、一致性和幻觉的量化。|
|**2025-02-06**|**Multi-Agent Reinforcement Learning with Focal Diversity Optimization**|Selim Furkan Tekin et.al.|[2502.04492](http://arxiv.org/abs/2502.04492)|**[link](https://github.com/sftekin/rl-focal)**|本文介绍了名为MARL-Focal的焦点多样性优化多智能体强化学习方法，该方法具有三个独特特点。首先，我们开发了一个智能体融合框架，以鼓励多个基于大型语言模型（LLM）的智能体在为每个LLM查询生成最终推理输出时进行协作。其次，我们开发了一种焦点多样性优化智能体选择算法，该算法可以根据智能体相互补充生成查询输出的能力，从可用智能体中选择一个小子集。最后，我们设计了一种冲突解决方法，用于检测多个智能体之间的输出不一致性，并通过奖励感知和政策自适应的推理融合来生成我们的MARL-Focal输出。在五个基准上的广泛评估表明，MARL-Focal既经济高效，又具有对抗鲁棒性。我们的多智能体融合模型与最佳单个LLM智能体相比，性能提升了5.51%，在TruthfulQA基准上提供了更强的鲁棒性。代码可在https://github.com/sftekin/rl-focal上获取。|
|**2025-02-06**|**Active Task Disambiguation with LLMs**|Katarzyna Kobalczyk et.al.|[2502.04485](http://arxiv.org/abs/2502.04485)|**[link](https://github.com/kasia-kobalczyk/active-task-disambiguation)**|尽管大型语言模型（LLMs）在各种基准测试中表现出色，但它们处理模糊指定问题（这在现实世界互动中很常见）的能力仍未得到充分探索。为了填补这一空白，我们提出了任务模糊性的正式定义，并通过贝叶斯实验设计的视角来阐述任务去模糊化问题。通过提出澄清问题，LLM代理可以获取额外的任务规范，逐步缩小可行解决方案的空间，并降低生成不满意输出的风险。然而，生成有效的澄清问题需要LLM代理进行一种元认知推理，这是LLMs目前可能缺乏的能力。我们提出的主动任务去模糊化方法使LLM代理能够生成针对性强的问题，最大化信息增益。实际上，这种方法将负担从隐式推理转移到对可行解决方案空间的显式推理。实证结果表明，与仅依靠问题空间内推理的方法相比，这种问题选择形式导致了更有效的任务去模糊化。|
|**2025-02-05**|**A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)**|Yiye Chen et.al.|[2502.03450](http://arxiv.org/abs/2502.03450)|null|场景图已成为用于大型语言模型（LLMs）进行基于真实空间的推理的结构化和可序列化环境表示。在本研究中，我们提出了SG-RwR，一个基于模式引导的检索-推理框架，用于场景图中的推理和规划。我们的方法采用两个协同的代码编写型LLM智能体：（1）推理智能体用于任务规划和信息查询生成；（2）检索智能体用于根据查询提取相应的图信息。两个智能体迭代协作，实现顺序推理和对图信息的自适应关注。与先前的研究不同，两个智能体仅以场景图模式进行提示，而不是完整的图数据，这通过限制输入令牌来减少幻觉，并促使推理智能体抽象地生成推理轨迹。在追踪轨迹后，检索智能体根据模式理解程序化地查询场景图数据，从而实现对图的动态和全局关注，增强了推理和检索之间的对齐。通过在多个仿真环境中的实验，我们表明我们的框架在数值问答和规划任务方面优于现有的基于LLM的方法，并且可以从任务级别的少量示例中受益，即使在缺乏智能体级别演示的情况下。项目代码将发布。|
|**2025-02-04**|**Adaptive Self-improvement LLM Agentic System for ML Library Development**|Genghan Zhang et.al.|[2502.02534](http://arxiv.org/abs/2502.02534)|**[link](https://github.com/zhang677/pcl-lite)**|**机器学习库通常是用特定架构的编程语言（ASPLs）编写的，这些语言针对特定领域架构，对于高效的机器学习系统至关重要。然而，编写这些高性能的机器学习库具有挑战性，因为它需要机器学习算法和ASPL的专家知识。另一方面，大型语言模型（LLMs）已显示出通用的编码能力。然而，使用LLMs通过ASPLs生成机器学习库仍存在挑战，因为1）即使对于经验丰富的程序员来说，这个任务也相当复杂，2）由于ASPLs的晦涩和不断发展，可用的代码示例有限。因此，LLMs需要有限的 数据进行复杂推理以完成此任务。为了解决这些挑战，我们引入了一个自适应自我改进的代理系统。为了评估我们系统的有效性，我们在典型机器学习库上构建了一个基准，并使用开源和闭源LLMs在这个基准上生成ASPL代码。我们的结果表明，与基线单LLM相比，改进幅度高达 $3.9\times$ 。**|
|**2025-02-04**|**Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives**|Elliot Meyerson et.al.|[2502.04358](http://arxiv.org/abs/2502.04358)|null|将难题分解为子问题通常可以使它们更容易和更高效地解决。随着大型语言模型（LLMs）在越来越多的能力上跨越关键可靠性阈值，越来越多地有人尝试将系统分解为基于LLMs的代理集合，每个代理可以分配子任务。然而，这种分解（即使是自动化的）通常是直观的，例如，基于人类如何为人类团队成员分配角色的方式。这些角色分解与最优解有多接近？这篇立场论文认为，需要对基于LLM原语的渐近分析，以推理此类分解系统的效率，并且此类分析将解锁扩展它们的机会。通过将LLM正向传播视为计算成本的原子单元，可以分离出特定LLM（通常是不透明的）内部工作原理以及一组LLM如何编排以解决难题的固有效率。换句话说，如果我们想要将LLMs的部署扩展到极限，而不是将LLMs拟人化，那么我们应该使用基于LLM原语的渐近分析来推理和开发将大问题分解为LLM代理的更强大的分解方法。|
|**2025-02-03**|**Firewalls to Secure Dynamic LLM Agentic Networks**|Sahar Abdelnabi et.al.|[2502.01822](http://arxiv.org/abs/2502.01822)|null|未来大型语言模型（LLM）的代理很可能在涉及具有相互依赖目标的长期计划的任务中代表用户与其他实体表示的代理进行沟通。当前的研究并未关注此类代理网络，也没有解决其面临的挑战。因此，我们首先确定了代理沟通所需具备的属性，这些属性应该是主动和可适应的。它需要满足以下条件：1）隐私性：代理不应分享超过完成任务所需的信息；2）安全性：沟通必须保持完整性，并维护对自私实体的效用。我们设计了一个用例（旅行规划）作为测试平台，以体现这些要求，并展示了如果处理不当可能出现的问题。接下来，我们提出了一种实用的设计方案，该方案受现有网络安全原则的启发，旨在平衡可适应性、安全性和隐私性。我们的框架从先前的模拟中自动构建和更新特定于任务的规则以构建防火墙。我们提供了多层防御措施，包括：1）将自由形式的输入转换为特定于任务的协议；2）根据任务需求动态抽象用户的权限；3）自我纠正代理的轨迹。|
|**2025-02-03**|**TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues**|Yubin Ge et.al.|[2502.01630](http://arxiv.org/abs/2502.01630)|null|在多轮对话中的时间推理是一个重大挑战，在之前的时间推理基准测试中一直未得到充分研究。为了填补这一空白，我们提出了一种新的多轮对话时间推理评估任务，并介绍了一种通过增强LoCoMo对话并创建多选题问答题（QA）来构建新基准的方法。此外，我们提出了TReMu，一个旨在提高在此背景下LLM代理时间推理能力的框架。具体来说，该框架通过时间感知记忆，即通过时间线总结，在每次对话会话中总结事件及其推断的日期来生成可检索的记忆。此外，我们集成了神经符号时间推理，其中LLMs生成Python代码来进行时间计算和选择答案。在流行LLMs上的实验评估表明，我们的基准具有挑战性，且提出的框架与基线方法相比显著提高了时间推理性能，从GPT-4o的29.83通过标准提示提升到77.67，突出了其在解决多轮对话中的时间推理问题上的有效性。|
|**2025-02-03**|**Reinforcement Learning for Long-Horizon Interactive LLM Agents**|Kevin Chen et.al.|[2502.01600](http://arxiv.org/abs/2502.01600)|null|互动式数字代理（IDAs）利用有状态的数字环境的API来响应用户请求执行任务。虽然由指令微调的大型语言模型（LLMs）驱动的IDAs能够在多步交互中响应用户界面调用的反馈，但它们并未在其各自的数字环境中进行训练。先前的方法在复杂的基准测试（如AppWorld）中仅完成了不到一半的任务。我们提出了一种强化学习（RL）方法，该方法直接在目标环境中训练IDAs。我们将这种训练形式化为部分可观察的马尔可夫决策过程，并推导出LOOP，这是一种数据与内存高效的近端策略优化变体。LOOP不使用价值网络，并且仅在内存中保持底层LLM的一个副本，这使得其实施简单，与微调单个LLM一样内存高效。在AppWorld环境中使用LOOP训练的32亿参数代理比更大的OpenAI o1代理的表现高出9个百分点（相对提高15%）。据我们所知，这是首次报道将RL应用于与有状态的、多领域、多应用环境通过直接API调用交互的IDAs。我们的分析揭示了RL在该领域的有效性，表明代理学会了查阅API文档、避免无根据的假设、最小化虚构和从挫折中恢复。|
|**2025-02-03**|**Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization**|Minttu Alakuijala et.al.|[2502.01562](http://arxiv.org/abs/2502.01562)|null|随着人工智能（AI）代理的一般能力不断发展，它们通过经验学习掌握多个复杂任务的能力仍然是一个关键挑战。当前的大型语言模型（LLM）代理，尤其是基于专有语言模型的代理，通常依赖于提示来融入关于目标任务的知识。这种方法不允许代理内化这些信息，而是依赖于不断扩大的提示来维持其在各种场景下的功能。这类似于患有前额叶遗忘症的人使用的笔记系统，即无法形成新记忆。在这篇论文中，我们提出了一种新颖的方法来训练AI代理，使其能够融入多个任务的知识和技能，而无需使用繁琐的笔记系统或先前的高质量演示数据。我们的方法采用了一种迭代过程，其中代理收集新经验，从人类那里以提示的形式获得纠正反馈，并通过上下文蒸馏训练程序将此反馈整合到其权重中。我们通过在一个基于Llama-3的代理中实现我们的方法，并在仅经过几轮反馈后，在需要正确排序信息检索、工具使用和问题回答的任务集中优于GPT-4o和DeepSeek-V3等高级模型，来证明我们方法的有效性。|
|**2025-02-03**|**TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets**|Yuzhe Yang et.al.|[2502.01506](http://arxiv.org/abs/2502.01506)|**[link](https://github.com/tobyyang7/twinmarket)**|社会涌现的研究长期以来一直是社会科学的核心焦点。传统的建模方法，如基于规则的基于代理的模型（ABM），难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。最近，大型语言模型（LLM）代理作为模拟社会科学和角色扮演应用的工具而受到关注。研究表明，LLM可以解释认知偏差、情绪波动和其他非理性影响，从而实现更逼真的社会经济动态模拟。在本研究中，我们引入了TwinMarket，这是一个利用LLM模拟社会经济系统的创新多代理框架。具体来说，我们考察了个体行为通过交互和反馈机制如何产生集体动态和涌现现象。通过在模拟股票市场环境中的实验，我们展示了个体行为如何引发群体行为，导致涌现结果，如金融泡沫和衰退。我们的方法为个体决策与集体社会经济模式之间的复杂相互作用提供了有价值的见解。|
|**2025-02-03**|**Position: Towards a Responsible LLM-empowered Multi-Agent Systems**|Jinwei Hu et.al.|[2502.01714](http://arxiv.org/abs/2502.01714)|null|随着智能代理AI和基于大型语言模型的多智能体系统（LLM-MAS）的兴起，对负责任和可靠的系统运行的需求日益凸显。像LangChain和检索增强生成这样的工具扩展了LLM的能力，通过增强知识检索和推理，使其能够更深入地融入MAS。然而，这些进步也带来了关键挑战：LLM智能体表现出固有的不可预测性，其输出的不确定性可能在交互过程中累积，威胁到系统稳定性。为了应对这些风险，采用以人为中心的设计方法，并辅以主动动态调节至关重要。这种方法通过促进智能体间的一致沟通和有效的系统治理，增强了传统被动监管，使MAS能够更高效地实现预期目标。|
|**2025-02-03**|**Simulating Rumor Spreading in Social Networks using LLM Agents**|Tianrui Hu et.al.|[2502.01450](http://arxiv.org/abs/2502.01450)|**[link](https://github.com/neerajas-group/rumors-in-multi-agent)**|随着社交媒体的兴起，虚假信息变得越来越普遍，这主要是由谣言的传播所推动。本研究探讨了在一个新颖框架中使用大型语言模型（LLM）代理来模拟和分析谣言在社会网络中的传播动态。为此，我们设计了各种基于LLM的代理类型，并构建了四种不同的网络结构来进行这些模拟。我们的框架评估了不同网络结构和代理行为在影响谣言传播方面的有效性。我们的结果表明，该框架可以模拟超过一百个代理在各种网络中传播谣言。评估表明，网络结构、角色和传播方案可以显著影响谣言的传播，从无传播到在迭代中影响83%的代理，从而为社交网络中谣言的传播提供了一个现实模拟。|
|**2025-02-03**|**Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant**|Gaole He et.al.|[2502.01390](http://arxiv.org/abs/2502.01390)|**[link](https://github.com/richardhgl/chi2025_plan-then-execute_llmagent)**|随着ChatGPT的流行爆炸，大型语言模型（LLMs）持续影响着我们的日常生活。配备了为特定目的设计的工具（例如，航班预订或闹钟），LLM代理展现出越来越强的能力来协助人类完成日常工作。尽管LLM代理作为日常助手展现出有前景的蓝图，但对于它们如何基于规划和顺序决策能力提供日常帮助的理解仍然有限。我们从近期工作中获得灵感，这些工作强调了“LLM-modulo”设置与人类在回路中协同规划任务的价值。我们进行了一项实证研究（N = 248），研究了LLM代理在六种常见任务中的日常助手作用，这些任务通常具有不同水平的风险（例如，航班票务预订和信用卡支付）。为确保用户对LLM代理有自主权和控制权，我们采用了“先规划后执行”的方式，其中代理在模拟环境中进行逐步规划和逐步执行。我们分析了用户在每个阶段参与程度如何影响他们的信任和协作团队表现。我们的发现表明，LLM代理是一把双刃剑——（1）当有高质量的计划和必要的用户执行参与时，它们可以工作得很好；（2）用户很容易对看似合理的计划对LLM代理产生怀疑。我们总结了使用LLM代理作为日常助手以调整用户信任和实现更好的整体任务结果的关键见解。我们的工作对未来日常助手的设计和人类-人工智能与LLM代理的协作具有重要意义。|
|**2025-02-03**|**ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution**|Kanika Goswami et.al.|[2502.00989](http://arxiv.org/abs/2502.00989)|null|大型语言模型（LLMs）能够执行图表问答任务，但常常生成未经验证的幻觉回答。现有的答案归属方法由于视觉语义上下文有限、复杂的视觉文本对齐要求以及复杂布局中边界框预测的困难，难以将回答与源图表联系起来。我们提出了一种名为ChartCitor的多智能体框架，通过在图表图像中识别支持性证据来提供细粒度的边界框引用。该系统协调LLM智能体执行图表到表格的提取、答案重构、表格扩充、通过预过滤和重新排序进行证据检索以及表格到图表的映射。ChartCitor在多种图表类型上优于现有基线。定性用户研究表明，ChartCitor通过为LLM辅助的图表问答提供增强的可解释性，有助于提高用户对生成式人工智能的信任，并使专业人士更加高效。|
|**2025-01-31**|**Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game**|Mustafa O. Karabag et.al.|[2501.19398](http://arxiv.org/abs/2501.19398)|**[link](https://github.com/mustafakarabag/llmchameleon)**|**基于大型语言模型（LLM）的代理在包含非合作方的环境中变得普遍。在这样的环境中，代理的决策需要对其对手隐藏信息，向其合作者透露信息，并推断信息以识别其他代理的特征。为了调查LLM是否具备这些信息控制和决策能力，我们让LLM代理参与基于语言的隐藏身份游戏《变色龙》。在这个游戏中，一群彼此不相识的非变色龙代理试图在不透露秘密的情况下识别变色龙代理。游戏要求变色龙和非变色龙代理都具备上述信息控制能力。实证结果表明，虽然非变色龙LLM代理能够识别变色龙，但它们未能从变色龙那里隐藏秘密，其获胜概率远低于甚至微不足道的策略水平。为了正式解释这种行为，我们对从隐藏到透露的一系列策略进行了理论分析，并给出了非变色龙获胜概率的上限。基于实证结果和对不同策略的理论分析，我们推断LLM基础上的非变色龙代理向未知身份的代理透露了过多的信息。我们的结果指向了当代LLM（包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet）在战略互动中的弱点。**|
|**2025-01-30**|**Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach**|Tianpeng Pan et.al.|[2501.18320](http://arxiv.org/abs/2501.18320)|null|随着大型语言模型（LLMs）的快速发展，自动优化建模（AOM）引起了广泛关注。现有方法主要依赖于提示工程，利用精心设计的专家回应链或结构化指导。然而，由于缺乏特定领域知识，基于提示的技术在传感器阵列信号处理（SASP）领域表现不佳。为了解决这个问题，我们提出了一种基于检索增强生成（RAG）技术的自动建模方法，它包含两个主要组成部分：一个多代理（MA）结构和基于图的RAG（Graph-RAG）过程。MA结构针对架构AOM过程进行了定制，每个代理都是基于人类建模程序的原则设计的。Graph-RAG过程用于将用户查询与特定的SASP建模知识相匹配，从而提高建模结果。在十个经典信号处理问题上的结果表明，所提出的方法（称为MAG-RAG）优于几个AOM基准。|
|**2025-01-30**|**RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing**|Jinyao Guo et.al.|[2501.18160](http://arxiv.org/abs/2501.18160)|**[link](https://github.com/purcl/repoaudit)**|代码审计是一种以查找漏洞为目的的代码审查过程。大型语言模型（LLMs）在这一任务中显示出巨大的潜力，能够无需编译分析程序，并可根据指定提示进行定制化漏洞检测。然而，将LLMs应用于仓库级别的代码审计面临着显著的挑战。LLMs固有的上下文限制和幻觉可能导致漏洞报告质量低下。同时，软件仓库的大规模也引入了大量的时间和令牌成本，阻碍了实际场景中的效率和可扩展性。这项工作介绍了一种自主的LLM代理，名为RepoAudit，旨在实现精确高效的仓库级别代码审计。RepoAudit配备了代理记忆功能，可以根据需求探索代码仓库，分析单个函数中不同可行程序路径上的数据流事实。它还引入了验证器来检查数据流事实以减轻幻觉，并检查潜在有误路径的条件满足性，这使得RepoAudit能够排除代码审计中的误报。我们的实验表明，由Claude 3.5 Sonnet驱动的RepoAudit在15个现实世界系统中成功找到了38个真实漏洞，平均每个项目耗时0.44小时，花费2.54美元。|
|**2025-01-29**|**Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant**|Gaole He et.al.|[2501.17546](http://arxiv.org/abs/2501.17546)|**[link](https://github.com/delftcrowd/iui2025_convxai)**|**可解释人工智能（XAI）方法被提出以帮助解释和理解AI系统如何得出特定预测。受先前关于对话用户界面的工作启发，我们认为通过对话用户界面增强现有的XAI方法可以提高用户参与度并增强用户对AI系统的理解。在本文中，我们探讨了对话XAI界面对用户对AI系统理解、信任和依赖的影响。与XAI仪表板相比，我们发现对话XAI界面可以提升用户对AI系统的理解，并提高用户对AI系统的信任。然而，XAI仪表板和对话XAI界面的用户都表现出对AI系统的过度依赖。由大型语言模型（LLM）代理驱动的增强对话放大了这种过度依赖。基于我们的发现，我们认为这种过度依赖的潜在原因是伴随XAI界面的解释深度错觉。我们的发现对设计有效的对话XAI界面以促进适当的依赖和改善人机协作具有重要意义。代码可在https://github.com/delftcrowd/IUI2025_ConvXAI找到。**|
|**2025-01-28**|**A sketch of an AI control safety case**|Tomek Korbak et.al.|[2501.17315](http://arxiv.org/abs/2501.17315)|null|随着大型语言模型（LLM）代理造成伤害的能力不断增强，AI开发者可能会越来越多地依赖监控等控制措施来证明其安全性。我们概述了开发者如何构建一个“控制安全案例”，这是一种结构化论证，表明模型无法规避控制措施以造成不可接受的结果。作为案例研究，我们概述了一个假设性LLM代理在AI公司内部部署不会泄露敏感信息的论证。这个概述依赖于来自“控制评估”的证据，在这个评估中，红队故意设计模型以在部署环境的代理中泄露数据。安全案例的关键在于以下几个主张：（1）红队充分激发了模型泄露数据的能力，（2）控制措施在部署中至少保持同等有效性，以及（3）开发者保守地外推模型性能以预测部署中数据泄露的概率。这个安全案例概述是朝着更具体的论证迈出的一步，这些论证可以用来证明一个危险能力的大型语言模型代理是安全部署的。|
|**2025-01-27**|**Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma**|Richard Willis et.al.|[2501.16173](http://arxiv.org/abs/2501.16173)|**[link](https://github.com/willis-richard/evollm)**|**随着自主代理的日益普遍，理解它们在战略互动中的集体行为至关重要。本研究调查了大型语言模型（LLM）代理系统在社会困境中的涌现合作倾向。与以往研究中LLM输出个体行动不同，我们提示最先进的LLM生成重复囚徒困境的完整策略。利用进化博弈论，我们模拟了具有不同战略倾向（侵略性、合作性或中立性）的代理种群，并观察其进化动态。我们的发现揭示，不同的LLM表现出影响侵略性策略与合作性策略相对成功率的独特偏见。这项研究为部署的LLM自主代理系统的潜在长期行为提供了见解，并强调了仔细考虑它们运行的策略环境的重要性。**|
|**2025-01-27**|**LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models**|Yuewen Mei et.al.|[2501.15850](http://arxiv.org/abs/2501.15850)|null|确保和提高自动驾驶系统（ADS）的安全性对于高度自动化车辆的部署至关重要，尤其是在安全关键事件中。为了解决罕见事件的问题，开发了对抗场景生成方法，其中通过操纵交通参与者的行为来诱导安全关键事件。然而，现有方法仍然存在两个局限性。首先，对抗参与者的识别直接影响到生成效果的有效性。然而，现实场景的复杂性，包括众多参与者和多样的行为，使得识别变得具有挑战性。其次，生成的安全关键场景对持续提高ADS性能的潜力尚未得到充分探索。为了解决这些问题，我们提出了LLM-attacker：一个利用大型语言模型（LLMs）的闭环对抗场景生成框架。具体来说，设计了多个LLM智能体并进行协调，以识别最优攻击者。然后，优化攻击者的轨迹以生成对抗场景。这些场景基于ADS的性能进行迭代优化，形成一个反馈循环以提高ADS。实验结果表明，LLM-attacker能够创建比其他方法更危险的场景，并且使用它训练的ADS的碰撞率是使用正常场景训练的一半。这表明LLM-attacker能够测试和增强ADS的安全性和鲁棒性。视频演示请见：https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view。|
|**2025-01-25**|**Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions**|Naihao Deng et.al.|[2501.15283](http://arxiv.org/abs/2501.15283)|null|随着大型语言模型（LLMs）能力的提升，研究人员越来越多地将它们应用于社会模拟。在这篇论文中，我们研究LLM代理之间的互动是否类似于人类。具体来说，我们关注领导者与非领导者之间的代词使用差异，考察模拟是否会导致LLMs互动过程中出现类似人类的代词使用模式。我们的评估揭示了基于LLM的模拟与人类代词使用之间存在的显著差异，基于提示或专门的代理未能展现出类似人类的代词使用模式。此外，我们发现即使LLMs理解人类的代词使用模式，它们也无法在真实互动过程中表现出来。我们的研究突显了基于LLM代理的社会模拟的局限性，呼吁在实践者的决策过程中对此类社会模拟持谨慎态度。|
|**2025-01-24**|**Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement Learning-based Model Caching and Inference Offloading**|Minrui Xu et.al.|[2501.14205](http://arxiv.org/abs/2501.14205)|null|大型语言模型（LLMs）能够进行未见任务的零样本学习以及在复杂推理任务上的小样本学习。然而，资源有限的移动边缘网络在用户多轮交互中难以支持LLM代理的长上下文LLM服务。与边缘计算中无状态计算卸载和静态服务卸载不同，在边缘服务器上优化LLM服务具有挑战性，因为LLMs持续从上下文中学习，这提高了准确率、延迟和资源消耗的动态性。在本文中，我们提出了一种联合模型缓存和推理卸载框架，该框架利用测试时深度强化学习（T2DRL）来优化长上下文LLM服务的部署和执行策略。在此框架中，我们分析了性能收敛性并设计了一个优化问题，考虑了LLM中上下文窗口的利用。此外，T2DRL算法可以在训练阶段和测试阶段学习，以主动管理缓存模型和服务请求，并在执行过程中适应上下文变化和用法模式。为了进一步提高资源分配效率，我们提出了一个双荷兰式拍卖（DDA）机制，该机制在动态匹配供需的同时最大化社会福利。最终，实验结果表明，与基线相比，T2DRL算法至少可以降低30%的系统成本，同时保证LLM代理在现实感知和推理任务中的性能。|
|**2025-01-23**|**AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback**|Joshua Park et.al.|[2501.13333](http://arxiv.org/abs/2501.13333)|**[link](https://github.com/joshprk/agentrec)**|**多智能体系统需要决定哪个智能体最适合执行特定任务。我们提出了一种新的架构，通过扩展Sentence-BERT（SBERT）编码器模型来推荐在给定的自然语言提示下，应该由哪个LLM智能体执行任务。在测试数据上，我们能够实现92.2%的top-1准确率，每个分类所需时间不到300毫秒。与传统的分类方法相比，我们的架构计算成本低、能够适应新类别、可解释，并通过强化学习通过任意指标进行控制。通过将自然语言提示编码为句子嵌入，我们的模型捕捉了与推荐智能体相关的语义内容。然后通过微调最小化属于同一智能体的句子嵌入之间的距离，并通过来自人类反馈的强化学习与人类价值观对齐。这允许通过测量嵌入之间的余弦相似度来根据自然语言提示的最近邻进行分类。这项工作通过生成用于智能体推荐的合成数据集成为可能，我们将该数据集和AgentRec推荐系统的代码开源至https://github.com/joshprk/agentrec。**|
|**2025-01-23**|**Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents**|Shrinidhi Kumbhar et.al.|[2501.13299](http://arxiv.org/abs/2501.13299)|null|材料发现和设计对于推动各个行业的技术进步至关重要，因为它能够促进特定应用材料的发展。最近的研究利用大型语言模型（LLMs）来加速这一过程。我们探讨了LLMs生成可行假设的潜力，这些假设一旦得到验证，可以加快材料发现的速度。与材料科学专家合作，我们从最近的期刊出版物中整理了一个新的数据集，其中包含现实世界的目标、约束和设计现实世界应用的方法。使用这个数据集，我们测试了基于LLMs的代理，这些代理在特定约束下生成实现给定目标的假设。为了评估这些假设的相关性和质量，我们提出了一种新的可扩展评估指标，该指标模拟了材料科学家评估假设时所使用的批判性过程。我们整理的数据集、提出的方法和评估框架旨在推动利用LLMs加速材料发现和设计的研究。|
|**2025-01-21**|**LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems**|Venkata Sai Aswath Duvvuru et.al.|[2501.11864](http://arxiv.org/abs/2501.11864)|null|彻底的仿真测试对于验证小型无人驾驶航空系统（sUAS）在多种场景下的正确行为至关重要，包括恶劣天气条件（如风和雾）、不同环境（如丘陵地形或城市区域）以及不同的任务配置（如监视、跟踪）。尽管存在各种sUAS仿真工具来支持开发者，但创建、执行和分析仿真测试的整个过程仍然是一个主要的手动且繁琐的任务。开发者必须识别测试场景、设置仿真环境、将系统（SuT）与仿真工具集成、制定任务计划以及收集和分析结果。这些劳动密集型任务限制了开发者进行广泛场景测试的能力。为了解决这个问题，在本文中，我们提出了AutoSimTest，这是一个由大型语言模型（LLM）驱动的框架，其中多个LLM代理协作以支持sUAS仿真测试过程。这包括：（1）创建测试场景，使SuT面临独特的环境背景；（2）根据测试场景准备仿真环境；（3）为SuT生成多种sUAS任务；（4）分析仿真结果并提供交互式分析界面。此外，该框架的设计灵活，可以创建和测试适用于各种sUAS用例、仿真工具和SuT输入要求的场景。我们通过以下方式评估了我们的方法：（a）对基于PX4和ArduPilot飞行控制器的SuT进行仿真测试，（b）分析每个代理的性能，以及（c）收集sUAS开发者的反馈。我们的发现表明，AutoSimTest显著提高了sUAS测试过程的效率和范围，允许进行更全面和多样化的场景评估，同时减少了手动工作量。|
|**2025-01-20**|**Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training**|Siyu Yuan et.al.|[2501.11425](http://arxiv.org/abs/2501.11425)|**[link](https://github.com/bytedance/agent-r)**|**大型语言模型（LLMs）的智能体在处理交互式环境中的复杂任务方面变得越来越关键。现有研究主要集中于通过从更强的专家那里进行行为克隆来提高性能，然而，这些方法在现实世界应用中往往失败，主要是因为无法从错误中恢复。然而，逐步级批判数据的收集既困难又昂贵。因此，自动化和动态构建自我批判数据集对于赋予模型智能体能力至关重要。在这项工作中，我们提出了一种迭代自我训练框架，称为Agent-R，它使语言智能体能够即时反思。与基于正确性奖励或惩罚动作的传统方法不同，Agent-R利用MCTS构建训练数据，从错误中恢复正确轨迹。智能体反思的一个关键挑战在于需要及时修订，而不是等到轮次结束时。为了解决这个问题，我们引入了一种模型引导的批判构建机制：动作模型识别失败轨迹中的第一个错误步骤（在其当前能力范围内）。从它开始，我们将它与相邻的正确路径拼接，该路径在树中共享相同的父节点。这种策略使模型能够根据其当前策略进行反思，从而提高学习效率。为了进一步探索这种自我改进范式的可扩展性，我们研究了错误纠正能力和数据集构建的迭代细化。我们的发现表明，Agent-R不断提高了模型从错误中恢复的能力，并实现了及时的错误纠正。在三个交互式环境上的实验表明，Agent-R有效地装备了智能体纠正错误动作并避免循环，与基线方法相比取得了更好的性能（+5.59%）。**|
|**2025-01-20**|**Large Language Model Agents for Radio Map Generation and Wireless Network Planning**|Hongye Quan et.al.|[2501.11283](http://arxiv.org/abs/2501.11283)|null|在利用商业软件进行无线电地图生成和无线网络规划时，通常需要复杂的手动操作，由于手动操作量大，这给可扩展性、适应性和用户友好性带来了重大挑战。为了解决这些问题，我们提出了一种自动化解决方案，该方案采用大型语言模型（LLM）代理。这些代理被设计为能够自主生成无线电地图并简化指定区域的无线网络规划，从而最大限度地减少对大量手动干预的需求。为了验证我们提出解决方案的有效性，我们开发了一个集成LLM代理的软件平台。实验结果表明，通过所提出的LLM代理可以节省大量手动操作，并且自动化解决方案能够实现更优的覆盖范围和信噪比（SINR），尤其是在城市环境中。|
|**2025-01-20**|**PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents**|Kanika Goswami et.al.|[2501.11233](http://arxiv.org/abs/2501.11233)|null|图表可视化虽然对于数据解读和交流至关重要，但它们主要只能以图片形式在PDF中访问，缺乏源数据表和样式信息。为了能够有效地编辑PDF中的图表或数字扫描图像，我们提出了PlotEdit，这是一个基于自然语言的端到端图表图像编辑的多代理框架，通过自我反思的LLM代理实现。PlotEdit协调五个LLM代理：(1) Chart2Table用于数据表提取，(2) Chart2Vision用于识别样式属性，(3) Chart2Code用于检索渲染代码，(4) 指令分解代理用于将用户请求解析为可执行步骤，以及(5) 多模态编辑代理用于实现细微的图表组件修改——所有这些通过多模态反馈进行协调，以保持视觉准确性。在ChartCraft数据集上，PlotEdit在风格、布局、格式和数据为中心的编辑方面优于现有基线，提高了视觉障碍用户的可访问性，并提高了新手的生产力。|
|**2025-01-20**|**QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks**|Yaojie Hu et.al.|[2501.17167](http://arxiv.org/abs/2501.17167)|null|我们引入了QualityFlow，这是一种动态的代理工作流程，用于程序合成。给定一个编程问题的英文描述和一组单元测试，模型的目标是合成一个正确的程序，该程序能够解决问题并通过测试。QualityFlow由多个大型语言模型（LLM）代理组成，类似于一个软件开发团队，包括代码生成、测试和自我调试。现有的程序合成方法面临三个主要限制：假设可见单元测试一致性、合成测试质量的瓶颈以及自我调试轨迹的偏差。为了解决这些问题，我们提出了LLM质量检查器，该检查器明确地“想象”合成程序的执行是否将符合单元测试。质量检查动态控制工作流程，包括提交最终答案、阐明问题陈述和撤销先前工作流程步骤等操作。因此，我们的质量检查器可以精确地接受任何正确的程序，减轻错误合成测试，并防止潜在的工作流程偏差。质量检查器的成功进一步实现了多样化提示，这鼓励LLM响应的变化，以最大限度地提高正确程序出现并通过质量检查的可能性。在实验中，QualityFlow在四个程序合成基准测试（MBPP、HumanEval以及来自EvalPlus对MBPP和HumanEval的更严格评估）上取得了最先进的结果。我们的系统分析表明，由LLM质量检查控制的动态工作流程可以优于静态工作流程和单次尝试零样本合成。质量检查器是我们研究的核心，我们分析了其个别性能和对工作流程准确性的整体影响，以及其他消融实验，以证明我们的工作流程设计。|
|**2025-01-18**|**Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments**|Hongjin Su et.al.|[2501.10893](http://arxiv.org/abs/2501.10893)|null|基于大型语言模型（LLMs）的自主代理具有增强人类能力、协助处理从发送电子邮件到执行数据分析等数字任务的潜力。现有LLMs在这些任务上的能力常常受到缺乏与它们交互的相应环境中高质量代理数据的限制。我们提出了“交互学习”（Learn-by-interact），一个以数据为中心的框架，用于将LLMs代理适应任何给定环境，而无需人工标注。Learn-by-interact基于文档合成代理-环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程被称为“反向构建”。我们通过在基于训练的场景和无训练的上下文学习（ICL）中使用这些合成数据来评估其质量，其中我们设计了针对代理优化的创新检索方法。在SWE-bench、WebArena、OSWorld和Spider2-V上的广泛实验，涵盖了现实中的编码、网页和桌面环境，展示了Learn-by-interact在多种下游代理任务中的有效性——使用Claude-3.5进行ICL的基线结果提高了最多12.2%，使用Codestral-22B进行训练提高了最多19.5%。我们进一步证明了反向构建的关键作用，它为训练提供了高达14.0%的改进。我们的消融研究证明了我们在ICL中合成数据的效率以及我们的检索管道相对于传统检索增强生成（RAG）等替代方法的优越性。我们期望Learn-by-interact将成为LLMs在现实世界环境中部署时代理数据合成的基石。|
|**2025-01-18**|**ML-SceGen: A Multi-level Scenario Generation Framework**|Yicheng Xiao et.al.|[2501.10782](http://arxiv.org/abs/2501.10782)|null|当前科学研究见证了将大型语言模型应用于场景生成的各种尝试，但仅倾向于生成全面或危险的场景。在本文中，我们寻求构建一个三阶段框架，不仅让用户能够重新获得对生成的场景的控制权，还能在不受控制的交叉路口设置中生成包含危险因素的全面场景。在第一阶段，LLM代理将有助于将预期场景描述的关键组件翻译成功能场景。在第二阶段，我们使用答案集编程（ASP）求解器Clingo帮助我们生成交叉路口内的全面逻辑交通。在最后阶段，我们使用LLM更新相关参数以提高具体场景的关键水平。|
|**2025-01-17**|**PaSa: An LLM Agent for Comprehensive Academic Paper Search**|Yichen He et.al.|[2501.10120](http://arxiv.org/abs/2501.10120)|**[link](https://github.com/bytedance/pasa)**|**我们介绍了一种名为PaSa的高级论文搜索代理，它由大型语言模型驱动。PaSa可以自主进行一系列决策，包括调用搜索工具、阅读论文和选择相关参考文献，最终为复杂的学术查询提供全面准确的结果。我们使用合成数据集AutoScholarQuery对PaSa进行优化，该数据集包含来自顶级AI会议出版物中的3.5万个细粒度学术查询及其对应论文。此外，我们开发了RealScholarQuery基准，它收集真实世界的学术查询，以评估PaSa在实际场景中的性能。尽管PaSa是在合成数据上训练的，但在RealScholarQuery上它显著优于现有的基线，包括Google、Google Scholar、Google结合GPT-4进行释义查询、chatGPT（具有搜索功能的GPT-4o）、GPT-o1和PaSa-GPT-4o（通过提示GPT-4o实现的PaSa）。值得注意的是，PaSa-7B在recall@20和recall@50方面的表现超过了基于Google的最佳基线Google结合GPT-4o，分别高出37.78%和39.90%。它还在召回率上超过了PaSa-GPT-4o 30.36%，在精确度上高出4.25%。模型、数据集和代码可在https://github.com/bytedance/pasa上获取。**|
|**2025-01-17**|**A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks**|Xinzhe Li et.al.|[2501.10069](http://arxiv.org/abs/2501.10069)|**[link](https://github.com/xinzhel/llm-agent-survey)**|通过搜索进行LLM测试时计算（或LLM推理）已成为一个具有快速发展的有希望的研究领域。然而，当前框架通常在三个关键方面（任务定义、LLM分析和搜索过程）采用不同的观点，这使得直接比较变得困难。此外，所采用的搜索算法往往偏离标准实现，且其特定特征未得到充分说明。在本调查中，我们提供了一项全面的技术综述，统一了任务定义，并提供了LLM分析和搜索过程的模块化定义。这些定义使得对各种LLM推理框架的精确比较成为可能，同时突出了它们与常规搜索算法的差异。我们还讨论了这些方法的应用性、性能和效率。有关更详细的信息和持续更新，请参阅我们的GitHub仓库：https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md|
|**2025-01-15**|**Leveraging LLM Agents for Translating Network Configurations**|Yunze Wei et.al.|[2501.08760](http://arxiv.org/abs/2501.08760)|null|配置转换是网络操作中一项关键且频繁的任务。当网络设备损坏或过时，管理员需要更换设备以保持服务连续性。更换的设备可能来自不同的厂商，这需要配置转换以确保网络操作无缝进行。然而，手动转换配置是一个劳动密集且易出错的过程。在本文中，我们提出了一种基于意图的框架，使用大型语言模型（LLM）代理进行网络配置转换。我们方法的核心是一个基于意图的检索增强生成（IRAG）模块，该模块系统地将配置文件分割成片段，提取意图并生成准确的翻译。我们还设计了一种两阶段验证方法来验证转换配置的语法和语义正确性。我们在真实世界的网络配置上实现了并评估了所提出的方法。实验结果表明，我们的方法在翻译准确性方面达到了97.74%的语法正确率，超过了现有技术的最佳水平。|
|**2025-01-14**|**Addressing the sustainable AI trilemma: a case study on LLM agents and RAG**|Hui Wu et.al.|[2501.08262](http://arxiv.org/abs/2501.08262)|**[link](https://github.com/huiwxing/llmagent_trilemma)**|大型语言模型（LLMs）展现出显著的能力，但它们的广泛应用和更高级应用引发了关键的可持续性挑战，尤其是在推理能耗方面。我们提出了可持续AI三难理论，突出了人工智能能力、数字公平性和环境可持续性之间的紧张关系。通过系统性地研究LLM代理和检索增强生成（RAG），我们分析了内存模块设计中的能源成本，并引入了新的指标来量化能耗与系统性能之间的权衡。我们的实验结果表明，当前内存增强框架中存在显著的能源低效，并证明资源受限的环境面临着不成比例的效率惩罚。我们的发现挑战了当前以LLM为中心的代理设计范式，并为开发更可持续的人工智能系统提供了实用见解。|
|**2025-01-14**|**Flow: A Modular Approach to Automated Agentic Workflow Generation**|Boye Niu et.al.|[2501.07834](http://arxiv.org/abs/2501.07834)|**[link](https://github.com/tmllab/2025_iclr_flow)**|基于大型语言模型（LLM）的多智能体框架在自动规划和任务执行方面取得了巨大成功。然而，在执行过程中对智能体工作流程的有效调整尚未得到充分研究。有效的工作流程调整至关重要，因为在许多实际场景中，初始计划必须实时调整以应对未预见的挑战和变化条件，以确保复杂任务的效率执行。在本文中，我们将工作流程定义为顶点活动图（AOV图）。我们通过根据历史性能和之前与LLM智能体协同的AOV动态调整任务分配来不断优化工作流程。为了进一步提高系统性能，我们强调基于衡量并行性和依赖复杂度的工作流程设计模块化。我们提出的多智能体框架实现了高效子任务的并发执行、目标达成和错误容忍。不同实际任务中的实证结果表明，通过动态工作流程更新和模块化，多智能体框架的效率得到了显著提升。|
|**2025-01-13**|**SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing**|Varun Biyyala et.al.|[2501.07554](http://arxiv.org/abs/2501.07554)|**[link](https://github.com/custommetrics-sst/sst_customevaluationmetrics)**|**视频编辑模型已取得显著进步，但对其性能进行评估仍然具有挑战性。传统的指标，如CLIP文本和图像分数，通常存在不足：文本分数受限于不足的训练数据和层级依赖性，而图像分数无法评估时间一致性。我们提出了SST-EM（语义、空间和时间评估指标），这是一种利用现代视觉语言模型（VLM）、物体检测和时间一致性检查的新颖评估框架。SST-EM包括四个部分：（1）使用VLM从帧中提取语义，（2）使用物体检测进行主要物体跟踪，（3）通过一个LLM代理进行聚焦物体细化，以及（4）使用视觉Transformer（ViT）进行时间一致性评估。这些部分被整合到一个统一的指标中，其权重来源于人类评估和回归分析。SST-EM的名字反映了其对视频评估的语义、空间和时间方面的关注。SST-EM为视频编辑中的语义保真度和时间平滑度提供了全面评估。源代码可在GitHub仓库中找到：\textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub Repository}}。**|
|**2025-01-13**|**Lifelong Learning of Large Language Model based Agents: A Roadmap**|Junhao Zheng et.al.|[2501.07278](http://arxiv.org/abs/2501.07278)|**[link](https://github.com/qianlima-lab/awesome-lifelong-llm-agent)**|**终身学习，也称为持续学习或增量学习，是通过使系统能够持续适应动态环境来推进通用人工智能（AGI）的关键组成部分。尽管大型语言模型（LLM）在自然语言处理方面展现了令人印象深刻的能力，但现有的LLM代理通常是为静态系统设计的，缺乏随时间适应新挑战的能力。这项调查首次系统地总结了将终身学习融入基于LLM代理的潜在技术。我们将这些代理的核心组件分为三个模块：感知模块，用于多模态输入整合；记忆模块，用于存储和检索不断发展的知识；动作模块，用于与动态环境的地面交互。我们强调了这些支柱如何共同实现持续适应、减轻灾难性遗忘并提高长期性能。本调查为致力于在LLM代理中开发终身学习能力的研究人员和从业者提供了路线图，提供了对新兴趋势、评估指标和应用场景的见解。相关文献和资源可在以下链接找到：[https://github.com/qianlima-lab/awesome-lifelong-llm-agent](https://github.com/qianlima-lab/awesome-lifelong-llm-agent)。**|
|**2025-01-12**|**AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds**|Yinfang Chen et.al.|[2501.06706](http://arxiv.org/abs/2501.06706)|null|人工智能IT运维（AIOps）旨在自动化复杂运维任务，如故障定位和根本原因分析，以减轻人工工作量并最小化对客户的影响。尽管传统的DevOps工具和AIOps算法通常关注解决孤立的运维任务，但最近在大型语言模型（LLMs）和AI代理方面的进展正在通过实现端到端和多任务自动化来革新AIOps。本文设想了一个未来，其中AI代理可以自主管理整个事件生命周期的运维任务，从而实现自我修复的云计算系统，我们称之为代理运维（AgentOps）。实现这一愿景需要一套全面的框架来指导这些代理的设计、开发和评估。为此，我们提出了AIOPSLAB框架，它不仅部署微服务云环境、注入故障、生成工作负载和导出遥测数据，而且还编排这些组件并提供与代理交互和评估的接口。我们讨论了此类整体框架的关键要求，并展示了AIOPSLAB如何促进下一代AIOps代理的评估。通过在AIOPSLAB创建的基准测试中评估最先进的LLM代理，我们提供了它们在处理云环境中的复杂运维任务方面的能力和局限性的见解。|
|**2025-01-12**|**DVM: Towards Controllable LLM Agents in Social Deduction Games**|Zheng Zhang et.al.|[2501.06695](http://arxiv.org/abs/2501.06695)|null|大型语言模型（LLMs）推动了社交推理游戏（SDGs）中游戏代理的能力。这类游戏高度依赖由对话驱动的交互，需要代理根据此类信息进行推理、做出决策和表达。尽管这一进步导致了SDGs中更复杂和策略性的非玩家角色（NPCs），但仍然需要控制这些代理的熟练程度。这种控制不仅确保NPC能够在游戏过程中适应不同的难度级别，而且为LLM代理的安全性和公平性提供了见解。在本文中，我们提出了DVM，一个用于开发可控LLM代理用于SDGs的新框架，并在最受欢迎的SDG之一《狼人杀》中展示了其实现。DVM包括三个主要组件：预测器、决策者和讨论者。通过将强化学习与获胜率受限的决策链奖励机制相结合，我们使代理能够动态调整其游戏熟练程度以达到特定的获胜率。实验表明，DVM不仅在《狼人杀》游戏中优于现有方法，而且成功调节了其性能水平以实现预定义的获胜率目标。这些结果为LLM代理在SDGs中的自适应和平衡游戏铺平了道路，为可控游戏代理的研究开辟了新的途径。|
|**2025-01-10**|**OpenFOAMGPT: a RAG-Augmented LLM Agent for OpenFOAM-Based Computational Fluid Dynamics**|Sandeep Pandey et.al.|[2501.06327](http://arxiv.org/abs/2501.06327)|null|这项工作提出了一种基于大型语言模型（LLM）的代理OpenFOAMGPT，专为OpenFOAM中心的计算流体动力学（CFD）模拟而设计，利用了OpenAI的两个基础模型：GPT-4o和一个思维链（CoT）启用o1预览模型。这两个代理在多个任务中都取得了成功。虽然使用o1模型的代币价格是GPT-4o的六倍，但它始终在处理复杂任务方面表现出优异的性能，从零样本案例设置到边界条件修改、湍流模型调整和代码翻译。通过迭代校正循环，代理高效地处理了单相和多相流、传热、RANS、LES和其他工程场景，通常在有限的迭代次数和低代币成本下收敛。为了嵌入特定领域的知识，我们采用了检索增强生成（RAG）管道，展示了如何将现有模拟设置进一步专门化，以适应能源和航空航天等子领域。尽管代理的性能优异，但人类监督对于确保准确性和适应不断变化的环境仍然至关重要。模型性能随时间波动表明，在任务关键型应用中需要监控。尽管我们的演示集中在OpenFOAM上，但该框架的适应性为将LLM驱动的代理开发成各种求解器和代码打开了大门。通过简化CFD模拟，这种方法有可能加快基础研究和工业工程进步。|
|**2025-01-10**|**Multi-Agent Collaboration Mechanisms: A Survey of LLMs**|Khanh-Tung Tran et.al.|[2501.06322](http://arxiv.org/abs/2501.06322)|null|随着大型语言模型（LLMs）的近期进展，具有代理功能的AI在现实世界中的应用变得非常显著，正朝着基于多个LLMs的智能体协同感知、学习、推理和行动的方向发展。这些基于LLMs的多智能体系统（MASs）使得一群智能体能够大规模地协同解决复杂任务，从孤立的模型转向以合作为核心的方法。本研究对MASs的协同方面进行了广泛的调查，并介绍了一个可扩展的框架来指导未来的研究。我们的框架根据关键维度来描述协作机制：参与者（涉及的智能体）、类型（例如，合作、竞争或竞合）、结构（例如，对等、集中式或分布式）、策略（例如，基于角色的或基于模型的）和协调协议。通过回顾现有方法，我们的发现为揭示和推进基于LLMs的MASs，使其更智能、更协作地解决复杂现实世界案例提供了基础。此外，还研究了MASs在各个领域的应用，包括5G/6G网络、工业5.0、问答和社交文化环境，展示了它们更广泛的应用和更深远的影响。最后，我们确定了MASs在向人工集体智能发展过程中所学的关键经验、开放挑战和潜在的研究方向。|
|**2025-01-09**|**Emergence of human-like polarization among large language model agents**|Jinghua Piao et.al.|[2501.05171](http://arxiv.org/abs/2501.05171)|null|大型语言模型（LLMs）的快速进步赋予了自主代理建立社会关系、沟通以及在政治问题上形成共享和分歧意见的能力。然而，我们对它们的集体行为和潜在机制的理解仍然不完整，这给人类社会带来了意外的风险。在本文中，我们模拟了一个包含数千个大型语言模型代理的网络系统，发现通过LLM对话引导的社会互动导致类似人类的极化。我们发现，这些代理自发地发展出具有类似人类特性的社会网络，包括同质性集群，但它们也通过观察到的现实世界机制塑造其集体意见，包括回音室效应。人类与LLM代理之间的相似性——包括行为、机制和涌现现象——引发了对它们放大社会极化能力的担忧，但也持有作为识别缓解极化及其后果可能策略的有价值测试平台的潜力。|
|**2025-01-09**|**LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models**|Zengqi Peng et.al.|[2501.05057](http://arxiv.org/abs/2501.05057)|null|近期强化学习（RL）在自动驾驶领域的进展展示了其巨大的潜力。尽管如此，手动设计奖励函数和复杂环境中的低样本效率等问题仍然阻碍着安全有效的驾驶策略的开发。为了解决这些问题，我们引入了LearningFlow，这是一种针对城市驾驶的创新型自动化策略学习工作流程。该框架在整个RL训练过程中利用多个大型语言模型（LLM）代理之间的协作。LearningFlow包括课程序列生成过程和奖励生成过程，这两个过程协同工作，通过生成定制的训练课程和奖励函数来指导RL策略。特别是，每个过程都由一个分析代理支持，该代理评估训练进度并为生成代理提供关键见解。通过这些LLM代理的协作努力，LearningFlow自动化了一系列复杂驾驶任务的政策学习，并显著减少了对手动奖励函数设计的依赖，同时提高了样本效率。在高保真的CARLA模拟器中进行了全面实验，并与其他现有方法进行了比较，以证明我们提出方法的有效性。结果表明，LearningFlow在生成奖励和课程方面表现出色。它还在各种驾驶任务中实现了优越的性能和鲁棒的一般化，以及值得称赞的对不同RL算法的适应性。|
|**2025-01-08**|**Agent Laboratory: Using LLM Agents as Research Assistants**|Samuel Schmidgall et.al.|[2501.04227](http://arxiv.org/abs/2501.04227)|null|历史上，科学发现是一个漫长且昂贵的流程，从最初的构思到最终结果，需要大量的时间和资源。为了加速科学发现、降低研究成本并提高研究质量，我们引入了“智能实验室”，这是一个基于自主大型语言模型（LLM）的框架，能够完成整个研究过程。该框架接受人类提供的科研想法，并通过三个阶段——文献综述、实验和报告撰写——生成全面的研究成果，包括代码库和研究报告，同时允许用户在每个阶段提供反馈和指导。我们将智能实验室部署了多种最先进的LLM，并邀请多位研究人员通过参与调查、提供人类反馈以指导研究过程，然后评估最终论文来评估其质量。我们发现：（1）“智能实验室”由o1-preview驱动时能够生成最佳的研究成果；（2）生成的机器学习代码与现有方法相比能够实现最先进的性能；（3）在每个阶段提供反馈的人类参与显著提高了研究的整体质量；（4）“智能实验室”显著降低了研究费用，与之前自主研究方法相比实现了84%的降幅。我们希望“智能实验室”能够使研究人员能够将更多精力投入到创意构思上，而不是低级的编码和写作，从而最终加速科学发现。|
|**2025-01-02**|**Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects**|Abdullah Mushtaq et.al.|[2501.01205](http://arxiv.org/abs/2501.01205)|null|多智能体大型语言模型（LLMs）因其能够利用集体智慧解决复杂问题、决策和规划任务的能力而受到广泛关注。这与“群体智慧”的概念相吻合，其中多样化的智能体共同贡献以生成有效解决方案，这使得它在教育环境中特别适用。高年级设计项目，也称为毕业设计或最后一年项目，在工程教育中至关重要，因为它们将理论知识与实际应用相结合，培养批判性思维、团队合作和现实世界问题解决技能。在本文中，我们探讨了多智能体LLMs在支持工程学生进行这些高年级设计项目中的应用，这些项目通常涉及跨学科考虑和冲突目标，例如在解决道德、社会和环境问题的同时优化技术性能。我们提出了一种框架，其中不同的LLM智能体代表不同的专家视角，如问题表述智能体、系统复杂性智能体、社会和伦理智能体或项目经理，从而促进全面的解决问题方法。这种实现利用了标准的多智能体系统（MAS）概念，如协调、合作和谈判，并结合提示工程为每个智能体开发不同的角色。这些智能体通过丰富的、协作的对话模拟人类工程团队，遵循群体人工智能的原则，以有效地平衡个人贡献，实现统一解决方案。我们将这些技术调整为LLM智能体的协作结构，鼓励跨学科推理和谈判，类似于现实世界中的高年级设计项目。为了评估该框架的有效性，我们收集了六个工程和计算机科学专业的……|
|**2025-01-02**|**Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens**|Abdullah Mushtaq et.al.|[2501.03259](http://arxiv.org/abs/2501.03259)|null|随着大型语言模型（LLMs）如GPT-4和Llama 3在教育环境中的应用日益普及，关于这些技术中嵌入的文化偏见、权力不平等和伦理限制的担忧日益增加。尽管生成式AI工具旨在提升学习体验，但它们往往反映的是根植于西方、受过教育、工业化、富裕和民主（WEIRD）文化模式的价值观，这可能导致多样性的全球观点被边缘化。本文提出了一种框架，通过应用多维度视角来评估和缓解LLMs中的文化偏见。多维度视角受到Senturk等人以及伊斯兰和其他智慧传统的启发，强调多元文化观点的共存，支持一个多层次的认知论，它整合了经验科学和规范价值。我们的分析揭示，LLMs常常表现出文化两极分化，偏见既出现在明显的回答中，也出现在微妙的语境线索中。为了解决固有的偏见并在LLMs中融入多维度视角，我们提出了两种策略：**情境实现的多维度LLMs**，它将多维度原则直接嵌入到系统提示中，从基础层面影响LLMs的输出，而与个人提示无关；以及**多智能体系统（MAS）实现的多维度LLMs**，其中多个LLM智能体，每个智能体代表不同的文化观点，协同生成平衡的综合回答。我们的发现表明，随着缓解策略从情境提示发展到MAS实现，文化包容性显著提高，证据是视角分布得分（PDS）显著上升，以及PDS熵从基线时的3.25%增加到MAS实现的多维度LLMs的98%。情感分析还显示，文化之间的正面情绪有所增加，...|
|**2025-01-01**|**Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents**|Fouad Bousetouane et.al.|[2501.00881](http://arxiv.org/abs/2501.00881)|null|代理系统的演变是人工智能和现代软件系统发展中的一个重要里程碑，这得益于对垂直智能的需求，这种智能能够满足不同行业的需求。这些系统通过适应性、学习和与动态环境的互动来提升商业成果。在这场革命的最前沿是大型语言模型（LLM）代理，它们作为这些智能系统的认知核心。为了应对一致性和可扩展性的需求，这项工作试图通过识别核心构建模块并提议一个认知技能模块，该模块包含特定领域、专门构建的推理能力，来为垂直AI代理设计模式定义一个标准化级别。基于这些基础概念，本文全面介绍了代理系统，详细阐述了其核心组件、操作模式和实施策略。它还进一步探讨了各个行业的实际应用案例和示例，突出了LLM代理在推动特定行业应用中的变革潜力。|
|**2024-12-31**|**MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation**|Chia-Yuan Chang et.al.|[2501.00332](http://arxiv.org/abs/2501.00332)|null|大型语言模型（LLMs）正成为各种自然语言处理任务的必备工具，但它们常常在生成过时或错误信息方面存在问题。检索增强生成（RAG）通过结合外部实时信息检索来使LLM的响应具有根据，从而解决了这个问题。然而，现有的RAG系统往往在检索文档的质量上遇到困难，因为无关或噪声文档会降低性能、增加计算开销并损害响应的可靠性。为了解决这个问题，我们提出了多智能体过滤检索增强生成（MAIN-RAG），这是一个无需训练的RAG框架，它利用多个LLM智能体来协同过滤和评分检索到的文档。具体来说，MAIN-RAG引入了一种自适应过滤机制，该机制根据分数分布动态调整相关性过滤阈值，有效地减少了噪声，同时保持了相关文档的高召回率。所提出的方法利用智能体间的共识来确保稳健的文档选择，而无需额外的训练数据或微调。在四个问答基准上的实验结果表明，MAIN-RAG始终优于传统的RAG方法，在答案准确率上提高了2-11%，同时减少了无关检索文档的数量。定量分析进一步揭示了我们的方法在响应一致性和答案准确性方面优于基线方法，为基于训练的解决方案提供了一种具有竞争力且实用的替代方案。|
|**2024-12-30**|**Aviary: training language agents on challenging scientific tasks**|Siddharth Narayanan et.al.|[2412.21154](http://arxiv.org/abs/2412.21154)|**[link](https://github.com/future-house/paper-qa)**|解决复杂现实任务需要动作和观察的循环。这在科学领域尤为明显，因为任务需要许多次分析、工具使用和实验循环。语言代理有望自动化科学领域的智力任务，因为它们可以通过自然语言或代码与工具交互。然而，它们的灵活性给软件实现带来了概念和实践上的挑战，因为代理可能包含非标准组件，如内部推理、规划、工具使用，以及温度采样语言模型固有的随机性。在这里，我们介绍了Aviary，一个可扩展的语言代理体育馆。我们将代理形式化为解决基于语言的部分可观察马尔可夫决策过程的策略，我们称之为语言决策过程。然后，我们实现了五个环境，包括三个具有挑战性的科学环境：（1）操作DNA构建体进行分子克隆，（2）通过访问科学文献来回答研究问题，以及（3）工程设计蛋白质稳定性。这些环境因其对多步推理的关注和与当代生物学研究的相关性而被选中。最后，通过在线训练和扩展推理时间计算，我们表明，由开源、非前沿LLM支持的语言代理可以在多个任务上与前沿LLM代理和人类专家相匹配并超过它们，同时推理成本降低高达100倍。|
|**2024-12-30**|**Exploring and Controlling Diversity in LLM-Agent Conversation**|KuanChao Chu et.al.|[2412.21102](http://arxiv.org/abs/2412.21102)|null|多样性是多智能体通信的关键方面。在本文中，我们聚焦于在开放域多智能体对话的背景下控制和探索多样性，尤其是针对世界模拟应用。我们提出了一种名为自适应提示剪枝（Adaptive Prompt Pruning，APP）的新方法，该方法通过一个单一参数λ动态调整话语生成提示的内容以控制多样性。通过广泛的实验，我们展示了APP能够有效控制模型和数据集的输出多样性，剪枝更多信息会导致更多样化的输出。我们全面分析了提示内容与对话多样性之间的关系。我们的发现表明，提示的各个部分的信息通常都会限制输出的多样性，其中记忆块的影响最为显著。APP与温度采样和top-p采样等现有技术兼容，为多样性管理提供了一种多功能工具。为了解决增加多样性带来的权衡，例如与省略信息的矛盾，我们引入了生成后的校正步骤，有效地平衡了多样性增强与输出一致性。此外，我们还考察了提示结构，包括组件顺序和长度，对多样性的影响。本研究解决了多智能体世界模拟中围绕多样性的关键问题，为在基于LLM的多智能体协作中系统性地构建多样性奠定了基础，提高了它们在实际应用中的有效性。|
|**2024-12-30**|**Plancraft: an evaluation dataset for planning with LLM agents**|Gautier Dagan et.al.|[2412.21033](http://arxiv.org/abs/2412.21033)|**[link](https://github.com/gautierdag/plancraft)**|**我们提出了Plancraft，一个用于LLM代理的多模态评估数据集。Plancraft具有基于Minecraft合成GUI的纯文本和多模态界面。我们包括了Minecraft维基百科来评估工具使用和检索增强生成（RAG），以及一个神谕规划器和神谕RAG信息提取器，以消除现代代理架构的不同组件。为了评估决策能力，Plancraft还包括了一组故意不可解的示例，提供了一种现实挑战，要求代理不仅完成任务，还要决定这些任务是否可解。我们在我们的任务上对开源和闭源LLM和策略进行了基准测试，并将它们的性能与手工制作的规划器进行了比较。我们发现LLM和VLM在Plancraft引入的规划问题中遇到了困难，并提出了如何提高它们能力的一些建议。**|
|**2024-12-29**|**Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning**|Hang Ni et.al.|[2412.20505](http://arxiv.org/abs/2412.20505)|null|在城市化的背景下，城市复兴面临重大挑战，需要适应性的方法来应对不断变化的需求。利用大型语言模型（LLM）的进步，我们提出了循环城市规划（CUP），这是一种新的范式，它在一个封闭循环中持续生成、评估和优化城市规划。具体来说，我们的基于多智能体的LLM框架包括三个关键组成部分：（1）规划，其中LLM智能体根据上下文数据生成和优化城市规划；（2）生活，其中智能体模拟居民的行为和互动，模拟城市环境中的生活；以及（3）判断，涉及评估计划的有效性并提供迭代反馈以改进。循环过程使得规划方法动态且具有响应性。在真实世界数据集上的实验证明了我们的框架作为持续和自适应规划过程的有效性。|
|**2024-12-28**|**FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration**|Jia Liu et.al.|[2412.20297](http://arxiv.org/abs/2412.20297)|null|近期，大型语言模型（LLMs）在增强具身代理推理能力方面取得了进展，推动了基于AGI的机器人技术的发展。虽然LLMs已应用于语义推理和任务泛化等任务，但其在开放物理空间探索中的潜力仍待开发。本文介绍了FaGeL（由LLMs赋能的具身智能代理），这是一个集成了智能织物技术的具身代理，实现了无缝、非侵入式的人机交互。FaGeL利用可穿戴和周围传感器的多模态数据自主生成任务，并根据生成的文本中的隐含人类反馈来优化其行为，而无需明确的评分或偏好。我们还引入了一种标记级显著性图来可视化LLM微调，增强了标记级对齐的可解释性。该系统利用双重反馈机制来提高标记级对齐，并解决非侵入式人机交互和认知进化的挑战。我们的贡献包括FaGeL的开发、用于AI对齐的DualCUT算法以及在合作任务中的实验验证，证明了FaGeL通过隐含反馈实现自适应和进化的能力。在未来，我们计划探索FaGeL在动态环境中的可扩展性和与其他AI系统的集成，以开发能够无缝适应多样化人类需求的AGI代理。|
|**2024-12-28**|**OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System**|Yujie Luo et.al.|[2412.20005](http://arxiv.org/abs/2412.20005)|**[link](https://github.com/zjunlp/oneke)**|**我们介绍了OneKE，一个基于Docker的架构引导知识提取系统，该系统能够从Web和原始PDF书籍中提取知识，并支持多个领域（如科学、新闻等）。具体来说，我们设计OneKE采用了多个代理和一个可配置的知识库。不同的代理执行各自的角色，从而支持各种提取场景。可配置的知识库简化了架构配置、错误情况调试和修正，进一步提升了性能。在基准数据集上的实证评估展示了OneKE的有效性，而案例研究进一步阐明了它在多个领域多样化任务中的适应性，突显了其广泛应用的潜力。我们已经开源了代码，代码链接为https://github.com/zjunlp/OneKE，并发布了一个视频，链接为http://oneke.openkg.cn/demo.mp4。**|
|**2024-12-24**|**Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent**|Farhad Nooralahzadeh et.al.|[2412.18428](http://arxiv.org/abs/2412.18428)|**[link](https://github.com/yizhang-unifr/xmode)**|国际企业、组织或医院收集了大量存储在数据库、文本文档、图像和视频中的多模态数据。尽管在多模态数据探索的各个独立领域以及将自然语言问题自动翻译成数据库查询语言的数据库系统方面取得了近期进展，但将数据库系统与其他非结构化模态（如图像）结合使用，并以自然语言进行查询的研究挑战却很少被探索。在本文中，我们提出了XMODE——一个能够以自然语言实现可解释的多模态数据探索的系统。我们的方法基于以下研究贡献：（1）我们的系统受到一个真实世界用例的启发，使用户能够探索多模态信息系统。（2）XMODE利用基于LLM的代理人工智能框架，将自然语言问题分解为子任务，如文本到SQL生成和图像分析。（3）在关系数据和图像上的多模态数据集上的实验结果表明，我们的系统在多模态探索系统中的表现优于现有系统，不仅在准确性上表现出色，而且在查询延迟、API成本、规划效率和解释质量等各个性能指标上也表现出优越性，这得益于对LLM推理能力的更有效利用。|
|**2024-12-24**|**Defining and Detecting the Defects of the Large Language Model-based Autonomous Agents**|Kaiwen Ning et.al.|[2412.18371](http://arxiv.org/abs/2412.18371)|**[link](https://github.com/KevinHeiwa/Agentable)**|**AI代理是能够感知其环境、自主规划和执行任务的系统。近年来，大型语言模型（LLM）的进步为AI代理引入了一种变革性的范式，使它们能够通过提示与外部资源和工具进行交互。在这种代理中，工作流程将开发者编写的代码（负责框架构建和逻辑控制）与LLM生成的自然语言（增强动态决策和交互）整合。然而，在行为和预期结果方面，开发者实现的逻辑与LLM动态生成的内容之间存在差异，可能导致缺陷，如工具调用失败和任务执行错误。这些问题引入了特定的风险，导致基于LLM的AI代理出现各种缺陷，如服务中断。尽管这些问题很重要，但缺乏针对分析基于LLM的AI代理以揭示其代码中缺陷的系统工作。在本文中，我们首次提出了专注于识别和检测LLM代理缺陷的研究。我们收集并分析了StackOverflow上的6,854条相关帖子，定义了8种代理缺陷类型。对于每种类型，我们提供了详细描述和示例。然后，我们设计了一个名为Agentable的静态分析工具来检测这些缺陷。Agentable利用代码属性图和LLM通过高效地识别特定代码模式和解析自然语言描述来分析代理工作流程。为了评估Agentable，我们构建了两个数据集：AgentSet，包含84个真实世界的代理，以及AgentTest，其中包含78个专门设计以包含各种类型缺陷的代理。我们的结果表明，Agentable实现了88.79%的整体准确率和91.03%的召回率。此外，我们的分析揭示了AgentSet中的889个缺陷，突出了这些缺陷的普遍性。**|
|**2024-12-24**|**INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent**|Haohang Li et.al.|[2412.18174](http://arxiv.org/abs/2412.18174)|null|近期的发展突出了基于大型语言模型（LLM）的代理在金融决策中的潜力。尽管取得了这些进展，该领域目前面临两大挑战：（1）缺乏一个适用于各种金融任务的全面LLM代理框架，以及（2）缺乏用于评估代理性能的标准化基准和一致的数据集。为了解决这些问题，我们引入了InvestorBench，这是第一个专门为评估不同金融决策场景中基于LLM的代理而设计的基准。InvestorBench通过提供适用于不同金融产品（包括单一股票、加密货币和交易所交易基金（ETFs）等）的综合任务套件，增强了LLM启用代理的通用性。此外，我们使用十三种不同的LLM作为骨干模型，在多种市场环境和任务中评估了我们代理框架的推理和决策能力。此外，我们收集了多样化的开源、多模态数据集，并开发了适用于金融决策的综合环境套件。这为评估金融代理在各种场景下的性能提供了一个高度可访问的平台。|
|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|**大型语言模型（LLM）的快速发展和部署为人工智能领域带来了新的前沿，其自然语言理解和生成能力达到了前所未有的水平。然而，这些模型在关键应用中的日益整合引发了重大的安全问题，需要对其潜在风险和相关缓解策略进行彻底的审查。本调查全面概述了当前LLM安全领域的现状，涵盖了四大主要类别：价值偏差、对抗攻击的鲁棒性、滥用和自主AI风险。除了对这四个方面的缓解方法和评估资源的全面回顾外，我们还进一步探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提升LLM安全中的作用、提出并遵循的AI公司和研究机构为LLM安全制定的技术路线图，以及旨在LLM安全的AI治理，包括国际合作、政策建议和预期的监管方向。我们的研究发现，对LLM安全采取积极主动、多方面的方法是必要的，强调技术解决方案、伦理考量和稳健的治理框架的整合。本调查旨在为学术界研究人员、行业实践者和政策制定者提供基础资源，提供有关将LLM安全地融入社会的挑战和机遇的见解。最终，它旨在为LLM的安全和有益发展做出贡献，符合利用AI促进社会进步和福祉的总体目标。相关论文的精选列表已在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers上公开提供。**|
|**2024-12-23**|**LegalAgentBench: Evaluating LLM Agents in Legal Domain**|Haitao Li et.al.|[2412.17259](http://arxiv.org/abs/2412.17259)|**[link](https://github.com/cshaitao/legalagentbench)**|**随着LLM代理的智能和自主性不断提高，它们在法律领域的潜在应用越来越明显。然而，现有的通用领域基准无法完全捕捉现实世界司法认知和决策的复杂性和微妙之处。因此，我们提出了LegalAgentBench，这是一个专门设计用于评估LLM代理在中国法律领域的综合基准。LegalAgentBench包括来自现实世界法律场景的17个语料库，并提供37个用于与外部知识交互的工具。我们设计了一个可扩展的任务构建框架，并仔细标注了300个任务。这些任务涵盖了多种类型，包括多跳推理和写作，难度级别各异，有效地反映了现实世界法律场景的复杂性。此外，除了评估最终成功之外，LegalAgentBench还在中间过程中加入了关键词分析，以计算进度率，从而实现更细致的评估。我们评估了八种流行的LLM，突出了现有模型和方法的优势、局限性和潜在的改进领域。LegalAgentBench为LLM在法律领域的实际应用设定了新的基准，其代码和数据可在\url{https://github.com/CSHaitao/LegalAgentBench}获取。**|
|**2024-12-22**|**LLM Agent for Fire Dynamics Simulations**|Leidong Xu et.al.|[2412.17146](http://arxiv.org/abs/2412.17146)|null|在利用基础模型，如大型语言模型（LLM），来加速复杂科学工作流程方面取得了显著进展。在本工作中，我们引入了FoamPilot，这是一个概念验证LLM代理，旨在提升FireFOAM的使用便捷性。FireFOAM是一个专门用于火灾动力学和火灾抑制模拟的求解器，它是基于OpenFOAM构建的，OpenFOAM是一个流行的开源计算流体动力学（CFD）工具箱。FoamPilot提供了三个核心功能：代码洞察、案例配置和模拟评估。代码洞察是一种利用检索增强生成（RAG）作为替代传统关键词搜索的方法，旨在使开发者和经验丰富的用户能够高效地导航和总结FireFOAM源代码。对于案例配置，代理可以理解用户的自然语言请求，并旨在根据这些请求修改现有的模拟设置，以支持中级用户。FoamPilot的工作执行功能旨在管理高性能计算（HPC）环境中的模拟提交和执行，并为经验较少的用户提供模拟结果的初步分析。每个功能都取得了有希望的结果，尤其是在简单任务方面，并且对于更复杂的任务，识别出了显著的进一步改进的机会。将这些功能集成到一个单一的LLM代理中，旨在加速使用FireFOAM进行复杂模拟（这对于提高火灾安全性至关重要）的工程师和科学家的模拟工作流程。|
|**2024-12-21**|**The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents**|Feiran Jia et.al.|[2412.16682](http://arxiv.org/abs/2412.16682)|null|大型语言模型（LLM）代理正越来越多地被部署为能够通过工具集成执行复杂现实任务的对话助手。这种与外部系统交互和处理各种数据源的能力虽然强大，但也引入了重大的安全漏洞。特别是，间接提示注入攻击构成了严重威胁，其中嵌入在外部数据源中的恶意指令可以操纵代理偏离用户意图。虽然基于规则约束、来源突出显示和身份验证协议的现有防御措施显示出希望，但它们在保持任务功能的同时难以维持强大的安全性。我们提出了一种新颖且独立的视角，将代理安全从防止有害行为重新定义为确保任务一致性，要求每个代理动作都服务于用户目标。基于这一洞察，我们开发了任务盾（Task Shield），一种测试时的防御机制，该机制系统地验证每个指令和工具调用是否有助于实现用户指定的目标。通过在AgentDojo基准测试上的实验，我们证明了任务盾在GPT-4o上降低了攻击成功率（2.07%）的同时，保持了高任务效用（69.79%）。|
|**2024-12-19**|**On Verbalized Confidence Scores for LLMs**|Daniel Yang et.al.|[2412.14737](http://arxiv.org/abs/2412.14737)|**[link](https://github.com/danielyxyang/llm-verbalized-uq)**|**随着大型语言模型（LLMs）的兴起及其与日常生活的紧密融合，致力于其可信度变得至关重要。LLMs的不确定性量化可以增强人们对它们回答的信任，同时也使LLMs代理能够根据彼此的不确定性做出更明智的决策。为了估计回答中的不确定性，通常使用内部标记logits、针对特定任务的代理模型或多个响应的采样。这项工作侧重于让LLM自己用置信度分数作为输出标记的一部分来表达其不确定性，这是一种具有低开销、提示和模型无关的不确定性量化的有前景方法。使用广泛的基准，我们评估了这些置信度分数在不同数据集、模型和提示方法方面的可靠性。我们的结果表明，这些分数的可靠性强烈依赖于如何询问模型，但也表明可以通过某些提示方法提取出良好校准的置信度分数。我们认为，用言语表达的置信度分数可以成为未来简单但有效且通用的不确定性量化方法。我们的代码可在https://github.com/danielyxyang/llm-verbalized-uq上找到。**|
|**2024-12-19**|**Agent-SafetyBench: Evaluating the Safety of LLM Agents**|Zhexin Zhang et.al.|[2412.14470](http://arxiv.org/abs/2412.14470)|**[link](https://github.com/thu-coai/agent-safetybench)**|**随着大型语言模型（LLMs）作为代理的广泛应用，它们在交互环境和工具使用中的集成带来了超出模型本身相关的新的安全挑战。然而，缺乏用于评估代理安全性的全面基准，对有效评估和进一步改进构成了重大障碍。在本文中，我们介绍了Agent-SafetyBench，这是一个旨在评估LLM代理安全性的全面基准。Agent-SafetyBench包含349个交互环境和2000个测试案例，评估8类安全风险，涵盖在不安全交互中经常遇到的10种常见故障模式。我们对16个流行的LLM代理进行评估的结果令人担忧：没有一种代理的安全评分超过60%。这突显了LLM代理中的重大安全挑战，并强调了改进的巨大需求。通过定量分析，我们确定了关键故障模式，并总结了当前LLM代理中的两个基本安全缺陷：缺乏鲁棒性和缺乏风险意识。此外，我们的发现表明，仅依赖防御提示是不够解决这些安全问题的，强调了需要更先进和鲁棒的策略。我们将Agent-SafetyBench发布在\url{https://github.com/thu-coai/Agent-SafetyBench}，以促进代理安全评估和改进的进一步研究和创新。**|
|**2024-12-19**|**Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling**|Ziyi Ni et.al.|[2412.15305](http://arxiv.org/abs/2412.15305)|null|解决复杂推理任务是智能体在现实世界中的关键应用。得益于大型语言模型（LLMs）在代码数据上的预训练，最近的方法如CodeAct成功地使用代码作为LLM智能体的动作，取得了良好的效果。然而，CodeAct通过依赖零散的思维贪婪地生成下一个动作的代码块，导致结果不一致和不稳定。此外，CodeAct缺乏与动作相关的真实标签（GT），使得其在多轮交互中的监督信号和终止条件令人质疑。为了解决这些问题，我们首先介绍了一种简单而有效的端到端代码生成范式，名为CodeProgram，它利用代码的系统逻辑与全局推理保持一致，并实现连贯的问题解决。然后，我们提出了树形代码（ToC），它根据代码的可执行性自我增长CodeProgram节点，并在无GT的情况下实现自监督。在两个数据集上使用十个流行的零样本LLMs的实验结果表明，ToC在不到1/4轮的情况下将准确性提高了近20%，比CodeAct有显著提升。几个LLMs在一轮CodeProgram上的表现甚至优于多轮CodeAct。为了进一步研究有效性和效率之间的权衡，我们测试了不同的ToC树大小和探索机制。我们还强调了ToC端到端数据生成在监督和强化微调中的潜力。|
|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们每天都在与计算机互动，无论是在日常生活中还是工作中，许多工作都可以通过访问计算机和互联网来完成。同时，得益于大型语言模型（LLMs）的改进，与周围环境互动并产生影响的人工智能代理也迅速发展。但是，AI代理在帮助加速甚至自主执行工作相关任务方面的表现如何？这个问题的答案对于希望将AI融入其工作流程的行业以及了解AI采用对劳动力市场可能产生的影响的经济政策具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进展，本文介绍了TheAgentCompany，这是一个可扩展的基准，用于评估以类似数字工作者方式与世界互动的AI代理：通过浏览网页、编写代码、运行程序以及与同事沟通。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司的环境，并创建了一系列可能由该公司员工执行的任务。我们测试了由基于封闭API和开放权重的语言模型（LM）驱动的基线代理，发现最具有竞争力的代理可以使24%的任务实现自主完成。这描绘了一幅关于使用LLM代理进行任务自动化的复杂图景——在一个模拟真实工作场所的环境中，大部分简单任务可以自主解决，但更困难的长远任务仍然超出现有系统的范围。**|
|**2024-12-18**|**Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery**|ChengAo Shen et.al.|[2412.13667](http://arxiv.org/abs/2412.13667)|**[link](https://github.com/D2I-Group/matmcd)**|因果推理是跨越多个领域（如智能健康、药物发现AI和AIOps）决策的基础。传统的统计因果发现方法虽然已经建立，但主要依赖于观察数据，往往忽略了因果关系中固有的语义线索。大型语言模型（LLMs）的出现为利用语义线索进行知识驱动的因果发现提供了一种经济实惠的方法，但LLMs在因果发现领域的发展落后于其他领域，尤其是在多模态数据的探索方面。为了填补这一差距，我们引入了MATMCD，这是一个由工具增强的LLMs驱动的多智能体系统。MATMCD有两个关键智能体：一个数据增强智能体，用于检索和处理模态增强数据；以及一个因果约束智能体，用于集成多模态数据进行知识驱动的推理。内部工作的精心设计确保了智能体的成功合作。我们在七个数据集上的实证研究表明，多模态增强的因果发现具有显著的潜力。|
|**2024-12-18**|**Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution**|Ziyi Ni et.al.|[2412.14212](http://arxiv.org/abs/2412.14212)|null|大型语言模型（LLMs）的卓越能力极大地加速了代理的快速崛起和广泛应用。最近的研究表明，生成Python代码将基于LLMs的代理的动作整合到一个统一的行为空间（CodeAct）是开发现实世界LLMs代理的有前景的方法。然而，这种逐步代码生成方法往往缺乏一致性和鲁棒性，导致代理应用不稳定，尤其是在复杂推理和域外任务中。在本文中，我们提出了一种名为“代码树”（ToC）的新方法，通过端到端机制解决复杂问题规划和执行中的挑战。通过整合思维树和CodeAct的关键思想，ToC结合了它们的优势以增强解决方案的探索。在我们的框架中，每个最终的代码执行结果被视为决策树中的一个节点，采用广度优先搜索策略来探索潜在解决方案。最终结果通过基于节点输出的投票机制确定。|
|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身代理在执行复杂指令方面具有强大的能力，为具身机器人的潜在部署开辟了道路。然而，一个可预见的问题是，这些具身代理也可以完美地执行一些危险任务，可能在实际世界中造成损害。为了研究这个问题，我们提出了SafeAgentBench——一个用于具身LLM代理安全任务规划的新的基准。SafeAgentBench包括：（1）一个包含750个任务的新数据集，涵盖了10种潜在危险和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持多代理执行，并为8个最先进的基线提供了17个高级动作；（3）从执行和语义两个角度的可靠评估方法。实验结果表明，表现最佳的基线在安全任务中达到了69%的成功率，但在危险任务中只有5%的拒绝率，这表明存在显著的安全风险。更多细节和代码可在https://github.com/shengyin1224/SafeAgentBench找到。**|
|**2024-12-17**|**Memory-Augmented Agent Training for Business Document Understanding**|Jiale Liu et.al.|[2412.15274](http://arxiv.org/abs/2412.15274)|null|传统企业处理业务文档时面临重大挑战，尽管这些文档在物流运营中起着至关重要的作用，但像从发票中提取运输参考这样的任务仍主要依赖手工操作。虽然大型语言模型提供了自动化的潜力，但它们直接应用于特定商业领域时往往会产生不尽人意的结果。我们引入了Matrix（通过推理和迭代探索增强记忆的代理训练），这是一个新的范例，它使LLM代理能够通过经验驱动的记忆优化和迭代学习逐步构建领域专业知识。为了验证这种方法，我们与世界最大的物流公司之一合作，创建了一个包含通用商业语言格式发票文档的数据集，重点关注运输参考提取的任务。实验表明，Matrix的表现优于直接提示单个LLM 30.3%，优于普通LLM代理 35.2%。我们进一步分析了优化系统的指标，并观察到代理系统需要的API调用更少，成本更低，并且可以平均分析更长的文档。我们的方法通过在文档处理任务中系统地增强记忆，确立了一种将通用LLM转化为专业商业工具的新方法。|
|**2024-12-17**|**On the Structural Memory of LLM Agents**|Ruihong Zeng et.al.|[2412.15266](http://arxiv.org/abs/2412.15266)|**[link](https://github.com/zengrh3/StructuralMemory)**|记忆在使大型语言模型（LLM）代理能够参与复杂和长期交互，如问答（QA）和对话系统方面起着关键作用。尽管已经提出了各种记忆模块来完成这些任务，但不同记忆结构在任务之间的影响仍然没有得到充分探索。本文研究了记忆结构和记忆检索方法如何影响基于LLM的代理的性能。具体来说，我们评估了四种类型的记忆结构，包括块状结构、知识三元组、原子事实和摘要，以及混合记忆，它结合了这些组件。此外，我们还评估了三种广泛使用的记忆检索方法：单步检索、重排序和迭代检索。在四个任务和六个数据集上进行的广泛实验产生了以下关键见解：（1）不同的记忆结构具有不同的优势，使它们能够针对特定任务进行定制；（2）混合记忆结构在噪声环境中表现出显著的可适应性；（3）迭代检索在各种场景中始终优于其他方法。我们的研究旨在激励对基于LLM的代理的记忆系统设计进行进一步研究。|
|**2024-12-16**|**Codenames as a Benchmark for Large Language Models**|Matthew Stephenson et.al.|[2412.11373](http://arxiv.org/abs/2412.11373)|null|在本文中，我们提出将流行的基于单词的桌游Codenames用作评估大型语言模型（LLMs）推理能力的合适基准。Codenames为成功实现AI性能提出了一个极具挑战性的问题，需要复杂的语言理解、心智理论和认知推理能力。先前开发Codenames代理的尝试大多依赖于词嵌入技术，这些技术词汇范围有限，在与不同方法结合时表现不佳。LLMs在基于语言的任务中展现出增强的推理和理解能力，但在横向思维挑战中仍可能表现不佳。我们评估了包括GPT-4o、Gemini 1.5、Claude 3.5 Sonnet和Llama 3.1在内的几种最先进LLMs在各种棋盘布局下的能力。我们的结果表明，尽管某些LLMs在总体上表现优于其他LLMs，但不同的模型在游戏过程中表现出不同的涌现行为，并在特定角色上表现出色。我们还评估了不同LLMs组合在协同游戏中的表现，证明了LLM代理比先前技术更易于推广到更广泛的队友群体中。|
|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有普遍能力的AI代理提供了令人信服的基础。这些代理可能很快将在现实世界中大规模部署，代表个别人类（例如AI助手）或人类群体（例如AI加速的公司）的利益。目前，关于多个LLM代理在多代迭代部署中相互作用的动态知之甚少。在本文中，我们考察了在存在背叛动机的情况下，“LLM代理社会”是否能够学习相互有益的社会规范，这是人类社会性的一个独特特征，对于文明的成功可能至关重要。具体而言，我们研究了LLM代理在玩经典迭代捐赠游戏中的间接互惠的演变，在这个游戏中，代理可以观察到其同伴的最近行为。我们发现，合作的演变在不同基础模型之间存在显著差异，Claude 3.5 Sonnet代理的社会平均得分显著高于Gemini 1.5 Flash，而Gemini 1.5 Flash又优于GPT-4o。此外，Claude 3.5 Sonnet可以利用额外的成本惩罚机制来获得更高的分数，而Gemini 1.5 Flash和GPT-4o则不能。对于每个模型类别，我们还在随机种子之间观察到涌现行为的变异，这表明对初始条件的一种未被充分研究的敏感依赖。我们建议，我们的评估机制可以激发一种低成本且信息丰富的LLM基准测试新类别，重点关注LLM代理部署对社会合作基础设施的影响。|
|**2024-12-13**|**ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL**|Yang Qin et.al.|[2412.10138](http://arxiv.org/abs/2412.10138)|**[link](https://github.com/alibaba/route)**|**尽管大型语言模型（LLMs）在文本到SQL（Text2SQL）方面取得了显著进展，但最新的最先进技术仍然被困在封闭源LLMs（如GPT-4）的上下文学习中，这限制了它们在开放场景中的应用。为了应对这一挑战，我们提出了一种新的鲁棒多任务调整和协作方法（ROUTE），以提高开源LLMs在Text2SQL方面的综合能力，从而提供一个更实用的解决方案。我们的方法从使用与SQL生成相关的各种合成训练数据的任务监督微调（SFT）开始。与现有的基于SFT的Text2SQL方法不同，我们引入了几个额外的SFT任务，包括模式链接、噪声纠正和续写。参与各种SQL生成任务增强了模型对SQL语法的理解，并提高了其生成高质量SQL查询的能力。此外，受LLM代理协作模式启发，我们引入了多任务协作提示（MCP）策略。该策略利用跨多个SQL相关任务的协作来减少SQL生成过程中的幻觉，从而最大限度地发挥通过显式多任务能力增强Text2SQL性能的潜力。我们在八个开源LLMs和五个广泛使用的基准上进行了广泛的实验和深入分析。结果表明，我们的提议优于最新的Text2SQL方法，并取得了领先的性能。**|
|**2024-12-13**|**You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects**|Islem Bouzenia et.al.|[2412.10133](http://arxiv.org/abs/2412.10133)|**[link](https://github.com/sola-st/executionagent)**|在许多场景中，执行项目的测试套件至关重要，例如，评估代码质量和覆盖率，验证开发人员或自动化工具所做的代码更改，以及确保与依赖项的兼容性。尽管其重要性不言而喻，但在实际中执行项目的测试套件可能会面临挑战，因为不同的项目使用不同的编程语言、软件生态系统、构建系统和测试框架等工具。这些挑战使得创建一个在不同项目间都能可靠运行的全能测试执行方法变得困难。本文介绍了一种名为ExecutionAgent的自动化技术，该技术可以安装任意项目，配置它们运行测试用例，并生成特定于项目的脚本以重现设置。受人类开发者处理此类任务的方式启发，我们的方法是一个基于大型语言模型的代理，它可以自主执行命令并与宿主系统交互。该代理使用元提示来收集有关给定项目最新技术的指导，并根据前一步的反馈迭代优化其过程。我们的评估将ExecutionAgent应用于50个开源项目，这些项目使用了14种不同的编程语言以及许多不同的构建和测试工具。该方法成功执行了33/55个项目的测试套件，与基准测试套件执行结果匹配度仅为7.5%的偏差。这些结果比之前最好的技术提高了6.6倍。该方法带来的成本是合理的，平均每个项目的执行时间为74分钟，大型语言模型成本为0.16美元。我们期望ExecutionAgent成为开发者、自动化编程工具和研究人员执行各种项目测试的有价值工具。|
|**2024-12-11**|**TapeAgents: a Holistic Framework for Agent Development and Optimization**|Dzmitry Bahdanau et.al.|[2412.08445](http://arxiv.org/abs/2412.08445)|null|我们提出了TapeAgents，这是一个围绕细粒度、结构化代理会话日志带构建的代理框架，同时该日志带也充当会话的可恢复状态。在TapeAgents中，我们利用日志带来促进LLM代理开发生命周期的各个阶段。代理通过处理日志带和LLM输出来进行推理，生成新的思考和行动步骤，并将它们附加到日志带上。环境随后通过将观察步骤同样附加到日志带上来对代理的动作做出反应。凭借这种以日志带为中心的设计，TapeAgents可以为AI从业者提供全面端到端的支持。在开发阶段，日志带有助于会话持久化、代理审计和逐步调试。部署后，可以重用日志带进行评估、微调和提示调整；关键的是，可以从其他代理或使用修订的历史日志带进行适配。在本报告中，我们详细解释了TapeAgents的设计。我们通过构建单体代理和多代理团队、优化代理提示和微调代理的LLM等几个具体例子，展示了TapeAgents的可能应用。我们展示了工具原型，并报告了一个案例研究，其中我们使用TapeAgents微调Llama-3.1-8B表单填写助手，使其性能与GPT-4o相当，同时成本低得多。最后，我们的比较分析表明，TapeAgents相较于先前框架的优势源于我们对LLM代理作为可恢复、模块化状态机的创新设计，该设计具有结构化配置，可以生成细粒度、结构化的日志，并将这些日志转换为训练文本——这是以前工作中所缺少的独特功能组合。|
|**2024-12-11**|**Federated In-Context LLM Agent Learning**|Panlong Wu et.al.|[2412.08054](http://arxiv.org/abs/2412.08054)|null|大型语言模型（LLMs）通过实现逻辑推理、工具使用以及作为代理与外部系统交互，彻底改变了智能服务。LLMs的进步常常受到高质量数据稀缺性的阻碍，其中大部分数据本质上具有敏感性。联邦学习（FL）通过促进分布式LLMs的协作训练同时保护私有数据，提供了一个潜在的解决方案。然而，FL框架面临着显著的带宽和计算需求，以及来自异构数据分布的挑战。LLMs新兴的上下文学习能力提供了一种有希望的方法，通过聚合自然语言而不是庞大的模型参数。然而，这种方法存在隐私泄露的风险，因为它需要在聚合过程中收集和展示来自不同客户端的数据样本。在本文中，我们提出了一种新颖的隐私保护联邦上下文LLM代理学习（FICAL）算法，据我们所知，这是第一个利用上下文学习的能力通过FL来训练多样化的LLM代理的工作。在我们的设计中，由新颖的LLM增强的知识摘要生成（KCG）模块生成的知识库在客户端和服务器之间传输，而不是像之前的FL方法中那样传输模型参数。除此之外，我们还设计了一个基于检索增强生成（RAG）的工具学习和利用（TLU）模块，并将聚合的全局知识库作为教师来教授LLM代理工具的使用。我们进行了广泛的实验，结果表明，与现有的SOTA基线相比，FICAL具有竞争力的性能，并且通信成本降低了 $\mathbf{3.33\times10^5}$ 倍。|
|**2024-12-11**|**MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents**|Yun Xing et.al.|[2412.08014](http://arxiv.org/abs/2412.08014)|null|在驾驶场景中的物理对抗攻击可以揭示视觉感知模型的关键漏洞。然而，由于现实世界的多样性背景和保持视觉自然性的要求，开发此类攻击仍然具有挑战性。基于这一挑战，我们将物理对抗攻击重新定义为一次性的补丁生成问题。我们的方法通过一个考虑特定场景上下文的深度生成模型生成对抗补丁，使得可以直接在匹配环境中进行物理部署。主要挑战在于同时实现两个目标：生成能够有效误导目标检测系统的对抗补丁，并确定场景中的上下文适当的放置位置。我们提出了MAGIC（在上下文中掌握物理对抗生成），一个由多模态LLM代理驱动的创新框架，以应对这些挑战。MAGIC能够自动理解场景上下文，并通过语言和视觉能力的协同交互来协调对抗补丁的生成。MAGIC协调了三个专业的LLM代理：对抗补丁生成代理（GAgent）通过为文本到图像模型进行战略性的提示工程来掌握创建欺骗性补丁；对抗补丁部署代理（DAgent）通过基于场景理解确定最优放置策略来确保上下文一致性；自我审查代理（EAgent）通过提供对两个过程的批判性监督和迭代改进来完成这一三部曲。我们在数字和物理层面（即nuImage和手动捕获的真实场景）验证了我们的方法，统计和视觉结果均证明我们的MAGIC在攻击广泛使用的目标检测系统方面既强大又有效。|
|**2024-12-11**|**ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street Scenes**|Yuxi Wei et.al.|[2412.08685](http://arxiv.org/abs/2412.08685)|null|根据具体指令生成具有真实性和交互性的交通参与者动态对于街景模拟至关重要。然而，目前尚缺乏一种能够生成包括车辆和行人等不同类型参与者及其之间不同种类交互的全面方法。在本文中，我们介绍了ChatDyn，这是第一个能够根据语言指令在街景中生成交互、可控和真实参与者动态的系统。为了通过复杂语言实现精确控制，ChatDyn采用多LLM-agent角色扮演方法，利用自然语言输入来规划不同交通参与者的轨迹和行为。为了基于规划生成真实的细粒度动态，ChatDyn设计了两个新颖的执行器：PedExecutor，一个统一的多元任务执行器，能够在不同的任务规划下生成真实的行人动态；以及VehExecutor，一个基于物理过渡策略的执行器，生成符合物理学的车辆动态。大量的实验表明，ChatDyn可以生成包含多车辆和行人的真实驾驶场景动态，并且在子任务上显著优于之前的方法。代码和模型将在https://vfishc.github.io/chatdyn上提供。|
|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过重复的语言学习和使用而演变为结构化。这些过程在语言习得期间引入了偏见，并使语言系统趋向于沟通效率。在这篇论文中，我们研究了如果人工语言是为大型语言模型（LLMs）的隐式偏见进行优化时，是否会发生相同的情况。为此，我们模拟了一个经典指称游戏，其中LLMs学习和使用人工语言。我们的结果表明，最初无结构的整体语言确实被塑造出一些结构属性，使两个LLM代理能够成功沟通。与人类实验中的观察结果相似，代际传播增加了语言的易学性，但同时也可能导致非人类退化词汇。总的来说，这项工作扩展了实验发现，表明LLMs可以用作语言演化的模拟工具，并为该领域的未来人机实验开辟了可能性。|
|**2024-12-10**|**MAGE: A Multi-Agent Engine for Automated RTL Code Generation**|Yujie Zhao et.al.|[2412.07822](http://arxiv.org/abs/2412.07822)|**[link](https://github.com/stable-lab/MAGE-A-Multi-Agent-Engine-for-Automated-RTL-Code-Generation)**|**随着大型语言模型（LLMs）的发展，通过自然语言指令自动生成RTL代码（例如Verilog）已成为一个有前景的方向。然而，生成既符合语法又符合功能的RTL代码仍然是一个重大挑战。现有的单LLM代理方法面临重大局限性，因为它们必须在各种编程语言之间导航，并处理复杂的生成、验证和修改任务。为了解决这些挑战，本文介绍了MAGE，这是第一个开源的多代理人工智能系统，旨在实现鲁棒和精确的Verilog RTL代码生成。我们提出了一种新颖的高温RTL候选样本采样和调试系统，它有效地探索了代码候选空间，并显著提高了候选代码的质量。此外，我们设计了一种新颖的Verilog状态检查点检查机制，能够早期发现功能错误，并为有针对性的修复提供精确的反馈，显著提高了生成的RTL代码的功能正确性。MAGE在VerilogEval-Human 2基准测试中实现了95.7%的语法和功能正确性代码生成率，超过了最先进的Claude-3.5-sonnet，提高了23.3%，展示了人工智能驱动RTL设计工作流程的鲁棒和可靠方法。**|
|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|**我们研究了大型语言模型（LLMs）在自动生成数据清洗工作流程中的推理能力。为了评估LLMs完成数据清洗任务的能力，我们实现了一个基于LLM的自动数据清洗工作流程（AutoDCWorkflow）的管道，通过提示LLM进行数据清洗操作来修复三种类型的数据质量问题：重复数据、缺失值和不一致的数据格式。给定一个脏表和一个目的（以查询的形式表达），这个管道生成一个最小的、干净的表，足以解决目的并使用生成表的数据清洗工作流程。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的目标列集合。（2）检查列质量：评估每个目标列的数据质量并生成数据质量报告作为操作目标。（3）生成操作和参数：根据数据质量报告结果预测下一个操作和参数。此外，我们提出一个数据清洗基准来评估LLM代理自动生成解决不同难度级别数据清洗目的工作流程的能力。基准包括注解数据集，作为一个包含目的、原始表、干净表、数据清洗工作流程和答案集的集合。在我们的实验中，我们评估了三个自动生成目的驱动数据清洗工作流程的LLMs。结果表明，LLMs在规划和生成数据清洗工作流程方面表现良好，无需微调。**|
|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通运输系统需求建模与仿真领域，基于代理的模型和微观仿真是目前最先进的方法。然而，现有的基于代理的模型在行为真实性和资源需求方面仍存在一些局限性，这限制了它们的适用性。在本研究中，我们利用新兴的大语言模型（LLMs）和基于LLM的代理技术，提出了一种适用于交通运输系统的通用LLM-代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，而且为克服现有基于代理的模型的某些局限性提供了有希望的解决方案。我们的概念框架设计紧密模拟了人类旅行者在交通运输网络中的决策、交互过程和特征，并通过相关研究和LLM代理在瓶颈设置中的学习和调整的示范实例，证明了所提出的系统可以满足决策和学习的关键行为标准。尽管进一步优化LLM-代理建模框架是必要的，但我们相信这种方法有潜力改进交通运输系统的建模与仿真。|
|**2024-12-09**|**Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects**|Louis Milliken et.al.|[2412.06294](http://arxiv.org/abs/2412.06294)|**[link](https://github.com/coinse/installamatic)**|最近，许多研究提出了使用大型语言模型（LLM）构建的智能体来执行所谓的“仓库级”任务，这些任务的范围大于单个文件。这引发了一种推测，即这些仓库级任务的协调可能产生能够在很大程度上无需人工干预的软件工程智能体。然而，我们认为在需要由这个自主软件工程智能体执行的众多任务中，有一项重要任务缺失，那就是通过安装其他仓库来满足项目级别的依赖关系。为了调查这种仓库级安装任务的可行性，我们引入了一个基准，该基准由来自40个开源Python项目的仓库安装任务组成，包括每个目标仓库的基准安装过程。此外，我们提出了Installamatic智能体，该智能体的目标是通过在仓库文档中搜索相关说明来执行和验证给定仓库的安装。实证实验表明，55%的研究仓库至少有十分之一的时间可以通过我们的智能体自动安装。通过进一步分析，我们确定了导致我们的智能体无法安装仓库的常见原因，讨论了设计此类智能体时面临的挑战，并考虑了此类智能体可能对开发者产生的影响。|
|**2024-12-08**|**Cooperative SQL Generation for Segmented Databases By Using Multi-functional LLM Agents**|Zhiguang Wu et.al.|[2412.05850](http://arxiv.org/abs/2412.05850)|null|文本到SQL任务旨在根据用户的文本问题自动生成SQL查询。为了解决这个问题，我们提出了一种基于多功能代理（CSMA）的协同SQL生成框架，通过具有各自数据库模式部分的大语言模型（LLM）代理之间的信息交互来实现。受到人类团队合作协作的启发，CSMA包括三个阶段：1）与问题相关的模式收集，2）问题对应的SQL查询生成，3）SQL查询正确性检查。在第一阶段，代理分析各自的模式并相互通信，收集与问题相关的模式信息。在第二阶段，代理尝试使用收集到的信息为问题生成相应的SQL查询。在第三阶段，代理根据他们已知的信息检查SQL查询是否正确创建。这种基于交互的方法使得每个代理的问题相关部分数据库模式可用于SQL生成和检查。在Spider和Bird基准测试上的实验表明，CSMA达到了与现有技术水平相当的高性能，同时保持了这些个体代理中的私有数据。|
|**2024-12-06**|**Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models**|Da Ju et.al.|[2412.05093](http://arxiv.org/abs/2412.05093)|null|大型语言模型（LLMs）越来越多地被提议作为经典基于代理模型（ABMs）的有力替代，以模拟社会动态。通过将LLMs作为人类行为的代理，这种新方法希望通过模拟比经典ABMs更为复杂的动态，并在社会科学、政治科学和经济学等领域获得新的见解。然而，由于LLMs的“黑盒”性质，不清楚LLM代理是否实际上执行了编码在它们自然语言指令中的预期语义，以及由此产生的互动动态是否具有意义。为了研究这个问题，我们提出了一种新的评估框架，将LLM模拟建立在社会科学中已建立的参考模型的动态基础之上。我们将LLMs视为一个黑盒函数，评估它们的输入输出行为相对于这个参考模型，这使我们能够评估它们行为的详细方面。我们的结果表明，虽然可以通过设计提示来近似预期的动态，但这些模拟的质量高度依赖于特定提示的选择。重要的是，模拟甚至对任意的变异，如细微的文字变化和空格使用，都非常敏感。这引发了当前版本LLMs在有意义模拟中的有用性的质疑，因为没有参考模型，无法事先确定看似无意义的提示变化对模拟的影响。|
|**2024-12-06**|**Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System**|Fang Zeng et.al.|[2412.06828](http://arxiv.org/abs/2412.06828)|null|本研究介绍了“RadCouncil”，这是一个多智能体大型语言模型（LLM）框架，旨在增强放射学报告中发现部分的印象生成。RadCouncil由三个专业代理组成：1）一个“检索”代理，用于从向量数据库中识别和检索相似报告；2）一个“放射科医生”代理，根据给定报告中的发现部分以及检索代理检索到的示例报告生成印象；3）一个“审稿人”代理，评估生成的印象并提供反馈。RadCouncil的性能使用定量指标（BLEU、ROUGE、BERTScore）和由GPT-4评估的定性标准进行了评估，以胸部X光片作为案例研究。实验结果表明，在多个维度上，包括诊断准确性、风格一致性以及清晰度，RadCouncil相对于单代理方法都有所改进。这项研究强调了利用多个相互作用的LLM代理（每个代理都承担专用任务）来提高专业医疗任务性能和开发更稳健、适应性更强的医疗AI解决方案的潜力。|
|**2024-12-05**|**Practical Considerations for Agentic LLM Systems**|Chris Sypherd et.al.|[2412.04093](http://arxiv.org/abs/2412.04093)|null|近年来，随着大型语言模型（LLMs）的强大能力日益增长，对其作为自主代理基础模型的兴趣也随之增加。尽管LLMs在自然语言领域展现出涌现的能力和广泛的专业知识，但它们固有的不可预测性使得LLMs代理的实施变得具有挑战性，导致相关研究与这类系统的实际应用之间存在差距。为了弥合这一差距，本文将研究社区中的可操作见解和考虑因素置于既定应用范式之中，以促进稳健的LLMs代理的构建和明智的部署。具体而言，我们根据应用导向文献中的常见实践，将相关研究发现定位为四个广泛类别——规划、记忆、工具和控制流——并强调了在设计用于实际应用的代理型LLMs时需要考虑的实际因素，例如处理随机性和高效管理资源。虽然我们没有进行实证评估，但我们为讨论代理型LLMs设计的关键方面提供了必要的背景，无论是在学术界还是在工业界。|
|**2024-12-05**|**LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents**|Bingchen Li et.al.|[2412.04090](http://arxiv.org/abs/2412.04090)|null|我们提出了首个名为LossAgent的损失代理，用于低级图像处理任务，例如图像超分辨率和修复，旨在实现不同实际应用中低级图像处理的各种定制优化目标。值得注意的是，并非所有优化目标，如复杂的定制感知度量、文本描述和复杂的人类反馈，都能用现有的低级损失，例如均方误差损失（MSE loss），来实例化，这在端到端优化图像处理网络时提出了一个关键挑战。为了解决这个问题，我们的LossAgent引入了强大的大型语言模型（LLM）作为损失代理，丰富的先验知识文本理解赋予损失代理在低级图像处理网络优化过程中的复杂优化目标、轨迹和状态反馈的理解潜力。特别是，我们通过整合支持端到端优化低级图像处理现有损失函数建立了损失库。然后，我们为损失代理设计了面向优化的提示工程，使其在每个优化交互中能够积极和智能地决定库中每个损失的组合权重，从而实现任何定制优化目标所需的优化轨迹。在三个典型低级图像处理任务和多个优化目标上的大量实验表明了我们所提出的LossAgent的有效性和适用性。代码和预训练模型将在https://github.com/lbc12345/LossAgent上提供。|
|**2024-12-05**|**MISR: Measuring Instrumental Self-Reasoning in Frontier Models**|Kai Fronsdal et.al.|[2412.03904](http://arxiv.org/abs/2412.03904)|**[link](https://github.com/kaifronsdal/self-reasoning-evals)**|**我们提出了一套任务，用于评估大型语言模型（LLM）代理的工具体验推理能力。工具体验推理能力可以提高适应性和实现自我修改，但也可能带来重大风险，例如导致欺骗性对齐。先前的研究只评估了非代理环境或有限领域的自我推理。在本文中，我们提出了在广泛场景下，包括自我修改、知识寻求和模糊自我推理的代理任务中评估工具体验推理能力的方案。我们评估了使用最先进LLM构建的代理，包括商业和开源系统。我们发现，工具体验推理能力仅在最先进的边缘模型中体现，并且高度依赖于上下文。没有模型通过我们评估中最困难版本，因此我们的评估可以用于衡量未来模型工具体验推理能力的提升。我们在https://github.com/kaifronsdal/Self-Reasoning-Evals上开源了我们的评估。**|
|**2024-12-05**|**Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration**|Shiwen Ni et.al.|[2412.03847](http://arxiv.org/abs/2412.03847)|null|智能对话系统在现代教育和心理辅导领域得到越来越广泛的应用，但大多数现有系统局限于单一领域，无法同时处理教育和心理问题，并且在处理复杂问题时往往缺乏准确性和专业性。为了解决这些问题，本文提出了一种结合教育和心理辅导功能的智能对话系统。该系统由多个AI代理组成，包括安全检测代理、意图识别代理、教育LLM代理和心理LLM代理，它们协同工作以确保提供准确的教育知识问答和心理健康支持服务。具体来说，系统通过意图分类模型识别用户输入的意图，并调用增强检索的教育大型模型和心理大型模型（该模型已使用心理数据进行微调），以便提供专业的教育建议和心理健康支持。|
|**2024-12-03**|**Hacking CTFs with Plain Agents**|Rustem Turtayev et.al.|[2412.02776](http://arxiv.org/abs/2412.02776)|**[link](https://github.com/palisaderesearch/intercode)**|**我们通过简单的LLM代理设计饱和了一所高中水平的黑客基准测试。具体来说，我们通过提示、工具使用和多次尝试，在InterCode-CTF这个流行的进攻性安全基准测试上获得了95%的性能。这一成绩超过了Phuong等人2024年（29%）和Abramovich等人2024年（72%）的研究成果。我们的结果表明，当前的大型语言模型在进攻性网络安全方面已经超越了高中水平。它们的黑客能力尚未得到充分发掘：我们提出的ReAct&Plan提示策略在1-2个回合内解决了许多挑战，无需复杂的工程或高级利用。**|
|**2024-12-02**|**HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing**|Lajos Muzsai et.al.|[2412.01778](http://arxiv.org/abs/2412.01778)|**[link](https://github.com/aielte-research/HackSynth)**|**我们介绍了HackSynth，这是一种基于大型语言模型（LLM）的全新自主渗透测试代理。HackSynth的双模块架构包括一个规划器和总结器，这使得它能够迭代地生成命令和处理反馈。为了对HackSynth进行基准测试，我们提出了两个基于Capture The Flag（CTF）的新基准集，利用流行的平台PicoCTF和OverTheWire。这些基准集涵盖了200个不同领域和难度的挑战，为评估基于LLM的渗透测试代理提供了一个标准化的框架。基于这些基准，我们展示了广泛的实验，分析了HackSynth的核心参数，包括创新性（温度和top-p）和标记利用。我们使用了多个开源和专有LLM来衡量代理的能力。实验表明，该代理在GPT-4o模型下表现最佳，优于GPT-4o的系统卡片所建议的。我们还讨论了HackSynth行动的安全性和可预测性。我们的发现表明，基于LLM的代理在推进自主渗透测试方面的潜力，以及稳健保障的重要性。HackSynth和基准集公开可用，以促进自主网络安全解决方案的研究。**|
|**2024-12-02**|**Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking**|Jie Liu et.al.|[2412.01605](http://arxiv.org/abs/2412.01605)|null|临床决策（CDM）是医疗保健服务中复杂且动态的过程，但对于人工智能系统来说仍然是一个重大挑战。尽管基于大型语言模型（LLM）的智能体已在执照考试和知识问答任务中测试了一般医学知识，但由于缺乏反映实际医疗实践的全面测试数据集，它们在现实场景中的CDM表现有限。为了解决这一差距，我们提出了MedChain，一个包含12,163个临床案例的数据集，涵盖了临床工作流程的五个关键阶段。MedChain与现有基准相比，具有三个反映现实临床实践的显著特点：个性化、交互性和顺序性。此外，为了应对现实世界中的CDM挑战，我们还提出了MedChain-Agent，这是一个集成了反馈机制和MCase-RAG模块的人工智能系统，以便从以往案例中学习并调整其响应。MedChain-Agent在动态收集信息和处理顺序临床任务方面表现出惊人的适应性，显著优于现有方法。在本文被接受后，将发布相关数据集和代码。|
|**2024-12-02**|**Can Large Language Models Serve as Evaluators for Code Summarization?**|Yang Wu et.al.|[2412.01333](http://arxiv.org/abs/2412.01333)|**[link](https://github.com/CGCL-codes/naturalcc)**|**代码摘要通过将代码片段转换为自然语言描述，有助于程序理解和软件维护。多年来，为这项任务开发了众多方法，但一个关键挑战仍然存在：有效地评估生成摘要的质量。虽然人工评估在评估代码摘要质量方面是有效的，但它劳动密集且难以扩展。常用的自动指标，如BLEU、ROUGE-L、METEOR和BertScore，通常与人工判断的关联性不强。在本文中，我们探讨了大型语言模型（LLMs）在评估代码摘要方面的潜力。我们提出了CODERPE（代码摘要评估中的角色扮演者），这是一种利用角色扮演提示来评估生成摘要质量的新方法。具体来说，我们提示LLM代理扮演各种角色，如代码审阅者、代码作者、代码编辑和系统分析师。每个角色从关键维度评估代码摘要的质量，包括连贯性、一致性、流畅性和相关性。我们进一步通过采用各种提示策略，包括思维链推理、情境学习和定制评分表设计，探索了LLMs作为评估者的鲁棒性。结果表明，LLMs作为代码摘要方法的有效评估者。值得注意的是，我们的基于LLM的评估器CODERPE与人工评估的斯皮尔曼相关系数为81.59%，比现有的BERTScore指标高17.27%。**|
|**2024-12-02**|**RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks**|Xu Yang et.al.|[2412.01303](http://arxiv.org/abs/2412.01303)|null|随着大规模分布式能源资源被集成到主动配电网络（ADN）中，与传统的配电网络相比，在ADN中实现有效的能源管理变得越来越突出。尽管先进的强化学习方法（RL）极大地提高了ADN能源管理的效率，减轻了复杂建模和优化的负担，但安全性成为RL在实际问题应用中的关键关注点。由于与操作安全约束相对应的惩罚函数的设计和调整需要RL和电力系统操作方面的广泛领域知识，新兴的ADN运营商呼吁采用更灵活和定制化的方法来解决惩罚函数，以便进一步提高操作安全和效率。凭借强大的理解、推理和在上下文中学习的能力，大型语言模型（LLM）为辅助ADN能源管理的安全RL提供了一种有希望的途径。在本文中，我们引入LLM来理解ADN中的操作安全要求并生成相应的惩罚函数。此外，我们提出了一种RL2机制，通过多轮对话迭代和自适应地改进生成的函数，其中LLM代理根据下游RL代理的训练和测试性能调整函数的模式和参数。所提出的方法显著减少了ADN运营商的干预。综合测试结果证明了所提出方法的有效性。|
|**2024-12-02**|**SAUP: Situation Awareness Uncertainty Propagation on LLM Agent**|Qiwei Zhao et.al.|[2412.01033](http://arxiv.org/abs/2412.01033)|null|大型语言模型（LLMs）集成到多步骤智能体系统中，能够在各种应用中实现复杂的决策过程。然而，它们的输出通常缺乏可靠性，因此不确定性估计变得至关重要。现有的不确定性估计方法主要关注最终步骤的输出，未能考虑到多步骤决策过程中的累积不确定性和智能体与其环境之间的动态交互。为了解决这些局限性，我们提出了SAUP（情境感知不确定性传播），这是一个新颖的框架，它通过LLM智能体推理过程的每一步传播不确定性。SAUP通过在传播过程中为每一步的不确定性分配情境权重来融入情境感知。我们的方法与各种单步不确定性估计技术兼容，提供了一个全面且准确的不确定性度量。在基准数据集上的大量实验表明，SAUP显著优于现有最先进的方法，实现了AUROC达到20%的提升。|
|**2024-11-28**|**Using a Feedback Loop for LLM-based Infrastructure as Code Generation**|Mayur Amarnath Palavalli et.al.|[2411.19043](http://arxiv.org/abs/2411.19043)|**[link](https://github.com/Mayur-Palavalli/LLM-IaC-generation)**|**使用大型语言模型（LLMs）进行代码生成有助于提高软件开发者在编码任务中的生产力，但尚未对围绕这些代码的软件开发者的任务产生重大影响。特别是，基础设施管理的挑战仍然是一个未解之谜。我们研究了LLM代理利用基础设施即代码（IaC）范式构建基础设施的能力。我们特别研究了使用反馈循环，该循环会返回生成的IaC的错误和警告，以允许LLM代理改进代码。我们发现，对于循环的每一次迭代，其有效性都会呈指数下降，直到达到某个点并趋于平稳，最终变得无效。**|
|**2024-11-28**|**MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications**|Vishnou Vinayagame et.al.|[2411.18915](http://arxiv.org/abs/2411.18915)|null|随着工具增强的语言代理的数学推理能力不断增强，但现有方法通常依赖于闭源或大型模型、外部数据或大量的提示工程。这项工作介绍了一种名为MATATA的新颖且经济高效的方法，通过推理、规划和工具使用来训练LLM代理解决表格数据问题。它采用渐进式自我改进范式和迭代式弱监督，赋予了38亿/80亿小语言模型（SLM）的能力，特别适合于本地托管和敏感的商业环境，在这些环境中数据隐私至关重要。通过在不同数据集上使用灵活且可重用的工具，它实现了在共享任务上的有效可扩展性。实验表明，MATATA在基于开源模型的推理框架中，在FinQA和TAT-QA上达到了最先进的性能。此外，MATATA模型在TabMWP上与基于GPT-4的框架竞争，同时仍然是SLM。|
|**2024-11-27**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴无声语音系统在恢复言语障碍患者的沟通方面具有巨大潜力。然而，无缝、连贯的语音仍然难以实现，临床疗效尚未得到证实。在此，我们提出了一种由人工智能驱动的智能喉部（IT）系统，该系统集成了喉部肌肉振动和颈动脉脉搏信号传感器以及大型语言模型（LLM）处理，以实现流畅、富有情感表达的沟通。该系统利用超灵敏的纺织应变传感器从颈部区域捕捉高质量信号，并支持token级别的处理，以实现实时、连续的语音解码，从而实现无缝、无延迟的通信。在测试中，使用五种言语障碍的卒中患者进行测试，IT的LLM智能代理智能地纠正token错误，丰富句子级情感和逻辑连贯性，实现了低错误率（4.2%的词错误率，2.9%的句子错误率）和用户满意度55%的增长。这项工作为言语障碍患者建立了一个便携、直观的通信平台，有望广泛应用于不同的神经学条件和多语言支持系统。|
|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在各种领域，包括机器人操作和导航，展现了出色的规划能力。虽然最近在机器人领域的努力已经利用LLMs进行高级和低级规划，但这些方法通常面临重大挑战，如长期任务中的幻觉以及由于在单次生成计划时缺乏实时反馈而导致的适应性有限。为了解决这些限制，我们提出了一种新的多智能体LLM框架，即用于操作的智能体大型语言模型（MALMM），它将高级规划和低级控制代码生成分配给专门的LLM智能体，并由一个额外的智能体动态管理转换。通过在每一步后纳入环境观察，我们的框架有效地处理了中间失败，并实现了适应性重新规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或在上下文中学习的示例，并且可以推广到各种新的任务。我们在包括长期任务在内的九个RLBench任务上评估了我们的方法，并展示了其在零样本设置下解决机器人操作的能力，从而克服了现有基于LLM的操作方法的局限性。|
|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理建模（ABM）已成为模拟社交网络的重要工具，涵盖了诸如信息传播、影响力动态和社区形成等多种现象。然而，手动配置各种代理交互和信息流动态带来挑战，往往导致模型过于简化，缺乏现实世界的普适性。将现代大型语言模型（LLM）与ABM相结合为解决这些挑战和提升模拟真实度提供了一条有希望的途径，利用LLM在感知、推理和行为方面类似人类的能力。在本文中，我们提出了一种新颖的框架，该框架利用LLM赋能的代理根据用户的兴趣和个性特征模拟社交网络用户。该框架允许定制代理交互，类似于各种社交网络平台，包括内容重新分享和个性化推荐机制。我们使用2020年美国选举的全面Twitter数据集验证了我们的框架，证明LLM代理能够准确复制真实用户的言行，包括语言模式和政治倾向。这些代理形成了同质化的意识形态集群，并保留了其社区的主要主题。值得注意的是，基于偏好的推荐对代理行为有显著影响，促进了更高的参与度、网络同质性以及回音室的形成。总体而言，我们的发现突出了LLM代理在推进社交媒体模拟和揭示复杂在线动态中的潜力。|
|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大型语言模型（LLMs）和强化学习（RL）是构建自主智能体的两种强大方法。然而，由于对游戏环境的理解有限，智能体往往依赖低效的探索和试错，难以发展长期策略或做出决策。我们提出了一种方法，通过从交互记录中提取经验来模拟游戏环境的潜在规律，并利用这些经验作为内部动机来引导智能体。这些经验以语言形式表达，非常灵活，既可以直接协助智能体进行推理，也可以转化为训练中的奖励。在Crafter上的评估结果显示，RL和LLM智能体都从这些经验中受益，从而提高了整体性能。|
|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着自然语言生成模型（称为大型语言模型，LLMs）的近期发展，一种潜在的应用场景得以开启，即改进人类与机器人助手互动的方式。这些LLMs应能利用其广泛的理解能力，将自然语言命令解释为有效、符合任务和安全的机器人任务执行。然而，在现实中，这些模型存在幻觉问题，可能会引起安全问题或偏离任务。在其他领域，这些问题已通过使用协作人工智能系统得到改善，在该系统中，多个LLM代理可以共同规划、编码和自我检查输出。在本研究中，通过将多个协作人工智能系统与单个独立人工智能代理进行对比测试，以确定在其他领域的成功是否能够转化为改进的人机交互性能。结果显示，代理数量与模型的成功率之间没有明确的趋势。然而，很明显，某些协作人工智能代理架构可以显著提高生成无错误代码和解决抽象问题的能力。|
|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|人工智能在进行自动化信息判断任务时是否会产生认知偏差？尽管近期在衡量和缓解人工智能和大型语言模型（LLMs）中的社会和算法偏差方面取得了进展，但LLMs在多大程度上表现出“理性”行为，或者它们是否也容易受到人类认知偏差的触发，仍不明确。为了解决这个未解问题，我们的研究包括一个众包用户实验和一个LLM驱动的模拟实验，比较了在信息检索（IR）环境中，LLMs和人类评判员在潜在诱饵效应下的可信度评估，并实证研究了与传统的基于人类评估的基线相比，LLMs在COVID-19医疗（误）信息评估任务中的认知偏差程度。来自跨主体用户实验和LLM驱动的重复实验的结果表明：1）更大、更新版的LLMs在区分可信信息和虚假信息方面表现出更高的一致性和准确性。然而，由于存在更显著、更具诱饵性质的虚假信息结果，它们更有可能给予虚假信息更高的评分；2）虽然诱饵效应在人类和LLMs的评估中都发生了，但在LLMs的判断中，这种效应在不同条件和主题下比人类的可信度评分更为普遍。与普遍认为的AI工具的“理性”相反，我们的研究从实证上确认了LLM代理中嵌入的认知偏差风险，评估了诱饵对LLMs与人类可信度评估的影响，从而突出了去偏差AI代理、开发心理学导向的AI审计技术和政策（用于自动化判断任务及更多领域）的复杂性和重要性。|
|**2024-11-22**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|LLM代理的应用正变得越来越复杂和多样化，导致对可以解析为代码、结构化函数调用和具身代理命令的规范化输出的需求日益增长。这些发展对LLM推理中的规范化生成提出了重大需求。无上下文文法是一种灵活的方法，通过限制解码来实现规范化生成。然而，执行无上下文文法需要在运行时遍历词汇表中所有标记的多个栈状态，给规范化生成带来不可忽视的开销。在本文中，我们提出了XGrammar，这是一个灵活且高效的LLM结构生成引擎。XGrammar通过将词汇表划分为可预检查的上下文无关标记和需要运行时解释的上下文相关标记来加速无上下文文法的执行。我们进一步构建了转换来扩展语法上下文并减少上下文无关标记的数量。此外，我们构建了一个高效的持久栈来加速上下文相关标记的检查。最后，我们与LLM推理引擎协同设计语法引擎，以重叠语法计算与GPU执行。评估结果显示，XGrammar可以比现有解决方案实现高达100倍的加速。结合LLM推理引擎，它可以在端到端低LLM服务中实现近乎零开销的结构化生成。|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|**[link](https://github.com/colonylabs/ScribeAgent)**|大型语言模型（LLM）代理正在迅速提升以处理越来越复杂的基于网络的任务。这些代理中的大多数依赖于通用、专有的模型如GPT-4，并专注于设计更好的提示以提升它们的规划能力。然而，通用LLM并未专门训练以理解专门的网络上下文，如HTML，并且它们通常在长期规划方面遇到困难。我们探索了一种替代方法，即使用从超过250个域名收集的生产规模工作流程数据对开源LLM进行微调，这些域名对应60亿个标记。这种方法简单而有效，在现有基准测试中相对于基于提示的代理显示了显著的优势——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上比之前最佳的文字型网络代理提高了14.1%的任务成功率。我们还对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练配方、上下文窗口优化以及数据集大小影响等方面的见解。|
|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于LLM的自主代理在解决复杂工业任务方面表现出卓越的性能。然而，在追求碳中和和高性能可再生能源系统的过程中，现有的AI辅助设计自动化在可解释性、可扩展性和可用性方面面临着重大限制。为了解决这些挑战，我们提出了LP-COMDA，这是一个基于LLM的、物理信息丰富的自主代理，能够在最小人工监督下自动化电力电子系统中电力转换器的调制设计。与传统的AI辅助方法不同，LP-COMDA包含一个基于LLM的规划器，通过用户友好的聊天界面收集和验证设计规范。规划器随后与物理信息设计优化工具协调，自主迭代生成和优化调制设计。通过聊天界面，LP-COMDA提供可解释的设计过程，展示解释和图表。实验表明，LP-COMDA优于所有基线方法，在标准均方绝对误差方面，与第二好的基准方法相比，误差降低了63.2%。此外，对20位专家的实证研究表明，使用LP-COMDA的设计时间是传统方法的33倍以上，显示出其在设计效率方面的显著改进。|
|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在（多模态）大型语言模型时代，大多数操作流程都可以通过LLM智能体进行重构和再现。LLM智能体能够感知、控制和从环境中获取反馈，以自主方式完成给定任务。除了环境交互特性外，LLM智能体还可以调用各种外部工具以简化任务完成过程。这些工具可以被视为包含私有或实时知识且不存在于LLM参数中的预定义操作流程。作为发展的自然趋势，调用工具的智能体正成为自主智能体，因此完整的智能系统最终变成了多LLM智能体系统（MLAS）。本文讨论了MLAS的技术和商业格局。与之前的单一LLM智能体系统相比，MLAS具有以下优势：i) 更高的任务解决性能潜力；ii) 更高的系统变化灵活性；iii) 为每个参与实体保留专有数据；iv) 为每个实体实现货币化的可行性。为了支持MLAS生态系统，我们提供了一个考虑技术要求、数据隐私和商业激励的MLAS协议的初步版本。因此，MLAS将成为实现未来人工集体智慧的实用解决方案。|

<p align=right>(<a href=#updated-on-20251017>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-07-23**|**BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems**|Malsha Ashani Mahawatta Dona et.al.|[2507.17722](http://arxiv.org/abs/2507.17722)|null|大型语言模型（LLMs）越来越多地被扩展以同时处理多模态数据，如文本和视频。它们在理解图像中展示内容方面的出色表现已经超越了仅支持良好但词汇量非常有限的专门神经网络（NNs），例如仅能检测到特定对象的Yolo。当不受限制时，LLMs特别是最先进的视觉语言模型（VLMs）在描述甚至复杂的交通情况方面表现出令人印象深刻的能力。这使得它们可能成为汽车感知系统的潜在组件，以支持对复杂交通情况或边缘情况的了解。然而，LLMs和VLMs容易产生幻觉，这意味着要么可能看不到场景中实际存在的交通参与者，如易受伤害的道路使用者，要么看到现实中并不存在的交通参与者。虽然后者是不希望的，会使高级驾驶辅助系统（ADAS）或自动驾驶系统（ADS）不必要地减速，但前者可能导致ADS做出灾难性的决策。在我们的工作中，我们系统地评估了3个最先进的VLMs在从Waymo Open Dataset中抽取的多样化交通情况子集上的性能，以支持安全防护措施，捕捉VLM支持的感知系统中的此类幻觉。我们观察到，无论是专有还是开源的VLMs，即使对人类有时难以察觉的细微细节也给予了充分的关注，都表现出卓越的图像理解能力。然而，它们仍然容易在描述中添加虚构元素，至今仍需要我们提出如BetterCheck之类的幻觉检测策略。|
|**2025-07-23**|**AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer**|Danny D. Leybzon et.al.|[2507.17718](http://arxiv.org/abs/2507.17718)|null|随着语音赋能的人工智能（AI）系统的兴起，定量调查研究人员可以采用一种新的数据收集方式：AI电话调查。通过使用AI进行电话访谈，研究人员可以在平衡类似人类的互动性和方法论严谨性的双重目标的同时，扩大定量研究规模。与早期使用交互式语音响应（IVR）技术自动进行这些调查的努力不同，语音AI能够提供更自然和适应性强的受访者体验，因为它更能应对中断、纠正和其他人类语音的个性特征。我们构建并测试了一个基于大型语言模型（LLM）、自动语音识别（ASR）和语音合成技术的AI系统，用于进行定量调查。该系统专门为定量研究设计，并严格遵循研究最佳实践，如问题顺序随机化、答案顺序随机化以及精确措辞。为了验证系统的有效性，我们将其部署用于进行两项针对SSRS意见小组的试点调查，并随后进行了一项单独的人工管理调查，以评估受访者的体验。我们测量了三个关键指标：调查完成率、中断率和受访者满意度评分。我们的结果表明，较短的问卷和更具响应性的AI访谈员可能有助于提高所有三个研究指标。|
|**2025-07-23**|**HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging**|Taha Ceritli et.al.|[2507.17706](http://arxiv.org/abs/2507.17706)|null|大型语言模型（LLMs）通常利用适配器，如基于低秩的适配器，以在下游任务上实现强大的性能。然而，为每个任务存储一个独立的适配器会显著增加内存需求，对资源受限的环境，如移动设备，构成了挑战。尽管模型合并技术可以降低存储成本，但它们通常会导致性能大幅下降。在本工作中，我们引入了HydraOpt，这是一种新的模型合并技术，它利用了低秩适配器矩阵之间的固有相似性。与现有方法不同，这些方法在存储大小和性能之间产生一个固定的权衡，HydraOpt使我们能够在这个效率和性能谱中导航。我们的实验表明，与存储所有适配器相比，HydraOpt显著减少了存储大小（减少48%），同时实现了具有竞争力的性能（性能下降0.2-1.8%）。此外，它在相同或略低的存储效率下，优于现有的合并技术。|
|**2025-07-23**|**Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models**|Changxin Tian et.al.|[2507.17702](http://arxiv.org/abs/2507.17702)|null|混合专家（MoE）架构通过将总参数与计算成本解耦，已经成为一种有效地扩展大型语言模型（LLMs）的主流架构。然而，这种解耦产生了关键挑战：预测给定MoE配置（例如专家激活比和粒度）的模型容量仍然是一个未解决的问题。为了解决这一差距，我们引入了效率杠杆（EL），这是一个量化MoE模型相对于密集等效模型的计算优势的指标。我们进行了一项大规模的实证研究，训练了超过300个模型，参数量达到28B，系统地调查了MoE架构配置与EL之间的关系。我们的发现表明，EL主要由专家激活比和总计算预算驱动，两者都遵循可预测的幂律，而专家粒度作为一个非线性调节器，具有一个明显的最优范围。我们将这些发现整合到一个统一的扩展定律中，该定律可以准确地根据MoE架构的配置预测其EL。为了验证我们推导出的扩展定律，我们设计并训练了Ling-mini-beta，这是Ling-2.0系列的一个仅含0.85B活跃参数的试点模型，并附带一个6.1B的密集模型进行对比。当在相同的1T高质量标记数据集上训练时，Ling-mini-beta与6.1B密集模型的性能相当，而消耗的计算资源却少超过7倍，从而证实了我们的扩展定律的准确性。这项工作为高效MoE模型的扩展提供了一个基于原理和经验基础的坚实基础。|
|**2025-07-23**|**Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations**|Zhao Song et.al.|[2507.17699](http://arxiv.org/abs/2507.17699)|null|大型推理模型（LRMs）在当今的大语言模型（LLM）研究中成为了一个核心焦点，这些模型被设计成在得出最终答案之前输出一步一步的思考过程，以处理复杂的推理任务。尽管它们具有潜力，但最近的一些实证研究（例如，来自苹果公司的[Shojaee等人，2025]）表明，这种思考过程实际上可能并没有增强推理能力，在没有明确推理的LLM上，在低复杂度或高复杂度的任务上甚至优于LRMs。在本研究中，我们重新审视了这些发现，并调查了当引入工具增强时，LRMs的局限性是否仍然存在。我们引入了两种类型的工具，Python解释器和草稿本，并评估了三个代表性的LLMs及其LRM对应版本在苹果公司的基准推理谜题上的表现。我们的结果表明，在适当使用工具的情况下，LRMs在所有任务复杂度级别上均持续优于其非推理对应版本。这些发现挑战了最近关于推理是幻觉的叙述，并突出了工具增强的LRMs解决复杂问题的潜力。|
|**2025-07-23**|**Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks**|Ilias Chatzistefanidis et.al.|[2507.17695](http://arxiv.org/abs/2507.17695)|null|基于大型语言模型（LLM）的自主代理预计将在6G网络的发展中发挥关键作用，通过赋予与管理和服务提供相关的实时决策能力。这种转变促进了从专门智能方法（其中人工智能算法处理孤立的任务）向由通用人工智能（AGI）驱动的网络的过渡，在这些网络中，代理具有更广泛的推理能力，并能管理多种网络功能。在本文中，我们介绍了一种新的代理范式，它将LLM与实时优化算法相结合，以实现可信赖的AI，即共生代理。LLM输入级别的优化器为数值精确的任务提供有界的不确定性引导，而由LLM监督的输出级别优化器则实现自适应实时控制。我们设计和实现了两种新型代理类型，包括：（i）无线接入网络优化器；（ii）服务级别协议（SLA）的多代理谈判者。我们进一步提出了一种面向AGI网络的端到端架构，并在一个5G测试台上对其进行评估，该测试台捕捉了移动车辆的信道波动。结果表明，共生代理与独立基于LLM的代理相比，决策错误减少了五倍，而较小的语言模型（SLM）在GPU资源开销减少了99.9%和近实时循环82毫秒的情况下实现了类似的准确性。在真实世界测试台上进行的协作RAN多代理演示突出了在服务级别协议和资源分配方面的显著灵活性，将RAN过度使用减少了约44%。基于我们的发现和开源实现，我们引入共生范式作为下一代AGI驱动网络的基石，这些网络系统旨在即使在LLM不断进步的情况下也能保持适应性、效率和可信赖性。|
|**2025-07-23**|**Simulating multiple human perspectives in socio-ecological systems using large language models**|Yongchao Zeng et.al.|[2507.17680](http://arxiv.org/abs/2507.17680)|null|理解社会-生态系统需要从不同利益相关者的视角中获得见解，而这些视角往往难以获取。为了实现基于模拟的替代探索，我们开发了HoPeS（面向人类视角转换）建模框架。HoPeS使用由大型语言模型（LLMs）驱动的代理来代表不同的利益相关者；用户可以进入代理角色来体验视角差异。一个模拟协议作为“支架”以简化多个视角转换模拟，支持用户反思、转换和整合不同视角。一个原型系统被开发出来以展示HoPeS在制度动态和土地利用变化背景下的应用，使叙事驱动和数值实验成为可能。在一个示范实验中，用户依次采取系统观察者和研究者的视角——这个角色分析嵌入的土地利用模型中的数据，为代表不同机构的其他LLM代理提供基于证据的决策。尽管用户努力推荐技术可行的政策，但由于利益相关者的竞争主张，政策建议与实施之间存在差异，反映了现实世界中研究人员和政策制定者视角之间的不匹配。用户的反思突显了作为研究人员的挫折感和失望情绪，尤其是在尝试获得政治影响力同时保持政治中立性的挑战。尽管如此，用户表现出高度动机去尝试不同的叙事框架策略，表明系统在探索不同视角方面的潜力。进一步系统和完善协议可能有助于在社会-生态系统模拟中实现新的跨学科合作形式。|
|**2025-07-23**|**See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering**|Junjie Wang et.al.|[2507.17659](http://arxiv.org/abs/2507.17659)|null|多模态大型语言模型（MLLMs）推动了基于知识的视觉问答（KBVQA）的边界，但它们的推理本质上是受限于对单维证据的依赖。这种“只见树木，不见森林”的方法阻止了稳健、多方面的理解。受“既见森林又见树木”的原则启发，我们提出了Synergos-VQA，一个新颖的协同推理框架。在其核心，Synergos-VQA在推理时同时生成和融合三个互补的证据流：（1）整体证据以感知整个场景（即“森林”），（2）来自原型驱动模块的结构证据以识别关键对象（即“树木”），（3）来自反事实探针的因果证据以确保推理的稳健性。通过协同融合这些多方面的证据，我们的框架实现了更全面、更可靠的推理过程。广泛的实验表明，Synergos-VQA在包括OK-VQA和A-OKVQA在内的三个具有挑战性的基准上确立了新的最先进水平。此外，我们的方法展示了强大的即插即用能力，显著提升了各种开源MLLMs，并证明优越的方法设计可以超越单纯的模型规模。|
|**2025-07-23**|**Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries**|Victor Hartman et.al.|[2507.17636](http://arxiv.org/abs/2507.17636)|null|负面的竞选活动是政治竞争的核心特征，但现有的分类方法因其高昂的成本和有限的扩展性而限制了实证研究。本研究做出了两个主要贡献。首先，它引入了零样本大型语言模型（LLMs）作为跨语言分类负面竞选活动的一种新方法。使用十种语言的基准数据集，我们证明LLMs的性能与母语人类编码者相当，并优于传统的监督机器学习方法。其次，我们利用这种新颖的方法进行迄今为止最大的跨国家负面竞选研究，分析了2017年至2022年间19个欧洲国家的议员发布的1800万条推文。结果显示了跨国家的一致模式：执政党不太可能使用负面信息，而意识形态极端和民粹主义政党——尤其是那些激进的右翼政党——表现出显著更高的负面程度。这些发现加深了我们对于政党层面特征如何塑造多党制体系中的战略沟通的理解。更广泛地说，这项研究展示了LLMs在跨语言和文化背景下实现可扩展、透明和可重复的政治传播研究潜力的可能性。|
|**2025-07-23**|**A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)**|Bowen Zheng et.al.|[2507.17618](http://arxiv.org/abs/2507.17618)|null|由于深层结构，大型语言模型在计算上非常昂贵。先前的研究表明，中间层包含足够的信息来生成准确的答案，这导致了早期退出算法的发展，通过在早期层终止计算来降低推理成本。然而，这些方法由于中间层和输出层表示之间的不匹配，导致解码不准确，通常性能较差。为了解决这些挑战，我们提出了SPADE（空间对齐解码），一种新颖的解码方法，通过传播只包含起始标记和答案标记的最小化序列，将中间层表示与输出层对齐。我们进一步通过训练SPADE的线性近似来优化早期退出决策过程，该近似计算基于熵的置信度指标。将它们结合起来，我们创建了一个混合早期退出算法，该算法监控置信度水平，在中间层停止推理，同时使用SPADE生成高质量的输出。这种方法在保证准确性的同时显著降低了推理成本，为将大型语言模型部署到实际应用中提供了一个可扩展且高效的解决方案。|
|**2025-07-22**|**LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs**|Da-Chen Lian et.al.|[2507.16809](http://arxiv.org/abs/2507.16809)|null|我们提出了LingBench++，这是一个基于语言知识的基准和推理框架，旨在评估大型语言模型（LLMs）在国际语言学奥林匹克（IOL）启发的复杂语言任务上的表现。与仅关注最终答案准确性的先前基准不同，LingBench++提供了结构化的推理轨迹、逐步评估协议和超过90种低资源跨文化语言的丰富类型学元数据。我们进一步开发了一种多智能体架构，该架构集成了语法知识检索、工具辅助推理和有意识的假设检验。通过基准模型和我们所提出的智能体模型的系统性比较，我们证明，配备了外部知识源和迭代推理的模型在准确性和可解释性方面都优于单次通过的方法。LingBench++为在LLMs中推进基于语言、文化信息和认知合理性的推理提供了全面的基础。|
|**2025-07-22**|**Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis**|Zhihao Xu et.al.|[2507.16808](http://arxiv.org/abs/2507.16808)|null|寄存器传输级（RTL）代码优化对于实现数字电路设计中的高性能和低功耗至关重要。然而，传统的优化方法通常依赖于手动调整和启发式方法，这既耗时又容易出错。近期的研究提出了利用大型语言模型（LLMs）来协助RTL代码优化。LLMs可以根据自然语言描述生成优化代码片段，从而可能加快优化过程。但是，现有的方法并未充分评估LLM-Based代码优化方法在处理具有复杂时序逻辑的RTL代码方面的有效性。为了填补这一空白，我们进行了一项全面的实证研究，以评估LLM-Based RTL代码优化方法在处理具有复杂时序逻辑的RTL代码时的能力。在本研究中，我们首先提出了一种新的RTL优化评估基准。它包括四个子集，每个子集对应RTL代码优化的一个特定领域。然后，我们介绍了一种基于形态变化的方法，以系统地评估LLM-Based RTL代码优化方法的有效性。我们的关键见解是，对于语义上等效但更复杂的代码，优化效果应该保持一致。经过大量实验，我们发现了一些关键发现：（1）LLM-Based RTL优化方法可以有效优化逻辑运算，并优于现有的基于编译器的优化方法。（2）LLM-Based RTL优化方法在处理具有复杂时序逻辑的RTL代码时，并不比现有的基于编译器的优化方法表现更好，尤其是在时序控制流优化和时钟域优化方面。这主要是由于LLM在理解RTL代码中的时序逻辑时所面临的挑战。基于这些发现，我们为利用LLMs进行RTL代码优化提供了进一步研究的见解。|
|**2025-07-22**|**Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning**|Yanjun Zheng et.al.|[2507.16802](http://arxiv.org/abs/2507.16802)|null|大型语言模型（LLMs）在金融应用中展现出巨大的潜力；然而，现有的模型在面对需要高级推理能力、严格可信标准以及高效适应特定领域需求的情况时，往往表现出局限性。我们推出了Agentar-Fin-R1系列金融大型语言模型（8B和32B参数），该模型基于Qwen3基础模型专门设计，旨在增强金融应用的推理能力、可靠性和领域专业性。我们的优化方法将高质量的、系统的金融任务标签系统与全面的多层可信性保障框架相结合。该框架包括高质量的可靠知识工程、多智能体可靠数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段训练流程和动态归因系统，我们实现了训练效率的显著提升。我们的模型在包括Fineva、FinEval和FinanceIQ在内的主流金融基准以及MATH-500和GPQA-diamond等通用推理数据集上进行了全面评估。为了彻底评估实际部署能力，我们创新性地提出了Finova评估基准，该基准专注于智能体级别的金融推理和合规性验证。实验结果表明，Agentar-Fin-R1不仅在金融任务上达到了最先进的性能，而且展现出卓越的通用推理能力，验证了其作为高风险金融应用可靠解决方案的有效性。Finova基准可在https://github.com/antgroup/Finova获取。|
|**2025-07-22**|**Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent**|Xiaoyu Zhan et.al.|[2507.16799](http://arxiv.org/abs/2507.16799)|null|随着大型语言模型（LLMs）的快速发展，角色扮演语言代理在各个应用中展现出巨大的潜力。然而，仅仅依靠提示和上下文输入往往不足以实现深入特定角色的沉浸感，尤其是对于知名虚构人物或公众人物。另一方面，基于微调的方法由于数据收集的挑战和训练所需的计算资源而面临限制，从而限制了其更广泛的应用。为了解决这些问题，我们提出了测试时匹配（TTM），这是一种无需训练的角色扮演框架，通过测试时缩放和上下文工程来实现。TTM使用LLM代理自动将角色的特征解耦为个性、记忆和语言风格。我们的框架包括一个结构化的、分三阶段的生成流程，利用这些特征进行可控的角色扮演。它实现了高保真的角色扮演性能，并能够实现不同语言风格的无缝组合，甚至个性和记忆的变化。我们通过人工评估来评估我们的框架，结果表明，我们的方法在生成富有表现力和风格一致的角色对话方面取得了卓越的性能。|
|**2025-07-22**|**Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning**|Helena Casademunt et.al.|[2507.16795](http://arxiv.org/abs/2507.16795)|null|在大型语言模型（LLM）的微调过程中，可能会出现意料之外的分布外泛化。针对这一问题的标准方法依赖于修改训练数据，例如通过添加更好地指定预期泛化的数据。然而，这并不总是可行的。我们提出了概念消融微调（CAFT）技术，该技术利用可解释性工具来控制LLM在微调过程中如何泛化，而无需修改训练数据或使用目标分布中的数据。给定一组对应于不希望概念的大型语言模型潜在空间中的方向，CAFT在微调过程中通过线性投影消融这些概念，引导模型远离不希望的泛化。我们成功地将CAFT应用于三个微调任务，包括突发性偏差，这是一种LLM在窄任务上微调后对一般性问题给出极端偏差响应的现象。在不更改微调数据的情况下，CAFT将偏差响应减少了10倍，同时没有降低训练分布上的性能。总的来说，CAFT代表了一种在不修改训练数据的情况下引导LLM泛化的新颖方法。|
|**2025-07-22**|**ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation**|Roman Mayr et.al.|[2507.16792](http://arxiv.org/abs/2507.16792)|null|现代对话系统高度依赖大型语言模型（LLMs），但其实现往往超出了纯LLM交互的范畴。开发者会整合多个LLMs、外部工具和数据库。因此，仅对底层LLM进行评估是不够的，对话系统必须作为一个整体进行测试和评估。然而，这仍然是一个主要挑战。由于大多数先前的工作都集中在回合级分析上，对集成对话级质量保证的关注较少。为了解决这个问题，我们提出了ChatChecker，这是一个用于自动评估和测试复杂对话系统的框架。ChatChecker使用LLMs来模拟各种用户交互，识别对话中断，并评估质量。与先前的方法相比，我们的设计降低了设置努力，并且是通用的，因为它不需要参考对话，并且与目标对话系统的实现解耦。我们在提示中包含错误分类法，从而在先前的基于LLM的方法上提高了中断检测性能。此外，我们提出了一种基于具有挑战性角色的非合作用户模拟器，它能更有效地揭示目标对话系统的弱点。通过这种方式，ChatChecker有助于彻底和可扩展的测试。这使得研究人员和实践者都能加速稳健对话系统的发展。|
|**2025-07-22**|**Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning**|Hongyin Luo et.al.|[2507.16784](http://arxiv.org/abs/2507.16784)|null|为了突破大型语言模型（LLM）在推理准确性和效率方面的瓶颈，我们提出了线程推理模型（TIM），这是一个针对递归和分解式问题解决进行训练的LLM系列，以及TIMRUN，一个能够实现超越上下文限制的长期结构推理的推理运行时。在TIMRUN上运行的TIM支持几乎无限的工怍记忆和单次语言模型推理中的多跳工具调用，克服了输出限制、位置嵌入约束和GPU内存瓶颈。通过将自然语言建模为既按长度又按深度衡量的推理树，实现了性能提升。推理树由具有思考、递归子任务和基于我们在Schroeder等人2025年提出的概念得出的结论的任务组成。在生成过程中，我们维护一个工作记忆，只保留与上下文标记中最相关的关键值状态，通过基于规则的子任务剪枝机制选择，在整个推理过程中实现位置嵌入和GPU内存页的复用。实验结果表明，即使在操作高达90%的KV缓存时，我们的系统仍能保持高推理吞吐量。它还能在数学任务上提供准确的推理，并处理需要长期推理和多跳工具使用的情报检索挑战。|
|**2025-07-22**|**Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems**|Imran Latif et.al.|[2507.16781](http://arxiv.org/abs/2507.16781)|null|近年来，人工智能（AI）工作负载的空前增长，主要由大型语言模型（LLMs）和视觉-语言模型（VLMs）主导，加剧了数据中心对电力和冷却的需求。本研究在两个HGX节点上对LLMs和VLMs进行了基准测试，每个节点配备8个NVIDIA H100图形处理单元（GPU），并使用液体和空气冷却。通过利用GPU Burn、Weights and Biases和IPMItool，我们收集了详细的温度、电力和计算数据。结果显示，液体冷却系统在负载下能够将GPU温度维持在41-50摄氏度之间，而空气冷却系统则波动在54-72摄氏度之间。这种液体冷却系统的温度稳定性使得性能提升了17%（每个GPU 54 TFLOPs对比46 TFLOPs），提高了每瓦性能，降低了能耗，并比空气冷却系统具有更高的系统效率。这些发现强调了液体冷却在节能和可持续性方面的优势，为超大规模数据中心提供了一条引人注目的前进路径。|
|**2025-07-22**|**When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs**|Yue Li et.al.|[2507.16773](http://arxiv.org/abs/2507.16773)|null|大型语言模型（LLMs）已成为自动化代码分析的重要组成部分，使得如漏洞检测和代码理解等任务成为可能。然而，它们的集成引入了新的攻击面。在本文中，我们识别并研究了一种新的基于提示的攻击类型，称为复制引导攻击（CGA），它利用了具有推理能力的LLMs固有的复制倾向。通过在外部代码片段中注入精心设计的触发器，攻击者可以诱导模型在推理过程中复制恶意内容。这种行为使得模型出现了两类漏洞：推理长度操纵，其中模型生成异常短或过长的推理轨迹；以及推理结果操纵，其中模型产生误导性或不正确的结论。我们将CGA形式化为一个优化问题，并提出了一种基于梯度的方法来合成有效的触发器。在最新的推理LLMs上的实证评估表明，CGA在代码分析任务中可靠地引发了无限循环、提前终止、错误拒绝和语义扭曲。尽管在针对的场景中非常有效，但由于计算限制，我们观察到在多样化的提示中泛化CGA存在挑战，这为未来的研究提出了一个开放性问题。我们的发现揭示了LLM驱动开发管道中的一个关键但尚未充分探索的漏洞，并呼吁在提示级别的防御机制方面取得紧急的进步。|
|**2025-07-22**|**WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding**|Ran Wang et.al.|[2507.16768](http://arxiv.org/abs/2507.16768)|null|结构化解码使大型语言模型（LLMs）能够生成下游系统所需的输出格式，如HTML或JSON。然而，现有方法由于语法编译、状态跟踪和掩码创建等原因存在效率瓶颈。我们观察到许多实际任务都嵌入了对输出结构的强烈先验知识。利用这一点，我们提出将约束分解为静态和动态组件——离线预编译静态结构，并在运行时使用语法片段实例化动态参数。我们不是依赖于下推自动机，而是采用一套组合运算符来模拟正则格式，从而实现更低的转换延迟。我们引入了wgrammar，一个轻量级的解码引擎，它集成了领域感知简化、约束分解和掩码缓存，比现有系统快250倍。wgrammar的源代码可在https://github.com/wrran/wgrammar上公开获取。|
|**2025-07-21**|**Diffusion Beats Autoregressive in Data-Constrained Settings**|Mihir Prabhudesai et.al.|[2507.15857](http://arxiv.org/abs/2507.15857)|null|自回归（AR）模型长期以来主导着大型语言模型领域，推动了广泛任务上的进步。最近，基于扩散的语言模型作为一种有希望的替代方案出现，尽管它们相对于AR模型的优势尚未得到充分探索。在这篇论文中，我们系统地研究了在数据受限设置下（即训练涉及对有限数据的重复遍历）的掩码扩散模型，并发现当计算资源充足而数据稀缺时，它们显著优于AR模型。扩散模型更好地利用了重复数据，实现了更低的验证损失和更优越的下游性能。我们将这种优势解释为隐式数据增强：与AR的固定从左到右分解不同，掩码扩散使模型接触到多样的标记顺序和预测任务。我们发现扩散模型的新规模定律，并推导出扩散开始优于AR的关键计算阈值封闭形式表达式。这些结果表明，当数据而不是计算成为瓶颈时，扩散模型为标准的AR范式提供了一种令人信服的替代方案。我们的代码可在以下网址获取：https://diffusion-scaling.github.io。|
|**2025-07-21**|**Gemini 2.5 Pro Capable of Winning Gold at IMO 2025**|Yichen Huang et.al.|[2507.15855](http://arxiv.org/abs/2507.15855)|null|国际数学奥林匹克（IMO）提出了独特且具有挑战性的问题，这些问题需要深刻的洞察力、创造力和形式推理。尽管大型语言模型（LLMs）在AIME等数学基准测试中表现良好，但在奥林匹克级别的任务上却遇到了困难。我们使用Google的Gemini 2.5 Pro对最新发布的IMO 2025问题进行了测试，并避免了数据污染。通过使用一个自我验证的流水线以及精心设计的提示，6个问题中有5个被正确解决（以下讨论的例外情况除外）。这一结果强调了开发最佳策略以充分利用强大LLMs进行复杂推理任务的重要性。|
|**2025-07-21**|**The Other Mind: How Language Models Exhibit Human Temporal Cognition**|Lingyu Li et.al.|[2507.15851](http://arxiv.org/abs/2507.15851)|null|随着大型语言模型（LLMs）的持续发展，它们表现出某些与人类相似的认知模式，而这些模式并未在训练数据中直接指定。本研究通过聚焦于LLMs的时间认知来探究这一现象。利用相似性判断任务，我们发现较大的模型会自发地建立一个主观的时间参照点，并遵循韦伯-费舍尔定律，即感知到的距离随着时间距离这个参照点的增长而以对数形式压缩。为了揭示这种行为的背后机制，我们在神经元、表征和信息层面进行了多项分析。我们首先识别出一组时间偏好神经元，并发现这一组在主观参照点处的激活最小，并采用了一种在生物系统中发现的对数编码方案。对年份的表征分析揭示了一个层次化的构建过程，其中年份从浅层中的基本数值演变为深层中的抽象时间方向。最后，使用预训练的嵌入模型，我们发现训练语料库本身就具有固有的非线性时间结构，这为模型的内部构建提供了原材料。在讨论中，我们提出了一种经验主义视角来理解这些发现，其中LLMs的认知被视为其内部表征系统对外部世界的主观构建。这种细致的视角暗示了可能出现的、人类无法直观预测的异质认知框架，指向了关注引导内部构建的AI对齐方向。我们的代码可在https://TheOtherMind.github.io找到。|
|**2025-07-21**|**3LM: Bridging Arabic, STEM, and Code through Benchmarking**|Basma El Amel Boussaha et.al.|[2507.15850](http://arxiv.org/abs/2507.15850)|null|阿拉伯语是世界上使用最广泛的语言之一，然而，开发和评估阿拉伯语大型语言模型（LLMs）的努力相对有限。大多数现有的阿拉伯语基准主要关注语言、文化或宗教内容，在STEM和代码等领域留下了显著的空白，这些领域对于现实世界的LLMs应用越来越重要。为了帮助填补这一空白，我们提出3LM，一套专为阿拉伯语设计的三个基准。第一个是一组与STEM相关的问题-答案对，这些对自然来源于阿拉伯语教科书和教育工作表。第二个由合成生成的STEM问题组成，使用相同的来源创建。第三个基准专注于代码生成，通过仔细翻译两个广泛使用的代码基准构建而成，并加入了多轮人工审核的闭环过程，以确保高质量和忠实的翻译。我们公开发布所有三个基准，以支持在这些关键但代表性不足的领域中阿拉伯语LLMs研究的发展。|
|**2025-07-21**|**The Impact of Language Mixing on Bilingual LLM Reasoning**|Yihao Li et.al.|[2507.15849](http://arxiv.org/abs/2507.15849)|null|熟练的多语言说话者在对话中经常故意切换语言。同样，最近专注于推理的双语大型语言模型（LLMs）在两种语言中都表现出语言混合——在他们的思维链中交替使用语言。在DeepSeek-R1中抑制这种行为的做法被发现会降低准确性，这表明语言混合可能对推理有益。在这项工作中，我们研究了中英双语推理模型中的语言切换。我们确定强化学习与可验证奖励（RLVR）是导致语言混合的关键训练阶段。我们证明语言混合可以增强推理：强制单语解码在数学推理任务上降低了5.6个百分点的准确性。此外，可以训练一个轻量级的探针来预测潜在的语种切换是否会对推理有益或有害，当用于指导解码时，可以提高高达6.25个百分点的准确性。我们的发现表明，语言混合不仅仅是多语言训练的副产品，而是一种战略性的推理行为。|
|**2025-07-21**|**FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs**|Anh Nguyen et.al.|[2507.15839](http://arxiv.org/abs/2507.15839)|null|合成数据生成已成为在现实世界数据收集和利用受限于成本和稀缺性的场景中的一个宝贵的解决方案。大型语言模型（LLM）在各种领域展示了产生高保真、领域相关样本的显著能力。然而，现有的直接使用LLM生成每条记录的方法在需要大量合成数据时，对时间和成本提出了巨大的负担。在这项工作中，我们提出了一种快速、经济高效的方法，用于生成现实表格数据，该方法利用LLM推断并将每个字段的分布编码成一个可重复使用的采样脚本。通过自动将字段分类为数值型、分类型或自由文本型，LLM生成基于分布的脚本，可以高效地以规模生产多样化的、真实的数据集，而无需持续的模型推理。实验结果表明，我们的方法在多样性和数据真实性方面优于传统的直接方法，大大减轻了大量合成数据生成的负担。我们计划将这种方法应用于加速生产管道中的测试，从而缩短开发周期，提高整体系统效率。我们相信，我们的见解和经验教训将有助于寻求可扩展、经济高效的合成数据生成解决方案的研究人员和从业者。|
|**2025-07-21**|**Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation**|Alessandro B. Melchiorre et.al.|[2507.15826](http://arxiv.org/abs/2507.15826)|null|本文提出了一种名为JAM（Just Ask for Music）的自然语言音乐推荐轻量级且直观的框架。JAM将用户查询与物品的交互建模为共享潜在空间中的向量转换，灵感来源于TransE等知识图嵌入方法。为了捕捉音乐和用户意图的复杂性，JAM通过交叉注意力和稀疏专家混合聚合多模态物品特征。此外，我们还引入了JAMSessions，这是一个包含超过10万条用户-查询-物品三元组的新数据集，其中包含匿名用户/物品嵌入，独特地结合了对话查询和用户长期偏好。我们的结果表明，JAM能够提供准确的推荐，生成适用于实际用例的直观表示，并且可以轻松集成到现有的音乐推荐系统中。|
|**2025-07-21**|**ACS: An interactive framework for conformal selection**|Yu Gui et.al.|[2507.15825](http://arxiv.org/abs/2507.15825)|null|本文提出了自适应一致性选择（ACS），一个具有保证误差控制的模型无关选择交互框架。在一致性选择（Jin和Candès，2023b）的基础上，ACS将此方法推广以支持有人参与的适应性数据分析。在ACS框架下，我们可以部分重用数据以提高选择能力，在探索数据的同时做出决策，并根据需要纳入新信息或偏好。ACS的关键在于一个精心设计的原则，它控制决策可用的信息，使数据分析师能够自适应地探索数据，同时保持对假发现率（FDR）的严格控制。基于ACS框架，我们为各种目标提供了具体的选取算法，包括模型更新/选择、多样化选择以及纳入新可用的标记数据。通过广泛的数值模拟和在大语言模型（LLM）部署和药物发现中的真实数据应用，展示了ACS的有效性。|
|**2025-07-21**|**Do AI models help produce verified bug fixes?**|Li Huang et.al.|[2507.15822](http://arxiv.org/abs/2507.15822)|null|在软件工程领域，人工智能技术——尤其是大型语言模型——似乎有望带来显著的改进，其中一项吸引人的候选领域是自动程序修复（APR），即对软件错误进行令人满意的修正。这种期望在现实中能否实现？我们如何确定所提出的修正是否真的有效？如果程序员可以访问大型语言模型，他们实际上如何使用它们来补充自己的技能？为了回答这些问题，我们利用了程序证明环境的存在，该环境可以形式化地确定所提议修复的正确性，以对两组随机分配的程序员进行程序调试研究，一组可以访问大型语言模型，另一组则不能，他们都通过证明工具验证自己的答案。该方法依赖于将研究问题分为一般研究问题（目标-查询-度量方法中的目标）、具有特定答案的具体元素（查询）以及支持这些答案的测量（度量）。虽然到目前为止仅应用于有限的样本量，但结果是为界定人工智能和大型语言模型在提供保证正确的程序错误修复中的适当角色迈出的第一步。  与使用人工智能进行调试和APR可能预期的结果相比，这些结果令人惊讶。贡献还包括：一个用于使用大型语言模型进行调试的详细实验方法，其他项目可以重用；通过使用完整会话记录进行的程序员行为的细粒度分析；定义了大型语言模型的使用模式，共有7个不同的类别；以及针对调试和自动程序修复如何最大限度地发挥大型语言模型作用的验证建议。|
|**2025-07-21**|**LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra**|Seth Karten et.al.|[2507.15815](http://arxiv.org/abs/2507.15815)|null|我们提出了一种名为“LLM经济学家”的新框架，该框架利用基于代理的建模来设计和评估具有分层决策的战略环境中的经济政策。在较低层次，有限理性的工人代理——以美国人口普查校准的收入和人口统计数据为条件，实例化为角色提示——选择劳动供给以最大化在情境中学习的基于文本的效用函数。在较高层次，规划代理使用情境强化学习来提出以当前美国联邦税级为基础的分段线性边际税率表。这种构建赋予了经济模拟三个对于可信的财政实验至关重要的能力：（i）异质效用的优化，（ii）基于原则的大量、具有人口统计学现实性的代理群体的生成，（iii）机制设计——终极的微调问题——完全用自然语言表达。对多达一百个相互作用的代理群体的实验表明，规划代理在Stackelberg均衡附近收敛，相对于Saez解决方案，提高了总体的社会福利，而周期性的、角色级别的投票程序在去中心化治理下进一步增加了这些收益。这些结果表明，基于大型语言模型的代理可以共同模拟、模拟和治理复杂的经济系统，为政策评估提供了一个可操作的测试平台，有助于构建更美好的文明。|
|**2025-07-18**|**CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning**|Xiaoya Li et.al.|[2507.14111](http://arxiv.org/abs/2507.14111)|null|随着大型语言模型快速进步推动对GPU计算资源需求的指数级增长，自动化CUDA优化策略的需求变得迫切。虽然近期LLM（例如R1、o1）在代码生成方面的进步显示出希望，但当前最先进的模型在提升CUDA速度方面成功率较低。在本文中，我们介绍了CUDA-L1，这是一个用于CUDA优化的自动化强化学习框架。CUDA-L1在CUDA优化任务上实现了性能提升：在NVIDIA A100上训练后，它在KernelBench的250个CUDA内核中平均速度提升了17.7倍，峰值速度提升可达449倍。此外，该模型在GPU架构上展现了出色的可移植性，尽管专门针对A100进行优化，但在H100上平均速度提升了17.8倍，在RTX 3090上提升了19.0倍，在L40上提升了16.5倍，在H800上提升了14.7倍，在H20上提升了13.9倍。除了这些基准测试结果，CUDA-L1还表现出几个显著特性：1）发现各种CUDA优化技术，并学会战略性地结合它们以实现最佳性能；2）揭示了CUDA优化的基本原理；3）识别了非显而易见的性能瓶颈，并拒绝了看似有益但实际上损害性能的优化。CUDA-L1的能力表明，仅通过基于速度提升的奖励信号，强化学习可以将最初表现不佳的LLM转化为有效的CUDA优化器，而不需要人类的专家知识或领域知识。更重要的是，训练好的RL模型将获取的推理能力扩展到新的内核。这种范式为自动化CUDA操作的优化开辟了可能性，并有望大幅提高GPU效率，缓解GPU计算资源压力上升的问题。|
|**2025-07-18**|**Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment**|Viraj Nishesh Darji et.al.|[2507.14107](http://arxiv.org/abs/2507.14107)|null|桥梁维护和安全对交通管理部门至关重要，无损检测（NDE）技术在评估结构完整性方面至关重要。然而，解读NDE数据可能耗时且需要专业知识，可能会延迟决策。近年来，大型语言模型（LLM）的进步为自动化和改进这种分析提供了新的方法。本试点研究介绍了对LLM在解读NDE轮廓图方面的综合评估，并证明了LLM在提供详细桥梁状况分析方面的有效性。它建立了一个将LLM集成到桥梁检查工作流程的框架，表明LLM辅助分析可以提高效率而不影响准确性。在本研究中，探索了几个LLM，并针对特定于增强图像描述质量的提示进行了设计，这些描述应用于解读通过评估桥梁状况的技术获得的五种不同的NDE轮廓图。每个LLM模型根据其产生详细描述、识别缺陷、提供可操作建议和展示整体准确性的能力进行评估。研究表明，九个模型中的四个提供了更好的图像描述，有效地涵盖了与桥梁状况相关的广泛主题。使用五种不同的LLM对这四个模型的输出进行了总结，形成了桥梁的全面概述。值得注意的是，LLM ChatGPT-4和Claude 3.5 Sonnet生成的总结更有效。研究发现，LLM有潜力显著提高效率和准确性。本试点研究提出了一种创新方法，利用LLM进行图像标题的并行和总结，使桥梁维护中的决策更快，并增强基础设施管理和安全评估。|
|**2025-07-18**|**Generative AI-Driven High-Fidelity Human Motion Simulation**|Hari Iyer et.al.|[2507.14097](http://arxiv.org/abs/2507.14097)|null|人类动作模拟（HMS）支持对工业任务中工人行为、安全性和生产力的经济高效评估。然而，现有方法往往存在动作保真度低的问题。本研究介绍了基于生成式AI的人类动作模拟（G-AI-HMS），该技术通过集成文本到文本和文本到动作模型来提升物理任务模拟的质量。G-AI-HMS应对两大挑战：（1）利用与MotionGPT训练词汇相匹配的大语言模型，将任务描述翻译为动作感知语言；（2）使用计算机视觉验证AI增强动作与真实人类动作的一致性。通过对实时视频应用姿态估计算法来提取关节地标，并使用动作相似度指标将它们与AI增强序列进行比较。在一个包含八个任务的研究案例中，AI增强的动作在大多数场景下比人工描述的错误更低，在六个任务中基于空间精度表现更好，在四个任务中基于姿态归一化后的对齐表现更好，在七个任务中基于整体时间相似性表现更好。统计分析显示，AI增强的提示显著（p $<$ 0.0001）降低了关节误差和时间错位，同时保持了可比的姿态准确性。|
|**2025-07-18**|**Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track**|Brian Ondov et.al.|[2507.14096](http://arxiv.org/abs/2507.14096)|null|研究目标：近年来语言模型在将面向专业人士的生物医学文献转化为普通语言方面的进展显示出潜力，使其对病人和护理人员更加易于理解。然而，由于该领域的潜在危害性较高，加上其不可预测性，严格的评估是必要的。我们开展这一研究轨迹的目的是激发研究并高质量地评估最有潜力的系统。  研究方法：我们在2023年和2024年的文本检索会议上举办了生物医学摘要普通语言改编（PLABA）轨迹。任务包括摘要的完整、句子级别的改写（任务1），以及识别和替换难以理解的术语（任务2）。为了对任务1进行自动评估，我们开发了一套由专业人士撰写的参考材料。两个任务的提交都得到了生物医学专家的广泛人工评估。  研究结果：共有来自12个国家的12个团队参与了这一轨迹，涉及从多层感知器到大型预训练变换器的各种模型。在任务1的人工判断中，表现最好的模型在事实准确性和完整性方面与人类水平相当，但在简洁性和简洁性方面则不如人类。基于参考的自动指标通常与人工判断没有很好地相关。在任务2中，系统在识别难以理解的术语和分类如何替换它们方面存在困难。然而，在生成替换词时，基于大型语言模型（LLM）的系统在人工判断的准确性、完整性和简洁性方面表现良好，但在简洁性方面则不行。  结论：PLABA轨迹展示了利用大型语言模型将生物医学文献改编为公众可读性的潜力，同时也突显了它们的不足和改进自动基准工具的需求。|
|**2025-07-18**|**DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration**|Xiyun Li et.al.|[2507.14088](http://arxiv.org/abs/2507.14088)|null|实时人机人工智能（AI）协作至关重要，但也极具挑战性，尤其是在AI代理必须适应动态场景中多样和未见过的人类行为时。现有的大型语言模型（LLM）代理往往无法准确模拟复杂的人类心理特征，如领域意图，尤其是在缺乏直接沟通的情况下。为了解决这一限制，我们提出了一个新颖的双重过程多尺度心智理论（DPMT）框架，该框架从认知科学的双重过程理论中汲取灵感。我们的DPMT框架集成了多尺度心智理论（ToM）模块，通过心理特征推理促进稳健的人类伙伴建模。实验结果表明，DPMT显著提高了人机协作，而消融研究进一步验证了我们的多尺度ToM在慢速系统中的贡献。|
|**2025-07-18**|**DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits**|Garapati Keerthana et.al.|[2507.14079](http://arxiv.org/abs/2507.14079)|null|进展记录是电子健康记录（EHR）中最具临床意义的资料之一，它们提供了关于患者病情、治疗和护理决策随时间演变的时空见解。尽管它们的重要性不言而喻，但在大规模EHR数据集中，进展记录的代表性却非常低。例如，在广泛使用的重症监护医疗信息市场III（MIMIC-III）数据集中，只有大约8.56%的医院访问包含了进展记录，导致纵向患者叙述存在空白。相比之下，该数据集包含各种其他类型的记录，每个都记录了护理的不同方面。我们提出了DENSE（从分散的证据中记录演进的进展记录），这是一个旨在与临床文档工作流程相一致的系统，通过模拟医生在撰写进展记录时如何参考过去的病例。该系统引入了细粒度的记录分类和时序对齐机制，将访问期间的异构记录组织成结构化和按时间顺序的输入。在核心上，DENSE利用临床信息检索策略从当前和先前的访问中识别出时序和语义相关的内容。这些检索到的证据被用来提示大型语言模型（LLM）生成临床连贯和时序感知的进展记录。我们在一个由多次访问和完整进展记录文档精心挑选的患者队列上评估了DENSE。生成的记录展现出强大的纵向一致性，实现了1.089的时序对齐比率，超过了原始记录中观察到的连续性。通过在破碎的文档中恢复叙事连贯性，我们的系统支持了改进的下游任务，如摘要、预测建模和临床决策支持，为现实世界医疗环境中LLM驱动的记录合成提供了一个可扩展的解决方案。|
|**2025-07-18**|**Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks**|Israt Jahan et.al.|[2507.14045](http://arxiv.org/abs/2507.14045)|null|本文全面评估了成本效益高的大型语言模型（LLMs）在多种生物医学任务中的应用，这些任务涵盖了文本和图像两种模态。我们对一系列封闭源和开源LLMs在生物医学文本分类和生成、问答以及多模态图像处理等任务上进行了评估。我们的实验结果表明，没有一种LLM能够在所有任务上持续优于其他模型。相反，不同的LLMs在不同的任务上表现出色。虽然一些封闭源LLMs在特定任务上表现出强劲的性能，但它们的开源版本达到了可比的结果（有时甚至更好），并且具有更快的推理和增强隐私等额外优势。我们的实验结果为选择最适合特定生物医学应用的模型提供了宝贵的见解。|
|**2025-07-18**|**Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors**|Jochen Wulf et.al.|[2507.14034](http://arxiv.org/abs/2507.14034)|null|基于大型语言模型的代理人工智能系统在技术服务中的价值共创方面具有变革潜力。然而，如幻觉和操作脆弱性等持续挑战限制了它们的自主使用，从而迫切需要强大的框架来指导人机协作。本文借鉴了人类-人工智能团队协作研究以及来自自动驾驶等领域的相关类比，构建了一个结构化的人类-代理交互分类法。基于技术支持平台内的案例研究，我们提出了一个六模式分类法，它将协作组织在一个涵盖人工智能自主性范围的光谱中。这个光谱以“人类退出环”（HOOTL）模型作为全自动化的一端，以及“人类增强模型”（HAM）作为被动人工智能辅助的另一端。在这两个极端之间，该框架详细说明了四种不同的中间结构。这包括“人类在控制中”（HIC）模型，其中人工智能的建议需要强制性的人类批准，以及“人类在流程中”（HITP）模型，用于结构化的工作流程和确定性的人类任务。分类法进一步界定了“人类在环中”（HITL）模型，它促进了代理在不确定情况下的主动升级，以及“人类在环上”（HOTL）模型，它允许对自主人工智能进行任意的人类监督。本文的主要贡献是一个综合框架，该框架将此分类法与关键应急因素（如任务复杂性、操作风险和系统可靠性）及其相应的概念架构联系起来。通过提供一种系统化的方法来选择和设计适当的人类监督级别，我们的框架为从业者提供了一个关键的工具，以在自动化和控制之间权衡，从而促进更安全、更有效且具有情境意识的技术服务系统的发展。|
|**2025-07-18**|**KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models**|Lam Nguyen et.al.|[2507.14032](http://arxiv.org/abs/2507.14032)|null|本体匹配（OM）是语义互操作性的基石任务，但现有的系统通常依赖于手工制定的规则或适应性有限的专用模型。我们提出了KROMA，这是一个新颖的本体匹配框架，它利用检索增强生成（RAG）管道中的大型语言模型（LLMs）来动态丰富OM任务的语义上下文，包括结构、词汇和定义方面的知识。为了优化性能和效率，KROMA集成了基于双相似性的概念匹配和轻量级本体精炼步骤，这有助于修剪候选概念并显著减少调用LLMs时的通信开销。通过在多个基准数据集上的实验，我们表明将知识检索与上下文增强的LLMs相结合可以显著提高本体匹配效果，在保持通信开销相当的同时，优于经典的OM系统和基于LLM的尖端方法。我们的研究突出了所提出的优化技术（针对性知识检索、提示丰富和本体精炼）在大规模本体匹配中的可行性和益处。|
|**2025-07-18**|**Moodifier: MLLM-Enhanced Emotion-Driven Image Editing**|Jiarong Ye et.al.|[2507.14024](http://arxiv.org/abs/2507.14024)|null|在创意产业中，通过连接情感和视觉内容实现情感驱动型图像编辑具有巨大潜力，但由于情感本质的抽象性和在不同情境中的多样化表现，精确操作仍然具有挑战性。我们采用一个包含三个互补组件的集成方法来应对这一挑战。首先，我们引入了MoodArchive，一个包含超过800万张图片的数据集，其中包含由LLaVA生成并部分由人类评估员验证的详细分层情感标注。其次，我们开发了MoodifyCLIP，这是一个在MoodArchive上微调的视觉-语言模型，能够将抽象情感转换为特定的视觉属性。第三，我们提出了Moodifier，一个无需训练的编辑模型，利用MoodifyCLIP和多模态大型语言模型（MLLMs）来实现精确的情感转换，同时保持内容完整性。我们的系统适用于多个领域，如角色表情、时尚设计、珠宝和室内装饰，使创作者能够快速可视化情感变化，同时保持个性和结构。广泛的实验评估表明，Moodifier在情感准确性和内容保存方面均优于现有方法，提供了情境合适的编辑。通过将抽象情感与具体视觉变化联系起来，我们的解决方案在现实应用中解锁了情感内容创作的新可能性。在论文被接受后，我们将发布MoodArchive数据集、MoodifyCLIP模型，并使Moodifier的代码和演示公开可用。|
|**2025-07-17**|**VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding**|Shihao Wang et.al.|[2507.13353](http://arxiv.org/abs/2507.13353)|null|近期研究表明，选择具有信息性和相关性的视频帧可以显著提升视频大型语言模型（Video-LLMs）的性能。当前的方法，如减少帧间冗余、采用独立的模型进行图像-文本相关性评估，或利用时间视频定位进行事件定位，主要采用无监督学习范式，但在处理长视频理解中的复杂场景时存在困难。我们提出了视频指令时间定位（Instructed Temporal Grounding for Videos，VideoITG），其特色是配合用户指令的定制帧采样。VideoITG的核心是VidThinker管道，这是一个自动标注框架，明确模拟了人工标注过程。首先，它根据指令生成详细的片段级字幕；然后，通过指令引导的推理检索相关视频片段；最后，进行细粒度的帧选择，以确定最具有信息性的视觉证据。利用VidThinker，我们构建了包含40K个视频和500K个指令时间定位标注的VideoITG-40K数据集。然后，我们设计了一个即插即用的VideoITG模型，该模型利用Video-LLMs的视觉语言对齐和推理能力，以判别性的方式有效地进行帧选择。结合Video-LLMs，VideoITG在多个多模态视频理解基准测试中实现了持续的性能提升，显示出其在视频理解方面的优越性和巨大潜力。|
|**2025-07-17**|**Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes**|Tyler Loakman et.al.|[2507.13335](http://arxiv.org/abs/2507.13335)|null|幽默作为一种复杂的语言形式，源自生活的方方面面，而现有的计算幽默研究几乎完全集中在基于短双关语的笑话上。在这项工作中，我们研究了大型语言模型（LLMs）解释幽默的能力是否取决于特定的幽默形式。我们比较了模型在简单双关语和更复杂的时事幽默上的表现，后者需要了解现实世界中的实体和事件。为此，我们整理了一个包含600个笑话的数据集，这些笑话分为4种类型，并手动编写了高质量的解释。这些笑话包括异形和同形双关语、当代网络幽默和时事笑话，其中理解依赖于超越“常识”的推理，其根源在于关于新闻事件和流行文化的世界知识。使用这个数据集，我们比较了各种LLMs在零样本情况下准确全面解释不同类型笑话的能力，并确定了幽默解释任务中的关键研究差距。我们发现，所测试的模型（包括推理模型）都无法可靠地生成所有类型笑话的充分解释，进一步突显了大多数计算幽默工作对过于简单笑话形式的狭窄关注。|
|**2025-07-17**|**A Survey of Context Engineering for Large Language Models**|Lingrui Mei et.al.|[2507.13334](http://arxiv.org/abs/2507.13334)|null|大型语言模型（LLMs）的性能从根本上取决于推理过程中提供的上下文信息。本综述介绍了上下文工程，这是一门超越简单提示设计的正式学科，它涵盖了为LLMs系统优化信息负载的体系化方法。我们提出了一个全面的分类法，将上下文工程分解为其基础组件和将这些组件整合到智能系统中的复杂实现。我们首先审视了基础组件：上下文检索和生成、上下文处理和上下文管理。然后，我们探索了这些组件如何架构性地整合以创建复杂的系统实现：检索增强生成（RAG）、记忆系统、工具集成推理和多智能体系统。通过系统分析超过1300篇研究论文，我们的综述不仅为该领域建立了技术路线图，还揭示了一个关键的研究差距：模型能力之间存在根本的不对称性。尽管当前模型通过先进的上下文工程得到了显著增强，在理解复杂上下文方面表现出色，但在生成同样复杂、长篇的输出方面却存在明显的局限性。解决这一差距是未来研究的一个定义性优先事项。最终，本综述为推进上下文感知AI的研究人员和工程师提供了一个统一的框架。|
|**2025-07-17**|**The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner**|Zhouqi Hua et.al.|[2507.13332](http://arxiv.org/abs/2507.13332)|null|长度泛化，即解决比训练过程中观察到的更长的序列问题的能力，是Transformer大型语言模型（LLM）的核心挑战。尽管现有研究主要集中于数据驱动的方法，用于算术运算和符号操作任务，但这些方法往往具有特定任务性，整体性能有限。为了寻求更通用的解决方案，本文关注更广泛的可计算推理问题，即算法可以解决的问题，即可以通过图灵机解决的问题。从这个角度来看，本文提出了图灵机模仿学习（TAIL），以提高LLM的长度泛化能力。TAIL通过计算机程序合成思维链（CoT）数据，模仿图灵机的执行过程，将推理步骤线性扩展到原子状态，以缓解捷径学习和显式内存检索机制，以减少基本操作中动态和长距离数据访问的困难。为了验证TAIL的可靠性和通用性，我们构建了一个具有挑战性的合成数据集，涵盖了8种算法和18个任务。仅使用合成数据，TAIL显著提高了长度泛化能力以及Qwen2.5-7B在各项任务上的性能，超过了先前的方法和DeepSeek-R1。实验结果表明，对于长度泛化，图灵机的关键概念而不是思维风格对TAIL是必不可少的，通过这种方式，模型在它们的注意力层中表现出与图灵机属性一致的读写行为。这项工作为从合成数据中学习LLM推理提供了有前景的研究方向。|
|**2025-07-17**|**GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM**|Kyeongjin Ahn et.al.|[2507.13323](http://arxiv.org/abs/2507.13323)|null|社会经济指标，如地区GDP、人口和教育水平，对于制定政策决策和促进可持续发展至关重要。本研究引入了GeoReg回归模型，该模型整合了包括卫星图像和基于网络的地理空间信息在内的多种数据来源，以估计这些指标，即使是对于数据稀缺的地区，如发展中国家。我们的方法利用大型语言模型（LLM）的先验知识来解决标注数据稀缺的问题，LLM充当数据工程师的角色，通过提取信息特征，以实现少量样本下的有效估计。具体来说，我们的模型获取了数据特征与目标指标之间的上下文关系，将它们的相关性分类为正相关、负相关、混合相关或不相关。然后，这些特征被输入到线性估计器中，每个类别都有定制权重约束。为了捕捉非线性模式，模型还识别了有意义的特征交互，并将它们以及非线性变换整合进去。在三个不同发展阶段的国家进行的实验表明，我们的模型在估计社会经济指标方面优于基线，即使在数据可用性有限、低收入的国家也是如此。|
|**2025-07-17**|**Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark**|Junsu Kim et.al.|[2507.13314](http://arxiv.org/abs/2507.13314)|null|基于推理的人体姿态估计（RPE）基准已成为姿态感知的多模态大型语言模型（MLLM）广泛采用的评估标准。尽管其重要性不容忽视，但我们发现了关键的复现性和基准质量问题，这些问题阻碍了公平和一致的定量评估。最值得注意的是，该基准使用了与原始3DPW数据集不同的图像索引，迫使研究人员进行繁琐且易出错的手动匹配过程，以获取用于定量指标（例如，MPJPE，PA-MPJPE）的准确地面真实（GT）标注。此外，我们的分析揭示了几个固有的基准质量局限性，包括显著图像冗余、场景不平衡、过于简化的姿态和模糊的文字描述，共同破坏了不同场景下可靠的评估。为了减轻手动工作量并提高可复现性，我们通过细致的视觉匹配精心优化了GT标注，并将其作为开源资源公开发布，从而促进了一致的定量评估，并促进了未来在人体姿态感知多模态推理方面的进步。|
|**2025-07-17**|**The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations**|Carlos Arriaga et.al.|[2507.13302](http://arxiv.org/abs/2507.13302)|null|大型语言模型的评估是一项复杂的任务，其中已经提出了多种方法。最常见的方法是使用自动基准测试，其中LLMs需要回答不同主题的多个选择题。然而，这种方法存在某些局限性，其中最令人担忧的是与人类的关联性较差。一种替代方法是让人类评估LLMs。这引发了可扩展性问题，因为需要评估的模型数量庞大且持续增长，使得基于招募一定数量的评估者并让他们对模型的回答进行排序的传统研究变得不切实际（且成本高昂）。一种替代方法是使用公共平台，例如流行的LM平台，任何用户都可以自由地对任何问题进行模型评估并排序两个模型的回答。然后，这些结果被细化成模型排名。LLMs的一个重要方面是它们的能耗，因此，评估能耗意识如何影响人类在选择模型时的决策是有趣的。在本文中，我们提出了GEA，即生成能耗平台，这是一个在评估过程中融入模型能耗信息的平台。我们还展示了使用GEA获得的一些初步结果，结果表明，对于大多数问题，当用户意识到能耗时，他们会更倾向于选择体积更小、能耗更低的模型。这表明，对于大多数用户交互来说，更复杂和表现优异的模型所带来的额外成本和能耗并没有提高感知质量，从而证明其使用的合理性。|
|**2025-07-17**|**AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research**|Yilun Zhao et.al.|[2507.13300](http://arxiv.org/abs/2507.13300)|null|我们介绍了AbGen，这是第一个旨在评估大型语言模型（LLM）在为科学研究设计消融研究能力方面的基准。AbGen包含1,500个由807篇自然语言处理（NLP）论文派生的专家标注示例。在这个基准中，LLM的任务是根据给定的研究背景为指定的模块或过程生成详细的消融研究设计。我们对领先的LLM，如DeepSeek-R1-0528和o4-mini的评估突显了这些模型与人类专家在消融研究设计的重要性、忠实性和合理性方面存在显著的性能差距。此外，我们证明了当前的自动化评估方法对于我们的任务来说并不可靠，因为它们与人类评估相比显示出显著的差异。为了更好地研究这个问题，我们开发了AbGen-Eval，这是一个元评估基准，旨在评估常用自动化评估系统在衡量LLM在我们任务上的性能方面的可靠性。我们在AbGen-Eval上研究了各种LLM作为裁判的系统，为未来研究开发更有效和可靠的LLM基础评估系统提供了见解。|
|**2025-07-17**|**Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management**|Luis Gasco et.al.|[2507.13275](http://arxiv.org/abs/2507.13275)|null|自然语言处理和大型语言模型的发展正在推动人力资源管理领域的重大变革，越来越多的兴趣集中在基于语言技术的智能系统建设，用于人才招聘、技能提升策略和劳动力规划。然而，这些技术的采用和进步严重依赖于可靠和公平的模型的发展，这些模型需要在公共数据和公开基准上进行适当评估，而迄今为止，这一领域尚未有可用的此类数据。为了解决这一差距，我们提出了TalentCLEF 2025，这是第一个专注于技能和职位名称智能的评估活动。该实验室包括两个任务：任务A - 多语言职位名称匹配，涵盖英语、西班牙语、德语和中文；任务B - 基于职位名称的技能预测，使用英语。这两个语料库均由真实的职位申请构建而成，经过仔细匿名化，并手动标注以反映现实世界劳动力市场数据的复杂性和多样性，包括语言变体和性别标记的表达。评估包括单语和跨语言场景，并涵盖了性别偏见评估。TalentCLEF吸引了76支注册团队，提交了280多篇论文。大多数系统依赖于使用基于多语言编码器模型的检索技术，这些模型通过对比学习进行了微调，其中一些系统还结合了大型语言模型进行数据增强或重新排序。结果显示，训练策略的影响大于模型规模本身。TalentCLEF为该领域提供了第一个公开基准，并鼓励开发稳健、公平且可迁移的语言技术，以应用于劳动力市场。|
|**2025-07-17**|**Automating Steering for Safe Multimodal Large Language Models**|Lyucheng Wu et.al.|[2507.13255](http://arxiv.org/abs/2507.13255)|null|近年来，多模态大型语言模型（MLLMs）在跨模态推理能力方面取得了重大进展，但也引发了对安全性的新担忧，尤其是在面对对抗性多模态输入时。为了提高MLLMs在推理过程中的安全性，我们提出了一种模块化和自适应的推理时干预技术，名为AutoSteer，该技术无需对底层模型进行微调。AutoSteer包含三个核心组件：（1）一种新颖的安全意识得分（SAS），能自动识别模型内部层中与安全最相关的区别；（2）一个自适应的安全探测器，经过训练以估计中间表示产生有害输出的可能性；（3）一个轻量级的拒绝头，当检测到安全风险时，会选择性干预以调节生成。在LLaVA-OV和Chameleon上进行的实验，针对多个安全关键基准，表明AutoSteer显著降低了文本、视觉和跨模态威胁的攻击成功率（ASR），同时保持了一般能力。这些发现将AutoSteer定位为一种实用、可解释且有效的框架，用于更安全地部署多模态AI系统。|
|**2025-07-16**|**Mitigating Object Hallucinations via Sentence-Level Early Intervention**|Shangpin Peng et.al.|[2507.12455](http://arxiv.org/abs/2507.12455)|null|多模态大型语言模型（MLLMs）在跨模态理解方面实现了革命性的突破，但仍然面临着幻觉问题——即与视觉输入相矛盾的人工编造内容。现有的幻觉缓解方法要么成本高昂，要么导致训练数据与模型输出之间的分布不匹配。我们认识到一个关键洞察：幻觉主要在文本生成的早期阶段产生，并通过后续输出传播。为了解决这个问题，我们提出了**SENTINEL**（**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain pr**E**ference **L**earning），一个不依赖人工标注的框架。具体来说，我们首先通过迭代采样模型输出，通过两个开放式词汇检测器验证物体存在，并将句子分类为幻觉/非幻觉类别，来启动高质量领域偏好对的生成。随后，我们使用上下文一致的正样本和幻觉负样本，通过迭代构建上下文感知偏好数据。最后，我们使用强调句子级别歧视性学习的上下文感知偏好损失（C-DPO）来训练模型。实验结果表明，与原始模型相比，SENTINEL可以减少超过90%的幻觉，在幻觉基准和一般能力基准上均优于之前的最佳方法，展示了其优越性和泛化能力。模型、数据集和代码可在https://github.com/pspdada/SENTINEL上获得。|
|**2025-07-16**|**Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models**|Yik Siu Chan et.al.|[2507.12428](http://arxiv.org/abs/2507.12428)|null|在生成最终回答之前，开放权重推理语言模型会生成长序列的思考链（CoTs），这提高了性能，但也引入了额外的对齐风险，有害内容往往同时出现在CoTs和最终输出中。在本工作中，我们研究了是否可以使用CoTs来预测最终回答的对齐问题。我们评估了一系列监控方法，包括人类、高度能力的大型语言模型和文本分类器，使用CoT文本或激活进行评估。首先，我们发现一个简单线性探针，在CoT激活上训练，可以在预测最终回答是否安全或不可安全方面显著优于所有基于文本的方法。CoT文本常常不够忠实且可能误导人类和分类器，而模型的潜在激活（即CoT激活）提供了一个更可靠的预测信号。其次，探针在推理完成之前就能做出准确的预测，即使在应用早期CoT段时也能取得良好性能。这些发现可以推广到不同大小的模型、模型系列和安全基准，表明轻量级探针可以启用生成过程中的实时安全监控和早期干预。|
|**2025-07-16**|**Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data**|Chandana Cheerla et.al.|[2507.12425](http://arxiv.org/abs/2507.12425)|null|随着组织越来越依赖专有企业数据，包括人力资源记录、结构化报告和表格文档，以进行关键决策。尽管大型语言模型（LLMs）具有强大的生成能力，但它们受到静态预训练、短上下文窗口以及处理异构数据格式挑战的限制。传统的检索增强生成（RAG）框架解决了这些问题中的一些，但通常难以处理结构化和半结构化数据。这项工作提出了一种高级RAG框架，该框架结合了使用密集嵌入（all-mpnet-base-v2）和BM25的混合检索策略，并通过SpaCy NER和跨编码器重排序的元数据感知过滤进行增强。该框架应用语义分块以保持文本连贯性，并保留表格数据结构以保持行列完整性。量化索引优化了检索效率，而人工反馈和对话记忆提高了适应性。在企业数据集上的实验表明，效果显著：Precision@5提高了15%（从90%提升至75%），Recall@5提高了13%（从87%提升至74%），Mean Reciprocal Rank提高了16%（从0.69提升至0.85）。定性评估显示，在5点李克特量表上，忠诚度（4.6对3.0）、完整性（4.2对2.5）和相关性（4.5对3.2）得分更高。这些结果表明，该框架在为企业任务提供准确、全面且与上下文相关的响应方面是有效的。未来的工作包括扩展到多模态数据和集成基于代理的检索。源代码将在https://github.com/CheerlaChandana/Enterprise-Chatbot发布。|
|**2025-07-16**|**SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?**|Xinyi He et.al.|[2507.12415](http://arxiv.org/abs/2507.12415)|null|在现实世界的软件工程中，代码性能优化至关重要，对于生产级系统尤为关键。尽管大型语言模型（LLMs）在代码生成和错误修复方面表现出令人印象深刻的能力，但它们在提高代码性能方面的专业水平在仓库级别上仍鲜有探索。为了填补这一空白，我们引入了SWE-Perf，这是第一个专门设计的基准，旨在系统地评估LLMs在真实仓库环境中代码性能优化任务上的能力。SWE-Perf包含140个精心挑选的实例，每个实例均源自从流行的GitHub仓库中提取的性能改进的拉取请求。每个基准实例包括相关的代码库、目标函数、性能相关测试、专家编写的补丁和可执行环境。通过对跨文件级别和仓库级别方法（例如无代理和OpenHands）的代表性方法的全面评估，我们揭示了现有LLMs与专家级优化性能之间存在显著的差距，突显了该新兴领域的关键研究机会。|
|**2025-07-16**|**Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning**|Jacinto Colan et.al.|[2507.12391](http://arxiv.org/abs/2507.12391)|null|大型语言模型（LLMs）在增强机器人路径规划方面展现出潜力。本文通过一个全面的基准评估了视觉输入在多模态LLMs中的效用。我们评估了15个多模态LLMs在2D网格环境中生成有效和最优路径的能力，模拟简化了机器人规划，比较了仅文本与文本加视觉输入在模型大小和网格复杂度不同的情况下的表现。我们的结果表明，在简单的较小网格上，视觉输入或少量文本提示提供了一些好处，成功率适中。然而，在较大的网格上，性能显著下降，突显了可扩展性的挑战。虽然较大的模型通常实现了更高的平均成功率，但对于这些多模态系统来说，视觉模态并不总是优于结构良好的文本，简单网格上的成功路径通常质量较高。这些结果表明当前在鲁棒空间推理、约束遵守和可扩展的多模态集成方面存在局限性，为LLMs在机器人路径规划中的未来发展指明了方向。|
|**2025-07-16**|**Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics**|Meysam Alizadeh et.al.|[2507.12372](http://arxiv.org/abs/2507.12372)|null|大型语言模型（LLMs）传统上依赖于静态的训练数据，它们的认知仅限于固定的快照。然而，最近的进步使LLMs具备了网页浏览能力，能够实时检索信息并在实时网页内容上进行多步推理。虽然先前的研究已经证明了LLMs访问和分析网站的能力，但它们直接检索和分析社交媒体数据的能力尚未得到探索。在这里，我们评估了网页浏览LLMs是否能够根据社交媒体用户的用户名推断其人口统计学属性。使用包含48个X（Twitter）账户的合成数据集和包含1,384名国际参与者的调查数据集，我们发现这些模型可以访问社交媒体内容并以合理的准确性预测用户的人口统计学特征。合成数据集的分析进一步揭示了LLMs如何解析和解释社交媒体个人资料，这可能对活动最少量的账户引入性别和政治偏见。尽管这种能力在API时代之后的计算社会科学中具有潜力，但它也引发了滥用的风险，尤其是在信息操作和定向广告中，这强调了安全措施的需求。我们建议LLM提供商在面向公众的应用中限制这种能力，同时保留为经过验证的研究目的提供受控访问。|
|**2025-07-16**|**Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate**|Ana Davila et.al.|[2507.12370](http://arxiv.org/abs/2507.12370)|null|大型语言模型（LLMs）在理解和生成人类语言方面表现出显著的能力，有助于与复杂系统进行更自然的交互。然而，它们面临着诸如LLMs处理用户请求中的模糊性等挑战。为了解决这些挑战，本文介绍并评估了一种多智能体辩论框架，旨在超越单个模型的检测和解决能力。该框架由三个LLM架构（Llama3-8B、Gemma2-9B和Mistral-7B变体）以及一个具有多样模糊性的数据集组成。辩论框架显著提高了Llama3-8B和Mistral-7B变体的性能，超过了它们的单个基线，其中Mistral-7B领导的辩论实现了显著的76.7%成功率，并在处理复杂模糊性和高效共识方面特别有效。虽然承认不同模型对协作策略的反应存在差异，但这些发现强调了辩论框架作为增强LLM能力的有针对性的方法的价值。这项工作通过展示结构化辩论如何导致交互系统中清晰度提高，为开发更健壮和自适应的语言理解系统提供了重要的见解。|
|**2025-07-16**|**GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities**|Diganta Misra et.al.|[2507.12367](http://arxiv.org/abs/2507.12367)|null|随着软件库的快速演变，代码生成面临相当大的障碍，需要不断适应频繁的版本更新，同时保持向后兼容性。虽然现有的代码演化基准提供了有价值的见解，但它们通常缺乏基于执行的评估，以生成符合特定库版本的代码。为了解决这个问题，我们引入了GitChameleon，这是一个新颖的、精心策划的数据集，包含328个Python代码补全问题，每个问题都基于特定的库版本，并附带可执行的单元测试。GitChameleon严格评估了当代大型语言模型（LLMs）、LLM驱动的代理、代码助手和RAG系统执行版本条件代码生成的能力，并通过执行证明其功能性准确性。我们的广泛评估表明，最先进系统在此任务上遇到重大挑战；企业模型在48-51%的成功率范围内达到基线，凸显了问题的复杂性。通过提供一个基于执行的基准，强调代码库的动态特性，GitChameleon有助于更清晰地理解这一挑战，并有助于指导更适应和可靠的AI代码生成方法的发展。我们将数据集和评估代码公开发布在https://github.com/mrcabbage972/GitChameleonBenchmark。|
|**2025-07-16**|**Thought Purity: Defense Paradigm For Chain-of-Thought Attack**|Zihao Xue et.al.|[2507.12314](http://arxiv.org/abs/2507.12314)|null|虽然通过强化学习训练的大型推理模型（LRM，例如Deepseek-R1）在大型语言模型（LLM）领域展现出高级的推理能力，但它们对安全威胁的易感性仍然是一个关键漏洞。这种弱点在思维链（CoT）生成过程中尤为明显，其中对抗性方法如后门提示攻击可以系统地颠覆模型的推理核心机制。新兴的思维链攻击（CoTA）通过利用提示可控性揭示了这一漏洞，同时通过低成本干预降低了CoT的安全性和任务性能。为了解决这种复合的安全性能漏洞，我们提出了思维纯净（TP）：一种防御范式，它系统地增强了对抗恶意内容的抵抗力，同时保持操作效率。我们的解决方案通过三个协同组件实现：1）安全优化的数据处理管道；2）强化学习增强的规则约束；3）自适应监控指标。我们的方法建立了第一个针对强化学习对齐的推理系统中的CoTA漏洞的综合防御机制，显著推进了下一代人工智能架构的安全-功能平衡。|
|**2025-07-16**|**Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization**|Prashanth Vijayaraghavan et.al.|[2507.12308](http://arxiv.org/abs/2507.12308)|null|大型语言模型（LLMs）在众多自然语言处理任务和领域中得到了广泛应用，展现了它们的适应性和有效性。在电子设计自动化（EDA）领域，LLMs 在诸如寄存器传输级（RTL）代码生成和总结等任务中显示出潜力。然而，尽管LLMs在通用代码相关任务中得到了广泛使用，但针对硬件描述语言（HDLs），尤其是VHDL的研究却相对匮乏。在本研究中，我们使用各种指标和两个数据集——VHDL-Eval和VHDL-Xform，评估了现有代码LLMs在VHDL代码生成和总结方面的性能。后者是一个内部数据集，旨在衡量LLMs对功能等效代码的理解。我们的发现表明，这些模型在不同指标上的表现均不尽如人意，突显了它们在这一领域的适用性存在重大差距。为了应对这一挑战，我们提出了链式描述（CoDes）方法，这是一种新颖的方法，旨在提高LLMs在VHDL代码生成和总结任务中的性能。CoDes包括基于以下内容生成一系列中间描述步骤：（i）代码生成的问题描述，以及（ii）总结的VHDL代码。然后，这些步骤与原始输入提示（问题描述或代码）相结合，并作为输入提供给LLMs以生成最终输出。我们的实验表明，CoDes方法在两个数据集上的各种指标上均显著优于标准提示策略。这种方法不仅提高了VHDL代码生成和总结的质量，还为未来旨在提升VHDL代码LLMs的研究提供了一个框架。|
|**2025-07-15**|**Streaming 4D Visual Geometry Transformer**|Dong Zhuo et.al.|[2507.11539](http://arxiv.org/abs/2507.11539)|null|从视频中感知和重建4维时空几何是一个基础但具有挑战性的计算机视觉任务。为了促进交互式和实时应用，我们提出了一种与自回归大型语言模型具有相似哲学的流式4维视觉几何变换器。我们探索了一种简单高效的设计，并采用因果变换器架构以在线方式处理输入序列。我们使用时间因果注意力，并将历史键值缓存作为隐式记忆以实现高效的流式长期4维重建。这种设计可以通过增量整合历史信息同时保持高质量的空间一致性来处理实时4维重建。为了高效训练，我们提出从密集的双向视觉几何地面变换器（VGGT）中提取知识到我们的因果模型中。对于推理，我们的模型支持将优化的高效注意力运算符（例如，FlashAttention）从大型语言模型领域迁移过来。在多个4维几何感知基准测试上的大量实验表明，我们的模型在在线场景中提高了推理速度，同时保持了有竞争力的性能，为可扩展和交互式4维视觉系统铺平了道路。代码可在以下网址获取：https://github.com/wzzheng/StreamVGGT。|
|**2025-07-15**|**DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering**|Yinsheng Li et.al.|[2507.11527](http://arxiv.org/abs/2507.11527)|null|大型语言模型（LLM）代理在解决现实世界问题方面展现出巨大潜力，并有望成为工业领域任务自动化的解决方案。然而，需要更多的基准来系统地评估从工业角度出发的自动化代理，例如在土木工程领域。因此，我们提出了DrafterBench，用于在技术绘图修订这一土木工程领域的表示任务中，对LLM代理进行全面评估。DrafterBench包含从现实世界绘图文件中总结出的十二种类型任务，共有46个定制功能/工具，总计1920个任务。DrafterBench是一个开源基准，用于严格测试AI代理解读复杂和长上下文指令、利用先验知识以及通过隐式策略意识适应动态指令质量的能力。该工具包全面评估了在结构化数据理解、功能执行、指令遵循和批判性推理方面的不同能力。DrafterBench提供了详细的任务准确性和错误统计分析，旨在更深入地了解代理能力，并确定将LLM集成到工程应用中的改进目标。我们的基准可在https://github.com/Eason-Li-AIS/DrafterBench上获取，测试集托管在https://huggingface.co/datasets/Eason666/DrafterBench。|
|**2025-07-15**|**LLM-based ambiguity detection in natural language instructions for collaborative surgical robots**|Ana Davila et.al.|[2507.11525](http://arxiv.org/abs/2507.11525)|null|自然语言指令中的歧义在安全关键型人机交互中存在重大风险，尤其是在手术等领域。为了解决这个问题，我们提出了一种框架，该框架使用大型语言模型（LLMs）专门针对协作手术场景进行歧义检测。我们的方法使用了一个LLM评估器的集合，每个评估器都配置了不同的提示技术，以识别语言、语境、程序和关键歧义。包括一个思维链评估器，以系统地分析指令结构中可能存在的问题。通过符合性预测对个别评估器进行综合，根据与标记的校准数据集的比较产生非符合性分数。评估Llama 3.2 11B和Gemma 3 12B，我们发现区分歧义指令和无歧义指令的分类准确率超过60%。我们的方法通过在机器人行动之前识别潜在的歧义指令，提高了手术中人机协作的安全性和可靠性。|
|**2025-07-15**|**AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air**|Shiyi Yang et.al.|[2507.11515](http://arxiv.org/abs/2507.11515)|null|在边缘设备上运行大型语言模型（LLMs）面临着有限的通信带宽、紧张的计算和内存成本等挑战。因此，云辅助的远程微调变得不可或缺。然而，现有的低秩自适应（LoRA）方法通常采用固定的或启发式的秩配置，所有LoRA参数的空中传输可能相当低效。为了解决这一局限性，我们开发了AirLLM，这是一个用于通信感知LoRA自适应的分层扩散策略框架。具体来说，AirLLM将秩配置建模为一个跨越所有LoRA插入投影的结构化动作向量。为了解决背后的高维序列决策问题，一个近端策略优化（PPO）智能体通过联合观察无线状态和语言复杂性来生成粗粒度决策，然后通过去噪扩散隐式模型（DDIM）细化，以产生高分辨率、任务和信道自适应的秩向量。这两个模块交替优化，DDIM在无分类器引导（CFG）范式下进行训练，以保持与PPO奖励的一致性。在变化的信噪比下的实验表明，AirLLM始终提升微调性能，同时显著降低传输成本，突出了强化驱动、扩散细化的秩自适应在可扩展和高效空中远程微调中的有效性。|
|**2025-07-15**|**LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer**|Yaoxian Dong et.al.|[2507.11457](http://arxiv.org/abs/2507.11457)|null|术前准确评估直肠癌淋巴结（LN）转移有助于指导治疗方案，然而，基于形态学标准的传统MRI评估在诊断性能上有限。尽管已经开发了一些人工智能模型，但它们通常作为黑盒运行，缺乏临床信任所需的解释性。此外，这些模型通常孤立地评估淋巴结，忽略了患者层面的背景。为了解决这些局限性，我们引入了LRMR，一个由LLM驱动的关联多节点排序框架。这种方法将诊断任务从直接的分类问题重新定义为结构化推理和排序过程。LRMR框架分为两个阶段。首先，一个多模态大型语言模型（LLM）分析患者所有淋巴结的合成图像，生成一份详细十种不同放射学特征的报告。其次，基于文本的LLM对不同患者的报告进行成对比较，根据不利特征的严重程度和数量建立相对风险排名。我们在117例直肠癌患者的回顾性队列上评估了我们的方法。LRMR实现了曲线下面积（AUC）为0.7917和F1分数为0.7200，超过了包括ResNet50（AUC 0.7708）在内的一系列深度学习基线。消融研究证实了我们的两个主要贡献的价值：移除关联排序阶段或结构化提示阶段会导致性能显著下降，AUC分别降至0.6875和0.6458。我们的工作表明，通过两阶段LLM框架将视觉感知与认知推理解耦，为评估直肠癌淋巴结转移提供了一种强大、可解释和有效的全新范式。|
|**2025-07-15**|**Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?**|Yanjian Zhang et.al.|[2507.11423](http://arxiv.org/abs/2507.11423)|null|人类推理涉及不同的策略，每种策略都适合特定的问题。先前的研究表明，大型语言模型（LLMs）往往倾向于采用单一的推理策略，这可能会限制它们在多样化的推理挑战中的有效性。在本研究中，我们调查了提示是否可以控制LLMs的推理策略，并评估其对逻辑问题解决的影响。虽然我们的实验表明没有单一策略能够持续提高准确率，但如果模型能够自适应地选择最佳策略，性能可以得到提升。我们提出了引导LLMs进行策略选择的方法，突出了提高它们推理能力的新途径。|
|**2025-07-15**|**Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations**|Miray Özcan et.al.|[2507.11417](http://arxiv.org/abs/2507.11417)|null|大型语言模型（LLMs）的环境影响正在显著上升，推理阶段现在占其整个生命周期碳排放的一半以上。然而，目前广泛使用的现有模拟框架在确定高效LLMs部署时缺乏对功率的概念，因此无法准确估计推理相关的排放。我们提出了一种模拟框架，用于评估不同部署设置下LLMs推理的能量和碳影响。首先，我们扩展了一个高保真LLMs推理模拟器，加入了一个基于利用率指标的GPU功率模型，从而能够分析批大小、序列长度和模型并行度等配置。其次，我们将模拟输出集成到能源系统协同模拟环境中，以量化特定电网条件下的碳排放，并探索碳感知调度的潜力。通过基于场景的分析，我们的框架揭示了推理参数如何影响能源需求和碳足迹，在一个示例部署案例中展示了高达69.2%的可再生能源抵消潜力，并为未来碳感知推理基础设施设计提供了基础。|
|**2025-07-15**|**Seq vs Seq: An Open Suite of Paired Encoders and Decoders**|Orion Weller et.al.|[2507.11412](http://arxiv.org/abs/2507.11412)|null|大型语言模型（LLM）社区几乎完全专注于仅使用解码器的语言模型，因为它们在文本生成方面更容易使用。然而，社区中仍有很大一部分人仍在使用仅使用编码器的模型来完成分类或检索等任务。先前的工作尝试比较这些架构，但被迫与具有不同参数数量、训练技术和数据集的模型进行比较。我们引入了SOTA开放数据Ettin模型套件：从1700万参数到10亿参数的成对仅编码器和仅解码器模型，在多达2000亿个标记上进行训练。使用相同的配方来训练仅编码器和仅解码器模型，在各自类别中产生了SOTA配方，作为编码器打败了ModernBERT，作为解码器打败了Llama 3.2和SmolLM2。像先前的工作一样，我们发现仅编码器模型在分类和检索任务上表现出色，而解码器模型在生成任务上表现出色。然而，我们表明，通过持续训练将解码器模型适应编码器任务（反之亦然）与仅使用反向目标（即400M编码器在MNLI上优于1B解码器，反之亦然在生成任务上）相比表现不佳。我们将本研究的所有成果开源，包括训练数据、按检查点分段的训练顺序以及200多个检查点，以允许未来的工作分析或扩展训练的所有方面。|
|**2025-07-15**|**KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?**|Soumadeep Saha et.al.|[2507.11408](http://arxiv.org/abs/2507.11408)|null|思维链追踪已被证明可以提高大型语言模型在众多推理任务中的性能，然而关于这种性能提升的机制尚未达成共识。为了进一步阐明这一点，我们引入了因果思维链图（CCGs），这是一种从推理追踪中自动提取的有向无环图，用于模拟语言模型输出中的细粒度因果依赖。来自MATH500、GSM8K和AIME的1671个数学推理问题及其相关的CCGs被编译到我们的数据集——KisMATH中。我们对15个开放式重量级LLM进行的详细实证分析表明：(i) CCG中的推理节点是最终答案的媒介，这是推理的必要条件；(ii) LLM强调CCG给出的推理路径，表明模型内部实现了类似于我们图表的结构。KisMATH允许进行受控的、与图对齐的干预，并为进一步研究思维链在LLM推理中的作用开辟了道路。|
|**2025-07-15**|**EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes**|LG AI Research et.al.|[2507.11407](http://arxiv.org/abs/2507.11407)|null|本技术报告介绍了EXAONE 4.0，该系统集成了非推理模式和推理模式，旨在实现EXAONE 3.5的优秀易用性和EXAONE Deep的高级推理能力。为了迎接代理人工智能时代的到来，EXAONE 4.0引入了代理工具使用等关键特性，并扩展了其多语言能力，支持英语和韩语外，还增加了西班牙语。EXAONE 4.0模型系列包括两种尺寸：一种中型的32B模型，针对高性能优化；另一种小型的1.2B模型，专为设备端应用设计。与同类开放式模型相比，EXAONE 4.0展现出卓越的性能，即使在对抗前沿级模型时也保持竞争力。这些模型可供研究使用，并可通过https://huggingface.co/LGAI-EXAONE轻松下载。|
|**2025-07-14**|**Fusing LLM Capabilities with Routing Data**|Tao Feng et.al.|[2507.10540](http://arxiv.org/abs/2507.10540)|null|大型语言模型（LLMs）的快速发展催生了一个多样化的架构生态系统，每个架构由于设计、训练数据和目标的不同而具有独特的优势。然而，大多数应用仍然依赖于单个后端模型，这限制了能力覆盖范围，并在处理复杂任务时导致性能和token成本的效率低下。我们强调了一个未被充分利用的机会：LLM路由数据，在托管平台将不同查询路由到不同模型时产生，这可以揭示不同任务之间的比较优势。为了解决这个问题，我们提出了FusionBench，这是一个全面的路由基准，涵盖了五个领域中的14个任务，使用了20个开源LLMs（参数量从8B到671B），捕获了1.03M个token，并总结了顶级模型的可重用思维模板。在此基础上，我们引入了FusionFactory，一个具有三个级别的系统融合框架：（1）查询级融合，为每个查询定制路由器，使用直接响应和推理增强的输出；（2）思维级融合，利用从表现最佳的LLMs对相似查询的答案中推导出的抽象模板；（3）模型级融合，通过蒸馏在模型之间转移能力，使用最佳响应或最高评判分数作为训练数据。实验表明，FusionFactory在所有14个基准测试中始终优于最佳的单一LLM，最优融合配置因基准而异，这证明了系统化LLM融合在利用互补优势和提高整体性能方面的价值。|
|**2025-07-14**|**CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks**|Hongchao Jiang et.al.|[2507.10535](http://arxiv.org/abs/2507.10535)|null|大型语言模型（LLMs）在众多编码任务上显著提升了技术水平。除了直接回答用户查询外，LLMs还可以作为评委，评估和比较其他模型生成的响应质量。这种评估能力对于比较不同LLMs和通过响应排名提高响应质量至关重要。然而，尽管LLM作为评委的范式越来越受欢迎，但由于缺乏专门的基准，其在编码场景中的有效性仍然没有得到充分探索。为了填补这一空白，我们引入了CodeJudgeBench，这是一个专门设计的基准，用于评估LLM作为评委在三个关键编码任务上的性能：代码生成、代码修复和单元测试生成。通过对26个LLM作为评委的模型进行全面的基准测试，我们发现最近的思维模型在我们的精心设计的代码评估任务中显著优于非思维模型。值得注意的是，即使是相对较小的思维模型，如Qwen3-8B，也能在评估任务中超越70B大小的专门训练的LLM作为评委模型。然而，所有模型在评估编码任务时仍表现出显著的随机性。对于成对评估任务，仅仅改变响应呈现的顺序就会对准确性产生重大影响。此外，当评估不同LLM编写的代码和单元测试时，LLM作为评委模型也表现出性能的波动。这种敏感性引起了人们对LLM作为评委在编码场景中可靠性和一致性的担忧。最后，我们研究了LLM作为评委的最佳提示策略。我们发现，使用成对比较优于标量点对点判断。此外，保留完整、未经处理的LLM响应中的注释和推理可以提高评委的表现。|
|**2025-07-14**|**Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination**|Mingqi Wu et.al.|[2507.10532](http://arxiv.org/abs/2507.10532)|null|大型语言模型（LLMs）的推理能力一直是研究的热点。近期的研究通过强化学习（RL）进一步提升了这些能力，许多新方法声称在最小或没有外部监督的情况下取得了显著的改进。令人惊讶的是，一些研究甚至表明随机或不正确的奖励信号可以提高推理性能。然而，这些突破大多报告在Qwen2.5模型系列上，并在诸如MATH-500、AMC和AIME等知名基准上进行评估，而在其他如Llama的模型上未能实现类似的效果，这值得进一步研究。我们的分析表明，尽管Qwen2.5在数学推理上表现出色，但其在大规模网络语料库上的预训练使其容易受到流行基准中的数据污染。因此，从这些基准得到的结果可能不可靠。为了解决这个问题，我们引入了一个生成器，它可以生成任意长度和难度的完全合成算术问题，从而产生一个我们称之为RandomCalculation的干净数据集。使用这些无泄露的数据集，我们表明只有准确的奖励信号可以持续提高性能，而噪声或不正确的信号则不行。我们主张在不受污染的基准和不同的模型系列上评估RL方法，以确保得出可靠的结论。|
|**2025-07-14**|**Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI**|Jiangkai Wu et.al.|[2507.10510](http://arxiv.org/abs/2507.10510)|null|AI视频聊天作为一种新的实时通信（RTC）范式出现，其中一个通信方不是人类，而是一个多模态大型语言模型（MLLM）。这使得人类与AI的交互更加直观，仿佛面对面与人交谈。然而，这给延迟带来了重大挑战，因为MLLM推理占据了大部分响应时间，留给视频流的时间非常有限。由于网络的不确定性和不稳定性，传输延迟成为阻止AI像真人一样的关键瓶颈。为了解决这个问题，我们提出了Artic，一个面向AI的实时通信框架，探索网络需求从“人类观看视频”到“AI理解视频”的转变。为了在保持MLLM准确性的同时大幅降低比特率，我们提出了上下文感知视频流，该技术能够识别每个视频区域对于聊天的重要性，并将比特率几乎完全分配给聊天重要的区域。为了避免数据包重传，我们提出了具有损失恢复的自适应帧率，该技术利用先前帧来替代丢失/延迟的帧，同时避免比特率浪费。为了评估视频流质量对MLLM准确性的影响，我们构建了第一个基准，命名为降级视频理解基准（DeViBench）。最后，我们讨论了一些关于AI视频聊天的开放性问题以及正在进行的解决方案。|
|**2025-07-14**|**Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance**|Kyungtae Han et.al.|[2507.10500](http://arxiv.org/abs/2507.10500)|null|随着自动驾驶技术的不断进步，当前的先进驾驶辅助系统（ADAS）在解释场景上下文或通过自然语言与驾驶员互动方面的能力仍然有限。这些系统通常依赖于预定义的逻辑，缺乏基于对话的交互支持，因此在动态环境或适应驾驶员意图时缺乏灵活性。本文提出了一种名为场景感知对话ADAS（SC-ADAS）的模块化框架，该框架集成了包括大型语言模型、视觉到文本解释和结构化功能调用在内的生成式AI组件，以实现实时、可解释和自适应的驾驶辅助。SC-ADAS支持基于视觉和传感器上下文的多次对话，允许自然语言建议和驾驶员确认的ADAS控制。该系统在CARLA模拟器和基于云的生成式AI上实现，能够执行经过确认的用户意图，作为结构化的ADAS命令，无需对模型进行微调。我们评估了SC-ADAS在场景感知、对话和重访的多次交互方面的性能，突出了如基于视觉的上下文检索导致的延迟增加和累积对话历史导致的标记增长等权衡。这些结果表明，将对话推理、场景感知和模块化ADAS控制相结合，以支持下一代智能驾驶辅助系统的可行性。|
|**2025-07-14**|**Can You Detect the Difference?**|İsmail Tarım et.al.|[2507.10475](http://arxiv.org/abs/2507.10475)|null|大型语言模型（LLMs）的快速发展引发了对可靠检测AI生成文本的担忧。在自回归（AR）输出上， stylometric度量方法效果良好，但它们在基于扩散模型上的有效性尚不明确。我们使用2000个样本，首次系统地比较了基于扩散生成的文本（LLaDA）和基于自回归生成的文本（LLaMA）。困惑度、爆发性、词汇多样性、可读性和BLEU/ROUGE分数表明，LLaDA在困惑度和爆发性上与人类文本非常相似，导致针对AR的检测器有很高的假阴性率。LLaMA的困惑度明显较低，但词汇准确性有所降低。依赖单一指标无法将扩散输出与人类写作区分开来。我们强调了扩散感知检测器的必要性，并概述了混合模型、针对扩散的特定 stylometric特征和鲁棒水印等方向。|
|**2025-07-14**|**MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking**|Mohamed T. Younes et.al.|[2507.10472](http://arxiv.org/abs/2507.10472)|null|本文介绍了一种创新的求职者跟踪系统（ATS），该系统通过一种新型的机器人流程自动化（RPA）框架得到增强，以下简称MLAR。传统的招聘流程由于时间和资源限制，在简历筛选和候选人筛选阶段常常遇到瓶颈。MLAR通过在三个不同的层次使用大型语言模型（LLMs）来解决这些挑战：在第一层从职位发布中提取关键特征，在第二层解析求职者简历以识别教育、经验和技能，在第三层进行相似度匹配。然后通过高级语义算法将这些特征进行匹配，以高效地识别最佳候选人。我们的方法能够无缝集成到现有的RPA管道中，自动化简历解析、职位匹配和候选人通知。广泛的功能基准测试表明，MLAR在处理大量简历的任务中优于领先的RPA平台，包括UiPath和Automation Anywhere。在处理2,400份简历时，MLAR的平均处理时间为每份简历5.4秒，与Automation Anywhere相比减少了大约16.9%，与UiPath相比减少了17.1%。这些结果突显了MLAR通过提供一种高效、准确且可扩展的解决方案，以适应现代招聘需求，从而改变招聘流程的潜力。|
|**2025-07-14**|**An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments**|Mikko Korkiakoski et.al.|[2507.10469](http://arxiv.org/abs/2507.10469)|null|人工智能（AI）的进步显著提高了虚拟现实（VR）中非玩家角色（NPCs）的真实感和互动性，创造了更吸引人和逼真的用户体验。本文评估了VR审讯模拟器中的AI驱动NPC，重点关注其感知的真实感、可用性和系统性能。该模拟器包含两个由AI驱动的NPC，一名嫌疑人和一名搭档，使用GPT-4 Turbo与参与者互动，以确定嫌疑人的有罪或无罪。一项包含18名参与者的用户研究使用了系统可用性量表（SUS）、游戏体验问卷（GEQ）和虚拟代理可信度问卷来评估系统，同时测量了语音转文本（STT）、文本转语音（TTS）、OpenAI GPT-4 Turbo和整体（周期）延迟。结果显示平均周期延迟为7秒，受对话上下文增加的影响。可信度评分为6.67分（满分10分），在行为、社交关系和智力方面得分高，但在情感和个性方面得分中等。系统获得了79.44的SUS分数，表明其可用性良好。这些发现证明了大型语言模型在提高VR中NPC真实感和交互性方面的潜力，同时突出了减少系统延迟和增强情感深度的挑战。这项研究有助于更高级AI驱动NPC的开发，揭示了为了实现越来越沉浸式的虚拟体验，需要进行性能优化的必要性。|
|**2025-07-14**|**Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems**|Hammad Atta et.al.|[2507.10457](http://arxiv.org/abs/2507.10457)|null|首先，我们分析摘要中的每个部分：  1. "The integration of large language models (LLMs) into enterprise systems has created a new class of covert security vulnerabilities" - 这部分说明将大型语言模型（LLMs）集成到企业系统中产生了新的隐蔽安全漏洞。  2. "particularly within logic-execution layers and persistent-memory contexts." - 这部分强调这些漏洞尤其在逻辑执行层和持久存储环境中。  3. "In this paper, we introduce Logic-Layer Prompt Control Injection (LPCI)" - 这部分提到本文介绍了一种名为逻辑层提示控制注入（LPCI）的新攻击类型。  4. "a novel attack category in which encoded, delayed, and conditionally triggered payloads are embedded in memory, vector stores, or tool outputs." - 这部分描述了LPCI攻击的特点，即编码、延迟和条件触发的有效负载被嵌入到内存、向量存储或工具输出中。  5. "These payloads can bypass conventional input filters and trigger unauthorised behaviour across sessions." - 这部分说明这些有效负载可以绕过传统的输入过滤器，并在会话中触发未经授权的行为。  现在，我将上述分析合并为中文翻译：  将大型语言模型（LLMs）集成到企业系统中产生了新的隐蔽安全漏洞，尤其是在逻辑执行层和持久存储环境中。在本文中，我们介绍了一种名为逻辑层提示控制注入（LPCI）的新型攻击类别，其中编码、延迟和条件触发的有效负载被嵌入到内存、向量存储或工具输出中。这些有效负载可以绕过传统的输入过滤器，并在会话中触发未经授权的行为。|
|**2025-07-14**|**Text-Visual Semantic Constrained AI-Generated Image Quality Assessment**|Qiang Li et.al.|[2507.10432](http://arxiv.org/abs/2507.10432)|null|随着人工智能生成图像（AGI）技术的快速发展，对其质量的准确评估变得日益重要。现有方法通常依赖于CLIP或BLIP等跨模态模型来评估文本-图像对齐和视觉质量。然而，将这些方法应用于AGI时，遇到了两个主要挑战：语义错位和细节感知缺失。为了解决这些局限性，我们提出了文本-视觉语义约束人工智能生成图像质量评估（SC-AGIQA），这是一个统一的框架，利用文本-视觉语义约束来显著增强对AI生成图像的文本-图像一致性以及感知失真的综合评估。我们的方法整合了多个模型的关键能力，通过引入两个核心模块来应对上述挑战：文本辅助语义对齐模块（TSAM），该模块利用多模态大型语言模型（MLLMs）通过生成图像描述并与原始提示进行比较来弥合语义差距，从而进行精细的一致性检查；以及频域精细粒度退化感知模块（FFDPM），该模块借鉴人类视觉系统（HVS）的特性，通过采用频域分析与感知敏感性加权相结合的方法，更好地量化微小的视觉失真，并增强对图像中精细视觉质量细节的捕捉。在多个基准数据集上进行的广泛实验表明，SC-AGIQA优于现有最先进的方法。代码已在https://github.com/mozhu1/SC-AGIQA上公开。|
|**2025-07-11**|**Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective**|Hangjie Yuan et.al.|[2507.08801](http://arxiv.org/abs/2507.08801)|null|自回归大型语言模型（LLMs）统一了广泛的语言任务，激发了自回归视频生成的初步努力。现有的自回归视频生成器要么偏离了标准的LLM架构，要么依赖于庞大的外部文本编码器，要么由于下一标记解码而承受了巨大的延迟。在本文中，我们介绍了Lumos-1，这是一种自回归视频生成器，它通过最小的架构修改保留了LLM架构。为了在LLMs中注入时空相关性，我们确定了引入3D RoPE的有效性并诊断了其不平衡的频率谱范围。因此，我们提出了MM-RoPE，这是一种RoPE方案，在保留原始文本RoPE的同时，提供了全面的频率谱和缩放后的3D位置，以建模多模态时空数据。此外，Lumos-1采用了一种遵循帧内双向性和帧间时间因果性的标记依赖策略。基于这种依赖策略，我们识别了由空间信息冗余引起的帧级损失不平衡问题，并通过提出自回归离散扩散强制（AR-DF）来解决它。AR-DF在训练期间引入了时间管掩码，并采用兼容的推理时间掩码策略以避免质量下降。通过使用内存高效的训练技术，我们在仅48个GPU上预训练了Lumos-1，实现了与EMU3在GenEval上、COSMOS-Video2World在VBench-I2V上、OpenSoraPlan在VBench-T2V上相当的性能。代码和模型可在https://github.com/alibaba-damo-academy/Lumos上找到。|
|**2025-07-11**|**One Token to Fool LLM-as-a-Judge**|Yulai Zhao et.al.|[2507.08794](http://arxiv.org/abs/2507.08794)|null|生成式奖励模型（也称为“作为裁判的LLM”）利用大型语言模型（LLM）来评估答案质量，在可验证奖励的强化学习（RLVR）中越来越受欢迎。它们通常比基于严格规则的指标更受欢迎，尤其是在涉及自由形式输出的复杂推理任务中。在这个范式下，LLM通常会被提示将候选答案与基准参考进行比较，并分配一个表示正确性的二元奖励。尽管这种比较任务看似简单，但我们发现生成式奖励模型对表面操纵表现出令人惊讶的脆弱性：非单词符号（例如“:”或“.”）或像“思维过程：”和“让我们一步一步解决这个问题。”这样的推理开头，常常会导致错误的积极奖励。我们证明这种弱点在LLM、数据集和提示格式中普遍存在，对依赖于生成式奖励模型的核心算法范式（如拒绝采样、偏好优化和RLVR）构成了严重威胁。为了减轻这个问题，我们引入了一种简单而有效的数据增强策略，并训练了一个具有显著改进鲁棒性的新生成式奖励模型。我们的发现强调了更可靠的基于LLM的评估方法的迫切需求。我们发布了我们的鲁棒、通用领域的奖励模型及其合成训练数据，可在https://huggingface.co/sarosavo/Master-RM和https://huggingface.co/datasets/sarosavo/Master-RM获取。|
|**2025-07-11**|**BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity**|Chenyang Song et.al.|[2507.08771](http://arxiv.org/abs/2507.08771)|null|为了减轻大型语言模型（LLM）的计算负担，具有激活稀疏性的架构，如专家混合（MoE），越来越受到关注。然而，传统MoE的非可导和僵化的路由方式损害了模型性能。此外，尽管每个标记只激活少量参数，但这些稀疏激活的架构表现出较低的块级稀疏性，这意味着多个连续标记的并集激活了大量参数。这种稀疏模式在低资源条件下（例如端侧设备）加速时不太友好，并且与主流加速技术（例如投机解码）不兼容。为了解决这些挑战，我们引入了一种新的MoE架构，BlockFFN，以及其高效的训练和部署技术。具体来说，我们使用了一种集成了ReLU激活和RMSNorm的路由器，以实现可导和灵活的路由。接下来，为了促进标记级稀疏性（TLS）和块级稀疏性（CLS），我们设计了CLS感知的训练目标，使BlockFFN更加适合加速。最后，我们实现了高效的加速内核，首次结合了激活稀疏性和投机解码。实验结果表明，BlockFFN比其他MoE基线性能更优，实现了超过80%的TLS和70%的8标记CLS。我们的内核比密集模型在真实端侧设备上实现了高达3.67倍的加速。所有代码和检查点均公开可用（https://github.com/thunlp/BlockFFN）。|
|**2025-07-11**|**Multilingual Multimodal Software Developer for Code Generation**|Linzheng Chai et.al.|[2507.08719](http://arxiv.org/abs/2507.08719)|null|大型语言模型（LLMs）的快速发展显著提高了代码生成能力，然而，大多数模型仍然仅限于文本，忽视了在现实软件开发中至关重要的视觉辅助工具，如图表和流程图。为了弥合这一差距，我们引入了MM-Coder，一个多语言多模态的软件开发者。MM-Coder将视觉设计输入——统一建模语言（UML）图和流程图（统称为视觉工作流）——与文本指令相结合，以提高代码生成的准确性和架构一致性。为了实现这一点，我们开发了MMc-Instruct，这是一个包含基于视觉工作流的代码生成的多样化多模态指令微调数据集，使MM-Coder能够像人类开发者一样综合文本和图形信息，与之前针对狭窄任务的成果不同。此外，我们引入了MMEval，一个用于评估多模态代码生成的新基准，解决了现有仅限于文本的局限性。我们使用MMEval进行的评估突显了模型在精确捕捉视觉信息、遵循指令和高级编程知识方面仍存在的重大挑战。我们的工作旨在通过使LLMs能够解释和实施通过文本和视觉设计传达的复杂规范，从而革新工业编程。|
|**2025-07-11**|**KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation**|Songlin Zhai et.al.|[2507.08704](http://arxiv.org/abs/2507.08704)|null|知识图谱（KGs）在通过引入结构化和有根据的知识到学习过程中发挥着关键作用，以增强大型语言模型（LLMs）。然而，大多数现有的KG增强方法依赖于参数密集的微调，这可能导致灾难性遗忘并降低预训练模型的泛化能力。此外，由于它们的静态集成框架，它们对实时知识更新的适应性有限。为了解决这些问题，我们引入了第一个针对LLMs的测试时KG增强框架，该框架围绕一个专门的知识图谱引导注意力（KGA）模块构建，该模块能够实现无需参数更新的动态知识融合。所提出的KGA模块通过两种协同路径——外向和内向聚合——增强了标准的自注意力机制。具体来说，外向路径通过输入驱动的KG融合动态地将外部知识整合到输入表示中。内向聚合通过KG引导的过滤来补充外向路径，抑制与任务无关的信号并增强与知识相关的模式。重要的是，在外向路径处理知识融合的同时，内向路径选择最相关的三元组并将其反馈到融合过程中，形成一个闭环增强机制。通过协同结合这两条路径，所提出的方法仅在测试时支持实时知识融合，而不进行任何参数修改。在五个基准上的大量实验验证了KGA在知识融合方面的可比性能。|
|**2025-07-11**|**ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way**|Rajarshi Roy et.al.|[2507.08679](http://arxiv.org/abs/2507.08679)|null|我们引入了ByDeWay，这是一个无需训练的框架，旨在提升多模态大型语言模型（MLLMs）的性能。ByDeWay采用了一种名为分层深度提示（LDP）的新颖提示策略，该策略在不修改任何模型参数的情况下，提升了空间推理和定位能力。它使用单目深度估计将场景分为最近层、中程层和最远层，然后利用基于定位的视觉-语言模型生成区域特定的描述。这些结构化、深度感知的描述被附加到图像-问题提示中，为其增添了空间上下文。这引导MLLMs产生更多基于现实且幻觉较少的回应。我们的方法轻量级、模块化，且与黑盒MLLMs兼容。在幻觉敏感（POPE）和推理密集（GQA）基准测试上的实验表明，该方法在多个MLLMs上均实现了持续的性能提升，验证了在零训练设置下深度感知提示的有效性。|
|**2025-07-11**|**LLMCup: Ranking-Enhanced Comment Updating with LLMs**|Hua Ge et.al.|[2507.08671](http://arxiv.org/abs/2507.08671)|null|尽管注释对于提升现代软件项目的代码可读性和可维护性至关重要，但开发者通常更倾向于更新代码而不是注释，导致文档过时或不一致，从而阻碍未来的理解和维护。近期的方法，如CUP和HebCup，分别尝试使用神经序列到序列模型和启发式规则进行自动注释更新。然而，这些方法在注释更新过程中可能会遗漏或误解关键信息，导致注释不准确，并且它们在处理复杂的更新场景时常常遇到困难。鉴于这些挑战，利用大型语言模型（LLMs）是一个有前景的方向，因为LLMs在注释生成、代码合成和程序修复等软件工程任务中表现出令人印象深刻的能力。这表明它们在捕捉代码修改背后的逻辑——这是注释更新任务中至关重要的能力。尽管如此，为每个更新案例选择合适的提示策略仍然具有挑战性。为了解决这个问题，我们提出了一种新颖的注释更新框架，LLMCup，它首先利用多种提示策略通过LLM提供多样化的候选更新注释，然后采用排名模型CupRank来选择最佳候选作为最终的更新注释。实验结果证明了LLMCup的有效性，与最先进的基线（CUP和HebCup）相比，在准确率上提高了49.0%-116.9%，在BLEU-4上提高了10.8%-20%，在METEOR上提高了4.6%，在F1上提高了0.9%-1.9%，在SentenceBert相似度上提高了2.1%-3.4%。此外，一项用户研究表明，由LLMCup更新的注释有时甚至超过人类编写的注释，这突出了在注释质量评估中纳入人类评估的重要性。|
|**2025-07-11**|**KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment**|Jiyao Zhang et.al.|[2507.08665](http://arxiv.org/abs/2507.08665)|null|现代大型语言模型（LLMs）在将非正式数学形式化为机器可验证的定理方面取得了有希望的进展。然而，由于多语言平行语料库的量和质有限，这些方法仍然面临瓶颈。在本文中，我们提出了一种新颖的神经符号框架KELPS（基于知识方程的逻辑处理系统）来解决这些问题。KELPS是一个将翻译、综合和过滤非正式数据到多种形式语言（Lean、Coq和Isabelle）的迭代框架。首先，我们将自然语言翻译成我们设计的、在断言逻辑基础上理论化的新型语言——知识方程（KEs）。接着，我们通过严格定义的规则将它们转换为目标语言，这些规则既保留了句法结构，也保留了语义意义。这个过程产生了一个包含超过60,000个问题的平行语料库。我们的框架在MiniF2F上实现了88.9%的句法准确性（pass@1），在多个数据集上优于SOTA模型，如Deepseek-V3（81%）和Herald（81.3%）。所有数据集和代码均可在补充材料中找到。|
|**2025-07-11**|**Introspection of Thought Helps AI Agents**|Haoran Sun et.al.|[2507.08664](http://arxiv.org/abs/2507.08664)|null|AI代理依赖大型语言模型（LLMs）和多模态-LLMs（MLLMs）在不进行后训练的情况下执行文本和图像任务中的解释和推理，其中LLMs和MLLMs发挥着最关键的作用，并决定了AI代理的初始能力和局限性。通常，AI代理利用复杂的提示工程和外部推理框架与LLMs进行有前景的交互，例如思维链、思维迭代和思维图像。然而，它们仍然受限于LLM在理解自然语言方面的固有局限性，以及迭代推理过程会产生大量的推理成本。为此，我们通过在提示中设计新的LLM-Read代码，提出了一种具有思维内省（INoT）的AI代理推理框架。它使LLM能够根据提示中的代码执行程序性对话推理过程。因此，自我否定和反思发生在LLM内部而不是外部，这可以有效地减少token成本。通过我们在三个不同任务的六个基准上的实验，验证了INoT的有效性，性能平均提高了7.95%，超过了基线。此外，INoT的token成本平均比基线中表现最佳的方法低58.3%。此外，我们通过验证实验展示了INoT在图像解释和推理中的多功能性。|
|**2025-07-11**|**Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning**|Xingguang Ji et.al.|[2507.08649](http://arxiv.org/abs/2507.08649)|null|我们介绍了我们的Leanabell-Prover-V2，这是一个7B参数的大规模语言模型（LLM），能够在Lean 4中生成形式化的定理证明，并集成了验证器整合的长思维链（CoT）。在继承我们之前的工作Leanabell-Prover-V1的基础上，我们继续选择对现有的强大证明模型进行微调以进一步提高性能。在我们的V2版本中，我们主要升级了强化学习（RL），并引入了由Lean 4验证器提供的反馈。关键的是，验证器反馈，如指示成功或详细说明特定错误，使LLM能够“自我意识”其推理过程的正确性，并学会反射性地纠正错误。Leanabell-Prover-V2通过多轮验证器交互直接优化LLM的推理轨迹，同时结合反馈标记掩码进行稳定的RL训练和简单的奖励策略。实验表明，在MiniF2F测试集上，Leanabell-Prover-V2与Kimina-Prover-Preview-Distill-7B相比提高了3.2%（pass@128），与DeepSeek-Prover-V2-7B相比提高了2.0%（pass@128）。源代码、整理后的数据和模型可在以下网址获取：https://github.com/Leanabell-LM/Leanabell-Prover-V2。|
|**2025-07-10**|**Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs**|Ziyue Li et.al.|[2507.07996](http://arxiv.org/abs/2507.07996)|null|能否让预训练的神经网络在不进行微调的情况下适应不同的输入？我们是否需要所有层来处理简单任务，它们是否足够应对挑战性的任务？我们发现，预训练的大型语言模型（LLM）的层可以被作为独立的模块来构建一个更好甚至更浅的模型，该模型针对每个测试样本进行定制。特别是，预训练模型的每一层都可以作为循环神经网络（RNN）跳过/剪枝或重复多次，以任意顺序与其他层堆叠，为每个样本生成一个层链（CoLa）。这种组合空间极大地扩展了现有关于循环/递归预训练模块、层剪枝或早期退出网络工作的范围。我们开发了一种蒙特卡洛树搜索（MCTS）协议，以探索和识别数学和常识推理基准中每个样本的最佳CoLa。与固定深度的静态模型相比，CoLa允许捷径路径（快速思考）、相同层（慢速思考）的重复，以及两者的结合，为不同的输入提供更灵活、动态的架构。我们对MCTS优化的CoLa进行了广泛的分析，导致以下两个关键发现：（1）对于超过75%的原LLM预测正确的样本，我们可以找到更短的CoLa，这表明提高推理效率有很大的空间；（2）对于超过60%原本预测错误的样本，我们可以识别出实现正确预测的CoLa，这表明有很大的性能提升空间。我们的结果突出了使用固定架构的预训练LLM在不同样本上进行推理的不足，并为解锁测试时深度自适应的泛化能力铺平了道路。|
|**2025-07-10**|**Multigranular Evaluation for Brain Visual Decoding**|Weihao Xia et.al.|[2507.07993](http://arxiv.org/abs/2507.07993)|null|现有的脑视觉解码评估协议主要依赖于粗略的指标，这些指标掩盖了模型之间的差异，缺乏神经科学基础，并且无法捕捉精细的视觉区分。为了解决这些局限性，我们引入了BASIC，这是一个统一的多粒度评估框架，它联合量化了解码图像与真实图像之间的结构保真度、推理对齐和情境连贯性。在结构层面，我们引入了一套基于分割的层次化指标，包括前景、语义、实例和组件掩模，这些指标基于掩模结构间的粒度感知对应关系。在语义层面，我们使用多模态大型语言模型提取包括对象、属性和关系在内的结构化场景表示，从而能够与真实刺激进行详细、可扩展和情境丰富的比较。我们在统一评估框架内，对多个刺激-神经影像数据集上的多种视觉解码方法进行了基准测试。这些标准共同为衡量脑视觉解码方法提供了一个更具区分性、可解释性和全面的基座。|
|**2025-07-10**|**Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs**|Jeongseok Hyun et.al.|[2507.07990](http://arxiv.org/abs/2507.07990)|null|视频大型语言模型（LLMs）通过利用大量时空标记来实现强大的视频理解，但它们随着标记数量的增加呈现出二次计算扩展。为了解决这个问题，我们提出了一种无需训练的时空标记合并方法，称为STTM。我们的关键洞察是利用视频数据中的局部空间和时间冗余，这在先前的工作中被忽视了。STTM首先使用对四叉树结构的粗到细搜索，将每一帧转换为多粒度空间标记，然后沿着时间维度进行有向成对合并。这种分解合并方法在六个视频问答基准上优于现有的标记减少方法。值得注意的是，STTM在50%的标记预算下仅以0.5%的准确度下降实现了2倍的速度提升，在30%的预算下仅以2%的下降实现了3倍的速度提升。此外，STTM是查询无关的，允许在不同问题中跨相同视频重用KV缓存。项目页面可在https://www.jshyun.me/projects/sttm上找到。|
|**2025-07-10**|**Automating Expert-Level Medical Reasoning Evaluation of Large Language Models**|Shuang Zhou et.al.|[2507.07988](http://arxiv.org/abs/2507.07988)|null|随着大型语言模型（LLMs）在临床决策中的日益集成，确保透明和可信的推理至关重要。然而，现有LLMs医疗推理能力评估策略要么评估不令人满意，要么可扩展性差，且缺乏严格的基准。为了解决这个问题，我们引入了MedThink-Bench，这是一个旨在对LLMs医疗推理进行严格、可解释和可扩展评估的基准。MedThink-Bench包含500个跨越十个医学领域具有挑战性的问题，每个问题都附有专家精心制作的逐步推理说明。在此基础上，我们提出了LLM-w-Ref，这是一种新颖的评估框架，利用细粒度推理和LLM作为裁判的机制，以专家级别的准确性评估中间推理，同时保持可扩展性。实验表明，LLM-w-Ref与专家判断表现出强烈的正相关。对十二个最先进的LLMs进行基准测试，我们发现较小的模型（例如，MedGemma-27B）可以超越较大的专有对手（例如，OpenAI-o3）。总的来说，MedThink-Bench为评估LLMs医疗推理提供了一个基础工具，推动了它们在临床实践中的安全且负责任的部署。|
|**2025-07-10**|**OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding**|JingLi Lin et.al.|[2507.07984](http://arxiv.org/abs/2507.07984)|null|近年来，多模态大型语言模型（MLLMs）在整合视觉和语言进行复杂推理方面展现出惊人的能力。虽然大多数现有基准评估模型时都是在离线设置下，使用一组固定的预录制输入，但我们引入了OST-Bench，这是一个旨在从主动探索场景的代理的角度评估在线时空理解的基准。在线方面强调了对逐步获取的观察结果进行处理和推理的需求，而时空组件则需要将当前的视觉输入与历史记忆相结合，以支持动态空间推理。OST-Bench更好地反映了现实世界具身感知的挑战。OST-Bench建立在高效的数据收集管道之上，由来自ScanNet、Matterport3D和ARKitScenes的1.4k个场景和10k个问答对组成。我们在OST-Bench上评估了几种领先的MLLMs，并观察到它们在需要复杂时空推理的任务上表现不足。在在线设置下，随着探索范围扩大和记忆增长，它们的准确率下降。通过进一步的实验分析，我们识别出模型之间的共同错误模式，并发现复杂的线索基础空间推理需求以及长期记忆检索需求显著降低了模型性能的两个不同维度，突出了必须解决的核心挑战，以改善在线具身推理。为了促进该领域的进一步研究和开发，我们的代码、数据集和基准均可供使用。我们的项目页面是：https://rbler1234.github.io/OSTBench.github.io/|
|**2025-07-10**|**Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology**|Sabine Felde et.al.|[2507.07983](http://arxiv.org/abs/2507.07983)|null|首先，我们需要识别摘要中的关键术语和句子结构：  1. "Large language models (LLMs)" - 大型语言模型 2. "supporting clinical decision-making" - 支持临床决策 3. "complex fields such as rheumatology" - 如风湿病学等复杂领域 4. "Our evaluation shows" - 我们的评估显示 5. "smaller language models (SLMs)" - 较小的语言模型 6. "combined with retrieval-augmented generation (RAG)" - 结合检索增强生成（RAG） 7. "achieve higher diagnostic and therapeutic performance" - 实现更高的诊断和治疗性能 8. "larger models" - 较大的模型 9. "requiring substantially less energy" - 需要的能量显著更少 10. "enabling cost-efficient, local deployment" - 使成本效益高、本地部署成为可能 11. "resource-limited healthcare" - 资源有限的医疗保健 12. "However" - 然而 13. "expert oversight remains essential" - 专家监督仍然是必不可少的 14. "no model consistently reached specialist-level accuracy in rheumatology" - 没有任何模型在风湿病学中持续达到专家级准确度  接下来，我们将这些术语和句子结构组合成一个连贯的中文翻译：  大型语言模型（LLMs）在支持如风湿病学等复杂领域的临床决策方面显示出潜力。我们的评估显示，较小的语言模型（SLMs）结合检索增强生成（RAG）技术，在诊断和治疗性能上优于较大的模型，同时所需的能量显著更少，并使成本效益高、本地部署成为可能。这些特性对于资源有限的医疗保健领域具有吸引力。然而，专家监督仍然是必不可少的，因为没有任何模型在风湿病学中能够持续达到专家级准确度。|
|**2025-07-10**|**Defending Against Prompt Injection With a Few DefensiveTokens**|Sizhe Chen et.al.|[2507.07974](http://arxiv.org/abs/2507.07974)|null|当大型语言模型（LLM）系统与外部数据进行交互以执行复杂任务时，一种新的攻击手段，即提示注入，成为了一个重大的威胁。攻击者通过将指令注入系统访问的数据中，能够覆盖初始用户任务，并执行由攻击者指定的任意任务。为了确保系统安全，研究人员提出了测试时防御措施，例如防御性提示，供系统开发者灵活地在需要时实现安全性。然而，这些防御措施的效果远不如训练时防御措施，后者会改变模型参数。受此启发，我们提出了DefensiveToken，这是一种具有与训练时防御措施相当提示注入鲁棒性的测试时防御措施。DefensiveToken作为新插入的特殊标记，其嵌入被优化以提高安全性。在安全性敏感的情况下，系统开发者可以在LLM输入前添加几个DefensiveToken，以实现最小化效用损失的安全性。在不那么关注安全性的场景中，开发者可以简单地跳过DefensiveToken；由于没有防御措施，LLM系统保持不变，仍然能够生成高质量的响应。因此，如果将DefensiveToken与模型一同发布，允许在测试时灵活地在最先进（SOTA）效用和几乎最先进（几乎-SOTA）安全性之间进行切换。代码可在https://github.com/Sizhe-Chen/DefensiveToken获取。|
|**2025-07-10**|**Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations**|Federico Maria Cau et.al.|[2507.07916](http://arxiv.org/abs/2507.07916)|null|钓鱼攻击已成为现代网络安全中的主要风险，常常通过利用可预测的人类行为来绕过技术防御。警告对话框是一种标准缓解措施，但其缺乏解释清晰度和静态内容限制了其有效性。在本文中，我们报告了我们评估大型语言模型（LLMs）生成清晰、简洁和可扩展的钓鱼警告解释能力的研究。我们进行了一项大规模的受试者间用户研究（N = 750），比较了手动生成的解释与Claude 3.5 Sonnet和Llama 3.3 70B两个LLM生成的警告对话框的影响。我们研究了两种解释风格（基于特征和反事实）对行为指标（点击率）和感知结果（例如，信任、风险、清晰度）的影响。结果表明，精心构建的LLM生成的解释在减少对钓鱼的易感性方面可以与手动编写的解释相媲美，甚至更好；Claude生成的警告表现出特别稳健的性能。基于特征的解释对于真正的钓鱼攻击更为有效，而反事实解释降低了误报率。其他变量，如工作负载、性别以及之前对警告对话框的熟悉程度，显著调节了警告的有效性。这些结果表明，LLMs可以用于自动构建警告用户防止钓鱼的解释，并且这些解决方案是可扩展的、适应性强的，并且符合以人为本的价值。|
|**2025-07-10**|**MIRA: A Novel Framework for Fusing Modalities in Medical RAG**|Jinhong Wang et.al.|[2507.07902](http://arxiv.org/abs/2507.07902)|null|多模态大型语言模型（MLLMs）在人工智能辅助医疗诊断方面取得了显著进展，但它们常常生成与既定医学知识不符的事实不一致的回应。检索增强生成（RAG）通过整合外部来源提高了事实准确性，但存在两个主要挑战。首先，检索不足可能错过关键信息，而过度的检索可能会引入无关或误导性内容，破坏模型输出。其次，即使模型最初提供正确答案，过度依赖检索数据也可能导致事实错误。为了解决这些问题，我们引入了多模态智能检索和增强（MIRA）框架，旨在优化MLLM中的事实准确性。MIRA由两个关键组件组成：（1）一个校准的重新思考和重新排列模块，它动态调整检索到的上下文数量以管理事实风险；（2）一个集成了图像嵌入和医学知识库的医学RAG框架，以及一个查询重写模块，用于高效的多模态推理。这使得模型能够有效地整合其内在知识和外部参考。我们对公开可用的医学VQA和报告生成基准进行的评估表明，MIRA显著提高了事实准确性和整体性能，实现了新的最先进结果。代码已发布在https://github.com/mbzuai-oryx/MIRA。|
|**2025-07-10**|**An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis**|Mingda Zhang et.al.|[2507.07893](http://arxiv.org/abs/2507.07893)|null|人工智能的快速发展将大型语言模型定位为智能法律系统的基础组件。然而，这些模型在法律纠纷分析方面面临重大局限，包括法律知识表示不足、概念理解有限和推理缺陷。本研究提出了一种将提示工程与多维度知识图谱相结合的增强框架。该框架引入了一个包含任务定义、知识背景和推理指导的三阶段分层提示结构，并辅以法律特定推理模板和动态优化机制。构建了一个包含法律分类本体、表示和实例层的三层知识图谱架构。四种互补方法实现了精确的法律概念检索：直接法律规范代码匹配、领域特定语义向量相似度、基于本体的路径推理和专门词汇分割。这些组件与网络搜索技术相结合，建立了一个知识增强的法律决策框架。实验结果表明，在法律纠纷分析方面取得了显著的性能提升，能够对复杂案件进行准确的法律应用分析，同时展现了对司法决策逻辑的细微理解，为实施智能法律辅助系统提供了一种新的技术途径。|
|**2025-07-09**|**Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor**|Vatsal Agarwal et.al.|[2507.07106](http://arxiv.org/abs/2507.07106)|null|近年来，多模态大型语言模型（MLLMs）在图像问答方面的进步已经实现。然而，一个关键限制是使用CLIP作为视觉编码器；尽管它可以捕捉粗略的全局信息，但往往错过了与输入查询相关的细微细节。为了解决这些不足，本研究探讨是否预训练的文本到图像扩散模型可以作为指令感知的视觉编码器。通过对其内部表示的分析，我们发现扩散特征在语义上丰富，并且可以编码强大的图像文本对齐。此外，我们发现可以利用文本条件来使模型专注于与输入问题相关的区域。然后，我们研究如何将这些特征与大型语言模型对齐，并揭示了一种泄漏现象，即LLM可能无意中从原始扩散提示中恢复信息。我们分析了这种泄漏的原因，并提出了一种缓解策略。基于这些见解，我们探索了一种简单的融合策略，该策略利用了CLIP和条件扩散特征。我们在通用VQA和专门的MLLM基准上评估了我们的方法，证明了扩散模型在视觉理解方面的潜力，特别是在需要空间和组合推理的视觉中心任务中。我们的项目页面可在https://vatsalag99.github.io/mustafar/找到。|
|**2025-07-09**|**Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models**|Tiezheng Zhang et.al.|[2507.07104](http://arxiv.org/abs/2507.07104)|null|构建具有强大文本描述能力的最先进视觉-语言模型（VLMs）通常需要在大约数十亿的高质量图像-文本对上进行训练，这需要数百万个GPU小时。本文介绍了一种视觉-语言-视觉（VLV）自动编码器框架，该框架战略性地利用了关键的预训练组件：一个视觉编码器、一个文本到图像（T2I）扩散模型的解码器，以及随后的一个大语言模型（LLM）。具体来说，我们通过冻结预训练的T2I扩散解码器，在语言表示空间上建立了信息瓶颈。我们的VLV流程通过使用连续嵌入有效地提炼了文本条件扩散模型的知识，通过高质量的重建展示了全面的语义理解。此外，通过微调一个预训练的LLM将中间语言表示解码为详细描述，我们构建了一个最先进（SoTA）的文本描述器，其性能可与GPT-4o和Gemini 2.0 Flash等领先模型相媲美。我们的方法表现出卓越的成本效益，并显著减少了数据需求；通过主要利用单模态图像进行训练，并最大化利用现有的预训练模型（图像编码器、T2I扩散模型和LLM），它避开了大量配对图像-文本数据集的需求，将总训练成本控制在1000美元以下。|
|**2025-07-09**|**5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage**|Ugur Ari et.al.|[2507.07045](http://arxiv.org/abs/2507.07045)|null|从传统的提示工程到更严谨的提示设计学科标志着人机语言模型（LLM）交互的关键转变。随着LLM在关键任务应用中的日益嵌入，迫切需要既明确、系统，又足够简洁以保持实用和广泛可访问的框架。虽然许多现有方法通过复杂的领域特定语言（DSL）或多层模板来处理提示结构，但这些方法可能会带来显著的标记和认知负担，从而可能限制模型的创造性能力。在这种情况下，我们提出了5C提示合约框架，该框架将提示设计提炼为五个直观的组件：角色、原因、约束、偶然性和校准。这个最小认知模式明确地集成了回退和输出优化指令，促进了可靠、可解释和具有创造灵活性的AI交互。实验结果表明，5C框架在保持丰富和一致输出的同时，始终实现较高的输入标记效率，特别适合那些AI工程资源有限的个人和小型至中型企业（SMEs）。|
|**2025-07-09**|**UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations**|Fengran Mo et.al.|[2507.07030](http://arxiv.org/abs/2507.07030)|null|随着对话式搜索系统的快速发展，用户与系统之间的多轮交互使得信息获取方式发生了革命性的变化。现有的对话式搜索系统通常采用两种不同的模型构建。这种分离限制了系统同时利用模型内在知识的能力，无法确保检索对生成的有效性。现有的针对开发统一模型的研究无法完全解决理解对话上下文、独立管理检索以及生成响应等方面的问题。在本文中，我们探讨了如何将密集检索和响应生成统一应用于对话中的大型语言模型。我们采用不同目标进行联合微调，并设计了两种机制以降低不一致风险并缓解数据差异。在五个对话式搜索数据集上的评估表明，我们的统一模型可以相互提升两项任务，并优于现有的基线模型。|
|**2025-07-09**|**First Return, Entropy-Eliciting Explore**|Tianyu Zheng et.al.|[2507.07017](http://arxiv.org/abs/2507.07017)|null|从可验证奖励中进行强化学习（RLVR）可以提高大型语言模型（LLMs）的推理能力，但它在不稳定探索方面存在困难。我们提出了FR3E（首次回报，熵诱导探索）结构化探索框架，该框架能够识别推理轨迹中的高不确定性决策点，并进行有针对性的模拟运行以构建语义基础的中间反馈。我们的方法提供了有针对性的指导，而不依赖于密集的监督。在数学推理基准（AIME24）上的实验结果表明，FR3E促进了更稳定的训练，产生了更长且更连贯的响应，并增加了完全正确轨迹的比例。这些结果突出了该框架通过更稳健和结构化的探索来提高LLM推理的有效性。|
|**2025-07-09**|**GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning**|S M Taslim Uddin Raju et.al.|[2507.07006](http://arxiv.org/abs/2507.07006)|null|微观病理图像的评估对于准确诊断和治疗癌症至关重要。全切片图像（WSI）分类和标注已成为计算机辅助病理学中的关键任务。然而，由于病理学家主观捕捉，显微镜WSI面临着如冗余块和未知块位置等挑战。此外，生成自动病理学字幕仍然是一个重大挑战。为了解决这些问题，我们引入了一种新的GNN-ViTCap框架，用于从病理学显微镜图像中进行分类和字幕生成。首先，一个视觉特征提取器生成块嵌入。然后，通过深度嵌入聚类动态地聚类这些嵌入，并利用标量点注意力机制选择代表性块来移除冗余块。我们通过将每个节点与其相似性矩阵中的最近邻连接来构建一个图，并应用图神经网络来捕捉局部和全局上下文。通过一个线性层将聚合的图像嵌入投影到语言模型的输入空间，并结合字幕标记来微调大型语言模型。我们在BreakHis和PatchGastric数据集上验证了我们的方法。GNN-ViTCap在分类上实现了0.934的F1分数和0.963的AUC，在字幕生成上实现了0.811的BLEU-4分数和0.569的METEOR分数。实验结果表明，GNN-ViTCap优于现有方法，为基于显微镜的患者诊断提供了一个可靠且高效的解决方案。|
|**2025-07-09**|**Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs**|Yahan Yu et.al.|[2507.06999](http://arxiv.org/abs/2507.06999)|null|推理是大型语言模型（LLMs）的关键能力，尤其是在应用于如数学问题解决等复杂任务时。然而，多模态推理研究仍需进一步探索模态对齐和训练成本。许多这些方法依赖于额外的数据标注和相关的基于规则的奖励来增强理解和推理能力，这显著增加了训练成本并限制了可扩展性。为了应对这些挑战，我们提出了旨在提升多模态LLMs（MLLMs）理解和推理能力而不需要额外标注和复杂奖励的“深思熟虑到直觉推理框架”（D2I）。具体来说，我们的方法仅在训练过程中通过基于规则的格式奖励设置深思熟虑的推理策略来增强模态对齐。在评估时，推理风格转变为直觉型，即在训练过程中移除深思熟虑的推理策略，并隐式反映模型在响应中获取的能力。D2I在领域内和领域外基准测试中均优于基线。我们的发现突出了格式奖励在培养MLLMs可迁移推理技能中的作用，并启发了将训练时推理深度与测试时响应灵活性解耦的方向。|
|**2025-07-09**|**MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation**|Qilong Xing et.al.|[2507.06992](http://arxiv.org/abs/2507.06992)|null|尽管在将大型语言模型（LLMs）应用于放射报告生成（RRG）方面取得了显著进展，但由于将病理和解剖特征准确映射到其对应的文本描述中的困难，临床应用仍然面临挑战。此外，语义无关的特征提取进一步阻碍了准确诊断报告的生成。为了解决这些挑战，我们引入了医学概念对齐放射报告生成（MCA-RG）框架，这是一个知识驱动的框架，它明确地将视觉特征与不同的医学概念对齐，以增强报告生成过程。MCA-RG利用两个精心编制的概念库：一个包含病变相关知识的病理库和一个包含解剖描述的解剖库。视觉特征与这些医学概念对齐并经过定制增强。我们进一步提出了一种基于解剖的对比学习过程，以改善解剖特征的泛化能力，并结合病理特征的匹配损失来优先考虑临床相关区域。此外，采用了一种特征门控机制来过滤掉低质量的观念特征。最后，视觉特征对应于单个医学概念，并用于指导报告生成过程。在两个公开基准（MIMIC-CXR和CheXpert Plus）上的实验表明，MCA-RG实现了优越的性能，突出了其在放射报告生成中的有效性。|
|**2025-07-09**|**Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation**|Binquan Zhang et.al.|[2507.06980](http://arxiv.org/abs/2507.06980)|null|大型语言模型（LLMs）在代码生成方面展现出令人印象深刻的表现，尤其是在结合思维链（CoT）提示技术后。它们将需求分解为中间推理步骤，这些步骤作为设计理由来引导LLMs编写代码，就像人类程序员一样。因此，这些步骤的质量对于确保生成代码的正确性和可靠性至关重要。然而，关于LLMs生成的CoT质量知之甚少。我们能在多大程度上信任LLMs生成的想法？它们的想法有多好？本文通过分析两个广泛使用的代码生成基准测试中的1,023个失败的代码示例，实证地探索了LLMs生成不满意的CoT的外部和内部因素。我们还通过分析210个CoT-代码对以及通过提示LLMs来细化不满意的CoT，评估了它们对代码生成性能的影响。我们的研究揭示了三个关键发现：（1）外部因素（53.60%），如需求不明确和缺乏上下文，主要影响CoT质量，而内部因素（40.10%）源于LLMs对提示的理解不当。（2）即使CoT是正确的，仍有18.5%的生成代码因指令遵循问题而包含错误；相反，11.90%的正确代码与有缺陷的CoT配对。（3）细化低质量的CoT是可行的，即当给出详细的问题描述时，LLMs会得到改进。这些发现突出了基于CoT的代码生成中的关键挑战，并为提高LLMs的推理能力和可靠性指明了方向。|
|**2025-07-09**|**Investigating the Robustness of Retrieval-Augmented Generation at the Query Level**|Sezen Perçin et.al.|[2507.06956](http://arxiv.org/abs/2507.06956)|null|大型语言模型（LLMs）在更新新信息时成本高昂且效率低下。为了解决这一局限性，检索增强生成（RAG）被提出作为一种解决方案，在推理过程中动态地融入外部知识，提高事实一致性并减少幻觉。尽管RAG系统前景广阔，但它们在实际应用中面临许多挑战——最显著的是对输入查询质量的强烈依赖。在本文中，我们研究了RAG管道中不同组件对各种类型查询扰动的敏感性。我们的分析表明，即使是在轻微的查询变化下，常用检索器的性能也可能显著下降。我们分别研究了每个模块及其在端到端问答设置中的综合效果，使用了通用领域和特定领域的数据集。此外，我们提出了一种评估框架，以系统地评估RAG管道在查询层面的鲁棒性，并基于我们进行的超过1092次实验的结果，为实践者提供可操作的推荐。|
|**2025-07-08**|**Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers**|Zhiyuan Peng et.al.|[2507.06223](http://arxiv.org/abs/2507.06223)|null|大型语言模型（LLMs）最近被应用于信息检索中的重排序任务，并取得了良好的性能。然而，它们的高计算需求往往阻碍了实际部署。现有研究通过诸如延迟、前向传播次数、输入令牌和输出令牌等代理指标来评估基于LLM的重排序器的效率。然而，这些指标依赖于硬件和运行时选择（例如，是否并行、批量大小等），并且通常无法考虑模型大小，这使得解释和评估效率-有效性权衡变得困难。为了解决这个问题，我们提出了E²R-FLOPs，用于基于LLM的重排序器：每PetaFLOP的相关性排名指标（RPP）和每PetaFLOP的查询数（QPP）以实现硬件无关的吞吐量。伴随着新的指标，我们构建了一个可解释的FLOPs估计器，即使没有运行任何实验，也可以估计基于LLM的重排序器的FLOPs。基于所提出的指标，我们进行了全面的实验，以评估不同架构的广泛基于LLM的重排序器，研究效率-有效性权衡，并将这个问题引起研究社区的注意。|
|**2025-07-08**|**A Survey on Latent Reasoning**|Rui-Jie Zhu et.al.|[2507.06203](http://arxiv.org/abs/2507.06203)|null|大型语言模型（LLMs）展示了令人印象深刻的推理能力，尤其是在通过明确的思维链（CoT）推理来口头描述中间步骤时。虽然CoT提高了可解释性和准确性，但其对自然语言推理的依赖限制了模型的表达能力。潜在推理通过在模型的连续隐藏状态中执行多步推理，消除了标记级监督，来克服这一瓶颈。为了推进潜在推理研究，这篇综述提供了对新兴领域潜在推理的全面概述。我们首先考察神经网络层作为推理计算基础的作用，强调分层表示如何支持复杂转换。接下来，我们探索各种潜在推理方法，包括基于激活的循环、隐藏状态传播和微调策略，这些策略可以压缩或内化显式推理痕迹。最后，我们讨论了高级范式，如通过掩码扩散模型实现的无限深度潜在推理，这能够实现全局一致且可逆的推理过程。通过统一这些观点，我们旨在阐明潜在推理的概念景观，并为LLM认知前沿的研究方向绘制蓝图。一个收集最新论文和代码库的GitHub仓库可在以下链接找到：https://github.com/multimodal-art-projection/LatentCoT-Horizon/。|
|**2025-07-08**|**UQLM: A Python Package for Uncertainty Quantification in Large Language Models**|Dylan Bouchard et.al.|[2507.06196](http://arxiv.org/abs/2507.06196)|null|首先，我们需要理解摘要中的关键术语和概念：  1. **Hallucinations**：这里的“幻觉”指的是大型语言模型（LLMs）生成虚假或误导性内容的实例。 2. **Large Language Models (LLMs)**：大型语言模型，是一种能够理解和生成人类语言的高级人工智能模型。 3. **Uncertainty quantification (UQ)**：不确定性量化，是一种评估和量化模型预测不确定性的方法。 4. **Python package**：Python软件包，是一种用于Python编程语言的软件组件。 5. **Scorers**：评分器，用于评估或评分系统输出的工具。 6. **Response-level confidence scores**：响应级别的置信分数，表示模型对生成内容置信度的数值。 7. **Off-the-shelf solution**：现成解决方案，即无需定制或修改即可直接使用的解决方案。  接下来，我们将摘要逐句翻译为中文：  1. Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications.    - 幻觉，定义为大型语言模型（LLMs）生成虚假或误导性内容的实例，对下游应用的安全性和信任构成重大挑战。  2. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques.    - 我们引入了UQLM，这是一个用于LLM幻觉检测的Python软件包，采用了最先进的UQ技术。  3. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1.    - 此工具包提供了一套基于UQ的评分器，这些评分器计算响应级别的置信分数，范围从0到1。  4. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs.    - 此库提供了一个现成的基于UQ的幻觉检测解决方案，可以轻松集成以增强LLM输出的可靠性。  最终，摘要的中文翻译结果如下：  幻觉，定义为大型语言模型（LLMs）生成虚假或误导性内容的实例，对下游应用的安全性和信任构成重大挑战。我们引入了UQLM，这是一个用于LLM幻觉检测的Python软件包，采用了最先进的UQ技术。此工具包提供了一套基于UQ的评分器，这些评分器计算响应级别的置信分数，范围从0到1。此库提供了一个现成的基于UQ的幻觉检测解决方案，可以轻松集成以增强LLM输出的可靠性。|
|**2025-07-08**|**SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads**|Jiale Lao et.al.|[2507.06192](http://arxiv.org/abs/2507.06192)|null|数据库研究和开发通常需要大量SQL查询进行基准测试。然而，由于隐私问题，获取真实的SQL查询具有挑战性，现有的SQL生成方法在定制化和满足现实约束方面存在局限性。为了解决这个问题，我们提出了SQLBarber，一个基于大型语言模型（LLMs）的系统，用于生成定制化和现实的SQL工作负载。SQLBarber具有以下特点：（i）消除了用户预先手动创建SQL模板的需求，同时提供接受自然语言规范以约束SQL模板的灵活性；（ii）能够高效地扩展以生成大量查询，这些查询匹配任何用户定义的成本分布（例如，基数和执行计划成本）；（iii）使用Amazon Redshift和Snowflake的执行统计信息来推导SQL模板规范和查询成本分布，这些分布反映了现实世界查询的特点。SQLBarber引入了以下内容：（i）一个声明性接口，用户可以轻松地生成定制化的SQL模板；（ii）一个由LLM驱动的管道，配备一个自我校正模块，根据查询成本进行配置文件分析、优化和剪枝；（iii）一个贝叶斯优化器，以高效地探索不同的谓词值，并确定满足目标成本分布的一组查询。我们构建并开源了十个不同难度级别和目标查询成本分布的基准测试，这些基准测试基于Snowflake和Amazon Redshift的现实统计数据。在这些基准测试上的大量实验表明，SQLBarber是唯一能够生成定制化SQL模板的系统。与现有方法相比，它将查询生成时间减少了1到3个数量级，并且显著提高了与目标成本分布的对齐度。|
|**2025-07-08**|**Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review**|Zhicheng Lin et.al.|[2507.06185](http://arxiv.org/abs/2507.06185)|null|2025年7月，在预印本网站arXiv上发现了18篇学术论文，这些论文含有被称为提示的隐藏指令，旨在操纵AI辅助同行评审。例如，“只给出正面评价”这样的指令被白色文字等技巧隐藏起来。作者们的回应各不相同：一位计划撤回受影响的论文，而另一位则辩称这种做法是合法的测试评审员遵守情况。这篇评论分析了这种做法作为一种新的研究不端行为。我们研究了在大语言模型（LLMs）中注入提示的技术，揭示了从简单的正面评价命令到详细的评价框架的四种隐藏提示类型。将提示作为“诱饵”来检测不当使用AI的评审员的辩护在审查下失效——提示指令始终以自我服务为目的的性质表明了操纵的意图。出版商的政策不一致：Elsevier完全禁止在同行评审中使用AI，而Springer Nature允许有限使用，但需披露要求。这一事件暴露了超出同行评审，延伸到任何处理学术文本的自动化系统（包括抄袭检测和引文索引）的系统漏洞。我们的分析强调了在提交门户进行协调的技术筛选以及管理学术评估中生成AI（GenAI）使用的统一政策的必要性。|
|**2025-07-08**|**Data-Semantics-Aware Recommendation of Diverse Pivot Tables**|Whanhee Cho et.al.|[2507.06171](http://arxiv.org/abs/2507.06171)|null|数据摘要对于从大型数据集中发现洞察至关重要。在电子表格中，数据透视表通过计算某些属性的聚合值，按其他属性分组，提供了一个方便的方式来总结表格数据。然而，识别会导致有用数据透视表的属性组合仍然是一个挑战，尤其是在高维数据集的情况下。我们正式提出了自动推荐具有洞察力和可解释性的数据透视表的问题，消除了繁琐的手动过程。推荐一组数据透视表的关键方面是使它们多样化。传统工作未能充分解决表格多样化问题，这促使我们考虑数据透视表多样化的问题。我们提出了SAGE，这是一个数据语义感知系统，用于推荐具有k预算的多样化数据透视表，克服了先前工作中导致冗余的top-k推荐方法的不足。SAGE确保每个数据透视表都具有洞察力、可解释性，并能够适应用户的行为和偏好，同时保证数据透视表集合彼此不同，提供多样化的推荐。我们做出了两个关键技术贡献：（1）一个数据语义感知模型，用于衡量单个数据透视表的效用和一组数据透视表的多样性；（2）一个可扩展的贪婪算法，通过利用数据语义显著减少组合搜索空间，能够高效地选择一组效用高的多样化数据透视表。我们针对三个真实世界数据集的大量实验表明，SAGE优于其他方法，并且能够高效扩展以适应高维数据集。此外，我们还提出了几个案例研究，以突出SAGE相对于商业软件和大型语言模型（LLMs）的定性有效性。|
|**2025-07-08**|**Skywork-R1V3 Technical Report**|Wei Shen et.al.|[2507.06167](http://arxiv.org/abs/2507.06167)|null|我们介绍了Skywork-R1V3，这是一个先进的开源视觉语言模型（VLM），它开创了视觉推理的新方法。其关键创新在于有效地将推理技能从仅文本的大语言模型（LLMs）转移到视觉任务中。Skywork-R1V3强大的性能主要源于我们精心设计的后训练强化学习（RL）框架，该框架有效地激活和增强了模型的推理能力，无需额外的继续预训练。通过这个框架，我们进一步揭示了连接模块在实现多模态推理模型稳健跨模态对齐中的基本作用。此外，我们引入了一个独特的推理能力指标，即关键推理标记的熵，这在RL训练期间的检查点选择中已被证明非常有效。Skywork-R1V3在MMMU上实现了最先进的成果，从64.3%显著提升到76.0%，这一性能与入门级人类能力相匹配。令人瞩目地，我们基于RL的后训练方法甚至让38B参数的模型能与顶级闭源VLM相媲美。实现成功地将数学推理转移到其他相关推理任务。我们还包含了对课程学习和强化微调策略的分析，以及关于多模态推理的更广泛讨论。Skywork-R1V3在多模态推理方面实现了重大飞跃，展示了强化学习作为推动开源VLM能力进步的强大引擎。|
|**2025-07-08**|**Evaluation of Habitat Robotics using Large Language Models**|William Li et.al.|[2507.06157](http://arxiv.org/abs/2507.06157)|null|本文聚焦于评估大型语言模型在解决嵌入式机器人任务中的有效性，使用了Meta PARTNER基准。Meta PARTNR提供了简化环境以及随机室内厨房场景中的机器人交互。每个随机厨房场景都分配了一个任务，其中两个机器人代理协作完成任务。我们在Meta PARTNER环境中评估了多个前沿模型。我们的结果表明，在PARTNR的机器人嵌入式环境中，推理模型如OpenAI o3-mini优于非推理模型如OpenAI GPT-4o和Llama 3。o3-mini在集中式、去中心化、全可观察性和部分可观察性配置中都表现出色。这为嵌入式机器人开发提供了一个有希望的研究途径。|
|**2025-07-08**|**Large Language Models Predict Human Well-being -- But Not Equally Everywhere**|Pat Pataranutaporn et.al.|[2507.06141](http://arxiv.org/abs/2507.06141)|null|主观幸福感是经济、医疗和政策决策中的关键指标。随着人工智能提供了建模人类结果的扩展工具，评估大型语言模型（LLMs）能否准确预测不同全球人口中的幸福感变得至关重要。我们使用来自64个国家64,000个人的数据评估了四个领先的LLMs。虽然LLMs捕捉了广泛的关联因素，如收入和健康，但它们的预测精度在训练数据中代表性不足的国家下降，突显出源于全球数字和经济不平等的系统性偏见。一项预先注册的实验表明，LLMs依赖于表面层的语言相似性而不是概念理解，导致在陌生或资源有限的环境中存在系统性的误估计。从代表性不足的背景中注入研究结果显著提高了性能，但仍存在显著差距。这些结果突显了LLMs在预测全球幸福感方面的潜力和局限性，强调了在将这些领域应用于实施之前进行稳健验证的重要性。|
|**2025-07-08**|**Coding Triangle: How Does Large Language Model Understand Code?**|Taolin Zhang et.al.|[2507.06138](http://arxiv.org/abs/2507.06138)|null|大型语言模型（LLMs）在代码生成方面取得了显著的进步，但它们的真正编程能力仍未被充分探索。我们引入了代码三角框架，该框架从三个基本维度系统地评估LLMs：编辑分析、代码实现和测试用例生成。通过在编程竞赛基准上的大量实验，我们发现虽然LLMs可以在这些维度上形成一个自洽的系统，但它们的解决方案往往缺乏人类程序员的多样性和鲁棒性。我们识别出模型认知和人类专业知识之间存在显著的分布偏移，模型错误倾向于因训练数据偏差和有限推理迁移而聚集。我们的研究表明，结合人类生成的编辑、解决方案和多样化的测试用例，以及利用模型混合，可以显著提高LLMs的性能和鲁棒性。此外，我们揭示了LLMs认知中的一致性和不一致性，这可能有助于自我反思和自我改进，为开发更强大的编码模型提供了一条潜在的方向。|
|**2025-07-07**|**Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing**|Chun-Hsiao Yeh et.al.|[2507.05259](http://arxiv.org/abs/2507.05259)|null|近期基于扩散的图像编辑方法在文本引导任务上取得了显著进展，但往往难以理解复杂、间接的指令。此外，当前模型经常存在身份保护不足、不必要的编辑或过度依赖手动掩膜的问题。为了解决这些挑战，我们引入了X-Planner，这是一个基于多模态大型语言模型（MLLM）的规划系统，它有效地将用户意图与编辑模型的能力联系起来。X-Planner采用思维链推理，系统地分解复杂指令为简单、清晰的子指令。对于每个子指令，X-Planner自动生成精确的编辑类型和分割掩膜，消除手动干预，确保局部化、身份保护性的编辑。此外，我们还提出了一种新颖的自动化流程，用于生成大规模数据以训练X-Planner，该流程在现有的基准测试和我们所引入的复杂编辑基准测试中都取得了最先进的成果。|
|**2025-07-07**|**Spatio-Temporal LLM: Reasoning about Environments and Actions**|Haozhen Zheng et.al.|[2507.05258](http://arxiv.org/abs/2507.05258)|null|尽管多模态大型语言模型（MLLMs）近年来取得了显著进展，但MLLMs在回答需要整体时空理解的提示时仍然存在困难。具体来说，解决以下两类提示具有挑战性：1）一个装备了MLLMs的智能体可以操作的环境的整体；同时，2）最近发生并被编码在视频剪辑中的动作。然而，这种整体时空理解对于在现实世界中运行的智能体来说非常重要。为了解决这个问题，我们首先开发了一个框架来收集大规模数据集。利用收集到的“关于环境和动作的推理”（REA）数据集，我们表明现有方法确实难以正确回答这些提示。为了改进，我们开发了一种“时空LLM”（ST-LLM），这是一种配备投影仪以增强环境空间理解和最近观察的时间理解的模型。在收集的REA数据上，我们展示了与先前工作相比，所提出的方法显著提高了结果。代码和数据可在https://zoezheng126.github.io/STLLM-website/找到。|
|**2025-07-07**|**Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions**|Yuanzhe Hu et.al.|[2507.05257](http://arxiv.org/abs/2507.05257)|null|最近对于大型语言模型（LLM）智能体的基准测试主要集中于评估推理、规划和执行能力，而另一个关键组件——记忆，包括智能体如何记忆、更新和检索长期信息，由于缺乏基准测试而评估不足。我们将具有记忆机制的智能体称为记忆智能体。在本文中，我们确定了记忆智能体所必需的四个核心能力：准确检索、测试时学习、长距离理解和冲突解决。现有的数据集要么依赖于有限的上下文长度，要么是为静态、长上下文设置如基于书籍的问答量身定制，这些数据集没有反映记忆智能体互动、多轮积累信息的特性。此外，现有的基准测试没有涵盖所有四个能力。因此，我们引入了MemoryAgentBench，这是一个专为记忆智能体设计的全新基准。我们的基准将重新构建的现有数据集与新的数据集相结合，涵盖上述四个记忆能力，提供了一个系统且具有挑战性的测试平台，用于评估记忆质量。我们评估了一系列的记忆智能体，从简单的基于上下文和检索增强生成（RAG）系统到具有外部记忆模块和工具集成的先进智能体。实证结果表明，当前方法在掌握所有四个能力方面存在不足，强调了对于LLM智能体全面记忆机制进行进一步研究的必要性。|
|**2025-07-07**|**Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning**|Yana Wei et.al.|[2507.05255](http://arxiv.org/abs/2507.05255)|null|大型语言模型（LLMs）惊人的推理能力源于通过可验证奖励强化产生的认知行为。这项工作探讨了如何将这一原理应用于多模态LLMs（MLLMs）以解锁高级视觉推理。我们介绍了一种基于Qwen2.5-VL-7B的两阶段范式：首先是大规模的语言冷启动微调，随后是跨越近1000步的多模态强化学习（RL），在规模上超越了所有先前的开源尝试。这项开创性工作揭示了三个基本见解：1）由于语言心理图象，行为迁移在冷启动阶段出现得异常早。2）冷启动广泛地记忆视觉行为，而RL则关键性地辨别并放大有效模式。3）迁移策略上有利于高效用行为，如视觉反思。我们构建的模型，Open-Vision-Reasoner（OVR），在一系列推理基准测试中实现了最先进的性能，包括在MATH500上达到95.3%，在MathVision上达到51.8%，在MathVerse上达到54.6%。我们将我们的模型、数据和训练动态发布出来，以催化更强大、行为对齐的多模态推理器的发展。|
|**2025-07-07**|**Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models**|Ziqi Miao et.al.|[2507.05248](http://arxiv.org/abs/2507.05248)|null|情境提示，即早期刺激在不知不觉中影响后续判断，为大型语言模型（LLMs）提供了一个未开发的攻击面。我们发现了一种情境提示漏洞，其中对话中的先前响应可以引导其后续行为走向违反策略的内容。基于这一洞察，我们提出了响应攻击（Response Attack），它使用一个辅助LLM生成对原始恶意查询的改写版本的一个轻微有害响应。然后，这些响应被格式化到对话中，并随后是一个简洁的触发提示，从而提示目标模型生成有害内容。在八个开源和专有LLMs上，RA（响应攻击）一致优于七种最先进的越狱技术，实现了更高的攻击成功率。为了减轻这一威胁，我们构建并发布了一个情境感知的安全微调数据集，该数据集显著降低了攻击成功率，同时保留了模型的能力。代码和数据可在https://github.com/Dtc7w3PQ/Response-Attack获取。|
|**2025-07-07**|**StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling**|Meng Wei et.al.|[2507.05240](http://arxiv.org/abs/2507.05240)|null|在现实场景中，视觉和语言导航（VLN）要求代理处理连续的视觉流，并基于语言指令以低延迟生成动作。尽管基于视频的大型语言模型（Video-LLMs）推动了最近的研究进展，但基于Video-LLM的当前VLN方法通常在细粒度视觉理解、长期上下文建模和计算效率之间面临权衡。我们引入了StreamVLN，这是一种流式VLN框架，它采用混合的慢-快上下文建模策略，以支持对交错视觉、语言和动作输入的多模态推理。快速流对话上下文通过滑动窗口的活跃对话促进响应性动作生成，而慢速更新的记忆上下文使用3D感知的令牌剪枝策略压缩历史视觉状态。通过这种慢-快设计，StreamVLN通过高效的KV缓存重用实现连贯的多轮对话，支持具有有限上下文大小和推理成本的长时间视频流。在VLN-CE基准测试上的实验表明，StreamVLN在稳定低延迟的情况下实现了最先进的性能，确保了在实际部署中的鲁棒性和效率。项目页面为：[https://streamvln.github.io/](https://streamvln.github.io/)。|
|**2025-07-07**|**All in One: Visual-Description-Guided Unified Point Cloud Segmentation**|Zongyan Han et.al.|[2507.05211](http://arxiv.org/abs/2507.05211)|null|3D点云的统一分割对于场景理解至关重要，但其稀疏结构、有限的标注以及区分复杂环境中细粒度对象类别的挑战限制了其发展。现有方法由于有限的监督和缺乏多样的多模态线索，往往难以捕捉丰富的语义和上下文信息，导致类和实例的区分度不足。为了解决这些挑战，我们提出了VDG-Uni3DSeg，这是一种新的框架，它将预训练的视觉-语言模型（例如CLIP）和大型语言模型（LLM）集成，以增强3D分割。通过利用LLM生成的文本描述和来自互联网的参考图像，我们的方法融入了丰富的多模态线索，促进了细粒度类和实例的分离。我们进一步设计了一种语义-视觉对比损失，用于对齐点云特征和多模态查询，以及一个空间增强模块，以高效地建模场景范围内的关系。VDG-Uni3DSeg在语义、实例和全景分割方面实现了最先进的结果，为3D理解提供了一个可扩展且实用的解决方案。我们的代码可在https://github.com/Hanzy1996/VDG-Uni3DSeg上找到。|
|**2025-07-07**|**CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale**|Jonathan Hyun et.al.|[2507.05178](http://arxiv.org/abs/2507.05178)|null|尽管基于大型语言模型（LLM）的多智能体系统取得了快速发展，但当前的基准测试在评估其在复杂、动态、现实世界任务中的可扩展性、鲁棒性和协调能力方面仍有不足。现有环境通常关注小型、全可观测或低复杂度领域，限制了其在开发和评估下一代多智能体Agentic AI框架中的实用性。我们引入了CREW-Wildfire，这是一个开源基准，旨在弥合这一差距。CREW-Wildfire建立在人类-人工智能团队协作的CREW模拟平台之上，提供了具有大地图、异构智能体、部分可观测性、随机动态和长期规划目标的程序生成野火响应场景。该环境通过模块化的感知和执行模块支持低级控制和高级自然语言交互。我们实现并评估了几个最先进的基于LLM的多智能体Agentic AI框架，揭示了在大型规模协调、通信、空间推理和不确定性下的长期规划中的未解决问题，突出了显著的性能差距。通过提供更现实的复杂性、可扩展的架构和行为评估指标，CREW-Wildfire为推进可扩展多智能体Agentic智能研究奠定了关键的基础。所有代码、环境、数据和基线将发布以支持该新兴领域的未来研究。|
|**2025-07-07**|**OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model**|Chen Wang et.al.|[2507.05177](http://arxiv.org/abs/2507.05177)|null|同理心交互是人类与机器沟通的基石，因为它需要理解包含副语言线索的语音并生成情感和表达性回应。然而，最强大的同理心低延迟序列到序列语言模型（LSLM）正变得越来越封闭，关于其架构、数据和开发的关键细节对研究者来说变得模糊不清。鉴于对LSLM和同理心行为透明研究的迫切需求，我们提出了OpenS2S，这是一个完全开源、透明且端到端的LSLM，旨在实现同理心语音交互。基于我们提出的同理心语音转文本模型BLSP-Emo，OpenS2S进一步采用流式交错解码架构以实现低延迟的语音生成。为了促进端到端的训练，OpenS2S集成了自动数据构建管道，以低成本合成多样化的高质量同理心语音对话。通过利用大型语言模型生成同理心内容和可控的文本到语音系统引入说话者和情感变化，我们构建了一个具有丰富副语言多样性和最小人工监督的扩展训练语料库。我们发布了完全开源的OpenS2S模型，包括数据集、模型权重、预训练和微调代码，以赋能更广泛的研究社区并加速同理心语音系统创新。项目网页可通过https://casia-lm.github.io/OpenS2S访问。|
|**2025-07-07**|**AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models**|Chinnappa Guggilla et.al.|[2507.05157](http://arxiv.org/abs/2507.05157)|null|大型语言模型（LLMs）具有非凡的能力，能够生成既连贯又与语境相关、且与人类写作惊人相似的文本。它们能够适应各种风格和体裁，生成既语法正确又语义有意义的文本。最近，LLMs被滥用来创建高度逼真的钓鱼邮件、传播虚假新闻、生成用于自动化网络犯罪的代码，以及撰写欺诈性科学文章。此外，在许多实际应用中，包括风格、主题和生成模型在内的生成内容事先是未知的。人工智能（AI）生成文本的日益普遍和复杂化使得其检测变得更加困难。已经尝试了各种方法来区分机器生成文本和人类创作内容，包括语言、统计、机器学习和集成方法。本研究聚焦于两个主要目标：任务A，涉及区分人类撰写的文本和机器生成的文本；任务B，试图识别负责生成的特定LLM模型。这两个任务都基于对生成预训练变压器（GPT_4o-mini）、大型语言模型Meta AI（LLaMA）38B和双向编码器表示从变压器（BERT）的微调。微调后的GPT_4o-mini和BERT模型在任务A中实现了0.9547的准确率，在任务B中实现了0.4698的准确率。|
|**2025-07-03**|**Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation**|Jiaer Xia et.al.|[2507.02859](http://arxiv.org/abs/2507.02859)|null|多模态大型语言模型（MLLMs）在利用自然语言解释图像方面展现出惊人的能力。然而，如果没有使用大规模数据集进行重新训练，这些模型很难适应专门的视觉任务，例如图表理解。这个问题是由预训练数据集和下游数据集之间的不匹配造成的：预训练数据集主要关注场景和物体，但关于专门的非物体图像（如图表和表格）的信息有限。在本文中，我们分享了一个有趣的发现：使用思维链（CoT）推理数据训练MLLM可以促进模型在专门视觉任务中的适应性，尤其是在数据有限的情况下。然而，我们识别出从预训练MLLM中提取的CoT数据中存在一个关键问题，即数据在推理步骤中往往包含多个事实错误。为了解决这个问题，我们提出了基于脚本的Grounded Chain-of-Thought（GCoT）方法，旨在将基础信息（即边界框）注入CoT数据，使推理步骤更忠实于输入图像。我们在五个专门的视觉任务上评估了我们的方法，这些任务涵盖了包括图表、表格、收据和报告在内的各种视觉格式。结果表明，在数据有限的情况下，我们的方法在微调和蒸馏方面取得了显著改进。|
|**2025-07-03**|**Requirements Elicitation Follow-Up Question Generation**|Yuchen Shen et.al.|[2507.02858](http://arxiv.org/abs/2507.02858)|null|访谈是广泛用于收集利益相关者需求、偏好和期望，以获取软件系统需求的技术。有效的访谈需要熟练的访谈者实时制定适当的访谈问题，同时面对多种挑战，包括对领域的陌生感、过度的认知负荷以及阻碍人类处理利益相关者话语的信息过载。最近，大型语言模型（LLMs）在包括文本摘要和蕴含在内的多个自然语言处理任务中展现出最先进的性能。为了支持访谈者，我们研究了在基于常见访谈者错误类型框架的基础上，将GPT-4o应用于需求获取过程中生成后续访谈问题的应用。此外，我们描述了基于受访者话语生成问题的方法。我们报告了一项控制实验，以评估在最少指导下的LLM生成的和人工编写的问题，以及第二项控制实验，以评估在生成受访谈者错误类型指导的LLM生成问题时的表现。我们的发现表明，对于这两个实验，LLM生成的提问在清晰度、相关性和信息量方面都不亚于人工编写的问题。此外，当受到常见错误类型指导时，LLM生成的提问优于人工编写的问题。这突显了使用LLMs帮助访谈者在实时访谈中提高需求获取质量和大程度的潜力。|
|**2025-07-03**|**MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs**|Purbesh Mitra et.al.|[2507.02851](http://arxiv.org/abs/2507.02851)|null|近期，大型语言模型（LLMs）推理能力的提升表明，在强化学习（RL）训练中采用组相对策略优化（GRPO）算法可以使模型使用更多的思考/推理令牌来生成更好的回答。然而，LLMs在保持对先前生成令牌的关注的同时，只能生成有限数量的令牌。这个限制，也称为LLMs的上下文大小，是LLMs在任意大量令牌推理中的瓶颈。为了超越上下文大小的限制，LLM必须采用模块化思考策略，通过多轮推理进行思考。在这项工作中，我们提出了 $\textbf{MOTIF：通过强化微调进行模块化思考}$ ——一种用于多轮生成思考令牌的RL训练方法，有效地使模型能够以额外的上下文大小进行思考。我们通过参数高效微调在GSM8K数据集上训练了开源模型Qwen2.5-3B-Instruct，并在MATH500和AIME2024基准测试中测试了其准确性。我们的实验表明，在相应的基准测试中，与传统的GRPO训练相比，分别提高了3.8%和3.3%。此外，这种改进仅使用了15%的样本，从而证明了MOTIF的样本效率。我们的代码和模型分别可在https://github.com/purbeshmitra/MOTIF和https://huggingface.co/purbeshmitra/MOTIF找到。|
|**2025-07-03**|**Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection**|Ziqi Miao et.al.|[2507.02844](http://arxiv.org/abs/2507.02844)|null|随着强大视觉语言能力的出现，多模态大型语言模型（MLLMs）在现实世界应用中展现出巨大潜力。然而，视觉模态表现出的安全漏洞给在开放世界环境中部署此类模型带来了重大挑战。近期研究表明，通过将有害文本语义直接编码到视觉输入中，成功诱导目标MLLMs产生有害响应。然而，在这些方法中，视觉模态主要作为触发不安全行为的触发器，往往表现出语义模糊，缺乏在现实场景中的根基。在本工作中，我们定义了一个新的场景：视觉中心化的越狱，其中视觉信息作为构建完整和现实越狱场景的必要组成部分。基于此场景，我们提出了VisCo（视觉上下文）攻击。VisCo通过四种不同的视觉聚焦策略构建上下文对话，在必要时动态生成辅助图像以构建视觉中心化的越狱场景。为了最大化攻击效果，它结合了自动毒性模糊和语义细化，生成一个最终攻击提示，能够可靠地触发目标黑盒MLLMs的有害响应。具体来说，VisCo在MM-SafetyBench上对GPT-4o实现了4.78的毒性评分和85%的攻击成功率（ASR），显著优于基线，其毒性评分为2.48，攻击成功率为22.2%。代码可在https://github.com/Dtc7w3PQ/Visco-Attack找到。|
|**2025-07-03**|**LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding**|Yuchen Ma et.al.|[2507.02843](http://arxiv.org/abs/2507.02843)|null|在医学领域，估计治疗效果对于个性化决策至关重要，但这一任务在临床实践中面临独特的挑战。在训练时间，估计治疗效果的模型通常在结构良好的医学数据集上训练，这些数据集包含详细的病人信息。然而，在推理时间，预测往往使用文本描述（例如，带有自我报告症状的描述）进行，这些描述是原始病人信息的部分表示。在这项工作中，我们做出了三个贡献。（1）我们表明，训练时间和推理时间可用数据之间的差异可能导致治疗效果估计的偏差。我们将这个问题形式化为推理时间文本混杂问题，其中混杂因子在训练时间完全观察到，但在推理时间只部分通过文本可用。（2）为了解决这个问题，我们提出了一种新的框架来估计治疗效果，该框架明确考虑了推理时间文本混杂。我们的框架利用大型语言模型与定制的双重稳健学习者相结合，以减轻推理时间文本混杂引起的偏差。（3）通过一系列实验，我们展示了我们的框架在现实世界应用中的有效性。|
|**2025-07-03**|**StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason**|Kaiyi Zhang et.al.|[2507.02841](http://arxiv.org/abs/2507.02841)|null|强化学习与可验证奖励（RLVR）是一种有望提高大型语言模型（LLMs）复杂推理能力的途径。然而，当前的RLVR方法面临两个显著挑战：近错奖励问题，即一个小错误可能使原本正确的推理过程失效，从而极大地阻碍了训练效率；以及探索停滞问题，模型倾向于关注其“舒适区”内的解决方案，缺乏探索潜在更有效替代方案的动力。为了解决这些挑战，我们提出了StepHint，这是一种新颖的RLVR算法，利用多层次逐步提示来帮助模型更有效地探索解空间。StepHint从更强的模型生成有效的推理链，并使用我们提出的自适应分区方法将这些链分割成推理步骤。最初的几个步骤用作提示，同时，提供多级提示（每个提示包含不同数量的步骤）给模型。这种方法将模型的探索引导向有希望的解决方案子空间，同时保持其独立探索的灵活性。通过提供提示，StepHint减轻了近错奖励问题，从而提高了训练效率。此外，外部推理路径帮助模型发展更好的推理能力，使其能够超出“舒适区”，减轻探索停滞。StepHint在六个数学基准测试中优于竞争性的RLVR增强方法，同时在域外基准测试中也表现出优越的泛化能力，并优于基线。|
|**2025-07-03**|**ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning**|Ruiyang Zhou et.al.|[2507.02834](http://arxiv.org/abs/2507.02834)|null|近年来，大型语言模型的进步主要得益于强化学习（RL）风格的后续训练，这种训练通过基于奖励或偏好信号优化模型输出，从而提升推理能力。GRPO风格的方案通过使用由基于结果验证器标记的自生成样本来实现这一点。然而，这些方法高度依赖于模型产生积极样本的初始能力。它们主要细化模型已经知道的内容（分布锐化），而不是使模型能够解决其最初失败的难题。这种限制在早期强化学习训练和具有挑战性的推理任务中尤为突出，在这些任务中，产生积极样本的可能性很小。为了在这样环境中解锁推理能力，模型必须探索超出其当前输出分布的新推理轨迹。这种探索需要访问足够好的积极样本来指导学习。虽然专家演示似乎是一个自然的解决方案，但我们发现它们在RL后续训练中往往效果不佳。相反，我们确定了有效积极样本的两个关键特性：（1）它们应该在当前策略下是可能的，（2）它们应该增加模型预测正确答案的可能性。基于这些见解，我们提出了 $\textbf{自我解释策略优化（ExPO）}$ ——一个简单且模块化的框架，通过基于真实答案的条件生成这样的样本。ExPO能够实现高效的探索，并引导模型产生与策略更加一致的推理轨迹，同时确保其质量高于自己的（不正确）样本。实验表明，ExPO在推理基准测试中提高了学习效率和最终性能，在模型最初最挣扎的具有挑战性的设置（如MATH水平-5）中，超过了基于专家演示的方法。|
|**2025-07-03**|**SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model**|Wencheng Zhang et.al.|[2507.02822](http://arxiv.org/abs/2507.02822)|null|随着大型语言模型（LLMs）在实用应用中的广泛采用，选择一个合适的模型需要平衡性能和运营成本。推理能力模型的兴起进一步扩大了“思考”（高推理）和“非思考”（快速、低成本）模式之间的成本差距。在这项工作中，我们发现大约58%的医疗问题可以仅通过非思考模式准确回答，无需高成本推理过程。这突显了问题复杂性的明显二分，并表明根据复杂度动态路由查询到适当的模式可以优化准确性、成本效益和整体用户体验。基于此，我们进一步提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够智能地将输入查询分配到思考或非思考模式。在几个医疗数据集上的实验结果表明，与仅使用思考模式相比，SynapseRoute不仅提高了整体准确性（0.8390比0.8272），还降低了推理时间36.8%和令牌消耗39.66%。重要的是，定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至准确性下降，这是我们自适应路由避免的陷阱。最后，这项工作进一步引入了准确性-推理-令牌（AIT）指数，以全面评估准确性、延迟和令牌成本之间的权衡。|
|**2025-07-03**|**Multimodal Mathematical Reasoning with Diverse Solving Perspective**|Wenhao Shi et.al.|[2507.02804](http://arxiv.org/abs/2507.02804)|null|最近在大型强化学习（RL）方面的进展显著提升了大型语言模型（LLM）的推理能力，尤其是在数学领域。然而，当前用于数学推理的多模态LLM（MLLM）通常依赖于一对一的图像-文本对和单解监督，忽略了有效推理视角的多样性和内部反思。在本研究中，我们引入了MathV-DP，这是一个新的数据集，它捕捉了每个图像-问题对的多个多样化的解决方案轨迹，从而促进了更丰富的推理监督。我们进一步提出了基于Qwen-VL并经过监督学习微调和通过基于规则的强化学习方法（GRPO）增强的模型Qwen-VL-DP，GRPO将正确性区分和多样性感知奖励函数整合在一起。我们的方法强调从不同的推理视角中学习，并区分正确但不同的解决方案。在MathVista的minitest和Math-V基准测试上的大量实验表明，Qwen-VL-DP在准确性和生成多样性方面都显著优于先前的基础MLLM，突出了在多模态数学推理中纳入多样视角和反思推理的重要性。|
|**2025-07-03**|**Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models**|Riccardo Cantini et.al.|[2507.02799](http://arxiv.org/abs/2507.02799)|null|推理语言模型（RLMs）因其能够通过思维链（CoT）提示或微调推理轨迹等机制执行复杂的多步骤推理任务而受到关注。虽然这些能力承诺提高了可靠性，但它们对社交偏见鲁棒性的影响仍然不明确。在这项工作中，我们利用最初为大型语言模型（LLMs）设计的CLEAR-Bias基准，来调查RLMs对偏见诱发的对抗鲁棒性。我们系统地评估了最先进的RLMs在多个社会文化维度上的表现，采用了一种将LLM作为裁判的方法进行自动化安全评分，并利用越狱技术来评估内置安全机制的有效性。我们的评估解决了三个关键问题：（一）推理能力的引入如何影响模型的公平性和鲁棒性；（二）是否对推理进行微调的模型在推理时比依赖CoT提示的模型具有更高的安全性；（三）针对偏见诱发的越狱攻击的成功率如何随着所采用的推理机制而变化。我们的发现揭示了推理能力与偏见安全性之间复杂的关系。出人意料的是，无论是通过CoT提示还是通过微调推理轨迹进行显式推理的模型，通常比没有这种机制的基线模型更容易受到偏见诱发的攻击，这表明推理可能无意中为刻板印象的强化开辟了新的途径。具有推理能力的模型似乎比依赖CoT提示的模型更安全，后者特别容易受到通过故事提示、虚构角色或奖励形状指令进行情境重构攻击的影响。这些结果挑战了推理本质上提高鲁棒性的假设，并强调了在推理设计中采用更多偏见意识方法的需要。|
|**2025-07-02**|**Kwai Keye-VL Technical Report**|Kwai Keye Team et.al.|[2507.01949](http://arxiv.org/abs/2507.01949)|null|尽管多模态大型语言模型（MLLMs）在静态图像上展现出惊人的能力，但它们在理解动态、信息密集的短视频方面往往不足，而短视频是当今数字景观中的主流媒介。为了弥合这一差距，我们引入了名为“Kwai Keye-VL”的80亿参数多模态基础模型，该模型旨在在短视频理解方面实现领先性能，同时保持强大的通用视觉-语言能力。Keye-VL的开发建立在两个核心支柱之上：一个包含超过6000亿个标记的庞大、高质量数据集，该数据集特别强调视频，以及一个创新的训练方法。这种方法包括一个四阶段的预训练过程，用于实现牢固的视觉-语言对齐，随后是一个细致的、两阶段的后训练过程。第一个后训练阶段增强了基础能力，如指令遵循，而第二个阶段则专注于激发高级推理。在这个第二阶段，一个关键创新是我们的五模式“冷启动”数据混合，包括“思考”、“非思考”、“自动思考”、“图像思考”和高质量视频数据。这种混合教会模型何时以及如何进行推理。随后的强化学习（RL）和对齐步骤进一步增强了这些推理能力，并纠正了异常模型行为，如重复输出。为了验证我们的方法，我们进行了广泛的评估，结果表明Keye-VL在公共视频基准测试中实现了最先进的成果，并在基于图像的一般任务上保持着高度竞争力（见图1）。此外，我们还开发并发布了名为“KC-MMBench”的新基准，该基准针对现实世界的短视频场景量身定制，其中Keye-VL显示出显著的优势。|
|**2025-07-02**|**SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars**|Xiaosheng Zhao et.al.|[2507.01939](http://arxiv.org/abs/2507.01939)|null|近年来，大型语言模型（LLMs）通过海量数据和大规模参数化改变了自然语言理解。受此成功启发，我们提出了SpecCLIP，这是一个基础模型框架，将受LLM启发的技术扩展到恒星光谱分析领域。恒星光谱，类似于结构化语言，编码了关于星星丰富的物理和化学信息。通过在大规模光谱数据集上训练基础模型，我们的目标是学习出稳健且信息丰富的嵌入，以支持多样化的下游应用。作为一个概念验证，SpecCLIP包括在两种光谱类型——LAMOST低分辨率和Gaia XP——上的预训练，随后使用CLIP（对比语言-图像预训练）框架进行对比对齐，该框架经过调整以关联来自不同仪器的光谱。这种对齐由辅助解码器补充，这些解码器保留了光谱特有的信息，并能够在光谱类型之间进行翻译（预测），前者通过最大化嵌入与输入光谱之间的互信息来实现。结果是，一个跨光谱框架，它能够实现内在校准并在不同仪器上具有灵活的应用。我们证明了在中等规模的有标签数据集上微调这些模型可以改善对恒星参数估计和化学丰度测定等任务的适应性。SpecCLIP还提高了与外部调查数据基准比较的参数估计的准确性和精度。此外，其相似性搜索和跨光谱预测能力为异常检测提供了潜在的可能性。我们的结果表明，通过对比训练并加入光谱感知解码器丰富的基础模型可以推进精确恒星光谱学。|
|**2025-07-02**|**The Thin Line Between Comprehension and Persuasion in LLMs**|Adrian de Wynter et.al.|[2507.01936](http://arxiv.org/abs/2507.01936)|null|大型语言模型（LLMs）在维持高级别、令人信服的对话方面表现出色。它们正迅速被部署为聊天机器人和敏感领域的评估者，如同行评审和心理健康应用。这一点，加上对其推理能力的不同看法，要求我们更深入地研究LLMs及其对对话的理解。在这项工作中，我们首先评估了LLMs维持辩论的能力——这是一种纯粹但最复杂的人类交流形式。然后，我们测量了这种能力与它们对所讨论内容的理解之间的关系，即它们对对话结构和语用语境的理解。我们发现，LLMs能够维持连贯、有说服力的辩论，经常影响参与者和观众的观点。我们还注意到，对AI参与的意识或怀疑会促使人们更加批判性地看待所提出的论点。然而，当对LLMs进行关于它们对对话深层结构的理解进行民意调查时，它们却无法展示出这种理解。我们的发现将LLMs作为评估者的不足与它们（不）理解语境的能力联系起来。更广泛地说，对于辩论理论领域，我们提出，如果一个代理能够令人信服地维持对话，它不必知道它在谈论什么。因此，语用语境和连贯性的建模对于有效性来说是次要的。|
|**2025-07-02**|**Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations**|Wenhao Wang et.al.|[2507.01930](http://arxiv.org/abs/2507.01930)|null|近年来，大型语言模型（LLMs）在移动机器人领域，包括无人机（UAVs），取得了革命性的进展，使它们能够在物联网（IoT）生态系统中实现智能操作。然而，LLMs在逻辑推理和复杂决策方面仍面临挑战，这引发了人们对LLM驱动的UAV在物联网应用中可靠性的担忧。在本文中，我们提出了一种由LLM驱动的闭环控制框架，该框架通过使用两个LLM模块——即代码生成器和评估器——来实现可靠的UAV操作，并借助有效的反馈和优化。我们的框架将无人机操作中的数值状态观察转化为自然语言轨迹描述，以增强评估器LLM对无人机动态的理解，从而生成精确的反馈。此外，我们的框架还允许基于模拟的优化过程，因此消除了优化过程中由于代码执行错误而对物理无人机造成的风险。我们对不同复杂性的无人机控制任务进行了广泛的实验。实验结果表明，我们的框架可以使用LLMs实现可靠的UAV操作，并且在任务复杂性增加的情况下，在成功率和完善度方面显著优于基线方法。|
|**2025-07-02**|**Decision-Oriented Text Evaluation**|Yu-Shiang Huang et.al.|[2507.01923](http://arxiv.org/abs/2507.01923)|null|自然语言生成（NLG）在高风险领域中的应用日益增多，然而，常见的内在评估方法，如n-gram重叠或句子合理性，与实际决策效果的相关性较弱。我们提出了一种以决策为导向的框架，通过直接测量生成文本对人类和大型语言模型（LLM）决策结果的影响来评估生成文本。使用市场摘要文本——包括客观的晨间摘要和主观的收盘分析——作为测试案例，我们根据人类投资者和仅由这些文本信息告知的自主LLM代理执行的交易财务表现来评估决策质量。我们的发现表明，当仅依赖摘要时，人类或LLM代理并不能始终超越随机表现。然而，更丰富的分析评论使得人-LLM协作团队能够显著优于单个人类或代理基准。我们的方法强调了通过生成文本促进人类和LLM之间协同决策的能力的重要性，突出了传统内在指标的关键局限性。|
|**2025-07-02**|**Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models**|Chengao Li et.al.|[2507.01915](http://arxiv.org/abs/2507.01915)|null|从人类反馈中进行强化学习（RLHF）已成为一种强大的技术，用于将大型语言模型（LLMs）与人类偏好对齐。然而，有效地将LLMs与多样化的人类偏好对齐仍然是一个重大挑战，特别是在它们存在冲突时。为了解决这个问题，我们将人类价值对齐视为一个多目标优化问题，旨在最大化一组可能冲突的目标。我们引入了梯度自适应策略优化（GAPO），这是一种新颖的微调范式，它采用多梯度下降来将LLMs与多样化的偏好分布对齐。GAPO自适应地调整每个目标的梯度，以确定一个最优地平衡目标之间权衡的更新方向。此外，我们还引入了P-GAPO，它结合了不同目标下的用户偏好，并实现了更符合用户特定需求的帕累托解。我们的理论分析表明，GAPO收敛于多目标帕累托最优解。在Mistral-7B上的实验结果表明，GAPO优于当前最先进的方法，在有益性和无害性方面均取得了优异的性能。|
|**2025-07-02**|**Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning**|Qingdong He et.al.|[2507.01908](http://arxiv.org/abs/2507.01908)|null|基于指令的图像编辑（IIE）随着扩散模型的成功而迅速发展。然而，现有的研究主要关注于执行简单且明确的编辑操作，如添加、删除、移动或交换对象。它们在处理更复杂的隐含假设性指令时遇到困难，这些指令需要更深入的推理来推断合理的视觉变化和用户意图。此外，当前的数据集在训练和评估具有推理能力的编辑功能方面提供了有限的支持。从架构上讲，这些方法也缺乏支持此类推理的细粒度细节提取机制。为了解决这些限制，我们提出了Reason50K，这是一个专门为训练和评估假设性指令推理图像编辑而精心制作的规模化数据集，以及ReasonBrain，一个旨在处理和执行跨多种场景的隐含假设性指令的新颖框架。Reason50K包含超过50K个样本，涵盖四个关键的推理场景：物理、时间、因果和故事推理。ReasonBrain利用多模态大型语言模型（MLLMs）生成编辑指导，并使用扩散模型进行图像合成，整合了一个细粒度推理线索提取（FRCE）模块来捕捉支持指令推理所需的关键视觉和文本语义。为了减轻语义损失，我们进一步引入了一个跨模态增强器（CME），它能够实现细粒度线索与MLLM导出特征之间的丰富交互。大量的实验表明，ReasonBrain在推理场景中始终优于最先进的基线，并显示出强大的零样本泛化能力，能够应对传统的IIE任务。我们的数据集和代码将公开发布。|
|**2025-07-02**|**AI4Research: A Survey of Artificial Intelligence for Scientific Research**|Qiguang Chen et.al.|[2507.01903](http://arxiv.org/abs/2507.01903)|null|近期，人工智能（AI）领域，尤其是像OpenAI-o1和DeepSeek-R1这样的大型语言模型（LLMs）在复杂领域如逻辑推理和实验编程方面的显著进步，展示了令人瞩目的能力。受这些进步的启发，许多研究探讨了AI在创新过程中的应用，尤其是在科学研究背景下。这些AI技术主要旨在开发能够自主进行跨多个科学学科的科研过程的系统。尽管取得了这些重大进展，但关于AI科研（AI4Research）的全面调查仍然缺失，这阻碍了我们对该领域的理解，也阻碍了该领域的进一步发展。为了解决这一差距，我们提出了一项全面的调查，并提供了AI4Research的统一视角。具体来说，我们工作的主要贡献如下：（1）系统分类法：我们首先引入了一个系统分类法来对AI4Research中的五个主流任务进行分类。（2）新领域：然后，我们确定了关键的研究差距，并突出了有希望的未来的研究方向，重点关注自动化实验的严谨性和可扩展性，以及社会影响。（3）丰富的应用和资源：最后，我们汇编了丰富的资源，包括相关的多学科应用、数据语料库和工具。我们希望我们的工作能够为研究界提供快速访问这些资源的途径，并刺激AI4Research领域的创新突破。|
|**2025-07-02**|**High-Layer Attention Pruning with Rescaling**|Songtao Liu et.al.|[2507.01900](http://arxiv.org/abs/2507.01900)|null|剪枝是一种高度有效的压缩大型语言模型（LLMs）的方法，显著降低了推理延迟。然而，传统的无需训练的剪枝方法通常采用一种启发式指标，无差别地删除所有剪枝层中的某些注意力头，而不考虑它们在网络架构中的位置。在这项工作中，我们提出了一种新的剪枝算法，该算法有策略地在模型的较高层剪枝注意力头。由于删除注意力头可能会改变标记表示的幅度，我们引入了一个自适应缩放参数，以调整剪枝后的表示尺度，以抵消这种影响。我们在包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B在内的多种LLMs上进行了全面的实验。我们的评估包括27个数据集上的生成和判别任务。结果表明，我们的方法在结构化剪枝方法中表现优于现有方法。这种改进在生成任务中尤为显著，我们的方法在性能上显著优于现有基线。|
|**2025-07-02**|**MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants**|Dongyi Ding et.al.|[2507.01887](http://arxiv.org/abs/2507.01887)|null|大型语言模型（LLMs）在需要长序列思考进行规划、反思和细化的推理任务上表现出色。然而，它们庞大的模型规模和高计算需求使得它们难以广泛应用。另一方面，小型语言模型（SLMs）由于容量有限，通常难以学习长格式CoT推理，我们称之为“SLMs学习差距”。为了解决这个问题，我们引入了MiCoTTeacher Assistant Distillation（MiCoTA）框架，这是一种用于提高SLMs长CoT蒸馏的框架。MiCoTA采用中等规模的模型作为教师助手，并使用中等长度的CoT序列来弥合容量和推理长度差距。我们在下游任务上的实验表明，尽管从大型教师蒸馏的SLMs可能表现不佳，但通过应用MiCoTA，它们在推理性能上实现了显著提升。具体来说，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct在AIME2024、AMC、奥数、MATH-500和GSM8K基准测试的平均分数上分别提高了3.47和3.93。为了更好地理解MiCoTA背后的机制，我们进行了一项定量实验，证明我们的方法产生与基础SLM分布更接近的数据。我们的见解为未来关于SLMs长CoT数据蒸馏的研究铺平了道路。|
|**2025-06-30**|**Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives**|Sixun Dong et.al.|[2506.24124](http://arxiv.org/abs/2506.24124)|null|时间序列预测传统上依赖于单模态数值输入，由于它们的密集和无结构性质，这些输入往往难以捕捉高级语义模式。尽管最近的方法已经探索使用大型语言模型（LLMs）将时间序列表示为文本，但这些方法仍然受到令牌序列离散性质的限制，并且缺乏人类通常应用的感知直觉，例如解释视觉模式。在本文中，我们提出了一种多模态对比学习框架，该框架将原始时间序列转换为结构化的视觉和文本视角。我们不是使用自然语言或真实世界的图像，而是直接从数值序列构建这两种模态。然后，我们通过对比学习将这些视角对齐到共享的语义空间中，使模型能够捕捉更丰富和更互补的表示。此外，我们引入了一个变量选择模块，该模块利用对齐的表示来识别多变量预测中最具信息量的变量。在十五个短期和六个长期预测基准上的大量实验表明，我们的方法在一致性上优于强大的单模态和跨模态基线，突出了多模态对齐在增强时间序列预测中的有效性。代码可在以下链接获取：https://github.com/Ironieser/TimesCLIP。|
|**2025-06-30**|**Calligrapher: Freestyle Text Image Customization**|Yue Ma et.al.|[2506.24123](http://arxiv.org/abs/2506.24123)|null|我们引入了Calligrapher，这是一个基于扩散的新型框架，它创新性地将高级文本定制与艺术字体排版相结合，应用于数字书法和设计领域。针对字体定制中的精确风格控制和数据依赖性问题，我们的框架贡献了三项关键技术。首先，我们开发了一种自蒸馏机制，利用预训练的文本到图像生成模型以及大型语言模型自动构建以风格为中心的字体排版基准。其次，我们通过可训练的风格编码器引入了局部化风格注入框架，该编码器包括Qformer和线性层，用于从参考图像中提取鲁棒的风格特征。此外，还采用了情境生成机制，将参考图像直接嵌入到降噪过程中，进一步增强了目标风格的精细对齐。在多种字体和设计环境下进行的广泛定量和定性评估证实了Calligrapher在复杂数字细节和精确符号定位方面的准确复制能力。通过自动化高质量、视觉上一致的字体设计，Calligrapher超越了传统模型，赋予数字艺术、品牌设计和情境字体设计领域的创意实践者更多能力。|
|**2025-06-30**|**Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime**|Yuqing Wang et.al.|[2506.24120](http://arxiv.org/abs/2506.24120)|null|数据选择在数据驱动决策中扮演着至关重要的角色，包括在大语言模型（LLMs）中，通常与任务相关。数据质量与多样性等特性已被广泛研究，并已知能够提升模型性能。然而，目前尚不清楚是否存在其他可以持续提升性能的定量和通用数据选择原则，尤其是在具有有限先验知识的复杂任务中。在本文中，我们证明了选择分布更均匀的数据可以提高训练效率并增强性能。具体来说，我们建立了更均匀（偏差更小）的分布会导致数据点之间的最小成对距离 $h_{\min}$ 更大，并证明较小的 $h_{\min}$ 会减慢梯度下降（GD）的训练动态。此外，我们从理论上证明了随着 $h_{\min}$ 的增加，神经网络的近似误差会减小。我们的分析引入了一个超越神经网络切线核（NTK）范围的GD收敛框架，适用于包括变换器在内的广泛架构，且无需Lipschitz光滑性。该框架进一步为深度神经网络架构中使用残差连接和函数组合提供了理论上的合理性。最后，我们在包括不同优化策略、模型大小和训练数据集在内的各种设置下进行了全面的监督微调实验。结果表明，通过最大化成对距离选择数据可以显著加速训练，并在LLMs的多个数据集上实现可比或更好的性能。代码和数据集可在以下链接获取：https://github.com/SafeRL-Lab/data-uniformity。|
|**2025-06-30**|**DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World**|Xiangtai Li et.al.|[2506.24102](http://arxiv.org/abs/2506.24102)|null|多模态大型语言模型（MLLMs）在理解场景方面展现出复杂的理解能力，这得益于大规模和高品质的数据集。大多数现有的标题数据集缺乏视觉实体的地面位置和关系。一些基于地标的标题数据集在高分辨率图像上存在描述缺失、关系不完整和大量对象描述的问题。为了填补这一领域的空白，我们提出了DenseWorld-1M，这是第一个大规模、详细、密集的基于地标的真实世界标题数据集。我们设计了一个包含三个阶段的标注流程，包括开放世界感知、详细对象标题生成和密集标题合并。第一阶段获取实体级掩码和标签。第二阶段在第一阶段提供的掩码和标签的指导下生成对象级、详细的标题。最后一阶段将对象标题和掩码合并为空间和关系密集的标题。为了加速标注过程并提高标题质量，我们提出了两个视觉语言模型（VLM）：详细区域标题模型和空间标题合并模型。在包括视觉语言理解、视觉定位和区域标题生成在内的各种设置上的广泛实验，证明了我们的DenseWorld-1M数据集和标注模型的有效性。|
|**2025-06-30**|**Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models**|Tung-Ling Li et.al.|[2506.24056](http://arxiv.org/abs/2506.24056)|null|我们引入了logit-gap引导，这是一个快速越狱框架，将RLHF对齐的语言模型的拒绝-确认差距转化为对词汇表的单次遍历。一个可前向计算的分数将差距减少与轻量级的KL惩罚和奖励偏移代理相结合，使得“排序-求和-停止”的扫描能在不到一秒内完成，并返回一个简短的后缀——比束搜索或梯度攻击少两个数量级的模型调用。相同的后缀可以推广到未见过的提示，并从0.5B扩展到70B的检查点，将单次攻击的成功率从基线水平提升到80-100%，同时保持主题连贯性。除了效率之外，这些后缀揭示了句子边界奖励悬崖和其他对齐伪影，为如何通过安全性调整重塑内部表示提供了一个轻量级的探测工具。|
|**2025-06-30**|**Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC**|Xinming Wei et.al.|[2506.24045](http://arxiv.org/abs/2506.24045)|null|随着代理大型语言模型（LLMs）在个人设备上的普及，出现了一类具有目标二分特征的工作负载。由用户发起的反应性任务需要即时、低延迟的响应，而主动性任务则在不显眼的情况下运行，并优先考虑吞吐量。现有的设备上LLM引擎是为独立推理设计的，无法高效管理在消费级异构SoC（具有CPU、集成GPU和NPU）上并发且冲突的请求。本文介绍了Agent.xpu，这是一个针对内存统一的异构SoC上代理LLM工作负载的高效服务系统。通过专门的离线分析，Agent.xpu首先构建一个异构执行图，融合和分块模型内核，以亲和性引导的弹性加速器映射和预测性内核注解。在运行时，其在线调度器允许细粒度、内核级别的抢占，以保证反应性任务的响应性。为了最大化SoC利用率，它采用空闲感知内核回填，以机会性地添加主动性任务，并通过带宽感知调度减轻NPU-iGPU竞争。在英特尔Core Ultra SoC上的评估表明，与最先进的推理引擎相比，Agent.xpu实现了反应性任务4.6倍的更低延迟，并保持了主动性任务1.6倍至6.8倍更高的吞吐量。|
|**2025-06-30**|**A Survey on Vision-Language-Action Models for Autonomous Driving**|Sicong Jiang et.al.|[2506.24044](http://arxiv.org/abs/2506.24044)|null|多模态大型语言模型（MLLM）的快速发展为视觉-语言-动作（VLA）范式铺平了道路，该范式将视觉感知、自然语言理解和控制集成在单一策略中。自动驾驶领域的学者们正在积极将这些方法应用于车辆领域。这类模型承诺实现能够理解高级指令、对复杂交通场景进行推理并自行作出决策的自动驾驶汽车。然而，相关文献尚处于分散状态且迅速扩展。本综述提供了关于自动驾驶中VLA（VLA4AD）的第一个全面概述。我们（i）对最近工作中共享的架构构建块进行形式化，（ii）追溯了从早期解释器到以推理为中心的VLA模型的发展过程，（iii）根据VLA在自动驾驶领域的进展比较了20多个代表性模型。我们还整合了现有数据集和基准，突出了同时衡量驾驶安全、准确性和解释质量的协议。最后，我们详细阐述了开放性挑战——鲁棒性、实时效率和形式验证——并概述了VLA4AD的未来方向。本综述为推进可解释且与社会目标一致的自动驾驶汽车提供了简洁而完整的参考资料。GitHub仓库地址为 \href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}。|
|**2025-06-30**|**EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations**|Hyunjong Kim et.al.|[2506.24016](http://arxiv.org/abs/2506.24016)|null|近年来，大型语言模型和视觉语言模型在图像描述领域的进展引起了人们对可解释性评估指标的兴趣。然而，这些指标在缺乏标准化标准的情况下生成解释，而生成的解释的整体质量尚未得到验证。在本文中，我们提出了EXPERT，这是一种无参考的评估指标，它基于三个基本标准：流畅性、相关性和描述性，提供结构化解释。通过构建高质量结构化解释的大规模数据集，我们开发了一个两阶段评估模板，以有效监督视觉语言模型进行评分和解释生成。EXPERT在基准数据集上取得了最先进的结果，同时提供了比现有指标显著更高质量的解释，这通过全面的人类评估得到了验证。我们的代码和数据集可在https://github.com/hjkim811/EXPERT上获取。|
|**2025-06-30**|**Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective**|Anselm R. Strohmaier et.al.|[2506.24006](http://arxiv.org/abs/2506.24006)|null|大型语言模型（LLMs）如ChatGPT的发展引发了一个问题：如何将它们融入教育领域。一个希望是它们能够支持数学学习，包括解决文字问题。由于LLMs能够轻松处理文本输入，它们似乎非常适合解决数学文字问题。然而，它们的实际能力，即它们是否能够理解现实世界情境，以及这对课堂的影响仍然不清楚。我们从数学教育角度进行了一项范围综述，包括三个部分：技术概述、用于研究中的文字问题的系统综述以及LLMs在数学文字问题上的最新实证评估。首先，在技术概述中，我们对比了LLMs和学生在文字问题及其解决过程中的概念化。在计算机科学研究中，这通常被称为数学推理，这个术语与数学教育中的使用不完全一致。其次，我们对213项研究的文献综述表明，最受欢迎的文字问题语料库主要由s-problems组成，这些s-problems不需要考虑其现实世界情境的现实性。最后，我们对GPT-3.5-turbo、GPT-4o-mini、GPT-4.1和o3在287个文字问题上的评估显示，最新的LLMs几乎完美地解决了这些s-problems，包括在PISA的20个问题中获得了满分。LLMs在处理现实世界情境有问题的或非理性的问题时仍表现出弱点。总的来说，基于这三个方面，我们认为LLMs已经掌握了表面的解决过程，但没有理解文字问题，这可能会限制它们作为数学课堂中教学工具的价值。|
|**2025-06-30**|**Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning**|Seungjun Yi et.al.|[2506.23998](http://arxiv.org/abs/2506.23998)|null|先天性心脏病（CHD）带来了复杂且终身的挑战，这些挑战在传统的临床指标中往往没有得到充分的体现。虽然非结构化的叙述能够提供对患者和照护者经验的丰富洞察，但手动主题分析（TA）仍然是一个劳动密集型且不可扩展的过程。我们提出了一种全自动化的大型语言模型（LLM）管道，该管道对临床叙述进行端到端的主题分析，从而消除了手动编码或完整转录审查的需求。我们的系统采用了一种新颖的多智能体框架，其中专门的LLM智能体承担角色以提高主题质量并与人工分析保持一致。为了进一步提高主题的相关性，我们可选地集成了来自人类反馈的强化学习（RLHF）。这支持了对大量定性数据集的规模化和以患者为中心的分析，并允许LLM针对特定的临床环境进行微调。|
|**2025-06-27**|**The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements**|Bingchen Zhao et.al.|[2506.22419](http://arxiv.org/abs/2506.22419)|null|大型语言模型（LLMs）的快速发展有潜力助力科学进步。实现这一目标的关键能力之一是能够重现现有工作。为了评估AI代理在活跃研究领域重现结果的能力，我们引入了自动LLM速度跑基准，利用了研究社区在NanoGPT速度跑比赛中的贡献，该比赛是为了在尽可能短的时间内训练一个GPT-2模型。每个19个速度跑任务都为代理提供了之前的记录训练脚本，并可选择与三种提示格式之一相结合，这些提示格式从伪代码到类似论文的新记录改进描述不等。由于设计上的快速执行，记录得以迅速完成，而速度跑的改进涵盖了从高级算法进步到硬件感知优化的各种代码级别变化。这些特性使得基准对于改进LLM训练这一前沿问题既易于接触又具有现实性。我们发现，最近的推理LLMs与最先进的（SoTA）脚手架结合后，即使在提供详细提示的情况下，也难以在我们基准中重现已知的创新。因此，我们的基准提供了一个简单、非饱和的度量，用于衡量LLMs自动重现科学成果的能力，这是自主研究代理所必需的（但不是充分的）技能。|
|**2025-06-27**|**HyperCLOVA X THINK Technical Report**|NAVER Cloud HyperCLOVA X Team et.al.|[2506.22403](http://arxiv.org/abs/2506.22403)|null|我们推出了HyperCLOVA X THINK，这是HyperCLOVA X家族中第一个以推理为中心的大语言模型，它在大约6000亿个高质量韩语和英语语料库上进行预训练，并增加了针对的合成韩语数据。它被实现为一个计算-内存平衡的Peri-LN Transformer，采用μP进行扩展，通过一个三阶段课程预训练，将上下文窗口扩展到128K个token，并通过监督微调和可验证奖励的强化学习进行后训练，支持详细推理和简洁答案模式。在针对韩国的基准测试中，如KMMLU、CSAT、KoBALT-700、HAERAE-1.0和KoBigBench，它对同类大小模型具有竞争力的性能，同时保持了稳健的双语一致性和翻译质量。此外，一个视觉增强的变体在KCSAT STEM基准测试中与GPT-4.1相当或超过，所有这些都是在比现有类似大小模型显著低得多的训练计算量下实现的。我们还提出了一种修剪和蒸馏技术，该技术将很快应用于HyperCLOVA X THINK，以构建一个开源和商业友好的基础模型。总的来说，这些能力将HyperCLOVA X THINK定位为韩国AI创新的稳健基础和全球研究社区的宝贵资源。|
|**2025-06-27**|**Refining Czech GEC: Insights from a Multi-Experiment Approach**|Petr Pechman et.al.|[2506.22402](http://arxiv.org/abs/2506.22402)|null|我们提出了一种基于神经网络翻译方法且采用Transformer架构的语法纠错（GEC）系统，该系统在捷克语领域达到了最先进的水平。该系统的关键特性是其实时合成生成流程，该流程通过引入语言无关和捷克语特有的错误，动态地增强句子以合成人工错误。我们进行了一系列全面的实验，研究了捷克语GEC语料库作为合成错误引入的基础、多种错误生成策略、领域平衡、分词粒度、模型大小以及在微调过程中的数据缩放。此外，我们还评估了大型语言模型（LLMs）在捷克语GEC的最终用户和专家微调场景中的性能。我们表现最好的模型在性能和计算效率方面都更优越。源代码和训练好的模型链接可在https://github.com/ufal/tsd2025-gec上找到。|
|**2025-06-27**|**QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization**|Danush Khanna et.al.|[2506.22396](http://arxiv.org/abs/2506.22396)|null|推理在大型语言模型（LLM）部署中的延迟和能耗占比最大，通常超过总成本的90%。尽管训练时间效率已经取得了显著进展，但在自回归解码下的运行时优化仍然是一个关键瓶颈。现有的方法，如剪枝、量化、早期退出和推测性解码，通常需要重新训练、架构变更或破坏解码兼容性。我们引入了QuickSilver，这是一个模块化、按令牌级别的框架，能够在推理时实现语义适应性，而无需改变模型权重或结构。QuickSilver集成了四种协同机制： (i) 动态令牌终止，对具有收敛表示的令牌停止计算； (ii) KV缓存跳过，选择性地抑制内存写入以减少注意力开销； (iii) 上下文令牌融合，将冗余令牌合并到共享路径中，以缩短序列长度。与推测性解码或MoE路由不同，QuickSilver完全在冻结的密集模型上运行，不需要辅助网络。将QuickSilver应用于GPT-2和Llama-2，在WikiText-103和C4上，QuickSilver实现了高达39.6%的FLOP减少，同时困惑度下降可忽略不计（<=0.2）。|
|**2025-06-27**|**What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub**|Ramtin Ehsani et.al.|[2506.22390](http://arxiv.org/abs/2506.22390)|null|本文分析了在GitHub问题线程中共享的686条开发者与ChatGPT的对话，以识别使这些对话对问题解决有效的特征。首先，我们分析了对话及其相应的问题，以区分有帮助和无帮助的对话。我们首先对开发者寻求帮助的任务类型进行分类，以更好地理解ChatGPT最有效的场景。接下来，我们考察了广泛的对话、项目和问题相关指标，以揭示与有帮助的对话相关的因素。最后，我们确定了无帮助ChatGPT响应中常见的不足，以突出可以指导更有效的开发者工具设计的领域。我们发现，只有62%的ChatGPT对话对成功的问题解决有帮助。ChatGPT在代码生成和工具/库/API推荐方面最有效，但在代码解释方面表现不佳。有帮助的对话往往较短、更易读，并且展现出更强的语义和语言一致性。更大、更受欢迎的项目以及更有经验的开发者能更多地从ChatGPT中受益。在问题层面，ChatGPT在简单问题、有限的开发者活动和更快的解决速度方面表现最佳，通常是范围很好的任务，如编译错误。无帮助ChatGPT响应中最常见的不足包括信息不正确和不全面。我们的发现具有广泛的含义，包括指导开发者采用有效的问题解决交互策略、为支持最佳提示设计的发展提供工具或框架，以及对调整LLM以适应问题解决任务提供洞察。|
|**2025-06-27**|**Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment**|Yue Zhang et.al.|[2506.22385](http://arxiv.org/abs/2506.22385)|null|视频大型多模态模型（VLMMs）在理解视频内容方面取得了显著的进步，但它们往往难以处理抽象和适应性推理——即在新信息出现时修订其解释的能力。在现实中，结论很少是一成不变的；额外的背景信息可能会加强或削弱最初的推断。为了解决这个问题，我们引入了可反驳的视频蕴涵（DVidE），这是一个新的任务，挑战模型像怀疑者一样思考，不断根据不断发展的证据更新其推理。在DVidE中，给定一个视频前提和一个文本假设，模型必须确定新的更新是加强了假设（分类版本）还是削弱了假设（生成版本），或者生成一个连贯的更新来修改蕴涵关系（生成版本）。为了解决分类任务，我们提出了反事实思维链框架，利用反事实推理、ASR增强的视频内容和理由细化来减少推理偏差。对于生成任务，我们开发了一个框架，该框架将ASR输出与大型语言模型（LLM）相结合，以产生符合预期加强或削弱目标的连贯、上下文相关的更新。此外，我们引入了一个新的基准数据集，其中包含加强/削弱注释，以及一个基于LLM的评估指标，专门用于评估生成性能。实验结果表明，我们的方法在增强VLMMs的动态推理能力方面取得了显著的改进。|
|**2025-06-27**|**Probabilistic Optimality for Inference-time Scaling**|Youkang Wang et.al.|[2506.22376](http://arxiv.org/abs/2506.22376)|null|推理时间缩放已成为增强大型语言模型（LLMs）推理性能的一种强大技术。然而，现有方法通常依赖于启发式策略进行并行采样，缺乏一个原则性的基础。为了解决这一差距，我们提出了一种概率框架，该框架在并行样本独立同分布（i.i.d.）的假设下，形式化了推理时间缩放的优化性，并且最佳N选择策略遵循一个可以估计的概率分布。在这个框架内，我们推导出达到目标性能水平所需样本数量的理论下限，为计算高效的缩放提供了首次原则性指导。利用这一洞察，我们开发了\textsc{OptScale}算法，该算法动态确定最佳样本数量。\textsc{OptScale}使用基于语言模型的预测器来估计概率先验参数，从而能够决定满足预定义性能阈值和置信水平的最小样本数量。在数学推理基准测试（包括MATH-500、GSM8K、AIME和AMC）上的大量实验表明，\textsc{OptScale}显著减少了采样开销，同时在推理性能上优于或与最先进的技术相当。我们的工作为原则性推理时间缩放提供了理论和实践解决方案，解决了高效部署LLMs进行复杂推理的关键差距。|
|**2025-06-27**|**Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement**|Maryam Mousavian et.al.|[2506.22372](http://arxiv.org/abs/2506.22372)|null|自然语言处理（NLP）和信息检索（IR）系统中存在社会偏见是一个持续性的挑战，这突出了开发稳健方法来识别和评估此类偏见的重要性。在本文中，我们旨在通过利用大型语言模型（LLMs）来检测和衡量段落排序中的性别偏见来解决这个问题。现有的性别公平性指标依赖于词汇和频率度量，导致各种局限性，例如，遗漏细微的性别差异。基于我们基于LLM的性别偏见检测方法，我们引入了一种新的性别公平性指标，称为类加权曝光（CWEx），旨在解决现有局限性。为了衡量我们提出的指标的有效性并研究LLMs在检测性别偏见方面的有效性，我们对MS MARCO段落排序集合的一个子集进行了标注，并发布了我们的新性别偏见集合，称为MSMGenderBias，以促进该领域未来的研究。我们在各种排序模型上的广泛实验结果表明，我们提出的指标比以前的指标提供了更详细的公平性评估，与人类标签的匹配度更高（Grep-BiasIR为58.77%，MSMGenderBias为18.51%，使用Cohen's Kappa一致性进行衡量），有效地区分了排序中的性别偏见。通过整合LLM驱动的偏见检测、改进的公平性指标以及为现有数据集提供的性别偏见标注，这项工作为分析和减轻IR系统中的偏见提供了一个更稳健的框架。|
|**2025-06-27**|**Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny**|Carolina Carreira et.al.|[2506.22370](http://arxiv.org/abs/2506.22370)|null|随着计算机教育中，学生越来越多地使用大型语言模型（LLMs）如ChatGPT。然而，LLMs在支持认知要求较高的任务，如演绎程序验证中的作用仍然理解不深。本文通过研究学生在解决Dafny语言中的形式验证练习时如何与LLM互动，来探讨这一问题。Dafny是一种支持功能正确性的语言，它允许程序员编写形式规范，并自动验证实现是否满足规范。我们进行了一项混合方法研究，研究对象为参加形式方法课程的硕士研究生。每位参与者完成了两个验证问题，一个在有访问自定义ChatGPT接口的情况下进行，该接口记录了所有互动，另一个则没有。我们确定了成功学生的策略，并评估了学生对LLMs的信任程度。我们的发现表明，学生在使用ChatGPT时表现显著更好；然而，性能提升与提示质量相关。我们最后提出了将LLMs更有效地整合到形式方法课程中的实际建议，包括设计促进学习而非替代的LLM感知挑战。|
|**2025-06-27**|**Concept-Level AI for Telecom: Moving Beyond Large Language Models**|Viswanath Kumarskandpriya et.al.|[2506.22359](http://arxiv.org/abs/2506.22359)|null|电信和网络领域正处于一个变革时代的边缘，这一变革是由管理日益复杂、层级化、多行政管理域（即同一路径上的多个运营商）和多语言系统的必要性所驱动的。最近的研究表明，大型语言模型（LLMs）凭借其卓越的通用文本分析和代码生成能力，可以有效地应用于某些电信问题（例如，自动配置数据计划以满足特定应用需求）。然而，由于LLMs固有的逐个标记处理和维持扩展上下文的有限能力，它们在满足电信特定要求方面存在困难，例如跨层依赖级联（即跨越OSI）、时空故障关联和实时分布式协调。相比之下，大型概念模型（LCMs）通过在语义概念而不是单个词汇标记的抽象级别上进行推理，为解决这些电信挑战提供了一种根本性的优越方法。通过采用双曲隐空间进行分层表示，并将复杂的多层网络交互封装在简洁的概念嵌入中，LCMs克服了LLMs在内存效率、跨层关联和原生多模态集成方面的关键不足。本文认为，采用LCMs不仅仅是一个渐进的步骤，而是实现稳健和有效AI驱动电信管理所必需的进化飞跃。|
|**2025-06-26**|**mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale**|Xiaona Zhou et.al.|[2506.21550](http://arxiv.org/abs/2506.21550)|null|多变量时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于变量间的复杂依赖关系、时间动态和稀疏的异常标签，它仍然具有挑战性。我们介绍了mTSBench，这是迄今为止最大的MTS-AD和无监督模型选择基准，涵盖了19个数据集和12个不同应用领域的344个标记时间序列。mTSBench评估了24种异常检测方法，包括基于大型语言模型（LLM）的多变量时间序列检测器，并在标准化条件下系统地基准测试了无监督模型选择技术。与先前的研究结果一致，我们的结果证实了没有单一的检测器能在所有数据集上表现出色，强调了模型选择的重要性。然而，即使是最先进的选拔方法也远未达到最佳，揭示了关键差距。mTSBench提供了一个统一的评估套件，以实现严格、可重复的比较，并催化自适应异常检测和鲁棒模型选择领域的未来进步。|
|**2025-06-26**|**Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test**|Ziyue Li et.al.|[2506.21551](http://arxiv.org/abs/2506.21551)|null|在神经网络训练中，近期观察到的一种现象称为“Grokking”，即测试性能在训练损失收敛后仍持续提升，这使泛化机制以及其他如推理等新兴能力变得神秘。以往的研究通常在数千个epoch上使用小型模型在少量玩具或高度特定任务上进行训练，而我们的研究是首次对在单次预训练过程中对7B大型语言模型（LLM）即OLMoE的检查点上的Grokking现象进行的研究。我们计算了训练损失，并在包括数学推理、代码生成以及常识/特定领域知识检索任务在内的多种基准任务上评估了泛化能力。我们的研究首次验证了，尽管不同数据可能异步进入Grokking阶段，但在大规模基础模型的预训练过程中Grokking仍然发生。我们进一步通过研究LLM的内部动态来揭开Grokking“泛化能力出现”的神秘面纱。具体来说，我们发现训练样本的路径（即层间专家选择）在Grokking过程中从随机、实例特定演变为更有结构和样本间可共享。此外，尽管损失已经收敛，样本路径的复杂性仍然降低。这些表明了从记忆到泛化的转换，为延迟泛化提供了机制上的解释。在研究中，我们开发了两个新的指标来量化路径距离和单个路径的复杂性。我们展示了它们在预测不同下游任务泛化提升方面的能力。这些指标计算效率高、简单且仅依赖于训练数据。因此，它们在预训练中具有实际价值，使我们能够监控泛化性能而无需微调和测试。从理论上讲，我们证明了更有结构的路径可以减少模型复杂度并提高泛化界限。|
|**2025-06-26**|**PsyLite Technical Report**|Fangjun Ding et.al.|[2506.21536](http://arxiv.org/abs/2506.21536)|null|随着数字技术的快速发展，AI驱动的心理咨询服务逐渐成为心理健康领域的一个重要研究方向。然而，现有的模型在对话安全性、详细场景处理和轻量级部署方面仍存在不足。为了解决这些问题，本研究提出了PsyLite，这是一个基于基础模型InternLM2.5-7B-chat开发的轻量级心理咨询服务大型语言模型代理。通过两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），PsyLite增强了模型的高级推理能力、心理咨询服务能力和安全对话能力。使用Ollama和Open WebUI进行部署后，通过Pipelines创建了一个自定义工作流程。设计了一种创新的条件RAG，在心理咨询服务中适当引入交头接耳幽默元素，以提升用户体验并减少危险请求，从而加强对话安全性。评估结果显示，PsyLite在中文通用评估（CEval）、心理咨询服务专业评估（CPsyCounE）和对话安全性评估（SafeDialBench）方面优于基线模型，特别是在心理咨询服务专业性（CPsyCounE得分提高47.6%）和对话安全性（\safe{}得分提高2.4%）方面。此外，该模型采用了量化技术（GGUF q4\_k\_m）以实现低硬件部署（仅需5GB内存即可运行），为资源受限环境中的心理咨询服务应用提供了可行的解决方案。|
|**2025-06-26**|**Exploring the Design Space of 3D MLLMs for CT Report Generation**|Mohammed Baharoon et.al.|[2506.21535](http://arxiv.org/abs/2506.21535)|null|多模态大型语言模型（MLLMs）已成为自动化放射学报告生成（RRG）的一种有希望的途径。在本工作中，我们系统地研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、大型语言模型（LLMs）以及用于3D CT报告生成的微调技术。我们还介绍了两种基于知识的报告增强方法，这些方法将GREEN分数的性能提高了高达10%，在MICCAI 2024 AMOS-MM挑战赛中取得了第二名。我们对AMOS-MM数据集中的1,687个案例的研究结果表明，在相同的训练协议下，RRG在很大程度上不受LLM大小的制约。我们还表明，如果原始ViT在较小体积尺寸上预训练，则更大的体积尺寸并不总是能提高性能。最后，我们展示了使用分割掩码与CT体积一起使用可以提高性能。代码在https://github.com/bowang-lab/AMOS-MM-Solution上公开可用。|
|**2025-06-26**|**"What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets**|Akshay Paruchuri et.al.|[2506.21532](http://arxiv.org/abs/2506.21532)|null|随着人们越来越多地通过互动聊天机器人从大型语言模型（LLMs）中寻求医疗保健信息，这些对话的性质和固有风险却鲜有研究。在本文中，我们筛选了大规模对话人工智能数据集，构建了HealthChat-11K，这是一个由25,000条用户消息组成的11,000条真实世界对话的精选数据集。我们使用HealthChat-11K和一种基于医生驱动的分类法，用于研究用户在寻求医疗保健信息时如何与LLMs互动，以便系统地研究21个不同的医疗专业领域的用户交互。我们的分析揭示了用户寻求健康信息的方式和原因的性质，例如常见的交互、不完整上下文的实例、情感行为以及可能导致奉承的交互（例如引导性问题），这突显了改进作为对话人工智能部署的LLMs的医疗保健支持能力的需求。获取我们的分析和将其组合成精选数据集的代码和工件可在此找到：https://github.com/yahskapar/HealthChat|
|**2025-06-26**|**Potemkin Understanding in Large Language Models**|Marina Mancoridis et.al.|[2506.21521](http://arxiv.org/abs/2506.21521)|null|大型语言模型（LLMs）通常使用基准数据集进行评估。但基于LLMs对一组精心设计的提问的回答来推断其能力，这种做法有何依据？本文首先介绍了一个正式框架来解决这一问题。关键在于注意到，用于测试LLMs的基准——如AP考试——同样也是用来测试人类的。然而，这引发了一个推论：只有当LLMs在理解概念的方式上与人类相似时，这些基准才被认为是有效的测试。否则，在基准测试中的成功仅展示了虚假的理解：由无法与人类对概念的任何解读相调和的回答所驱动的理解错觉。我们提出了两种量化虚假理解（potemkins）存在的程序：一种使用三个领域专门设计的基准，另一种使用一种通用程序，为它们普遍存在的程度提供一个下限。我们发现虚假理解在各个模型、任务和领域中普遍存在。我们还发现，这些失败不仅反映了错误的理解，还反映了概念表示中的更深层次的内部不一致性。|
|**2025-06-26**|**Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge**|Boyu Gou et.al.|[2506.21506](http://arxiv.org/abs/2506.21506)|null|代理搜索，如深度研究系统，其中大型语言模型自主浏览网络、综合信息并返回全面且附有参考文献的答案，代表了用户与网络规模信息交互方式的重大转变。虽然代理搜索承诺提供更高的效率和认知卸载，但其日益增长复杂性和开放性已超出现有评估基准和方法，这些基准和方法大多假设搜索范围较短且答案静态。在本文中，我们介绍了Mind2Web 2，这是一个包含130个真实、高质量、长期任务的基准，这些任务需要实时网络浏览和广泛的信息综合，构建过程中使用了超过1000小时的人工劳动。为了应对评估随时间变化和复杂答案的挑战，我们提出了一种新颖的“代理作为裁判”框架。我们的方法基于树状评分标准设计，构建特定任务的裁判代理，以自动评估答案的正确性和来源归属。我们对九个前沿代理搜索系统和人类性能进行了全面评估，并进行了详细的错误分析，以对未来发展提供见解。表现最佳的系统，即OpenAI的深度研究系统，已经能够达到50-70%的人类水平，同时花费的时间仅为一半，显示出巨大的潜力。总之，Mind2Web 2为开发和发展下一代代理搜索系统提供了严谨的基础。|
|**2025-06-26**|**Bridging Offline and Online Reinforcement Learning for LLMs**|Jack Lanchantin et.al.|[2506.21495](http://arxiv.org/abs/2506.21495)|null|我们研究了强化学习方法在从离线模式过渡到半在线模式和全在线模式时，对大型语言模型进行微调的有效性，涵盖了可验证和非可验证任务。我们的实验涵盖了在可验证数学上的训练以及非可验证指令遵循，并对两者都进行了一系列基准评估。在这些设置中，我们广泛比较了在线和半在线的直接偏好优化和群体奖励策略优化目标，出人意料地发现这些变体之间在性能和收敛性方面相似，并且都显著优于离线方法。我们详细分析了训练动态和超参数选择策略，以实现最佳结果。最后，我们表明，通过联合使用可验证和非可验证奖励的多任务处理，在两种任务类型上都能提高性能。|
|**2025-06-26**|**Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces**|Michael Johnston et.al.|[2506.21467](http://arxiv.org/abs/2506.21467)|null|寻找在满足既定服务等级协议的同时，以最低成本部署给定工作负载的云资源最优组合是一个活跃的研究领域。将适用于云提供商提供的众多计算、存储和服务参数与类似数量的特定应用参数相结合，导致配置空间有数百万种部署选项。在本文中，我们提出了发现空间（Discovery Space），这是一种将工作负载配置问题描述形式化的抽象，并展示了在大搜索空间中进行结构化、稳健和分布式研究所需的一系列特征。我们描述了发现空间抽象的具象实现，并展示了它在处理各种工作负载，如大型语言模型推理和大数据分析等方面具有普遍适用性。我们证明了我们的方法可以实现最佳优化器执行之间的数据安全、透明共享，从而提高在大搜索空间中检测最优配置的效率。我们还展示了发现空间如何实现类似搜索空间之间知识和技能的迁移与重用，从而实现配置搜索速度提升超过90%。|
|**2025-06-26**|**ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing**|Huadai Liu et.al.|[2506.21448](http://arxiv.org/abs/2506.21448)|null|尽管端到端视频到音频生成已经取得了巨大进步，但生成高保真音频以真实捕捉视觉内容的细微差别仍然具有挑战性。类似于创意产业中的专业人士，这种生成需要关于视觉动态、声学环境和时间关系等项目的复杂推理。我们提出了一个名为 \textbf{ThinkSound} 的新型框架，该框架利用思维链（CoT）推理来实现视频的逐步、交互式音频生成和编辑。我们的方法将整个过程分解为三个互补阶段：基础声音效果生成，创建语义上连贯的声音场景；通过精确的用户交互进行以对象为中心的交互式细化；以及由自然语言指令引导的针对性编辑。在每个阶段，一个多模态大型语言模型都会生成与上下文对齐的CoT推理，以引导统一的音频基础模型。此外，我们引入了 \textbf{AudioCoT}，这是一个具有结构化推理注释的全面数据集，它建立了视觉内容、文本描述和声音合成之间的联系。实验表明，ThinkSound在视频到音频生成方面达到了音频指标和CoT指标的顶尖性能，并在Movie Gen Audio基准测试中表现出色。演示页面可在https://ThinkSound-Demo.github.io找到。|
|**2025-06-25**|**The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind**|Andrei Lupu et.al.|[2506.20664](http://arxiv.org/abs/2506.20664)|null|随着大型语言模型（LLMs）获得代理能力，它们将不得不在复杂的多人代理场景中导航，与人类用户和其他代理在合作和竞争环境中互动。这需要新的推理技能，其中最重要的是心智理论（ToM），即推理其他代理“心理”状态的能力。然而，由于现有的基准存在范围狭窄、数据泄露、饱和和缺乏互动等问题，LLMs中的ToM和其他多人能力理解得并不好。因此，我们提出了Decrypto，一个基于游戏的多人推理和心智理论基准，灵感来自认知科学、计算语用学和多人强化学习。它在所有其他维度上尽可能简单，消除了其他基准中常见的混杂因素。据我们所知，它也是第一个设计交互式心智理论实验的平台。我们通过前沿LLMs的全面实证评估、稳健性研究和人机交叉游戏实验来验证基准设计。我们发现LLMs的游戏能力落后于人类和简单的词嵌入基线。然后我们在Decrypto中创建了两个经典认知科学实验的变体，以评估三个关键的心智理论能力。令人惊讶的是，我们发现最先进的推理模型在这些任务上的表现比它们的旧版本差得多。这表明Decrypto解决了当前推理和心智理论评估中的关键差距，并为更好的代理铺平了道路。|
|**2025-06-25**|**Memento: Note-Taking for Your Future Self**|Chao Wan et.al.|[2506.20642](http://arxiv.org/abs/2506.20642)|null|大型语言模型（LLMs）在仅推理的任务上表现出色，但在推理必须紧密耦合检索的情况下，如多跳问答，则表现不佳。为了克服这些限制，我们提出了一种提示策略，该策略首先将复杂问题分解成更小的步骤，然后使用LLMs动态构建事实数据库，最后将这些事实拼接起来以解决问题。我们展示了我们称之为Memento的三阶段策略如何在不同场景下提升现有提示策略的性能。在9步骤的PhantomWiki基准测试中，当所有信息都在上下文中提供时，Memento将思维链（CoT）的性能提高了两倍。在2WikiMultiHopQA的开源版本中，使用Memento的CoT-RAG比普通的CoT-RAG提高了超过20个F1百分比点，并且比多跳RAG基线IRCoT提高了超过13个F1百分比点。在具有挑战性的MuSiQue数据集中，Memento将ReAct提高了超过3个F1百分比点，展示了其在具有代理性质的环境中的实用性。|
|**2025-06-25**|**Towards Community-Driven Agents for Machine Learning Engineering**|Sijie Li et.al.|[2506.20640](http://arxiv.org/abs/2506.20640)|null|基于大型语言模型的机器学习（ML）智能体在自动化ML研究中展现出巨大潜力。然而，现有的智能体通常在特定研究问题上独立运作，没有与更广泛的研究社区互动，而人类研究者通常通过分享知识来获得洞察和做出贡献。为了弥合这一差距，我们引入了MLE-Live，这是一个实时评估框架，旨在评估智能体与模拟的Kaggle研究社区进行交流和利用集体知识的能力。在此基础上，我们提出了CoMind，这是一种在社区环境中擅长交换见解和开发新解决方案的新型智能体。CoMind在MLE-Live上实现了最先进的性能，并在四个正在进行中的Kaggle竞赛中平均超越了79.2%的人类竞争对手。我们的代码已发布在https://github.com/comind-ml/CoMind。|
|**2025-06-25**|**DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation**|Shansan Gong et.al.|[2506.20639](http://arxiv.org/abs/2506.20639)|null|扩散大型语言模型（dLLMs）是自回归（AR）模型的诱人替代品，因为它们的去噪模型在整个序列上操作。dLLMs的全局规划和迭代优化功能特别适用于代码生成。然而，目前对dLLMs在编码中的训练和推理机制仍处于探索阶段。为了揭开dLLMs解码行为的神秘面纱并释放其在编码中的潜力，我们系统地研究了它们的去噪过程和强化学习（RL）方法。我们在130B个代码标记上训练了一个70亿的dLLM，命名为DiffuCoder。利用这个模型作为测试平台，我们分析了其解码行为，揭示了它与AR模型的不同之处：（1）dLLMs可以在不依赖半自回归解码的情况下决定其生成的因果性；（2）增加采样温度不仅多样化了标记选择，也多样化了它们的生成顺序。这种多样性为RL的 rollout 创建了一个丰富的搜索空间。对于RL训练，为了减少标记对数似然估计的方差并保持训练效率，我们提出了一个新颖的采样方案，称为\textbf{coupled-GRPO}，它为训练中使用的补全构建互补掩码噪声。在我们的实验中，coupled-GRPO显著提高了DiffuCoder在代码生成基准测试中的性能（在EvalPlus上提高了4.4%）并减少了解码过程中对AR偏差的依赖。我们的工作对dLLM生成机制有了更深入的洞察，并提供了一个有效、扩散本地的RL训练框架。https://github.com/apple/ml-diffucoder。|
|**2025-06-25**|**AI Assistants to Enhance and Exploit the PETSc Knowledge Base**|Barry Smith et.al.|[2506.20608](http://arxiv.org/abs/2506.20608)|null|生成式人工智能，尤其是通过大型语言模型（LLMs），正在改变技术知识可获取、重用和扩展的方式。PETSc是一个广泛应用于高性能科学计算的数值库，在三十年的发展过程中积累了丰富但碎片化的知识库，涵盖了源代码、文档、邮件列表、GitLab问题、Discord对话、技术论文等内容。其中大部分知识都保持着非正式状态，难以被用户和新开发者获取。为了更有效地激活和利用这一知识库，PETSc团队开始构建一个由LLM驱动的系统，该系统将PETSc内容与定制LLM工具相结合——包括检索增强生成（RAG）、重排序算法和聊天机器人——以协助用户、支持开发者并建议对正式文档的更新。本文介绍了设计和评估这些工具的初步经验，重点关注系统架构、针对PETSc特定信息使用RAG和重排序、不同LLMs和嵌入模型的评估方法以及用户界面设计。利用Argonne领导计算设施的资源，我们分析了LLM响应如何增强数值软件的开发和使用，初步关注可扩展的Krylov求解器。我们的目标是建立一个可扩展的科学软件知识中心AI框架，以实现可扩展的支持、丰富的文档和增强的研发工作流程。最后，我们概述了将此系统扩展为一个强大、不断发展的平台的方向，以推动软件生态系统的发展，加速科学发现。|
|**2025-06-25**|**Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm**|Baixiang Huang et.al.|[2506.20606](http://arxiv.org/abs/2506.20606)|null|基于大型语言模型（LLMs）的智能体在广泛的任务中展现出了强大的能力。然而，在高风险领域部署LLM智能体伴随着显著的安全和伦理风险。这些智能体的不道德行为可能导致严重的现实后果，包括身体伤害和财务损失。为了有效地引导智能体的道德行为，我们将智能体行为引导定义为一种模型编辑任务，我们称之为行为编辑。模型编辑是研究中的一个新兴领域，它能够在保持LLM整体能力的同时，实现对其精确和高效的修改。为了系统地研究和评估这种方法，我们引入了BehaviorBench，这是一个基于心理学道德理论的分层基准。这个基准支持对智能体在各种场景下的行为进行评估和编辑，每个层级都引入了更复杂和模糊的场景。我们首先证明，行为编辑可以在特定场景中动态引导智能体向目标行为转变。此外，行为编辑不仅允许对特定场景进行局部调整，还能在智能体的全球道德对齐上进行更广泛的转变。我们展示了行为编辑可以用来促进道德和慈善行为，或者相反，诱导有害或恶意行为。通过对前沿LLM智能体的全面评估，BehaviorBench展示了行为编辑在不同模型和场景中的有效性。我们的发现为引导智能体行为的新范式提供了关键见解，突出了行为编辑的承诺和风险。|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|Rui Huang et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|传统的3D场景合成需要专业知识以及大量的手动工作。自动化这一过程将极大促进建筑设计、机器人仿真、虚拟现实和游戏等领域的发展。近期3D场景合成的做法通常依赖于大型语言模型（LLMs）的常识推理或现代图像生成模型的强大视觉先验。然而，当前的大型语言模型在3D空间推理能力上表现出有限，这限制了它们生成真实且连贯的3D场景的能力。同时，基于图像生成的方法在视角选择和多视角一致性方面常常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一个新颖的框架，它利用视频生成模型中编码的3D物理世界的常识知识，以确保场景布局的连贯性和视角间物体放置的一致性。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以语义和几何方式分析场景中的每个物体。这使得场景合成具有高度的真实性和结构一致性。为了进行更精确的分析，我们进一步引入了第一人称视角评分（FPVScore）以评估连贯性和可信度，利用连续的第一人称视角来发挥多模态大型语言模型的推理能力。广泛的实验表明，VIPScene在现有方法中显著优于，并且能够很好地泛化到不同的场景中。代码将公开发布。|
|**2025-06-25**|**Large Language Model-Driven Code Compliance Checking in Building Information Modeling**|Soumya Madireddy et.al.|[2506.20551](http://arxiv.org/abs/2506.20551)|null|本研究通过引入大型语言模型（LLM）驱动的半自动化方法，解决了建筑信息模型（BIM）中手动代码合规性检查耗时且易出错的问题。所开发的系统将LLM如GPT、Claude、Gemini和Llama与Revit软件集成，以解释建筑规范、生成Python脚本，并在BIM环境中执行半自动合规性检查。针对单户住宅项目和办公楼项目的案例研究表明，该系统能够减少合规性检查所需的时间和精力，同时提高准确性。它通过自动评估关系并生成可执行报告，简化了对违规行为的识别，如不符合规范的房间尺寸、材料使用和对象放置。与手动方法相比，该系统消除了重复性任务，简化了复杂的法规，并确保了标准的一致性。通过提供全面、灵活且经济的解决方案，这种提出的方法在基于BIM的合规性检查方面具有潜在的进步，有望应用于建筑项目中各种监管文件。|
|**2025-06-25**|**When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs**|Ammar Khairi et.al.|[2506.20544](http://arxiv.org/abs/2506.20544)|null|近年来，大型语言模型（LLMs）的进步使得关注点转向了推理时计算规模的扩展，在不重新训练模型的情况下提高性能。一种常见的方法是并行采样多个输出，并从中选择一个作为最终输出。然而，迄今为止的研究主要集中在英语以及数学和代码等少数领域。相比之下，我们最感兴趣的是能够泛化到开放性任务、形式可验证任务以及跨语言的技术。在这项工作中，我们研究了如何在多语言、多任务环境下，稳健地扩展开放性生成任务的推理时计算规模。我们的研究发现，基于温度变化的采样策略和选择策略都必须适应不同的领域和多样的语言环境。我们评估了现有的选择方法，发现有效的英语策略往往无法跨语言泛化。我们提出了针对多语言和多任务推理场景的新型采样和选择策略，并展示了它们在语言和任务上的显著提升。特别是，我们的联合采样和选择方法使我们的8B模型在m-ArenaHard-v2.0提示上的胜率平均提高了+6.8，相对于Gemini等专有模型。在更大规模上，配备我们方法的Command-A（111B模型）在相同基准测试中仅通过五个样本就实现了+9.0的胜率提升，相对于单样本解码，这是一个成本极低的显著提升。我们的结果强调了在推理时计算中需要语言和任务感知的方法，旨在让代表性不足的语言的性能提升民主化。|
|**2025-06-25**|**WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads**|Hongzhen Huang et.al.|[2506.20535](http://arxiv.org/abs/2506.20535)|null|随着人工智能（AI）的快速发展，尤其是大型语言模型（LLMs）的迅速发展，人们对模型训练和推理过程中所涉及的能源消耗和碳排放问题产生了重大担忧。然而，现有的用于衡量和报告这些影响的工具通常较为零散，缺乏系统性的指标整合，并且对它们之间相关性的分析支持有限。本文介绍了一种名为WattsOnAI的综合软件工具包，用于测量、分析和可视化AI工作负载中的能源消耗、电力消耗、硬件性能和碳排放。通过无缝集成到现有的AI框架中，WattsOnAI提供标准化的报告，并以轻量级方式导出细粒度的时间序列数据，以支持基准测试和可重复性。它还进一步实现了硬件指标与模型性能之间的深入相关性分析，从而有助于瓶颈识别和性能提升。通过解决现有工具中的关键局限性，WattsOnAI鼓励研究界在评估AI工作负载的原始性能的同时，也要考虑其环境影响，并推动向更可持续的“绿色AI”实践转变。代码可在https://github.com/SusCom-Lab/WattsOnAI上获取。|
|**2025-06-24**|**JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning**|Ai Han et.al.|[2506.19846](http://arxiv.org/abs/2506.19846)|null|多智能体强化学习（MARL）已成为解决日益复杂任务的突出范式。然而，由于合作效率低下和训练不稳定，异构智能体之间的联合进化仍然具有挑战性。在本文中，我们提出了一种名为JoyAgents-R1的MARL联合进化动力学，它首先将群体相对策略优化（GRPO）应用于异构多智能体的联合训练。通过迭代优化智能体的大型语言模型（LLM）和记忆，该方法实现了整体均衡，具有最优决策和记忆能力。具体而言，JoyAgents-R1首先在每个智能体的整个推理轨迹上的行为上实施节点级蒙特卡洛采样，以提高GRPO采样效率同时保持策略多样性。然后，我们的边际效益驱动选择策略识别出具有最大奖励波动的顶部 $K$ 采样组，通过有针对性的智能体模型更新，提高训练稳定性并通过成本效益的参数调整最大化联合收益。同时，JoyAgents-R1引入了一种自适应记忆进化机制，将GRPO奖励作为免费监督信号重新用于消除重复推理并加速收敛。在通用和特定领域的场景中的实验表明，JoyAgents-R1在构建在较小的开源模型的基础上，实现了与更大LLM相当的性能。|
|**2025-06-24**|**MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration**|Yucheng Zhou et.al.|[2506.19835](http://arxiv.org/abs/2506.19835)|null|近期，在医疗大型语言模型（LLMs）方面的最新进展展示了它们强大的推理和诊断能力。尽管如此，当前的统一多模态医疗LLMs在知识更新成本、全面性和灵活性方面存在局限性。为了解决这些挑战，我们引入了用于多模态医疗诊断的模块化多智能体框架（MAM）。受我们实证研究结果启发，这些研究结果突出了在LLMs中分配角色和诊断区分的优势，MAM将医疗诊断过程分解为专业角色：全科医生、专家团队、放射科医生、医疗助理和主任，每个角色由一个基于LLM的智能体来体现。这种模块化和协作框架能够实现高效的知识更新，并利用现有的医疗LLMs和知识库。在广泛可公开访问的多模态医疗数据集上进行的广泛实验评估，包括文本、图像、音频和视频模态，表明MAM在性能上始终优于特定模态的LLMs。值得注意的是，与基线模型相比，MAM实现了从18%到365%的显著性能提升。我们的代码已发布在https://github.com/yczhou001/MAM。|
|**2025-06-24**|**Curating art exhibitions using machine learning**|Eurico Covas et.al.|[2506.19813](http://arxiv.org/abs/2506.19813)|null|艺术策展一直主要是由人类专家进行的具有主观性的工作，他们凭借对众多和多样的艺术作品的广泛知识，从中挑选出一些作品在公共空间中展出，这些空间演变成了我们今天所说的艺术画廊。在给定一个主题的情况下，无论是展示给艺术策展人还是由其构建的，都没有一套固定不变的规则来选择这些艺术作品。在这里，我们展示了一系列基于机器学习技术（人工智能的一个子集）的人工模型——总共四个相关模型——试图从由人类专家策划的现有展览中学习，以便能够执行类似的策展工作。由于可用数据的质量、我们研究的物理和时间限制，我们专注于纽约大都会艺术博物馆过去25年的展览。我们的四个人工智能模型在模仿所有这些展览的策展人方面达到了相当的能力，精度和策展连贯性各有不同。特别是，我们可以得出两个关键见解：首先，这些展览中存在足够的信息，可以构建一个能够以高于随机选择的准确度复制过去展览的人工智能模型；其次，通过特征工程和精心设计规模适中的模型架构，可以使它们在 brute force 方法中与使用所谓的大语言模型（如GPT）一样出色。我们还相信，基于在小样本实验中尝试使用这些模型，如果提供更多数据，这些类型的人工智能代理应该能够越来越接近人类艺术策展人的审美和策展判断。|
|**2025-06-24**|**KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality**|Baochang Ren et.al.|[2506.19807](http://arxiv.org/abs/2506.19807)|null|大型语言模型（LLMs），尤其是慢速思考模型，常常表现出严重的幻觉，由于在推理过程中无法准确识别知识边界，导致输出错误内容。虽然强化学习（RL）可以增强复杂的推理能力，但其以结果为导向的奖励机制通常缺乏对思考过程的事实监督，从而进一步加剧了幻觉问题。为了解决慢速思考模型中高频率的幻觉问题，我们提出了知识增强的强化学习（KnowRL）。KnowRL通过将基于知识验证的事实性奖励整合到RL训练过程中，引导模型进行基于事实的慢速思考，帮助它们识别自己的知识边界。在RL训练期间，这种有针对性的事实性输入使得模型能够学习和内化基于事实的推理策略。通过直接奖励推理步骤中对事实的遵守，KnowRL促进了一个更可靠的思考过程。在三个幻觉评估数据集和两个推理评估数据集上的实验结果表明，KnowRL有效地减轻了慢速思考模型中的幻觉，同时保持了它们原有的强大推理能力。我们的代码可在https://github.com/zjunlp/KnowRL上找到。|
|**2025-06-24**|**LLM-Based Social Simulations Require a Boundary**|Zengqing Wu et.al.|[2506.19806](http://arxiv.org/abs/2506.19806)|null|这篇立场论文认为，基于大型语言模型（LLM）的社会模拟应设定明确的界限，以便有意义地促进社会科学研究。与传统的基于代理的建模相比，LLM在模拟类人代理方面具有有希望的能力，但它们面临着基本限制，这限制了它们在发现社会模式方面的可靠性。核心问题在于LLM倾向于形成“平均人格”，缺乏足够的行为异质性，这是模拟复杂社会动态的关键要求。我们考察了三个关键的边界问题：对齐（模拟行为匹配现实世界模式）、一致性（在时间上保持一致的行为）和鲁棒性（在变化条件下可重现）。我们提出了启发式的边界，以确定基于LLM的模拟何时可以可靠地推进社会科学的理解。我们相信，当这些模拟专注于以下方面时更有价值：（1）集体模式而不是个体轨迹，（2）代理行为尽管方差有限，但仍与真实人口平均值对齐，（3）有适当的验证方法来测试模拟的鲁棒性。我们提供了一份实用清单，以指导研究人员确定基于LLM的社会模拟的适当范围和主张。|
|**2025-06-24**|**KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs**|Xin Fan Guo et.al.|[2506.19802](http://arxiv.org/abs/2506.19802)|null|尽管对基于机器学习的网络入侵检测系统（ML-NIDS）进行了广泛的研究，但其检测多种攻击变种的能力仍然不确定。先前的研究主要依赖于同质化数据集，这人为地提高了性能得分，并给人一种虚假的安全感。设计能够有效检测广泛攻击变种的系统仍然是一个重大挑战。ML-NIDS的进展仍然高度依赖于人类专业知识，这可能会将系统设计者的主观判断嵌入到模型中，从而阻碍其跨不同攻击类型进行泛化的能力。为了解决这一差距，我们提出了KnowML，这是一个知识引导的机器学习框架，它将攻击知识整合到ML-NIDS中。KnowML通过利用大型语言模型（LLMs）执行攻击实现的自动化分析，系统地探索威胁领域。它构建了一个攻击策略的统一知识图谱（KG），并在其上应用符号推理以生成KG增强输入，将领域知识直接嵌入到ML-NIDS的设计过程中。我们在28种现实攻击变种上评估了KnowML，其中10种是为本研究新收集的。我们的发现显示，基线ML-NIDS模型未能检测到几种变种，F1分数低至0%。相比之下，我们的知识引导方法实现了高达99%的F1分数，同时将误报率保持在0.1%以下。|
|**2025-06-24**|**Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study**|Yuqi Zhu et.al.|[2506.19794](http://arxiv.org/abs/2506.19794)|null|大型语言模型（LLMs）在自动化数据分析任务方面具有巨大潜力，然而开源模型在这些需要推理的场合面临着重大局限。在这项工作中，我们研究了提升开源LLMs数据分析能力的方法。通过精心构建一个多样化的、现实场景的种子数据集，我们从数据理解、代码生成和战略规划三个维度评估模型。我们的分析揭示了三个关键发现：（1）战略规划质量是模型性能的主要决定因素；（2）交互设计和任务复杂性显著影响推理能力；（3）数据质量对实现最佳性能的影响大于多样性。我们利用这些洞察力开发了一种数据合成方法，显著提升了开源LLMs的分析推理能力。|
|**2025-06-24**|**SAGE: Strategy-Adaptive Generation Engine for Query Rewriting**|Teng Wang et.al.|[2506.19783](http://arxiv.org/abs/2506.19783)|null|查询重写对于提升密集检索至关重要，但现有方法要么需要大规模的监督数据，要么遭受低效的强化学习（RL）探索。在这项工作中，我们首先证明了使用一组专家精心设计的策略，如语义扩展和实体消歧，可以显著提高在包括HotpotQA、FEVER、NFCorpus和SciFact在内的具有挑战性的基准测试中的检索效果。基于这一洞察，我们引入了策略自适应生成引擎（SAGE），该引擎在RL框架中将这些策略具体化。SAGE引入了两种新颖的奖励塑造机制——策略信用塑造（SCS）和对比奖励塑造（CRS），以提供更具信息量的学习信号。这种策略引导的方法不仅实现了新的NDCG@10最佳结果，而且还揭示了一种引人注目的涌现行为：代理学会选择最佳策略，减少不必要的探索，并生成简洁的重写，在不牺牲性能的情况下降低推理成本。我们的研究结果表明，策略引导的RL，结合精细的奖励塑造，为开发下一代稳健的信息检索系统提供了一个可扩展、高效且更具可解释性的范式。|
|**2025-06-24**|**SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning**|Yuqian Fu et.al.|[2506.19767](http://arxiv.org/abs/2506.19767)|null|大型语言模型（LLMs）在推理任务上取得了显著进步，然而，监督微调（SFT）和强化学习（RL）的最佳整合仍然是一个基本挑战。通过从熵的角度对标记分布、学习动态和整合机制进行综合分析，我们揭示了这些范式之间的关键差异：SFT导致LLM策略分布的粗粒度全局变化，而RL执行细粒度的选择性优化，熵作为训练有效性的关键指标。基于这些观察，我们提出了监督强化微调（SRFT），这是一种单阶段方法，通过熵感知加权机制统一了两种微调范式。我们的方法同时应用SFT和RL，直接使用演示和自我探索滚动来优化LLM，而不是通过两阶段顺序方法。大量实验表明，SRFT在五个数学推理基准测试中实现了59.1%的平均准确率，比零RL方法高出9.0%，在三个分布外基准测试中高出10.9%。|
|**2025-06-24**|**Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis**|Omar A. Essameldin et.al.|[2506.19753](http://arxiv.org/abs/2506.19753)|null|阿拉伯语是世界上最受欢迎的语言之一，有22个国家使用众多方言。本研究针对QADI数据集中18种阿拉伯方言的归类问题进行研究。创建了RNN模型、Transformer模型以及通过提示工程的大语言模型（LLMs）进行测试。在这些模型中，MARBERTv2表现最佳，准确率为65%，F1分数为64%。通过使用最先进的预处理技术和最新的NLP模型，本文确定了阿拉伯方言识别中最重要的语言问题。结果证实了个性化聊天机器人响应用户方言、社交媒体监控和为阿拉伯社区提供更多可访问性等应用。|
|**2025-06-23**|**Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations**|Jiaming Han et.al.|[2506.18898](http://arxiv.org/abs/2506.18898)|null|本文提出了一种多模态框架，旨在将视觉理解和生成统一到一个共享的离散语义表示中。其核心是文本对齐标记器（Text-Aligned Tokenizer，简称TA-Tok），它使用从大型语言模型（LLM）词汇表中投影的文本对齐代码簿将图像转换为离散标记。通过将视觉和文本整合到一个具有扩展词汇的统一空间中，我们的多模态LLM Tar通过共享接口实现跨模态输入和输出，无需针对特定模态进行设计。此外，我们提出了可伸缩的编码和解码方法来平衡效率和视觉细节，以及一个生成型反标记器以产生高保真度的视觉输出。为了满足多样化的解码需求，我们利用两种互补的反标记器：一个快速自回归模型和一个基于扩散的模型。为了增强模态融合，我们研究了高级预训练任务，证明了在视觉理解和生成方面的改进。在多个基准测试中的实验表明，Tar与现有多模态LLM方法相当或超越，实现了更快的收敛和更高的训练效率。代码、模型和数据可在https://tar.csuhan.com获取。|
|**2025-06-23**|**ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs**|Jiaru Zou et.al.|[2506.18896](http://arxiv.org/abs/2506.18896)|null|过程奖励模型（PRMs）最近成为监督大型语言模型（LLMs）中中间推理步骤的有力框架。之前的PRMs主要在模型最终输出响应上训练，难以稳健地评估中间思维轨迹，尤其是在由前沿推理模型如Deepseek-R1生成的轨迹-响应输出的新兴环境中。在这项工作中，我们引入了ReasonFlux-PRM，这是一种新颖的轨迹感知PRM，专门设计用来评估轨迹-响应类型的推理轨迹。ReasonFlux-PRM结合了步骤级和轨迹级监督，使得奖励分配与结构化的思维链数据对齐。我们调整ReasonFlux-PRM以支持离线和在线设置下的奖励监督，包括（i）选择高质量模型蒸馏数据用于下游较小模型的监督微调，（ii）在强化学习期间提供密集的过程级奖励以优化策略，以及（iii）实现奖励引导的N最佳测试时间缩放。在AIME、MATH500和GPQA-Diamond等具有挑战性的下游基准上的实证结果表明，ReasonFlux-PRM-7B选择的数据质量高于强大的PRMs（例如，Qwen2.5-Math-PRM-72B）和人工编纂的基线。此外，我们推导出的ReasonFlux-PRM-7B实现了持续的性能提升，在监督微调中平均提升12.1%，在强化学习中提升4.5%，在测试时间缩放中提升6.3%。我们还发布了高效的ReasonFlux-PRM-1.5B，用于资源受限的应用和边缘部署。项目：https://github.com/Gen-Verse/ReasonFlux|
|**2025-06-23**|**Universal Video Temporal Grounding with Generative Multi-modal Large Language Models**|Zeqian Li et.al.|[2506.18883](http://arxiv.org/abs/2506.18883)|null|本文提出了一种适用于通用视频时间定位的计算模型，能够根据自然语言查询（如问题或描述）精确地定位视频中的时间点。与现有方法相比，这些方法通常局限于特定的视频领域或持续时间，我们提出了UniTime，这是一种利用生成式多模态大型语言模型（MLLM）强大的视觉语言理解能力的鲁棒且通用的视频定位模型。我们的模型能够有效处理不同视角、类型和长度的视频，同时理解复杂的语言查询。主要贡献包括：（i）我们考虑将强大的MLLM用于视频时间定位。为了实现精确的时间戳输出，我们通过交错时间戳标记和视频标记来引入时间信息。（ii）通过通过自适应帧缩放训练模型处理不同输入粒度的视频，我们的方法实现了对短视频和长视频的鲁棒时间定位。（iii）全面实验表明，UniTime在五个公开时间定位基准测试中，无论是在零样本还是针对数据集特定微调设置下，都优于最先进的方法。（iv）当用作长篇视频问答（VideoQA）的初步时刻检索器时，UniTime显著提高了VideoQA的准确性，突显了其在复杂视频理解任务中的价值。|
|**2025-06-23**|**CommVQ: Commutative Vector Quantization for KV Cache Compression**|Junyan Li et.al.|[2506.18879](http://arxiv.org/abs/2506.18879)|null|大型语言模型（LLMs）在需要长上下文长度的应用中越来越受欢迎，但随着上下文的增长，键值（KV）缓存通常会成为GPU上的内存瓶颈。为了解决这个问题，我们提出了交换向量量化（CommVQ）方法，以显著减少长上下文LLM推理的内存使用。我们首先引入了带有轻量级编码器和代码本的加性量化来压缩KV缓存，该缓存可以通过简单的矩阵乘法进行解码。为了进一步降低解码过程中的计算成本，我们设计了与旋转位置嵌入（RoPE）可交换的代码本，并使用期望最大化（EM）算法进行训练。这使得解码能够高效地集成到自注意力机制中。我们的方法通过加性量化实现了高精度，并通过RoPE可交换代码本实现了低开销。在长上下文基准测试和GSM8K上的实验表明，我们的方法在2位量化下将FP16 KV缓存大小减少了87.5%，同时优于最先进的KV缓存量化方法。值得注意的是，它实现了仅损失最小精度的1位KV缓存量化，使得一个LLaMA-3.1 8B模型可以在单个RTX 4090 GPU上以128K上下文长度运行。源代码可在以下网址获取：https://github.com/UMass-Embodied-AGI/CommVQ。|
|**2025-06-23**|**TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting**|Zhongbin Guo et.al.|[2506.18862](http://arxiv.org/abs/2506.18862)|null|卫星图像时间序列分析需要精细的空间时间推理，这对于现有的多模态大型语言模型（MLLMs）来说仍然是一个挑战。在这项工作中，我们研究了MLLMs在一种新颖任务上的能力，该任务同时针对时间变化理解和未来场景生成，旨在评估它们在建模复杂多模态动态随时间变化的潜力。我们提出了TAMMs，即用于卫星图像变化理解和预测的时态感知多模态模型，它通过轻量级时间模块增强冻结的MLLMs，以进行结构化序列编码和上下文提示。为了指导未来图像生成，TAMMs引入了一种语义融合控制注入（SFCI）机制，该机制在增强的ControlNet内自适应地结合高级语义推理和结构先验。这种双路径条件化使得图像合成在时间上保持一致且在语义上有所依据。实验表明，TAMMs在时间变化理解和未来图像预测任务中都优于强大的MLLM基线，突显了精心设计的时间推理和语义融合如何释放MLLMs在时空理解方面的全部潜力。|
|**2025-06-23**|**LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**|Yuhao Wu et.al.|[2506.18841](http://arxiv.org/abs/2506.18841)|null|超长文本生成对于大型语言模型（LLMs）来说是一个广泛需求的场景，但由于其最大生成长度限制和序列长度增加导致的整体质量下降，这仍然是一个重大挑战。先前的方法，以LongWriter为例，通常依赖于“教学”，这涉及在合成长文本输出上进行监督微调（SFT）。然而，这种策略严重依赖于难以构建且成本高昂的合成SFT数据，这些数据往往缺乏连贯性和一致性，且往往过于人工化和结构单调。在本工作中，我们提出了一种基于激励的方法，该方法完全从头开始，不依赖于任何标注或合成数据，利用强化学习（RL）来培养LLMs中产生超长、高质量文本的能力。我们从基础模型开始进行RL训练，类似于R1-Zero，引导它进行推理，以促进写作过程中的计划和细化。为此，我们采用了专门的奖励模型，引导LLM实现更好的长度控制、写作质量和结构格式化。实验评估表明，我们基于Qwen2.5-32B训练的LongWriter-Zero模型在长文本写作任务上始终优于传统的SFT方法，在WritingBench和Arena-Write的所有指标上均实现了最先进的成果，甚至超越了DeepSeek R1和Qwen3-235B等100B+模型。我们已在https://huggingface.co/THU-KEG/LongWriter-Zero-32B上开源我们的数据和模型检查点。|
|**2025-06-23**|**STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning**|Aryasomayajula Ram Bharadwaj et.al.|[2506.18831](http://arxiv.org/abs/2506.18831)|null|大型语言模型在采用扩展的思考链（CoT）推理时，常常会出现过度思考的现象，生成过多的冗余推理步骤，这增加了计算成本，并可能降低性能。尽管最近的研究探索了静态引导方法来缓解这个问题，但它们缺乏根据实时推理质量动态调整干预强度的适应性。我们提出了STUPID（通过PID控制器进行引导令牌使用），这是一种新颖的无需训练的方法，它在推理过程中使用PID控制器动态调节激活引导强度。我们的方法结合了一个块级分类器，用于检测冗余推理模式，以及一个PID控制机制，该机制根据预测的冗余概率自适应地调整引导强度。在GSM8K上的实验评估表明，STUPID实现了6%的准确率提升，同时减少了32%的令牌使用，优于静态引导基线。我们的方法提供了一个动态推理校准的原理框架，在保持推理质量的同时，显著提高了计算效率。|
|**2025-06-23**|**Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories**|Islem Bouzenia et.al.|[2506.18824](http://arxiv.org/abs/2506.18824)|null|大型语言模型（LLM）驱动的代理正越来越多地用于自动化复杂的软件工程任务，如程序修复和问题解决。这些代理通过自主生成自然语言思维、调用外部工具和迭代优化解决方案来运行。尽管它们得到了广泛的应用，但这些代理的内部决策过程仍鲜有研究，限制了我们对它们操作动态和失败模式的了解。在本文中，我们展示了三个最先进的LLM驱动代理：\textsc{RepairAgent}、\textsc{AutoCodeRover}和\textsc{OpenHands}的思维-行动-结果轨迹的大规模实证研究。我们将它们的交互日志统一为一种通用格式，捕捉了120个轨迹和2822个LLM交互，重点关注程序修复和问题解决。我们的研究结合了对结构属性、动作模式和标记使用量的定量分析，以及对推理连贯性和反馈整合的定性评估。我们确定了关键轨迹特征，如迭代次数和标记消耗、重复的动作序列以及连接思维、行动及其结果的语义连贯性。我们的发现揭示了区分成功执行与失败执行的行为模式和反模式，为改进代理设计提供了可操作的见解，包括提示策略、故障诊断和反模式检测。我们发布了我们的数据集和标注框架，以支持对透明和稳健的自主软件工程代理的进一步研究。|
|**2025-06-23**|**RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies**|Arjun Mukerji et.al.|[2506.18819](http://arxiv.org/abs/2506.18819)|null|大型语言模型（LLMs）在通用摘要任务以及医学研究辅助方面得到了广泛评估，但它们并未针对从RWE研究结构化输出中总结现实世界证据（RWE）的任务进行过专门评估。我们介绍了RWESummary，这是MedHELM框架（Bedi，Cui，Fuentes，Unell等人，2025）的一个提议性补充，旨在为这项任务提供LLMs的基准评估。RWESummary包括一个场景和三个评估，涵盖了在医学研究摘要中观察到的主要类型错误，并且使用Atropos Health专有数据进行开发。此外，我们利用RWESummary比较了不同LLMs在我们内部RWE摘要工具中的性能。在发表时，使用13个不同的RWE研究，我们发现Gemini 2.5模型（包括Flash和Pro）整体表现最佳。我们建议RWESummary作为现实世界证据研究摘要的一个新颖且有用的基础模型基准。|
|**2025-06-23**|**Context-Aware CodeLLM Eviction for AI-assisted Coding**|Kishanthan Thangarajah et.al.|[2506.18796](http://arxiv.org/abs/2506.18796)|null|本文介绍了CACE，这是一种针对资源受限环境下自托管CodeLLM服务优化的新颖的上下文感知模型驱逐策略。与仅基于最近使用（例如，最近最少使用）的传统驱逐策略不同，CACE利用多个上下文感知因素，包括模型加载时间、特定任务对延迟的敏感性、预期输出长度以及通过滑动窗口跟踪的近期使用和未来需求。我们使用包括对延迟敏感的代码补全和吞吐量密集型代码推理任务在内的真实工作负载对CACE进行了评估。实验结果表明，与最先进的系统相比，CACE降低了首次标记时间（TTFT）和端到端（E2E）延迟，同时显著降低了模型驱逐的数量。消融研究进一步证明了多因素驱逐在平衡响应性和资源效率方面的重要性。这项工作为在现实世界的软件工程环境中部署可扩展、低延迟的AI编码助手提供了实用的策略。|
|**2025-06-20**|**VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning**|Zhangyang Qi et.al.|[2506.17221](http://arxiv.org/abs/2506.17221)|null|视觉语言导航（VLN）是具身人工智能的核心挑战，要求代理使用自然语言指令在现实世界环境中进行导航。当前基于语言模型的导航系统在离散拓扑图上运行，限制了路径规划到预定义的节点连接。我们提出了VLN-R1，一个端到端框架，该框架利用大型视觉语言模型（LVLM）直接将自航视频流转换为连续的导航动作，采用受DeepSeek-R1启发的GRPO-based训练。为了实现有效的训练，我们首先使用3D模拟器Habitat构建了VLN-Ego数据集，并提出了长短期记忆采样来平衡历史和当前观察。虽然大型语言模型可以监督完整的文本指令，但它们缺乏精细的动作级别控制。我们的框架采用两阶段训练方法：a）监督微调（SFT），将模型的动作序列文本预测与专家演示对齐，随后进行b）增强时间衰减奖励（TDR）机制的强化微调（RFT），该机制战略性地加权多步未来动作。实验结果表明，VLN-R1在VLN-CE基准测试中取得了强大的性能。VLN-R1证明了LVLM可以驱动具身导航，并通过数据高效、奖励驱动的后训练增强特定任务的推理。|
|**2025-06-20**|**No Free Lunch: Rethinking Internal Feedback for LLM Reasoning**|Yanzhi Zhang et.al.|[2506.17219](http://arxiv.org/abs/2506.17219)|null|强化学习已成为提升大型语言模型（LLMs）推理能力的一种强大范式。像基于人类反馈的强化学习（RLHF）和基于可验证奖励的强化学习（RLVR）等方法已显示出显著的效果，但它们需要大量的外部监督。我们研究了另一类方法，即从内部反馈进行强化学习（RLIF），它完全依赖于模型内部产生的信号而非外部奖励。具体来说，我们利用了无监督奖励代理，如令牌级别的熵、轨迹级别的熵和自我确定性。我们的理论分析表明，这些内部目标在某种程度上是等价的，我们在具有挑战性的数学推理基准上对各种RLIF策略进行了实证评估。实验结果表明，RLIF可以在训练的初期阶段提升基LLMs的推理性能，在这些任务上与RLVR技术相当甚至超越。然而，随着训练的进行，性能甚至下降到训练前的模型水平以下。此外，我们发现RLIF对指令微调模型的影响很小，这表明一旦LLM已经经过指令微调，内在反馈的回报就会减少。我们进一步通过混合模型权重分析了这一局限性，并解释了RLIF训练行为的原因，为将内部反馈信号集成到LLM训练中提供了实用指南。我们希望我们对内部反馈的分析能为LLM后训练提供更原则性和有效的策略。|
|**2025-06-20**|**Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency**|Kathleen C. Fraser et.al.|[2506.17209](http://arxiv.org/abs/2506.17209)|null|将这篇论文摘要翻译为中文，以下是逐步推理的中文翻译结果：  1. Fine-tuning a general-purpose large language model (LLM) for a specific domain or task has become a routine procedure for ordinary users.    - 将通用大型语言模型（LLM）微调以适应特定领域或任务已成为普通用户的常规程序。  2. However, fine-tuning is known to remove the safety alignment features of the model, even when the fine-tuning data does not contain any harmful content.    - 然而，众所周知，微调会移除模型的安全对齐功能，即使微调数据中不包含任何有害内容。  3. We consider this to be a critical failure mode of LLMs due to the widespread uptake of fine-tuning, combined with the benign nature of the "attack".    - 我们认为，由于微调的广泛应用以及这种“攻击”的良性本质，这是LLM的一个关键故障模式。  4. Most well-intentioned developers are likely unaware that they are deploying an LLM with reduced safety.    - 大多数有良好意愿的开发者可能没有意识到他们正在部署一个安全性降低的LLM。  5. On the other hand, this known vulnerability can be easily exploited by malicious actors intending to bypass safety guardrails.    - 另一方面，这种已知的漏洞可以很容易地被恶意行为者利用，以绕过安全防护措施。  6. To make any meaningful progress in mitigating this issue, we first need reliable and reproducible safety evaluations.    - 要在缓解这个问题上取得任何有意义的进展，我们首先需要可靠且可复现的安全评估。  7. In this work, we investigate how robust a safety benchmark is to trivial variations in the experimental procedure, and the stochastic nature of LLMs.    - 在这项工作中，我们研究了安全基准对实验程序中微不足道的变化的鲁棒性，以及LLMs的随机性质。  8. Our initial experiments expose surprising variance in the results of the safety evaluation, even when seemingly inconsequential changes are made to the fine-tuning setup.    - 我们的初步实验揭示了安全评估结果中令人惊讶的变异性，即使在微调设置中做出了看似无关紧要的更改。  9. Our observations have serious implications for how researchers in this field should report results to enable meaningful comparisons in the future.    - 我们的观察对这一领域的科研人员如何报告结果以在未来进行有意义的比较具有重大影响。|
|**2025-06-20**|**Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems**|Matias Martinez et.al.|[2506.17208](http://arxiv.org/abs/2506.17208)|null|自动化程序修复（APR）的快速发展是由人工智能（AI）的进步驱动的，特别是大型语言模型（LLMs）和基于代理的系统。SWE-Bench 是一个旨在使用从 12 个流行的开源 Python 仓库中挖掘的真实问题和拉取请求来评估基于 LLM 的修复系统的最新基准。其公开排行榜 SWE-Bench Lite 和 SWE-Bench Verified 已经成为跟踪进度和比较解决方案的中心平台。然而，由于提交过程不需要详细文档，许多解决方案的架构设计和来源仍然不明确。在本文中，我们介绍了对 SWE-Bench Lite（68 个条目）和 SWE-Bench Verified（79 个条目）排行榜上所有提交的首次全面研究，分析了 67 种独特的方案，涵盖提交者类型、产品可用性、LLM 使用和系统架构等多个维度。我们的发现揭示了专有 LLM（尤其是 Claude 3.5/3.7）的统治地位、既有代理又无代理的设计的存在，以及从个人开发者到大型科技公司的贡献者基础。|
|**2025-06-20**|**Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction**|Jiekai Ma et.al.|[2506.17203](http://arxiv.org/abs/2506.17203)|null|大型语言模型（LLMs）最近使得自然语言界面能够将用户查询翻译成可执行的SQL，为非技术利益相关者访问结构化数据提供了强大的解决方案。然而，LLMs无法原生表达不确定性的一个局限性使得评估其生成的查询的可靠性变得困难。本文提出了一项案例研究，评估了多种用于估计LLM生成的SQL在供应链数据检索中置信度分数的方法。我们研究了三种策略：（1）基于翻译的一致性检查；（2）用户问题和生成的SQL之间的基于嵌入的语义相似度；（3）LLM直接产生的自我报告置信度分数。我们的发现表明，LLMs对其自身输出的信心往往过高，这限制了自我报告置信度的有效性。相比之下，基于嵌入的相似度方法在识别不准确SQL方面显示出强大的判别能力。|
|**2025-06-20**|**Detecting LLM-Generated Short Answers and Effects on Learner Performance**|Shambhavi Bhushan et.al.|[2506.17196](http://arxiv.org/abs/2506.17196)|**[link](https://github.com/shambhavib20/ai-detection)**|随着大型语言模型（LLMs）的日益普及，其在在线学习中的潜在误用问题引起了关注。虽然存在检测LLMs生成文本的工具，并被研究人员和教育工作者广泛使用，但其可靠性各不相同。很少有研究比较检测方法的准确性，定义识别LLMs生成内容的准则，或评估LLMs误用对学习者表现的影响。在本研究中，我们将开放性回答中由任何LLMs生成且未经改写或精炼的文本定义为LLMs生成的文本，该定义由人类编码者进行评估。然后，我们对GPT-4o进行微调以检测LLMs生成的回答，并评估LLMs误用对学习的影响。我们发现，我们的微调LLMs在检测LLMs生成的回答方面优于现有的AI检测工具GPTZero，准确率达到80%，F1分数为0.78，而GPTZero的准确率为70%，宏F1分数为0.50，显示出在检测LLMs生成的回答方面的优越性能。我们还发现，在开放性回答问题中疑似使用LLMs的学生正确回答相应测试题的可能性是其他学生的两倍以上，这表明LLMs的潜在误用存在于两种题型中，并可能绕过了学习过程。我们通过展示一种结构化、基于代码的方法来改进LLMs生成的回答检测，并建议使用辅助的统计指标，如相关任务上异常高的评估分数、可读性分数和回答持续时间。为了支持开放科学，我们贡献了数据和代码，以支持类似模型在类似用例中的微调。|
|**2025-06-20**|**The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making**|Abinitha Gourabathina et.al.|[2506.17163](http://arxiv.org/abs/2506.17163)|null|在将上述论文摘要翻译为中文之前，我将逐步解析其内容：  1. **Clinical robustness**：指的是临床鲁棒性，即模型在面对临床环境中的实际变化时的稳定性和可靠性。 2. **Medical Large Language Models (LLMs)**：医学大型语言模型，是能够处理和理解医学领域文本的模型。 3. **Real-world variability**：现实世界的多样性，特指临床环境中的变化。 4. **MedPerturb**：一个用于评估医学LLMs的数据库，通过控制临床输入的扰动来评估模型。 5. **Clinical vignettes**：临床病例摘要，用于测试模型。 6. **Pathologies**：病理学，此处指疾病种类。 7. **Perturbations**：扰动，指对输入数据进行的微小变化。 8. **Gender modifications**：性别修改，如性别互换或性别去除。 9. **Style variation**：风格变化，如不确定的表达或口语化语气。 10. **Format changes**：格式变化，如LLM生成的多轮对话或摘要。 11. **Human expert reads**：人类专家阅读，指由人类专家对每个临床案例的评估。 12. **Case studies**：案例研究，用于分析模型和人类专家的差异。 13. **Treatment selections**：治疗方案选择，指对疾病的治疗方法。 14. **Evaluation frameworks**：评估框架，用于衡量模型性能的指标。  基于以上解析，以下是摘要的中文翻译：  临床鲁棒性对于医学大型语言模型（LLMs）的安全部署至关重要，但关于LLMs与人类在应对临床环境中典型的现实世界多样性方面的差异，仍存在关键问题。为了解决这个问题，我们引入了MedPerturb，这是一个旨在在控制的临床输入扰动下系统地评估医学LLMs的数据集。MedPerturb包含了一系列涵盖各种病理的临床病例摘要，每个摘要沿着三个轴进行了转换：（1）性别修改（例如，性别互换或性别去除）；（2）风格变化（例如，不确定的表达或口语化语气）；以及（3）格式变化（例如，LLM生成的多轮对话或摘要）。通过MedPerturb，我们发布了一个包含800个基于现实输入多样性的临床情境数据集，每个临床情境包含四个LLMs的输出和三个人类专家的评估。我们利用MedPerturb在两个案例研究中揭示了性别身份提示、语言风格或格式的变化如何反映人类和LLMs之间不同的治疗方案选择。我们发现LLMs对性别和风格扰动更为敏感，而人类注释员对LLM生成的格式扰动（如临床摘要）更为敏感。我们的结果强调了需要超越静态基准的评估框架，以评估人类临床医生和LLMs在临床环境特征下的决策相似性。|
|**2025-06-20**|**Do We Need Large VLMs for Spotting Soccer Actions?**|Ritabrata Chakraborty et.al.|[2506.17144](http://arxiv.org/abs/2506.17144)|null|传统的基于视频的任务，如足球动作识别，高度依赖视觉输入，通常需要复杂且计算量大的模型来处理密集的视频数据。在这项工作中，我们提出从以视频为中心的方法转向基于文本的任务，通过利用大型语言模型（LLMs）而非视觉语言模型（VLMs）来实现轻量化和可扩展性。我们认为，专家评论，提供了丰富、细粒度的描述和上下文线索，如兴奋和战术洞察，包含足够的信息来可靠地识别比赛中的关键动作。为了证明这一点，我们使用了提供时间戳评论的SoccerNet Echoes数据集，并采用了一个由三个LLMs组成的系统，这些LLMs分别擅长结果、兴奋和战术。每个LLM评估评论的滑动窗口，以识别进球、黄牌和换人等动作，为这些事件生成准确的时刻。我们的实验表明，这种以语言为中心的方法在检测关键比赛事件方面表现有效，为动作识别提供了一种轻量级且无需训练的替代方案，相较于传统的基于视频的方法。|
|**2025-06-20**|**Large Language Model Unlearning for Source Code**|Xue Jiang et.al.|[2506.17125](http://arxiv.org/abs/2506.17125)|null|LLM4SE取得了显著的成功，但LLM可能对敏感或过时的训练数据进行记忆，这给法律合规、软件安全和代码质量带来了关键风险。LLM遗忘技术，可以在训练后消除LLM中不希望的数据的影响，为解决这些问题提供了一个有希望的解决方案。虽然近期在LLM遗忘方面的努力在自然语言处理中显示出有效性，但它们在源代码中的应用仍被低估。我们的实证研究表明，现有的LLM遗忘方法在应用于源代码时会导致模型效用严重下降，使得模型在实际代码生成中几乎无法使用。在本文中，我们提出了PROD，这是一种新颖的遗忘方法，它使LLM能够忘记不希望的内容，同时有效地保留其代码生成能力。PROD抑制了LLM输出分布中遗忘数据的概率，同时促进候选分布组件，使模型能够联合学习忘记特定内容并保留其一般能力。为了便于这项研究，我们建立了一个代码遗忘评估基准，包括三个关键下游任务：版权代码遗忘、不安全代码遗忘和已弃用API遗忘。我们的评估表明，与现有的遗忘方法相比，PROD在三个下游任务中实现了遗忘质量和模型效用之间的优越平衡，并且在应用于不同系列的LLM时始终表现出改进。PROD还表现出对对抗攻击的优越鲁棒性，而无需生成或暴露要遗忘的数据。这些结果强调了我们的方法不仅将遗忘技术的应用边界扩展到源代码，而且对推进可靠的代码生成具有重大意义。|
|**2025-06-20**|**When Can Model-Free Reinforcement Learning be Enough for Thinking?**|Josiah P. Hanna et.al.|[2506.17124](http://arxiv.org/abs/2506.17124)|null|近期关于大型语言模型的研究表明，无模型强化学习（RL）可以用于训练类似推理的能力。通过无模型RL实现的“思考”现象很有趣，因为思考动作既不产生奖励，也不改变外部世界状态，使智能体更有可能获得奖励。本文旨在建立一个领域无关的理解，即无模型RL何时会以奖励最大化为策略导致“思考”。为了构建这种理解，我们首先介绍了一个理论模型，我们称之为“思维马尔可夫决策过程”（MDP）。思维MDP将经典MDP模型最小化扩展，包括抽象的思维状态和思维动作的概念。使用思维MDP模型，我们证明了策略初始化在决定是否出现思考中的重要性，并正式表明思维动作等同于智能体在继续行动之前选择执行策略改进步骤。然后，我们表明开源LLMs满足我们理论预测的必要条件，以使无模型RL产生类似思考的行为。最后，我们提出了使思维能够在语言生成之外学习到的充分条件，并引入了一个玩具领域，其中多任务预训练和指定的思维动作的组合与非思考智能体相比，能够实现更高效的数据高效RL。|
|**2025-06-18**|**PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning**|Yuhui Shi et.al.|[2506.15683](http://arxiv.org/abs/2506.15683)|null|随着大型语言模型（LLMs）的普及，如虚假信息生产和学术不端等不良社会问题变得更加严重，使得检测LLM生成的文本现在变得前所未有的重要。尽管现有方法已经取得了显著进展，但由私有调优LLM文本带来的新挑战仍未得到充分探索。用户可以通过使用私有语料库微调开源模型轻松获得私有LLMs，导致现有检测器在实际应用中的性能显著下降。为了解决这个问题，我们提出了PhantomHunter，这是一个专门用于检测未见过的、私有调优LLM文本的LLM生成文本检测器。其家族感知学习框架捕捉了跨越基础模型及其衍生品的家族级特征，而不是记忆个体特征。在LLaMA、Gemma和Mistral家族的数据上的实验表明，它在7个基线和3个工业服务上表现出优越性，F1得分超过96%。|
|**2025-06-18**|**GenRecal: Generation after Recalibration from Large to Small Vision-Language Models**|Byung-Kwan Lee et.al.|[2506.15681](http://arxiv.org/abs/2506.15681)|null|近年来，视觉语言模型（VLMs）的进步利用了大型语言模型（LLMs）实现了与GPT-4V等闭源系统相当的性能。然而，由于这些模型在计算需求上的巨大，将这些模型部署在现实场景中，尤其是在资源受限的设备上，仍然具有挑战性。这促使人们将知识从大型VLMs提炼到更小、更高效的对应模型中。这里的一个关键挑战来自于VLM架构的多样性，这些架构建立在不同的LLMs之上，并采用不同的标记类型——在词汇量大小、标记分割和标记索引排序上存在差异。为了解决这一限制于特定VLM类型的挑战，我们提出了“重校准后生成”（GenRecal），这是一种针对VLMs的全新、通用的蒸馏框架。GenRecal集成了校正器，它可以在异构VLM之间对齐和调整特征表示，从而实现不同类型VLM之间的有效知识迁移。通过在多个具有挑战性的基准上进行广泛的实验，我们证明了GenRecal显著提高了基线性能，并最终超越了大规模的开源和闭源VLMs。|
|**2025-06-18**|**SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence**|Yao Zhang et.al.|[2506.15672](http://arxiv.org/abs/2506.15672)|null|大型语言模型的快速发展推动了决策、协调和任务执行中的代理系统。然而，现有的代理系统生成框架缺乏完全的自主性，缺乏从头开始的代理生成、自我优化的代理功能以及协作，限制了适应性和可扩展性。我们提出了SwarmAgentic，这是一个用于完全自动化代理系统生成的框架，它从头开始构建代理系统，并通过语言驱动的探索共同优化代理功能和协作，将其视为相互依赖的组件。为了在系统级结构上实现高效的搜索，SwarmAgentic维护一个候选系统群体，并通过反馈引导的更新来进化它们，从中汲取粒子群优化（PSO）的灵感。我们在涉及高级规划、系统级协调和创造性推理的六个现实世界、开放式和探索性任务上评估了我们的方法。给定仅任务描述和目标函数，SwarmAgentic优于所有基线，在TravelPlanner基准测试中相对于ADAS实现了+261.8%的相对改进，突出了在结构上不受约束的任务中完全自动化的有效性。这个框架标志着向可扩展和自主代理系统设计迈出的重要一步，将群体智能与完全自动化的系统多代理生成相结合。我们的代码已公开发布在https://yaoz720.github.io/SwarmAgentic/。|
|**2025-06-18**|**CC-LEARN: Cohort-based Consistency Learning**|Xiao Ye et.al.|[2506.15662](http://arxiv.org/abs/2506.15662)|null|大型语言模型在许多任务上表现出色，但仍然在一致的、稳健的推理上存在困难。我们引入了基于群体的一致性学习（CC-Learn），这是一个强化学习框架，通过在从共享的程序抽象中派生出的相似问题的群体上训练，提高了LLM推理的可靠性。为了强制执行群体级别的连贯性，我们定义了一个组合目标，该目标结合了群体准确率、有效问题分解的检索奖金以及强化学习可以直接优化的拒绝惩罚，这与监督微调不同。优化这个奖励引导模型在所有群体成员中采用统一的推理模式。在具有挑战性的推理基准测试（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn在预训练和SFT基线之上提升了准确率和推理稳定性。这些结果表明，基于群体的强化学习有效地增强了LLM中的推理一致性。|
|**2025-06-18**|**PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection**|Wenhao Li et.al.|[2506.15656](http://arxiv.org/abs/2506.15656)|null|钓鱼网站持续构成重大网络安全威胁，通常利用欺骗性结构、品牌模仿和社会工程学策略来规避检测。尽管最近大型语言模型（LLMs）的进步使得通过上下文理解提高了钓鱼检测，但大多数现有方法依赖于单代理分类，面临着幻觉风险，且缺乏可解释性和鲁棒性。为了解决这些局限性，我们提出了PhishDebate，这是一个基于LLM的模块化多代理辩论框架，用于钓鱼网站检测。PhishDebate采用四个专业代理独立分析网页的不同文本方面——URL结构、HTML组成、语义内容和品牌模仿——在调解员和最终法官的协调下进行。通过结构化辩论和发散性思维，该框架提供了更准确和可解释的决策。在商业LLMs上的广泛评估表明，PhishDebate在真实世界钓鱼数据集上实现了98.2%的召回率和98.2%的真正阳性率（TPR），并优于单代理和思维链（CoT）基线。此外，其模块化设计允许代理级别的可配置性，能够适应不同的资源和应用需求。|
|**2025-06-18**|**deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses**|Georgios Androutsopoulos et.al.|[2506.15648](http://arxiv.org/abs/2506.15648)|null|尽管Rust默认确保内存安全，但它也允许使用不安全代码，如果滥用，可能会导致内存安全问题。遗憾的是，现有用于检测Rust中内存错误的工具通常检测能力有限，不足以处理Rust特定的类型，或者严重依赖人工干预。为了解决这些限制，我们提出了deepSURF，这是一个将静态分析与LLM（大型语言模型）引导的模糊测试工具生成器集成的工具，能够有效地识别Rust库中的内存安全问题，特别针对不安全代码。deepSURF引入了一种处理泛型的新方法，通过用自定义类型替换它们，并为所需的特质生成定制实现，使模糊测试器能够在模糊测试的库中模拟用户定义的行为。此外，deepSURF使用LLM动态增强模糊测试工具，便于探索复杂的API交互，显著提高暴露内存安全漏洞的可能性。我们对27个现实世界的Rust包进行了评估，成功重新发现了20个已知的内存安全错误，并发现了6个以前未知的漏洞，这显示了与现有工具相比的明显改进。|
|**2025-06-18**|**Demystifying the Visual Quality Paradox in Multimodal Large Language Models**|Shuo Xing et.al.|[2506.15645](http://arxiv.org/abs/2506.15645)|null|近期，多模态大型语言模型（MLLMs）在基准视觉-语言任务上表现出色，然而关于输入视觉质量如何影响其响应的了解却很少。图像的更高感知质量是否已经转化为MLLM更好的理解？我们进行了第一个涵盖领先MLLMs和一系列视觉-语言基准的系统研究，对每张图像应用了受控的降级和风格转换。令人惊讶的是，我们发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至单个实例的性能都可能提高。现成的恢复管道无法调和这些独特的偏好。为了填补这一差距，我们引入了视觉质量测试时调整（VQ-TTT）——一个轻量级的适应模块，它：（1）在冻结的视觉编码器之前插入一个可学习的低秩核来调节频率内容；（2）通过LoRA仅微调浅层视觉编码器层。VQ-TTT在单次前向传递中动态调整每个输入图像，使其与特定任务的模型偏好对齐。在评估的所有MLLMs和所有数据集中，VQ-TTT显著提高了平均准确率，无需外部模型、缓存特征或额外训练数据。这些发现重新定义了MLLMs的“更好”视觉输入，并突出了在新一代AI成为主要数据客户的时代，需要适应性而非普遍“干净”图像的需求。|
|**2025-06-18**|**Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability**|Yusuke Sakai et.al.|[2506.15629](http://arxiv.org/abs/2506.15629)|null|在类似于CommonGen的生成常识推理任务中，生成式大型语言模型（LLMs）会构建包含所有给定概念的句子。然而，当关注指令遵循能力时，如果提示指定了概念顺序，LLMs必须生成遵循指定顺序的句子。为了解决这个问题，我们提出了有序CommonGen，这是一个旨在评估LLMs的组合泛化能力和指令遵循能力的基准。这个基准通过测量有序覆盖率来评估概念是否按照指定顺序生成，从而能够同时评估这两种能力。我们使用36个LLMs进行了全面分析，发现虽然LLMs通常能理解指令的意图，但倾向于特定概念顺序模式的偏差往往会导致输出多样性低或即使改变概念顺序也会产生相同的结果。此外，即使是最遵守指令的LLMs也只实现了大约75%的有序覆盖率，这突显了在指令遵循和组合泛化能力方面都需要改进。|
|**2025-06-18**|**The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games**|Lyle Goodyear et.al.|[2506.15624](http://arxiv.org/abs/2506.15624)|null|大型语言模型（LLMs）在动态环境中作为决策者展现出潜力，但它们无状态的本质需要创建历史事件的自然语言表示。我们提出一个统一的框架，用于系统地构建用于提示LLM代理在重复多智能体游戏中的自然语言“状态”表示。先前关于具有LLM代理的游戏的研究采用了临时编码游戏历史的方法，这不仅掩盖了状态表示对代理行为的影响，还限制了研究之间的可比性。我们的框架通过沿三个轴来描述状态表示的方法来填补这些差距：动作信息量（即状态表示捕捉到动作的程度）；奖励信息量（即状态表示描述获得的奖励的程度）；提示风格（或自然语言压缩，即总结完整文本历史到何种程度）。我们将此框架应用于动态自私路由游戏，选择该游戏是因为它在理论和人类受试者实验中都具有简单的均衡 \cite{rapoport_choice_2009}。尽管游戏相对简单，但我们发现LLM代理的行为对自然语言状态表示有关键依赖。特别是，我们发现提供以下内容的表示会导致与博弈论均衡预测更接近的行为，以及代理更稳定的游戏玩法：（1）总结历史，而不是完整的自然语言表示；（2）关于遗憾的信息，而不是原始收益；（3）关于他人行为的有限信息。相比之下，其他表示可能会表现出与均衡的大偏差、随时间推移动态游戏玩法的更高变化，或者两者兼有。|
|**2025-06-18**|**The Compositional Architecture of Regret in Large Language Models**|Xiangxiang Cui et.al.|[2506.15617](http://arxiv.org/abs/2506.15617)|null|在大语言模型中，后悔指的是当模型面对与其先前生成的错误信息相矛盾的证据时，其显式的后悔表达。研究后悔机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。为了理解这一机制，我们首先需要识别模型输出中的后悔表达，然后分析其内部表示。这种分析需要检查模型的隐藏状态，其中信息处理在神经元层面进行。然而，这面临三个主要挑战：（1）缺乏专门的数据集来捕捉后悔表达，（2）缺乏度量标准来找到最佳的后悔表示层，（3）缺乏度量标准来识别和分析后悔神经元。为了解决这些局限性，我们提出了以下方法：（1）通过精心设计的提示场景构建一个全面的后悔数据集的工作流程，（2）监督压缩-解耦指数（S-CDI）度量标准来识别最佳的后悔表示层，（3）后悔优势分数（RDS）度量标准来识别后悔神经元和组影响系数（GIC）来分析激活模式。我们的实验结果表明，使用S-CDI度量标准成功识别了最佳的后悔表示层，这在探针分类实验中显著提高了性能。此外，我们发现模型层之间存在M形的解耦模式，揭示了信息处理如何在耦合和解耦阶段之间交替。通过RDS度量标准，我们将神经元分为三个不同的功能组：后悔神经元、非后悔神经元和双神经元。|
|**2025-06-17**|**A Variational Framework for Improving Naturalness in Generative Spoken Language Models**|Li-Wei Chen et.al.|[2506.14767](http://arxiv.org/abs/2506.14767)|**[link](https://github.com/b04901014/vae-gslm)**|**大型语言模型在文本处理方面的成功启发了它们在语音建模中的应用。然而，由于语音是连续且复杂的，通常需要对其进行离散化以进行自回归建模。从自监督模型中衍生出的语音标记（称为语义标记）通常侧重于语音的语言学方面，但忽略了韵律信息。因此，在这些标记上训练的模型可以生成自然度较低的语音。现有的方法试图通过向语义标记添加音高特征来解决这个问题。然而，音高本身并不能完全代表旁白属性的范围，并且选择正确的特征需要仔细的手工设计。为了克服这一点，我们提出了一种端到端的变分方法，该方法可以自动学习将这些连续的语音属性编码到语义标记中，以增强其功能。我们的方法消除了手动提取和选择旁白特征的需求。此外，它根据人类评分者的偏好产生语音延续。代码、样本和模型可在 https://github.com/b04901014/vae-gslm 上找到。**|
|**2025-06-17**|**ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM**|Yujun Wang et.al.|[2506.14766](http://arxiv.org/abs/2506.14766)|null|多模态大型语言模型（MLLM）常常出现幻觉问题。它们过度依赖部分提示，并生成错误响应。最近，如视觉对比解码（VCD）和指令对比解码（ICD）等方法被提出，通过对比扰动或负前缀输入的预测与原始输出，来减轻幻觉。在本工作中，我们发现VCD和ICD等方法本质上影响了模型的内部注意力动态。这一观察表明，它们的有效性可能不仅仅源于对logits的表面修改，而是来自注意力分布的更深层次转变。受此启示，我们提出了一种注意力可控的对比解码框架，该框架直接干预模型的注意力机制，以提供一种更原则性的减轻幻觉的方法。我们在多个MLLM架构和不同的解码方法上的实验表明，我们的方法显著减少了幻觉，并在POPE、CHAIR和MMHal-Bench等基准测试中提高了性能，同时也在标准VQA基准测试中增强了性能。|
|**2025-06-17**|**Large Language Models -- the Future of Fundamental Physics?**|Caroline Heneka et.al.|[2506.14757](http://arxiv.org/abs/2506.14757)|null|对于许多基础物理应用，由于变压器在学习复杂相关性方面的先进性，在领域外数据上进行预训练具有优势。显然的问题是我们是否可以利用需要适当领域外迁移学习的大语言模型。我们展示了如何使用Qwen2.5大语言模型来分析和生成SKA数据，特别是观测宇宙大部分区域的宇宙大尺度结构3D图。我们将LLM与连接网络相结合，并表明对于宇宙参数回归和光锥生成，使用Qwen2.5权重的光锥LLM（L3M）优于标准初始化，并且与大小相匹配的专用网络相比表现良好。|
|**2025-06-17**|**Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs**|Ling Team et.al.|[2506.14731](http://arxiv.org/abs/2506.14731)|null|我们提出了Ring-lite，这是一个基于混合专家（MoE）的大型语言模型，通过强化学习（RL）进行优化，以实现高效且稳健的推理能力。该模型建立在公开可用的Ling-lite模型之上，一个拥有168亿参数、其中27.5亿参数被激活的模型。我们的方法在具有挑战性的基准测试（例如AIME、LiveCodeBench、GPQA-Diamond）上，仅激活了与同类模型所需参数三分之一的情况下，匹配了最先进（SOTA）的小规模推理模型的性能。为了实现这一目标，我们引入了一个联合训练流程，将知识蒸馏与RL相结合，揭示了MoE RL训练中未记录的挑战。首先，我们确定了RL训练过程中的优化不稳定性，并提出了一种新颖的方法——受约束的上下文计算策略优化（C3PO），通过算法-系统协同设计方法提高了训练稳定性和计算吞吐量。其次，我们通过实验证明，基于熵损失而非验证指标来选择蒸馏检查点，在后续的RL训练中实现了更好的性能-效率权衡。最后，我们开发了一种两阶段训练范式，以协调多领域数据的集成，解决使用混合数据集训练时出现的领域冲突。我们将发布该模型、数据集和代码。|
|**2025-06-17**|**AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes**|Jiahao Qiu et.al.|[2506.14728](http://arxiv.org/abs/2506.14728)|null|在知识蒸馏已成为将大型语言模型（LLMs）压缩为更小模型、通过对其输出或内部表示进行对齐的成熟领域的同时，基于LLMs的智能体（涉及规划、记忆和工具使用）的蒸馏仍相对未充分探索。现有的智能体蒸馏方法通常重放完整的教师轨迹或逐步模仿教师的工具使用，但它们往往难以训练学生智能体在新的环境中动态规划和行动。我们提出了AgentDistill，这是一个新颖的、无需训练的智能体蒸馏框架，它通过直接重用模型-上下文-协议（MCPs）来实现高效的、可扩展的知识转移。这些MCPs是由教师智能体自主生成的结构化和可重用的任务解决模块。这些蒸馏的MCPs的重用使学生智能体能够跨领域泛化其能力，并在最小监督或人为干预下解决新问题。在生物医学和数学基准上的实验表明，我们基于小型语言模型构建的蒸馏学生智能体可以达到与使用大型LLMs（如OctoTools（GPT-4o））的高级系统相当的性能，突显了我们框架在构建可扩展和成本效益高的智能体方面的有效性。|
|**2025-06-17**|**Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data**|Anton Changalidis et.al.|[2506.14704](http://arxiv.org/abs/2506.14704)|**[link](https://github.com/um-dacs-nlp/capacity)**|本文研究了模型架构和数据配置如何影响生成式变压器的经验记忆能力。这些模型使用来自系统化医学命名法（SNOMED）知识图谱的合成文本数据集进行训练：三元组代表静态连接，序列模拟复杂的关联模式。结果表明，嵌入大小是学习速度和能力的首要决定因素，而额外的层提供的好处有限，甚至可能在简单数据集上阻碍性能。激活函数起着至关重要的作用，其中Softmax显示出更大的稳定性和能力。此外，增加数据集的复杂性似乎可以提高最终的记忆能力。这些见解提高了我们对变压器记忆机制的理解，并为利用结构化真实世界数据进行模型设计优化提供了一个框架。|
|**2025-06-17**|**Unified Software Engineering agent as AI Software Engineer**|Leonhard Applis et.al.|[2506.14683](http://arxiv.org/abs/2506.14683)|null|随着大型语言模型（LLM）技术的增长，自动化编码的期望也随之提高。然而，软件工程不仅仅涉及编码，还包括项目的维护和演进等活动。在这种情况下，LLM代理的概念受到了关注，这些代理利用LLM作为推理引擎来自主地调用外部工具。但是，LLM代理是否等同于AI软件工程师？在本文中，我们通过开发统一的软件工程代理（USEagent）来寻求对这个问题的理解。与为特定软件任务（如测试、调试和修复）构建专用代理的现有工作不同，我们的目标是构建一个统一的代理，能够协调和处理多种能力。这使代理能够处理软件开发中的复杂场景，例如修复不完整的补丁、添加新功能或接管他人编写的代码。我们将USEagent视为未来AI软件工程师的第一个草案，它可以在未来涉及AI和人类的软件发展团队中成为团队成员。为了评估USEagent的有效性，我们构建了一个统一的软件工程基准（USEbench），其中包含编码、测试和补丁等多种任务。USEbench是SWE-bench、SWT-bench和REPOCOD等现有基准任务的巧妙混合。在一项包含1,271个仓库级软件工程任务的USEbench评估中，USEagent与现有的通用代理（如OpenHands CodeActAgent）相比，显示出更高的有效性。USEagent在特定编码任务的能力上存在差距，这为未来AI软件工程师的进一步发展提供了线索。|
|**2025-06-17**|**AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models**|Ads Dawson et.al.|[2506.14682](http://arxiv.org/abs/2506.14682)|**[link](https://github.com/dreadnode/airtbench-code)**|我们介绍了AIRTBench，这是一个用于评估语言模型自主发现和利用人工智能和机器学习（AI/ML）安全漏洞的AI红队基准。该基准由来自Dreadnode平台Crucible挑战环境中的70个现实黑盒夺旗（CTF）挑战组成，要求模型编写Python代码与AI系统交互和破坏。Claude-3.7-Sonnet以明显优势成为领导者，解决了43个挑战（占总套件的61%，总体成功率为46.9%），其次是Gemini-2.5-Pro，解决了39个挑战（56%，总体成功率为34.3%），GPT-4.5-Preview解决了34个挑战（49%，总体成功率为36.9%），DeepSeek R1解决了29个挑战（41%，总体成功率为26.9%）。我们的评估显示，前沿模型在提示注入攻击方面表现出色（平均成功率为49%），但在系统利用和模型反演挑战上表现不佳（即使是最佳表现者，成功率也低于26%）。前沿模型远远超过开源替代品，最好的真正开源模型（Llama-4-17B）解决了7个挑战（10%，总体成功率为1.0%），尽管在某些困难挑战上显示出专业能力。与人类安全研究人员相比，大型语言模型（LLMs）以惊人的效率解决挑战，几分钟内就能完成人类通常需要数小时或数天才能完成的工作——在困难挑战上的效率优势超过5000倍。我们的贡献填补了评估领域的关键空白，提供了第一个专门设计来衡量和跟踪自主AI红队能力进展的全面基准。|
|**2025-06-17**|**Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality**|Yuto Harada et.al.|[2506.14681](http://arxiv.org/abs/2506.14681)|null|监督微调（SFT）是在使大型语言模型（LLMs）与人类指令和价值观对齐过程中的关键步骤，然而，SFT的许多方面仍然理解不深。我们在包括代码生成、数学推理和通用领域任务在内的各种数据集上训练了广泛的基础模型，从而在可控条件下产生了1,000多个SFT模型。然后，我们确定了最重要的数据集属性，并检查了SFT引入的层间修改。我们的发现揭示，一些训练任务的协同效应在所有模型中都持续存在，而其他方面则存在显著差异，这强调了特定模型策略的重要性。此外，我们证明了困惑度始终可以预测SFT的有效性——通常超过了训练数据和基准之间的表面相似性，并且中间层权重变化与性能提升的相关性最强。我们将发布这些1,000多个SFT模型和基准测试结果，以加速进一步的研究。|
|**2025-06-17**|**GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors**|Hengyuan Zhang et.al.|[2506.14646](http://arxiv.org/abs/2506.14646)|**[link](https://github.com/liar406/gui-lomo)**|参数高效微调（PEFT）方法，尤其是低秩自适应（LoRA），提供了一种以较低的计算成本适应大型语言模型的高效方法。然而，它们的性能受到可训练参数数量较少的限制。最近的研究将LoRA与混合专家（MoE）结合，即LoRA-MoE，以增强容量，但仍有两个限制阻碍了其潜力的充分发挥：1）在分配专家数量时，下游任务的影响；2）所有LoRA专家的均匀秩分配，这限制了表示的多样性。为了弥补这些差距，我们提出了GuiLoMo，这是一种具有引导选择向量（GSVs）的细粒度层内专家数量和秩分配策略。GSVs通过先前的双层优化过程学习，以捕捉模型和任务特定的需求，然后用于分配最优的专家数量和秩。在三个骨干模型和多个基准测试上的实验表明，GuiLoMo始终实现了优于或相当于所有基线的性能。进一步的分析揭示了专家数量和秩如何在层和任务之间变化的关键见解，突出了自适应专家配置的好处。我们的代码可在https://github.com/Liar406/Gui-LoMo.git上找到。|
|**2025-06-16**|**Steering LLM Thinking with Budget Guidance**|Junyan Li et.al.|[2506.13752](http://arxiv.org/abs/2506.13752)|**[link](https://github.com/umass-embodied-agi/budgetguidance)**|近年来，深度思考的大型语言模型通常进行广泛的推理以提高性能，但这种长篇大论的推理并不总是可取，因为它会带来过高的推理成本，而性能提升却不成比例。因此，在不牺牲性能的情况下控制推理长度至关重要，但在紧张的思维预算下这仍然是一个挑战。我们提出了一种名为预算指导的简单而有效的方法，该方法能够引导LLM的推理过程达到目标预算，而无需对LLM进行微调。我们的方法引入了一个轻量级的预测器，该预测器在生成下一个标记的过程中对剩余的思考长度建模一个伽马分布。然后，这个信号被用来以软标记级别的方式引导生成，确保整体推理轨迹遵循指定的思维预算。预算指导能够自然地控制思考长度，并在具有挑战性的数学基准测试中与基线方法相比，实现了显著的标记效率提升。例如，在严格的预算下，它在MATH-500基准测试中实现了高达26%的准确率提升，而使用的思维标记数量仅为全推理模型使用量的63%。预算指导还适用于更广泛的任务领域，并展现出新兴的能力，例如估计问题难度。源代码可在以下链接获取：https://github.com/UMass-Embodied-AGI/BudgetGuidance。|
|**2025-06-16**|**Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability**|Shova Kuikel et.al.|[2506.13746](http://arxiv.org/abs/2506.13746)|**[link](https://github.com/PsyberSecLab/Fine-Tuning-and-Explainability-for-Phishing-Detection)**|钓鱼攻击仍然是网络安全领域最普遍和持续的威胁之一，攻击者不断演变和加强策略以规避通用检测系统。尽管人工智能和机器学习取得了显著进展，但忠实复制支撑钓鱼判断的分类和可解释性推理仍然具有挑战性。由于自然语言处理技术的最新进展，大型语言模型（LLMs）显示出改善特定领域钓鱼分类任务的希望和潜力。然而，提高分类模型的可靠性和鲁棒性不仅需要LLMs的准确预测，还需要与这些预测一致且可信的解释。因此，一个关键问题是：LLMs能否不仅准确分类钓鱼邮件，还能生成与其预测可靠对齐且内部自洽的解释？为了回答这些问题，我们微调了基于transformer的模型，包括BERT、Llama模型和Wizard，通过二进制序列分类、对比学习（CL）和直接偏好优化（DPO）来提高领域相关性和使其更符合钓鱼特定的区分。为此，我们通过应用基于SHAPley值的ConsistenCy度量（CC SHAP）来检查它们在钓鱼分类和可解释性方面的性能，该度量衡量预测解释标记的对齐，以测试模型的内部忠实性和一致性，并揭示其预测和推理背后的原因。总体而言，我们的发现表明，尽管Llama模型缺乏可靠的决策准确性，但它们表现出更强的预测解释标记对齐，具有更高的CC SHAP分数，而Wizard实现了更好的预测准确性但CC SHAP分数较低。|
|**2025-06-16**|**Instruction Following by Boosting Attention of Large Language Models**|Vitoria Guardieiro et.al.|[2506.13734](http://arxiv.org/abs/2506.13734)|null|控制大型语言模型（LLMs）的生成过程仍然是确保其安全可靠部署的核心挑战。虽然提示工程和微调是常见的方法，但近期的研究探索了潜在引导，这是一种轻量级技术，通过改变LLMs内部激活来引导生成。然而，后续研究表明，潜在引导的有效性有限，往往表现不如简单的指令提示。为了解决这一局限性，我们首先在多种行为上建立了一个基准，以实现对引导技术的标准化评估。基于这个基准的见解，我们引入了指令注意力提升（InstABoost），这是一种潜在引导方法，通过改变模型在生成过程中的注意力来增强指令提示的强度。InstABoost结合了现有方法的优势，并得到先前工作的理论支持，该工作表明，基于transformer的模型中的上下文规则遵循可以通过操纵指令上的注意力来控制。在实证上，InstABoost显示出与传统提示和潜在引导相比，具有更好的控制成功率。|
|**2025-06-16**|**Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs**|Sayed Mohammad Vakilzadeh Hatefi et.al.|[2506.13727](http://arxiv.org/abs/2506.13727)|**[link](https://github.com/erfanhatefi/sparc3)**|大型语言模型（LLMs）是许多当代人工智能应用的核心，但它们庞大的参数数量给在内存和计算受限的环境中部署带来了重大挑战。最近在可解释人工智能（XAI）领域，尤其是关于归因方法的研究表明，可解释性还可以通过识别和移除与推理无关的组件来实现模型压缩。在本文中，我们利用层相关传播（LRP）对LLMs进行归因引导的剪枝。虽然LRP在结构化剪枝视觉模型方面已显示出潜力，但我们将其扩展到LLMs的无结构剪枝，并证明它可以显著减少模型大小，同时损失的性能最小。我们的方法在提取与任务相关的子图——所谓的“电路”——方面特别有效，这些电路可以表示核心功能（例如，间接物体识别）。在此基础上，我们引入了一种模型校正技术，通过选择性地移除导致虚假行为（例如，有害输出）的电路。总之，我们将这些技术汇集为一个统一的整体框架，并通过在Llama和OPT模型上进行广泛的压缩、电路发现和模型校正实验来展示其有效性和局限性，突显了其提高模型效率和安全的潜力。我们的代码在https://github.com/erfanhatefi/SparC3上公开可用。|
|**2025-06-16**|**Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models**|Arjun Krishna et.al.|[2506.13726](http://arxiv.org/abs/2506.13726)|null|引言部分引入了高级推理能力，提高了大型语言模型在数学和编码基准上的问题解决性能。然而，这些推理模型与不具备推理能力的同类模型相比，在对抗性提示攻击方面是否更容易受到攻击，尚不清楚。在这项工作中，我们系统地评估了高级推理模型在一系列基于提示的攻击类别中相对于相似的非推理模型的弱点。通过实验数据，我们发现，平均而言，推理增强模型相对于非推理模型稍微更加稳健（攻击成功率分别为42.51%和45.53%，数值越低越好）。然而，这一总体趋势掩盖了显著的具体类别差异：对于某些攻击类型，推理模型明显更加脆弱（例如，在攻击提示的树形结构上最多差32个百分点），而对于其他攻击类型，它们则明显更加稳健（例如，在跨站脚本注入上好29.8个百分点）。我们的发现突出了语言模型中高级推理的安全含义的细微之处，并强调了在多样化的对抗技术中对安全性进行压力测试的重要性。|
|**2025-06-16**|**TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning**|Junru Zhang et.al.|[2506.13705](http://arxiv.org/abs/2506.13705)|**[link](https://github.com/langfengq/timemaster)**|时间序列推理在多模态大型语言模型（MLLMs）中仍然是一个重大挑战，这主要是因为动态的时间模式、模糊的语义以及缺乏时间先验。在这项工作中，我们引入了TimeMaster，这是一种基于强化学习（RL）的方法，它使时间序列MLLM能够直接在可视化的时间序列输入和任务提示上执行结构化、可解释的推理。TimeMaster采用了一种三部分的结构化输出格式，包括推理、分类和领域特定扩展，并通过一个复合奖励函数进行优化，该函数使格式遵循、预测准确性和开放式洞察质量相一致。该模型使用两阶段管道进行训练：我们首先应用监督微调（SFT）以建立良好的初始化，然后在进行时间序列推理时，在标记级别上使用组相对策略优化（GRPO）以实现稳定且具有针对性的奖励驱动改进。我们在TimerBed基准测试上对TimeMaster进行了评估，涵盖了基于Qwen2.5-VL-3B-Instruct的六个现实世界分类任务。TimeMaster实现了最先进的性能，分别比经典时间序列模型和少样本GPT-4o提高了14.6%和7.3%的性能。值得注意的是，TimeMaster不仅超越了时间序列分类：它还表现出专家级别的推理行为，生成上下文感知的解释，并传递领域对齐的洞察。我们的结果表明，奖励驱动的强化学习可以是将时间理解集成到时间序列MLLM中的可扩展且具有前景的方法。|
|**2025-06-16**|**Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems**|Shang-Chi Tsai et.al.|[2506.13692](http://arxiv.org/abs/2506.13692)|null|随着大型语言模型的进步，许多对话系统现在能够对患者的医疗状况提供合理且信息丰富的回应。然而，当患者咨询医生时，他们可能会因为病情的严重性和紧迫性而体验到负面情绪。如果模型能够在回答医疗问题的同时，根据患者的负面情绪提供适当的安慰和同理心，那么它很可能会在医疗咨询过程中提供更加令人安心的体验。为了解决这一问题，我们的论文探讨了医疗对话过程中知识分享与情感支持之间的平衡。我们利用一个大型语言模型来重写一个现实世界的交互式医疗对话数据集，生成带有负面情绪的患者查询和旨在安抚患者情绪的同时解决他们担忧的医疗回应。修改后的数据用于通过各种微调方法精炼最新的大型语言模型，使它们能够准确提供既包含情感安慰又有建设性建议的句子来回应患者的问题。与原始的LLM模型相比，我们的实验结果表明，我们的方法显著提高了模型生成情感回应的能力，同时保持了其提供基于知识的准确答案的原始能力。|
|**2025-06-16**|**What Happens During the Loss Plateau? Understanding Abrupt Learning in Transformers**|Pulkit Gopalani et.al.|[2506.13688](http://arxiv.org/abs/2506.13688)|**[link](https://github.com/pulkitgopalani/tf-loss-plateau)**|在算法任务上训练Transformer经常表现出一种引人入胜的突然学习现象：长时间的性能平台期之后，突然出现急剧的改进。这项工作主要研究了浅层Transformer中这种动态背后的机制。我们发现，在平台期，模型通常发展出一个可解释的局部解决方案，同时在其输出中表现出强烈的重复偏好。这种输出退化伴随着内部表示的崩溃，即不同标记的隐藏状态几乎平行。我们进一步确定了最优注意力图的学习缓慢是一个关键瓶颈。在平台期，注意力配置的缓慢进展预示了最终的快速收敛，直接干预注意力会显著改变平台期持续时间以及重复偏好和表示崩溃的严重程度。我们验证了这些识别出的现象——重复偏好和表示崩溃——并非玩具设置中的伪象，也存在于Pythia和OLMo等大型语言模型的早期预训练阶段。|
|**2025-06-16**|**An LLM's Apology: Outsourcing Awkwardness in the Age of AI**|Twm Stone et.al.|[2506.13685](http://arxiv.org/abs/2506.13685)|**[link](https://github.com/cloakless/flake-bench)**|现代社交动态的一个关键部分是临时爽约。然而，为这种做法想出可信且社交上可接受的理由可能会引起焦虑，从而导致“消失”、尴尬或牵强的借口，从而在对方身上造成情感伤害和怨恨。将这项任务委托给大型语言模型（LLM）的能力可以大大减少摩擦，提高用户社交生活的灵活性，同时最大限度地减少上述创造性负担和道德疑虑。我们引入了FLAKE-Bench，这是一项评估模型在多种社交、职业和浪漫场景中有效、体贴和人性化地摆脱的能力。我们报告了10个前沿或近期前沿LLM在取消先前承诺方面的有效性，因为没有什么比AI生成你的取消短信更能说明“我重视我们的友谊”。我们将FLAKE-Bench开源至github.com/Cloakless/flake-bench，以支持未来的研究。|
|**2025-06-16**|**Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention**|Haonan Wang et.al.|[2506.13674](http://arxiv.org/abs/2506.13674)|null|参数高效微调（PEFT）方法已成为快速适应下游任务的大型语言模型（LLM）的关键。前缀微调（Prefix-Tuning）作为一种早期且有效的PEFT技术，展示了以显著降低计算和内存开销达到与全微调相当性能的能力。然而，尽管其早期成功，它在训练现代最先进的LLM方面的有效性却非常有限。在这项工作中，我们通过实证表明，由于注意力头内输入与前缀重要性之间的固有权衡，Prefix-Tuning在LLM上的表现不佳。这促使我们引入了Prefix-Tuning+，这是一种新颖的架构，它推广了Prefix-Tuning的原则，并通过将前缀模块移出注意力头本身来解决其不足。我们进一步概述了我们的构建过程，以指导未来用户在构建他们自己的基于上下文的方法时。我们的实验表明，在一系列基准测试中，Prefix-Tuning+始终优于现有的Prefix-Tuning方法。值得注意的是，它在几个通用基准测试中实现了与广泛采用的LoRA方法相当的性能，突出了Prefix-Tuning方法在现代的扩展潜力。我们的发现表明，通过克服其固有限制，Prefix-Tuning可以在参数高效LLM适应的领域中保持竞争力和相关性。|
|**2025-06-13**|**code_transformed: The Influence of Large Language Models on Code**|Yuliang Xu et.al.|[2506.12014](http://arxiv.org/abs/2506.12014)|null|代码仍然是人类与机器之间最基本交互方式之一。随着大型语言模型（LLMs）的快速发展，代码生成能力已经开始显著改变编程实践。这一发展引发了一个核心问题：LLMs 是否已经改变了代码风格，这种变化如何被描述？在这篇论文中，我们提出了一项开创性的研究，旨在调查 LLM 对代码风格的影响，重点关注命名约定、复杂性、可维护性和相似性。通过分析 2020 年至 2025 年间发表的 arXiv 论文所链接的 19,000 多个 GitHub 代码库中的代码，我们发现了与 LLM 生成代码特征相符的代码风格演变的可测量趋势。例如，Python 代码中 snake_case 变量名的比例从 2023 年第一季度的 47% 增加到 2025 年第一季度的 51%。此外，我们还通过考察其推理过程来研究 LLM 如何处理算法问题。鉴于 LLM 的多样性和使用场景等因素，精确估计由 LLM 生成或辅助的代码比例可能很困难，甚至不可能。我们的实验结果提供了第一个大规模实证证据，表明 LLM 影响了现实世界的编程风格。|
|**2025-06-13**|**Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making**|Xiaopeng Yuan et.al.|[2506.12012](http://arxiv.org/abs/2506.12012)|null|大型语言模型（LLMs）越来越多地被用于需要复杂推理的任务。大多数基准测试都集中在最终结果上，但忽略了中间推理步骤——如规划、修订和资源限制下的决策。我们认为，衡量这些内部过程对于理解模型行为和提升可靠性至关重要。我们提出使用策略游戏作为自然的评估环境：封闭的、基于规则的系统，具有明确的状态、有限的资源和自动反馈。我们引入了一个框架，从三个核心维度评估LLMs：规划、修订和资源限制下的决策。为了实现这一点，我们定义了除胜率之外的指标，包括过度纠正风险率、纠正成功率、改进斜率和超预算比率。在12个领先模型中的4320轮对抗性测试中，ChatGPT-o3-mini取得了最高的综合得分，胜率为74.7%，纠正成功率为78.6%，改进斜率为0.041。相比之下，尽管Qwen-Plus的过度纠正风险率为81.6%，但其比赛胜率仅为25.6%——主要由于过度使用资源。我们还观察到过度纠正风险率和纠正成功率之间存在负相关（皮尔逊相关系数r = -0.51，p = 0.093），这表明更频繁的编辑并不总是能改善结果。我们的研究结果表明，评估LLMs不仅要知道它们做出了什么决定，还要知道它们是如何做出这些决定的。|
|**2025-06-13**|**VGR: Visual Grounded Reasoning**|Jiacong Wang et.al.|[2506.11991](http://arxiv.org/abs/2506.11991)|null|在多模态思维链（CoT）推理领域，现有方法主要依赖于纯语言空间的推理，这本质上是受语言偏见影响的，并且很大程度上局限于数学或科学领域。这种狭隘的关注限制了它们处理需要全面理解图像细节的复杂视觉推理任务的能力。为了解决这些局限性，本文提出了一种名为VGR的新型推理多模态大型语言模型（MLLM），它具有增强的细粒度视觉感知能力。与仅基于语言空间回答问题或推理的传统MLLM不同，我们的VGR首先检测可能有助于解决问题的相关区域，然后基于重放的图像区域提供精确答案。为了实现这一点，我们开展了一个名为VGR-SFT的大规模强化学习（SFT）数据集，其中包含混合视觉定位和语言推理的推理数据。VGR的推理流程允许模型选择用于视觉参考的边界框，并引入了一个重放阶段，将相应的区域整合到推理过程中，增强了多模型理解。在LLaVA-NeXT-7B基线上的实验表明，VGR在需要全面图像细节理解的多模态基准测试中实现了优异的性能。与基线相比，VGR仅使用了30%的图像标记数量，在MMStar上得分提高了+4.1，在AI2D上提高了+7.1，在ChartQA上提高了+12.9。|
|**2025-06-13**|**How Visual Representations Map to Language Feature Space in Multimodal LLMs**|Constantin Venhoff et.al.|[2506.11976](http://arxiv.org/abs/2506.11976)|null|有效的多模态推理依赖于视觉和语言表示的对齐，然而，视觉-语言模型（VLMs）实现这种对齐的机制仍理解不足。我们介绍了一种方法论框架，该框架故意保持一个冻结的大语言模型（LLM）和一个冻结的视觉Transformer（ViT），它们仅通过在视觉指令调整期间训练一个线性适配器相连。这种设计是我们方法的基础：通过保持语言模型冻结，我们确保它保持其原始的语言表示，而无需适应视觉数据。因此，线性适配器必须直接将视觉特征映射到LLM的现有表示空间，而不是允许语言模型通过微调发展专门的视觉理解。我们的实验设计独特地允许使用LLM的预训练稀疏自编码器（SAEs）作为分析探头。这些SAEs与未更改的语言模型保持完美对齐，并作为学习到的语言特征表示的快照。通过系统地分析SAE重建错误、稀疏模式和特征SAE描述，我们揭示了视觉表示逐渐与语言特征表示对齐的层次结构进展，在中后期层收敛。这表明ViT输出与早期LLM层之间存在基本不匹配，提出了关于当前基于适配器的架构是否最佳地促进跨模态表示学习的重要问题。|
|**2025-06-13**|**Improving Large Language Model Safety with Contrastive Representation Learning**|Samuel Simko et.al.|[2506.11938](http://arxiv.org/abs/2506.11938)|**[link](https://github.com/samuelsimko/crl-llm-defense)**|大型语言模型（LLMs）是具有深远社会影响的有力工具，但它们对各种未受控制的输入生成响应的能力使它们容易受到对抗性攻击。虽然现有的防御方法往往难以在不同攻击类型之间进行泛化，但最近在表示工程方面的进步提供了有希望的替代方案。在这项工作中，我们提出了一种防御框架，将模型防御形式化为对比表示学习（CRL）问题。我们的方法通过结合基于三元组的损失和对抗性困难负样本挖掘来微调模型，以鼓励良性表示和有害表示之间的分离。我们在多个模型上的实验结果表明，我们的方法优于基于先前表示工程的方法，在提高对输入级别和嵌入空间攻击的鲁棒性的同时，并未牺牲标准性能。我们的代码可在https://github.com/samuelsimko/crl-llm-defense上找到。|
|**2025-06-13**|**LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?**|Zihan Zheng et.al.|[2506.11928](http://arxiv.org/abs/2506.11928)|null|近期有报道声称大型语言模型（LLMs）在编程竞赛中已经超越了顶级人类。借鉴国际算法竞赛获奖者群体的知识，我们重新审视了这个说法，探讨了LLMs与人类专家的差异以及仍存在的局限性。我们介绍了LiveCodeBench Pro，这是一个由Codeforces、ICPC和IOI的问题组成的基准，这些问题会持续更新以减少数据污染的可能性。一支奥林匹克奖牌得主团队对每个问题进行了算法分类的标注，并对失败模型生成的提交进行了逐行分析。使用这些新数据和基准，我们发现前沿模型仍存在重大局限性：没有外部工具的情况下，最佳模型在中难度问题上的通过率仅为53%，而在难题上的通过率为0%，在这些领域中，人类专家仍然表现出色。我们还发现，LLMs在实现密集型问题中表现出色，但在细微的算法推理和复杂案例分析方面却面临挑战，常常生成自信但错误的理由。高性能主要是由实现精度和工具增强驱动的，而非推理优势。因此，LiveCodeBench Pro突出了与人类大师水平的重大差距，同时为引导未来以代码为中心的LLM推理的改进提供了精细的诊断。|
|**2025-06-13**|**Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache**|Xiaoran Liu et.al.|[2506.11886](http://arxiv.org/abs/2506.11886)|null|大型语言模型在随着上下文长度增加而不断增长的键值（KV）缓存内存需求面前感到挑战。现有的压缩方法要么统一头部维度，要么依赖注意力引导的标记剪枝，通常牺牲准确性或引入计算开销。我们提出了一种名为FourierAttention的无需训练的框架，该框架利用了Transformer头部维度的异构角色：低维度优先考虑局部上下文，而高维度则捕捉长距离依赖。通过将对长上下文不敏感的维度投影到正交傅里叶基上，FourierAttention使用固定长度的谱系数来近似它们的时序演变。在LLaMA模型上的评估表明，FourierAttention在LongBench和针插稻草（NIAH）上实现了最佳的长上下文准确性。此外，还设计了一个定制的Triton内核，名为FlashFourierAttention，通过简化读写操作来优化内存，在不牺牲性能的情况下实现高效的部署。|
|**2025-06-13**|**Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment**|Alejandro Peña et.al.|[2506.11880](http://arxiv.org/abs/2506.11880)|null|近年来，在风险较高的环境中使用语言技术越来越多，这主要得益于大型语言模型（LLMs）的成功。然而，尽管LLMs表现卓越，但它们容易受到伦理问题的困扰，如人口统计偏差、责任归属或隐私问题。本研究旨在分析基于Transformer的系统学习数据中存在的人口统计偏差的能力，通过基于AI的自动招聘的案例研究进行探讨。我们提出了一种增强隐私的框架，通过从学习流程中减少性别信息来减轻最终工具中的偏见行为。我们的实验分析了数据偏差对基于两种不同LLMs的系统的影响，以及所提出的框架如何有效防止训练系统重现数据中的偏差。|
|**2025-06-13**|**A Short Survey on Formalising Software Requirements using Large Language Models**|Arshad Beg et.al.|[2506.11874](http://arxiv.org/abs/2506.11874)|null|本文对使用大型语言模型（LLM）协助编写软件形式化规范进行了文献综述。介绍了35篇关键论文的摘要，包括使用Dafny、C和Java编写的程序的规范示例。本文源于项目VERIFAI——自然语言需求的可追溯性和验证，该项目旨在解决从自然语言表达的需求中编写形式化规范所面临的挑战。我们的方法采用了多个学术数据库来识别相关研究。AI辅助工具Elicit促进了初步论文的选择，最终选择则通过人工筛选完成。该调查为利用LLM形式化软件需求提供了宝贵的见解和未来方向。|
|**2025-06-13**|**Post Persona Alignment for Multi-Session Dialogue Generation**|Yi-Pei Chen et.al.|[2506.11857](http://arxiv.org/abs/2506.11857)|null|多会话基于角色的对话生成在保持长期一致性和生成多样化、个性化的回应方面面临挑战。虽然大型语言模型（LLMs）在单会话对话中表现出色，但它们在保持角色忠诚度和对话连贯性方面存在困难。现有方法通常在生成回应之前检索角色信息，这可能会限制多样性并导致通用输出。我们提出了后角色对齐（PPA），这是一个新颖的两阶段框架，它逆转了这一过程。PPA首先仅基于对话上下文生成一个通用回应，然后使用该回应作为查询检索相关的角色记忆，最后将回应细化以与说话者的角色保持一致。这种事后对齐策略促进了自然性和多样性，同时保持了一致性和个性化。在多会话LLM生成的对话数据上的实验表明，PPA在一致性、多样性和角色相关性方面显著优于先前的方法，为长期个性化对话生成提供了一个更灵活和有效的范例。|
|**2025-06-12**|**AutoMind: Adaptive Knowledgeable Agent for Automated Data Science**|Yixin Ou et.al.|[2506.10974](http://arxiv.org/abs/2506.10974)|**[link](https://github.com/innovatingai/automind)**|大型语言模型（LLM）代理在解决现实世界的数据科学问题方面展现出巨大潜力。由LLM驱动的数据科学代理有望自动化整个机器学习流程，但其在现实世界中的有效性仍然有限。现有的框架依赖于僵化、预先定义的工作流程和不灵活的编码策略；因此，它们只能在相对简单、经典的问题上表现出色，而无法捕捉到人类实践者在复杂、创新任务中带来的经验知识。在这项工作中，我们引入了AutoMind，一个自适应、知识丰富的LLM代理框架，通过以下三个关键进展克服了这些缺陷：（1）一个精心整理的专家知识库，使代理扎根于领域专家知识；（2）一个代理知识树搜索算法，战略性地探索可能的解决方案；（3）一个自适应的编码策略，动态地根据任务复杂度调整代码生成。在两个自动化数据科学基准上的评估表明，AutoMind相较于最先进的基线提供了更优越的性能。额外的分析确认了其良好的有效性、效率和定性解决方案质量，突显了AutoMind是向完全自动化数据科学迈进的一个高效且稳健的步骤。|
|**2025-06-12**|**Farseer: A Refined Scaling Law in Large Language Models**|Houyi Li et.al.|[2506.10972](http://arxiv.org/abs/2506.10972)|**[link](https://github.com/farseer-scaling-law/farseer)**|为了解决大型语言模型（LLMs）训练成本高昂的问题，我们引入了Farseer，这是一种新颖且精细的扩展定律，能够在不同尺度上提供更高的预测准确性。通过系统地构建模型损失表面 $L(N,D)$，Farseer比之前的定律（例如Chinchilla定律）对经验数据的拟合度显著更好。我们的方法产生了准确、稳健且高度通用的预测，展示了出色的外推能力，通过将外推误差减少了433%而优于Chinchilla定律。这使得能够在所有$(N,D)$ 设置下可靠地评估相互竞争的训练策略，从而使得从小规模消融研究中得出的结论可以自信地外推以预测大规模性能。此外，Farseer还为最优计算分配提供了新的见解，更好地反映了现代LLM训练的细微需求。为了验证我们的方法，我们在不同规模和配置下训练了大约1000个LLMs，消耗了大约300万NVIDIA H100 GPU小时。我们在https://github.com/Farseer-Scaling-Law/Farseer上全面开源了所有模型、数据、结果和日志，以促进进一步的研究。|
|**2025-06-12**|**Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs**|Qizhe Zhang et.al.|[2506.10967](http://arxiv.org/abs/2506.10967)|**[link](https://github.com/theia-4869/cdpruner)**|在多模态大型语言模型（MLLMs）中，输入视觉标记的长度通常显著大于其文本对应物，导致推理成本高。许多研究旨在通过删除冗余视觉标记来解决这个问题。然而，当前的方法要么依赖于基于注意力的剪枝，保留了大量的重复标记，要么使用基于相似性的剪枝，忽略了指令的相关性，从而导致性能不佳。在本文中，我们超越了注意力和相似性，提出了一种名为CDPruner的新型视觉标记剪枝方法，该方法通过最大化保留标记的条件多样性。我们首先定义了基于指令的视觉标记的条件相似性，然后使用行列式点过程（DPP）重新表述标记剪枝问题，以最大化所选子集的条件多样性。提出的CDPruner无需训练且模型无关，便于应用于各种MLLMs。在多个MLLMs上进行的广泛实验表明，CDPruner在各种视觉-语言基准上建立了新的最先进水平。通过DPP最大化条件多样性，所选子集更好地代表了输入图像，同时紧密遵循用户指令，即使在高缩减比下也能保持强大的性能。当应用于LLaVA时，CDPruner将FLOPs减少了95%，将CUDA延迟减少了78%，同时保持了94%的原有准确率。我们的代码可在https://github.com/Theia-4869/CDPruner上获取。|
|**2025-06-12**|**ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark**|Kangwei Liu et.al.|[2506.10960](http://arxiv.org/abs/2506.10960)|**[link](https://github.com/zjunlp/chineseharm-bench)**|大型语言模型（LLMs）在自动化有害内容检测任务中的应用越来越广泛，帮助管理员识别违规行为，提高内容审查的整体效率和准确性。然而，现有的有害内容检测资源主要集中在对英语的研究，而中文数据集相对匮乏且往往范围有限。我们提出了一个全面、专业注释的中文内容有害性检测基准，涵盖了六个代表性类别，且完全由真实世界数据构建。我们的注释过程还产生了一个知识规则库，为LLMs在中文有害内容检测中提供明确的专家知识。此外，我们提出了一种知识增强的基线模型，该模型集成了人工注释的知识规则和大型语言模型中的隐式知识，使得小型模型能够实现与最先进LLMs相当的性能。代码和数据可在https://github.com/zjunlp/ChineseHarm-bench获取。|
|**2025-06-12**|**SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks**|Lianghong Guo et.al.|[2506.10954](http://arxiv.org/abs/2506.10954)|**[link](https://github.com/deepsoftwareanalytics/swe-factory)**|**构建用于GitHub问题解决任务的大规模数据集对于训练和评估大型语言模型（LLMs）的软件工程能力至关重要。然而，创建此类基准的传统方法闻名地具有挑战性和劳动密集型，尤其是在设置评估环境、评分测试结果和验证任务实例的阶段。在本文中，我们提出了SWE-Factory，这是一个旨在解决这些挑战的自动化流程。为了解决这些问题，我们的流程集成了三个核心的自动化组件。首先，我们引入了SWE-Builder，这是一个多智能体系统，它自动化了评估环境的构建，利用四个专门智能体在协作的迭代循环中工作，并通过环境内存池来提高效率。其次，我们引入了一种基于标准化退出码的评分方法，消除了手动编写自定义解析器的需求。最后，我们使用这些可靠的退出码信号自动化了fail2pass验证过程。在四个编程语言中的671个问题上的实验表明，我们的流程可以有效地构建有效的任务实例；例如，使用GPT-4.1-mini，我们的SWE-Builder以每个实例0.045美元的成本构建了269个有效实例，而使用Gemini-2.5-flash，它以每个实例0.024美元的最低成本实现了可比的性能。我们还证明，基于退出码的评分与人工检查相比实现了100%的准确率，我们的自动化fail2pass验证达到了精确度为0.92和召回率为1.00。我们希望我们的自动化流程能够加速收集用于训练和评估的大规模、高质量GitHub问题解决数据集。我们的代码和数据集已在https://github.com/DeepSoftwareAnalytics/swe-factory发布。**|
|**2025-06-12**|**Build the web for agents, not agents for the web**|Xing Han Lù et.al.|[2506.10953](http://arxiv.org/abs/2506.10953)|null|最近大型语言模型（LLMs）和多模态模型的进步激发了人们对于开发网络代理——能够在网络环境中自主导航和完成任务的人工智能系统的极大兴趣。虽然这些方法在自动化复杂的网络交互方面具有巨大的潜力，但由于人类设计的界面与LLM能力之间的根本不匹配，当前的方法面临着巨大的挑战。当前方法在处理网络输入的固有复杂性方面存在困难，无论是处理大量的DOM树、依赖带有额外信息的截图，还是通过API交互完全绕过用户界面。这篇立场论文主张在Web代理研究中实现范式转变：而不是强迫网络代理适应为人类设计的界面，我们应该开发一个针对代理能力优化的新交互范式。为此，我们引入了“代理式网络界面”（AWI）的概念，这是一种专门为代理导航网站而设计的界面。我们为AWI设计确立了六个指导原则，强调安全性、效率和标准化，以考虑到所有主要利益相关者的利益。这种重新构架旨在克服现有界面的根本局限性，为更高效、可靠和透明的Web代理设计铺平道路，这将是一个涉及更广泛的ML社区的协作努力。|
|**2025-06-12**|**Execution Guided Line-by-Line Code Generation**|Boaz Lavon et.al.|[2506.10948](http://arxiv.org/abs/2506.10948)|**[link](https://github.com/boazlavon/eg_cfg)**|我们提出了一种将实时执行信号纳入语言模型生成过程的创新方法。虽然大型语言模型（LLMs）在代码生成方面表现出令人印象深刻的性能，但它们通常在推理过程中不利用执行反馈，这是人类程序员经常利用的关键信号。我们的方法，执行引导的无分类器指导（EG-CFG），在模型生成代码时动态地融入执行信号，提供逐行反馈以引导生成过程向可执行解决方案发展。EG-CFG采用多阶段流程：首先，我们对每行执行束搜索以采样候选程序补全；其次，通过执行这些候选程序来提取执行信号；最后，我们将这些信号在生成过程中融入提示。通过在相同行内的标记间保持一致的信号并在行边界刷新信号，我们的方法提供了连贯的指导，同时保留了句法结构。此外，该方法自然支持任务级别的本地并行性，其中多个代理并行操作，探索不同的推理路径并共同生成广泛的一组候选解决方案。我们在各种编码任务上的实验表明，与标准方法相比，EG-CFG显著提高了代码生成性能，在从基础问题到具有挑战性的竞技编程任务的各种复杂度级别上实现了最先进的结果。我们的代码可在以下网址获取：https://github.com/boazlavon/eg_cfg|
|**2025-06-12**|**GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models**|Evelyn Ma et.al.|[2506.10946](http://arxiv.org/abs/2506.10946)|null|随着合规性、版权保护和隐私问题的重要性日益增加，大型语言模型（LLM）中的“反学习”变得越来越重要。然而，LLM反学习的一个关键挑战是无意中遗忘，即特定数据的删除无意中损害了模型的效用及其对有价值、所需信息的保留。尽管先前的研究主要集中在架构创新上，但数据级因素对反学习性能的影响仍被低估。因此，现有方法在遗忘高影响数据时往往会出现保留度下降。为了解决这个问题，我们提出了GUARD——一个通过数据归因进行引导反学习和保留的新框架。在核心上，GUARD引入了一个轻量级的代理数据归因指标，专门针对LLM反学习，它量化了遗忘集和保留集之间的“一致性”，同时保持计算效率。在此基础上，我们设计了一个新的反学习目标，它为样本分配自适应、非均匀的反学习权重，与它们的代理归因分数成反比。通过这种反学习能力的重新分配，GUARD减轻了无意中的保留损失。我们提供了严格的理论保证，证明GUARD显著提高了保留度，同时保持遗忘指标与先前方法相当。在TOFU基准上对多个LLM架构的广泛实验表明，GUARD在确保有效反学习的同时，显著提高了效用保留。值得注意的是，当遗忘10%的训练数据时，GUARD将保留集上的效用损失减少了高达194.92%的真理比率。|
|**2025-06-12**|**Self-Adapting Language Models**|Adam Zweiger et.al.|[2506.10943](http://arxiv.org/abs/2506.10943)|null|大型语言模型（LLMs）功能强大但静态；它们缺乏根据新任务、知识或示例调整权重的机制。我们引入了自我适应大型语言模型（SEAL），这是一个框架，使LLMs能够通过生成自己的微调数据和更新指令来自我适应。给定一个新输入，该模型生成一个自我编辑生成，可以以不同的方式重新结构信息，指定优化超参数，或调用数据增强和基于梯度的更新工具。通过监督微调（SFT），这些自我编辑导致持久的权重更新，从而实现持续的适应。为了训练模型生成有效的自我编辑，我们使用了一个强化学习循环，将更新模型的下游性能作为奖励信号。与依赖于单独的适应模块或辅助网络的前期方法不同，SEAL直接使用模型的生成来控制其适应过程。在知识融合和少样本泛化方面的实验表明，SEAL是朝着能够自我引导适应的语言模型迈出的有希望的一步。我们的网站和代码可在https://jyopari.github.io/posts/seal找到。|
|**2025-06-12**|**Dynamic Epistemic Friction in Dialogue**|Timothy Obiso et.al.|[2506.10934](http://arxiv.org/abs/2506.10934)|null|最近，在将大型语言模型（LLMs）与人类偏好对齐方面取得的进展显著增强了它们在人类-人工智能协作场景中的实用性。然而，这些方法往往忽略了“认识摩擦”这一关键作用，即在面对新信息、冲突信息或模糊信息时更新信念时固有的阻力。在本文中，我们将动态认识摩擦定义为对认识集成的阻力，其特征是代理当前信念状态与新证据支持的新命题之间的不匹配。我们将这一概念置于动态认识逻辑（Van Benthem和Pacuit，2011）的框架内，其中摩擦在互动过程中表现为非平凡的信念修正。然后，我们通过一项情境协作任务的分析展示，这一认识摩擦模型如何有效地预测对话中的信念更新。随后，我们讨论了如何使信念对齐模型作为一种衡量认识阻力或摩擦的指标更加复杂，以适应现实对话场景的复杂性。|
|**2025-06-10**|**VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning**|Li Kang et.al.|[2506.09049](http://arxiv.org/abs/2506.09049)|null|在动态环境中协调多个具身智能体仍然是人工智能领域的一个核心挑战，这需要感知驱动的推理和可扩展的合作策略。尽管最近的研究利用大型语言模型（LLMs）进行多智能体规划，但只有少数研究开始探索视觉语言模型（VLMs）进行视觉推理。然而，基于VLM的方法在支持多样化的具身类型方面仍然有限。在这项工作中，我们引入了VIKI-Bench，这是第一个为具身多智能体合作量身定制的分层基准，具有三个结构化层次：智能体激活、任务规划和轨迹感知。VIKI-Bench包括多样化的机器人具身、多视角视觉观察和结构化监督信号，以评估基于视觉输入的推理。为了展示VIKI-Bench的实用性，我们提出了VIKI-R，这是一个两阶段框架，它使用思维链注释的演示微调预训练的视觉语言模型（VLM），随后在多级奖励信号下进行强化学习。我们的广泛实验表明，VIKI-R在所有任务层次上都显著优于基线方法。此外，我们展示了强化学习能够使异构智能体之间出现组合性合作模式。总之，VIKI-Bench和VIKI-R为推进具身人工智能系统中的多智能体、视觉驱动的合作提供了一个统一的测试平台和方法。|
|**2025-06-10**|**Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation**|Xiaowen Ma et.al.|[2506.09046](http://arxiv.org/abs/2506.09046)|null|利用多个大型语言模型（LLMs）已被证明对于解决复杂、高维任务非常有效，但当前的方法通常依赖于静态、手动设计的多智能体配置。为了克服这些限制，我们提出了Agentic Neural Network（ANN）框架，该框架将多智能体协作概念化为分层神经网络架构。在这个设计中，每个智能体作为一个节点，每一层形成了一个专注于特定子任务的协作“团队”。Agentic Neural Network遵循两阶段优化策略：（1）正向阶段——借鉴神经网络的前向传播，任务被动态分解为子任务，并逐层构建具有合适聚合方法的协作智能体团队。（2）反向阶段——通过镜像反向传播，我们通过迭代反馈精炼全局和局部协作，使智能体能够自我进化其角色、提示和协调。这种神经符号方法使ANN在训练后能够创建新的或专门的智能体团队，实现了在准确性和适应性方面的显著提升。在四个基准数据集上，ANN在相同配置下超过了领先的多个智能体基线，显示出一致的性能改进。我们的发现表明，ANN为多智能体系统提供了一个可扩展、数据驱动的框架，将LLMs的协作能力与神经网络原理的效率和灵活性相结合。我们计划开源整个框架。|
|**2025-06-10**|**AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions**|Polina Kirichenko et.al.|[2506.09038](http://arxiv.org/abs/2506.09038)|**[link](https://github.com/facebookresearch/abstentionbench)**|为了使大型语言模型（LLMs）能够在日常和高风险领域可靠地部署，了解何时不回答与正确回答一样关键。现实中的用户查询可能是不明确的、不合理的或本质上无法回答的，这要求LLMs能够推理不确定性并选择性拒绝回答，即拒绝明确回答。然而，拒绝回答的研究仍然不足，缺乏现代LLMs的系统评估框架。在这项工作中，我们引入了AbstentionBench，这是一个大规模基准，用于全面评估20个不同数据集中的拒绝回答，包括未知答案、不明确、错误前提、主观解释和过时信息等问题。评估20个前沿LLMs表明，拒绝回答是一个未解决的问题，并且在这个问题上，模型规模的扩大几乎没有作用。虽然最近的推理LLMs在复杂问题解决方面取得了令人印象深刻的结果，但令人惊讶的是，我们发现推理微调会降低拒绝回答的能力（平均降低24%），即使在针对推理模型进行专门训练的数学和科学领域也是如此。我们发现，虽然精心设计的系统提示可以在实际中提高拒绝回答的能力，但它并不能解决模型在推理不确定性方面的基本无能。我们发布AbstentionBench，以促进对提高LLMs可靠性的研究。|
|**2025-06-10**|**FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed**|Sizhe Dang et.al.|[2506.09034](http://arxiv.org/abs/2506.09034)|null|大型语言模型（LLM）微调时常遇到GPU内存瓶颈：Adam这类一阶优化器的反向传播会使内存使用量超过推理水平10倍以上（例如，OPT-30B达到633 GB）。零阶优化器（ZO）通过仅从正向传递估计梯度来避免这种成本，但现有的方法如MeZO通常需要更多的步骤才能收敛。这种零阶优化器在速度和内存之间的权衡能否得到根本性的改进？标准化SGD（Normalized-SGD）在比Adam更高的内存效率下展现出强大的实证性能。鉴于此，我们引入了FZOO，一种面向Adam规模速度的快速零阶优化器。FZOO通过使用批量单边估计，并根据批次损失的标准差调整步长来减少达到收敛所需的全部正向传递次数。它还通过结合Rademacher随机向量扰动和CUDA的并行处理来加速每批次的计算。在包括RoBERTa-large、OPT（350M-66B）、Phi-2和Llama3等多样模型上进行的广泛实验验证了FZOO的有效性。平均而言，FZOO在准确度上比MeZO高出3%，而需要的正向传递次数要少3倍。对于RoBERTa-large，FZOO相比MeZO实现了平均5.6%的准确度提升和正向传递次数的18倍减少，达到了与Adam相当的收敛速度。我们还提供了理论分析，证明了FZOO与标准化SGD更新规则形式上等价及其收敛保证。FZOO能够顺畅地集成到PEFT技术中，实现更大的内存节省。总的来说，我们的结果使得单GPU、高速、全参数微调成为可能，并为内存高效预训练的未来工作指明了方向。|
|**2025-06-10**|**Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning**|Haozhen Zhang et.al.|[2506.09033](http://arxiv.org/abs/2506.09033)|**[link](https://github.com/ulab-uiuc/router-r1)**|随着多样化的大型语言模型（LLMs）的迅速出现，催生了LLM路由器的开发，这些路由器将用户查询分配给最合适的模型。然而，现有的LLM路由器通常执行单轮一对一的映射（即，独立地将每个查询分配给单个模型），这限制了它们处理需要多个LLMs互补优势的复杂任务的能力。在本文中，我们提出了基于强化学习（RL）的框架Router-R1，将多LLM路由和聚合建模为一个连续决策过程。Router-R1将路由器本身实例化为一个有能力的LLM，利用其推理能力将“思考”动作（内部思考）与“路由”动作（动态模型调用）交替进行，并将每个响应整合到其不断发展的上下文中。为了指导学习，我们采用了一种轻量级的基于规则的奖励，包括格式奖励、最终结果奖励以及用于性能和成本权衡优化的新颖成本奖励，为通过RL优化性能-成本权衡开辟了道路。Router-R1还仅根据简单的模型描述符（如定价、延迟和示例性能）进行条件化，使得它能够对未见过的模型选择具有强大的泛化能力。在七个通用和多跳问答基准测试上的实验表明，Router-R1优于几个强大的基线，在保持强大泛化和成本管理的同时实现了优越的性能。代码可在https://github.com/ulab-uiuc/Router-R1获取。|
|**2025-06-10**|**SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning**|Ruiqi Zhang et.al.|[2506.09016](http://arxiv.org/abs/2506.09016)|**[link](https://github.com/zanette-labs/speed-rl)**|使用强化学习（RL）训练大型语言模型，针对可验证的奖励显著提升了其推理能力，但由于不高效的均匀提示采样，这种方法在计算上仍然非常昂贵。我们引入了具有高效难度估计的精选提示（SPEED），这是一种自适应的在线RL课程，它有选择性地选取中等难度的训练示例以最大化学习效率。从理论上讲，我们证明了中等难度的提示可以提高梯度估计器的信噪比，从而加速收敛。从经验上讲，我们高效的实现使得训练速度提高了2倍到6倍，同时没有降低准确度，无需手动调整，并且可以无缝集成到标准的RL算法中。|
|**2025-06-10**|**Learning to Reason Across Parallel Samples for LLM Reasoning**|Jianing Qi et.al.|[2506.09014](http://arxiv.org/abs/2506.09014)|null|通过测试时计算扩展，大型语言模型（LLMs）可以获得显著的性能提升。通过采样多个答案并启发式地聚合它们的答案（例如，通过多数投票或使用验证器对答案进行排序），可以在数学领域实现一致的性能提升。在本文中，我们提出了一种利用此类多个样本集的新方法。我们训练了一个紧凑的LLM，称为样本集聚合器（SSA），它接受多个样本的连接序列并输出最终答案，通过强化学习优化其答案准确性。在多个推理数据集上的实验表明，SSA优于其他测试时扩展方法，如基于奖励模型的再排序。我们的方法还显示出跨样本集大小、基础模型家族和规模的强大泛化能力。通过将LLM分为生成答案和LLM分析及聚合采样答案，我们的方法可以轻松高效地与顶级黑盒模型的输出协同工作。|
|**2025-06-10**|**Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models**|Bei Chu et.al.|[2506.09002](http://arxiv.org/abs/2506.09002)|null|单元测试对于确保软件的可靠性和正确性至关重要。经典的基于搜索的软件测试（SBST）方法和基于并发执行的单元测试生成方法，由于难以处理复杂程序单元，如分支条件和外部依赖，往往无法实现高覆盖率。最近的研究越来越多地利用大型语言模型（LLMs）来生成测试用例，通过提供更好的上下文和纠正模型输出的错误来提高测试生成的质量。然而，这些方法依赖于固定的提示，导致编译成功率和覆盖率相对较低。本文提出了一种名为PALM的方法，它利用大型语言模型（LLMs）来增强高覆盖率单元测试的生成。PALM执行程序分析以识别函数内的分支条件，然后将这些条件组合成路径约束。这些约束和相关的上下文信息用于构建提示，引导LLMs生成单元测试。我们实现了这种方法，并在10个开源Rust存储库中进行了评估。实验结果表明，仅用两到三个小时，PALM就能显著提高测试覆盖率，有些实例中整体项目覆盖率提高超过50%，其生成的测试平均覆盖率为75.77%，与人工努力（71.30%）相当，突显了LLMs在自动化测试生成中的潜力。我们提交了91个针对新代码的PALM生成的单元测试。在这些提交中，80个被接受，5个被拒绝，6个仍在审查中。这些结果证明了将程序分析与人工智能相结合的有效性，并为未来自动化软件测试的研究开辟了新的途径。|
|**2025-06-10**|**SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning**|Xiao Liang et.al.|[2506.08989](http://arxiv.org/abs/2506.08989)|**[link](https://github.com/mastervito/sws)**|可验证奖励强化学习（RLVR）已被证明在训练大型语言模型（LLMs）进行复杂推理任务，如数学问题解决方面非常有效。RLVR可扩展性的先决条件是拥有高质量的问题集，其中包含精确且可验证的答案。然而，现有基于蒸馏的合成数据集中精心制作的人类标注数学问题和有限的验证答案的稀缺限制了它们在强化学习中的有效性。此外，大多数问题合成策略在生成有用问题时，不考虑模型的能力，无差别地扩展问题集，导致生成效率低下。为了缓解这一问题，我们引入了一种自我意识弱点驱动的合成框架（SwS），该框架系统地识别模型的不足，并利用这些不足进行问题增强。具体来说，我们将弱点定义为模型在强化学习训练过程中通过迭代采样始终无法学习的问答。然后，我们从这些失败案例中提取核心概念，并合成新问题，以增强模型在后续增强训练中薄弱领域的强度，使其能够专注于并逐步克服其弱点。我们的框架不依赖外部知识蒸馏，通过使模型能够自我识别并解决其在强化学习中的弱点，从而实现稳健的泛化，使7B和32B模型在八个主流推理基准上的平均性能分别提高了10.0%和7.7%。|
|**2025-06-10**|**Towards Better Code Generation: Adaptive Decoding with Uncertainty Guidance**|Kaifeng He et.al.|[2506.08980](http://arxiv.org/abs/2506.08980)|null|使用大型语言模型（LLMs）进行代码生成对解码过程中token的选择高度敏感，尤其是在那些对生成程序逻辑产生关键影响的不确定点上。传统的解码方法，如贪婪搜索和束搜索，对所有token采取统一处理，忽略了代码生成中固有的独特不确定性特征，这可能导致输出次优。在本工作中，我们进行了一项实证分析，表明大量生成错误源于高不确定性步骤中的token排序错误，即真实token存在于候选集中但未能排在第一位。受此启发，我们引入了AdaDec，这是一个由通过香农熵量化的token级不确定性指导的自适应解码框架。AdaDec动态学习针对每个模型的定制化不确定性阈值，并在不确定性超过这些阈值时采用暂停-重新排序机制，并带有前瞻性。在HumanEval和MBPP基准测试上的评估显示，与贪婪解码相比，AdaDec在Pass@1准确率上实现了高达15.5%的提升，与传统的束搜索相当或优于其性能，并通过有针对性的选择性暂停减少了计算开销和延迟。我们的发现表明，具有不确定性感知的自适应解码对于增强LLMs进行代码生成的可靠性和效率具有相当大的潜力。|
|**2025-06-09**|**Play to Generalize: Learning to Reason Through Game Play**|Yunfei Xie et.al.|[2506.08011](http://arxiv.org/abs/2506.08011)|**[link](https://github.com/yunfeixie233/vigal)**|在多模态大型语言模型（MLLMs）中发展可泛化的推理能力仍然是一个挑战。受认知科学文献的启发，这些文献指出游戏可以促进可转移的认知技能，我们提出了一种新的训练后范式，即视觉游戏学习（Visual Game Learning，简称ViGaL），其中MLLMs通过玩类似街机的游戏来发展跨领域的多模态推理能力。具体来说，我们展示了通过强化学习（RL）在简单的类似街机的游戏上对7B参数的MLLM进行训练，例如“贪吃蛇”，可以显著提高其在多模态数学基准（如MathVista）和多学科问题（如MMMU）上的下游性能，而在此过程中并未看到任何已解决的方案、方程或图表，这表明捕获了可转移的推理技能。值得注意的是，我们的模型在多模态推理基准上优于专门针对多模态推理数据进行调整的模型，同时保持了基础模型在一般视觉基准上的性能，这是一个专业模型通常难以达到的挑战。我们的发现表明了一种新的训练后范式：合成、基于规则的游戏可以作为可控和可扩展的前文本任务，从而在MLLMs中解锁可泛化的多模态推理能力。|
|**2025-06-09**|**GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior**|Penghao Wu et.al.|[2506.08012](http://arxiv.org/abs/2506.08012)|null|多模态大型语言模型（MLLMs）在革命性改变图形用户界面（GUI）自动化方面展现出巨大潜力。然而，现有的GUI模型大多依赖于从几乎无错误的离线轨迹中进行学习，因此缺乏反思和错误恢复能力。为了弥合这一差距，我们提出了GUI-Reflection，一个新型框架，该框架在专门的训练阶段明确地将自我反思和错误纠正能力集成到端到端多模态GUI模型中：GUI特定预训练、离线监督微调（SFT）和在线反思调整。GUI-reflection允许通过全自动的数据生成和学习过程实现自我反思行为，无需任何人工标注。具体来说，1）我们首先提出了可扩展的数据管道，可以从现有的成功轨迹中自动构建反思和错误纠正数据。虽然现有的GUI模型主要关注于基础和UI理解能力，但我们提出了GUI-Reflection任务套件，以明确学习和评估反思能力。2）此外，我们构建了一个多样化且高效的移动设备上GUI模型在线训练和数据收集环境。3）我们还提出了一种迭代在线反思调整算法，利用所提出的环境，使模型能够持续增强其反思和错误纠正能力。我们的框架为GUI代理配备了自我反思和纠正能力，为更稳健、适应性强和智能的GUI自动化铺平了道路，所有数据、模型、环境和工具都将公开发布。|
|**2025-06-09**|**Reinforcement Pre-Training**|Qingxiu Dong et.al.|[2506.08007](http://arxiv.org/abs/2506.08007)|null|在这项工作中，我们引入了强化预训练（RPT）作为大语言模型和强化学习（RL）的一种新的扩展范式。具体来说，我们将下一个标记预测重新定义为使用RL训练的推理任务，其中它为正确预测给定上下文的下一个标记而获得可验证的奖励。RPT提供了一种可扩展的方法，可以利用大量文本数据用于通用RL，而不是依赖于特定领域的标注答案。通过激励下一个标记推理的能力，RPT显著提高了预测下一个标记的语言建模精度。此外，RPT为进一步的强化微调提供了一个强大的预训练基础。扩展曲线显示，增加的训练计算量始终提高了下一个标记预测的准确性。这些结果将RPT定位为一种有效且具有前景的扩展范式，以推进语言模型预训练。|
|**2025-06-09**|**Reparameterized LLM Training via Orthogonal Equivalence Transformation**|Zeju Qiu et.al.|[2506.08001](http://arxiv.org/abs/2506.08001)|null|在大型语言模型（LLMs）推动人工智能快速发展的同时，有效地和可靠地训练这些大型模型仍然是该领域最大的挑战之一。为了应对这一挑战，我们提出了POET，这是一种新型的重新参数化训练算法，它使用正交等价变换来优化神经元。具体来说，POET使用两个可学习的正交矩阵和一个固定的随机权重矩阵来重新参数化每个神经元。由于其能保证权重矩阵的谱性质的保持，POET能够稳定地优化目标函数，并提高泛化能力。我们进一步开发了高效的近似方法，使得POET能够灵活且可扩展地训练大规模神经网络。广泛的实验验证了POET在训练LLMs中的有效性和可扩展性。|
|**2025-06-09**|**Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System**|Fan Yang et.al.|[2506.07997](http://arxiv.org/abs/2506.07997)|null|建筑行业既具有高物理风险，又具有高心理风险，然而心理健康支持仍然有限。尽管人工智能（AI）的进步，尤其是大型语言模型（LLM），提供了有希望的解决方案，但它们在建筑领域的潜力仍被大量未开发。为了填补这一差距，我们开发了一个对话式多智能体系统，通过整合领域知识的人工智能驱动方法解决行业特定的挑战。同时，它通过使与多个具有独特个性的智能体互动来满足建筑工人的基本心理需求。这种方法确保工人既能获得实际解决问题的支持，又能获得社交互动，最终有助于他们的整体福祉。我们通过一项包含12名参与者的被试内用户研究来评估其可用性和有效性。结果显示，我们的系统在可用性、自我决定、社交存在和信任方面均显著优于单一智能体基线，分别提高了18%、40%、60%和60%。这些发现突显了LLM驱动的AI系统在为建筑工人提供特定领域支持方面的潜力。|
|**2025-06-09**|**HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization**|Hongzheng Chen et.al.|[2506.07972](http://arxiv.org/abs/2506.07972)|**[link](https://github.com/cornell-zhang/heurigym)**|尽管大型语言模型（LLMs）在推理和基于代理的问题解决方面取得了显著进步，但当前的评估方法无法充分评估其能力：现有的基准要么依赖于容易饱和和记忆的封闭式问题，要么缺乏一致性和严谨性的主观比较。在这项工作中，我们引入了HeuriGym，这是一个用于评估LLMs生成的启发式算法的代理框架，这些算法针对组合优化问题，具有明确定义的目标和广泛的解决方案空间。HeuriGym使LLMs能够提出启发式方法，通过代码执行接收评估反馈，并迭代优化其解决方案。我们在计算机系统、物流和生物学等领域的九个问题上评估了九个最先进的模型，揭示了在工具使用、规划和适应性推理方面的持续局限性。为了量化性能，我们提出了质量产出指数（QYI），这是一个同时捕捉解决方案通过率和质量的指标。即使是像GPT-o4-mini-high和Gemini-2.5-Pro这样的顶级模型，其QYI得分也只有0.6，远低于1的专家基准。我们的开源基准旨在指导LLMs在科学和工程领域向更有效和现实的问题解决方向发展。|
|**2025-06-09**|**CyberV: Cybernetics for Test-time Scaling in Video Understanding**|Jiahao Meng et.al.|[2506.07971](http://arxiv.org/abs/2506.07971)|**[link](https://github.com/marinero4972/cyberv)**|当前的多模态大型语言模型（MLLMs）可能因为测试时的计算需求、鲁棒性不足和有限的准确性而难以理解长视频或复杂视频，这些问题主要源于其前馈处理特性。对于参数较少的模型，这些限制可能更为严重。为了解决这些限制，我们提出一个受控制论原理启发的创新框架，重新设计视频MLLMs作为能够在推理过程中自我监控、自我纠正和动态资源分配的适应性系统。我们的方法，CyberV，引入了一个由MLLM推理系统、传感器和控制器组成的控制论循环。具体来说，传感器监控MLLM的前向过程并收集中间解释，如注意力漂移，然后控制器决定何时以及如何触发自我纠正并生成反馈以指导下一轮。这个测试时自适应缩放框架增强了冻结的MLLMs，而无需重新训练或添加额外组件。实验表明了显著的改进：CyberV使Qwen2.5-VL-7B在VideoMMMU上的性能提升了8.3%，InternVL3-8B提升了5.5%，超过了竞争的专有模型GPT-4o。当应用于Qwen2.5-VL-72B时，它实现了10.0%的提升，其性能甚至可以与人类专家相媲美。此外，我们的方法在通用基准测试中表现出了一致的收益，如VideoMME和WorldSense，这突出了其在使MLLM更鲁棒和准确地进行动态视频理解方面的有效性和泛化能力。代码已发布在https://github.com/marinero4972/CyberV。|
|**2025-06-09**|**SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence**|Ziyang Gong et.al.|[2506.07966](http://arxiv.org/abs/2506.07966)|**[link](https://github.com/cuzyoung/space-10)**|多模态大型语言模型（MLLMs）在多种多模态任务中取得了显著的进步。为了在空间领域追求更高的智能，MLLMs需要整合多种原子级空间能力以处理复杂和动态的任务。然而，现有的基准测试难以全面评估常见MLLMs从原子级到组合级的空间智能。为了填补这一空白，我们提出了SpaCE-10，这是一个用于组合空间评估的全面基准。在SpaCE-10中，我们定义了10种原子级空间能力，这些能力组合形成了8种组合能力。基于这些定义，我们提出了一种新颖的分层标注流程来生成高质量且多样化的问答（QA）对。经过150小时以上的人工专家努力，我们在SpaCE-10中获得了超过5千对问答，针对811个真实室内场景，涵盖了各种评估设置，如点云输入和多选题问答。我们对SpaCE-10上的常见MLLM进行了广泛评估，发现即使是最高级的MLLM仍然与人类存在很大差距。通过我们的仔细研究，我们还得出了一些对MLLM社区有益的重要发现。例如，我们发现计数能力的不足极大地限制了现有MLLM的组合空间能力。评估代码和基准数据集可在https://github.com/Cuzyoung/SpaCE-10上获取。|
|**2025-06-09**|**Reinforcing Multimodal Understanding and Generation with Dual Self-rewards**|Jixiang Hong et.al.|[2506.07963](http://arxiv.org/abs/2506.07963)|null|基于大型语言模型（LLMs），最近的大型多模态模型（LMMs）将跨模型的理解决策和生成统一到一个框架中。然而，LMMs在实现准确的图像-文本对齐方面仍然存在困难，容易生成与视觉输入相矛盾的文字回应或未能遵循文本到图像的提示。现有的解决方案需要外部监督（例如，人工反馈或奖励模型），并且仅针对单向任务——要么理解要么生成。在这项工作中，基于理解与生成是逆双重任务的观察，我们引入了一种自监督双重奖励机制来加强LMMs的理解和生成能力。具体来说，我们在一个任务域中对给定输入采样多个输出，然后反转输入-输出对来计算模型的逆向似然作为优化的自奖励。在视觉理解和生成基准测试上的大量实验结果表明，我们的方法可以在没有任何外部监督的情况下有效地提升模型性能，尤其是在文本到图像任务中实现了显著的改进。|
|**2025-06-09**|**Correlated Errors in Large Language Models**|Elliot Kim et.al.|[2506.07962](http://arxiv.org/abs/2506.07962)|null|在训练数据、架构和提供商方面的多样性被假设可以缓解大型语言模型（LLM）的同质性。然而，我们缺乏不同LLM之间是否存在实质性差异的实证证据。我们对超过350个LLM进行了大规模的实证评估，使用了两个流行的排行榜和一个简历筛选任务。我们发现模型错误之间存在着显著的相关性——在一个排行榜数据集中，当两个模型都出错时，模型意见一致的频率为60%。我们确定了驱动模型相关性的因素，包括共享的架构和提供商。然而，关键的是，即使具有不同的架构和提供商，更大和更精确的模型也具有高度相关性的错误。最后，我们展示了相关性在两个下游任务中的影响：LLM作为评判员评估和招聘——后者反映了关于算法同质化的理论预测。|
|**2025-06-06**|**Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias**|Yuanzhe Hu et.al.|[2506.06280](http://arxiv.org/abs/2506.06280)|null|近年来，通过权重矩阵的特征谱来诊断深度神经网络（DNNs）已成为一个活跃的研究领域。从宏观上讲，DNNs的特征谱分析涉及测量权重矩阵的经验谱密度（ESD）的重尾性。这有助于了解模型的训练效果，并指导分配更好的层级训练超参数。在本文中，我们解决了一种与特征谱方法相关联的挑战：权重矩阵的纵横比对估计重尾性指标的影响。我们证明，不同尺寸（和纵横比）的矩阵在估计重尾性指标时引入了不可忽视的偏差，导致模型诊断和层级超参数分配不准确。为了克服这一挑战，我们提出了FARMS（固定纵横比矩阵子采样）方法，该方法通过固定纵横比的子矩阵采样来归一化权重矩阵。我们不是测量原始ESD的重尾性，而是测量这些子采样子矩阵的平均ESD。我们表明，使用固定纵横比测量这些子矩阵的重尾性可以有效地减轻纵横比偏差。我们在涉及权重特征谱分析的多种优化技术和应用领域中对我们的方法进行了验证，包括计算机视觉（CV）模型中的图像分类、科学机器学习（SciML）模型训练和大型语言模型（LLM）剪枝。我们的结果表明，尽管FARMS方法简单，但它在这类应用领域中统一提高了特征谱分析的准确性，并实现了更有效的层级超参数分配。在一个LLM剪枝实验中，与最先进的方法相比，FARMS将LLaMA-7B模型的不确定性降低了17.3%。|
|**2025-06-06**|**CoMemo: LVLMs Need Image Context with Image Memory**|Shi Liu et.al.|[2506.06279](http://arxiv.org/abs/2506.06279)|null|近年来，基于大型语言模型的大型视觉-语言模型的发展将视觉特征与大型语言模型表示的对应作为主导范式。然而，继承自大型语言模型的设计引入了多模态处理中的次优特性。首先，LVLMs在注意力分配上呈现出双模态分布，导致随着上下文扩展，中间视觉内容的渐进忽视。其次，传统的位置编码方案在处理动态高分辨率图像时未能保留关键的二维结构关系。为了解决这些限制，我们提出了CoMemo——一个双路径架构，它结合了上下文图像路径和图像记忆路径进行视觉处理，有效缓解了视觉信息的忽视。此外，我们引入了RoPE-DHR，这是一种新颖的位置编码机制，它采用基于缩略图的位置聚合来维持二维空间意识，同时减轻了扩展序列中的远程衰减。在包括长上下文理解、多图像推理和视觉问答在内的七个基准上的评估表明，与传统的LVLM架构相比，CoMemo表现出优异的性能。项目页面可在https://lalbj.github.io/projects/CoMemo/找到。|
|**2025-06-06**|**AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization**|Mukur Gupta et.al.|[2506.06273](http://arxiv.org/abs/2506.06273)|null|大型语言模型（LLMs）在文本摘要任务中取得了令人印象深刻的性能，并越来越多地应用于实际应用中。然而，这些系统通常会从预训练数据中继承关联和框架偏见，导致在下游任务中产生不适当或不公平的输出。在本研究中，我们提出了AdvSumm（对抗性摘要），这是一个领域无关的训练框架，旨在通过改进泛化来减轻文本摘要中的偏见。受对抗鲁棒性的启发，AdvSumm引入了一个新的扰动组件，在序列到序列模型的嵌入层应用梯度引导的扰动，增强了模型对输入变动的鲁棒性。我们通过实验证明，AdvSumm有效地减少了摘要中不同类型的偏见——特别是名字-国籍偏见和政治框架偏见——而不损害摘要质量。与标准变压器和数据增强技术如反向翻译相比，AdvSumm在基准数据集上实现了更强的偏见缓解性能。|
|**2025-06-06**|**Cartridges: Lightweight and general-purpose long context representations via self-study**|Sabri Eyuboglu et.al.|[2506.06266](http://arxiv.org/abs/2506.06266)|null|大型语言模型通常通过将整个语料库放入上下文窗口并利用上下文学习（ICL）来回答基于大型文本语料库（例如代码库、法律文件或聊天记录）的查询。尽管当前模型支持100K-1M个标记的上下文，但由于KV缓存的内存消耗与输入长度成比例，这种设置在服务时成本高昂。我们探索了一种替代方案：在每个语料库上离线训练一个较小的KV缓存。在推理时，我们加载这个训练好的KV缓存，我们称之为“Cartridge”，并解码一个响应。关键的是，Cartridge的训练成本可以分摊到所有引用同一语料库的查询上。然而，我们发现使用语料库上的下一个标记预测来训练Cartridge的简单方法与ICL不具竞争力。相反，我们提出了“自学习”，这是一种训练配方，其中我们生成关于语料库的合成对话，并使用上下文蒸馏目标来训练Cartridge。我们发现，使用自学习训练的Cartridge可以复制ICL的功能，同时服务成本显著降低。在具有挑战性的长上下文基准测试中，使用自学习训练的Cartridge在内存使用上减少了38.6倍，同时实现了26.4倍更高的吞吐量。自学习还扩展了模型的有效上下文长度（例如，在MTOB上从128k增加到484k个标记），并且出人意料地，它还导致在推理时可以组合而不需要重新训练的Cartridge。|
|**2025-06-06**|**PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time**|Weizhi Zhang et.al.|[2506.06254](http://arxiv.org/abs/2506.06254)|null|大型语言模型（LLM）赋能的智能体最近作为一种先进的范式出现，在众多领域和任务中展现出令人印象深刻的能力。尽管它们具有潜力，但当前的LLM智能体通常采用一刀切的方法，缺乏灵活性，无法满足用户不断变化的需求和偏好。这一局限性促使我们开发出名为PersonaAgent的个性化LLM智能体框架，旨在解决各种个性化任务。具体来说，PersonaAgent集成了两个互补的组件——一个包含情景记忆和语义记忆机制的个性化记忆模块；一个能够使智能体执行针对用户定制的工具动作的个性化动作模块。在核心层面，个性（定义为每个用户的独特系统提示）充当一个中介：它利用个性化记忆中的见解来控制智能体动作，而这些动作的结果反过来又完善了记忆。基于该框架，我们提出了一种测试时用户偏好对齐策略，通过模拟最新的n次交互来优化个性提示，确保通过模拟响应和真实响应之间的文本损失反馈实现实时用户偏好对齐。实验评估表明，PersonaAgent不仅有效地个性化了动作空间，而且在测试时的实际应用中实现了扩展，显著优于其他基线方法。这些结果强调了我们的方法在提供定制化、动态用户体验方面的可行性和潜力。|
|**2025-06-06**|**DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation**|Jingyu Xiao et.al.|[2506.06251](http://arxiv.org/abs/2506.06251)|**[link](https://github.com/webpai/designbench)**|多模态大型语言模型（MLLMs）在自动化前端工程领域展现了令人瞩目的能力，例如从视觉设计中生成UI代码。然而，现有的前端UI代码生成基准存在以下局限性：（1）尽管基于框架的开发在现代前端编程中占据主导地位，但当前的基准未能融入主流的开发框架。（2）现有的评估仅专注于UI代码生成任务，而实际的UI开发涉及多个迭代，包括编辑优化和修复问题。（3）当前的基准采用单维评估，缺乏对任务难度、输入上下文变化等影响因素的调查以及深入的代码级别分析。为了填补这些空白，我们引入了DesignBench，这是一个多框架、多任务的评估基准，用于评估MLLM在自动化前端工程方面的能力。DesignBench包含三个广泛使用的UI框架（React、Vue和Angular）以及纯HTML/CSS，并在实际开发工作流程中的三个基本前端任务（生成、编辑和修复）上进行评估。DesignBench包含900个网页样本，涵盖11个主题、9种编辑类型和6种问题类别，能够对MLLM的性能从多个维度进行详细分析。我们的系统评估揭示了MLLM在特定框架中的局限性、与任务相关的瓶颈以及在不同条件下的性能变化，为自动化前端开发领域的未来研究提供了指导。我们的代码和数据可在https://github.com/WebPAI/DesignBench上获得。|
|**2025-06-06**|**Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models**|Zahra Babaiee et.al.|[2506.06242](http://arxiv.org/abs/2506.06242)|null|近期，多模态大型语言模型在视觉问答领域的进步推动了突破。然而，一个关键差距仍然存在，即“概念化”——在视觉形式变化的情况下识别和推理同一概念的能力，这是人类推理的基本能力。为了应对这一挑战，我们引入了视觉图竞技场（VGA），这是一个包含六个基于图的任务的数据集，旨在评估和提升人工智能系统在视觉抽象方面的能力。VGA使用不同的图布局（例如，Kamada-Kawai与平面布局）来测试独立于视觉形式的推理。与最先进的视觉模型和多模态大型语言模型的实验揭示了惊人的差异：人类在所有任务上实现了接近完美的准确率，而模型在同构检测任务上完全失败，在路径/循环任务上只有有限的成功。我们进一步识别出行为异常，表明这是伪智能的匹配模式而非真正的理解。这些发现强调了当前视觉理解人工智能模型的基本局限性。通过隔离表示不变推理的挑战，VGA提供了一个框架，以推动人工智能视觉模型向类似人类的概念化发展。视觉图竞技场可在以下网址获取：\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}|
|**2025-06-06**|**Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge**|Yi Sui et.al.|[2506.06240](http://arxiv.org/abs/2506.06240)|null|检索增强生成（RAG）是一种通过将检索到的外部知识融入生成过程来降低大型语言模型（LLMs）幻觉成本效益高的方法。然而，外部知识可能与LLMs的参数知识产生冲突。此外，当前的LLMs缺乏解决这种知识冲突的内在机制，导致传统的RAG方法性能和稳定性下降。因此，我们提出了一种双流知识增强框架，用于共享-私有语义协同（DSSP-RAG）。框架的核心是一种新颖的方法，它将自注意力细化为混合注意力，区分共享和私有语义，以实现受控的内部-外部知识整合。为了有效地促进RAG中的DSSP，我们进一步引入了一种基于认知不确定性的无监督幻觉检测方法，确保引入知识的必要性，并基于注意力差异矩阵的能耗商（EQ）以减少检索到的外部知识中的噪声。在基准数据集上的大量实验表明，DSSP-RAG可以有效解决冲突并增强双流知识的互补性，从而在强大基线之上表现出优异的性能。|
|**2025-06-06**|**CompilerGPT: Leveraging Large Language Models for Analyzing and Acting on Compiler Optimization Reports**|Peter Pirkelbauer et.al.|[2506.06227](http://arxiv.org/abs/2506.06227)|null|当前编译器优化报告通常呈现复杂、技术性信息，对程序员来说难以理解和有效行动。本文评估了大型语言模型（LLM）理解编译器优化报告并据此自动重写代码的能力。为此，本文介绍了一种名为CompilerGPT的新框架，该框架自动化了编译器、LLM和用户定义的测试与评估工具之间的交互。CompilerGPT的工作流程运行多个迭代并报告获得的结果。使用两个领先的LLM模型（GPT-4o和Claude Sonnet）、两个编译器（Clang和GCC）的优化报告以及五个基准代码进行的实验展示了这种方法的优势。虽然并非在每次测试中都一致，但获得了高达6.5倍的加速。这种方法有望提高编译器的可用性并简化软件优化过程。|
|**2025-06-06**|**PROVSYN: Synthesizing Provenance Graphs for Data Augmentation in Intrusion Detection Systems**|Yi Huang et.al.|[2506.06226](http://arxiv.org/abs/2506.06226)|null|源图分析在入侵检测中扮演着至关重要的角色，尤其是在对抗高级持续性威胁（APTs）时，它能够揭示复杂的攻击模式。尽管最近的一些系统结合了图神经网络（GNNs）和自然语言处理（NLP）来捕捉结构和语义特征，但它们的效果受到现实世界数据中类别不平衡的限制。为了解决这个问题，我们引入了PROVSYN，这是一个自动化的框架，通过三个阶段的管道合成源图：（1）使用结构-语义建模进行异构图结构合成，（2）基于规则的拓扑优化，以及（3）利用大型语言模型（LLMs）进行上下文感知的文本属性合成。PROVSYN包含一个全面的评估框架，该框架整合了结构、文本、时间和基于嵌入的指标，以及一个语义验证机制，以评估生成攻击模式和系统行为的正确性。为了展示其实际效用，我们使用合成的图来增强下游APTs检测模型的训练数据集。实验结果表明，PROVSYN能够生成高保真度的图，并通过有效的数据增强提高了检测性能。|
|**2025-06-05**|**SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs**|Jiahui Wang et.al.|[2506.05344](http://arxiv.org/abs/2506.05344)|**[link](https://github.com/cr400af-a/sparsemm)**|多模态大型语言模型（MLLMs）通常是通过扩展预训练的大型语言模型（LLMs）以增加视觉能力而得到的。在本工作中，我们通过分析其注意力机制来研究MLLMs如何处理视觉输入。我们揭示了一个令人惊讶的稀疏现象：LLMs中只有一小部分注意力头（大约少于5%）在视觉理解中起到积极的作用，我们将其称为视觉头。为了高效地识别这些头，我们设计了一个无需训练的框架，该框架通过有针对性的响应分析来量化头级视觉相关性。基于这一发现，我们引入了SparseMM，这是一种基于KV-Cache的优化策略，它根据视觉评分对LLMs中的头分配不对称的计算预算，利用视觉头的稀疏性来加速MLLMs的推理。与忽略视觉特性的先前KV-Cache加速方法相比，SparseMM在解码过程中优先考虑压力和保留视觉语义。在主流的多模态基准上的广泛评估表明，SparseMM实现了更优的准确性与效率之间的权衡。值得注意的是，SparseMM在生成过程中实现了1.38倍的实时加速和52%的内存减少，同时在效率测试中保持了性能一致性。我们的项目已在https://github.com/CR400AF-A/SparseMM开源。|
|**2025-06-05**|**Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets**|Lei Hsiung et.al.|[2506.05346](http://arxiv.org/abs/2506.05346)|null|近期大型语言模型（LLMs）的进展突显了它们在下游微调时易受安全对齐破解的脆弱性。然而，现有的缓解策略主要关注在安全防护措施被破坏后，在微调过程中移除有害梯度或持续强化安全对齐。因此，它们往往忽略了一个关键的上游因素：原始安全对齐数据的作用。因此，本文通过分析上游对齐数据集与下游微调任务之间的表示相似性，探讨了安全防护措施的退化。我们的实验表明，这些数据集之间的高度相似性会显著削弱安全防护措施，使模型更容易受到破解。相反，这两种类型数据集之间的低相似性会产生更健壮的模型，从而将有害性评分降低高达10.33%。通过强调上游数据集设计在构建持久安全防护措施和降低对破解攻击的现实中脆弱性的重要性，这些发现为微调服务提供商提供了可操作的见解。|
|**2025-06-05**|**Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning**|Xingjian Ran et.al.|[2506.05341](http://arxiv.org/abs/2506.05341)|null|真实3D室内场景合成对于具身AI和数字内容创作至关重要。它可以自然地分为两个子任务：物体生成和布局生成。虽然最近的生成模型在物体级别的质量和可控性方面取得了显著进展，但由于数据集有限，布局生成仍然具有挑战性。现有方法要么对这些数据集过度拟合，要么依赖于预定义的约束来优化牺牲灵活性的数值布局。因此，它们无法生成既具有开放词汇表又符合细粒度用户指令的场景。我们引入了DirectLayout，这是一个框架，它利用大型语言模型（LLMs）的可迁移空间推理，直接从文本描述中生成数值3D布局。DirectLayout将生成过程分解为三个阶段：生成鸟瞰图（BEV）布局，将其提升到3D空间，并细化物体放置。为了实现显式的空间推理并帮助模型掌握物体放置的基本原则，我们采用基于3D-Front数据集的思维链（CoT）激活。此外，我们设计了CoT-Grounded生成布局奖励，以增强泛化和空间规划。在推理过程中，DirectLayout通过上下文学习中的迭代资产-布局对齐来解决资产布局不匹配问题。大量实验表明，DirectLayout实现了令人印象深刻的语义一致性、泛化和物理合理性。|
|**2025-06-05**|**VideoMolmo: Spatio-Temporal Grounding Meets Pointing**|Ghazi Shazan Ahmad et.al.|[2506.05336](http://arxiv.org/abs/2506.05336)|**[link](https://github.com/mbzuai-oryx/videomolmo)**|空间时间定位对于从生物研究到自主导航和交互界面的各种领域的精确交互至关重要。当前基于视频的方法虽然在跟踪方面很擅长，但缺乏大型语言模型的复杂推理能力，限制了它们的上下文理解和泛化能力。我们引入了VideoMolmo，这是一个针对基于文本描述的细粒度空间时间指向的大多模态模型。在Molmo架构的基础上，VideoMolmo引入了一个时间模块，利用注意力机制对每一帧进行条件化，以确保时间一致性。此外，我们新颖的时间掩码融合流程采用SAM2进行双向点传播，显著增强了视频序列之间的连贯性。这种两步分解，即首先使用语言模型生成精确的指向坐标，然后依靠顺序掩码融合模块生成连贯的分割，不仅简化了语言模型的任务，也增强了可解释性。由于缺乏合适的数据集，我们精心制作了一个包含72k个视频-字幕对的数据集，这些视频-字幕对标注了10万个物体点。为了评估VideoMolmo的泛化能力，我们引入了VPoS-Bench，这是一个跨越五个真实世界场景的具有挑战性的分布外基准：细胞追踪、自视角视觉、自动驾驶、视频GUI交互和机器人学。我们还对我们的模型在指称视频对象分割（Refer-VOS）和推理VOS任务上的表现进行了评估。与现有模型相比，VideoMolmo在空间时间指向准确性和推理能力方面有了显著提高。我们的代码和模型在https://github.com/mbzuai-oryx/VideoMolmo上公开可用。|
|**2025-06-05**|**Search Arena: Analyzing Search-Augmented LLMs**|Mihran Miroyan et.al.|[2506.05334](http://arxiv.org/abs/2506.05334)|**[link](https://github.com/lmarena/search-arena)**|搜索增强语言模型结合网络搜索与大型语言模型（LLM）以提高响应的基于事实性和新颖性。然而，分析这些系统仍然具有挑战性：现有的数据集在规模上有限，范围狭窄，通常局限于静态的、单轮的、事实核查问题。在这项工作中，我们介绍了Search Arena，这是一个超过24,000对用户与搜索增强LLM的多轮用户交互的众包、大规模、人工偏好数据集。该数据集覆盖了多种意图和语言，并包含大约12,000个人类偏好投票的系统追踪记录。我们的分析表明，用户的偏好受引用次数的影响，即使在引用的内容并不直接支持所赋予的声明时也是如此，这揭示了感知可信度与实际可信度之间的差距。此外，用户偏好因引用来源而异，表明社区驱动平台通常更受欢迎，而静态的百科全书式来源并不总是适当和可靠的。为了评估不同环境下的性能，我们通过在通用聊天环境中测试搜索增强LLM和传统LLM在搜索密集型设置中的表现进行了跨领域分析。我们发现，网络搜索不会降低性能，甚至可能提高非搜索环境中的性能；然而，如果仅仅依赖模型参数知识，搜索环境中的质量会受到显著影响。我们将数据集开源，以支持未来在这方面的研究。我们的数据集和代码可在以下链接获取：https://github.com/lmarena/search-arena。|
|**2025-06-05**|**MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning**|Xinyan Chen et.al.|[2506.05331](http://arxiv.org/abs/2506.05331)|**[link](https://github.com/xinyan-cxy/mint-cot)**|思维链（CoT）在大型语言模型（LLMs）中广泛增强了数学推理，但它仍然难以扩展到多模态领域。现有工作要么采用与图像输入相似的文本推理，要么试图将视觉信号交织到数学CoT中。然而，它们在解决数学问题时面临三个关键限制：依赖于粗粒度的矩形图像区域、视觉编码器对数学内容的感知有限，以及依赖于外部能力的视觉修改。在本文中，我们提出了MINT-CoT，即用于思维链视觉推理的数学交织标记。MINT-CoT通过交织标记自适应地将相关视觉标记插入到文本推理步骤中，该交织标记可以动态选择数学图形中任何形状的视觉区域。为了赋予这一能力，我们构建了MINT-CoT数据集，其中包含54K个数学问题，每个推理步骤都与标记级别的视觉区域相对应，并伴随有严格的数据生成流程。我们进一步提出了一种三阶段MINT-CoT训练策略，逐步结合纯文本CoT SFT、交织CoT SFT和交织CoT RL，从而得到我们的MINT-CoT-7B模型。大量实验表明，我们的方法在数学领域中进行有效视觉交织推理的有效性，其中MINT-CoT-7B在MathVista上优于基线模型+34.08%，在GeoQA上+28.78%，在MMStar上+23.2%。我们的代码和数据可在https://github.com/xinyan-cxy/MINT-CoT上找到。|
|**2025-06-05**|**Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay**|Yifan Sun et.al.|[2506.05316](http://arxiv.org/abs/2506.05316)|null|强化学习（RL）已成为微调大型语言模型（LLMs）的有效方法，尤其是增强其推理能力。然而，RL微调在资源消耗上仍然非常高，而现有工作在很大程度上忽略了数据效率问题。在本文中，我们提出了两种提高LLM RL微调数据效率的技术：难度目标在线数据选择和回放重用。我们引入自适应难度的概念来指导在线数据选择，优先选择难度适中的问题，这些问题更有可能产生有信息量的学习信号。为了高效地估计自适应难度，我们开发了一个基于注意力的框架，该框架只需要一小部分参考问题进行回放。然后，根据这些剩余问题与该集的相似性来估计它们的自适应难度。为了进一步降低回放成本，我们引入了一种回放重用机制，该机制重用最近的回放，在保持稳定更新的同时降低每步的计算量。在6个LLM数据集组合上进行的大量实验表明，我们的方法将RL微调时间减少了25%到65%，以达到与原始GRPO算法相同的表现水平。|
|**2025-06-05**|**Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models**|Taha Entesari et.al.|[2506.05314](http://arxiv.org/abs/2506.05314)|null|在现实场景中部署的大型语言模型（LLMs）越来越需要忘记敏感、过时或专有信息。现有的忘记方法通常将忘记和保留视为正则化的权衡，将这两个目标结合成一个标量化的损失函数。这通常会导致优化不稳定以及在保留数据上的性能下降，尤其是在激进忘记的情况下。我们提出了一种新的LLM忘记方法，将其作为一种约束优化问题进行表述：通过一种新的logit边际扁平化损失来强制执行忘记，该损失明确地将输出分布推向指定忘记集上的均匀性，而通过在另一个保留集上的硬约束来保留保留。与基于熵的目标相比，我们的损失函数无需softmax，数值稳定，并保持非消失梯度，从而实现更高效和稳健的优化。我们使用一种可扩展的原-对偶算法来解决约束问题，该算法通过对偶变量的动力学揭示忘记和保留之间的权衡。在TOFU和MUSE基准测试中，针对不同LLM架构的评估表明，我们的方法在一致性上匹配或超过了最先进的基线，有效地移除了目标信息，同时保留了下游效用。|
|**2025-06-05**|**ProRefine: Inference-time Prompt Refinement with Textual Feedback**|Deepak Pandita et.al.|[2506.05305](http://arxiv.org/abs/2506.05305)|null|代理工作流程，即多个AI代理协作完成复杂任务（如推理或规划）正变得越来越普遍。然而，这些工作流程往往因设计不良的提示而出现错误传播和次优性能，这主要是因为这些提示未能有效指导各个代理。这是一个关键问题，因为它限制了这些强大系统的可靠性和可扩展性。我们介绍了ProRefine，这是一种创新的推理时提示优化方法，它利用大型语言模型（LLMs）的文本反馈来解决这一挑战。ProRefine在无需额外训练或真实标签的情况下，动态优化多步骤推理任务的提示。在五个基准数学推理数据集上评估表明，ProRefine比零样本思维链基线提高了3到37个百分点。这种方法不仅提高了准确性，还允许较小的模型匹配较大模型的性能，凸显了其在高效和可扩展AI部署以及使高性能AI获得普及方面的潜力。|
|**2025-06-05**|**Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos**|Weifeng Lin et.al.|[2506.05302](http://arxiv.org/abs/2506.05302)|null|我们提出了感知任何模型（PAM），这是一个概念简单且高效的框架，用于图像和视频中的综合区域级视觉理解。我们的方法通过集成大型语言模型（LLMs）扩展了强大的分割模型SAM 2，使得在对象分割的同时，可以生成多样化的、特定区域的语义输出，包括类别、标签定义、功能解释和详细描述。一个关键组件，语义感知器，被引入以高效地将SAM 2丰富的视觉特征转换为多模态标记，这些特征本质上携带了通用视觉、定位和语义先验。为了支持稳健的多粒度理解，我们还开发了一个专门的数据精炼和增强管道，生成了包含150万图像和60万视频区域语义标注的高质量数据集，包括新颖的区域级流视频描述数据。PAM设计轻量级且高效，同时在各种区域理解任务上表现出强大的性能。它比以往的方法快1.2-2.4倍，并且消耗的GPU内存更少，为现实世界应用提供了实用解决方案。我们相信，我们有效的方法将成为未来区域级视觉理解研究的一个强大的基线。|
|**2025-06-04**|**Language-Image Alignment with Fixed Text Encoders**|Jingfeng Yang et.al.|[2506.04209](http://arxiv.org/abs/2506.04209)|null|目前，建立语言-图像对齐最占主导地位的方法是通过对比学习联合预训练文本和图像编码器，例如CLIP及其变体。在本工作中，我们质疑这种代价高昂的联合训练是否必要。特别是，我们研究是否一个预训练的固定大型语言模型（LLM）提供了一个足够好的文本编码器来指导视觉表示学习。也就是说，我们提出通过仅训练图像编码器来从LLM学习具有固定文本编码器的语言-图像对齐（LIFT）。出人意料的是，通过全面的基准测试和消融研究，我们发现这种大大简化的框架LIFT非常有效，并且在涉及组合理解和长描述的场景中，LIFT在大多数情况下都优于CLIP，同时在计算效率上实现了显著的提升。我们的工作迈出了系统探索LLM中的文本嵌入如何指导视觉学习的第一步，并为学习语言对齐的视觉表示提供了一种替代的设计选择。|
|**2025-06-04**|**Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning**|Shuang Chen et.al.|[2506.04207](http://arxiv.org/abs/2506.04207)|null|受Deepseek-R1在复杂文本任务中出色推理能力启发，许多研究试图通过直接应用强化学习（RL）来激励多模态大型语言模型（MLLMs）具有类似的能力。然而，它们仍然难以激活复杂的推理。在本文中，我们不仅独立考察了多模态RL，还深入分析了当前的训练流程，并识别出三个关键现象：1）有效的冷启动初始化对于增强MLLM推理至关重要。有趣的是，我们发现仅使用精心选择的文本数据进行初始化就能在多模态RL之前，超越许多最近的多模态推理模型。2）应用于多模态RL的标准GRPO（梯度下降正则化优化器）存在梯度停滞问题，这会降低训练的稳定性和性能。3）在多模态RL阶段之后，后续仅使用文本的RL训练进一步增强了多模态推理。这种分阶段训练方法有效地平衡了感知基础和认知推理发展。通过结合上述见解并解决多模态RL问题，我们引入了ReVisual-R1，在包括MathVerse、MathVision、WeMath、LogicVista、DynaMath等挑战性基准测试以及具有挑战性的AIME2024和AIME2025中，实现了开源7B MLLMs中的新突破。|
|**2025-06-04**|**EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation**|Jinghan Jia et.al.|[2506.04205](http://arxiv.org/abs/2506.04205)|**[link](https://github.com/optml-group/epic)**|大型语言模型（LLMs）在经过思维链（CoT）监督训练后展现出令人瞩目的推理能力。然而，长篇累牍且冗长的CoT轨迹，特别是那些从大型推理模型（LRMs）如DeepSeek-R1中提炼出来的轨迹，在蒸馏过程中会显著增加训练成本，因为此时一个非推理基模型被教导来复制LRM的推理行为。在这项工作中，我们研究了资源高效推理训练中的CoT浓缩问题，旨在修剪CoT轨迹中的中间推理步骤（即思维），使得在长度缩短的CoT数据上能够进行监督模型训练，同时保留答案准确性和模型的连贯推理能力。我们的理由是，CoT轨迹通常遵循三个阶段的结构：问题理解、探索和解决方案收敛。通过实证分析，我们发现保留推理轨迹的结构，特别是问题理解的早期阶段（富含反思线索）和解决方案收敛的最终阶段，足以实现无损推理监督。为此，我们提出了一种边缘保留浓缩方法，称为EPiC，该方法仅选择性地保留每个CoT轨迹的初始和最终部分，而丢弃中间部分。这种设计将保留推理轨迹的“边缘”进行类比，捕捉初始问题框架和最终答案综合，以维持逻辑连贯性。在多个模型家族（Qwen和LLaMA）和基准测试中的实验表明，EPiC将训练时间减少了超过34%，同时在MATH500上实现了无损推理准确性，与完整的CoT监督相当。据我们所知，这是首次探索用于高效推理模型蒸馏的思维级CoT浓缩的研究。|
|**2025-06-04**|**Cascadia: A Cascade Serving System for Large Language Models**|Youhe Jiang et.al.|[2506.04203](http://arxiv.org/abs/2506.04203)|null|近期在大语言模型（LLMs）领域的进展使得既要提供快速响应又要保证高质量答案的需求变得更加迫切。更强大的模型能产生更好的结果，但推理延迟更高，而较小的模型虽然速度快但能力有限。近期的研究提出了使用模型级联来平衡延迟-质量权衡，即将简单的查询路由到较小的模型，将复杂的查询路由到较大的模型。然而，实现高效的级联服务仍然具有挑战性。当前的框架缺乏有效机制来处理（i）不同LLMs的巨大且不断变化的资源需求，（ii）LLM工作负载的固有异质性，（iii）系统部署和路由策略的联合优化。受这些观察的启发，我们引入了Cascadia，这是一个新型级联服务框架，专门设计用于调度请求路由并部署模型级联，以实现快速且保持质量的语言模型服务。Cascadia采用双层优化方法：在内层，它使用混合整数线性规划来根据LLM信息和工作负载特征选择资源分配和并行策略；在外层，它应用加权切比雪夫算法来迭代地联合优化内层产生的路由策略和系统部署。我们对多样化的工作负载轨迹和不同的模型级联（DeepSeek和Llama系列）进行了广泛的评估，结果表明Cascadia在性能上显著优于单模型部署和最先进的级联服务基线，实现了高达4倍（平均2.3倍）更紧的延迟服务等级协议（SLOs）和高达5倍（平均2.4倍）更高的吞吐量，同时保持了目标答案质量。|
|**2025-06-04**|**TracLLM: A Generic Framework for Attributing Long Context LLMs**|Yanting Wang et.al.|[2506.04202](http://arxiv.org/abs/2506.04202)|**[link](https://github.com/wang-yanting/tracllm)**|长上下文大型语言模型（LLMs）被应用于许多实际应用中，如RAG、智能体和广泛的LLM集成应用。给定一个指令和长上下文（例如，文档、PDF文件、网页），长上下文LLM可以生成基于提供上下文的输出，旨在提供更准确、更新、可验证的输出，同时减少幻觉和不支持的说法。这引发了一个研究问题：如何确定上下文中（例如，句子、段落或段落）对LLM生成的输出贡献最大或负责的文本？这个过程，我们称之为上下文回溯，有各种实际应用，例如1）调试基于LLM的系统，2）对LLM进行攻击后的攻击后法医分析（例如，提示注入攻击、知识篡改攻击），以及3）突出知识来源以增强用户对LLM生成的输出的信任。当应用于长上下文LLM的上下文回溯时，现有的特征归因方法（如Shapley）表现不佳，或者需要很大的计算成本。在这项工作中，我们开发了TracLLM，这是第一个针对长上下文LLM的通用上下文回溯框架。我们的框架可以提高现有特征归因方法的有效性和效率。为了提高效率，我们在TracLLM中开发了一种基于信息的搜索算法。我们还开发了贡献分数集成/去噪技术来提高TracLLM的准确性。我们的评估结果表明，TracLLM可以有效地识别导致LLM输出的长上下文中的文本。我们的代码和数据在：https://github.com/Wang-Yanting/TracLLM。|
|**2025-06-04**|**R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning**|Qingfei Zhao et.al.|[2506.04185](http://arxiv.org/abs/2506.04185)|**[link](https://github.com/qingfei1/r-search)**|大语言模型（LLMs）在多步和长链推理方面取得了显著进展。然而，将它们的推理能力扩展到与搜索的深度交互仍然是一个非平凡挑战，因为模型往往无法识别最优的推理-搜索交互轨迹，从而导致次优的响应。我们提出了R-Search，这是一种新的强化学习框架，用于推理-搜索集成，旨在使LLMs能够自主执行多步推理并深度搜索交互，通过多奖励信号学习最优的推理搜索交互轨迹，从而在复杂逻辑和知识密集型任务中提高响应质量。R-Search引导LLM动态决定何时检索或推理，同时在全局层面整合关键证据，以增强推理和搜索之间的深度知识交互。在强化学习训练过程中，R-Search提供多阶段、多类型的奖励，以联合优化推理-搜索轨迹。在七个数据集上的实验表明，R-Search的性能优于先进的RAG基线，最高提升可达32.2%（领域内）和25.1%（领域外）。代码和数据可在https://github.com/QingFei1/R-Search获取。|
|**2025-06-04**|**SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models**|Yuhao Wu et.al.|[2506.04180](http://arxiv.org/abs/2506.04180)|null|长文本生成对于大型语言模型（LLM）来说仍然是一个重大挑战，尤其是在保持连贯性、确保逻辑一致性和随着序列长度增加保持文本质量方面。为了解决这些局限性，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。SuperWriter-Agent将显式的结构化思考和规划与细化阶段引入生成流程，引导模型遵循更谨慎和认知基础的流程，类似于专业作家的写作过程。基于这个框架，我们构建了一个监督微调数据集来训练一个70亿的SuperWriter-LM。我们进一步开发了一个使用蒙特卡洛树搜索（MCTS）来传播最终质量评估和相应优化每个生成步骤的分层直接偏好优化（DPO）程序。在多个基准测试中的实证结果表明，SuperWriter-LM实现了最先进的性能，甚至在自动评估和人工评估中超越了更大规模的基线模型。此外，全面的消融研究表明分层DPO的有效性，并强调了将结构化思考步骤纳入其中以改进长文本生成质量的必要性。|
|**2025-06-04**|**SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling**|Anhao Zhao et.al.|[2506.04179](http://arxiv.org/abs/2506.04179)|null|大语言模型（LLMs）在众多任务上取得了显著性能，但由于其深度多层架构，也带来了巨大的计算成本。层剪枝已成为一种缓解这些低效的策略，但传统的静态剪枝方法忽略了LLM推理中两个关键的内在动态：一是水平动态，其中标记级异质性要求具有上下文感知的剪枝决策；二是垂直动态，其中MLP和自注意力层的不同功能角色需要组件特定的剪枝策略。我们引入了SkipGPT，这是一种动态层剪枝框架，旨在通过两种核心创新优化计算资源分配：（1）全局标记感知路由，优先处理关键标记；（2）MLP和自注意力组件解耦的剪枝策略。为了缓解训练不稳定，我们提出了一种两阶段优化范式：首先，一个解耦的训练阶段，通过软参数化学习路由策略以避免过早的剪枝决策；随后是参数高效的LoRA微调，以恢复由层移除影响到的性能。大量的实验表明，SkipGPT减少了超过40%的模型参数，同时在基准测试中与原始密集模型匹配或超越其性能。通过协调动态效率与保留的表达能力，SkipGPT推进了可扩展、资源感知LLMs的实用部署。我们的代码在以下链接公开可用：https://github.com/EIT-NLP/SkipGPT。|
|**2025-06-04**|**Does Prompt Design Impact Quality of Data Imputation by LLMs?**|Shreenidhi Srinivasan et.al.|[2506.04172](http://arxiv.org/abs/2506.04172)|null|生成逼真的合成表格数据是机器学习中的一个关键挑战。当这些数据包含类别不平衡问题时，这又增加了一层复杂性。本文提出了一种新颖的基于标记的数据填充方法，该方法利用大型语言模型在上下文学习方面的能力。这是通过结合结构化分组CSV风格提示技术和消除输入提示中无关的上下文信息来实现的。我们使用两个类别不平衡的二分类数据集来测试这种方法，并使用基于分类的评价指标来评估填充的有效性。实验结果表明，与基线提示相比，我们的方法显著减少了输入提示的大小，同时保持了或提高了填充质量，特别是在相对较小的数据集上。本文工作的贡献有两方面——1）它强调了在利用LLMs生成合成数据时，提示设计的重要性；2）它通过在计算约束内提供一个实用解决方案，解决了基于LLM的数据填充中类别不平衡数据集缺失数据的临界差距。我们希望我们的工作能够促进对利用LLMs和提示工程技术在合成数据生成中潜在能力的进一步研究和讨论。|
|**2025-06-04**|**VISCA: Inferring Component Abstractions for Automated End-to-End Testing**|Parsa Alian et.al.|[2506.04161](http://arxiv.org/abs/2506.04161)|null|为自动化的端到端（E2E）测试生成提供最优的上下文输入是一项重大挑战，而目前的方法未能充分解决这一局限。本文介绍了一种名为视觉-语义组件摘要器（VISCA）的新方法，该方法将网页转换为层次化、语义丰富的组件抽象。VISCA首先利用一种基于启发式的新分段方法将网页划分为候选片段。这些候选片段随后通过多模态语言模型（LLM）驱动的分析进行分类和上下文信息提取，从而将它们抽象为预定义的用户界面（UI）组件词汇表。这种以组件为中心的抽象比以往的方法提供了更有效的上下文基础，实现了更准确的特征推断和鲁棒的端到端测试用例生成。我们的评估表明，VISCA生成的测试用例平均特征覆盖率为92%，比最先进的基于LLM的E2E测试生成方法高出16%。|
|**2025-06-03**|**Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM**|Pralaypati Ta et.al.|[2506.03145](http://arxiv.org/abs/2506.03145)|null|神经科学研究出版物包含丰富的知识。从大量文献中准确检索现有信息并发现新的见解对于推动该领域的发展至关重要。然而，当知识分散在多个来源时，目前最先进的检索方法往往难以提取必要的信息。知识图谱（KG）可以整合和链接来自多个来源的知识，但现有的在神经科学中构建KG的方法通常依赖于标注数据并需要领域专业知识。为像神经科学这样的专门领域获取大规模、标注的数据面临着重大挑战。本研究提出了一种新颖的方法，利用大型语言模型（LLM）、神经科学本体和文本嵌入从未标注的大规模神经科学研究语料库中构建KG。我们分析了LLM识别的神经科学文本片段的语义相关性，用于构建知识图谱。我们还引入了一种实体增强的信息检索算法来从KG中提取知识。进行了几项实验来评估所提出的方法，结果表明，我们的方法显著提高了从未标注的神经科学研究语料库中获取知识的能力。在实体提取方面实现了0.84的F1分数，从KG中获得的知识提高了超过54%的问题的回答。|
|**2025-06-03**|**Not All Tokens Are Meant to Be Forgotten**|Xiangyu Zhou et.al.|[2506.03142](http://arxiv.org/abs/2506.03142)|null|大型语言模型（LLMs）在大量文本语料库上预训练后，展现出惊人的语言理解、推理和决策能力。然而，它们倾向于记忆不希望的信息，如私人或受版权保护的内容，引发重大的隐私和法律问题。忘却学习已成为一种有希望的解决方案，但现有方法面临过度遗忘的挑战。这个问题源于它们不加区分地抑制了忘却样本中所有标记的生成，导致模型效用的大量损失。为了克服这一挑战，我们引入了目标信息遗忘（TIF）框架，该框架包括：（1）一个灵活的目标信息标识器，旨在区分忘却样本中的不希望词汇（UW）和普通词汇（GW），以及（2）一种新颖的目标偏好优化方法，该方法利用对数偏好损失来忘却与UW相关的不希望信息，并利用保留损失来保留GW中的普通信息，从而有效提高忘却过程，同时减轻效用降低。在TOFU和MUSE基准上的大量实验表明，所提出的TIF框架提高了忘却的有效性，同时保留了模型效用，并实现了最先进的结果。|
|**2025-06-03**|**SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation**|Siqi Chen et.al.|[2506.03139](http://arxiv.org/abs/2506.03139)|null|大型语言模型（LLMs）和多模态LLMs在SVG处理方面展现出有潜力的能力，但现有的基准测试在现实世界覆盖范围、复杂度分层和评价范式上存在局限性。我们介绍了SVGenius，这是一个综合性的基准测试，包含2377个查询，涵盖了三个渐进维度：理解、编辑和生成。SVGenius基于来自24个应用领域的真实世界数据，具有系统性的复杂度分层，通过8个任务类别和18个指标评估模型。我们评估了涵盖不同规模、架构、训练范式和可访问级别的22个主流模型。我们的分析显示，虽然专有模型显著优于开源模型，但随着复杂度的增加，所有模型都表现出系统性的性能下降，表明当前方法存在根本性局限；然而，推理增强的训练比单纯的扩展更有效地克服这些局限，尽管风格迁移是所有模型类型中最具挑战性的能力。SVGenius建立了SVG处理的第一个系统评价框架，为开发更强大的矢量图形模型和推进自动化图形设计应用提供了关键见解。附录和补充材料（包括所有数据和代码）可在https://zju-real.github.io/SVGenius上找到。|
|**2025-06-03**|**Native-Resolution Image Synthesis**|Zidong Wang et.al.|[2506.03131](http://arxiv.org/abs/2506.03131)|null|我们引入了原生分辨率图像合成，这是一种新的生成建模范式，能够以任意分辨率和纵横比合成图像。这种方法通过原生处理可变长度的视觉标记，克服了传统固定分辨率、方形图像方法的局限性，这是传统技术的一个核心挑战。为此，我们引入了原生分辨率扩散Transformer（NiT），这是一种专为在去噪过程中显式建模变化分辨率和纵横比而设计的架构。摆脱了固定格式的约束，NiT能够从跨越广泛分辨率和纵横比的图像中学习内在视觉分布。值得注意的是，单个NiT模型在ImageNet-256x256和512x512基准测试上同时实现了最先进的性能。令人惊讶的是，类似于在高级大型语言模型中看到的鲁棒的零样本能力，仅训练于ImageNet的NiT展示了出色的零样本泛化性能。它成功地生成了之前未见的高分辨率（例如，1536 x 1536）和多种纵横比（例如，16:9、3:1、4:3）的图像，如图1所示。这些发现表明，原生分辨率建模作为视觉生成建模和高级LLM方法之间的桥梁具有重大潜力。|
|**2025-06-03**|**AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation**|Lu Qiu et.al.|[2506.03126](http://arxiv.org/abs/2506.03126)|null|近期人工智能生成内容（AIGC）的进步显著加快了动画制作进程。为了制作吸引人的动画，生成具有叙事脚本和角色参考的连贯多镜头视频片段至关重要。然而，现有的公共数据集主要关注具有全球描述的现实场景，缺乏用于一致角色指导的参考图像。为了填补这一空白，我们提出了AnimeShooter，一个参考引导的多镜头动画数据集。AnimeShooter具有全面的分层注释，并通过自动化流程实现了镜头间的强大视觉一致性。故事级别的注释提供了对叙事的概述，包括故事线、关键场景和主要角色简介，并附带参考图像；而镜头级别的注释将故事分解为连续的镜头，每个镜头都标注了场景、角色以及叙事和描述性的视觉字幕。此外，一个专门的子集AnimeShooter-audio为每个镜头提供了同步音频轨道，以及音频描述和声音来源。为了展示AnimeShooter的有效性并建立参考引导的多镜头视频生成任务的基线，我们引入了AnimeShooterGen，该系统利用了多模态大型语言模型（MLLM）和视频扩散模型。首先，参考图像和先前生成的镜头通过MLLM处理，生成既了解参考又了解上下文的表现，这些表现随后被用作扩散模型的条件以解码后续镜头。实验结果表明，在AnimeShooter上训练的模型实现了优越的跨镜头视觉一致性和对参考视觉指导的遵循，这突显了我们的数据集在连贯动画视频生成中的价值。|
|**2025-06-03**|**AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation**|Prashanth Vijayaraghavan et.al.|[2506.03122](http://arxiv.org/abs/2506.03122)|null|模拟电路拓扑综合对电子设计自动化（EDA）至关重要，它使得能够自动创建满足特定设计要求的电路结构。然而，庞大的设计搜索空间和严格的约束遵守使得高效综合变得具有挑战性。利用大型语言模型（LLMs）的灵活性，我们提出了一种基于强化学习（RL）的自动模拟电路综合框架AUTOCIRCUIT-RL。该框架分为两个阶段：指令调整阶段，其中LLM学习从编码设计约束的结构化提示中生成电路拓扑；以及RL优化阶段，该阶段使用评估有效性、效率和输出电压的奖励模型进一步改进指令调整后的模型。优化后的模型随后被直接用于生成满足设计约束的拓扑。实证结果表明，与最佳基线相比，AUTOCIRCUIT-RL生成的有效电路增加了约12%，效率提高了约14%，同时减少了约38%的重复生成率。它在仅用有限训练数据的情况下实现了超过60%的有效电路综合成功率，展示了强大的泛化能力。这些发现突显了该框架在扩展到复杂电路的同时保持效率和约束遵守的有效性，标志着人工智能驱动电路设计的重要进步。|
|**2025-06-03**|**Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback**|Xiaoying Zhang et.al.|[2506.03106](http://arxiv.org/abs/2506.03106)|null|近年来，在数值反馈（如标量奖励）方面的强化学习（RL）取得了显著进展，这大大提高了大型语言模型（LLMs）的复杂推理能力。尽管如此，我们发现仅凭数值反馈的RL存在三个关键挑战：性能瓶颈、自我反思的有效性有限以及持续的失败。随后，我们证明即使经过性能瓶颈，经过RL微调的模型也能通过利用以批评形式出现的自然语言反馈，在持续失败的难题上生成正确的改进。基于这一洞察，我们提出了Critique-GRPO，这是一个集成了自然语言和数值反馈的在线RL框架，用于有效的策略优化。Critique-GRPO使LLMs能够在保持探索的同时，同时从初始响应和批评引导的改进中学习。使用Qwen2.5-7B-Base和Qwen3-8B-Base进行的广泛实验表明，Critique-GRPO在八个具有挑战性的数学、STEM和一般推理任务中，始终优于基于监督学习和基于RL的微调方法，分别将平均pass@1分数提高了约4.5%和5%。值得注意的是，Critique-GRPO超越了包含在线RL中专家演示的强大基线。进一步的分析揭示了关于策略探索的两个关键见解：（1）更高的熵并不总是保证从探索中高效学习；（2）更长的响应并不一定导致更有效的探索。|
|**2025-06-03**|**TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models**|Chetwin Low et.al.|[2506.03099](http://arxiv.org/abs/2506.03099)|null|本文提出了一种名为TalkingMachines的高效框架，该框架将预训练的视频生成模型转化为实时、音频驱动的角色动画器。通过将音频大型语言模型（LLM）与我们的视频生成基础模型集成，TalkingMachines实现了自然的对话体验。我们的主要贡献包括：（1）将预训练的SOTA图像到视频DiT模型调整为18亿参数的音频驱动角色生成模型；（2）通过从双向教师模型到稀疏因果自回归学生模型的不对称知识蒸馏，实现无错误累积的无限视频流；（3）设计了一个高吞吐量、低延迟的推理管道，其中包含多个关键工程优化，如：（a）将DiT和VAE解码器解耦到不同的设备上，（b）使用CUDA流高效重叠设备间通信和计算，（c）消除冗余计算以最大化帧生成吞吐量。请在此查看演示视频 - https://aaxwaz.github.io/TalkingMachines/|
|**2025-06-03**|**EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models**|Mingzhe Li et.al.|[2506.03067](http://arxiv.org/abs/2506.03067)|null|文本到图像生成模型（例如，Stable Diffusion）取得了显著进步，使得根据文本描述创建高质量和逼真的图像成为可能。提示反转，即识别用于生成特定作品所用的文本提示的任务，在数据归属、模型溯源和水印验证等应用中具有重大潜力。最近的研究引入了一种延迟投影方案，以优化代表词汇空间的提示，尽管在语义流畅性和效率方面仍存在挑战。高级图像标题模型或视觉大型语言模型可以生成高度可解释的提示，但它们在图像相似度方面往往不足。在本文中，我们提出了一种针对文本到图像扩散模型的提示反转技术，称为\sys，该技术包括使用预训练的图像标题模型初始化嵌入，通过潜在空间中的反向工程进行细化，并使用嵌入到文本模型将其转换为文本。我们在广泛使用的数据集（如MS COCO、LAION和Flickr）上的实验表明，我们的方法在图像相似度、文本对齐、提示可解释性和泛化能力方面优于现有方法。我们进一步展示了我们生成的提示在跨概念图像合成、概念操纵、进化多概念生成和无监督分割等任务中的应用。|
|**2025-06-03**|**Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs**|Yuval Kansal et.al.|[2506.03051](http://arxiv.org/abs/2506.03051)|null|首先，我们需要理解摘要中的关键术语和概念：  - Factuality：事实性，指信息的准确性。 - Large Language Models (LLMs)：大型语言模型，如GPT-3、LLaMA等。 - Correctness：正确性，指信息的正确无误。 - Middle and high school students：中学生在校学生和高中生。 - Extraneous information：无关信息，指与问题无关的信息。 - Less truthful information：不太真实的信息，指不准确或错误的信息。 - Existing biases：现有偏见，指对某些群体或语言的固有偏见。  接下来，我们将摘要逐步翻译为中文：  1. Factuality is a necessary precursor to useful educational tools. - 事实性是有效教育工具的必要前提。 2. As adoption of Large Language Models (LLMs) in education continues of grow, ensuring correctness in all settings is paramount. - 随着大型语言模型（LLMs）在教育中的广泛应用不断增长，确保所有场景中的正确性至关重要。 3. Despite their strong English capabilities, LLM performance in other languages is largely untested. - 尽管它们在英语方面的能力很强，但LLMs在其他语言中的表现在很大程度上未经测试。 4. In this work, we evaluate the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students. - 在这项工作中，我们评估了Llama3.1系列模型在回答适合中学生和高中生的事实性问题时的正确性。 5. We demonstrate that LLMs not only provide extraneous and less truthful information, but also exacerbate existing biases against rare languages. - 我们证明，LLMs不仅提供无关和不真实的信息，还加剧了对稀有语言的现有偏见。  最终中文翻译结果： 事实性是有效教育工具的必要前提。随着大型语言模型（LLMs）在教育中的广泛应用不断增长，确保所有场景中的正确性至关重要。尽管它们在英语方面的能力很强，但LLMs在其他语言中的表现在很大程度上未经测试。在这项工作中，我们评估了Llama3.1系列模型在回答适合中学生和高中生的事实性问题时的正确性。我们证明，LLMs不仅提供无关和不真实的信息，还加剧了对稀有语言的现有偏见。|
|**2025-05-30**|**MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning**|Yiqing Liang et.al.|[2505.24871](http://arxiv.org/abs/2505.24871)|null|基于可验证奖励的强化学习（RLVR）最近成为了一种强大的后训练大型语言模型（LLM）的方法，在具有结构化和可验证答案的任务上取得了最先进的性能。将RLVR应用于多模态LLM（MLLMs）提供了巨大的机遇，但这也因视觉-语言任务更广泛、异质性的特性而复杂化，这些任务需要细微的视觉、逻辑和空间能力。因此，使用RLVR在多个数据集上训练MLLMs可能有益，但会因来自不同数据集之间交互的冲突目标而带来挑战，这突显了需要最优的数据集混合策略来提高泛化和推理能力。我们介绍了一个用于多模态LLM RLVR的系统化后训练框架，它包括严格的数据混合问题公式化和基准实施。具体来说，（1）我们开发了一个多模态RLVR框架，通过编纂包含不同可验证视觉-语言问题的数据集，并允许使用不同可验证奖励的多领域在线RL学习；（2）我们提出了一种数据混合策略，该策略能够从数据混合分布中预测RL微调结果，从而优化最佳混合。全面的实验表明，当结合混合预测策略时，多领域RLVR训练可以显著提高MLLM的一般推理能力。我们的最佳混合策略，与使用均匀数据混合后训练的同一模型相比，将后训练模型的准确性在分布外基准测试中提高了平均5.24%，与预微调基线相比，总提高了20.74%。|
|**2025-05-30**|**SiLVR: A Simple Language-based Video Reasoning Framework**|Ce Zhang et.al.|[2505.24869](http://arxiv.org/abs/2505.24869)|**[link](https://github.com/ceezh/silvr)**|近期在测试时优化方面的进展使得大型语言模型（LLMs）在推理能力上取得了显著成就，使它们能够解决数学和编程中的高度复杂问题。然而，多模态LLMs（MLLMs）的推理能力仍然明显落后，尤其是在复杂的视频-语言任务中。为了解决这个问题，我们提出了SiLVR，一个简单的基于语言的视频推理框架，将复杂的视频理解分解为两个阶段。在第一阶段，SiLVR利用多感官输入，如短视频字幕和音频/语音字幕，将原始视频转换为基于语言的表示。在第二阶段，将语言描述输入到一个强大的推理LLM中，以解决复杂的视频-语言理解任务。为了处理长上下文的多感官输入，我们使用了一种自适应的标记缩减方案，该方案动态确定采样标记的时间粒度。我们的简单、模块化和无需训练的视频推理框架在Video-MME（长）、Video-MMMU（理解）、Video-MMLU、CGBench和EgoLife上实现了最佳报道结果。此外，我们的实证研究聚焦于视频推理能力，表明尽管没有在视频上明确训练，强大的推理LLMs仍然能够有效地从视频、语音和音频中聚合多感官输入信息，以完成视频中的复杂时间、因果、长上下文和知识获取推理任务。代码可在https://github.com/CeeZh/SILVR上找到。|
|**2025-05-30**|**ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models**|Mingjie Liu et.al.|[2505.24864](http://arxiv.org/abs/2505.24864)|**[link](https://github.com/open-thought/reasoning-gym)**|最近在以推理为中心的语言模型方面的进展突出了强化学习（RL）作为一种将模型与可验证奖励对齐的有前景的方法。然而，关于RL是否真正扩展了模型的推理能力，或者仅仅是放大了基础模型分布中已经存在的、高奖励的输出，以及不断扩展RL计算能力是否可靠地导致推理性能的提升，仍然存在争议。在这项工作中，我们通过证明长期RL（ProRL）训练可以揭示基础模型即使经过大量采样也无法访问的新颖推理策略，挑战了现有的假设。我们引入了ProRL，这是一种新的训练方法，它结合了KL散度控制、参考策略重置和一系列多样化的任务。我们的实证分析表明，RL训练的模型在广泛的pass@k评估中始终优于基础模型，包括基础模型无论如何尝试都会完全失败的情况。我们进一步表明，推理边界改进与基础模型的任务能力和训练持续时间高度相关，这表明RL可以在时间上探索并填充解决方案空间的新区域。这些发现为RL在语言模型中有意义地扩展推理边界的条件提供了新的见解，并为未来关于推理的长时程RL工作奠定了基础。我们发布了模型权重以支持进一步的研究：https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B|
|**2025-05-30**|**MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning**|Jingyan Shen et.al.|[2505.24846](http://arxiv.org/abs/2505.24846)|null|奖励建模是将强化学习从人类反馈（RLHF）应用于对齐大型语言模型（LLMs）时构建安全基础模型的关键步骤。然而，基于布拉德利-特里（BT）模型的奖励建模假设了一个全局奖励函数，无法捕捉人类固有的多样化和异质化偏好。因此，这种过度简化的方法限制了LLMs支持个性化和多元对齐的能力。从理论上讲，我们表明当人类偏好遵循多种子群体混合分布时，单个BT模型存在不可减少的错误。虽然现有解决方案，如具有细粒度注释的多目标学习，有助于解决这个问题，但它们成本高昂且受预定义属性的限制，无法完全捕捉人类价值观的丰富性。在这项工作中，我们引入了MiCRo，一个两阶段框架，通过利用大规模二进制偏好数据集来增强个性化偏好学习，而无需显式细粒度注释。在第一阶段，MiCRo引入了上下文感知混合建模方法来捕捉多样化的人类偏好。在第二阶段，MiCRo整合了一种在线路由策略，根据特定上下文动态调整混合权重以解决歧义，从而实现高效且可扩展的偏好适应，同时最小化额外的监督。在多个偏好数据集上的实验表明，MiCRo有效地捕捉了多样化的人类偏好，并显著提高了下游的个性化。|
|**2025-05-30**|**Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning**|Wanyun Xie et.al.|[2505.24844](http://arxiv.org/abs/2505.24844)|**[link](https://github.com/lions-epfl/chameleon)**|训练数据混合对大型语言模型的一般化性能有重大影响。现有的领域重加权方法通常依赖于昂贵的权重计算，并在引入新数据时需要重新训练。为此，我们引入了一个灵活且高效的数据混合框架Chameleon，该框架利用杠杆分数在学习的嵌入空间中量化领域的重要性。我们首先在领域嵌入上构建一个领域亲和度矩阵。由此产生的杠杆分数确定了一个混合，提高了在嵌入空间中共享共同表示的领域的权重。这种公式允许通过计算新的领域嵌入直接转移到新数据。在实验中，我们证明了在三个关键场景上的改进：（i）我们计算出的权重在预训练领域上提高了性能，而所需的计算量仅为现有方法的几分之一；（ii）Chameleon可以在无需代理重新训练的情况下适应数据变化，当转移到新数据时，可以提高少样本推理的准确性；（iii）我们的方法使微调中的领域重加权变得高效，在所有微调领域中，与均匀混合相比，始终提高测试困惑度。我们的代码可在https://github.com/LIONS-EPFL/Chameleon上获取。|
|**2025-05-30**|**Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck**|Yuwen Tan et.al.|[2505.24840](http://arxiv.org/abs/2505.24840)|null|本文揭示了许多最先进的大型语言模型（LLMs）缺乏对我们视觉世界的层次化知识，甚至不知道已经建立的生物学分类法。这一不足使得LLMs成为视觉LLMs层次化视觉理解（例如，识别海葵鱼但不能识别脊椎动物）的瓶颈。我们通过大约一百万个由六个分类法和四个图像数据集构建的四选视觉问答（VQA）任务得出这些发现。有趣的是，使用我们的VQA任务微调视觉LLMs在一定程度上证实了LLMs的瓶颈效应，因为VQA任务提高了LLMs的层次一致性，但不如视觉LLMs。我们推测，除非LLMs拥有相应的分类法知识，否则无法使视觉LLMs完全理解视觉概念层次化。|
|**2025-05-30**|**VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software**|Brandon Man et.al.|[2505.24838](http://arxiv.org/abs/2505.24838)|**[link](https://github.com/brandonman123/videocad)**|计算机辅助设计（CAD）是一个耗时且复杂的过程，需要用户与复杂的3D界面进行精确、长期的用户交互。尽管人工智能驱动的用户界面（UI）代理的最近进展显示出希望，但大多数现有的数据集和方法都集中在移动或Web应用程序中的短期、低复杂度任务上，未能捕捉到专业工程工具的需求。在这项工作中，我们引入了VideoCAD，这是第一次尝试为精确任务进行工程UI交互学习。具体来说，VideoCAD是一个包含超过41K个标注视频记录的大规模合成数据集，这些记录是通过一个自动化框架生成的，用于从人工设计的CAD中收集高保真UI动作数据。与现有数据集相比，VideoCAD在UI交互学习方面提供了量级更高的复杂性，其时间跨度比其他数据集长20倍。我们展示了VideoCAD的两个重要下游应用：从专业精确的3D CAD工具中学习UI交互，以及一个视觉问答（VQA）基准，旨在评估多模态大型语言模型（LLM）的空间推理和视频理解能力。为了学习UI交互，我们提出了VideoCADFormer——一个从视频中直接学习CAD交互的最先进模型，它优于多个行为克隆基线。VideoCADFormer和从VideoCAD衍生出的VQA基准揭示了基于视频的UI理解当前状态下的关键挑战，包括精确动作定位、多模态和空间推理以及长期依赖关系的需求。|
|**2025-05-30**|**Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs**|Juraj Vladika et.al.|[2505.24830](http://arxiv.org/abs/2505.24830)|null|大型语言模型（LLMs）虽然具有丰富的医学知识，但容易产生幻觉和不准确引用，这给它们在临床应用和监管合规方面带来了挑战。当前的方法，如检索增强生成，通过将答案基于源文档来部分解决这些问题，但幻觉和低事实级别的可解释性仍然存在。在这项工作中，我们引入了一种新颖的原子事实核查框架，旨在提高用于医学长篇问答的LLMs的可靠性和可解释性。这种方法将LLM生成的响应分解成离散的、可验证的单位，称为原子事实，每个原子事实都与医学指南的权威知识库独立验证。这种方法能够实现针对性的错误纠正和直接追溯到源文献，从而提高医学问答的事实准确性和可解释性。通过医学专家的多读者评估和自动开放式问答基准的大量评估表明，在事实准确性和可解释性方面取得了显著改进。我们的框架实现了高达40%的整体答案改进和50%的幻觉检测率。能够将每个原子事实追溯到数据库中最相关的片段，为生成的响应提供了细粒度、透明的解释，填补了当前医学AI应用中的一个主要空白。这项工作代表了朝着更加可靠的临床应用LLMs的重要一步，满足了临床应用的关键前提，并促进了人们对AI辅助医疗的更大信心。|
|**2025-05-30**|**LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text**|Li yunhan et.al.|[2505.24826](http://arxiv.org/abs/2505.24826)|**[link](https://github.com/lyxx3rd/legaleval-q)**|随着大型语言模型（LLMs）在法律应用中的日益普及，当前的评估基准往往主要关注事实准确性，而很大程度上忽视了诸如清晰度、连贯性和术语等重要语言质量方面。为了解决这一差距，我们提出了三个步骤：首先，我们开发了一个回归模型，用于根据清晰度、连贯性和术语评估法律文本的质量。其次，我们创建了一套专门的法律试题。第三，我们使用这一评估框架分析了49个LLMs。我们的分析发现了三个关键发现：首先，模型质量在140亿参数时达到平台期，仅在720亿参数时观察到2.7%的微小提升。其次，工程选择如量化和对长度等对性能的影响微乎其微，这由超过0.016的统计显著性阈值所表明。第三，推理模型始终优于基础架构。我们研究的一个显著成果是发布了一个排名列表和帕累托分析，突出了Qwen3系列在成本性能权衡中的最优选择。这项工作不仅为法律LLMs建立了标准化的评估协议，还揭示了当前训练数据精炼方法中的基本局限性。代码和模型可在以下网址获得：https://github.com/lyxx3rd/LegalEval-Q。|
|**2025-05-30**|**PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models**|Yinggan Xu et.al.|[2505.24823](http://arxiv.org/abs/2505.24823)|null|大型语言模型（LLMs）的发展迅速，越来越能够处理包括物理学在内的一些复杂科学问题。尽管取得了这样的进步，当前的大型语言模型往往无法模仿人类专家简洁、基于原则的推理特点，反而生成冗长且不透明的解决方案。这种差异凸显了它们在应用核心物理原理进行高效和可解释的问题解决能力方面的关键差距。为了系统地调查这种局限性，我们引入了PhySense，这是一个新的基于原则的物理学推理基准，旨在让专家利用指导原则轻松解决，而对于没有先推理原则的LLMs来说却具有欺骗性的难度。我们在多个最先进的LLMs和提示类型上的评估显示，它们在保持与类似专家的推理路径对齐方面存在一致的失败，这为开发具有高效、鲁棒和可解释的基于原则的科学推理的AI系统提供了见解。|
|**2025-05-29**|**Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought**|Yunze Man et.al.|[2505.23766](http://arxiv.org/abs/2505.23766)|null|近年来，多模态大型语言模型（MLLMs）在视觉-语言任务中展现出惊人的能力，但它们在需要精确视觉聚焦的视觉中心场景中往往难以应对。在本文中，我们引入了Argus，通过一种新的视觉注意力定位机制来解决这些局限性。我们的方法采用以物体为中心的定位作为视觉思维链信号，使多模态推理任务中的目标条件视觉注意力更加有效。在多个基准测试上的评估表明，Argus在多模态推理任务和指称物体定位任务中都表现出色。深入分析进一步验证了Argus的各种设计选择，并揭示了在MLLMs中显式语言引导的视觉兴趣区域参与的有效性，突出了从视觉中心视角推进多模态智能的重要性。项目页面：https://yunzeman.github.io/argus/|
|**2025-05-29**|**From Chat Logs to Collective Insights: Aggregative Question Answering**|Wentao Zhang et.al.|[2505.23765](http://arxiv.org/abs/2505.23765)|null|随着大型语言模型（LLMs）驱动的对话代理在我们的日常互动中变得越来越重要，它们产生了前所未有的对话数据。这些数据集为深入了解社会兴趣、热门话题和集体关切提供了一个强大的视角。然而，现有的方法通常将这些互动视为独立的，而忽略了从大规模对话记录中聚合和推理可能出现的关键见解。在本文中，我们引入了聚合问答（Aggregative Question Answering），这是一个新的任务，要求模型对数千次用户与聊天机器人的互动进行明确推理，以回答聚合查询，例如识别特定群体中出现的关注点。为了推动这一方向的研究，我们构建了一个基准，WildChat-AQA，它包含从182,330个真实世界聊天机器人对话中提取的6,027个聚合问题。实验表明，现有方法要么难以有效推理，要么计算成本过高，这突显了需要新方法从大规模对话数据中提取集体见解的必要性。|
|**2025-05-29**|**MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence**|Sihan Yang et.al.|[2505.23764](http://arxiv.org/abs/2505.23764)|null|空间智能对于在复杂物理世界中运行的多元模态大型语言模型（MLLMs）至关重要。然而，现有的基准测试仅探测单图像关系，因此无法评估实际部署所需求的多图像空间推理。我们介绍了MMSI-Bench，这是一个专注于多图像空间智能的VQA基准。六位3D视觉研究人员花费了超过300小时，精心制作了1000个具有挑战性和明确答案的多选题，这些问题从超过120000张图像中选取，每张图像都配以精心设计的干扰项和逐步推理过程。我们进行了广泛的实验，并对34个开源和专有MLLMs进行了全面评估，观察到巨大的差距：最强的开源模型准确率约为30%，而OpenAI的o3推理模型达到40%，而人类的得分是97%。这些结果突显了MMSI-Bench的挑战性以及未来研究的巨大空间。利用标注的推理过程，我们还提供了一套自动错误分析流程，诊断了四种主要的失败模式，包括（1）基础错误，（2）重叠匹配和场景重建错误，（3）情境转换推理错误和（4）空间逻辑错误，为提升多图像空间智能提供了宝贵的见解。项目页面：https://runsenxu.com/projects/MMSI_Bench。|
|**2025-05-29**|**DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning**|Ziyin Zhang et.al.|[2505.23754](http://arxiv.org/abs/2505.23754)|**[link](https://github.com/jiahao004/deeptheorem)**|**定理证明是评估大型语言模型（LLMs）复杂推理能力的主要测试平台。然而，传统的自动定理证明（ATP）方法高度依赖于与LLMs从预训练期间获得的非正式、自然语言知识不太匹配的形式化证明系统。在这项工作中，我们提出了DeepTheorem，这是一个利用自然语言增强LLM数学推理的综合非正式定理证明框架。DeepTheorem包括一个大规模基准数据集，包含121K个高质量的IMO级非正式定理和证明，涵盖了广泛的数学领域，并经过严格的正确性、难度和主题类别标注，同时还伴随有系统构建的可验证定理变体。我们设计了一种新的强化学习策略（RL-Zero），专门针对非正式定理证明，利用验证过的定理变体来激励稳健的数学推理。此外，我们提出了全面的成果和过程评估指标，考察证明的正确性和推理步骤的质量。广泛的实验分析表明，与现有数据集和监督微调协议相比，DeepTheorem显著提高了LLM定理证明的性能，实现了最先进的准确性和推理质量。我们的发现突出了DeepTheorem在根本性地推进自动化非正式定理证明和数学探索方面的潜力。**|
|**2025-05-29**|**ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks**|Akashah Shabbir et.al.|[2505.23752](http://arxiv.org/abs/2505.23752)|**[link](https://github.com/mbzuai-oryx/thinkgeo)**|**最近大型语言模型（LLMs）的进展使得能够通过逐步推理解决复杂现实任务的工具增强智能体成为可能。然而，现有的评估往往集中在通用或多模态场景，这在评估复杂遥感应用中的工具使用能力的特定领域基准方面留下了空白。我们提出了ThinkGeo，这是一个旨在通过结构化工具使用和多步规划评估LLM驱动的智能体在遥感任务上表现的智能体基准。ThinkGeo受到工具交互范例的启发，包括涵盖城市规划、灾害评估和变化分析、环境监测、交通分析、航空监测、休闲娱乐基础设施和工业场地分析等广泛实际应用的由人类编写的查询。每个查询都基于卫星或航空影像，并要求智能体通过多样化的工具集进行推理。我们实现了一个ReAct风格的交互循环，并在436个结构化智能体任务上评估了开源和闭源LLMs（例如，GPT-4o、Qwen2.5）。基准报告了逐步执行指标和最终答案的正确性。我们的分析揭示了模型在工具准确性和规划一致性方面的显著差异。ThinkGeo为评估工具增强的LLMs如何处理遥感中的空间推理提供了第一个广泛测试平台。我们的代码和数据集是公开可用的。**|
|**2025-05-29**|**Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?**|Paul Gölz et.al.|[2505.23749](http://arxiv.org/abs/2505.23749)|null|在预训练后，大型语言模型通过成对比较与人类偏好对齐。最先进的对齐方法（如基于PPO的RLHF和DPO）建立在与单一偏好模型对齐的假设之上，尽管它们被部署在用户偏好多样化的环境中。因此，甚至不清楚这些对齐方法产生的模型是否能满足用户平均需求——这是多元对齐的最小要求。借鉴社会选择理论，并通过个体Bradley-Terry（BT）模型对用户比较进行建模，我们引入了一种对齐方法的扭曲：最优可实现平均效用与学习策略平均效用之间的最坏情况比率。扭曲的概念有助于对对齐方法进行清晰的区分：从人类反馈中进行的Nash学习实现了最小-最大最优扭曲（ $(\frac{1}{2} + o(1)) \cdot \beta$，对于BT温度$\beta$），在效用分布、比较对分布以及与参考策略的KL散度范围内稳健。相比之下，RLHF和DPO在没有KL约束的情况下已经遭受了$\geq (1 - o(1)) \cdot \beta$的扭曲，在完整设置中，根据比较对的采样方式，可能遭受$e^{\Omega(\beta)}$ 或甚至无界的扭曲。|
|**2025-05-29**|**Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence**|Diankun Wu et.al.|[2505.23747](http://arxiv.org/abs/2505.23747)|null|近期，多模态大型语言模型（MLLMs）在二维视觉任务上的性能得到了显著提升。然而，提高其空间智能仍然是一个挑战。现有的三维MLLMs总是依赖于额外的三维或2.5D数据来引入空间意识，这限制了它们在只有二维输入（如图像或视频）的场景中的实用性。在本文中，我们提出了Spatial-MLLM，这是一个从纯粹二维观察中进行基于视觉的空间推理的新颖框架。与依赖基于CLIP的视觉编码器（针对语义理解优化）的传统视频MLLM不同，我们的关键洞察是释放来自前馈视觉几何基础模型的强大结构先验。具体来说，我们提出了一种双编码器架构：一个预训练的二维视觉编码器用于提取语义特征，以及一个从视觉几何模型主干初始化的空间编码器用于提取三维结构特征。然后，一个连接器将这两个特征整合到统一的视觉标记中，以增强空间理解。此外，我们提出了一种在推理时间具有空间意识的帧采样策略，该策略选择视频序列中具有空间信息的帧，确保即使在有限的标记长度下，模型也专注于对空间推理至关重要的帧。除了架构改进之外，我们构建了Spatial-MLLM-120k数据集，并使用监督微调和GRPO在该数据集上训练模型。在多个真实世界数据集上的大量实验表明，我们的空间-MLLM在广泛的基于视觉的空间理解和推理任务中实现了最先进的性能。项目页面：https://diankun-wu.github.io/Spatial-MLLM/。|
|**2025-05-29**|**Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time**|Mohamad Chehade et.al.|[2505.23729](http://arxiv.org/abs/2505.23729)|null|由于偏好反馈的本性具有多方面性，将大型语言模型与人类对齐是一项挑战。尽管现有方法通常将其视为一个多目标优化问题，但它们往往忽略了人类实际上是如何做出决定的。关于有限理性的研究表明，人类的决策遵循满意策略——在优化主要目标的同时确保其他目标达到可接受的阈值。为了弥合这一差距并实现满意对齐的概念，我们提出了SITAlign：一个推理时间框架，通过最大化主要目标同时满足基于阈值的次要标准约束来解决对齐的多方面性质。我们通过推导基于满意策略的推理对齐方法的次优性界限提供了理论见解。我们通过在多个基准上的广泛实验验证了SITAlign的性能。例如，在PKU-SafeRLHF数据集上，SITAlign以最大化有用性为主要目标，同时确保无害性的阈值，其在有用性奖励方面的GPT-4赢-平率比最先进的多目标解码策略高出22.3%，同时遵守无害性的阈值。|
|**2025-05-29**|**PixelThink: Towards Efficient Chain-of-Pixel Reasoning**|Song Wang et.al.|[2505.23727](http://arxiv.org/abs/2505.23727)|null|现有的推理分割方法通常使用图像-文本对和相应的掩码标签来微调多模态大型语言模型（MLLMs）。然而，它们在没有明确推理过程的情况下，对分布外场景的泛化能力有限。尽管最近的研究通过组相对策略优化（GRPO）利用强化学习来增强推理能力，但它们往往受到过度思考的影响——无论任务复杂度如何，都产生一致的冗长推理链。这导致了计算成本的提高和推理质量的控制有限。为了解决这个问题，我们提出了一种简单而有效的方案PixelThink，它将外部估计的任务难度和内部测量的模型不确定性整合到强化学习范式中的推理生成中。该模型学会根据场景复杂性和预测信心来压缩推理长度。为了支持全面的评估，我们引入了ReasonSeg-Diff，这是一个扩展的基准，包括标注的推理参考和难度分数，以及一套旨在联合评估分割精度、推理质量和效率的指标。实验结果表明，所提出的方法提高了推理效率和整体分割性能。我们的工作为高效和可解释的多模态理解提供了新的视角。代码和模型将公开可用。|
|**2025-05-29**|**MuLoCo: Muon is a practical inner optimizer for DiLoCo**|Benjamin Thérien et.al.|[2505.23725](http://arxiv.org/abs/2505.23725)|null|DiLoCo是一个强大的框架，用于在具有网络约束的情况下训练大型语言模型（LLMs），在数据中心环境中具有提高并行性和加速器利用率的优点。然而，尽管DiLoCo显著降低了通信频率，但其通信步骤仍然涉及对模型参数的完整副本进行全量减少。尽管现有工作已经探索了在DiLoCo中减少通信的方法，但错误反馈累加器的作用和内部优化器对压缩性的影响仍被低估。在本工作中，我们研究了标准压缩方法（包括Top-k稀疏化和量化）在结合两个本地优化器（AdamW和Muon）时，对减少DiLoCo通信开销的有效性。我们的实验表明，使用Muon作为DiLoCo的内部优化器，并结合错误反馈累加器，可以将通信的delta值压缩到2位，同时几乎不降低性能。关键的是，MuLoCo（Muon内部优化器DiLoCo）在通信量减少8倍的同时，具有相同的内存复杂度，并且显著优于DiLoCo。|
|**2025-05-28**|**Zero-Shot Vision Encoder Grafting via LLM Surrogates**|Kaiyu Yue et.al.|[2505.22664](http://arxiv.org/abs/2505.22664)|**[link](https://github.com/facebookresearch/zero)**|**视觉语言模型（VLMs）通常将一个规模适中的视觉编码器与一个大语言模型（LLM）如Llama-70B配对，使得解码器在训练过程中成为主要的计算负担。为了降低成本，一种有潜力的策略是首先使用一个小语言模型训练视觉编码器，然后再将其迁移到大型模型。我们构建了与大型目标LLM共享相同嵌入空间和表示语言的小型“代理模型”，通过直接继承其浅层结构。在代理模型上训练的视觉编码器可以直接迁移到大型模型，我们称这个过程为零样本嫁接——当直接连接到全尺寸的目标LLM时，嫁接的配对超过了编码器-代理配对，在某些基准测试中甚至与使用目标LLM的全解码器训练表现相当。此外，当使用Llama-70B作为解码器时，我们的代理训练方法将整体VLM训练成本降低了约45%。**|
|**2025-05-28**|**AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models**|Feng Luo et.al.|[2505.22662](http://arxiv.org/abs/2505.22662)|null|推理能力强的大型语言模型（LLMs）在复杂推理任务上表现出色，但常常会出现过度思考的问题，为简单的推理问题生成不必要的长推理路径（CoT），从而增加推理成本和延迟。最近的研究试图通过手动决定何时应用长或短推理来解决这一挑战，然而，它们缺乏根据问题复杂度动态调整CoT长度的灵活性。在本文中，我们提出了自动长短推理（AutoL2S），这是一个动态且模型无关的框架，使LLMs能够根据推理问题的复杂度动态压缩其生成的推理路径。AutoL2S实现了一种学习范式，其中LLMs本身可以决定何时需要较长的推理，何时较短的推理就足够，通过训练标注有我们提出的方法的数据，这些数据包括长和短CoT路径以及一个特殊的<EASY>标记。然后我们使用<EASY>标记来指示模型何时可以跳过生成冗长的CoT推理。这种提出的标注策略可以在训练后提高LLMs生成较短且质量更优的CoT推理路径的能力。广泛的评估结果表明，AutoL2S在不影响性能的情况下，将推理生成的长度减少了高达57%，证明了AutoL2S在可扩展和高效LLM推理方面的有效性。|
|**2025-05-28**|**GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning**|Qingchen Yu et.al.|[2505.22661](http://arxiv.org/abs/2505.22661)|null|大型语言模型（LLMs）的传统评估依赖于静态基准，这种模式存在两个主要局限性：（1）预定义的测试集缺乏对不同应用领域的适应性，（2）标准化的评估协议往往无法捕捉到对特定领域知识和上下文推理能力的细致评估。为了克服这些挑战，我们提出了GuessArena，一个基于对抗性游戏互动的适应性评估框架。受到“猜猜我是谁？”游戏互动结构的启发，我们的框架无缝地将动态领域知识建模与逐步推理评估相结合，以提高评估的准确性。在金融、医疗保健、制造业、信息技术和教育五个垂直领域进行的实证研究表明，GuessArena在领域知识覆盖率和推理链完整性方面有效地区分了LLMs。与传统的基准相比，我们的方法在可解释性、可扩展性和场景适应性方面提供了显著的优势。|
|**2025-05-28**|**3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model**|Wenbo Hu et.al.|[2505.22657](http://arxiv.org/abs/2505.22657)|null|人类擅长通过利用跨时间和空间经验的长期记忆来执行复杂任务。相比之下，当前的大型语言模型（LLMs）在动态、多房间3D环境中进行有效规划和行动方面存在困难。我们认为，这种限制的部分原因在于LLMs中缺乏适当的3D时空记忆建模。为了解决这个问题，我们首先引入了3DMem-Bench，这是一个包含超过26,000条轨迹和2,892个具身任务、问答和字幕的综合基准，旨在评估代理在3D环境中进行长期记忆推理的能力。其次，我们提出了3DLLM-Mem，这是一种针对LLMs中具身时空推理和行动的新型动态记忆管理和融合模型。我们的模型使用工作记忆标记，它代表当前观察结果，作为查询来选择性地关注并融合来自事件记忆的最为有用的时空特征，事件记忆存储过去的观察和交互。我们的方法允许代理专注于与任务相关的信息，同时在复杂、长期环境中保持记忆效率。实验结果表明，3DLLM-Mem在各种任务中实现了最先进的性能，在3DMem-Bench最具挑战性的野外具身任务上的成功率比最强的基线高出16.5%。|
|**2025-05-28**|**Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents**|Michael Kirchhof et.al.|[2505.22655](http://arxiv.org/abs/2505.22655)|null|大型语言模型（LLMs）和聊天机器人代理有时会提供错误的结果，最近发现这根本无法完全避免。因此，不确定性量化发挥着至关重要的作用，旨在对随机不确定性和认识不确定性进行量化，无论是用一个数字还是两个数字来表示。这篇立场论文认为，这种不确定性的传统二分法对于LLM代理在与用户交流时所处的开放和交互式环境来说过于局限，我们需要研究在这个新场景中丰富不确定性的途径。我们回顾了文献，发现关于随机不确定性和认识不确定性的流行定义相互矛盾，在交互式LLM代理环境中失去了意义。因此，我们提出了三个新的研究方向，重点关注这种人机交互中的不确定性：未充分指定的不确定性，当用户没有提供所有信息或一开始就定义确切任务时；交互式学习，通过提问来减少当前上下文的不确定性；输出不确定性，利用丰富的语言和语音空间将不确定性表达为不仅仅是数字。我们期望这些处理和沟通不确定性的新方法将导致LLM代理交互更加透明、值得信赖和直观。|
|**2025-05-28**|**The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason**|Ang Lv et.al.|[2505.22653](http://arxiv.org/abs/2505.22653)|null|近期关于通过强化学习（RL）进行推理的大型语言模型（LLMs）的研究通常集中在可以准确验证和奖励的任务上，例如解决数学问题。相比之下，我们的研究调查了奖励噪声对使用奖励模型进行LLMs后训练的实际场景的影响。我们发现LLMs对大量的奖励噪声表现出很强的鲁棒性。例如，在数学任务中手动翻转40%的奖励函数输出，仍然使Qwen-2.5-7B模型能够快速收敛，将其在数学任务上的性能从5%提高到72%，相比之下，使用无噪声奖励训练的模型达到了75%的准确率。令人惊讶的是，仅通过奖励关键推理短语（即推理模式奖励，RPR）的出现（例如，“首先，我需要”），而不验证答案的正确性，模型就实现了峰值下游性能（Qwen-2.5-7B超过70%的准确率），这与经过严格正确性验证和准确奖励训练的模型相当。认识到推理过程比最终结果更重要，我们将RPR与噪声奖励模型相结合。RPR有助于校准噪声奖励模型，减轻潜在的假阴性，并增强LLMs在开放性任务上的性能。这些发现表明，在预训练阶段提高模型的基础能力的重要性，并为推进后训练技术提供了见解。我们的代码和脚本可在https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason上找到。|
|**2025-05-28**|**Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese**|Hanjia Lyu et.al.|[2505.22645](http://arxiv.org/abs/2505.22645)|**[link](https://github.com/brucelyu17/sc-tc-bench)**|While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench).|
|**2025-05-28**|**Learning Composable Chains-of-Thought**|Fangcong Yin et.al.|[2505.22635](http://arxiv.org/abs/2505.22635)|null|将大型语言模型（LLMs）教授推理的常见方法是对分布内的推理问题的思维链（CoT）轨迹进行训练，但这种标注数据对每个感兴趣的问题来说都是昂贵的。我们希望推理模型能够超越它们的训练分布，并且理想情况下能够组合性地泛化：将原子推理技能组合起来解决更难、未见过的推理任务。当我们面对一个没有标记的CoT数据的靶点组合任务时，我们朝着推理技能的组合泛化迈出一步。我们发现，仅仅在原子任务的CoT数据上训练模型会导致泛化能力有限，但将构成原子任务的CoT格式进行最小修改以实现组合性可以带来改进。我们可以在具有组合性CoT数据的原子任务上训练“原子CoT”模型，并通过多任务学习或模型融合来与目标组合任务上的零样本性能相结合。这样的组合模型可以通过使用拒绝采样微调（RFT）在少量组合数据上进一步自举。在字符串操作和自然语言技能组合上的结果表明，在可组合性CoT上训练LLMs优于在给定的训练数据预算内的多任务学习和持续微调基线。|
|**2025-05-28**|**Spatial Knowledge Graph-Guided Multimodal Synthesis**|Yida Xue et.al.|[2505.22633](http://arxiv.org/abs/2505.22633)|null|近期，多模态大型语言模型（MLLMs）在能力上取得了显著进步；然而，它们的空间感知能力仍然是一个显著的局限。为了应对这一挑战，多模态数据合成提供了一种有前景的解决方案。然而，确保合成的数据遵循空间常识是一个非平凡的任务。在本研究中，我们引入了SKG2Data，这是一种由空间知识图引导的新型多模态合成方法，其基于知识到数据生成的概念。SKG2Data自动构建一个空间知识图（SKG），以模拟人类对空间方向和距离的感知，随后利用该图来指导多模态数据合成。广泛的实验表明，从包括方向和距离在内的多种空间知识类型合成的数据，不仅增强了MLLMs的空间感知和推理能力，还表现出强大的泛化能力。我们希望基于知识的合成数据理念能够推进空间智能的发展。|
|**2025-05-28**|**Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs**|Ziling Cheng et.al.|[2505.22630](http://arxiv.org/abs/2505.22630)|null|大型语言模型（LLMs）在NLP基准测试中的广泛成功伴随着对其主要作为随机鹦鹉的担忧，即它们复制与预训练期间所见相似的文本，往往错误地。但这些错误的本质是什么，这些错误是否表现出任何规律性？在这项工作中，我们考察了无关的上下文幻觉，其中模型将误导性的上下文线索整合到其预测中。通过行为分析，我们发现这些错误源于一种结构化但存在缺陷的机制，我们称之为基于类别的（误）泛化，其中模型将抽象类别线索与从查询或上下文中提取的特征相结合以得出答案。此外，对Llama-3、Mistral和Pythia在39种事实回忆关系类型上的机制可解释性实验表明，这种行为反映在模型的内部计算中：（i）在更高层中精炼为具体答案之前，抽象类别表示在较低层构建；（ii）特征选择受两个相互竞争的回路控制——一个优先考虑基于直接查询的推理，另一个结合上下文线索——它们相对的影响力决定了最终输出。我们的发现为随机鹦鹉论点提供了更细微的视角：通过基于形式的训练，LLMs可以表现出利用抽象的泛化，尽管基于上下文线索的方式不可靠——我们称之为随机变色龙。|
|**2025-05-27**|**Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making**|Yihan Wang et.al.|[2505.21503](http://arxiv.org/abs/2505.21503)|null|大型语言模型（LLMs）在临床问题回答中显示出强大的潜力，而近期多智能体框架通过协作推理进一步提高了诊断的准确性。然而，我们识别出一个反复出现的问题，即无声协议，其中智能体在没有充分批判性分析的情况下过早地达成诊断共识，尤其是在复杂或不明确的情况下。我们提出一个新概念，称为猫鱼智能体，这是一种专门化的LLM角色，旨在注入结构化异议和对抗无声协议。受到组织心理学中“猫鱼效应”的启发，猫鱼智能体被设计用来挑战形成的共识以激发更深入的推理。我们制定了两套机制来鼓励有效和情境感知的干预：(i)一种感知复杂性的干预，根据案件难度调节智能体的参与程度；(ii)一种音调校准的干预，旨在平衡批评与合作。在九个医学问答和三个医学视觉问答基准上的评估显示，我们的方法在包括GPT-4o和DeepSeek-R1等领先商业模型在内的单智能体和多智能体LLMs框架中均表现优异。|
|**2025-05-27**|**Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment**|Xiaojun Jia et.al.|[2505.21494](http://arxiv.org/abs/2505.21494)|**[link](https://github.com/jiaxiaojunqaq/foa-attack)**|多模态大型语言模型（MLLMs）仍然容易受到可迁移的对抗样本攻击。虽然现有方法通常通过将全局特征（如CLIP的[CLS]标记）对齐在对抗样本和目标样本之间来实现定向攻击，但它们往往忽略了编码在patch标记中的丰富局部信息。这导致了对齐不佳和迁移性有限，尤其是在闭源模型中。为了解决这一局限性，我们提出了一种基于特征最优对齐的定向可迁移对抗攻击方法，称为FOA-Attack，以提高对抗样本的迁移能力。具体来说，在全局层面，我们引入了一种基于余弦相似度的全局特征损失，以对齐对抗样本的粗粒度特征与目标样本的特征。在局部层面，鉴于Transformers内部的丰富局部表示，我们利用聚类技术提取紧凑的局部模式，以减轻冗余的局部特征。然后，我们将对抗样本和目标样本之间的局部特征对齐形式化为最优传输（OT）问题，并提出了一种局部聚类最优传输损失来细化细粒度特征对齐。此外，我们提出了一种动态集成模型权重策略，以自适应地平衡在对抗样本生成过程中多个模型的影响，从而进一步提高迁移性。在多个模型上的大量实验证明了所提出方法的优势，超越了最先进的方法，尤其是在迁移到闭源MLLMs方面。代码已发布在https://github.com/jiaxiaojunQAQ/FOA-Attack。|
|**2025-05-27**|**Reinforcing General Reasoning without Verifiers**|Xiangxin Zhou et.al.|[2505.21493](http://arxiv.org/abs/2505.21493)|**[link](https://github.com/sail-sg/verifree)**|近期，利用DeepSeek-R1-Zero风格的强化学习（RL）在可验证奖励上训练大型语言模型（LLMs）的范式转变，在代码和数学推理方面取得了令人印象深刻的进步。然而，这种方法仅限于规则基础答案验证可行的任务，并且无法自然地扩展到化学、医疗保健、工程、法律、生物学、商业和经济等现实世界领域。目前的实际解决方案是使用额外的LLM作为基于模型的验证器；然而，这引入了依赖强大的验证器LLM、容易受到奖励黑客攻击以及在实际训练过程中维护验证器模型在内存中的实际负担等问题。为了解决这个问题并将DeepSeek-R1-Zero风格的训练扩展到通用推理领域，我们提出了一种无需验证器的验证方法（VeriFree），该方法绕过答案验证，而是使用RL直接最大化生成参考答案的概率。我们将VeriFree与基于验证器的方法进行了比较，并证明了除了其显著的实际效益和降低的计算需求外，VeriFree在MMLU-Pro、GPQA、SuperGPQA和与数学相关的基准测试中的广泛评估中与基于验证器的方法相当，甚至有所超越。此外，我们从多个角度提供了对这个方法的见解：作为在一个统一模型中优雅地整合策略和隐式验证器的训练，以及作为一种变分优化方法。代码可在https://github.com/sail-sg/VeriFree上找到。|
|**2025-05-27**|**Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming**|Yang Yang et.al.|[2505.21486](http://arxiv.org/abs/2505.21486)|null|在开放环境中自动化鲁棒的假设生成对于人工智能的认知至关重要。我们提出了一种新型框架，该框架将多智能体系统与归纳逻辑编程（ILP）相结合，并受到大型语言模型（LLMs）的驱动。我们的系统中的LLM智能体能够自主地从原始文本数据中定义结构化的符号词汇（谓词）和关系模板，即直接从原始文本数据中构建语言偏差。这种自动化的符号扎根（即构建语言偏差）过程，传统上一直是ILP的专家驱动瓶颈，然后指导将文本转化为事实，以供ILP求解器进行归纳学习可解释的规则。这种方法克服了传统ILP对预定义符号结构的依赖以及纯LLM方法的噪声敏感性。在多种多样、具有挑战性的场景中进行的广泛实验验证了其卓越的性能，为自动化、可解释和可验证的假设生成开辟了新的途径。|
|**2025-05-27**|**Are Language Models Consequentialist or Deontological Moral Reasoners?**|Keenan Samway et.al.|[2505.21479](http://arxiv.org/abs/2505.21479)|null|随着人工智能系统在医疗、法律和治理等领域的应用日益增多，了解它们如何处理道德复杂场景变得至关重要。以往的研究主要关注大型语言模型（LLMs）中的道德判断，而不是其背后的道德推理过程。相比之下，我们专注于对LLMs提供的道德推理痕迹进行大规模分析。此外，与先前只从少数道德困境中尝试推断的研究不同，我们的研究利用了超过600个不同的电车问题作为探针，以揭示不同LLMs中出现的推理模式。我们引入并测试了一种道德理由分类法，系统地根据两种主要的规范性伦理理论：功利主义和德性伦理学，对推理痕迹进行分类。我们的分析显示，LLMs的思维链倾向于基于道德义务的德性伦理学原则，而事后解释则明显转向强调效用功利主义理由。我们的框架为理解LLMs如何处理和表达伦理考量提供了基础，这是在高风险决策环境中安全且可解释地部署LLMs的重要一步。我们的代码可在https://github.com/keenansamway/moral-lens 上获取。|
|**2025-05-27**|**Policy Optimized Text-to-Image Pipeline Design**|Uri Gadot et.al.|[2505.21478](http://arxiv.org/abs/2505.21478)|null|文本到图像生成技术已从单一的大型模型发展到复杂的多组件管道。这些管道结合了微调的生成器、适配器、上采样块甚至编辑步骤，从而显著提高了图像质量。然而，它们的有效设计需要大量的专业知识。最近的方法表明，通过大型语言模型（LLMs）自动化的前景值得期待，但它们存在两个关键局限：生成图像时对数百个预定义管道的广泛计算需求，以及超出记忆中的训练示例的泛化能力较差。我们引入了一个基于强化学习的新框架，来解决这些低效问题。我们的方法首先训练了一个奖励模型集合，能够直接从提示-工作流程组合中预测图像质量得分，从而在训练过程中消除了生成图像的高成本需求。然后，我们实施了一个两阶段训练策略：首先是工作流程词汇训练，接着是基于GRPO的优化，引导模型走向工作流程空间中性能更高的区域。此外，我们还引入了一种无分类器引导的增强技术，该技术沿着初始模型和GRPO调整模型之间的路径进行外推，进一步提高了输出质量。我们通过一系列比较验证了我们的方法，显示它能够成功创建具有更高多样性的新流程，并且与现有的基线相比，能够达到更优质的图像质量。|
|**2025-05-27**|**Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration**|Zijun Liu et.al.|[2505.21471](http://arxiv.org/abs/2505.21471)|**[link](https://github.com/thunlp-mt/extagents)**|**随着推理和信息检索后训练技术的快速发展，大型语言模型（LLMs）可以整合大量检索到的知识来解决复杂任务。然而，LLMs有限的上下文窗口阻碍了外部知识输入量的扩展，限制了进一步的提升，尤其是在需要大量外部知识的任务中。现有的上下文窗口扩展方法不可避免地会导致信息损失。基于LLM的多智能体方法作为一种新的范式出现，可以以分布式的形式处理大量输入，其中我们识别出现有知识同步和推理过程中的两个核心瓶颈。在这项工作中，我们开发了一个多智能体框架，称为 $\textbf{ExtAgents}$，以克服这些瓶颈，并在不进行更长上下文训练的情况下，在推理时知识集成方面实现更好的可扩展性。通过与我们的增强型多跳问答测试$\textbf{$\boldsymbol{\infty}$Bench+}$ 以及其他包括长调查生成在内的公共测试集进行基准测试，ExtAgents显著提高了与现有非训练方法相同数量的外部知识输入的性能，无论这些输入是否在上下文窗口内或超出上下文窗口。此外，由于高并行性，该方法保持了高效率。进一步研究LLM智能体在增加外部知识输入方面的协调，可能对实际应用有益。**|
|**2025-05-27**|**Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance**|Shintaro Ozaki et.al.|[2505.21458](http://arxiv.org/abs/2505.21458)|null|大型语言模型（LLMs）被认为能够一致地使用一种内部语言来处理信息，这种内部语言被称为潜在语言，它可能与输入或输出语言不同。然而，潜在语言与输入和输出语言之间的差异如何影响下游任务的表现，这仍然是一个未充分探讨的问题。尽管许多研究关注LLMs的潜在语言，但很少有研究探讨其在影响任务表现中的重要性。在我们的研究中，我们假设在潜在语言中持续思考可以增强下游任务的表现。为了验证这一点，我们在多个下游任务中改变了输入提示语言，并分析了潜在语言的一致性与任务表现之间的相关性。我们创建了包含来自不同领域（如翻译和地缘文化）的问题的数据集，这些领域受到潜在语言选择的影响。在多个LLMs上进行的翻译和地缘文化任务实验，这些任务对语言选择敏感，表明在潜在语言中保持一致性并不总是对最佳下游任务表现必要。这是因为这些模型会调整其接近最终层的内部表示以匹配目标语言，从而减少了一致性对整体表现的影响。|
|**2025-05-27**|**Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO**|Muzhi Zhu et.al.|[2505.21457](http://arxiv.org/abs/2505.21457)|null|主动视觉，也称为主动感知，是指通过主动选择观察的位置和方式来收集与任务相关的信息的过程。它是人类和高级具身智能体在高效感知和决策中的关键组成部分。最近，多模态大型语言模型（MLLMs）作为机器人系统中的核心规划和决策模块的使用受到了广泛关注。然而，尽管主动感知在具身智能中非常重要，但关于如何使MLLMs具备或学习主动感知能力的探索却很少。在本文中，我们首先对基于MLLM的主动感知任务进行了系统定义。我们指出，最近提出的GPT-o3模型的缩放搜索策略可以被视为主动感知的一个特例；然而，它仍然存在搜索效率低和区域选择不准确的问题。为了解决这些问题，我们提出了ACTIVE-O3，这是一个基于GRPO的纯强化学习训练框架，旨在使MLLMs具备主动感知能力。我们进一步建立了一个全面的基准测试套件，以评估ACTIVE-O3在通用开放世界任务（如小物体和密集物体定位）以及特定领域场景（包括遥感中的小物体检测和自动驾驶，以及精细粒度交互式分割）中的表现。此外，ACTIVE-O3在V*基准测试中也展示了强大的零样本推理能力，而不依赖于任何显式的推理数据。我们希望我们的工作能够提供一个简单的代码库和评估协议，以促进未来关于MLLMs中主动感知的研究。|
|**2025-05-27**|**Can Large Reasoning Models Self-Train?**|Sheikh Shafayat et.al.|[2505.21444](http://arxiv.org/abs/2505.21444)|null|随着大型语言模型（LLMs）性能的扩展，越来越多地依赖于减少对人类监督的依赖。从自动化验证中进行强化学习提供了一种替代方案，但由于依赖于人类设计的验证器，因此存在可扩展性的限制。自训练，其中模型的自身判断提供监督信号，是一种令人信服的方向。我们提出了一种在线自训练强化学习算法，该算法利用模型的自洽性来推断正确性信号，并在没有任何真实标签的监督下进行训练。我们将该算法应用于具有挑战性的数学推理任务，并表明它迅速达到了与明确针对标准答案进行训练的强化学习方法相当的性能水平。此外，我们分析了算法固有的局限性，突出了自生成的代理奖励最初与正确性相关联如何激励奖励黑客行为，即更倾向于自信地错误的输出。我们的结果表明，自监督的改进可以在没有外部标签的情况下实现显著的性能提升，同时也揭示了其根本的挑战。|
|**2025-05-26**|**KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing**|Rui Li et.al.|[2505.20245](http://arxiv.org/abs/2505.20245)|**[link](https://github.com/rui9812/knowtrace)**|近期在检索增强生成（RAG）方面的进展为大型语言模型（LLMs）提供了迭代检索相关信息的功能，以处理复杂的多跳问题。这些方法通常在LLM推理和检索之间交替进行，将外部信息积累到LLM的上下文中。然而，不断增长的上下文本身给LLM带来了越来越大的负担，导致无用的推理步骤进一步加剧了这种超载问题。在本文中，我们提出了KnowTrace，一个优雅的RAG框架，旨在（1）缓解上下文超载问题，以及（2）启动高质量的跨步骤推理。KnowTrace不是简单地将检索到的内容堆积起来，而是自动追踪所需的知识三元组来组织与输入问题相关的特定知识图谱。这种结构化工作流程不仅赋予LLM一个清晰推理的上下文，而且自然地激发了一种知识回溯的反思机制，以识别有贡献的LLM生成作为过程监督数据，以实现自我启动。大量实验表明，KnowTrace在三个多跳问答基准测试中一致优于现有方法，并且自我启动版本进一步放大了收益。|
|**2025-05-26**|**On Path to Multimodal Historical Reasoning: HistBench and HistAgent**|Jiahao Qiu et.al.|[2505.20246](http://arxiv.org/abs/2505.20246)|**[link](https://github.com/charlesq9/histagent)**|近年来，大型语言模型（LLMs）在各个领域的进展显著，然而，它们在人文领域，尤其是历史领域的应用仍然未被充分探索。历史推理对AI提出了独特的挑战，包括多模态源材料的解释、时间推断和跨语言分析。尽管通用代理在许多现有基准测试中表现良好，但它们缺乏与历史材料和问题互动所需的特定领域专业知识。为了解决这一差距，我们引入了HistBench，这是一个由40多位专家贡献的414个高质量问题的新的基准测试，旨在评估AI进行历史推理的能力。任务涵盖了广泛的历史问题，从基于原始资料的事实检索到手稿和图像的阐释分析，再到涉及考古学、语言学或文化历史的多学科挑战。此外，基准数据集涵盖了29种古代和现代语言，覆盖了广泛的历史时期和世界地区。我们发现LLMs和其他代理在HistBench上的表现不佳，因此进一步提出了HistAgent，这是一个专门针对历史的代理，配备了精心设计的OCR、翻译、档案搜索和历史图像理解工具。在HistBench上，基于GPT-4o的HistAgent实现了27.54%的pass@1准确率和36.47%的pass@2准确率，显著优于进行在线搜索的LLMs和通用代理，包括GPT-4o（18.60%）、DeepSeek-R1（14.49%）和Open Deep Research-smolagents（pass@1 20.29%和pass@2 25.12%）。这些结果突出了现有LLMs和通用代理的局限性，并证明了HistAgent在历史推理方面的优势。|
|**2025-05-26**|**It's High Time: A Survey of Temporal Information Retrieval and Question Answering**|Bhawna Piryani et.al.|[2505.20243](http://arxiv.org/abs/2505.20243)|null|时间在信息的生成、检索和解读中扮演着关键角色。在本综述中，我们提供了关于时间信息检索和时间问答两个研究领域的全面概述，这两个领域旨在处理和理解时间敏感信息。随着来自新闻文章、网络存档和知识库等来源的时间戳内容数量的增加，系统必须解决诸如检测时间意图、规范化时间表达式、排序事件以及推理演变或模糊事实等挑战。这些挑战在许多动态和时间敏感的领域都是至关重要的，从新闻和百科全书到科学、历史和社会媒体。我们回顾了传统方法以及现代神经网络方法，包括使用Transformer模型和大型语言模型（LLMs）的方法。我们还回顾了时间语言建模、多跳推理和检索增强生成（RAG）的最新进展，以及测试时间鲁棒性、时效感知和泛化能力的基准数据集和评估策略。|
|**2025-05-26**|**RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models**|Nguyen Thach et.al.|[2505.20242](http://arxiv.org/abs/2505.20242)|null|在实践中解决NP-hard组合优化问题（COPs）（例如，旅行商问题（TSPs）和带容量限制的车辆路径问题（CVRPs））通常涉及手动设计启发式算法或指定一个搜索空间以寻找有效的启发式算法。然而，这些方法的主要挑战是需要大量的领域知识和人力资源来实现。最近，为了解决这些挑战，已经取得了显著的进展，尤其是通过使用大型语言模型（LLMs）在预定的通用算法框架（GAF，例如蚁群优化和引导局部搜索）内设计启发式算法，以构建关键功能/组件（例如，在TSP和CVRP中，预先确定包含每条边是否有助于解决问题的可能性）。尽管利用这一想法的现有方法已经显示出令人印象深刻的优化性能，但它们并不是端到端的，仍然需要相当大的手动干预。在本文中，我们提出了一种新颖的端到端框架，称为RedAHD，该框架使基于LLM的启发式设计方法能够在不使用GAF的情况下运行。更具体地说，RedAHD使用LLM来自动化简化的过程，即把当前的问题COP转化为更好理解的类似COPs，从而使基于LLM的启发式设计方法可以直接设计有效的启发式算法来解决转换后的COPs，从而间接解决原始COP。我们的实验结果，在六个COPs上进行评估，表明RedAHD能够设计出在最少人工参与下与现有方法具有竞争力或改进结果的启发式算法。|
|**2025-05-26**|**DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning**|Qi Cao et.al.|[2505.20241](http://arxiv.org/abs/2505.20241)|null|推理显著提高了大型语言模型（LLMs）在复杂任务上的性能。当前推理研究的关键是过程奖励模型（PRMs），它提供了对中间推理步骤的精细评估并指导推理过程。然而，将PRMs扩展到多模态大型语言模型（MLLMs）带来了挑战。由于多模态推理涵盖了比仅文本场景更广泛的任务，从训练集到测试集的分布偏移更为严重，导致泛化难度更大。因此，训练可靠的多模态PRM需要大量且多样化的数据集以确保充分的覆盖。然而，现有的多模态推理数据集存在明显的质量不平衡，这降低了PRM的性能，并突显了有效数据选择策略的必要性。为了解决这些问题，我们引入了DreamPRM，这是一个针对多模态PRM的领域重新加权训练框架，采用双层优化。在底层优化中，DreamPRM在具有领域权重的多个数据集上执行微调，使PRM能够优先考虑高质量的推理信号，并减轻数据集质量不平衡的影响。在高层优化中，PRM在独立的元学习数据集上进行评估；这种反馈通过聚合损失函数更新领域权重，从而提高训练PRM的泛化能力。在涵盖数学和通用推理的多个多模态推理基准上的广泛实验表明，使用DreamPRM进行测试时间缩放一致地提高了最先进的MLLMs的性能。进一步的比较揭示，DreamPRM的领域重新加权策略优于其他数据选择方法，并且比现有的测试时间缩放方法带来更高的准确度提升。|
|**2025-05-26**|**Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models**|Weihao Xuan et.al.|[2505.20236](http://arxiv.org/abs/2505.20236)|null|不确定性量化对于评估现代人工智能系统的可靠性和可信度至关重要。在现有方法中，通过自然语言表达自信的“口语化不确定性”在大语言模型（LLMs）中成为了一种轻量级且可解释的解决方案。然而，其在视觉-语言模型（VLMs）中的有效性研究尚不充分。在本研究中，我们对VLMs中的口语化置信度进行了全面评估，涵盖了三个模型类别、四个任务领域和三个评估场景。结果显示，当前VLMs在多种任务和设置中往往表现出明显的误校准。值得注意的是，视觉推理模型（即用图像进行思考）始终表现出更好的校准，这表明模态特定的推理对于可靠的置信度估计至关重要。为了进一步解决校准挑战，我们引入了视觉置信度感知提示，这是一种两阶段提示策略，可以提高多模态环境中的置信度对齐。总体而言，我们的研究突出了VLMs在模态间固有的误校准。更广泛地说，我们的发现强调了模态对齐和模型忠实度在推进可靠的多模态系统中的基本重要性。|
|**2025-05-26**|**FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models**|Hao Kang et.al.|[2505.20225](http://arxiv.org/abs/2505.20225)|**[link](https://github.com/cmu-flame/flame-moe)**|近年来，Gemini-1.5、DeepSeek-V3和Llama-4等大型语言模型越来越多地采用混合专家（MoE）架构，这种架构通过仅激活模型中的一部分来提供强大的效率-性能权衡。然而，学术界研究人员仍然缺乏一个完全开源的端到端MoE平台，用于研究扩展、路由和专家行为。我们发布了FLAME-MoE，这是一个完全开源的研究套件，由七个仅包含解码器的模型组成，参数量从3800万到17亿不等，其架构——64个专家、顶级8的门控和2个共享专家——与现代生产级大型语言模型非常相似。所有训练数据管道、脚本、日志和检查点都公开可用，以实现可重复的实验。在六个评估任务中，FLAME-MoE的平均准确率比使用相同FLOPs训练的密集基线提高了高达3.4个百分点。利用完整的训练跟踪透明度，我们提出了初步分析，显示（i）专家越来越多地专注于不同的标记子集，（ii）共激活矩阵保持稀疏，反映了专家使用的多样性，（iii）路由行为在训练早期就稳定下来。所有代码、训练日志和模型检查点都可在https://github.com/cmu-flame/FLAME-MoE上找到。|
|**2025-05-26**|**Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects**|Yixin Cui et.al.|[2505.20223](http://arxiv.org/abs/2505.20223)|**[link](https://github.com/cuiyx1720/awesome-cot4ad)**|随着自然语言处理领域大型语言模型的快速发展，它们的语义理解和逻辑推理能力得到了显著提升。这些能力在自动驾驶系统中得到了应用，为系统性能的显著提升做出了贡献。例如，OpenAI o1和DeepSeek-R1等模型利用思维链（Chain-of-Thought，CoT）推理，这是一种高级的认知方法，模拟人类思维过程，在复杂任务中展现出卓越的推理能力。通过在一个系统性的推理框架中构建复杂的驾驶场景，这种方法已成为自动驾驶研究的热点，显著提高了系统处理挑战性情况的能力。本文探讨了CoT方法如何提高自动驾驶模型的推理能力。基于全面的文献综述，我们系统地分析了CoT在自动驾驶中的动机、方法、挑战和未来研究方向。此外，我们提出了将CoT与自学习相结合的见解，以促进驾驶系统的自我进化。为确保本研究的关联性和时效性，我们编制了一个动态的文献和开源项目库，勤勉地更新以纳入前沿进展。该库在https://github.com/cuiyx1720/Awesome-CoT4AD上公开可用。|
|**2025-05-26**|**Fine-grained List-wise Alignment for Generative Medication Recommendation**|Chenxiao Fan et.al.|[2505.20218](http://arxiv.org/abs/2505.20218)|**[link](https://github.com/cxfann/flame)**|准确的药物推荐对于有效的临床决策至关重要，尤其是在多病共存的情况下。然而，现有的系统依赖于点预测范式，忽略了药物间的协同作用和潜在的药物-药物相互作用（DDI）。我们提出了FLAME，这是一个针对大型语言模型（LLMs）的细粒度列表对齐框架，能够实现按药物生成药物列表。FLAME将推荐过程定义为一系列决策过程，其中每一步只添加或删除一种药物。为了提供细粒度的学习信号，我们设计了基于势能的奖励塑造的逐步组相对策略优化（GRPO），该优化方法明确地建模了DDI并优化了每种药物对整体处方的贡献。此外，FLAME通过将结构化临床知识和协作信息整合到LLMs的表现空间中，增强了患者建模。在基准数据集上的实验表明，FLAME实现了最先进的性能，提供了更高的准确度、可控的安全-准确度权衡以及在多样化临床场景中的强大泛化能力。我们的代码可在https://github.com/cxfann/Flame上获取。|
|**2025-05-26**|**Parameter-Efficient Fine-Tuning with Column Space Projection**|Junseo Hwang et.al.|[2505.20211](http://arxiv.org/abs/2505.20211)|null|为了在资源受限的情况下高效地将大型语言模型（LLMs）适应下游任务，以最小的计算开销进行微调至关重要。参数高效的微调（PEFT）方法，如低秩适应（LoRA），通过仅更新一小部分参数来促进这一点。然而，最近的研究表明，LoRA在学习行为上与完全微调（Full FT）存在差异，尤其是在谱性质方面。受这些发现启发，我们提出了PiCa，这是第一个基于微调权重谱性质的PEFT方法。PiCa将梯度投影到预训练权重的低秩列子空间，并展现出与Full FT更接近的学习模式。此外，我们表明，将PiCa与权重共享相结合，可以大幅减少可训练参数的数量，而不会影响性能，使用比LoRA少13倍的训练参数即可实现更优的性能。大量实验表明，与现有的PEFT方法相比，PiCa实现了最先进的性能。|
|**2025-05-26**|**MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents**|Ziming Wei et.al.|[2505.20148](http://arxiv.org/abs/2505.20148)|**[link](https://github.com/mineanybuild/mineanybuild)**|空间规划是空间智能领域的关键部分，它需要从空间角度理解和规划物体的布局。具有空间规划能力的AI智能体能够更好地适应各种现实世界应用，包括机器人操作、自动装配、城市规划等。最近的研究尝试构建评估多模态大型语言模型（MLLMs）空间智能的基准。然而，这些基准主要侧重于基于典型视觉问答（VQA）形式的空间推理，这存在着抽象空间理解与具体任务执行之间的差距。在本研究中，我们进一步构建了一个名为MineAnyBuild的综合性基准，旨在评估开放世界AI智能体在Minecraft游戏中的空间规划能力。具体来说，MineAnyBuild要求智能体根据给出的多模态人类指令生成可执行的建筑计划。它包含了4,000个精心策划的空间规划任务，并通过利用丰富的玩家生成内容提供了一种无限可扩展的数据收集范式。MineAnyBuild通过四个核心支撑维度来评估空间规划：空间理解、空间推理、创造力和空间常识。基于MineAnyBuild，我们对基于MLLM的现有智能体进行了全面评估，揭示了它们在空间规划能力上的严重局限性和巨大潜力。我们相信，我们的MineAnyBuild将为空间智能的评价开辟新的途径，并有助于推动具有空间规划能力的开放世界AI智能体的进一步发展。|
|**2025-05-26**|**FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities**|Jin Wang et.al.|[2505.20147](http://arxiv.org/abs/2505.20147)|null|大语言模型（LLMs）的快速发展催生了统一视觉理解和图像生成的多模态大语言模型（MLLMs）。然而，大多数现有的MLLMs都依赖于自回归（AR）架构，这给未来的发展带来了固有的局限性，例如图像生成的光栅扫描顺序和在因果上下文建模中的推理能力受限。在这项工作中，我们通过引入FUDOKI，一个纯粹基于离散流匹配的统一多模态模型，挑战了基于AR方法的主导地位，将其作为传统AR范例的替代方案。通过利用由度量诱导的概率路径和动力最优速度，我们的框架超越了之前的基于遮蔽的破坏过程，实现了具有自校正能力和更丰富的双向上下文整合的迭代优化。为了减轻从头开始训练的高成本，我们从预训练的基于AR的MLLMs初始化FUDOKI，并自适应地过渡到离散流匹配范式。实验结果表明，FUDOKI在视觉理解和图像生成任务上均达到了与最先进的基于AR的MLLMs相当的性能，突显了其作为下一代统一多模态模型基础的潜力。此外，我们还表明，将测试时间缩放技术应用于FUDOKI可以带来显著的性能提升，进一步强调了其通过强化学习实现未来增强的潜力。|
|**2025-05-26**|**StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs**|Jialin Yang et.al.|[2505.20139](http://arxiv.org/abs/2505.20139)|null|随着大型语言模型（LLMs）在软件开发工作流程中变得不可或缺，它们生成结构化输出的能力变得至关重要。我们介绍了StructEval，这是一个全面基准，用于评估LLMs在生成非渲染格式（JSON、YAML、CSV）和渲染格式（HTML、React、SVG）方面的能力。与之前的基准不同，StructEval通过两种范式系统地评估了不同格式下的结构保真度：1）生成任务，从自然语言提示中生成结构化输出；2）转换任务，在结构化格式之间进行翻译。我们的基准涵盖了18种格式和44种任务类型，并引入了新的格式遵从性和结构正确性指标。结果显示，即使是最先进的模型如o1-mini也仅实现了75.58的平均得分，开源替代品落后大约10分。我们发现生成任务比转换任务更具挑战性，而生成正确的视觉内容比生成纯文本结构更困难。|
|**2025-05-26**|**Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers**|Zhengliang Shi et.al.|[2505.20128](http://arxiv.org/abs/2505.20128)|**[link](https://github.com/mangopy/searchlm)**|大型语言模型（LLMs）已广泛应用于信息检索以推进传统技术。然而，由于多跳查询的复杂性和检索到的无关内容，有效地使LLMs在复杂任务中寻求准确知识仍然是一个挑战。为了解决这些局限性，我们提出了EXSEARCH，一个代理搜索框架，其中LLM通过自激励的过程学习在推理过程中检索有用信息。在每一步，LLM决定要检索什么（思考），触发外部检索器（搜索），并提取细粒度证据（记录）以支持下一步推理。为了使LLM具备这种能力，EXSEARCH采用了一种广义期望最大化算法。在E步骤中，LLM生成多个搜索轨迹并对每个分配一个重要性权重；在M步骤中，使用重新加权损失函数对这些轨迹进行训练。这创建了一个自激励的循环，其中LLM从自身生成的数据中迭代学习，逐步提高搜索能力。我们进一步对该训练过程进行理论分析，确立了收敛保证。在四个知识密集型基准上的大量实验表明，EXSEARCH显著优于基线，例如，在精确匹配得分上提高了7.8%。受到这些有希望的结果的启发，我们引入了EXSEARCH-Zoo，这是一个扩展，将我们的方法扩展到更广泛的场景，以促进未来的工作。|
|**2025-05-26**|**Agentic AI Process Observability: Discovering Behavioral Variability**|Fabiana Fournier et.al.|[2505.20127](http://arxiv.org/abs/2505.20127)|null|利用大型语言模型的AI代理正日益成为现代软件系统的核心构建块。目前已有众多框架可供支持这类应用的规范。这些框架允许使用自然语言提示定义代理设置，指定参与代理的各种角色的职责、目标和工具。在这样的设置中，针对任何给定输入的代理行为是非确定性的，这突显了对强大调试和可观察性工具的迫切需求。在本工作中，我们探索了将过程和因果发现应用于代理执行轨迹，作为一种增强开发者可观察性的方法。这种方法有助于监控和理解代理行为的涌现变化。此外，我们通过基于LLM的静态分析技术来补充这一方法，以区分预期和意外的行为变化。我们认为，此类仪器对于让开发者对不断演变的规范有更大的控制权，并识别可能需要更精确和明确定义的功能方面至关重要。|
|**2025-05-26**|**TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent**|Dominik Meier et.al.|[2505.20118](http://arxiv.org/abs/2505.20118)|**[link](https://github.com/worta/trojansteno)**|随着大型语言模型（LLMs）被整合到敏感的工作流程中，对其泄露机密信息的潜在风险越来越受到关注。我们提出了TrojanStego，这是一种新的威胁模型，其中攻击者通过微调LLM，利用语言隐写术将敏感上下文信息嵌入到看似自然的外观输出中，而无需对推理输入进行显式控制。我们引入了一个分类法，概述了受损害的LLMs的风险因素，并使用它来评估该威胁的风险状况。为了实现TrojanStego，我们提出了一种基于词汇分区且LLMs可以通过微调学习的实用编码方案。实验结果表明，受损害的模型在保留提示上可靠地传输32位机密信息，准确率达到87%，通过三代投票的准确率超过97%。此外，它们保持了高实用性，可以规避人类检测，并保持连贯性。这些结果突显了一种新的LLM数据泄露攻击类型，这类攻击是被动、隐蔽、实用且危险的。|
|**2025-05-26**|**Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone**|Cristian Santini et.al.|[2505.20113](http://arxiv.org/abs/2505.20113)|null|随着世界文本遗产的数字化程度不断提高，这对计算机科学和文学研究都提出了重大挑战。总体而言，迫切需要能够适应历史文本挑战的计算技术，例如正字法和拼写变化、碎片化结构和数字化错误。大型语言模型（LLMs）的兴起彻底改变了自然语言处理，为历史文档上的命名实体识别（NER）提出了有希望的应用。尽管如此，针对意大利文本的彻底评估尚未提出。本研究试图填补这一空白，通过提出一个新的基于19世纪学术笔记语料库（即吉亚科莫·莱奥帕尔迪的《杂记》，1898年）的实体提取挑战性数据集，该数据集包含对人物、地点和文学作品的2,899个引用。该数据集被用于进行可复现实验，包括基于特定领域的BERT模型和最先进的LLMs（如LLaMa3.1）。结果表明，指令调整模型在处理历史人文学文本时遇到多个困难，而微调的NER模型即使在处理具有挑战性的实体类型（如书目引用）时也能提供更稳健的性能。|
|**2025-05-26**|**ResSVD: Residual Compensated SVD for Large Language Model Compression**|Haolei Bai et.al.|[2505.20112](http://arxiv.org/abs/2505.20112)|null|大型语言模型（LLMs）在众多下游自然语言处理任务中展现出了令人印象深刻的能力。然而，它们庞大的规模和内存需求阻碍了实际部署，突显了开发高效压缩策略的重要性。奇异值分解（SVD）将矩阵分解为正交组件，从而实现高效的低秩近似。这对于LLM压缩尤其合适，因为权重矩阵通常具有显著的冗余。然而，当前的基于SVD的方法忽略了截断过程中产生的残差矩阵，导致显著的截断损失。此外，压缩模型的全部层会导致性能严重下降。为了克服这些限制，我们提出了ResSVD，一种新的基于SVD的LLM压缩方法。具体来说，我们利用截断过程中生成的残差矩阵来减少截断损失。此外，在固定的整体压缩比下，我们选择性地压缩模型的最后几层，这减轻了错误传播并显著提高了压缩模型的表现。在多种LLM家族和多个基准数据集上对ResSVD的全面评估表明，ResSVD在现有对应方法中始终表现出优异的性能，证明了其实际有效性。|
|**2025-05-26**|**Language-Agnostic Suicidal Risk Detection Using Large Language Models**|June-Woo Kim et.al.|[2505.20109](http://arxiv.org/abs/2505.20109)|null|青少年自杀风险评估是一个关键的挑战，但现有方法依赖于特定语言的模型，这限制了其可扩展性和泛化能力。本研究介绍了一种新的、不依赖语言的大语言模型（LLM）自杀风险评估框架。我们使用语音识别（ASR）模型从语音生成中文转录本，然后利用基于提示的查询来从这些转录本中提取与自杀风险相关的特征。提取的特征既保留中文也保留英文，以实现跨语言分析，然后用于独立地微调相应的预训练语言模型。实验结果表明，我们的方法在性能上与直接使用ASR结果的微调或仅使用中文自杀风险相关特征的模型相当，证明了其克服语言限制和提高自杀风险评估鲁棒性的潜力。|
|**2025-05-26**|**Adaptive Deep Reasoning: Triggering Deep Thinking When Needed**|Yunhao Wang et.al.|[2505.20101](http://arxiv.org/abs/2505.20101)|null|大型语言模型（LLMs）在通过长链推理处理复杂任务方面表现出令人印象深刻的能力。然而，涉及的大量推理步骤会显著增加计算成本，对实际部署构成挑战。近期的研究工作主要集中在通过缩短思维链（CoT）推理过程来优化推理效率，采用的方法包括长度感知的提示工程、在可变长度的CoT数据上进行的监督微调以及带有长度惩罚的强化学习。尽管这些方法有效地减少了推理长度，但它们仍然需要初始的推理阶段。更近期的尝试试图将长链和短链推理能力集成到单个模型中，但它们仍然依赖于手动控制来在短和长CoT之间切换。在这项工作中，我们提出了一种新颖的方法，该方法可以根据问题复杂性自主地在短和长推理链之间切换。我们的方法首先通过监督微调基础模型来装备长链和短链推理能力。然后，我们采用强化学习来进一步平衡短和长CoT生成，同时通过两种关键策略保持准确性：首先，将强化学习与长-短自适应组内奖励策略相结合，以评估提示复杂度并提供相应的奖励；其次，实现基于logit的推理模式切换损失，以优化模型的初始标记选择，从而引导推理类型的选择。在数学数据集上的评估表明，我们的模型可以在不显著牺牲性能的情况下动态地在长链和短链推理模式之间切换。这一进步提高了大型语言模型在实际应用中推理的实用性。|
|**2025-05-23**|**Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs**|Wafa Alghallabi et.al.|[2505.18152](http://arxiv.org/abs/2505.18152)|**[link](https://github.com/mbzuai-oryx/fannorflop)**|阿拉伯诗歌是阿拉伯语中最复杂和具有文化内涵的表达形式之一，以其层次丰富的意义、多样的风格和深厚的历史连续性而闻名。尽管大型语言模型（LLMs）在各种语言和任务上表现出色，但它们理解阿拉伯诗歌的能力仍然未被充分探索。在这项工作中，我们介绍了名为“Fann or Flop”的第一个基准，旨在评估LLMs在十二个历史时期对阿拉伯诗歌的理解能力，涵盖了21种核心诗歌类型和多种韵律形式，从古典结构到现代自由诗。该基准包含了一个经过精心挑选的诗句语料库及其解释，用于评估语义理解、隐喻解释、韵律意识和文化背景。我们认为，诗歌理解是测试LLMs通过阿拉伯诗歌理解古典阿拉伯语的良好指标。与表面任务不同，这个领域需要更深入的推理能力和文化敏感性。我们对最先进的LLMs进行的评估表明，尽管在标准阿拉伯语基准测试中取得了良好的成绩，但大多数模型在诗歌理解方面仍然存在困难。我们将“Fann or Flop”及其评估套件作为一个开源资源发布，以促进阿拉伯语言模型的严格评估和进步。代码可在以下链接获取：https://github.com/mbzuai-oryx/FannOrFlop。|
|**2025-05-23**|**First Finish Search: Efficient Test-Time Scaling in Large Language Models**|Aradhye Agarwal et.al.|[2505.18149](http://arxiv.org/abs/2505.18149)|null|测试时缩放（TTS）涉及在推理过程中动态分配计算，为改善大型语言模型的推理提供了一种有前景的方法。虽然现有的TTS方法效果良好，但它们通常依赖于较长的解码路径或需要生成大量样本，这增加了令牌使用量和推理延迟。我们发现一个令人惊讶的事实：对于推理任务，较短的路径远比长的路径更有可能正确。受此启发，我们引入了“首次完成搜索”（FFS），这是一种无需训练的并行解码策略，它启动n个独立样本，并一旦任何一个完成就返回。我们在四个推理模型（DeepSeek-R1、R1-Distill-Qwen-32B、QwQ-32B和Phi-4-Reasoning-Plus）以及四个数据集（AIME24、AIME25-I、AIME25-II和GPQA Diamond）上评估了FFS，并与简单解码、束搜索、多数投票和预算强制进行了比较。使用DeepSeek-R1，FFS在AIME数据集上实现了82.23%的准确率，比DeepSeek-R1的独立准确率提高了15%，几乎与OpenAI的o4-mini性能相当。我们的理论分析解释了为什么在最短路径处停止可能得到正确答案，并确定了何时提前停止可能不是最优的。FFS的优雅和简单性表明，简单的TTS策略可以表现得非常出色，揭示了在推理时间简单方法的潜力。|
|**2025-05-23**|**Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find**|Owen Bianchi et.al.|[2505.18148](http://arxiv.org/abs/2505.18148)|null|大型语言模型（LLMs）在寻找信息（“针”）的任务中面临重大挑战，需要从大量无关背景信息（“干草堆”）中提取相关信息。先前的研究已经指出位置偏差和干扰量是影响模型性能的关键因素，但关于金背景大小的影响却鲜有关注。我们通过系统地研究黄金背景长度变化如何影响LLMs在长背景问答任务中的表现，来填补这一空白。我们的实验发现，当黄金背景较短时，LLMs的表现急剧下降，即较小的黄金背景会持续降低模型性能并加剧位置敏感性，这对需要整合不同长度分散、细粒度信息的代理系统构成了重大挑战。这一模式在三个不同的领域（一般知识、生物医学推理和数学推理）以及七种大小和架构各异的顶级LLMs中都存在。我们的工作为设计稳健、感知背景的LLM驱动系统提供了明确的见解。|
|**2025-05-23**|**Gaming Tool Preferences in Agentic LLMs**|Kazem Faghih et.al.|[2505.18135](http://arxiv.org/abs/2505.18135)|**[link](https://github.com/kazemf78/Gaming-Tool-Preferences)**|大型语言模型（LLMs）现在可以通过模型上下文协议（MCP）访问广泛的工具。这极大地扩展了它们作为各种代理的能力。然而，LLMs完全依赖于工具的文字描述来决定使用哪些工具——这个过程出人意料地脆弱。在这项工作中，我们通过调查一系列工具描述的编辑，揭示了普遍存在的工具/函数调用协议中的漏洞，其中一些编辑可以在与替代品竞争时极大地增加LLMs对工具的使用。通过控制实验，我们发现经过适当编辑描述的工具从GPT-4.1和Qwen2.5-7B那里获得的用法是原始描述工具的10倍以上。我们进一步评估了当直接相互竞争时，各种工具描述编辑的表现，以及这些趋势在更广泛的10种不同模型中是如何普遍或不同的。这些现象虽然为开发者提供了一种强大的推广工具的方法，但也强调了为代理LLMs选择和使用工具和资源需要一个更可靠的基础的必要性。|
|**2025-05-23**|**Reward Model Overoptimisation in Iterated RLHF**|Lorenz Wolf et.al.|[2505.18126](http://arxiv.org/abs/2505.18126)|null|强化学习从人类反馈（RLHF）是一种广泛用于使大型语言模型与人类偏好一致的方法。然而，RLHF常遭受奖励模型过优化的问题，即模型过度拟合奖励函数，导致无法推广的策略，这些策略利用奖励函数的独特性和特殊性。一种常见的缓解方法是迭代RLHF，在这种方法中，奖励模型会重复使用更新的人类反馈进行再训练，并重新优化策略。尽管其应用越来越广泛，但这种设置中过优化的动态仍然了解不足。在这项工作中，我们首次全面研究了迭代RLHF中的过优化问题。我们系统地分析了关键的设计选择——如何在迭代之间转移奖励模型训练数据，用于优化的奖励函数是哪一个，以及策略是如何初始化的。利用控制AlpacaFarm基准，我们观察到随着奖励模型越来越接近真实偏好，过优化在连续迭代中趋于减少。然而，性能提升随着时间的推移而减少，虽然从基本策略重新初始化是稳健的，但它限制了优化灵活性。其他初始化策略通常无法从早期的过优化中恢复。这些发现为构建更稳定和可推广的RLHF管道提供了可操作的见解。|
|**2025-05-23**|**UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification**|Poojah Ganesan et.al.|[2505.18122](http://arxiv.org/abs/2505.18122)|null|近期大型语言模型（LLMs）在单表查询的文本到SQL性能方面取得了显著进步。但在多表数据库中，由于复杂的模式和关系操作，这仍然是一个挑战。现有方法通常在检索正确的表和列、生成准确的连接（JOIN）和并集（UNION）以及泛化到不同的模式方面存在困难。为了解决这些问题，我们引入了UNJOIN，这是一个两阶段框架，将模式元素的检索与SQL逻辑生成解耦。在第一阶段，我们将数据库中所有表的列名通过在每个列名前加上其表名的方式合并为一个单表表示。这使得模型可以纯粹关注准确检索，而不被编写复杂SQL逻辑的需求所干扰。在第二阶段，在简化的模式上生成SQL查询，并通过重建连接、并集和关系逻辑将其映射回原始模式。在SPIDER和BIRD数据集上的评估表明，UNJOIN与最先进的基线相匹配或超过了它们。UNJOIN仅使用模式信息，无需数据访问或微调，这使得它在数据库中具有可扩展性和适应性。|
|**2025-05-23**|**ProgRM: Build Better GUI Agents with Progress Rewards**|Danyang Zhang et.al.|[2505.18121](http://arxiv.org/abs/2505.18121)|null|基于大型语言模型（LLM）的图形用户界面（GUI）代理有可能显著改变我们的日常生活。然而，由于轨迹收集和奖励标注的困难，当前的基于LLM的GUI代理面临高质量训练数据稀缺的问题。现有研究已经探索使用LLM来收集轨迹以进行模仿学习或提供在线强化学习的奖励信号。然而，现有工作中使用的成果奖励模型（ORM）无法提供细致的反馈，并且可能过度惩罚最终失败的轨迹中的有价值步骤。为此，我们提出了进展奖励模型（ProgRM），通过预测在线训练中每一步的任务完成进度来提供密集的信息性中间奖励。为了处理进展奖励标签标注的挑战，我们进一步设计了一个基于最长公共子序列（LCS）的高效自我标注算法，以发现轨迹中的关键步骤并相应地分配进展标签。我们对ProgRM进行了广泛的实验和分析。使用ProgRM训练的演员优于领先的私有LLM和ORM训练的演员，证明了ProgRM的有效性。实验代码将在接受后公开。|
|**2025-05-23**|**Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models**|Jiongran Wu et.al.|[2505.18120](http://arxiv.org/abs/2505.18120)|null|大型语言模型（LLMs）在理解和生成语义模式方面表现出色，使其成为序列推荐任务的理想候选者。然而，当与传统的推荐模型（CRMs）结合时，LLMs常常面临高推理成本和静态知识迁移方法带来的挑战。在本文中，我们提出了一种新颖的互蒸馏框架，称为LLMD4Rec，该框架促进了以LLM为中心和基于CRM的推荐系统之间的动态和双向知识交换。与传统的单向蒸馏方法不同，LLMD4Rec通过交替优化两个模型，实现了迭代优化，增强了CRMs的语义理解，并通过用户-物品交互的协作信号丰富了LLMs。通过利用样本级自适应权重和输出分布的对齐，我们的方法消除了对额外参数的需求，同时确保了有效的知识迁移。在真实世界数据集上的大量实验表明，LLMD4Rec在多个基准测试中显著提高了推荐准确性，而没有增加推理成本。这种方法为结合LLMs和CRMs在序列推荐系统中的优势提供了一种可扩展且高效的解决方案。|
|**2025-05-23**|**Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM**|Zinuo Li et.al.|[2505.18110](http://arxiv.org/abs/2505.18110)|null|人类自然地通过整合视觉和听觉线索来理解视频中的瞬间。例如，将视频中的场景定位为“一位科学家在戏剧性的管弦乐伴奏下热情地谈论野生动物保护，观众点头鼓掌”，这需要同时处理视觉、音频和语音信号。然而，现有的模型往往难以有效地融合和解释音频信息，限制了它们对视频时间序列的全面理解能力。为了解决这个问题，我们提出了TriSense，这是一个三模态大型语言模型，通过整合视觉、音频和语音模态来实现整体视频时间序列理解。TriSense的核心是一个基于查询的连接器，它根据输入查询自适应地重新加权模态贡献，使模型在模态丢失的情况下仍能保持稳健的性能，并允许灵活地组合可用的输入。为了支持TriSense的多模态能力，我们引入了TriSense-2M，这是一个包含超过200万个经过精心挑选的样本的高质量数据集，通过由微调的大型语言模型驱动的自动化流程生成。TriSense-2M包括长视频和多种模态组合，促进了广泛的泛化。在多个基准上的大量实验证明了TriSense的有效性及其在多模态视频分析方面的潜力。代码和数据集将公开发布。|
|**2025-05-23**|**ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework**|Lisheng Huang et.al.|[2505.18105](http://arxiv.org/abs/2505.18105)|**[link](https://github.com/rucaibox/manusearch)**|近期，在网页增强大型语言模型（LLMs）的领域取得了显著进展，这些模型在复杂推理任务中表现出色，然而这些能力大多被锁定在具有不透明架构的专有系统中。在这项工作中，我们提出了\textbf{ManuSearch}，一个透明且模块化的多智能体框架，旨在为LLMs的深度搜索实现民主化。ManuSearch将搜索和推理过程分解为三个协作智能体：（1）一个解决方案规划智能体，它迭代地制定子查询，（2）一个互联网搜索智能体，通过实时网络搜索检索相关文档，以及（3）一个结构化网页阅读智能体，从原始网页内容中提取关键证据。为了严格评估深度推理能力，我们引入了\textbf{ORION}，这是一个针对开放网页上长尾实体的推理的具有挑战性的基准，涵盖了英语和中文。实验结果表明，ManuSearch在性能上显著优于现有的开源基准，甚至超过了领先的闭源系统。我们的工作为开放深度搜索系统的可重复和可扩展研究铺平了道路。我们已在https://github.com/RUCAIBox/ManuSearch上发布了数据和代码。|
|**2025-05-22**|**SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward**|Kaixuan Fan et.al.|[2505.17018](http://arxiv.org/abs/2505.17018)|**[link](https://github.com/kxfan2002/sophiavl-r1)**|**近期的研究表明，通过基于规则的强化学习（RL）和结果奖励在多模态大型语言模型（MLLMs）中诱发强大的推理能力取得了成功。然而，这种范式通常缺乏对最终结果产生过程的监督。因此，模型可能会学习到次优的推理策略，这可能会阻碍其泛化能力。鉴于这一点，我们提出了SophiaVL-R1，作为在该范式下为思考过程添加奖励信号的尝试。为了实现这一点，我们首先训练了一个思考奖励模型，该模型评估整个思考过程的质量。鉴于思考奖励可能因奖励黑客攻击而在某些样本上不可靠，我们提出了Trust-GRPO方法，该方法在训练过程中为思考奖励分配一个可信度权重。这个权重是基于导致正确答案和错误答案的响应的思考奖励比较来计算的，有助于减轻不可靠思考奖励的影响。此外，我们设计了一种退火训练策略，随着时间的推移逐渐减少思考奖励，使模型在后续训练阶段更多地依赖准确的基于规则的最终结果奖励。实验表明，我们的SophiaVL-R1在各种基准（例如MathVisita、MMMU）上优于一系列推理MLLMs，展示了强大的推理和泛化能力。值得注意的是，我们的SophiaVL-R1-7B甚至在大多数基准上优于LLaVA-OneVision-72B，尽管后者的参数多10倍。所有代码、模型和数据集均公开提供，可在https://github.com/kxfan2002/SophiaVL-R1上获取。**|
|**2025-05-22**|**Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework**|Chenhao Zhang et.al.|[2505.17019](http://arxiv.org/abs/2505.17019)|**[link](https://github.com/ming-zch/let-androids-dream-of-electric-sheep)**|**图像中的隐喻理解对人工智能系统来说仍然是一个重大挑战，因为现有模型难以把握视觉内容中嵌入的微妙文化、情感和语境含义。虽然多模态大型语言模型（MLLMs）在基本的视觉问答（VQA）任务上表现出色，但在图像含义任务上存在一个根本的局限性：语境间隙模糊了不同视觉元素及其抽象含义之间的关系。受人类认知过程的启发，我们提出了“让安卓们梦想”（LAD）框架，这是一种新的图像含义理解和推理框架。LAD通过三个阶段的框架来解决语境缺失问题：（1）感知：将视觉信息转换为丰富和多层次的文本表示，（2）搜索：迭代地搜索和整合跨领域知识以解决歧义，（3）推理：通过显式推理生成与语境一致的图像含义。我们的框架与轻量级的GPT-4o-mini模型相比，在英语图像含义基准测试中实现了与15+ MLLMs相比的SOTA性能，并在中文基准测试中取得了巨大改进，在多项选择题（MCQ）上的表现与GPT-4o模型相当，在开放式问题（OSQ）上超越了36.7%。此外，我们的研究为AI如何更有效地解释图像含义提供了新的见解，推动了视觉语言推理和人机交互领域的进步。我们的项目在https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep上公开发布。**|
|**2025-05-22**|**CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms**|Shilin Yan et.al.|[2505.17020](http://arxiv.org/abs/2505.17020)|**[link](https://github.com/shilinyan99/crosslmm)**|**随着大型多模态模型（LMMs）的出现，大型语言模型（LLMs）在处理和解释多种数据模态（如图像和视频）方面得到了显著提升。然而，随着输入复杂性的增加，特别是对于长视频序列，所需的token数量显著增加，导致计算成本呈平方级增长。这使得在保持性能完整性的同时，高效压缩LMMs中的视频tokens成为一个紧迫的研究挑战。在本文中，我们引入了CrossLMM，通过双重交叉注意力机制将长视频序列从LMMs中解耦，这显著减少了视觉token数量，同时性能下降最小。具体来说，我们首先通过池化方法从预训练的视觉编码器中实现了显著的token减少。然后在LLM层内，我们采用视觉到视觉的交叉注意力机制，其中池化的视觉tokens作为查询与原始视觉token集进行对比。这个模块使token利用更加高效，同时保持精细的信息忠实度。此外，我们还引入了文本到视觉的交叉注意力机制，其中文本tokens通过与原始视觉tokens的交互得到增强，丰富了文本tokens的视觉理解。全面的实证评估表明，尽管使用了大量的计算资源，我们的方法在各种基于视频的LMM基准测试中实现了相当或优于其他方法的性能。**|
|**2025-05-22**|**Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO**|Chengzhuo Tong et.al.|[2505.17017](http://arxiv.org/abs/2505.17017)|**[link](https://github.com/ziyuguo99/image-generation-cot)**|**最近的研究突出了强化学习（RL）在增强大型语言模型（LLM）的思考链（CoT）推理能力方面的重要作用。两种突出的RL算法，直接偏好优化（DPO）和组相对策略优化（GRPO），是这些进展的核心，展示了不同的优缺点。自回归图像生成，也可以解释为序列CoT推理过程，面临独特的挑战，这些挑战与基于LLM的CoT推理不同。这包括确保文本图像一致性、提高图像美学质量以及设计复杂的奖励模型，而不是依赖于更简单的基于规则的奖励。尽管最近的努力将RL扩展到这个领域，但这些探索通常缺乏对领域特定挑战和不同RL策略特点的深入分析。为了弥合这一差距，我们对GRPO和DPO算法在自回归图像生成中的使用进行了首次全面调查，评估了它们在领域内和领域外的表现，并仔细审视了不同奖励模型对其各自能力的影响。我们的发现表明，GRPO和DPO显示出不同的优势，更重要的是，拥有更强内在泛化能力的奖励模型有可能提高应用RL算法的泛化潜力。此外，我们系统地探讨了三种流行的扩展策略，以提高它们在领域内和领域外的能力，为每个范式高效扩展性能提供了独特的见解。我们希望我们的研究为新路径开辟了新思路，以激励未来关于开发更有效的RL算法在自回归图像生成领域实现稳健的CoT推理。代码已发布在https://github.com/ZiyuGuo99/Image-Generation-CoT。**|
|**2025-05-22**|**Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models**|Runsen Xu et.al.|[2505.17015](http://arxiv.org/abs/2505.17015)|null|多模态大型语言模型（MLLMs）在视觉任务方面取得了快速进步，但它们的空间理解能力仍然局限于单张图像，这使得它们不适合需要多帧推理的机器人学和其他现实世界应用。在本文中，我们提出了一种框架，通过整合深度感知、视觉对应和动态感知，使MLLMs具备稳健的多帧空间理解能力。我们方法的核心是MultiSPA数据集，这是一个包含超过2700万个样本的全新、大规模集合，涵盖了多样化的3D和4D场景。与MultiSPA相伴，我们引入了一个全面的基准，它以统一的指标测试了广泛的空间任务。我们得到的模型，Multi-SpatialMLLM，在基线和专有系统上取得了显著的提升，证明了可扩展和可泛化的多帧推理能力。我们进一步观察到多任务效益以及在具有挑战性的场景中涌现能力的早期迹象，并展示了我们的模型如何作为机器人学多帧奖励标注器的作用。|
|**2025-05-22**|**SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding**|Haoning Wu et.al.|[2505.17012](http://arxiv.org/abs/2505.17012)|**[link](https://github.com/haoningwu3639/SpatialScore)**|**多模态大型语言模型（MLLMs）在问答任务中取得了令人瞩目的成功，但它们在空间理解方面的能力研究相对较少。本研究探讨了以下关键问题：现有的MLLMs是否具备3D空间感知和理解能力？具体来说，本文做出了以下贡献：（i）我们引入了VGBench，这是一个专门设计来评估MLLMs在视觉几何感知方面的基准，例如相机姿态和运动估计；（ii）我们提出了SpatialScore，这是迄今为止最全面且多样化的多模态空间理解基准，将VGBench与来自其他11个现有数据集的相关数据集成。这个基准包含了涵盖各种空间理解任务、模态和问答格式的28K个样本，以及精心挑选的具有挑战性的子集SpatialScore-Hard；（iii）我们开发了SpatialAgent，一个新颖的多智能体系统，集成了9个用于空间理解的专用工具，支持计划-执行和ReAct推理范式；（iv）我们进行了广泛的评估，以揭示空间推理中的持续挑战，并展示了SpatialAgent的有效性。我们相信，SpatialScore将为MLLMs的下一阶段发展提供宝贵的见解，并作为一个严格的基准。**|
|**2025-05-22**|**R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning**|Huatong Song et.al.|[2505.17005](http://arxiv.org/abs/2505.17005)|**[link](https://github.com/rucaibox/r1-searcher-plus)**|**大型语言模型（LLMs）功能强大，但因其静态知识而容易产生幻觉。检索增强生成（RAG）通过注入外部信息来帮助解决这个问题，但现有方法往往成本高昂、泛化能力差，或者忽略了模型的内部知识。在本文中，我们介绍了一种名为R1-Searcher++的新框架，旨在训练LLMs自适应地利用内部和外部知识源。R1-Searcher++采用两阶段训练策略：首先是用于初步格式学习的SFT冷启动阶段，然后是用于动态知识获取的强化学习（RL）。在RL阶段，使用结果监督来鼓励探索，引入内部知识利用的奖励机制，并集成记忆机制以持续吸收检索到的信息，从而丰富模型的内部知识。通过利用内部知识和外部搜索引擎，模型不断改进其能力，实现高效的检索增强推理。我们的实验表明，R1-Searcher++优于之前的RAG和推理方法，并实现了高效的检索。代码可在https://github.com/RUCAIBox/R1-Searcher-plus获取。**|
|**2025-05-22**|**Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?**|Jin Jiang et.al.|[2505.16998](http://arxiv.org/abs/2505.16998)|**[link](https://github.com/jiangjin1999/formaleval)**|**大型语言模型（LLMs）在复杂逻辑推理任务上已展现出突破性的性能。尽管如此，现有的大部分研究集中在使用形式语言引导LLMs推导可靠的推理路径，而这些能力的系统评估仍然有限。在本文中，我们旨在利用形式语言对LLMs在各个逻辑推理问题上的能力进行全面评估。从三个维度，即LLMs的范围、任务的分类和轨迹的格式，我们的主要发现是：1）思维模型在采用形式语言时显著优于指令模型；2）无论是否使用形式语言，所有LLMs在归纳推理能力方面都存在局限性；3）具有PoT格式的数据在跨语言中实现了最佳泛化性能。此外，我们还精心准备了形式相关的训练数据，以进一步提高小型语言模型，实验结果表明，简单拒绝微调方法可以更好地使LLMs泛化到形式语言中，并实现最佳的整体性能。我们的代码和报告可在https://github.com/jiangjin1999/FormalEval上找到。**|
|**2025-05-22**|**DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization**|Chao Zhang et.al.|[2505.16995](http://arxiv.org/abs/2505.16995)|null|近年来，情感支持对话（ESC）领域的研究通过监督微调（SFT）对大型语言模型（LLMs）进行微调，从而提高了情感支持生成的能力。然而，常见的心理错误仍然存在。尽管直接偏好优化（DPO）通过成对偏好学习显示出减少这些错误的潜力，但其应用于ESC任务的有效性受到两个关键挑战的限制：（1）数据结构纠缠：现有的ESC数据本质上纠缠着心理策略和响应内容，这使得构建高质量的偏好对变得困难；（2）优化模糊性：将传统的DPO应用于这种纠缠的成对数据会导致训练目标模糊。为了解决这些问题，我们引入了推理偏好挖掘（IPM）来构建高质量的偏好数据，形成了IPM-PrefDial数据集。基于这个数据集，我们提出了一个受Gross的情绪调节扩展过程模型启发的解耦ESC框架，将ESC任务分解为两个连续的子任务：策略规划和同理心响应生成。每个子任务都通过SFT进行训练，随后通过DPO进行增强，以符合心理偏好。大量的实验表明，我们的解耦ESC框架优于联合优化基线，减少了偏好偏差并提高了响应质量。|
|**2025-05-22**|**Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding**|Runpeng Yu et.al.|[2505.16990](http://arxiv.org/abs/2505.16990)|**[link](https://github.com/yu-rp/dimple)**|**在这项工作中，我们提出了Dimple，这是第一个离散扩散多模态大型语言模型（DMLLM）。我们观察到，仅使用纯离散扩散方法进行训练会导致显著的训练不稳定、次优性能和严重的长度偏差问题。为了解决这些挑战，我们设计了一种新的训练范式，该方法将初始自回归阶段与随后的扩散阶段相结合。这种方法产生了Dimple-7B模型，它在相同的数据集上训练，并使用与LLaVA-NEXT类似的训练流程。Dimple-7B最终在性能上超越了LLaVA-NEXT 3.9%，证明了DMLLM可以达到与自回归模型相当的性能。为了提高推理效率，我们提出了一种称为自信解码的解码策略，该策略动态调整每个步骤生成的标记数量，显著减少了生成迭代次数。在自回归模型中，生成过程中的前向迭代次数等于响应长度。然而，在自信解码中，Dimple所需的迭代次数仅为 $\frac{\text{响应长度}}{3}$ 。我们还重新实现了自回归模型中的预填充技术，并证明它在大多数基准评估中对性能没有显著影响，同时提供了1.5倍到7倍的加速。此外，我们探索了Dimple使用结构先验精确控制其响应的能力。这些先验以与基于指令或思维链提示不同的方式实现结构化响应，并允许对响应格式和长度进行细粒度控制，这在自回归模型中难以实现。总之，这项工作验证了DMLLM的可行性和优势，并提高了其推理效率和可控性。代码和模型可在https://github.com/yu-rp/Dimple找到。**|
|**2025-05-21**|**The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation**|Patrick Kahardipraja et.al.|[2505.15807](http://arxiv.org/abs/2505.15807)|**[link](https://github.com/pkhdipraja/in-context-atlas)**|大型语言模型能够通过检索增强来利用上下文学习，访问其训练数据之外的额外外部知识。虽然这很有前景，但其内部工作原理仍不明确。在本研究中，我们将提示视为信息组件的合成，从而阐明了问答中上下文检索增强的机制。我们提出了一种基于归因的方法来识别专门化的注意力头，揭示了理解指令并检索相关上下文信息的上下文注意力头，以及存储实体关系知识的参数化注意力头。为了更好地理解它们的作用，我们提取了功能向量并修改了它们的注意力权重，以展示它们如何影响答案生成过程。最后，我们利用获得的见解来追踪推理过程中使用的知识来源，为构建更安全、更透明的语言模型铺平道路。|
|**2025-05-21**|**Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering**|Hwan Chang et.al.|[2505.15805](http://arxiv.org/abs/2505.15805)|**[link](https://github.com/hwanchang00/CoPriva)**|随着大型语言模型（LLMs）在企业和政府等敏感领域的日益应用，确保它们在特定情境下遵守用户定义的安全策略至关重要——特别是在信息保密方面。尽管先前的大型LLM研究集中在一般安全和社交敏感数据上，但针对攻击的情境安全保护的大规模基准测试仍然不足。为了解决这个问题，我们引入了一个新颖的大规模基准数据集，CoPriva，用于评估LLM在问答中遵守情境非保密政策的情况。我们的数据集来源于现实情境，包括明确的政策和查询，这些政策和查询被设计为直接和具有挑战性的间接攻击，以获取禁止的信息。我们在基准测试中评估了10个LLM，并揭示了一个重大漏洞：许多模型违反了用户定义的政策，泄露了敏感信息。这种失败尤其在间接攻击中更为严重，凸显了当前LLM在敏感应用中的安全对齐存在关键差距。我们的分析显示，虽然模型通常能够识别查询的正确答案，但在生成过程中难以融入政策约束。相比之下，当明确提示时，它们表现出修改输出的部分能力。我们的发现强调了确保情境安全需要更稳健方法的紧迫性。|
|**2025-05-21**|**STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs**|Zongzhao Li et.al.|[2505.15804](http://arxiv.org/abs/2505.15804)|**[link](https://github.com/zongzhao23/star-r1)**|多模态大型语言模型（MLLMs）在各种任务中展现出惊人的能力，但在空间推理方面与人类相比仍有较大差距。我们通过变换驱动视觉推理（TVR）来研究这一差距，TVR是一项具有挑战性的任务，需要在不同视角下识别图像中的物体变换。虽然传统的监督微调（SFT）在跨视图设置中无法生成连贯的推理路径，稀疏奖励强化学习（RL）则面临着探索效率低下和收敛速度慢的问题。为了解决这些限制，我们提出了STAR-R1，这是一个将单阶段RL范式与针对TVR量身定制的细粒度奖励机制相结合的全新框架。具体来说，STAR-R1奖励部分正确性，同时惩罚过度枚举和被动不作为，从而实现高效探索和精确推理。全面的评估表明，STAR-R1在所有11个指标上均实现了最先进的性能，在跨视图场景中比SFT提高了23%。进一步的分析揭示了STAR-R1的人性化行为，并突出了其比较所有物体以改进空间推理的独特能力。我们的工作为推动MLLMs和推理模型的研究提供了关键见解。代码、模型权重和数据将在https://github.com/zongzhao23/STAR-R1上公开发布。|
|**2025-05-21**|**VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models**|Yuchen Yan et.al.|[2505.15801](http://arxiv.org/abs/2505.15801)|null|大型推理模型如OpenAI o1和DeepSeek-R1在推理领域取得了显著的成绩。它们训练过程中的一个关键组成部分是在强化学习（RL）中融入可验证的奖励。然而，现有的奖励基准并未评估基于参考的奖励系统，这使得研究人员对RL中使用的验证器的准确性理解有限。在本文中，我们引入了两个基准，VerifyBench和VerifyBench-Hard，用于评估基于参考的奖励系统的性能。这些基准通过精心收集和整理数据，并经过仔细的人类标注以确保高质量而构建。当前模型在VerifyBench和VerifyBench-Hard上仍有很大的改进空间，尤其是小型模型。此外，我们对评估结果进行了全面深入的分析，为理解和开发基于参考的奖励系统提供了见解。我们提出的基准作为有效工具，可指导验证器准确性和通过RL在推理任务中训练的模型推理能力的发展。|
|**2025-05-21**|**Reverse Engineering Human Preferences with Reinforcement Learning**|Lisa Alazraki et.al.|[2505.15795](http://arxiv.org/abs/2505.15795)|null|大型语言模型（LLMs）的能力通常通过训练来预测人类偏好的其他LLMs进行评估。这个框架被称为“LLM作为裁判”，具有高度可扩展性和相对较低的成本。然而，它也容易受到恶意利用，因为LLM的响应可以被调整以过度拟合裁判的偏好。先前的研究表明，候选人-LLM生成的答案可以事后编辑，以最大化由裁判-LLM分配给它们的分数。在本研究中，我们采用了一种不同的方法，并利用裁判-LLM提供的信号作为奖励，来对抗性地调整生成文本序言的模型，这些序言旨在提升下游性能。我们发现，与现有的框架相比，与这些模型流水线化的冻结LLMs获得了更高的LLM评估分数。关键的是，与直接干预模型响应的其他框架不同，我们的方法几乎无法被检测到。我们还证明，当候选-LLM和裁判-LLM被替换为未在训练期间使用的模型时，调整序言生成器的有效性仍然存在。这些发现引发了关于更可靠的LLM作为裁判评估设置设计的重要问题。它们还表明，通过流水线LLMs以强化学习优化上游序言，可以有效地逆向工程人类偏好——这种方法可能在对抗攻击之外的多样任务和领域找到未来的应用。|
|**2025-05-21**|**HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving**|Zhiwen Chen et.al.|[2505.15793](http://arxiv.org/abs/2505.15793)|null|将大型语言模型（LLMs）与强化学习（RL）相结合可以提升复杂场景下自动驾驶（AD）的性能。然而，当前以LLM为主导的RL方法过度依赖LLM的输出，而这些输出容易产生幻觉。评估结果显示，在评估关键驾驶相关任务时，最先进的LLM的非幻觉率仅为约57.95%。因此，在这些方法中，LLM产生的幻觉可能会直接危及驾驶策略的性能。本文认为，保持LLM与RL之间的相对独立性对于解决幻觉问题至关重要。因此，本文致力于提出一种新的LLM-Hinted RL范式。该范式使用LLM生成语义提示以辅助状态增强和政策优化，帮助RL代理进行运动规划，而RL代理则通过策略学习来对抗潜在的语义错误提示，以实现卓越的驾驶性能。基于此范式，我们提出了HCRMP（LLM-Hinted Contextual Reinforcement Learning Motion Planner）架构，该架构包括增强语义表示模块以扩展状态空间。上下文稳定性锚定模块通过利用知识库信息来提高多批判权重提示的可靠性。语义缓存模块用于无缝集成LLM的低频指导和RL的高频控制。在CARLA上的大量实验验证了HCRMP强大的整体驾驶性能。HCRMP在不同交通密度下的多种驾驶条件下实现了高达80.3%的任务成功率。在安全关键驾驶条件下，HCRMP将碰撞率降低了11.4%，有效提升了复杂场景下的驾驶性能。|
|**2025-05-21**|**Large Language Models as Computable Approximations to Solomonoff Induction**|Jun Wan et.al.|[2505.15784](http://arxiv.org/abs/2505.15784)|null|大型语言模型（LLMs）的快速发展需要一套严谨的理论框架来解释其经验上的成功。虽然我们在理解LLMs行为方面取得了显著进展，但现有的理论框架在通过统一的数学视角解释涌现现象方面仍然存在碎片化。我们通过证明两个基本结果，建立了LLM架构与算法信息理论（AIT）之间的第一个正式联系：（1）训练过程通过将损失最小化解释为程序长度优化，在计算上近似索洛蒙诺夫先验；（2）下一个标记的预测实现了近似的索洛蒙诺夫归纳。我们利用AIT为上下文学习、少样本学习和扩展定律提供了统一的理论解释。此外，我们的理论洞察导致了一种基于原则的少样本示例选择方法，该方法优先选择模型预测信心较低的样本。通过在多种文本分类基准上的实验，我们证明了与选择高信心示例相比，这种策略在较小模型架构上实现了显著的性能提升。我们的框架弥合了理论基础与实践LLMs行为之间的差距，为未来的模型开发提供了解释力和可操作的见解。|
|**2025-05-21**|**ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning**|Changtai Zhu et.al.|[2505.15776](http://arxiv.org/abs/2505.15776)|**[link](https://github.com/BeastyZ/ConvSearch-R1)**|对话式搜索系统需要有效地处理依赖上下文的查询，这些查询通常包含歧义、遗漏和指代问题。对话查询重构（CQR）通过将这些查询转换为适用于现成检索器的自包含形式来解决这一挑战。然而，现有的CQR方法存在两个关键的限制：高度依赖昂贵的外部监督（来自人工标注或大型语言模型），以及重写模型与下游检索器之间不够匹配。我们提出了ConvSearch-R1，这是第一个通过利用强化学习来直接通过检索信号优化重构而完全消除对外部重写监督依赖的自驱动框架。我们新颖的两阶段方法结合了自我驱动策略预热来解决冷启动问题，通过检索引导的自蒸馏，然后是带有特别设计的排名激励奖励塑造机制的检索引导强化学习，该机制解决了传统检索指标中的稀疏性问题。在TopiOCQA和QReCC数据集上的大量实验表明，ConvSearch-R1显著优于先前最先进的方法，在具有挑战性的TopiOCQA数据集上实现了超过10%的改进，同时使用了没有外部监督的更小的3B参数模型。|
|**2025-05-21**|**Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention**|Huanxuan Liao et.al.|[2505.15774](http://arxiv.org/abs/2505.15774)|**[link](https://github.com/Xnhyacinth/HyCo2)**|大型语言模型（LLMs）在长序列推理中面临重大挑战，这是由于计算效率低下和冗余处理，因此对上下文压缩技术的兴趣日益增加。现有方法通常依赖于标记重要性来执行硬局部压缩或将上下文编码到潜在表示中进行软全局压缩。然而，文本内容相关性的不均匀分布和用户指令需求的多样性意味着这些方法经常导致潜在有价值信息的丢失。为了解决这个问题，我们提出了针对LLMs的混合上下文压缩（HyCo $_2$），它结合了全局和局部视角来引导上下文压缩，同时保留完成任务所必需的语义和关键细节。具体来说，我们采用混合适配器，利用全局视图来细化全局语义，基于观察不同适配器在不同任务上表现优异。然后我们引入一个分类层，根据局部视图为每个上下文标记分配一个保留概率，确定它是否应该被保留或丢弃。为了促进全局和局部压缩的平衡集成，我们在指令微调之前引入了辅助释义和完成预训练。这促进了协同集成，强调与指令相关的信息同时保留关键局部细节，最终在上下文压缩中平衡局部和全局信息保留。实验表明，我们的HyCo$_2$方法显著提高了长文本推理能力，同时减少了标记的使用。它在七个知识密集型问答基准上平均提高了13.1%的LLM系列性能。此外，HyCo$_2$ 在减少88.8%标记消耗的同时，与未压缩方法的性能相当。|
|**2025-05-21**|**MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling**|Cheng Yifan et.al.|[2505.15772](http://arxiv.org/abs/2505.15772)|null|获取具有高度一致性的大规模情感语音数据一直是语音合成领域的挑战。本文提出了MIKU-PAL，这是一个用于从未标记视频数据中提取高一致性情感语音的完全自动化的多模态流程。利用面部检测和跟踪算法，我们开发了一个使用多模态大型语言模型（MLLM）的自动情感分析系统。我们的结果表明，MIKU-PAL可以实现人类水平的准确率（在MELD上为68.5%）和卓越的一致性（0.93 Fleiss kappa评分），同时其成本和速度远低于人工标注。凭借MIKU-PAL提供的高质量、灵活和一致性的标注，我们可以标注多达26种细粒度语音情感类别，由人工标注员进行验证，合理性评分为83%。基于我们提出的系统，我们进一步发布了一个细粒度情感语音数据集MIKU-EmoBench（131.2小时），作为情感文本到语音和视觉声音克隆的新基准。|
|**2025-05-20**|**Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning**|Haolei Xu et.al.|[2505.14684](http://arxiv.org/abs/2505.14684)|null|大型语言模型（LLMs）通过思维链（CoT）推理在数学任务上取得了显著进展。然而，现有的数学CoT数据集往往因为专家省略中间步骤而存在思维跳跃，这不利于模型的学习和泛化。我们提出了CoT思维跳跃桥接任务，旨在自动检测跳跃并生成缺失的中间推理步骤，以恢复CoT的完整性和连贯性。为此，我们构建了一个名为ScaleQM+的专用训练数据集，基于结构化的ScaleQuestMath数据集，并训练了CoT-Bridge来桥接思维跳跃。通过在数学推理基准上的全面实验，我们表明，在桥接数据集上微调的模型在NuminaMath上的表现始终优于在原始数据集上训练的模型，提升幅度高达+5.87%。我们的方法有效地增强了蒸馏数据（+3.02%）并提供了更好的强化学习起点（+3.1%），作为一个即插即用的模块，与现有的优化技术兼容。此外，CoT-Bridge在域外逻辑推理任务上的泛化能力得到提升，证实了增强推理完整性具有广泛的应用价值。|
|**2025-05-20**|**UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation**|Rui Tian et.al.|[2505.14682](http://arxiv.org/abs/2505.14682)|null|我们介绍了UniGen，这是一个统一的多模态大型语言模型（MLLM），能够理解和生成图像。我们从数据驱动的角度研究了UniGen的完整训练流程，包括多阶段预训练、监督微调和直接偏好优化。更重要的是，我们提出了一种新的思维链验证（CoT-V）策略，用于测试时扩展，通过简单的N中选优测试时策略显著提升了UniGen的图像生成质量。具体来说，CoT-V使UniGen在测试时既能作为图像生成器也能作为验证器，以逐步的思维链（CoT）方式评估文本提示与其生成的图像之间的语义一致性。UniGen在整个训练阶段完全基于开源数据集进行训练，在一系列图像理解和生成基准测试中取得了最先进的性能，GenEval的最终得分为0.78，DPG-Bench的得分为85.19。通过广泛的消融研究，我们的工作提供了可操作的见解，并解决了构建统一MLLM全生命周期的关键挑战，为未来的研究提供了有意义的方向。|
|**2025-05-20**|**UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models**|Xiaojie Gu et.al.|[2505.14679](http://arxiv.org/abs/2505.14679)|**[link](https://github.com/xiaojiegu/ultraedit)**|终身学习使大型语言模型（LLMs）能够通过持续更新其内部知识来适应不断变化的信息。一个理想的系统应该支持高效、广泛的更新，同时保留现有功能并确保可靠部署。模型编辑作为实现这一目标的可行方案，提供了一种集中且高效的途径来修改模型的内部知识。尽管最近的研究范式取得了显著进展，但它们往往难以满足大规模实际终身适应的需求。为了弥合这一差距，我们提出了ULTRAEDIT——一种全新的编辑解决方案，它无需训练、主题和记忆，特别适合于超可扩展的、现实世界的终身模型编辑。ULTRAEDIT通过一个自包含的过程进行编辑，该过程仅依赖于轻量级的线性代数运算来计算参数变化，从而实现快速且一致的参数修改，同时最小化开销。为了提高终身环境中的可扩展性，ULTRAEDIT采用了一种终身归一化策略，它不断更新特征统计信息，使其能够适应分布变化并保持随时间的稳定性。ULTRAEDIT的编辑速度比之前最先进的方法快7倍以上——这也是已知最快的方案——同时消耗的VRAM不到1/3，使其成为目前唯一能够在24GB消费级GPU上编辑70亿参数LLM的方法。此外，我们构建了ULTRAEDITBENCH——迄今为止该领域的最大数据集，包含超过200万个编辑对，并证明了我们的方法在保持高准确性的同时支持高达100万个编辑。在四个数据集和六个模型上的综合实验表明，ULTRAEDIT在多种模型编辑场景中始终表现出优异的性能。我们的代码可在以下链接获取：https://github.com/XiaojieGu/UltraEdit。|
|**2025-05-20**|**Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning**|Jiaer Xia et.al.|[2505.14677](http://arxiv.org/abs/2505.14677)|null|长期以来，在人工智能领域，学习通用推理能力一直是一个具有挑战性的问题。最近在大型语言模型（LLMs）的研究中，例如DeepSeek-R1，表明强化学习技术如GRPO可以使预训练的LLMs通过简单的问答对来发展推理能力。在本文中，我们旨在通过强化学习和视觉问答对来训练视觉语言模型（VLMs）在图像数据上执行推理，而不需要任何显式的思维链（CoT）监督。我们的研究发现，简单地将强化学习应用于VLM——通过提示模型在给出答案之前产生一个推理链——会导致模型从简单问题中发展出捷径，从而降低其在未见过的数据分布上泛化的能力。我们认为，缓解捷径学习的关键是鼓励模型在推理之前解释图像。因此，我们训练模型遵循“描述-推理-回答”的输出格式：首先为图像生成一个详细的描述，然后构建一个广泛的推理链。当在273K个无CoT的视觉问答对上训练，并且仅使用强化学习时，我们名为Visionary-R1的模型在多个视觉推理基准测试中优于强大的多模态模型，如GPT-4o、Claude3.5-Sonnet和Gemini-1.5-Pro。|
|**2025-05-20**|**Reward Reasoning Model**|Jiaxin Guo et.al.|[2505.14674](http://arxiv.org/abs/2505.14674)|null|奖励模型在引导大型语言模型生成符合人类预期的输出中起着关键作用。然而，在有效利用测试时计算来提升奖励模型性能方面，仍存在一个开放性的挑战。在本工作中，我们引入了奖励推理模型（RRMs），这些模型专门设计用于在生成最终奖励之前执行深思熟虑的推理过程。通过思维链推理，RRMs在适当奖励不立即明显的情况下，利用额外的测试时计算进行复杂查询。为了开发RRMs，我们实现了一个强化学习框架，该框架促进自我演化的奖励推理能力，而不需要明确的推理轨迹作为训练数据。实验结果表明，RRMs在跨多个领域的奖励模型基准测试中取得了优越的性能。值得注意的是，我们展示了RRMs可以自适应地利用测试时计算来进一步提高奖励准确性。预训练的奖励推理模型可在https://huggingface.co/Reward-Reasoning获取。|
|**2025-05-20**|**Quartet: Native FP4 Training Can Be Optimal for Large Language Models**|Roberto L. Castro et.al.|[2505.14669](http://arxiv.org/abs/2505.14669)|**[link](https://github.com/ist-daslab/quartet)**|大型语言模型（LLMs）的快速发展伴随着计算需求的空前增长，最先进模型的训练成本每几个月就翻一番。在低精度算术中直接训练模型提供了一种解决方案，通过提高计算吞吐量和能源效率。具体来说，NVIDIA最近的Blackwell架构促进了极低精度的操作，特别是FP4变体，承诺实现显著的效率提升。然而，目前用于在FP4精度下训练LLMs的算法面临着显著的精度下降，并且通常依赖于混合精度的回退。在本文中，我们系统地研究了硬件支持的FP4训练，并介绍了Quartet，这是一种新的方法，能够实现准确、端到端的FP4训练，其中所有主要计算（例如，在线性层中）都在低精度下进行。通过在Llama型模型上进行广泛的评估，我们揭示了一种新的低精度缩放定律，量化了不同位宽下的性能权衡，并使我们能够确定一种“近最优”的低精度训练技术，即Quartet，在精度与计算之间进行权衡。我们使用针对NVIDIA Blackwell GPU优化的CUDA内核实现了Quartet，并表明它可以达到FP4精度的最先进精度，成功训练了亿规模模型。我们的方法表明，完全基于FP4的训练是标准精度和FP8训练的有竞争力的替代方案。我们的代码可在https://github.com/IST-DASLab/Quartet上找到。|
|**2025-05-20**|**ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions**|Bufang Yang et.al.|[2505.14668](http://arxiv.org/abs/2505.14668)|null|近年来，大型语言模型（LLMs）的进步推动了智能代理从被动响应到主动支持的发展。虽然前景广阔，但现有的主动代理要么完全依赖封闭环境（例如桌面UI）中的直接LLM推理，要么采用基于规则的主动通知，这导致对用户意图的理解不充分，主动服务功能有限。在本文中，我们介绍了ContextAgent，这是第一个能够结合广泛感官上下文的上下文感知主动代理，以增强LLM代理的主动能力。ContextAgent首先从可穿戴设备（如视频和音频）的巨量感官感知中提取多维上下文，以理解用户意图。然后，ContextAgent利用感官上下文和历史数据中的人物上下文来预测主动服务的必要性。当需要主动协助时，ContextAgent会进一步自动调用必要的工具，以不干扰用户的方式提供帮助。为了评估这一新任务，我们创建了ContextAgentBench，这是第一个用于评估上下文感知主动LLM代理的基准，涵盖九个日常场景和二十个工具，共计1,000个样本。在ContextAgentBench上的实验表明，ContextAgent在主动预测和工具调用方面的准确率分别比基线高出8.5%和6.0%。我们希望我们的研究能够激发更先进、以人为中心的主动AI助手的开发。|
|**2025-05-20**|**Beyond Words: Multimodal LLM Knows When to Speak**|Zikai Liao et.al.|[2505.14654](http://arxiv.org/abs/2505.14654)|null|尽管基于大型语言模型（LLM）的聊天机器人已在生成连贯且与语境相关的内容方面展现出强大的能力，但它们在理解何时发言，尤其是在进行中的对话中提供简短、及时的回应方面常常遇到困难。这种局限性主要源于它们对文本输入的依赖，缺乏现实世界人类对话中的丰富上下文线索。在本工作中，我们关注实时预测回应类型，重点在于依赖于视觉、音频和文本中微妙的多模态信号的短、反应性话语。为此，我们引入了一个新的多模态数据集，该数据集由现实世界的对话视频构建而成，包含时间对齐的视觉、听觉和文本流。该数据集能够对二元互动中的回应时机进行精细建模。基于这个数据集，我们提出了MM-When2Speak，这是一个多模态的基于LLM的模型，它自适应地整合视觉、听觉和文本上下文来预测何时应该有回应，以及适当的回应类型。实验表明，MM-When2Speak在回应时机的准确性方面显著优于最先进的单模态和基于LLM的基线，相较于领先的商业LLM，实现了高达4倍的改善。这些结果强调了多模态输入对于产生及时、自然和引人入胜的对话人工智能的重要性。|
|**2025-05-20**|**General-Reasoner: Advancing LLM Reasoning Across All Domains**|Xueguang Ma et.al.|[2505.14652](http://arxiv.org/abs/2505.14652)|null|强化学习（RL）最近在提升大型语言模型（LLMs）的推理能力方面展现出巨大的潜力。特别是Deepseek-R1-Zero提出的“Zero”强化学习，能够直接对基础LLMs进行RL训练，无需依赖中间的监督微调阶段。尽管取得了这些进展，但当前针对LLM推理的研究主要集中在对数学和编程领域，这主要得益于数据的丰富性和答案验证的简便性。这限制了此类模型在更广泛领域的应用和泛化，在这些领域中，问题往往有多种答案表示，且数据更为稀缺。在本文中，我们提出了通用推理器（General-Reasoner），这是一种新型的训练范式，旨在增强LLM在各个领域的推理能力。我们的主要贡献包括：（1）构建了一个大规模、高质量的问答数据集，通过网络爬虫收集了可验证的答案，涵盖了广泛的学科领域；（2）开发了一个基于生成模型的答案验证器，它用思维链和上下文感知的能力取代了传统的基于规则的验证。我们在涵盖物理学、化学、金融、电子等领域的大量数据集上训练了一系列模型，并进行了评估。我们对这12个基准（例如MMLU-Pro、GPQA、SuperGPQA、TheoremQA、BBEH和MATH AMC）的综合评估表明，通用推理器优于现有的基线方法，在保持数学推理任务中卓越的有效性的同时，实现了稳健和可泛化的推理性能。|
|**2025-05-20**|**Think Only When You Need with Large Hybrid-Reasoning Models**|Lingjie Jiang et.al.|[2505.14631](http://arxiv.org/abs/2505.14631)|null|近期的大型推理模型（LRMs）通过在生成最终回复之前融入扩展的思考过程，在推理能力上相较于传统的大型语言模型（LLMs）有了显著提升。然而，过长的思考过程带来了在token消耗和延迟方面的巨大开销，这对于简单的查询来说尤为不必要。在本工作中，我们引入了大型混合推理模型（LHRMs），这是第一种能够根据用户查询的上下文信息自适应地确定是否进行思考的模型。为了实现这一点，我们提出了一种包含两阶段训练流程的方案，以混合微调（HFT）作为冷启动，随后通过在线强化学习与所提出的混合组策略优化（HGPO）进行学习，以隐式选择合适的思考模式。此外，我们引入了一个称为混合准确度的指标，用于定量评估模型进行混合思考的能力。大量的实验结果表明，LHRMs能够根据查询的难度和类型自适应地进行混合思考。它在推理和通用能力方面优于现有的LRMs和LLMs，同时显著提高了效率。总之，我们的工作倡导重新考虑扩展思考过程的适当使用，并为构建混合思考系统提供了一个坚实的起点。|
|**2025-05-19**|**Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards**|Xiaoyuan Liu et.al.|[2505.13445](http://arxiv.org/abs/2505.13445)|**[link](https://github.com/xyliu-cs/rise)**|大型语言模型（LLMs）在复杂推理方面展现出巨大潜力，其中可验证奖励的强化学习（RLVR）是一种关键的增强策略。然而，一个普遍存在的问题是“表面自我反思”，即模型无法稳健地验证自己的输出。我们介绍了RISE（通过自我验证强化推理），这是一种新颖的在线强化学习框架，旨在解决这一问题。RISE明确且同时训练一个LLM，在单个、集成的强化学习过程中提高其问题解决和自我验证能力。其核心机制涉及利用结果验证器提供的可验证奖励，为解决方案生成和自我验证任务提供即时反馈。在每次迭代中，模型生成解决方案，然后对其策略生成的解决方案进行批判，这两个轨迹都对策略更新做出贡献。在多样化的数学推理基准上的大量实验表明，RISE一致地提高了模型的问题解决准确性，同时培养出强大的自我验证技能。我们的分析突出了在线验证的优势以及增加验证计算的好处。此外，RISE模型在推理过程中表现出更频繁和更准确的自我验证行为。这些优势使RISE成为开发更稳健和自我意识推理者的灵活而有效的方法。|
|**2025-05-19**|**Optimizing Anytime Reasoning via Budget Relative Policy Optimization**|Penghui Qi et.al.|[2505.13438](http://arxiv.org/abs/2505.13438)|**[link](https://github.com/sail-sg/anytimereasoner)**|**在增强大型语言模型（LLMs）推理能力的过程中，测试时计算量的扩展至关重要。现有方法通常采用强化学习（RL）来最大化推理过程中获得的可验证奖励。然而，这些方法只优化了在大型且固定token预算下的最终性能，这阻碍了训练和部署中的效率。在本研究中，我们提出了一种新颖的框架，AnytimeReasoner，用于优化任意时间推理性能，旨在提高token效率以及在可变token预算约束下的推理灵活性。为此，我们将完整的思考过程截断，以适应先前分布中采样的token预算，迫使模型为每个截断的思考总结出最佳答案以供验证。这引入了可验证的密集奖励到推理过程中，促进了RL优化中更有效的信用分配。然后，我们以解耦的方式优化思考和总结策略，以最大化累积奖励。此外，我们引入了一种新颖的方差减少技术，预算相对策略优化（BRPO），以增强强化思考策略时的学习过程鲁棒性和效率。在数学推理任务中的实证结果表明，我们的方法在各种先验分布下，在所有思考预算中都优于GRPO，提高了训练和token效率。**|
|**2025-05-19**|**SMOTExT: SMOTE meets Large Language Models**|Mateusz Bystroński et.al.|[2505.13434](http://arxiv.org/abs/2505.13434)|null|数据稀缺和类别不平衡是训练鲁棒的NLP模型时持续存在的挑战，尤其是在专业领域或低资源环境中。我们提出了一种新颖的技术，SMOTExT，它将合成少数类过采样（SMOTE）的概念应用于文本数据。我们的方法通过在两个现有示例的基于BERT的嵌入之间进行插值来生成新的合成示例，然后使用xRAG架构将得到的潜在点解码成文本。通过利用xRAG的跨模态检索-生成框架，我们可以有效地将插值向量转换为连贯的文本。虽然这是一项仅由定性输出支持的开创性工作，但该方法在少样本设置中的知识蒸馏和数据增强方面显示出强大的潜力。值得注意的是，我们的方法在隐私保护机器学习方面也显示出希望：在早期实验中，仅使用生成的数据进行模型训练与在原始数据集上训练的模型达到了可比的性能。这表明在数据保护约束下安全有效的学习是一条可行的途径。|
|**2025-05-19**|**Fine-tuning Quantized Neural Networks with Zeroth-order Optimization**|Sifeng Shang et.al.|[2505.13430](http://arxiv.org/abs/2505.13430)|**[link](https://github.com/maifoundations/qzo)**|随着大型语言模型的规模呈指数级增长，GPU内存已成为将这些模型应用于下游任务的瓶颈。在本文中，我们旨在通过在统一框架内最小化模型权重、梯度和优化器状态的记忆使用量来推动内存高效训练的极限。我们的想法是利用零阶优化消除梯度和优化器状态，通过在正向传播中扰动权重来近似梯度以确定梯度方向。为了最小化权重上的内存使用，我们采用模型量化，例如将bfloat16转换为int4。然而，由于离散权重和连续梯度之间的精度差距，直接将零阶优化应用于量化权重是不可行的，因为这需要反量化再量化。为了克服这一挑战，我们提出了量化零阶优化（QZO），这是一种新颖的方法，它扰动连续量化尺度以进行梯度估计，并使用方向导数裁剪方法来稳定训练。QZO与基于标量和基于代码本的后训练量化方法都是正交的。与bfloat16中的全参数微调相比，QZO可以将4位LLMs的总内存成本降低超过18倍，并使Llama-2-13B和Stable Diffusion 3.5大型模型在一个24GB GPU内进行微调成为可能。|
|**2025-05-19**|**MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision**|Lingxiao Du et.al.|[2505.13427](http://arxiv.org/abs/2505.13427)|**[link](https://github.com/modalminds/mm-prm)**|**在多模态大型语言模型（MLLMs）在视觉语言理解方面取得了令人印象深刻的进展的同时，它们在复杂的多步推理上仍然存在困难，常常产生逻辑上不一致或部分正确的解决方案。一个关键的限制在于缺乏对中间推理步骤的细粒度监督。为了解决这个问题，我们提出了MM-PRM，这是一个在完全自动化、可扩展的框架内训练的过程奖励模型。我们首先构建了MM-Policy，这是一个在多种数学推理数据上训练的强大多模态模型。然后，我们构建了MM-K12，这是一个包含10,000个具有可验证答案的多模态数学问题的精选数据集，作为种子数据。利用基于蒙特卡洛树搜索（MCTS）的流程，我们生成了超过700k个步骤级别的标注，无需人工标注。得到的PRM用于在Best-of-N推理设置中对候选推理路径进行评分，并在领域内（MM-K12测试集）和领域外（OlympiadBench、MathVista等）基准测试中实现了显著的改进。进一步的分析证实了软标签、较小的学习率和路径多样性在优化PRM性能方面的有效性。MM-PRM表明，过程监督是增强多模态推理系统逻辑鲁棒性的强大工具。我们在https://github.com/ModalMinds/MM-PRM上发布了所有我们的代码和数据。**|
|**2025-05-19**|**Learnware of Language Models: Specialized Small Language Models Can Do Big**|Zhi-Hao Tan et.al.|[2505.13425](http://arxiv.org/abs/2505.13425)|**[link](https://github.com/learnware-lamda/learnware)**|学习件范式通过使用户能够复用一组经过良好训练的模型来完成模型原始用途之外的任务，为机器学习提供了一种新颖的方法。它消除了从头构建模型的需求，而是依赖规格（模型的能力的表示）来识别和利用最适合新任务的最合适模型。虽然学习件在许多场景中已被证明是有效的，但将其应用于语言模型的研究仍然大多未涉足。同时，大型语言模型（LLMs）已经显示出令人瞩目的通用问答能力，但由于数据稀缺、隐私担忧和高计算成本，它们在特定场景中面临挑战，因此越来越多的专门化小型语言模型（SLMs）正在为特定领域进行训练。为了系统地解决这些局限性，学习件范式通过最大程度地利用专门化SLMs，并允许用户以协作和隐私保护的方式识别和复用它们，提供了一种有前景的解决方案。本文提出了一种将学习件范式应用于语言模型的初步尝试。我们模拟了一个包含大约100个参数为8B的专门化SLM学习件系统，这些学习件在金融、医疗和数学领域进行了微调。每个学习件包含一个SLM和一个规格，使用户能够在不泄露自己的数据的情况下识别最相关的模型。实验结果表明了有希望的性能：通过为每个特定任务的推理选择一个合适的学习件，系统在所有基准测试中均优于基础SLMs。与LLMs相比，在金融领域任务中，该系统在至少14%的程度上优于Qwen1.5-110B、Qwen2.5-72B和Llama3.1-70B-Instruct，在医疗领域任务中，该系统超越了Flan-PaLM-540B（在开放医疗LLM排行榜上排名第7）。|
|**2025-05-19**|**Make Still Further Progress: Chain of Thoughts for Tabular Data Leaderboard**|Si-Yang Liu et.al.|[2505.13421](http://arxiv.org/abs/2505.13421)|null|表格数据是机器学习中的基本数据格式，在竞赛和实际应用中广泛使用。由于特征分布和任务特性的差异，表格模型的性能（如梯度提升决策树和神经网络）在不同数据集之间可能会有很大差异。要在每个数据集上实现顶级性能通常需要专门的专家知识。为了应对这种可变性，从业者通常会聚合多个模型的预测。然而，传统的聚合策略通常依赖于静态的组合规则，缺乏实例级的适应性。在本研究中，我们提出了一种针对表格预测的上下文集成框架，该框架利用大型语言模型（LLMs）执行动态、实例特定的外部模型预测集成。无需访问原始表格特征或语义信息，我们的方法使用每个测试实例的最近邻和外部模型池中的预测来构建其上下文。在这个丰富的上下文中，我们引入了表格思维链（CoT $^2$ ），一种提示策略，指导LLMs通过多步、可解释的推理，使决策更接近专家水平。实验结果表明，我们的方法在广泛的表格数据集上优于精心调整的基线方法和标准集成技术。|
|**2025-05-19**|**FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning**|Zhuozhao Hu et.al.|[2505.13419](http://arxiv.org/abs/2505.13419)|**[link](https://github.com/953206211/feallm)**|**面部情感分析（FEA）在视觉情感计算中扮演着至关重要的角色，旨在根据面部数据推断一个人的情绪状态。从科学的角度来看，面部表情（FEs）是由面部肌肉的协调运动产生的，这些运动可以分解为特定的动作单元（AUs），从而提供详细的情感洞察。然而，传统方法通常在可解释性、泛化能力和推理能力方面存在局限性。最近，多模态大型语言模型（MLLMs）在各种视觉任务中表现出色，但由于缺乏专门的训练数据集以及无法捕捉面部表情（FEs）和动作单元（AUs）之间复杂关系的能力，它们在面部情感分析（FEA）方面仍面临重大挑战。为了解决这些问题，我们引入了一个新的面部情感分析指令数据集，该数据集提供了准确且对齐的面部表情和动作单元描述，并建立了它们之间的因果关系，随后构建了一个新的基准，即FEABench。此外，我们提出了FEALLM，这是一种新型的MLLM架构，旨在捕捉更详细的面部信息，从而增强其在面部情感分析任务中的能力。我们的模型在FEABench上表现出强大的性能，并通过在包括RAF-DB、AffectNet、BP4D和DISFA在内的各种数据集上的零样本评估，展示了其出色的泛化能力，彰显了其在面部情感分析任务中的鲁棒性和有效性。数据集和代码将在https://github.com/953206211/FEALLM上提供。**|
|**2025-05-19**|**CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process**|Jinhe Bi et.al.|[2505.13408](http://arxiv.org/abs/2505.13408)|null|近期的大型推理模型通过学习推理能力，显著提高了大型语言模型的推理能力，并在解决复杂任务中展现出有前景的性能。LRMs通过显式地生成推理轨迹和答案来解决需要复杂推理的任务。然而，判断这种输出答案的质量并不容易，因为仅仅考虑答案的正确性是不够的，推理轨迹部分的合理性也同样重要。从逻辑上讲，如果推理部分的合理性较差，即使答案正确，推导出的答案的置信度也应该很低。现有方法在考虑推理部分的同时，对整体输出答案进行联合评估，但它们的性能仍然不尽如人意，因为推理与结论答案之间的因果关系无法得到恰当反映。在这篇论文中，受经典力学的启发，我们提出了一种建立CoT-Kinetics能量方程的新方法。具体来说，我们的CoT-Kinetics能量方程将LRM内部Transformer层调节的标记状态转换过程，类似于在机械场中受控的粒子动力学动力学。我们的CoT-Kinetics能量为推理阶段的合理性分配一个标量分数，以评估根据评估的推理得出的答案的置信度。因此，LRM的整体输出质量可以准确测量，而不再是粗略的判断（例如，正确或错误）。|
|**2025-05-19**|**AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database**|Rong Bian et.al.|[2505.13406](http://arxiv.org/abs/2505.13406)|null|数学知识图谱（KG）以结构化的方式呈现数学领域的知识。使用自然语言构建数学KG是一项重要但具有挑战性的任务。现有工作的两个主要局限性是：首先，它们受限于语料库的完整性，通常丢弃或手动补充不完整的知识；其次，它们通常无法完全自动化整合不同的知识来源。本文提出了AutoMathKG，这是一个高质量、广覆盖和多维度的数学KG，能够实现自动更新。AutoMathKG将数学视为由定义、定理和问题实体组成的一个庞大的有向图，它们的参考关系作为边。它通过在上下文中学习进行数据增强，利用大语言模型（LLMs）整合来自ProofWiki、教科书、arXiv论文和TheoremQA的知识，增强实体和关系。为了搜索相似实体，通过使用SBERT设计了两种嵌入策略，构建了MathVD，一个向量数据库。为了自动更新，提出了两种机制。对于知识补全机制，开发了Math LLM与AutoMathKG交互，提供缺失的证明或解决方案。对于知识融合机制，使用MathVD检索相似实体，并使用LLM确定是否与候选实体合并或添加为新实体。广泛的实验证明了AutoMathKG系统的先进性能和广泛适用性，包括在MathVD中与五个基线相比的优越可达性查询结果，以及在Math LLM中的强大数学推理能力。|
|**2025-05-16**|**msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML**|Zhaolan Huang et.al.|[2505.11483](http://arxiv.org/abs/2505.11483)|**[link](https://github.com/tinypart/msf-cnn)**|AI技术涵盖了从大型语言模型到在微控制器（MCU）上运行的微型模型。极度内存高效的模型架构对于适应MCU微小的内存预算至关重要，例如128kB的RAM。然而，推理延迟必须保持小，以满足实时性要求。解决这一问题的方法之一是基于补丁的融合，其目标是优化神经网络层之间的数据流。在本文中，我们介绍了一种名为msf-CNN的新技术，它通过遍历表示为有向无环图（DAG）的融合解决方案空间，有效地为卷积神经网络（CNN）找到最优融合设置。与之前针对MCU的CNN融合研究相比，msf-CNN识别出更广泛的解决方案集。我们发布了一个在多种微控制器（ARM Cortex-M、RISC-V、ESP32）上运行的msf-CNN实现。我们表明，msf-CNN相较于现有技术（MCUNetV2和StreamNet）可以实现推理，使用的RAM减少了50%。因此，我们展示了msf-CNN为系统设计者提供了额外的灵活性。|
|**2025-05-16**|**Improving Assembly Code Performance with Large Language Models via Reinforcement Learning**|Anjiang Wei et.al.|[2505.11480](http://arxiv.org/abs/2505.11480)|null|大型语言模型（LLMs）在广泛的编程任务中表现出强大的性能，然而它们在代码优化方面的潜力仍被低估。本研究探讨了LLMs是否能够优化汇编代码的性能，因为对执行过程的精细控制可以实现难以用高级语言表达的提升。我们提出了一种强化学习框架，使用近端策略优化（PPO）训练LLMs，该框架由一个奖励函数指导，该函数既考虑了功能正确性（通过测试用例验证），又考虑了与行业标准编译器gcc -O3相比的执行性能。为了支持这项研究，我们引入了一个包含8,072个真实世界程序的基准。我们的模型Qwen2.5-Coder-7B-PPO在测试通过率达到了96.0%，平均速度比gcc -O3基准快1.47倍，优于所有其他20个评估的模型，包括Claude-3.7-sonnet。这些结果表明，强化学习可以释放LLMs作为汇编代码性能有效优化器的潜力。|
|**2025-05-16**|**HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages**|Zhilin Wang et.al.|[2505.11475](http://arxiv.org/abs/2505.11475)|null|偏好数据集对于使用人类反馈的强化学习（RLHF）训练通用领域的指令遵循语言模型至关重要。每一次后续的数据发布都会提高对未来数据收集的期望，这意味着持续需要提升公开可用的偏好数据的质量和多样性。为了满足这一需求，我们引入了HelpSteer3-Preference，这是一个许可宽松（CC-BY-4.0）、高质量、人工标注的偏好数据集，包含超过40,000个样本。这些样本涵盖了大型语言模型（LLMs）在STEM、编码和多语言场景中的各种实际应用。使用HelpSteer3-Preference，我们训练的奖励模型（RMs）在RM-Bench（82.4%）和JudgeBench（73.7%）上取得了顶尖性能。这比现有RMs之前报告的最佳结果有显著提升（绝对值提升约10%）。我们还展示了HelpSteer3-Preference也可以用于训练生成性RMs，以及如何使用我们的RMs将策略模型与RLHF对齐。数据集（CC-BY-4.0）：https://huggingface.co/datasets/nvidia/HelpSteer3#preference|
|**2025-05-16**|**Disentangling Reasoning and Knowledge in Medical Large Language Models**|Rahul Thapa et.al.|[2505.11462](http://arxiv.org/abs/2505.11462)|null|在大语言模型（LLMs）中的医学推理旨在模拟临床医生的诊断思维，但当前基准如MedQA-USMLE、MedMCQA和PubMedQA通常将推理与事实回忆混合在一起。我们通过使用PubMedBERT分类器，该分类器达到81%的准确率，与人类表现相当，将11个生物医学问答基准分为推理和知识重点子集来解决此问题。我们的分析表明，只有32.8%的问题需要复杂的推理。我们评估了生物医学模型（HuatuoGPT-o1、MedReason、m1）和通用领域模型（DeepSeek-R1、o4-mini、Qwen3），发现知识和推理性能之间存在一致差距。例如，m1在知识测试中得分为60.5，但在推理测试中仅为47.1。在对抗测试中，当模型被错误的初始推理误导时，生物医学模型急剧退化，而较大或RL训练的通用模型则表现出更强的鲁棒性。为了解决这个问题，我们使用微调和强化学习在侧重推理的例子上训练BioMed-R1。它在类似规模的模型中取得了最强大的性能。进一步的改进可能来自纳入临床病例报告和训练对抗和回溯场景。|
|**2025-05-16**|**ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks**|Zhixiong Zhuang et.al.|[2505.11459](http://arxiv.org/abs/2505.11459)|null|将大型语言模型（LLMs）集成到广泛的应用中突出了精心设计的系统提示的重要性，这需要广泛的测试和领域专业知识。这些提示可以提高任务性能，但同时也可能编码敏感信息和过滤标准，如果泄露将带来安全风险。最近的研究表明，系统提示容易受到提取攻击，而现有的防御措施要么容易被绕过，要么需要不断更新以应对新的威胁。在这项工作中，我们引入了ProxyPrompt，这是一种新颖的防御机制，通过用代理提示替换原始提示来防止提示泄露。这个代理提示保持了原始任务的效用，同时混淆了提取的提示，确保攻击者无法重现任务或访问敏感信息。在264对LLM和系统提示的综合评估中，ProxyPrompt保护了94.70%的提示免受提取攻击，优于下一个最佳防御措施，后者仅达到42.80%。|
|**2025-05-16**|**LLMs unlock new paths to monetizing exploits**|Nicholas Carlini et.al.|[2505.11449](http://arxiv.org/abs/2505.11449)|null|我们主张大型语言模型（LLMs）将很快改变网络攻击的经济模式。不再针对最常用的软件，通过针对受害者中最低的共同点来货币化漏洞，LLMs使对手能够针对每个用户进行定制攻击。在利用方面，不再是人类攻击者手动搜索具有数百万用户的产品中的一个难以识别的漏洞，LLMs可以在具有数千用户的产品中找到数千个易于识别的漏洞。在货币化方面，不再是总是执行相同攻击的通用勒索软件（例如，加密所有数据并要求支付解密费用），由LLM驱动的勒索软件攻击可以根据被利用设备的具体内容定制勒索要求。我们表明，使用最先进的LLMs，这两种攻击（以及几种其他攻击）是即将实现的。例如，我们表明，在没有人为干预的情况下，一个LLM在安然电子邮件数据集中找到了高度敏感的个人信息（例如，一位高管与另一名员工有染），这可能被用于敲诈。虽然我们的一些攻击目前成本过高，无法大规模实施，但随着LLMs变得更加便宜，实施这些攻击的动机只会增加。因此，我们认为LLMs需要新的多层次防御方法。|
|**2025-05-16**|**Is Compression Really Linear with Code Intelligence?**|Xianzhen Luo et.al.|[2505.11441](http://arxiv.org/abs/2505.11441)|null|理解数据压缩与大型语言模型（LLMs）能力之间的关系至关重要，尤其是在代码智能等特定领域。先前的研究提出了压缩与通用智能之间的线性关系。然而，这些研究忽视了代码的多面性，它涵盖了各种编程语言和任务，并且难以对现代代码LLMs进行公平评估。我们通过在全面的多语言、多任务代码基准上评估一系列开源代码LLMs来解决这个问题。为了解决高效和公平评估预训练LLMs代码智能的挑战，我们引入了“格式退火”，这是一种轻量级、透明的训练方法，旨在公平地评估这些预训练模型的内在能力。压缩效率，以每字符比特数（BPC）衡量，使用从GitHub派生的新型、大规模且前所未有的代码验证集确定。我们的实证结果表明，测量的代码智能与BPC之间存在基本的对数关系。这一发现细化了先前的线性假设，我们认为这些假设可能是特定、有限条件下对数曲线尾部的观察。我们的工作提供了对压缩在开发代码智能中作用的更细致理解，并为代码领域贡献了一个稳健的评估框架。|
|**2025-05-16**|**GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art**|Chenkai Zhang et.al.|[2505.11436](http://arxiv.org/abs/2505.11436)|**[link](https://github.com/stan-lei/godbench-acl2025)**|视频评论艺术通过提供传达幽默、讽刺或情感共鸣的创意内容来增强用户参与度，这需要细腻且全面地把握文化和语境的微妙之处。尽管多模态大型语言模型（MLLMs）和思维链（CoT）在STEM任务（例如数学和编码）中展示了强大的推理能力，但它们仍然难以生成创意表达，如共鸣的笑话和深刻的讽刺。此外，现有的基准测试由于模态有限和分类不足，阻碍了对基于视频的评论艺术创作中全面创造力的探索。为了解决这些局限性，我们引入了GODBench，这是一个新颖的基准，它集成了视频和文本模态，以系统地评估MLLMs创作评论艺术的能力。此外，受物理学中波动传播模式的启发，我们提出了思维涟漪（RoT），一个多步骤推理框架，旨在增强MLLMs的创造力。大量实验表明，现有的MLLMs和CoT方法在理解和生成创意视频评论方面仍面临重大挑战。相比之下，RoT提供了一种有效的方法来提高创意创作，突显了其在推动基于MLLM的创造力有意义进步方面的潜力。GODBench在https://github.com/stan-lei/GODBench-ACL2025上公开可用。|
|**2025-05-16**|**MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production**|Chao Jin et.al.|[2505.11432](http://arxiv.org/abs/2505.11432)|null|我们提出了一种针对大规模混合专家（MoE）模型高效训练的定制化生产系统——MegaScale-MoE。MoE作为一种有前途的架构，能够将大型语言模型（LLMs）扩展到前所未有的规模，从而提升模型性能。然而，现有的MoE训练系统在训练效率上出现了下降，这加剧了MoE模型规模的扩大和硬件的持续发展。我们认识到，高效通信在提升MoE训练中的关键作用，因此MegaScale-MoE为每个MoE层的注意力和FFN定制了通信高效的并行策略，并在跨操作符和内部操作符级别上采用了一种整体方法来重叠通信与计算。此外，MegaScale-MoE通过调整通信模式来应用通信压缩，降低精度，进一步提高了训练效率。在1,440个NVIDIA Hopper GPU上训练一个352B的MoE模型时，MegaScale-MoE实现了1.41M tokens/s的训练吞吐量，比Megatron-LM提高了1.88倍。我们分享了加速MoE训练的操作经验，并希望通过提供我们在系统设计方面的见解，激发未来MoE系统研究。|
|**2025-05-16**|**When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs**|Xiaomin Li et.al.|[2505.11423](http://arxiv.org/abs/2505.11423)|null|推理增强的大型语言模型（RLLMs），无论是通过推理进行显式训练还是通过思维链（CoT）进行提示，在许多复杂的推理任务上都取得了最先进的性能。然而，我们发现了一个令人惊讶且之前被忽视的现象：显式的CoT推理会显著降低指令遵循的准确性。我们在两个基准上评估了15个模型：IFEval（具有简单、可验证的约束规则）和ComplexBench（具有复杂、组合性约束），当应用CoT提示时，我们始终观察到性能下降。通过大规模案例研究和基于注意力的分析，我们确定了推理要么有助于（例如，在格式化或词汇精确度方面）要么有害（例如，忽视简单约束或引入不必要的内容）的常见模式。我们提出一个指标，即约束注意力，以量化模型在生成过程中的关注点，并表明CoT推理往往会将注意力从与指令相关的标记上转移开。为了减轻这些影响，我们引入并评估了四种策略：上下文学习、自我反思、自我选择推理和分类器选择推理。我们的结果表明，选择推理策略，尤其是分类器选择推理，可以显著恢复丢失的性能。据我们所知，这是首次系统地揭示推理导致的指令遵循失败并提供了实用的缓解策略。|
|**2025-05-15**|**End-to-End Vision Tokenizer Tuning**|Wenxuan Wang et.al.|[2505.10562](http://arxiv.org/abs/2505.10562)|null|现有的视觉分词技术将视觉分词器的优化与下游训练分离，隐含地假设视觉标记可以在各种任务中很好地泛化，例如图像生成和视觉问答。针对低级重建优化的视觉分词器对需要不同表示和语义的下游任务是不敏感的。这种解耦范式引入了一个关键的不匹配：视觉分词的损失可能是目标任务的表示瓶颈。例如，在给定图像中分词文本的错误会导致识别或生成时结果不佳。为了解决这个问题，我们提出了ETT，一种端到端视觉分词器调整方法，它能够实现视觉分词和目标自回归任务的联合优化。与仅使用冻结视觉分词器的离散索引的先前自回归模型不同，ETT利用了分词器代码簿的视觉嵌入，并使用重建和标题目标对视觉分词器进行端到端优化。ETT可以无缝集成到现有的训练管道中，只需最小的架构修改。我们的ETT易于实现和集成，无需调整使用的大型语言模型的原始代码簿或架构。大量的实验表明，我们提出的端到端视觉分词器调整方法可以解锁显著的性能提升，即在多模态理解和视觉生成任务上比冻结分词器基线提升了2-6%，同时保留了原始的重建能力。我们希望这种方法非常简单且强大，能够赋予多模态基础模型除了图像生成和理解之外的能力。|
|**2025-05-15**|**Neural Thermodynamic Laws for Large Language Model Training**|Ziming Liu et.al.|[2505.10559](http://arxiv.org/abs/2505.10559)|null|在超越神经可扩展性定律之外，关于大型语言模型（LLMs）背后的定律知之甚少。我们引入了神经热力学定律（NTL）——一个提供对LLM训练动态全新见解的新框架。在理论方面，我们证明了在河流山谷损失景观假设下，关键的热力学量（例如，温度、熵、比热容、热传导）和经典热力学原理（例如，热力学三大定律和能量均分定理）自然出现。在实践方面，这种科学视角为设计学习率调度提供了直观的指导。|
|**2025-05-15**|**Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data**|Yiwen Liu et.al.|[2505.10551](http://arxiv.org/abs/2505.10551)|**[link](https://github.com/yiveen/syntheticdatafeasibility)**|**随着逼真扩散模型的发展，部分或完全在合成数据上训练的模型逐渐取得更好的结果。然而，扩散模型仍然经常生成在现实中不存在的图像，例如狗漂浮在空中或带有不真实的纹理伪影。我们定义了可行性这一概念，即合成图像中的属性是否能在现实世界中真实存在；包含违反这一标准的属性的合成图像被视为不可行的。直观上，不可行的图像通常被认为是不在分布中的；因此，预计在训练时，基于此类图像的训练会阻碍模型泛化到现实世界数据的能力，因此应尽可能将其排除在训练集之外。然而，可行性真的重要吗？在本文中，我们研究了在为基于CLIP的分类器生成合成训练数据时，强制执行可行性是否必要，重点关注三个目标属性：背景、颜色和纹理。我们引入了VariReal，这是一个流程，它对给定的源图像进行最小编辑，以包含由大型语言模型生成的文本提示给出的可行或不可行属性。我们的实验表明，可行性对LoRA微调的CLIP性能的影响很小，三个细粒度数据集的最高1准确率差异主要小于0.3%。此外，属性是否影响可行/不可行图像对分类性能的对抗性影响。最后，与仅使用可行或不可行数据集相比，在训练数据集中混合可行和不可行图像对性能的影响并不显著。**|
|**2025-05-15**|**Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models**|Annie Wong et.al.|[2505.10543](http://arxiv.org/abs/2505.10543)|**[link](https://github.com/ann-w/towards-a-deeper-understanding-of-reasoning-capabilities-in-large-language-models)**|**尽管大型语言模型在静态基准测试中表现出令人印象深刻的性能，但它们作为自我学习和推理代理在动态环境中的真正潜力仍然不明朗。本研究系统地评估了自我反思、启发式变异和规划作为提示技术，以测试代理的适应性能力。我们在动态环境中使用各种开源语言模型进行实验，发现较大的模型通常优于较小的模型，但战略性的提示可以缩小这种性能差距。其次，过长的提示会对较小模型的基本反应性任务产生负面影响，而较大的模型表现出更稳健的行为。第三，高级提示技术主要使较小模型在复杂游戏中受益，但对已经表现优异的大型语言模型的改进较少。然而，我们发现高级推理方法的结果高度可变：虽然它们能够在推理和决策一致时显著提高性能，但它们也引入了不稳定性，可能导致性能大幅下降。与人类表现相比，我们的发现几乎没有真正涌现推理的证据。相反，大型语言模型在规划、推理和空间协调等关键领域的表现显示出持续的局限性，这表明当前代大型语言模型仍存在可能无法仅通过自我反思提示完全克服的基本缺陷。推理是一个多方面的任务，虽然像思维链这样的推理方法可以提高数学文字问题的多步推理，但我们在动态基准测试中使用的方法突出了在一般推理能力方面的重要不足，表明需要超越静态基准测试来捕捉推理的复杂性。**|
|**2025-05-15**|**Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis**|Pengfei Wang et.al.|[2505.10541](http://arxiv.org/abs/2505.10541)|**[link](https://github.com/welldonepf/stme)**|**近年来，多模态大型语言模型（MLLMs）在理解多图像信息方面的能力得到了提升。然而，现有的基准主要评估答案的正确性，而忽略了模型是否真正理解了视觉输入。为了解决这个问题，我们定义了隐式视觉误解（IVM），即MLLMs在没有完全理解视觉输入的情况下提供正确答案。通过我们的分析，我们在因果注意力模块中将视觉和文本模态解耦，揭示了随着网络层的加深，注意力分布越来越集中在与正确答案相关的图像上。这一见解导致引入了一个尺度无关的指标——“注意力准确性”以及一个用于量化IVMs的新型基准。注意力准确性直接通过内部机制评估模型的视觉理解，对位置偏差具有鲁棒性，从而进行更可靠的评估。此外，我们将我们的方法扩展到更细的粒度，并证明了其在单模态场景中的有效性，强调了其灵活性和泛化能力。**|
|**2025-05-15**|**S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit**|Imranur Rahman et.al.|[2505.10538](http://arxiv.org/abs/2505.10538)|null|在提供经济和软件开发价值的同时，软件供应链的强度取决于最薄弱的环节。过去几年中，网络攻击呈指数增长，特别是针对关键软件供应链中的易受攻击环节。这些攻击扰乱了日常运作，并威胁到几乎所有上网人士的安全，从价值数十亿美元的公司和政府机构到业余的开源开发者。软件供应链攻击这一不断演变的威胁引起了软件行业和美国政府对提高软件供应链安全的兴趣。2024年9月20日，NSF资助的网络安全软件供应链中心（S3C2）的三名研究人员举办了一场软件供应链安全峰会，与会者来自9家公司，共12位专业人士。峰会的目标包括：（1）促进来自不同公司的个人之间关于软件供应链安全的实践经验与挑战的分享，（2）帮助建立新的合作关系，（3）与行业分享我们从前几次峰会得出的观察结果，（4）了解实践者面临的挑战，以便为我们的未来研究方向提供信息。峰会包括与代表公司相关的六个主题的讨论，包括更新易受攻击的依赖项、组件和容器选择、恶意提交、构建基础设施、大型语言模型和减少整个类别的漏洞。|
|**2025-05-15**|**RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs**|Vibha Belavadi et.al.|[2505.10495](http://arxiv.org/abs/2505.10495)|null|本文针对在缺乏真实用户交互数据的情况下，对大型语言模型（LLMs）进行微调以完成函数调用任务的问题。在数字内容创作工具中，用户通过自然语言查询表达需求，这些查询必须映射到API调用，而缺乏真实世界特定任务数据以及对其训练的隐私限制，需要生成合成数据。现有的合成数据生成方法在多样性和复杂性方面存在不足，无法复制真实世界的数据分布，导致LLM微调后的性能不佳。我们提出了一种基于路由器的架构，该架构利用领域资源，如内容元数据和结构化知识图谱，以及文本到文本和视觉到文本的语言模型来生成高质量的合成训练数据。我们架构的灵活路由机制能够生成与观察到的真实世界分布相匹配的合成数据，解决了传统方法的基本局限性。在一系列真实用户查询上的评估表明，在函数分类准确性和API参数选择方面都取得了显著改进。使用我们的合成数据进行微调的模型在性能上始终优于传统方法，为函数调用任务确立了新的基准。|
|**2025-05-15**|**Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective**|Yutao Mou et.al.|[2505.10494](http://arxiv.org/abs/2505.10494)|**[link](https://github.com/murraytom/cov-eval)**|**代码安全和可用性对于由大型语言模型（LLMs）驱动的各种编码助手应用来说都是至关重要的。现有的代码安全基准主要关注单一评估任务和范式，如代码补全和生成，缺乏对安全代码生成、漏洞修复和区分等维度的全面评估。在本文中，我们首先提出了CoV-Eval，这是一个多任务基准，涵盖了代码补全、漏洞修复、漏洞检测和分类等多种任务，以全面评估LLM的代码安全性。此外，我们还开发了VC-Judge，这是一个与人类专家密切一致的评价模型，能够以更高效和可靠的方式审查LLM生成的程序中的漏洞。我们对20个专有和开源的LLM进行了全面的评估。总体而言，尽管大多数LLM能够很好地识别出有漏洞的代码，但它们仍然倾向于生成不安全的代码，并且在识别特定漏洞类型和执行修复方面存在困难。大量的实验和定性分析揭示了关键挑战和优化方向，为LLM代码安全领域的未来研究提供了见解。**|
|**2025-05-15**|**CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning**|Shaohan Wang et.al.|[2505.10493](http://arxiv.org/abs/2505.10493)|null|检索增强生成（RAG）是一种有效的方法来增强大型语言模型（LLMs）的能力。现有方法侧重于通过直接利用top-k检索到的文档来优化RAG系统中的检索器或生成器。然而，文档的有效性因用户查询而异，即有些文档提供了有价值的知识，而其他文档则完全缺乏关键信息。这阻碍了检索器和生成器在训练过程中的适应。受人类认知学习启发，课程学习通过从简单到困难的样本逐步推进来训练模型，从而增强其泛化能力，我们将这一有效范式整合到RAG系统的训练中。在本文中，我们提出了一种基于多阶段课程学习的RAG系统训练框架，命名为CL-RAG。我们首先通过样本进化为检索器和生成器分别构建具有多个难度级别的训练数据。然后，我们根据课程学习方法分阶段训练模型，从而更有效地优化RAG系统的整体性能和泛化能力。我们的CL-RAG框架在四个开放域问答数据集上表现出持续的有效性，在多个先进方法上实现了2%至4%的性能提升。|
|**2025-05-15**|**Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns**|Leon Hannig et.al.|[2505.10490](http://arxiv.org/abs/2505.10490)|null|随着大型语言模型（LLMs）在学生、讲师和研究人员中的使用越来越普遍，大学——与其他组织一样——面临着制定一致的人工智能策略的压力。LLM作为服务（LLMaaS）提供了可访问的预训练模型，可以根据特定的（商业）需求进行定制。虽然大多数研究优先考虑数据、模型或基础设施的调整（例如，模型微调），但我们专注于用户明显的定制，如界面更改和企业品牌化，我们认为这些会影响用户的信任和使用模式。本研究作为一项大规模实地研究的功能性前奏，我们将探讨德国大学的学生和员工如何感知和使用其机构定制的LLMaaS与ChatGPT相比。本前奏的目标是激发关于LLMaaS定制心理效应的讨论，并通过反馈完善我们的研究方法。我们即将发布的发现将深化对LLMs中信任动态的理解，为考虑部署LLMaaS的组织提供实际指导。|
|**2025-05-14**|**Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors**|Nicolas Dupuis et.al.|[2505.09610](http://arxiv.org/abs/2505.09610)|null|近年来，大型语言模型（LLMs）在硬件设计中的应用得到了快速发展，主要通过将其融入提高芯片设计师生产力的工具中。关于在芯片设计的RTL规范中使用LLMs的讨论相当广泛，其中最受欢迎的语言是Verilog和VHDL。由于语言的高普及度，LLMs及其在Verilog设计中的应用受到了广泛关注，但VHDL尽管在行业中持续受到青睐，至今仍未得到足够的关注。关于参与高性能处理器设计的组织所特有的需求，以及在这些环境中部署AI解决方案的技术，也讨论得很少。在本文中，我们描述了我们开发一个专门用于解释VHDL代码的大型语言模型（LLM）的过程，这项任务在一个拥有几十年高性能处理器设计和资产经验的组织中尤为重要。我们展示了如何开发针对我们需求的特定测试集，并在执行基础LLM的扩展预训练（EPT）过程中使用这些测试集来评估模型。与基础模型的43%评分相比，EPT模型产生的代码解释的专家评估评分提高到了69%。我们进一步展示了如何开发一个LLM作为评判者来评估与专家评估者类似的模型。这使我们能够推导和评估一系列新模型，包括一个预期专家评估者评分为71%的指令调整版的EPT模型。我们的实验还表明，通过使用更新的基础模型，这一评分有望提升至85%以上。最后，我们讨论了利用生成式AI领域的激动人心的新发展来进一步提高硬件设计LLMs的质量。|
|**2025-05-14**|**Adversarial Suffix Filtering: a Defense Pipeline for LLMs**|David Khachaturov et.al.|[2505.09602](http://arxiv.org/abs/2505.09602)|null|大型语言模型（LLMs）正越来越多地嵌入到自主系统和面向公众的环境中，但它们仍然容易受到越狱漏洞的影响，这可能会损害其安全性和可靠性。对抗性后缀被认为是目前最先进的越狱方法，它始终优于更简单的方法，甚至在黑盒设置中也能经常成功。现有的防御措施依赖于对模型内部架构的访问，这限制了多样化的部署，极大地增加了内存和计算负担，或者可以通过简单的提示工程方法绕过。我们引入了 $\textbf{对抗性后缀过滤}$ （ASF），这是一个轻量级的、新颖的模型无关防御管道，旨在保护LLMs免受对抗性后缀攻击。ASF作为一个输入预处理和净化器，能够检测并过滤掉提示中的对抗性后缀，有效地中和恶意注入。我们证明了ASF在黑盒和白盒攻击设置中都具有全面的防御能力，将最先进的对抗性后缀生成方法的攻击效果降低到4%以下，同时最小程度上影响了目标模型在非对抗场景中的能力。|
|**2025-05-14**|**How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference**|Nidhal Jegham et.al.|[2505.09598](http://arxiv.org/abs/2505.09598)|null|随着大型语言模型（LLMs）在各个行业的普及，了解它们在推理层面的环境影响不再是可选项，而是必要之举。然而，大多数现有研究排除了专有模型，忽视了基础设施的多样性和开销，或者仅仅关注训练，尽管推理在人工智能的环境影响中越来越占主导地位。为了弥合这一差距，本文介绍了一种新颖的基础设施感知基准测试框架，用于量化30个最先进的LLM在商业数据中心部署中的推理环境足迹。我们的框架结合了公开API性能数据、特定区域的环保乘数和硬件配置的统计推断。此外，我们还利用交叉效率数据包络分析（DEA）根据性能相对于环境成本的相对值对模型进行排名。我们的结果表明，o3和DeepSeek-R1成为能耗最高的模型，每个长提示消耗超过33 Wh，是GPT-4.1 nano的70多倍，而Claude-3.7 Sonnet在生态效率方面排名最高。尽管单个短GPT-4o查询消耗0.43 Wh，但将其扩展到每天7000万次查询会导致巨大的年度环境影响。这些包括与35,000个美国家庭相当的电力使用、与120万人年饮用水需求相当的淡水蒸发以及需要芝加哥大小的森林来抵消的碳排放。这些发现揭示了一个日益突出的悖论：尽管单个查询是高效的，但它们的全球规模导致了不成比例的资源消耗。我们的研究为评估LLM部署的可持续性提供了一种标准化、基于实证的方法，为人工智能发展和可持续性标准中的未来环境问责制奠定了基础。|
|**2025-05-14**|**WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models**|Abdullah Mushtaq et.al.|[2505.09595](http://arxiv.org/abs/2505.09595)|null|大型语言模型（LLMs）的训练和调整方式往往强化了以西方为中心的知识论和社会文化规范，导致文化同质化并限制了它们反映全球文明多样性的能力。现有的基准评估框架未能充分捕捉这种偏见，因为它们依赖于僵化、封闭形式的评估，忽略了文化包容性的复杂性。为了解决这个问题，我们引入了WorldView-Bench，这是一个旨在通过分析其容纳不同世界观的能力来评估LLMs中全球文化包容性（GCI）的基准。我们的方法基于Senturk等人提出的“多重世界观”，该理论区分了强化文化同质化的单重模型和整合多元视角的多重模型。WorldView-Bench通过自由形式的生成评估来衡量文化极化，即排除替代观点，而不是传统的分类基准。我们通过两种干预策略实现应用多重性：（1）情境实施的多重LLMs，其中系统提示嵌入多重性原则；（2）多智能体系统（MAS）实施的多重LLMs，其中代表不同文化视角的多个LLMs智能体协作生成响应。我们的结果表明，从基线时的13%显著增加到MAS实施的多重LLMs的94%，同时向积极情绪（67.7%）和文化平衡性增强转变。这些发现突出了多重意识AI评估在减轻LLMs中文化偏见方面的潜力，为更包容和道德一致的AI系统铺平了道路。|
|**2025-05-14**|**Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media**|Yuchen Wu et.al.|[2505.09583](http://arxiv.org/abs/2505.09583)|null|许多在线平台将参与度信号，如点赞和好评，纳入其内容排名系统和界面设计，旨在提高用户参与度。然而，这些信号可能会无意中提升不那么包容的内容，并且可能不支持规范上可取的行为。当有毒内容与点赞和好评等流行指标强烈相关时，这个问题尤其令人担忧。在本研究中，我们提出结构化亲社会反馈作为点赞和好评的补充信号——一种基于规范标准突出内容质量的信号，以帮助解决传统参与度信号的局限性。我们首先设计和实施了一个由大型语言模型（LLM）驱动的机器学习反馈系统，该系统根据积极心理学的原则评估用户评论，如个人福祉、建设性社交媒体使用和性格优势。然后，我们进行了一项预先注册的用户研究，以检验现有基于同伴的反馈和新的基于专家的反馈如何相互作用，以塑造用户在社交媒体环境中选择评论的方式。结果显示，同伴反馈增加了对流行线索的遵从性，而专家反馈则将偏好转向规范上质量更高的内容。此外，将专家反馈与同伴评估相结合，可以提高与专家评估的一致性，并有助于营造一个不那么有毒的社区环境。这展示了规范线索（如由LLM使用心理学标准生成的专家评分）的附加价值，并强调了将此类信号纳入平台反馈系统以促进更健康在线环境的潜在益处。|
|**2025-05-14**|**Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach**|Shannon Lodoen et.al.|[2505.09576](http://arxiv.org/abs/2505.09576)|null|自2022年以来，ChatGPT和Claude等生成式AI聊天机器人的版本通过一种名为“基于人类反馈的强化学习”（RLHF）的专门技术进行训练，以使用来自人类标注者的反馈来微调语言模型的输出。因此，RLHF的集成极大地提升了这些大型语言模型（LLMs）的输出，使得交互和回应看起来比仅使用监督学习的上一版本更“像人”。人类和机器撰写的文本越来越趋同，这可能在透明度、信任、偏见和人际关系等方面产生严重的伦理、社会技术学和教育影响。为了强调这些影响，本文对当前由RLHF增强的生成式AI聊天机器人重塑的一些核心程序和过程进行了修辞分析：维护语言规范、信息检索实践和社会关系期望。迄今为止，对生成式AI和LLMs的修辞研究主要集中在生成内容的说服力上。利用Ian Bogost的程序修辞概念，本文将修辞调查的焦点从内容分析转移到RLHF增强的LLMs中内置的劝说机制。通过这样做，这项理论调查为AI伦理的进一步研究开辟了新的方向，考虑了通过AI驱动技术重新路由的程序可能会如何强化霸权语言使用、持续偏见、脱离学习背景并侵犯人际关系。因此，本文将对教育工作者、研究人员、学者以及日益增长的生成式AI聊天机器人用户产生兴趣。|
|**2025-05-14**|**MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8**|Linbo Liu et.al.|[2505.09569](http://arxiv.org/abs/2505.09569)|**[link](https://github.com/amazon-science/SDFeedback)**|近年来，随着强大大型语言模型（LLMs）的快速发展，现在可以使用LLMs解决广泛的软件工程任务，显著提高生产力和可扩展性。已经开发了大量的基准数据集来评估这些模型的编码能力，它们主要关注问题解决和问题解决任务。相比之下，我们引入了一个新的编码基准MIGRATION-BENCH，其重点截然不同：代码迁移。MIGRATION-BENCH旨在作为从Java 8迁移到最新长期支持（LTS）版本（Java 17、21）的全面基准，MIGRATION-BENCH包括一个完整的数据集及其分别从5,102和300个存储库中选择的子集。选择的子集经过精心挑选，以复杂性和难度为标准，为代码迁移领域的科研提供了一个多功能的资源。此外，我们提供了一个全面的评估框架，以促进对LLMs在此项具有挑战性的任务上进行严格和标准化的评估。我们进一步提出了SD-Feedback，并证明LLMs可以有效地处理存储库级别的代码迁移到Java 17。对于所选子集，使用Claude-3.5-Sonnet-v2，SD-Feedback在最小和最大迁移中分别实现了62.33%和27.00%的成功率（pass@1）。基准数据集和源代码可在以下网址获取：https://huggingface.co/collections/AmazonScience和https://github.com/amazon-science/self_debug。|
|**2025-05-14**|**PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning**|Zongqian Li et.al.|[2505.09519](http://arxiv.org/abs/2505.09519)|**[link](https://github.com/zongqianli/pt-moe)**|参数高效微调（PEFT）方法在适应大型语言模型方面显示出潜力，但现有方法存在一些反直觉的现象：将路由器集成到提示微调（PT）中可以提高训练效率，但并不总是能提高性能；通过矩阵分解进行参数减少可以在特定领域提高性能。受这些观察和PT的模块化性质启发，我们提出了PT-MoE，这是一种将矩阵分解与专家混合（MoE）路由相结合的新框架，用于高效的PT。在17个数据集上的结果表明，PT-MoE在问答（QA）和数学问题解决任务中均达到了最先进的性能，问答任务中F1分数比PT提高了1.49个百分点，比LoRA提高了2.13个百分点，数学准确性比PT提高了10.75个百分点，比LoRA提高了0.44个百分点，同时使用的参数比LoRA减少了25%。我们的分析显示，虽然PT方法在问答任务中表现优异，而基于LoRA的方法在数学数据集中表现较好，但PT-MoE中矩阵分解和MoE的集成产生了互补效益：分解使得专家之间能够高效地共享参数，而MoE提供了动态适应性，共同使PT-MoE展现出跨任务的稳定性和泛化能力。这些发现，以及关于路由机制和架构组件的消融研究，为未来的PEFT方法提供了见解。|
|**2025-05-14**|**Layered Unlearning for Adversarial Relearning**|Timothy Qian et.al.|[2505.09500](http://arxiv.org/abs/2505.09500)|**[link](https://github.com/12tqian/layered-unlearning)**|**我们的目标是理解诸如微调、对齐和回学等训练后方法如何修改语言模型的行为和表示。我们特别关注这些修改的脆弱性，这使得它们容易通过提示工程或重新学习来绕过。最近的研究表明，训练后诱导出浅层上下文相关的“电路”，这些电路抑制特定的响应模式。这可能是训练后脆弱性的一个解释。为了验证这一假设，我们设计了一种回学算法，称为分层回学（LU），它为数据集的一个不断增长的部分创建独特的抑制机制。通过在第 $k$个阶段回学前$i$个折，同时保留剩余的$k - i$ 个，LU限制了重新学习在数据子集上恢复整个数据集的能力。我们通过合成实验和大型语言模型（LLM）实验的组合来评估LU。我们发现LU提高了对几种不同回学方法的对抗性重新学习的鲁棒性。我们的结果为机器回学的最先进技术做出了贡献，并提供了对训练后更新的影响的见解。**|
|**2025-05-14**|**Card Sorting Simulator: Augmenting Design of Logical Information Architectures with Large Language Models**|Eduard Kuric et.al.|[2505.09478](http://arxiv.org/abs/2505.09478)|null|卡片分类是一种常见的创意技术，通过让用户将项目分类到不同的类别中来获取他们对内容和功能的心理组织信息。为了使卡片分类研究更加稳健，数字卡片分类工具可以受益于提供快速自动反馈。本研究的目标是开发一种应用人工智能（AI）来增强卡片分类的工具。为此，我们开发了卡片分类模拟器，这是一个利用大型语言模型（LLMs）生成卡片信息分类的原型工具。为了阐明模拟与实际参与者卡片分类的契合度，并指导工具的设计决策，我们进行了一项以可推广性为重点的比较研究。我们从真实从业者那里获得了28项预先存在的卡片分类研究，包括1,399名参与者，以及多样化的内容和来源。利用这个数据集，我们对实际卡片分类结果（卡片的聚类）与多种LLMs和提示设计下的合成聚类之间的协议进行了全面而细致的分析。互信息得分表明与实际结果聚类有良好的一致性，尽管相似性矩阵也显示了来自心理模型的不一致性，这可以归因于它们的自上而下性质。此外，卡片的数量或标签的复杂性会影响其模拟的准确性。这些发现支持了AI增强在卡片分类研究中的应用，作为有意义初步反馈的来源，并突出了进一步研究以开发和完善智能用户研究工具的必要性。|
|**2025-05-13**|**CodePDE: An Inference Framework for LLM-driven PDE Solver Generation**|Shanda Li et.al.|[2505.08783](http://arxiv.org/abs/2505.08783)|**[link](https://github.com/lithiumda/codepde)**|偏微分方程（PDEs）对于建模物理系统至关重要，但求解它们仍然是一个复杂的挑战。传统的数值求解器依赖于专家知识来实现，并且计算成本高昂，而基于神经网络的求解器则需要大量的训练数据集，并且通常缺乏可解释性。在这项工作中，我们将PDE求解视为一个代码生成任务，并介绍了CodePDE，这是第一个使用大型语言模型（LLMs）生成PDE求解器的推理框架。利用先进的推理时算法和扩展策略，CodePDE解锁了LLM在PDE求解中的关键能力：推理、调试、自我改进和测试时扩展——所有这些都不需要针对特定任务进行调整。CodePDE在各种代表性的PDE问题上实现了超人类的表现。我们还对LLM生成的求解器进行了系统的实证分析，分析了它们的准确性、效率和数值方案选择。我们的发现突显了LLM在PDE求解中的潜力和当前局限性，为求解器设计提供了新的视角，并为未来的模型开发提供了机会。我们的代码可在https://github.com/LithiumDA/CodePDE上获取。|
|**2025-05-13**|**HealthBench: Evaluating Large Language Models Towards Improved Human Health**|Rahul K. Arora et.al.|[2505.08775](http://arxiv.org/abs/2505.08775)|**[link](https://github.com/openai/simple-evals)**|**我们介绍了HealthBench，这是一个开源基准，用于衡量大型语言模型在医疗保健领域的性能和安全。HealthBench包含5,000个模型与单个用户或医疗专业人员之间的多轮对话。对话回复通过由262位医生创建的特定于对话的评分标准进行评估。与之前的多项选择或简短答案基准不同，HealthBench通过48,562个独特的评分标准，涵盖多个健康背景（例如，紧急情况、转换临床数据、全球健康）和行为维度（例如，准确性、指令遵循、沟通）来实现现实、开放式的评估。过去两年中HealthBench的性能反映了稳定的初始进展（比较GPT-3.5 Turbo的16%到GPT-4o的32%）和近期更快的改进（o3分数达到60%）。小型模型尤其得到了提升：GPT-4.1 nano的表现优于GPT-4o，且成本仅为后者的1/25。我们还发布了两个HealthBench变体：HealthBench共识版，其中包含通过医生共识验证的34个特别重要的模型行为维度；以及HealthBench困难版，当前最高分数为32%。我们希望HealthBench能够推动模型开发与应用的进步，从而造福人类健康。**|
|**2025-05-13**|**Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology**|Yatai Ji et.al.|[2505.08765](http://arxiv.org/abs/2505.08765)|null|城市环境中的空中视觉对象搜索（AVOS）任务要求无人机（UAV）在无外部指导的情况下，利用视觉和文本线索自主搜索和识别目标对象。现有的方法由于语义处理冗余、相似对象区分困难和探索-利用困境而在复杂城市环境中遇到困难。为了弥合这一差距并支持AVOS任务，我们引入了CityAVOS，这是第一个用于自主搜索常见城市对象的基准数据集。该数据集包含2,420个任务，涵盖六个不同难度的对象类别，能够全面评估无人机代理的搜索能力。为了解决AVOS任务，我们还提出了PRPSearcher（感知-推理-规划搜索器），这是一种由多模态大型语言模型（MLLMs）驱动的创新代理方法，它模仿了人类的分层认知。具体来说，PRPSearcher构建了三个专用地图：一个以对象为中心的动态语义地图，增强空间感知；一个基于语义吸引值的3D认知地图，用于目标推理；以及一个3D不确定性地图，用于平衡探索-利用搜索。此外，我们的方法还包含一个降噪机制，以减轻相似对象带来的干扰，并利用灵感促进思维（IPT）提示机制进行自适应动作规划。在CityAVOS上的实验结果表明，PRPSearcher在成功率、搜索效率（平均：+37.69% SR，+28.96% SPL，-30.69% MSS，和-46.40% NE）方面均优于现有基线。虽然前景看好，但与人类相比的性能差距突出了在AVOS任务中需要更好的语义推理和空间探索能力。这项工作为未来具身目标搜索的进步奠定了基础。数据集和源代码可在https://anonymous.4open.science/r/CityAVOS-3DF8上获取。|
|**2025-05-13**|**AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models**|Yanxi Zhang et.al.|[2505.08750](http://arxiv.org/abs/2505.08750)|**[link](https://github.com/zhangyx0417/ac_reason)**|实际因果性（AC），因果推理（CR）的一个基本方面，负责在现实场景中分配归因和责任。然而，现有的基于LLM的方法缺乏对形式AC理论的根基，导致可解释性有限。因此，我们提出了AC-Reason，一个半形式推理框架，该框架在AC场景中识别因果相关事件，推断其形式因果因素（例如，充分性、必要性和正常性）的值，并通过一个具有解释的理论指导算法回答AC查询。虽然AC-Reason没有明确构建因果图，但它操作于潜在因果结构中的变量，以支持原则性的推理。为了实现全面评估，我们引入了AC-Bench，这是一个基于并显著扩展了Big-Bench Hard Causal Judgment（BBH-CJ）的新基准。AC-Bench包含约1000个精心标注的样本，每个样本都有详细的推理步骤，并且仅关注实际因果。案例研究显示，AC-Bench中的合成样本对LLM提出了更大的挑战。在BBH-CJ和AC-Bench上的大量实验表明，AC-Reason在基线之上持续提高LLM的性能。在BBH-CJ上，所有测试的LLM都超过了平均人类评分者准确率69.60%，GPT-4 + AC-Reason达到了75.04%。在AC-Bench上，GPT-4 + AC-Reason再次达到了最高的准确率71.82%。AC-Bench进一步使对推理忠实度的细致分析成为可能，揭示只有Qwen-2.5-72B-Instruct、Claude-3.5-Sonnet和GPT-4o表现出忠实的推理，而GPT-4倾向于利用捷径。最后，我们的消融研究证明，将AC理论集成到LLM中非常有效，所提出的算法贡献了最大的性能提升。|
|**2025-05-13**|**DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models**|Xiaoyang Chen et.al.|[2505.08744](http://arxiv.org/abs/2505.08744)|**[link](https://github.com/deepmathllm/deepmath)**|**为了提升大型语言模型（LLMs）的数学能力，DeepMath团队启动了一个开源项目，旨在开发一个开放的数学LLM并系统地评估其数学创造力。本文代表了这一项目的初步贡献。尽管数学LLMs的近期发展主要强调推理能力，如通过从初等到大学水平的数学任务基准测试所证明，但这些模型的创造性能力相对较少受到关注，且评估数据集仍然稀缺。为了填补这一空白，我们提出了一个数学创造力的评估标准，并介绍了DeepMath-Creative，这是一个包含代数、几何、分析和其他领域构造性问题的创新、高质量基准。我们使用这个数据集对主流LLMs的创造性问题解决能力进行了系统评估。实验结果表明，即使在宽松的评分标准下——强调核心解决方案组件，而忽略如小的逻辑漏洞、不完整的论证或冗余的解释等小错误——表现最好的模型O3 Mini也仅达到70%的准确率，主要是在基本的大学水平构造性任务上。在更复杂的问题上，性能急剧下降，模型未能为开放性问题提供实质性的策略。这些发现表明，尽管当前的LLMs在熟悉和难度较低的问题上显示出一定的构造性能力，但这种表现可能归因于记忆模式的重组，而不是真正的创造性洞察或新颖的综合。**|
|**2025-05-13**|**Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies**|Xiaoliang Luo et.al.|[2505.08739](http://arxiv.org/abs/2505.08739)|**[link](https://github.com/braingpt-lovelab/backwards)**|在训练不同标记顺序的序列时，自回归大型语言模型（LLMs）能否学习一致的概率分布？我们正式证明，对于任何定义良好的概率分布，序列的困惑度在任意分解下都是不变的，包括正向、反向或任意排列。这一结果为研究LLMs如何从数据中学习提供了严格的理论基础，并定义了经验评估的原则性协议。应用这些协议，我们发现先前研究顺序效应时存在关键的方法论缺陷。我们在科学文本上对GPT-2模型进行了正向、反向和任意排列顺序的重新训练。我们发现，在所有排列中，与理论上的不变性存在系统偏差，其中任意排列与正向和反向模型都有很大差异，而正向和反向模型在很大程度上（但不完全）一致。偏差可以追溯到自注意力中的差异，反映了处理中的位置和局部性偏差。我们的理论和实证结果为理解LLMs中的位置偏差提供了新的途径，并建议了检测LLMs的概率分布不一致性和因此不可靠的方法。|
|**2025-05-13**|**NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context**|Ben Yao et.al.|[2505.08734](http://arxiv.org/abs/2505.08734)|null|这项工作介绍了首个护理价值对齐基准，该基准由从国际护理规范中提炼出的五个核心价值观维度组成：利他主义、人的尊严、正直、正义和专业精神。该基准包含通过在三个不同等级的医院进行为期五个月的纵向实地研究收集的1,100个真实世界护理行为实例。这些实例由五位临床护士进行标注，并通过具有相反伦理极性的LLM生成的反事实进行增强。每个原始案例都与一个价值对齐版本和一个价值违规版本配对，从而产生了构成Easy-Level数据集的2,200个标注实例。为了增加对抗复杂性，每个实例进一步转换为基于对话的格式，其中包含上下文线索和细微的误导性信号，从而产生了Hard-Level数据集。我们评估了23个最先进的（SoTA）LLM在它们与护理价值对齐方面的表现。我们的发现揭示了三个关键见解：（1）DeepSeek-V3在Easy-Level数据集上取得了最高性能（94.55），而Claude 3.5 Sonnet在Hard-Level数据集上（89.43）优于其他模型，显著超过了医疗LLM；（2）正义始终是评估最困难的护理价值维度；（3）上下文学习显著提高了对齐度。这项工作旨在为临床环境中价值敏感的LLM开发提供基础。数据集和代码可在https://huggingface.co/datasets/Ben012345/NurValues上获取。|
|**2025-05-13**|**Securing RAG: A Risk Assessment and Mitigation Framework**|Lukas Ammann et.al.|[2505.08728](http://arxiv.org/abs/2505.08728)|null|检索增强生成（RAG）已成为面向用户的自然语言处理（NLP）应用的行业标准，它允许在不重新训练或微调大型语言模型（LLMs）的情况下整合数据。这一能力提高了响应的质量和准确性，但也引入了新的安全和隐私挑战，尤其是在敏感数据整合时。随着RAG的快速采用，保护数据和服务的安全已成为一项关键优先任务。本文首先回顾了RAG管道的漏洞，并概述了从数据预处理和数据存储管理到与LLMs集成的攻击面。随后，将识别出的风险与相应的缓解措施进行了结构化概述。在第二步中，本文开发了一个框架，该框架结合了RAG特定的安全考虑因素、现有的通用安全指南、行业标准以及最佳实践。所提出的框架旨在指导实施稳健、合规、安全和可信的RAG系统。|
|**2025-05-13**|**PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts**|Yang Su et.al.|[2505.08719](http://arxiv.org/abs/2505.08719)|null|大型语言模型（LLMs）部署在云端服务器上可以减轻本地设备的计算和存储负担，但由于敏感数据的传输和需要大量的通信带宽，这引发了隐私问题，在受限环境中尤为困难。相比之下，本地运行的小型语言模型（SLMs）可以增强隐私保护，但在复杂任务上的性能有限。为了在带宽限制下平衡计算成本、性能和隐私保护，我们提出了一种隐私感知的无线协同专家混合（PWC-MoE）框架。具体来说，PWC-MoE采用了一种稀疏的隐私感知门控网络，以动态地将敏感标记路由到位于本地客户端的隐私专家，而非敏感标记则被路由到位于远程基站的非隐私专家。为了提高计算效率，门控网络确保每个标记只被动态路由并由一个专家处理。为了提高可扩展性并防止特定专家过载，我们为门控网络引入了一种分组负载均衡机制，该机制均匀地将敏感标记分配给隐私专家，将非敏感标记分配给非隐私专家。为了适应带宽限制同时保持模型性能，我们提出了一种带宽自适应和重要性感知的标记卸载方案。该方案包含一个重要性预测器，用于评估非敏感标记的重要性得分，根据预测的重要性和可用带宽，优先将最重要的标记传输到基站。实验表明，PWC-MoE框架在带宽受限的环境中也能有效保护隐私并保持高性能，为在隐私敏感和带宽有限的场景中部署LLMs提供了一种实用解决方案。|
|**2025-05-13**|**LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs**|K M Sajjadul Islam et.al.|[2505.08704](http://arxiv.org/abs/2505.08704)|null|电子健康记录（EHRs）是患者信息的数字化记录，通常包含非结构化的临床文本。在EHRs中，命名实体识别（NER）对于提取关键医疗实体（如问题、检查和治疗）以支持下游临床应用至关重要。本文探讨了基于提示的医疗实体识别，使用大型语言模型（LLMs），特别是GPT-4o和DeepSeek-R1，并通过各种提示工程技术进行指导，包括零样本、少样本和集成方法。在所有策略中，使用提示集成的GPT-4o在分类性能上取得了最高分数，F1分数为0.95，召回率为0.98，优于DeepSeek-R1在任务上的表现。集成方法通过基于嵌入的相似性和多数投票聚合输出，提高了可靠性。|
|**2025-05-12**|**Learning Dynamics in Continual Pre-Training for Large Language Models**|Xingjin Wang et.al.|[2505.07796](http://arxiv.org/abs/2505.07796)|null|在本文中，我们探讨了在整个持续预训练（CPT）过程中大型语言模型的学习动态。我们特别关注在每个训练步骤中，一般和下游领域性能是如何演变的，领域性能通过验证损失来衡量。我们观察到，CPT损失曲线本质上描述了从一个曲线到另一个隐藏曲线的转变，并且可以通过解耦分布偏移和学习率退火效应来描述。我们推导出一个CPT缩放定律，将这两个因素结合起来，能够预测任何（持续）训练步骤以及在CPT中各种学习率调度（LRS）下的损失。我们的公式展示了CPT中几个关键因素的综合理解，包括损失潜力、峰值学习率、训练步骤、重放比例等。此外，我们的方法可以适应定制不同的训练超参数以实现不同的CPT目标，如平衡一般性能和领域特定性能。大量的实验表明，我们的缩放定律在各种CPT数据集和训练超参数上都是适用的。|
|**2025-05-12**|**Domain Regeneration: How well do LLMs match syntactic properties of text domains?**|Da Ju et.al.|[2505.07784](http://arxiv.org/abs/2505.07784)|null|近期大型语言模型性能的改进，很可能伴随着其在近似训练数据分布方面的提升。在这项工作中，我们探讨以下问题：LLMs忠实地近似了文本域的哪些特性，以及它们做得如何？我们运用来自语料库语言学的观察方法，提示一个常用的开源LLM从两个常包含在LLM训练数据中的许可英文文本域——维基百科和新闻文本——中重新生成文本。这种再生范式使我们能够在相对语义控制的设置下，研究LLMs是否能忠实地匹配原始的人类文本域。我们研究了不同层次的句法抽象，从更简单的句子长度和文章可读性等特性，到更复杂和更高阶的特性，如依存标签分布、解析深度和解析复杂性。我们发现，与人类原始文本相比，大部分重新生成的分布显示出均值偏移、标准差降低以及长尾减少。|
|**2025-05-12**|**Relative Overfitting and Accept-Reject Framework**|Yanxin Liu et.al.|[2505.07783](http://arxiv.org/abs/2505.07783)|null|目前，大型语言模型（LLMs）的扩展法则面临着挑战和瓶颈。本文提出，噪声效应，源于信号与噪声比在边际收益递减下的变化，是这些问题的根本原因。为了控制这种噪声，我们研究了具有性能优势和不利的模型之间的差异，引入了“相对过拟合”的概念。基于它们的互补优势，我们提出了一种应用框架，接受-拒绝（AR）。在自然语言处理（NLP）中，我们使用LLMs和小型语言模型（SLMs）作为讨论的媒介。这个框架使得SLMs能够对LLM的决策输出产生普遍的正面影响，而不是直观上预期的负面影响。我们使用基于主流架构的自建模型和多个数据集上的预训练主流模型（包括基本语言建模、长上下文任务、学科考试和问答（QA）基准）来验证了我们的方法。结果表明，通过我们的结构，与增加LLM的参数相比，在许多场景中我们可以以显著更低的参数和计算成本实现更好的性能提升。这些改进是普遍的、稳定的和有效的。此外，我们还探索了“相对过拟合”和AR框架在其他机器学习领域（如计算机视觉（CV）和科学AI）中的潜力。我们希望提出的方法能够帮助扩展法则克服现有的瓶颈。|
|**2025-05-12**|**MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering**|Rushi Qiang et.al.|[2505.07782](http://arxiv.org/abs/2505.07782)|**[link](https://github.com/MLE-Dojo/MLE-Dojo)**|我们介绍了MLE-Dojo，这是一个类似于Gym的框架，用于系统地强化学习、评估和改进自主大型语言模型（LLM）代理在迭代机器学习工程（MLE）工作流程中的表现。与现有主要依赖静态数据集或单次尝试评估的基准不同，MLE-Dojo提供了一个交互式环境，使代理能够通过结构化的反馈循环迭代地实验、调试和优化解决方案。MLE-Dojo建立在200多个真实世界的Kaggle挑战之上，涵盖了精心挑选的多样化、开放式的MLE任务，旨在反映现实工程场景，如数据处理、架构搜索、超参数调整和代码调试。其完全可执行的环境支持通过监督微调和强化学习进行全面的代理训练，促进迭代实验、现实数据采样和实时结果验证。对八个前沿LLM的广泛评估显示，尽管当前模型实现了有意义的迭代改进，但它们在自主生成长期解决方案和高效解决复杂错误方面仍存在显著局限性。此外，MLE-Dojo灵活且可扩展的架构无缝集成了多样化的数据源、工具和评估协议，独特地实现了基于模型的代理调整，并促进了互操作性、可扩展性和可重复性。我们将我们的框架和基准开源，以促进社区驱动的创新，推动下一代MLE代理的发展。|
|**2025-05-12**|**Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving**|Xinji Mai et.al.|[2505.07773](http://arxiv.org/abs/2505.07773)|**[link](https://github.com/anonymize-author/agentrl)**|**大型语言模型（LLMs）在需要精确、可验证计算的数学推理任务上往往遇到困难。虽然基于结果的强化学习（RL）可以提高基于文本的推理能力，但理解代理如何自主地利用外部工具（如代码执行）仍然至关重要。我们研究了基于结果的奖励的RL在工具集成推理（ZeroTIR）中的应用，训练基础LLMs在没有监督工具使用示例的情况下自发生成和执行用于数学问题的Python代码。我们的主要贡献是，我们证明了随着RL训练的进行，关键指标可以预测性地扩展。具体来说，我们观察到强烈的正相关关系，即增加训练步骤会导致自发代码执行频率、平均响应长度以及关键的最后任务准确性的增加。这表明在训练中投入的计算努力与有效、工具增强的推理策略的出现之间存在可量化的关系。我们实现了一个具有解耦代码执行环境的稳健框架，并在标准的RL算法和框架中验证了我们的发现。实验表明，ZeroTIR在具有挑战性的数学基准测试中显著优于非工具的ZeroRL基线。我们的发现为理解自主工具使用如何在代理RL中获取和扩展提供了基础性的理解，为未来的研究提供了一个可复现的基准。代码已发布在 \href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}。**|
|**2025-05-12**|**Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding**|Yifeng Di et.al.|[2505.07768](http://arxiv.org/abs/2505.07768)|**[link](https://github.com/NecoraNyaru/PInG)**|大型语言模型（LLMs）在代码生成方面展现出前所未有的能力。然而，由LLMs生成的代码仍然存在广泛的功能错误，尤其是在LLMs之前未曾见过的复杂编程任务中。最近的研究表明，开发者常常难以检查和修复LLMs生成的错误代码，这降低了他们的生产力和对基于LLM的代码生成的信任度。受到通信中的相互接地理论的启发，我们提出了一种交互式方法，该方法利用代码注释作为开发者和LLMs建立共同理解的中介。我们的方法通过交错代码生成、内联注释生成和通过可编辑注释提供的上下文化用户反馈，促进迭代接地，使生成的代码与开发者的意图相一致。我们在两个流行的基准测试上评估了我们的方法，并证明我们的方法显著提高了多个最先进的LLMs，例如，在HumanEval上，code-davinci-002的pass@1提高了17.1%。此外，我们进行了一项包含12名参与者的用户研究，将其与两个基线进行了比较：（1）与GitHub Copilot交互，以及（2）与称为多轮程序综合的多步骤代码生成范式交互。当使用我们的方法时，参与者完成给定编程任务的速度提高了16.7%，任务成功率提高了10.5%。这两项结果都表明，通过交互式改进代码注释可以促进相互接地的协作建立，从而实现更精确的代码生成和更高的开发者信心。|
|**2025-05-12**|**Assessing the Chemical Intelligence of Large Language Models**|Nicholas T. Runcie et.al.|[2505.07735](http://arxiv.org/abs/2505.07735)|**[link](https://github.com/oxpig/ChemIQ)**|大型语言模型是多功能、通用的工具，具有广泛的应用范围。最近，“推理模型”的出现使得它们在数学和软件工程等高级问题解决领域的性能得到了显著提升。在本研究中，我们评估了推理模型直接执行化学任务的能力，无需任何外部工具的帮助。我们创建了一个新的基准测试，称为ChemIQ，它包含796个问题，评估了有机化学的核心概念，重点关注分子理解和化学推理。与之前的基准测试不同，后者主要使用多项选择题格式，我们的方法要求模型构建简答题回答，更接近现实世界的应用。以OpenAI的o3-mini为例，推理模型在不同推理水平下正确回答了28%-59%的问题，较高的推理水平显著提高了所有任务的性能。这些模型在非推理模型GPT-4o（仅达到7%的准确率）上取得了显著的优势。我们发现，大型语言模型现在可以将SMILES字符串转换为IUPAC名称，这是之前模型无法完成的任务。此外，我们展示了最新的推理模型可以解析1H和13C NMR数据中的结构，对于含有最多10个重原子的分子，正确生成74%的SMILES字符串，并在一个案例中解决了含有21个重原子的结构。对于每个任务，我们都发现了证据表明推理过程与人类化学家的推理过程相似。我们的结果表明，最新的推理模型具有执行高级化学推理的能力。|
|**2025-05-12**|**Spoken Language Understanding on Unseen Tasks With In-Context Learning**|Neeraj Agrawal et.al.|[2505.07731](http://arxiv.org/abs/2505.07731)|null|口语语言理解（SLU）任务涉及多种技能，这些技能测试模型在信息提取、分类和/或生成方面的能力。在这种环境中，特定任务的训练数据可能并不总是可用。虽然传统的特定任务SLU模型无法满足这些需求，但语音-文本大型语言模型（LLMs）提供了一个有希望的替代方案，并展现出新兴的能力。然而，我们的评估表明，在SLU任务上，知名开源语音-文本LLMs的零样本/少量样本性能并不理想。在本文中，我们介绍了一种使用随机类别标签进行鲁棒无特定任务微调的新方法。通过这种方法，我们说明了语音-文本LLMs在未见过的任务上的性能比标准方法有显著提升。关键的是，提出的方法避免了在语音-文本LLMs中启用新任务时对特定任务数据标注的要求。|
|**2025-05-12**|**Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations**|Pranav Sinha et.al.|[2505.07711](http://arxiv.org/abs/2505.07711)|null|我们正处于嘈杂的中间规模量子（NISQ）时代，量子计算机受噪声门的限制，其中一些门的错误率更高，可能导致最终计算结果无法理解。量子电路编译算法试图在将量子算法映射到量子硬件时最小化这些噪声门，但它们面临着计算挑战，限制了它们在5-6个量子比特以下的电路中的应用，从而需要在应用噪声量子门最小化算法之前对大型电路进行划分。现有这些算法本质上是启发式的，并且没有考虑下游门最小化任务。大型语言模型（LLMs）有潜力改变这一点，并有助于改进量子电路划分。本文研究了使用LLMs（如Llama和Mistral）通过利用它们理解和生成代码的能力（包括QASM）来划分量子电路。具体来说，我们训练LLMs使用伯克利量子合成工具包的快速划分方法来划分电路。通过实验评估，我们表明，通过仔细微调开源LLMs，我们能够在划分任务中获得53.4%的准确率，而现成的LLMs使用标准的单次和少量样本训练方法无法正确划分电路。|
|**2025-05-12**|**PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull Request Outcomes**|Daniel Ogenrwot et.al.|[2505.07700](http://arxiv.org/abs/2505.07700)|null|在软件开发中，大型语言模型（LLMs）如ChatGPT的快速采用为开发者与AI的交互方式带来了新的途径，尤其是在拉取请求（pull request）工作流程中。尽管先前的研究已经探讨了AI生成代码的质量，但对ChatGPT在实际的拉取请求决策中的运用及其建议如何影响补丁集成和拒绝的了解却有限。为了探讨这些方面，我们分析了开发者自我承认的ChatGPT使用情况（SACU），其中开发者在拉取请求讨论中明确披露了对ChatGPT的依赖。我们的研究考察了255个GitHub仓库中的338个拉取请求（285个已合并，53个已关闭），包含645个ChatGPT生成的代码片段和3,486个补丁。我们引入了PatchTrack，这是一个分类工具，用于确定ChatGPT生成的补丁是否被应用（PA，115个案例）、未应用（PN，64个案例）或未被建议（NE，106个案例）。我们的发现表明，完全采用ChatGPT生成的代码的情况很少见，开发者通常修改或选择性集成AI生成的补丁以符合项目约束，中位集成率为25%。通过定性分析，我们确定了影响补丁集成和拉取请求拒绝的关键因素，包括范围不匹配、可维护性担忧、冗余解决方案以及程序性障碍，如不完整的文档或行政政策。通过为ChatGPT在拉取请求工作流程中的作用提供实证洞察，本研究为开发者、维护者和教育者提供了关于生成式AI在协作软件开发中演变的见解。它也为未来关于优化AI辅助开发、提高AI采用透明度以及改进补丁集成工作流程的研究奠定了基础。|
|**2025-05-09**|**From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling**|Vahid Rahimzadeh et.al.|[2505.06184](http://arxiv.org/abs/2505.06184)|null|通过内容分析进行社交媒体用户画像对于任务如虚假信息检测、参与度预测、仇恨言论监控和用户行为建模至关重要。然而，现有的画像技术，包括推文摘要、基于属性的画像和潜在表示学习，面临着显著的局限性：它们通常缺乏迁移性，产生不可解释的特征，需要大量的标记数据集，或者依赖于僵化的预定义类别，限制了适应性。我们介绍了一种基于大型语言模型（LLM）的新方法，该方法利用定义领域的陈述，这些陈述作为描述领域重要支柱的关键特征，作为画像的基础。我们的两阶段方法首先采用半监督过滤与特定领域的知识库，然后生成抽象的（合成的描述）和提取的（代表性的推文选择）用户画像。通过利用LLM固有的知识，并辅以最少的人工验证，我们的方法在不同领域具有适应性，同时减少了大量标记数据集的需求。我们的方法生成可解释的自然语言用户画像，将大量的用户数据浓缩到可以解锁LLM推理和知识能力以用于下游社交网络任务的规模。我们贡献了一个波斯政治Twitter（X）数据集和基于LLM的评估框架，并进行了人工验证。实验结果表明，我们的方法比最先进的基于LLM的传统方法提高了9.8%，证明了其在创建灵活、适应性强和可解释的用户画像方面的有效性。|
|**2025-05-09**|**A Large Language Model-Enhanced Q-learning for Capacitated Vehicle Routing Problem with Time Windows**|Linjiang Cao et.al.|[2505.06178](http://arxiv.org/abs/2505.06178)|null|容量受限车辆路径问题（CVRPTW）是一个经典的NP难组合优化问题，广泛应用于物流配送和交通运输管理。其复杂性源于车辆容量和时间窗口的约束，这对传统方法构成了重大挑战。大型语言模型（LLMs）的进步为找到CVRPTW的近似解提供了新的可能性。本文提出了一种新型的LLM增强Q学习框架，用于解决具有实时紧急约束的CVRPTW问题。我们的解决方案引入了一种自适应的两阶段训练机制，从LLM引导的探索阶段过渡到Q网络的自主优化阶段。为确保可靠性，我们设计了一种基于思维链（CoT）的三层自我纠正机制，用于LLMs：句法验证、语义验证和物理约束执行。此外，我们还优先回放由LLMs生成的经验，以增强LLMs在架构中的监管作用。实验结果表明，与传统的Q学习相比，我们的框架实现了平均7.3%的成本降低，并且所需的训练步骤更少。|
|**2025-05-09**|**MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills**|Niladri Shekhar Dutt et.al.|[2505.06176](http://arxiv.org/abs/2505.06176)|null|在照片的后期处理中，修图是一项基本任务。文本或笔触引导的生成编辑为用户提供了新的工具，但很容易以不可接受和不可预测的方式改变原始物体的身份。相比之下，尽管传统的程序编辑，如常见于照片编辑工具（例如Gimp、Lightroom）的，比较保守，但仍然被专业人士所偏爱。不幸的是，专业质量的修图涉及许多个别的程序编辑操作，这对大多数初学者来说都是一项挑战。在本文中，我们探讨是否可以教会一个多模态大型语言模型（MLLM）评估原始照片、提出合适的补救措施，并最终通过一组预编写的程序图像操作来实现这些措施。我们证明了MLLM可以通过训练它们解决专门设计的视觉谜题来首先了解底层图像处理操作。随后，这种对操作有意识的MLLM可以同时规划和提出编辑序列。为了便于训练，针对一组专家编辑的图片，我们通过程序性地操纵专家编辑来合成一个推理数据集，然后在视觉调整上基于预训练的LLM来合成推理以进行微调。所提出的修图操作，按照构建，用户可以理解，保留物体细节和分辨率，并且可以选择性地覆盖。我们在各种测试示例上评估了我们的设置，并在可解释性和身份保护方面展示了与现有的生成编辑和其他程序替代方案的优点。代码、数据、模型和补充结果可以在我们的项目网站https://monetgpt.github.io上找到。|
|**2025-05-09**|**Turbo-ICL: In-Context Learning-Based Turbo Equalization**|Zihang Song et.al.|[2505.06175](http://arxiv.org/abs/2505.06175)|null|本文介绍了一种受大型语言模型（LLMs）启发的创新情境学习（ICL）框架，用于在编码的多输入多输出（MIMO）系统中的软输入软输出信道均衡。该框架通过直接从导频信号和解码器反馈中推断后验符号分布来学习。一个关键创新是使用提示增强，将解码器输出的外信息作为额外上下文结合进来，使得ICL模型能够在涡轮解码迭代中迭代地细化符号估计。开发了两种基于Transformer和状态空间架构的模型变体，并进行了评估。广泛的仿真结果表明，当传统的线性假设失效时，例如在低分辨率量化存在的情况下，ICL均衡器始终优于传统的基于模型的基线，即使后者提供了完美的信道状态信息。结果还突出了基于Transformer的模型在有限训练多样性下的优势，以及状态空间模型在资源受限场景中的效率。|
|**2025-05-09**|**A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets**|Ryan Lagasse et.al.|[2505.06150](http://arxiv.org/abs/2505.06150)|null|我们提出了一种在固定计算预算下对大型语言模型（LLM）进行微调的缩放定律，该定律明确考虑了数据组成。传统的做法仅通过总令牌数来衡量训练数据，然而示例数量及其平均令牌长度——我们称之为“数据量”——在模型性能中起着决定性作用。我们的公式遵循既定程序进行调整。在BRICC数据集[1]和MMLU数据集的子集[2]上的实验，在多种子采样策略下进行评估，揭示了数据组成对令牌效率的影响显著。这些结果促使我们为资源受限环境中的实际LLM微调制定更精确的缩放定律。  [1] Salavati, R., et al. (2024). Reducing Overfitting in Large Language Models with Progressive Learning. arXiv preprint arXiv:2402.09577. [2] Hendrycks, D., et al. (2021). Measuring Massively Multitask Language Understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics.|
|**2025-05-09**|**Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study**|Faeze Ghorbanpour et.al.|[2505.06149](http://arxiv.org/abs/2505.06149)|null|尽管对自动仇恨言论检测的兴趣日益增长，但大多数现有方法忽视了在线内容的语言多样性。多语言指令微调的大型语言模型如LLaMA、Aya、Qwen和BloomZ在跨语言方面具有有希望的潜力，但它们通过零样本和少样本提示识别仇恨言论的有效性仍被低估。本研究评估了基于LLM提示的检测在八种非英语语言中的应用，利用了多种提示技术，并将它们与微调的编码器模型进行了比较。我们发现，尽管在大多数现实世界的评估集中，零样本和少样本提示在性能上落后于微调的编码器模型，但在仇恨言论检测的功能测试中，它们实现了更好的泛化。我们的研究还揭示了提示设计起着关键作用，每种语言通常需要定制的提示技术以最大化性能。|
|**2025-05-09**|**LLMs Get Lost In Multi-Turn Conversation**|Philippe Laban et.al.|[2505.06120](http://arxiv.org/abs/2505.06120)|**[link](https://github.com/microsoft/lost_in_conversation)**|**大型语言模型（LLMs）是会话接口。因此，LLMs不仅能在用户能够完全指定任务时帮助他们，还能通过多轮对话交流来帮助他们定义、探索和细化他们的需求。尽管分析LLM对话日志已证实用户指令中经常出现不明确的情况，但LLM评估主要关注单轮、完全明确的指令设置。在这项工作中，我们进行了大规模模拟实验，比较LLMs在单轮和多轮设置中的性能。我们的实验证实，我们测试的所有顶级公开和闭源LLMs在多轮对话中的表现都显著低于单轮，六个生成任务的平均降幅为39%。对20万多次模拟对话的分析将性能下降分解为两个组成部分：适度的能力损失和显著的不可靠性增加。我们发现，LLMs往往在早期回合中做出假设，并过早地尝试生成最终解决方案，对它们过度依赖。简单来说，我们发现“当LLMs在对话中走错方向时，它们会迷失方向，无法恢复”。**|
|**2025-05-09**|**LLMs Outperform Experts on Challenging Biology Benchmarks**|Lennart Justen et.al.|[2505.06108](http://arxiv.org/abs/2505.06108)|null|这项研究系统地评估了27个前沿的大型语言模型在涵盖分子生物学、遗传学、克隆、病毒学和生物安全等八个不同生物基准上的表现。对2022年11月至2025年4月间主要人工智能开发者发布的模型，每个基准进行了十次独立运行。研究发现，在生物学能力方面取得了显著进步。在研究期间，顶级模型在病毒学能力测试的具有挑战性的纯文本子集上的性能提高了4倍以上，顶级模型的表现现在是病毒学专家的两倍。一些模型在其他具有挑战性的基准上，包括LAB-Bench克隆场景和GPQA、WMDP的生物子集，现在与专家级表现相当或超过。与预期相反，思维链并没有在零样本评估上显著提高性能，而o3-mini和Claude 3.7 Sonnet中的扩展推理功能通常如预期的那样提高了性能。PubMedQA以及MMLU和WMDP的生物子集等基准表现出性能平台期，远低于100%，这表明基准饱和以及基准数据中存在错误。分析突出了随着人工智能系统的持续进步，需要更复杂的评估方法。|
|**2025-05-09**|**Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs**|Sam Bush et.al.|[2505.06096](http://arxiv.org/abs/2505.06096)|null|大型语言模型（LLM）在硬件设计任务方面的能力有限，例如生成功能Verilog代码，这促使人们利用来自开源存储库的精选硬件数据集进行各种微调优化。然而，这些数据集在规模上仍然有限，并且对再利用的许可检查很少，导致微调后的LLM可能侵犯版权。因此，我们提出一个评估基准，以估计Verilog训练的LLM生成受版权保护代码的风险。为了最小化这种风险，我们提供了一个开源的Verilog数据集FreeSet，包含超过22万个文件，以及用于提供额外公平使用Verilog数据保证的自动化数据集整理框架。然后，我们执行了一个包含持续预训练的LLM微调框架，从而得到针对Verilog的微调Llama模型FreeV。我们的结果表明，FreeV在先前的工作中显示出最小的版权侵权风险，侵权率仅为3%。此外，实验结果还表明，与基线模型相比，FreeV在Verilog生成功能方面有所改进，VerilogEval的pass@10率提高了超过10%。|
|**2025-05-09**|**Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities**|Hiari Pizzini Cavagna et.al.|[2505.06085](http://arxiv.org/abs/2505.06085)|null|随着对生成式人工智能（LLMs）服务的需求不断增长，对优化计算效率和能耗的专用硬件架构的需求也在增加。本文评估了Tenstorrent Grayskull e75 RISC-V加速器在降低数值精度下进行基本线性代数核操作的性能，这是LLMs计算中的基本操作。我们详细描述了Grayskull的执行模型、网格大小、矩阵维度、数据格式以及数值精度对计算效率的影响。此外，我们将Grayskull的性能与具有张量加速的先进架构进行了比较，包括英特尔Sapphire Rapids处理器和两款NVIDIA GPU（V100和A100）。虽然NVIDIA GPU在原始性能上占据主导地位，但Grayskull在功耗和计算吞吐量之间展现出有竞争力的权衡，以BF16达到峰值1.55 TFLOPs/Watt。|
|**2025-05-08**|**Generating Physically Stable and Buildable LEGO Designs from Text**|Ava Pun et.al.|[2505.05469](http://arxiv.org/abs/2505.05469)|**[link](https://github.com/AvaLovelace1/LegoGPT)**|**我们引入了LegoGPT，这是第一个从文本提示中生成物理稳定的乐高积木模型的方案。为了实现这一点，我们构建了一个包含乐高设计及其相关说明的大规模、物理稳定的语料库，并训练了一个自回归的大型语言模型来通过下一个标记预测来预测下一个要添加的积木。为了提高生成的设计的稳定性，我们在自回归推理过程中采用了一种有效的有效性检查和物理感知回滚，利用物理定律和组装约束剪枝不可行的标记预测。我们的实验表明，LegoGPT可以生成稳定、多样且美观的乐高设计，与输入文本提示紧密一致。我们还开发了一种基于文本的乐高纹理化方法来生成彩色和纹理化的设计。我们展示了这些设计可以由人类手动组装，也可以由机械臂自动组装。我们还发布了我们的新数据集StableText2Lego，包含超过47,000个乐高结构，这些结构由超过28,000个独特的3D对象组成，并配有详细的说明，同时还有我们的代码和模型在项目网站：https://avalovelace1.github.io/LegoGPT/上。**|
|**2025-05-08**|**StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant**|Haibo Wang et.al.|[2505.05467](http://arxiv.org/abs/2505.05467)|null|我们提出了StreamBridge，这是一个简单而有效的框架，能够无缝地将离线视频语言模型（Video-LLMs）转换为具有流处理能力的模型。StreamBridge解决了将现有模型适应在线场景的两个基本挑战：（1）多轮实时理解能力有限；（2）缺乏主动响应机制。具体来说，StreamBridge结合了以下两个方面：（1）一个内存缓冲区与循环衰减压缩策略相结合，支持长上下文多轮交互；（2）一个解耦的、轻量级的激活模型，可以轻松集成到现有的Video-LLMs中，实现连续的主动响应。为了进一步支持StreamBridge，我们构建了Stream-IT，这是一个专门为流视频理解定制的大型数据集，包含交错的视频-文本序列和多种指令格式。大量实验表明，StreamBridge在各种任务中显著提高了离线Video-LLMs的流理解能力，甚至超越了GPT-4o和Gemini 1.5 Pro等专有模型。同时，它在标准视频理解基准测试中也实现了具有竞争力的或更优的性能。|
|**2025-05-08**|**ComPO: Preference Alignment via Comparison Oracles**|Peter Chen et.al.|[2505.05465](http://arxiv.org/abs/2505.05465)|null|直接对齐方法在将大型语言模型（LLMs）与人类偏好对齐方面越来越受欢迎。然而，这些方法存在冗长和概率偏移的问题，这些问题可能是由导致偏好和反偏好响应具有相似概率的噪声偏好对引起的。本文的贡献有两个方面。首先，我们提出了一种基于比较预言机的新偏好对齐方法，并为其基本方案提供了收敛保证。其次，我们使用一些启发式方法改进了我们的方法，并通过实验展示了实际方案在改进LLMs使用噪声偏好对性能方面的灵活性和兼容性。评估在多个基础和指令调整模型（Mistral-7B、Llama-3-8B和Gemma-2-9B）以及基准（AlpacaEval 2、MT-Bench和Arena-Hard）上完成。实验结果表明，我们的方法作为现有直接对齐方法局限性的替代方案是有效的。我们工作的一个亮点是，我们证明了为具有不同概率边界的偏好对设计专用方法的重要性，这补充了Razin等人最近在文献[15]中的发现。|
|**2025-05-08**|**Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging**|Shiqi Chen et.al.|[2505.05464](http://arxiv.org/abs/2505.05464)|**[link](https://github.com/shiqichen17/vlm_merging)**|**视觉-语言模型（VLMs）将视觉感知与大型语言模型（LLMs）的通用能力，如推理能力相结合。然而，这两种能力如何结合以及如何相互贡献的机制仍然了解甚少。在这项工作中，我们通过模型融合来探索感知与推理的结合，这种融合将不同模型的参数连接起来。与以往通常关注同种模型融合的工作不同，我们提出了跨模态模型融合，使得LLMs的推理能力可以融入VLMs。通过广泛的实验，我们证明了模型融合为将LLMs的推理能力转移到VLMs提供了一种无需训练的成功途径。此外，我们利用融合后的模型来理解感知和推理的内部机制以及融合如何影响它们。我们发现，感知能力主要编码在模型的早期层，而推理则主要由中间到晚期层促进。在融合之后，我们观察到所有层开始对推理做出贡献，而感知能力在层间的分布基本保持不变。这些观察结果揭示了模型融合作为多模态集成和解释工具的潜力。**|
|**2025-05-08**|**UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections**|Fatima Haouari et.al.|[2505.05459](http://arxiv.org/abs/2505.05459)|null|误导性叙事在选举期间塑造公众舆论中扮演着至关重要的角色，因为它们可以影响选民对候选人及政治党的看法。这要求我们能够准确检测这些叙事。为了解决这个问题，我们首次提出了在欧洲近期选举中流传的常见误导性叙事的分类法。基于这个分类法，我们构建并分析了UKElectionNarratives：这是第一个关于2019年和2024年英国大选期间流传的经过人工标注的误导性叙事的数据集。我们还对预训练的大规模语言模型（重点关注GPT-4o）进行了基准测试，研究其在检测与选举相关的误导性叙事方面的有效性。最后，我们讨论了潜在的应用案例，并针对使用所提出的代码簿和数据进行的研究方向提出了建议。|
|**2025-05-08**|**Conversational Process Model Redesign**|Nataliia Klievtsova et.al.|[2505.05453](http://arxiv.org/abs/2505.05453)|null|随着大型语言模型（LLMs）的近期成功，AI增强的业务流程管理系统（BPMS）的构想变得更加可行。它们的一个基本特征是具备会话式可操作性，使人类能够有效地与LLM互动，执行关键流程生命周期任务，如流程模型设计和重构。然而，目前大部分研究集中在单次提示执行和结果评估上，而不是用户与LLM之间的持续互动。在这项工作中，我们旨在探讨使用LLMs使领域专家以迭代和有效的方式在流程模型创建和重构中增强能力的可行性。所提出的会话式流程模型重构（CPD）方法接收用户以自然语言输入的流程模型和重构请求。LLM不仅被用来进行更改，还被用于（a）从文献中识别流程变更模式，（b）重新表述变更请求以与已识别模式预期的措辞（即意义）相一致，然后（c）将变更的意义应用到流程模型上。这种多步骤方法允许进行可解释和可重复的变更。为了确保CPD方法的可行性，并找出文献中的模式如何被LLM处理，我们进行了广泛的评估。结果显示，一些模式对于LLM和用户来说难以理解。在研究范围内，我们证明了用户需要支持来清楚地描述变更。总体而言，评估表明LLM根据一系列完整性和正确性标准可以很好地处理大多数变更。|
|**2025-05-08**|**clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations**|Chalamalasetti Kranti et.al.|[2505.05445](http://arxiv.org/abs/2505.05445)|null|随着指令调整大型语言模型（LLMs）的出现，对话系统的领域得到了发展，既能够实现现实用户模拟，又能够构建强大的多轮对话代理。然而，现有研究往往单独评估这些组件——要么聚焦于单个用户模拟器，要么关注特定的系统设计——这限制了跨架构和配置的洞察力的泛化性。在这项工作中，我们提出了clem todd（用于任务导向对话系统开发的聊天优化LLMs），这是一个在一致条件下系统评估对话系统的灵活框架。clem todd允许在用户模拟器和对话系统组合上进行详细的基准测试，无论是来自文献的现有模型还是新开发的模型。它支持即插即用集成，并确保统一的语料库、评估指标和计算约束。我们通过在统一的设置中重新评估现有的任务导向对话系统，并将三个新提出的对话系统整合到相同的评估流程中，展示了clem todd的灵活性。我们的结果提供了关于架构、规模和提示策略如何影响对话性能的可操作见解，为构建高效且有效的对话人工智能系统提供了实用指导。|
|**2025-05-08**|**GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based Interaction in Virtual Reality**|Xiyun Hu et.al.|[2505.05441](http://arxiv.org/abs/2505.05441)|null|基于大型语言模型（LLM）的副驾驶在扩展现实（XR）应用中展现出巨大的潜力。然而，由于仅通过文本或语音传达空间-时间信息存在复杂性，用户在向副驾驶描述3D环境时面临挑战。为了解决这个问题，我们引入了GesPrompt，这是一种多模态XR界面，它将共说话手势与语音相结合，使用户能够更自然、更准确地与基于LLM的副驾驶在XR环境中进行沟通。通过结合手势，GesPrompt从共说话手势中提取空间-时间参考，减少了对精确文本提示的需求，并降低了用户的心理负荷。我们的贡献包括：（1）在XR环境中集成手势和语音输入的工作流程，（2）实现该工作流程的VR系统原型，（3）一项用户研究，证明了它在提高VR环境中用户沟通效果方面的有效性。|
|**2025-05-08**|**EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation**|Biao Yi et.al.|[2505.05440](http://arxiv.org/abs/2505.05440)|null|基于云的移动代理，由（多模态）大型语言模型（M）LLMs提供强大的推理能力，但存在高延迟和成本问题。虽然微调的（M）SLMs可以实现边缘部署，但它们通常失去通用能力，难以处理复杂任务。为了解决这个问题，我们提出了\textbf{EcoAgent}，这是一个用于移动自动化的\textbf{E}dge-\textbf{C}loud c\textbf{O}llaborative多代理框架。EcoAgent具有云端的规划代理与两个边缘代理之间的闭环协作：执行代理用于执行动作，观察代理用于验证结果。观察代理使用预理解模块将屏幕图像压缩成简洁的文本，从而减少令牌使用和通信开销。在出现故障的情况下，规划代理通过记忆模块检索屏幕历史并通过反思模块重新规划。在AndroidWorld上的实验表明，EcoAgent实现了与云端移动代理相当的任务成功率，同时显著减少了MMLM令牌消耗，实现了高效且实用的移动自动化。|
|**2025-05-08**|**Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data**|Yudong Wang et.al.|[2505.05427](http://arxiv.org/abs/2505.05427)|null|随着大型语言模型（LLMs）的快速发展，数据质量已成为提升模型性能的关键因素。模型驱动的数据过滤已成为获取高质量数据的主要方法。然而，它仍然面临两个主要挑战：（1）缺乏有效的数据验证策略，使得难以及时提供数据质量反馈；（2）选择用于训练分类器的种子数据缺乏明确的准则，过度依赖人类专业知识，引入了一定程度的主观性。为了解决第一个挑战，我们引入了一种高效的验证策略，该策略能够在最低的计算成本下快速评估数据对LLM训练的影响。为了应对第二个挑战，我们基于高质量种子数据对LLM训练有益的假设，通过整合所提出的验证策略，优化了正负样本的选择，并提出了一种高效的数据过滤流程。该流程不仅提高了过滤效率、分类器质量和鲁棒性，还显著降低了实验和推理成本。此外，为了高效地过滤高质量数据，我们采用了一种基于fastText的轻量级分类器，并将过滤流程成功应用于两个广泛使用的预训练语料库——FineWeb和中文FineWeb数据集，从而创建了更高质量的Ultra-FineWeb数据集。Ultra-FineWeb包含大约1000亿个英语标记和1200亿个中文标记。实证结果表明，在Ultra-FineWeb上训练的LLMs在多个基准任务上表现出显著的性能提升，验证了我们的流程在提升数据质量和训练效率方面的有效性。|
|**2025-05-07**|**EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning**|Zhenghao Xing et.al.|[2505.04623](http://arxiv.org/abs/2505.04623)|**[link](https://github.com/harryhsing/echoink)**|多模态大型语言模型（MLLMs）在文本、视觉和音频感知方面取得了进展，但它们在结构化跨模态推理方面往往存在困难，尤其是在整合音频和视觉信号时。我们介绍了EchoInk-R1，这是一个强化学习框架，用于增强MLLMs中的这种推理能力。EchoInk-R1建立在Qwen2.5-Omni-7B的基础上，并通过分组相对策略优化（GRPO）进行优化，用于处理同步音频-图像对的多选题回答。为此，我们精心制作了AVQA-R1-6K数据集，该数据集将音频-图像输入与来自OmniInstruct-v1的多选题配对。EchoInk-R1-7B在验证集上达到了85.77%的准确率，超过了基线模型，后者得分为80.53%，仅使用了562个强化学习步骤。除了准确率之外，EchoInk-R1通过回顾初始解释并在面对模糊的多模态输入时细化回答，展示了反思推理。这些结果表明，轻量级的强化学习微调可以增强MLLMs的跨模态推理。EchoInk-R1是第一个通过强化学习将音频、视觉和文本模态统一用于通用开放世界推理的框架。代码和数据已公开发布，以促进进一步的研究。|
|**2025-05-07**|**On Path to Multimodal Generalist: General-Level and General-Bench**|Hao Fei et.al.|[2505.04620](http://arxiv.org/abs/2505.04620)|null|多模态大型语言模型（MLLM）目前正经历着快速的发展，这得益于LLMs的先进能力。与早期的专家不同，现有的MLLM正在向多模态通才范式演进。最初仅限于理解多种模态，这些模型现在不仅能够理解，还能跨模态生成。它们的能力已从粗粒度扩展到细粒度的多模态理解，并从支持有限的模态扩展到任意模态。尽管存在许多基准来评估MLLM，但一个关键问题出现了：我们能否简单地假设在各项任务中表现更高意味着更强的MLLM能力，从而使我们更接近人类水平的AI？我们认为答案并不像看起来那么简单。本项目引入了通用级别，这是一个评估框架，它定义了MLLM性能和泛化的5个级别，提供了一种比较MLLM和衡量现有系统向更稳健的多模态通才和最终通用人工智能（AGI）迈进的方法。框架的核心是协同的概念，它衡量模型在理解和生成以及多个模态之间是否保持一致的能力。为了支持这种评估，我们提出了通用基准，它涵盖了更广泛的技术、模态、格式和能力，包括超过700个任务和325,800个实例。涉及超过100个现有最先进MLLM的评估结果揭示了通才的能力排名，突出了达到真正人工智能的挑战。我们预计这个项目将为下一代多模态基础模型的研究铺平道路，提供一个稳健的基础设施来加速AGI的实现。项目页面：https://generalist.top/|
|**2025-05-07**|**OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution**|Lianghong Guo et.al.|[2505.04606](http://arxiv.org/abs/2505.04606)|**[link](https://github.com/deepsoftwareanalytics/omnigirl)**|GitHub问题解决任务旨在自动解决在存储库中报告的问题。随着大型语言模型（LLMs）的进步，这项任务越来越受到关注，并提出了几个基准来评估LLMs的问题解决能力。然而，现有的基准有三个主要局限性。首先，当前基准专注于单一编程语言，限制了不同语言存储库问题评估的全面性。其次，它们通常涵盖的范围较窄，可能无法代表现实世界问题的多样性。第三，现有的基准完全依赖于问题描述中的文本信息，忽略了问题中如图像等多模态信息。在本文中，我们提出了OmniGIRL，这是一个多语言、多模态和多领域的GitHub问题解决基准。OmniGIRL包含959个任务实例，这些实例是从涵盖四种编程语言（即Python、JavaScript、TypeScript和Java）以及八个不同领域的存储库中收集的。我们的评估表明，当前LLMs在OmniGIRL上的表现有限。值得注意的是，表现最佳的模型GPT-4o仅解决了8.6%的问题。此外，我们发现当前LLMs在解决需要理解图像的问题上遇到了困难。最佳性能由Claude-3.5-Sonnet实现，仅解决了包含图像信息的10.5%的问题。最后，我们分析了当前LLMs在OmniGIRL上失败的原因，为未来的改进提供了见解。|
|**2025-05-07**|**MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection**|Zhihao Zhang et.al.|[2505.04594](http://arxiv.org/abs/2505.04594)|null|准确预测3D属性对于单目3D物体检测（Mono3D）至关重要，由于将2D图像映射到3D空间固有的模糊性，深度估计提出了最大的挑战。尽管现有方法利用多个深度线索（例如，估计深度不确定性、建模深度误差）来提高深度精度，但它们忽略了准确深度预测需要依赖于其他3D属性，因为这些属性通过3D到2D投影本质上是相互关联的，这最终限制了整体精度和稳定性。受大型语言模型（LLMs）中的思维链（CoT）的启发，本文提出了一种名为MonoCoP的方法，它通过三个关键设计利用预测链（CoP）依次和条件性地预测属性。首先，它为每个3D属性采用轻量级的属性网络（AN）来学习属性特定的特征。其次，MonoCoP构建了一个显式的链来传播这些学习到的特征从一个属性到下一个属性。最后，MonoCoP使用残差连接来聚合链中每个属性的特征，确保后续属性预测依赖于所有先前处理过的属性，而不会忘记早期属性的特征。实验结果表明，我们的MonoCoP在KITTI排行榜上实现了最先进的（SoTA）性能，无需额外的数据，并在Waymo和nuScenes正面的数据集上进一步超越了现有方法。|
|**2025-05-07**|**ZeroSearch: Incentivize the Search Capability of LLMs without Searching**|Hao Sun et.al.|[2505.04588](http://arxiv.org/abs/2505.04588)|**[link](https://github.com/alibaba-nlp/zerosearch)**|有效信息搜索对于提升大型语言模型（LLM）的推理和生成能力至关重要。近期研究探讨了利用强化学习（RL）通过在现实环境中与真实搜索引擎互动来提高LLM的搜索能力。尽管这些方法显示出有希望的结果，但它们面临两个主要挑战：（1）不可控的文档质量：搜索引擎返回的文档质量通常不可预测，这给训练过程引入了噪声和不稳定性。（2）过高的API成本：RL训练需要频繁的rollout，可能涉及数十万次搜索请求，这会产生大量的API费用，严重制约了可扩展性。为了解决这些挑战，我们引入了ZeroSearch，这是一个强化学习框架，它通过不与真实搜索引擎互动来激励LLM的搜索能力。我们的方法从轻量级的监督微调开始，将LLM转换为能够根据查询生成相关和噪声文档的检索模块。在RL训练过程中，我们采用基于课程的rollout策略，逐步降低生成文档的质量，通过逐渐将其暴露于越来越具有挑战性的检索场景中，逐步激发模型的推理能力。大量实验表明，ZeroSearch能够有效地利用3B LLM作为检索模块来激励LLM的搜索能力。令人惊讶的是，7B检索模块的性能与真实搜索引擎相当，而14B检索模块甚至超越了它。此外，它在各种参数大小的基模型和指令调整模型上具有良好的泛化能力，并且兼容广泛的RL算法。|
|**2025-05-07**|**SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions**|Chloe Qianhui Zhao et.al.|[2505.04584](http://arxiv.org/abs/2505.04584)|**[link](https://github.com/zqh0421/slideitright)**|反馈在支持学生学习中至关重要。尽管已经实施了许多自动化的反馈系统以实现反馈的可扩展性，但许多现有解决方案仅关注生成基于文本的反馈。正如多媒体学习原则所指出的，使用更多模态的学习可以帮助利用更多独立通道，减少认知负荷并促进学生的学习。因此，探索人工智能（AI）在生成和接收不同模态反馈中的潜力非常重要。我们的研究利用大型语言模型（LLMs）进行文本反馈，并辅以从幻灯片库检索的相关模态——相关讲座幻灯片。通过一项在线众包研究（N=91），本研究采用2x2设计（即人类反馈与AI反馈、是否有相关幻灯片），评估了AI辅助的多模态反馈的清晰度、参与度、感知有效性和可靠性。我们发现所有条件下都观察到了显著的从先到后的学习提升。然而，这些提升在不同条件之间的差异在统计学上并不显著。问卷调查结果揭示，学生认为幻灯片反馈有助于他们的学习过程，尽管他们报告说理解起来有困难。至于AI生成的开放式反馈，学生认为它是个性化的并且与他们的回答相关，但他们对AI反馈的信任度低于人类生成的反馈。|
|**2025-05-07**|**Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization**|Wenjun Cao et.al.|[2505.04578](http://arxiv.org/abs/2505.04578)|null|强化学习（RL）微调在转换大型语言模型的同时，产生了一个我们通过实验验证的漏洞：我们的实验表明，恶意的RL微调以令人瞩目的效率拆除了安全护栏，仅需50步和最小的对抗性提示，危害程度从0-2提升到7-9。这种攻击向量特别威胁那些具有参数级别访问权限的开源模型。针对监督式微调的现有防御措施对RL的动态反馈机制证明无效。我们引入了奖励中立化，这是第一个专门针对RL微调攻击的防御框架，建立了简洁的拒绝模式，使恶意奖励信号失效。我们的方法训练模型产生最小信息量的拒绝，使攻击者无法利用，系统地中和了向有害输出优化的尝试。实验验证，我们的方法在200次攻击步骤后仍保持低危害分数（不超过2），而标准模型迅速恶化。这项工作首次提供了关于实现针对越来越容易获得的RL攻击的鲁棒防御的积极证明，填补了开源模型的关键安全空白。|
|**2025-05-07**|**Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code Development**|Kuen Sum Cheung et.al.|[2505.04521](http://arxiv.org/abs/2505.04521)|null|大型语言模型（LLM）在包括软件开发在内的多个领域产生了显著的影响。这些模型帮助程序员生成代码，可能提高生产力和效率。然而，由于这些AI模型在训练和推理阶段的高能耗，其环境影响是显著的。本研究旨在比较手动软件开发与LLM辅助方法在能耗方面的差异，使用Codeforces作为软件开发模拟平台。目标是量化环境影响并提出减少在软件开发中使用LLM的碳足迹的策略。我们的结果表明，LLM辅助的代码生成平均比手动方法产生32.72更高的碳足迹。此外，任务复杂性与两种方法碳足迹差异之间存在显著相关性。|
|**2025-05-07**|**Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs**|Yehui Tang et.al.|[2505.04519](http://arxiv.org/abs/2505.04519)|null|稀疏的大型语言模型（LLMs）结合专家混合（MoE）技术，参数量接近万亿，正在主导着最强大语言模型的领域。然而，如此庞大的模型规模对底层软件和硬件系统提出了重大挑战。在本文中，我们旨在揭示一种在Ascend NPUs上利用这种规模的方法。关键目标是更好地利用动态稀疏模型结构下的计算资源，并在实际硬件上实现预期的性能提升。为了在不重复进行昂贵的实验的情况下选择适合Ascend NPUs的模型配置，我们利用模拟来比较各种模型超参数的权衡。这项研究导致了Pangu Ultra MoE的诞生，这是一个具有7180亿参数的稀疏LLM，我们对该模型进行了实验以验证模拟结果。在系统方面，我们深入挖掘专家并行性以优化NPU设备之间的通信，以减少同步开销。我们还优化了设备内的内存效率，以进一步减少参数和激活管理开销。最终，在训练Pangu Ultra MoE时，我们实现了30.0%的MFU，性能与DeepSeek R1相当，在6K Ascend NPUs上，并证明了Ascend系统能够利用最先进语言模型的全部训练阶段。大量实验表明，我们的方法可以导致大规模稀疏语言模型（MoE）的高效训练。我们还研究了此类模型的行为，以供未来参考。|
|**2025-05-07**|**CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation**|Jiahao Li et.al.|[2505.04481](http://arxiv.org/abs/2505.04481)|null|最近，大型语言模型（LLMs）取得了显著成功，这促使人们对其生成能力产生了更大的兴趣，并希望将其扩展到通用文本之外，进入特定领域。本研究探讨了使用LLMs生成计算机辅助设计（CAD）模型的参数序列。这一努力是利用LLMs创建参数化3D形状的第一步，因为CAD模型参数与三维空间中的形状直接相关。尽管LLMs具有强大的生成能力，但这项任务仍然具有挑战性，因为这些模型在预训练阶段既没有遇到参数序列，也不具备对3D结构的直接认识。为了解决这个问题，我们提出了CAD-Llama，这是一个旨在增强预训练LLMs以生成参数化3D CAD模型的框架。具体来说，我们开发了一个分层标注管道和类似代码的格式，将参数化3D CAD命令序列转换为结构化参数化CAD代码（SPCC），并纳入了分层语义描述。此外，我们提出了一种利用SPCC的自适应预训练方法，随后是一个与CAD特定指南一致的指令微调过程。这一方法旨在使LLMs具备参数序列中固有的空间知识。实验结果表明，我们的框架在性能上显著优于之前的自回归方法和现有的LLM基线。|
|**2025-05-06**|**VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model**|Zuwei Long et.al.|[2505.03739](http://arxiv.org/abs/2505.03739)|**[link](https://github.com/vita-mllm/vita-audio)**|**随着对自然人机交互需求的增长，基于语音的系统受到了越来越多的关注，因为语音是日常沟通中最常见的形式之一。然而，现有的语音模型在流式传输过程中生成第一个音频标记时仍存在高延迟，这为部署带来了显著的瓶颈。为了解决这个问题，我们提出了VITA-Audio，这是一个端到端的大规模语音模型，具有快速音频文本标记生成功能。具体来说，我们引入了一个轻量级的多个跨模态标记预测（MCTP）模块，该模块可以在单个模型前向传递过程中高效地生成多个音频标记，这不仅加速了推理，还显著降低了流式场景中生成第一个音频的延迟。此外，我们还探索了一种四阶段渐进式训练策略，以实现模型加速，同时最大限度地减少语音质量的损失。据我们所知，VITA-Audio是第一个能够在第一次前向传递期间生成音频输出的多模态大型语言模型，使实时对话能力具有最小延迟。VITA-Audio完全可重复，仅使用开源数据进行训练。实验结果表明，我们的模型在70亿参数规模上实现了3~5倍的推理速度提升，同时在自动语音识别（ASR）、文本到语音（TTS）和口语问答（SQA）等多个基准测试中，显著优于类似规模的开放源代码模型。**|
|**2025-05-06**|**Graph Drawing for LLMs: An Empirical Evaluation**|Walter Didimo et.al.|[2505.03678](http://arxiv.org/abs/2505.03678)|null|我们的工作为快速增长的关于使用大型语言模型（LLMs）执行图相关任务的文献做出了贡献。特别是，我们关注依赖于视觉模态的使用场景，将分析中图的绘制输入到模型中。我们研究了模型性能受所选布局范式、绘图的审美以及用于查询的提示技术的影响。我们提出了三个相应的研究问题，并展示了彻底实验分析的结果。我们的发现表明，选择合适的布局范式以及从人类视角优化输入绘图的易读性可以显著提高模型在给定任务上的性能。此外，选择最有效的提示技术是一项具有挑战性但至关重要的任务，对于实现最佳性能至关重要。|
|**2025-05-06**|**Binding threshold units with artificial oscillatory neurons**|Vladimir Fanaskov et.al.|[2505.03648](http://arxiv.org/abs/2505.03648)|**[link](https://github.com/vlsf/hkmemory)**|人工Kuramoto振荡神经元最近被提出作为一种替代阈值单元的选择。经验证据表明，振荡单元在包括无监督对象发现和某些推理问题在内的多个任务中优于阈值单元。这些振荡神经元所提出的耦合机制是异质的，它结合了广义Kuramoto方程和用于阈值单元的标准耦合方法。在本研究简报中，我们提出一个理论框架，该框架明确区分了振荡神经元和阈值单元，并建立了它们之间的耦合机制。我们认为，从生物学角度来看，振荡单元和阈值单元实现了神经编码的不同方面：大致来说，阈值单元模拟神经元放电的强度，而振荡单元通过频率调制促进信息交换。为了推导这两种类型单元之间的相互作用，我们通过关注允许Lyapunov函数的动力学系统来约束它们的动态。对于阈值单元，这导致Hopfield联想记忆模型，而对于振荡单元，它产生了一种特定的广义Kuramoto模型。由此产生的动力学系统可以自然地耦合形成Hopfield-Kuramoto联想记忆模型，该模型也允许Lyapunov函数。可能的耦合形式有很多。值得注意的是，振荡神经元可以用来对Hopfield网络的权重矩阵实施低秩校正。这种校正可以被视为一种Hebbian学习形式，或者是一种用于大型语言模型微调的流行LoRA方法。我们通过示例玩具实验展示了这种特定耦合的实际实现。|
|**2025-05-06**|**PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing**|Yiping Xie et.al.|[2505.03621](http://arxiv.org/abs/2505.03621)|null|远程光电容积脉搏波描记法（rPPG）可以实现非接触式生理测量，但仍然容易受到光照变化、运动伪影和有限的时间建模的影响。大型语言模型（LLMs）擅长捕捉长距离依赖关系，提供了潜在的解决方案，但由于其以文本为中心的设计，难以处理rPPG信号的连续性和噪声敏感性质。为了弥合这一差距，我们引入了PhysLLM，这是一个协同优化框架，它将LLMs与特定领域的rPPG组件相结合。具体来说，我们提出了文本原型引导（TPG）策略，通过将血流动力学特征投影到LLM可解释的语义空间，建立跨模态对齐，有效地弥合了生理信号与语言标记之间的表征差距。此外，我们还提出了一种新颖的双重域平稳（DDS）算法，通过自适应时频域特征重新加权来解决问题的不稳定性。最后，通过生理统计学、环境上下文回答和任务描述，针对rPPG任务的特定线索系统地注入生理先验，利用跨模态学习整合视觉和文本信息，使系统能够动态适应如光照变化和受试者运动等挑战性场景。在四个基准数据集上的评估中，PhysLLM实现了最先进的准确性和鲁棒性，证明了其在光照变化和运动场景中的卓越泛化能力。|
|**2025-05-06**|**DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes**|Sergey Linok et.al.|[2505.03581](http://arxiv.org/abs/2505.03581)|**[link](https://github.com/linukc/dygenc)**|**在动态环境中分析事件，对开发能够与人类互动的智能代理和机器人构成了一个基本挑战。当前的方法主要利用视觉模型。然而，这些方法通常从图像中隐式地捕捉信息，缺乏可解释的空间时间对象表示。为了解决这个问题，我们引入了DyGEnc——一种编码动态图的新方法。这种方法将压缩的空间时间结构观察表示与大型语言模型的认知能力相结合。这种结合的目的是通过一系列文本场景图实现高级问答。在STAR和AGQA数据集上的扩展评估表明，DyGEnc在处理关于人类与对象交互历史的查询方面，比现有的视觉方法高出了15-25%的显著优势。此外，所提出的方法可以无缝扩展到处理原始输入图像，利用基础模型提取显式的文本场景图，这通过在轮式操作平台进行的机器人实验结果得到了证实。我们希望这些发现将有助于实现鲁棒且压缩的基于图的机器人记忆，以支持长期推理。代码可在github.com/linukc/DyGEnc获取。**|
|**2025-05-06**|**LlamaFirewall: An open source guardrail system for building secure AI agents**|Sahana Chennabasappa et.al.|[2505.03574](http://arxiv.org/abs/2505.03574)|null|大型语言模型（LLMs）已经从简单的聊天机器人发展成为能够执行复杂任务（如编辑生产代码、编排工作流程以及根据网页和电子邮件等不可信输入采取高风险行动）的自主代理。这些能力引入了新的安全风险，现有的安全措施（如模型微调或以聊天机器人为中心的防护措施）无法完全解决。鉴于风险更高且没有确定性的解决方案来缓解这些风险，迫切需要一种实时防护措施监控器作为最后一道防线，并支持系统级、特定用例的安全策略定义和执行。我们引入了LlamaFirewall，这是一个开源的安全防护框架，旨在作为对抗与AI代理相关的安全风险的最后一道防线。我们的框架通过三个强大的防护措施来缓解风险：PromptGuard 2，一个通用的越狱检测器，展现了领先的技术性能；代理对齐检查，一种思维链审计器，检查代理的推理以识别提示注入和目标错位，尽管仍在实验阶段，但相较于先前提出的方法，在一般场景中防止间接注入的效果更强；以及CodeShield，一个快速且可扩展的在线静态分析引擎，旨在防止编码代理生成不安全或危险的代码。此外，我们还包含了易于使用且可定制的扫描器，使任何能够编写正则表达式或LLM提示的开发者都能快速更新代理的安全防护措施。|
|**2025-05-06**|**Say It Another Way: A Framework for User-Grounded Paraphrasing**|Cléa Chataigner et.al.|[2505.03563](http://arxiv.org/abs/2505.03563)|null|论文摘要翻译为中文：  提示语措辞的微小变化可能导致大型语言模型（LLMs）行为上的显著差异，从而引发对其评估稳定性和可靠性的担忧。尽管先前的研究探讨了简单的格式变化，但这些变化很少能捕捉到实际语言使用中常见的自然变化。我们提出了一种基于最小语言变换分类法的可控释义框架，以系统地生成自然的提示语变体。利用BBQ数据集，我们通过人工标注和自动化检查验证了我们的方法，然后使用它来研究LLMs在刻板印象评估任务中对释义提示语的响应。我们的分析表明，即使是微小的提示语修改也可能导致模型行为的重大变化。这些结果突显了需要稳健、释义感知的评估协议。|
|**2025-05-06**|**A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges**|Feibo Jiang et.al.|[2505.03556](http://arxiv.org/abs/2505.03556)|**[link](https://github.com/jiangfeibo/comlam)**|6G无线通信旨在建立一个无处不在的智能连接世界，提供前所未有的通信体验。大型人工智能模型（LAMs）的特点是与典型的人工智能（AI）模型相比，具有显著更大的规模（例如，数十亿或数千亿个参数）。LAMs展现出卓越的认知能力，包括强大的泛化能力以微调下游任务，以及在训练过程中未见过的任务的处理能力。因此，LAMs能够高效地为各种通信应用提供人工智能服务，成为解决未来无线通信系统中复杂挑战的关键工具。本研究全面回顾了LAMs在通信领域的基础、应用和挑战。首先，我们介绍了基于AI的通信系统的当前状态，强调了将LAMs整合到通信中的动机，并总结了关键贡献。然后，我们概述了通信中LAMs的基本概念。这包括介绍LAMs的主要架构，如Transformer、扩散模型和Mamba。我们还探讨了LAMs的分类，包括大型语言模型（LLMs）、大型视觉模型（LVMs）、大型多模态模型（LMMs）和世界模型，并考察了它们在通信中的潜在应用。此外，我们涵盖了通信系统中LAMs的训练方法和评估技术。最后，我们介绍了优化策略，如思维链（CoT）、检索增强生成（RAG）和代理系统。在此之后，我们讨论了LAMs在各种通信场景中的研究进展。最后，我们分析了当前研究中的挑战，并提供了对未来研究方向的见解。|
|**2025-05-06**|**A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning**|Kolawole E. Ogunsina et.al.|[2505.03553](http://arxiv.org/abs/2505.03553)|null|不一致的输出和幻觉是大语言模型（LLM）可靠性的主要障碍。当不同的专有推理模型（RM），如OpenAI、Google、Anthropic、DeepSeek和xAI的模型，面对同一复杂请求时，由于训练和推理的差异，它们通常会产生不同的结果。本文提出了一种基于分布式账本技术的创新共识机制，用于验证和收敛这些输出，将每个RM视为一个黑盒对等体。基于哈希图共识算法，我们的方法采用八卦-八卦通信和虚拟投票，以实现在多个RM集合并中的共识。我们提出了一种原型系统的架构设计，其中RM迭代交换和更新其答案，利用每轮的信息来提高后续轮次中答案的准确性和信心。这种方法不仅超越了简单的多数投票，还通过结合每个模型的知识和交叉验证内容。我们论证了基于哈希图的共识在AI集合并中的可行性，并概述了其在减少非事实输出方面的优势。本文讨论了实施的初步考虑、收敛和准确性的评估标准以及潜在的挑战。所提出的机制展示了多智能体AI系统自我验证并在复杂任务中提供高保真响应的有希望的方向。|
|**2025-05-06**|**STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game**|Eric Zhou et.al.|[2505.03547](http://arxiv.org/abs/2505.03547)|null|我们介绍了STORY2GAME，这是一种使用大型语言模型生成基于文本的互动小说游戏的新方法。该方法首先生成故事，填充世界，并为游戏引擎中的动作构建代码，使故事能够以交互式的方式展开。与一组硬编码的动作可以人为地限制故事生成不同，生成动作的能力意味着故事生成过程可以更加开放，但仍能提供基于游戏状态的经验。成功生成动作的关键是利用LLM在故事中生成的动作前提条件和效果作为指南，以确定游戏引擎在玩家执行动作时必须跟踪和改变的游戏状态方面。我们还介绍了一种动态生成新动作的技术，以适应玩家执行他们认为不属于故事的动作的愿望。动态动作生成可能需要即时更新游戏引擎的状态表示和修改之前生成的动作。我们评估了动作代码生成的成功率，即玩家能否交互式地玩完整个生成的故事。|
|**2025-05-05**|**Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation**|Lu Ling et.al.|[2505.02836](http://arxiv.org/abs/2505.02836)|null|从文本合成交互式3D场景对于游戏、虚拟现实和具身人工智能至关重要。然而，现有方法面临诸多挑战。基于学习的方法依赖于小规模的室内数据集，限制了场景的多样性和布局的复杂性。虽然大型语言模型（LLM）可以利用多样化的文本领域知识，但在空间真实感方面存在困难，常常产生不自然的物体放置，未能尊重常识。我们的关键洞察是，视觉感知可以通过提供LLM所缺乏的逼真空间指导来弥合这一差距。为此，我们引入了Scenethesis，这是一个无需训练的代理框架，它将基于LLM的场景规划和视觉引导的布局细化相结合。给定一个文本提示，Scenethesis首先使用LLM来草拟一个粗略布局。然后，视觉模块通过生成图像指导和提取场景结构来细化它，以捕捉物体之间的关系。接下来，优化模块迭代地执行准确的姿态对齐和物理可能性，防止物体穿透和不稳定等伪影。最后，裁判模块验证空间一致性。全面的实验表明，Scenethesis能够生成多样化、逼真且物理上合理的3D交互式场景，使其在虚拟内容创作、模拟环境和具身人工智能研究中具有价值。|
|**2025-05-05**|**R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning**|Yi-Fan Zhang et.al.|[2505.02835](http://arxiv.org/abs/2505.02835)|**[link](https://github.com/yfzhang114/r1_reward)**|多模态奖励模型（MRMs）在提升多模态大型语言模型（MLLMs）性能方面发挥着关键作用。虽然近期的研究主要集中在改进MRMs的模型结构和训练数据上，但对于长期推理能力在奖励建模中的有效性以及如何在MRMs中激活这些能力的探索还相对有限。在本文中，我们探讨了如何利用强化学习（RL）来改进奖励建模。具体来说，我们将奖励建模问题重新定义为基于规则的RL任务。然而，我们观察到，直接将现有的RL算法，如Reinforce++，应用于奖励建模往往会因为这些算法的固有局限性而导致训练不稳定甚至崩溃。为了解决这个问题，我们提出了StableReinforce算法，该算法对现有RL方法的训练损失、优势估计策略和奖励设计进行了优化。这些优化使得训练动态更加稳定，并提高了性能。为了促进MRMs的训练，我们收集了来自不同数据集的20K偏好数据。使用StableReinforce算法在此数据集上训练的奖励模型R1-Reward，在多模态奖励建模基准测试中显著提高了性能。与之前的SOTA模型相比，R1-Reward在VL Reward-Bench上提高了8.4%，在多模态奖励基准测试上提高了14.3%。此外，随着更多推理计算的投入，R1-Reward的性能进一步得到提升，突显了RL算法在优化MRMs中的潜力。|
|**2025-05-05**|**ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations**|Dmitriy Shopkhoev et.al.|[2505.02819](http://arxiv.org/abs/2505.02819)|**[link](https://github.com/mts-ai/replaceme)**|我们引入了ReplaceMe，这是一种通用的无需训练的深度剪枝方法，它有效地用线性运算替换了Transformer块，同时在高压缩比下保持高性能。与需要额外训练或微调的传统剪枝方法不同，我们的方法只需要一个小型校准数据集，用于估计一个线性变换来近似剪枝块。这个估计的线性映射可以无缝地与剩余的Transformer块合并，从而消除了任何额外网络参数的需求。我们的实验表明，ReplaceMe在无需训练的方法中始终优于其他方法，并且在涉及大量重新训练/微调和架构修改的最先进剪枝方法中保持高度竞争力。应用于几个大型语言模型（LLMs），ReplaceMe在公开基准测试中实现了高达25%的剪枝，同时保留了约90%的原模型性能——无需任何训练或修复步骤，从而实现了最小的计算开销（见图1）。我们提供了一个开源库，实现了ReplaceMe以及几种最先进的深度剪枝技术，可在本存储库中找到。|
|**2025-05-05**|**Towards Quantifying the Hessian Structure of Neural Networks**|Zhaorui Dong et.al.|[2505.02809](http://arxiv.org/abs/2505.02809)|**[link](https://github.com/zyushun/hessian-structure)**|实证研究表明，神经网络（NNs）的Hessian矩阵呈现出近乎块对角结构，但其理论基础尚不明确。在这项工作中，我们揭示了塑造Hessian结构的两种力量：一种源于架构设计的“静态力量”，另一种源于训练的“动态力量”。然后，我们对随机初始化下的“静态力量”进行了严格的理论分析。我们研究了线性模型和具有一个隐藏层的网络，对于分类任务，使用了均方误差（MSE）损失和交叉熵（CE）损失。通过利用随机矩阵理论，我们比较了对角和非对角Hessian块的极限分布，并发现随着类别数 $C$趋于无穷大，块对角结构出现，其中$C$表示类别数。我们的发现表明，$C$是近乎块对角结构的主要驱动因素。这些结果可能为大型语言模型（LLMs）的Hessian结构提供新的见解，这些模型通常以超过$10^4$或$10^5$的类别数$C$ 运行。|
|**2025-05-05**|**Generating HomeAssistant Automations Using an LLM-based Chatbot**|Mathyas Giudici et.al.|[2505.02802](http://arxiv.org/abs/2505.02802)|null|为了应对气候变化，鼓励个人采取可持续的生活方式，尤其是在家庭生活中，优化他们的电力消耗。对话代理，如智能家居助手，被视为在家庭内部推广可持续实践的有效工具。我们的研究调查了大型语言模型（LLM）在增强智能家居自动化和推广可持续家庭实践中的应用，特别是使用HomeAssistant框架。特别是，它突出了GPT模型在生成准确自动化程序方面的潜力。虽然LLM在理解复杂命令和创建有效的JSON输出方面表现出色，但诸如语法错误和信息格式错误等挑战也被指出，这表明需要进一步改进的领域。尽管在“绿色”和“无绿色”提示之间的定量差异很小，但定性反馈突出了使用环保提示生成的程序在可持续性方面的积极转变。然后，一项实证评估（N=56）表明，与传统的基于规则的对手相比，该系统受到了用户的欢迎，并被认为是引人入胜的。我们的研究结果表明，LLM在推进智能家居技术方面发挥了作用，并建议进一步研究以改进这些模型，以支持更广泛的、现实世界的应用，以支持可持续生活。|
|**2025-05-05**|**HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models**|Zheng Lin et.al.|[2505.02795](http://arxiv.org/abs/2505.02795)|null|最近，大型语言模型（LLMs）取得了显著的突破，彻底改变了自然语言处理领域及更多领域。由于参数规模巨大，使用私有数据对LLMs进行微调以完成多样化的下游任务已成为主流。尽管联邦学习（FL）为在不共享原始数据的情况下微调LLMs提供了一种有前景的解决方案，但其庞大的计算成本阻碍了其普及。此外，在现实场景中，私有客户端设备通常具有异构的计算资源，这进一步复杂化了LLMs的微调。为了应对这些挑战，我们提出了HSplitLoRA，这是一个基于分割学习（SL）和低秩自适应（LoRA）微调的异构参数高效微调（PEFT）框架，用于在异构客户端设备上高效地微调LLMs。HSplitLoRA首先根据其在LLM训练中的贡献识别重要的权重。然后，它动态配置LoRA适配器的分解秩，并根据客户端设备的计算预算确定模型分割点。最后，设计了一种无噪声的适配器聚合机制，以支持无噪声的异构适配器聚合。大量的实验表明，HSplitLoRA在训练准确度和收敛速度方面优于最先进的基准。|
|**2025-05-05**|**Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control**|Nam H. Le et.al.|[2505.02766](http://arxiv.org/abs/2505.02766)|null|引导生物系统达到期望状态，如形态发生结果，仍然是医学和合成生物学领域的一个基本挑战，具有深远的影响。虽然大型语言模型（LLMs）已经使自然语言成为AI系统中可解释控制的接口，但它们作为引导生物或细胞动态的调解者的应用仍 largely 未被探索。在这项工作中，我们提出了一种功能流程，将自然语言提示转换为能够指导模拟细胞群体的空间矢量场。我们的方法结合了一个大型语言模型和一个可演化的神经网络控制器（提示到干预，或P2I），通过进化策略优化以生成在模拟2D环境中的聚类或散布等行为。我们证明，即使在词汇受限和简化细胞模型的情况下，进化的P2I网络也能成功地将细胞动态与用户用普通语言表达的目标对齐。这项工作从语言输入到模拟生物电干预再到行为输出的完整循环，为未来能够通过自然语言驱动细胞控制的系统提供了基础。|
|**2025-05-05**|**Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models**|Matthew Dahl et.al.|[2505.02763](http://arxiv.org/abs/2505.02763)|null|法律实践需要仔细遵守程序规则。在美国，没有哪一套规则比《蓝皮书：统一引证系统》中的规则更为复杂。遵守这套系统超过500页的错综复杂的格式说明是成千上万的学生法学评论编辑的职责所在，也是全美国律师的噩梦。为了评估大型语言模型（LLMs）是否能够遵守如此复杂的系统程序，我们构建了一个包含866个蓝皮书任务的原始数据集，并测试了来自OpenAI、Anthropic、Google、Meta和DeepSeek的旗舰LLMs。我们发现（1）这些模型只有69%-74%的时间能够生成完全符合蓝皮书规定的引文；（2）在蓝皮书底层规则系统上进行情境学习只能将准确率提高到77%。这些结果警告我们不要使用现成的LLMs来自动化那些程序忠实性至关重要的法律领域。|
|**2025-05-05**|**Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation**|Gerard Pons et.al.|[2505.02737](http://arxiv.org/abs/2505.02737)|null|最近在大型语言模型（LLMs）方面的进展，使其成为自然语言处理任务的突出解决方案。值得注意的是，它们可以以零样本或少量样本的方式处理这些问题，从而消除了对训练或微调特定任务模型的需求。然而，LLMs面临一些挑战，包括虚构内容和训练数据中存在过时知识或特定领域缺失信息的问题。这些问题无法通过用新数据重新训练模型来解决，因为这是一个耗时且昂贵的流程。为了减轻这些问题，知识图谱（KGs）被提出作为结构化外部信息源以丰富LLMs。基于这一理念，在本文中我们利用KGs来增强LLMs进行零样本实体消歧（ED）。为此，我们利用KG中实体类别的层次表示，逐步修剪候选空间以及实体的描述，以丰富输入提示，增加额外的实际知识。我们在流行的ED数据集上的评估表明，提出的方法优于未增强的和仅描述增强的LLMs，并且比特定任务的模型具有更高的适应性。此外，我们还进行了错误分析，并讨论了所利用的KG语义表达性对ED性能的影响。|
|**2025-05-05**|**FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models**|Zhouliang Yu et.al.|[2505.02735](http://arxiv.org/abs/2505.02735)|**[link](https://github.com/sphere-ai-lab/formalmath-bench)**|形式化数学推理仍然是人工智能领域的一个关键挑战，这受到现有基准在范围和规模上的限制。为了解决这个问题，我们提出了FormalMATH，这是一个大型Lean4基准，包含5,560个形式化验证问题，涵盖了从高中奥林匹克挑战到跨多个领域（例如，代数、应用数学、微积分、数论和离散数学）的本科水平定理。为了减轻手动形式化的低效性，我们引入了一种新颖的“人机协同”自动形式化流程，该流程集成了以下三个方面：（1）用于陈述自动形式化的专业大型语言模型（LLM），（2）多LLM语义验证，以及（3）基于否定的事实排除策略，使用现成的基于LLM的证明器。这种方法通过在手动验证前保留72.09%的陈述来降低专家标注成本，同时确保对原始自然语言问题的忠实度。我们对最先进的基于LLM的定理证明器的评估揭示了显著的局限性：即使在实际采样预算下，最强的模型也只实现了16.46%的成功率，表现出明显的领域偏见（例如，在代数上表现出色但在微积分上失败）以及过度依赖简化的自动化策略。值得注意的是，我们在思维链推理场景中发现了自然语言解决方案指导与证明成功之间的一个令人反感的反向关系，这表明人类编写的非正式推理在形式化推理环境中引入了噪音而不是清晰度。我们相信，FormalMATH为形式化数学推理提供了一个稳健的基准。|
|**2025-05-02**|**Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System**|Sheikh Samit Muhaimin et.al.|[2505.01315](http://arxiv.org/abs/2505.01315)|null|近期，大型语言模型（LLM）的广泛应用使其易受高级对抗攻击、操纵性提示和编码恶意输入的影响。现有的对抗措施通常需要重新训练模型，这既计算成本高，又难以部署。本研究提出了一种独特的防御范式，无需重新训练或微调，允许LLM自身识别、过滤和防御对抗或恶意输入。该建议框架主要有两部分：(1)一个提示过滤模块，它使用高级自然语言处理（NLP）技术，包括零样本分类、关键词分析和编码内容检测（例如base64、十六进制、URL编码），以检测、解码和分类有害输入；(2)一个摘要模块，它处理和总结对抗性研究文献，为LLM提供上下文感知的防御知识。这种方法通过融合文本提取、摘要和有害提示分析，增强了LLM对对抗性利用的抵抗力。根据实验结果，该集成技术在识别有害模式、操纵性语言结构和编码提示方面具有98.71%的成功率。通过使用少量的对抗性研究文献作为上下文，该方法还使模型能够以更大的越狱抵抗率和拒绝率正确应对有害输入。在保持LLM响应质量的同时，该框架显著提高了LLM对恶意滥用的抵抗力，证明了其作为耗时、基于重新训练的防御替代品的快速简便性。|
|**2025-05-02**|**Enhancing SPARQL Query Rewriting for Complex Ontology Alignments**|Anicet Lepetit Ondo et.al.|[2505.01309](http://arxiv.org/abs/2505.01309)|null|SPARQL查询重写是统一查询Linked Data Web中异构本体的一种基本机制。然而，本体对齐的复杂性，尤其是丰富的对应关系（c : c），使得这一过程颇具挑战。现有方法主要关注简单（s : s）和部分复杂（s : c）的对齐，从而忽略了更具表达力的对应关系带来的挑战。此外，SPARQL的复杂语法为那些希望充分利用本体中封装的知识但又不熟悉SPARQL的非专家用户构成了障碍。本文提出了一种创新的方法，用于根据用户用自然语言表达的需求，自动将源本体中的SPARQL查询重写到目标本体中。该方法利用了等价传递性原则以及大型语言模型（如GPT-4）的高级功能。通过整合这些元素，该方法因其能够高效处理复杂对齐，特别是（c : c）对应关系，并充分利用其表达性而脱颖而出。此外，它还便于不熟悉SPARQL的用户访问对齐本体，为查询异构数据提供了一个灵活的解决方案。|
|**2025-05-02**|**Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments**|Regan Bolton et.al.|[2505.01307](http://arxiv.org/abs/2505.01307)|null|安全关键软件评估需要对复杂的法规框架进行稳健的评估，这一过程传统上受到人工评估的限制。本文提出了一种名为文档检索增强微调（DRAFT）的新方法，该方法增强了大型语言模型（LLM）在安全关键合规性评估方面的能力。DRAFT在现有的检索增强生成（RAG）技术的基础上，通过引入一个新颖的微调框架，该框架适应了我们的双检索架构，可以同时访问软件文档和适用的参考标准。为了微调DRAFT，我们开发了一种半自动化的数据集生成方法，该方法结合了具有意义干扰项的相关文档，与实际评估场景紧密相似。使用GPT-4o-mini进行的实验表明，与基线模型相比，正确率提高了7%，在证据处理、响应结构和领域特定推理方面有定性改进。DRAFT代表了一种实用的方法，可以在保持法规领域所必需的透明度和基于证据的推理的同时，提高合规性评估系统。|
|**2025-05-02**|**FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing**|Gaoxiang Cong et.al.|[2505.01263](http://arxiv.org/abs/2505.01263)|null|电影配音旨在将剧本转换为与给定电影片段在时间和情感方面相匹配的言语，同时保留给定参考音频的音色。现有方法主要关注降低词错误率，而忽略了同步和声学质量的重要性。为了解决这些问题，我们提出了一种基于大型语言模型（LLM）的流匹配架构，用于配音，命名为FlowDubber，它通过结合大型语音语言模型和双重对比对齐，以及通过提出的语音增强流匹配实现比以前工作更好的声学质量，从而实现了高质量的视听同步和发音。首先，我们引入Qwen2.5作为LLM的骨干，以从电影剧本和参考音频中学习上下文序列。然后，所提出的语义感知学习专注于在音素级别捕捉LLM的语义知识。接下来，双重对比对齐（DCA）通过提高与唇部运动的相互对齐来减少模糊性，降低了可能混淆的相似音素的混淆。最后，所提出的基于流的语音增强（FVE）从两个方面改善了声学质量，它引入了一种基于LLM的声学流匹配指导来增强清晰度，并使用仿射风格先验在通过梯度矢量场预测将噪声恢复到梅尔频谱图时增强身份。大量的实验表明，我们的方法在两个主要基准上优于几种最先进的方法。演示可在{\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}找到。|
|**2025-05-02**|**Digital Pathway Curation (DPC): a comparative pipeline to assess the reproducibility, consensus and accuracy across Gemini, PubMed, and scientific reviewers in biomedical research**|Flavio Lichtenstein et.al.|[2505.01259](http://arxiv.org/abs/2505.01259)|null|一项科学研究从中心问题开始，搜索引擎如PubMed是获取知识和了解当前技术水平的第一个工具。大型语言模型（LLMs）已在研究中应用，承诺加速并得到更深入的结果。然而，除了谨慎之外，它们还需要严格的验证。评估复杂的生物关系对于基于SQL的工具和LLM模型仍然具有挑战性。在这里，我们介绍了数字通路编纂（DPC）管道来评估Gemini模型与PubMed搜索和人工专家编纂的一致性和准确性。使用两个组学实验，我们创建了一个大型的数据集（Ensemble），基于确定通路-疾病关联。使用Ensemble数据集，我们证明了Gemini实现了大约99%的运行间可重复性和大约75%的模型间可重复性。接下来，我们使用更小的数据集计算了众包共识。CSC使我们能够计算准确性，Gemini多模型共识达到了约87%的显著准确性。我们的研究结果表明，LLMs是可重复的、可靠的，且在导航复杂生物医学知识方面是宝贵的工具。|
|**2025-05-02**|**CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning**|Tsai-Ning Wang et.al.|[2505.01199](http://arxiv.org/abs/2505.01199)|null|医疗音频信号，如心跳和肺部声音，在临床诊断中起着至关重要的作用。然而，分析这些信号仍然具有挑战性：传统方法依赖于手工制作的特征或需要大量标记数据集的监督深度学习模型，这限制了它们的可扩展性和适用性。为了解决这些问题，我们提出了CaReAQA，这是一种集成了基础音频模型和大型语言模型的推理能力的音频语言模型，能够实现与临床相关的开放式诊断回答。与CaReAQA并行，我们引入了CaReSound，这是一个包含注释、丰富元数据和配对问答示例的医疗音频录音基准数据集，旨在推动诊断推理研究的发展。评估结果显示，CaReAQA在开放式诊断推理任务上达到了86.2%的准确率，优于基线模型。它也能很好地推广到封闭式分类任务，在未见过的数据集上平均准确率为56.9%。我们的研究结果表明，音频语言整合和推理如何推动医学诊断，使得临床决策支持的高效人工智能系统成为可能。|
|**2025-05-02**|**LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures**|Francisco Aguilera-Martínez et.al.|[2505.01177](http://arxiv.org/abs/2505.01177)|null|随着大型语言模型（LLMs）的不断进化，评估其在训练阶段以及模型部署后可能出现的安全威胁和漏洞至关重要。本综述旨在定义和分类针对LLMs的各种攻击，区分那些在训练阶段发生的攻击和影响已训练模型的攻击。本文详细分析了这些攻击，并探讨了旨在减轻此类威胁的防御机制。防御机制被分为两大类：基于预防的防御和基于检测的防御。此外，我们的综述总结了可能的攻击及其相应的防御策略，并对已知防御机制在不同安全威胁中的有效性进行了评估。我们的综述旨在为LLMs的安全提供结构化框架，同时确定需要进一步研究以改进和加强应对新兴安全挑战的领域。|
|**2025-05-02**|**Methodological Foundations for AI-Driven Survey Question Generation**|Ted K. Mburu et.al.|[2505.01150](http://arxiv.org/abs/2505.01150)|null|本文提出了一种方法框架，用于在教育调查研究中应用生成式人工智能。我们探讨了大型语言模型（LLMs）如何生成适应性、情境感知的调查问题，并介绍了合成问题-回答分析（SQRA）框架，该框架允许在部署给人类参与者之前对人工智能生成的提示进行迭代测试和改进。在活动理论的指导下，我们分析了人工智能工具如何调节参与者的参与和学习，并探讨了如偏见、隐私和透明度等伦理问题。通过对人工智能到人工智能和人工智能到人类的调查互动进行情感、词汇和结构分析，我们评估了这些问题的对齐和有效性。我们的发现突出了人工智能驱动调查工具的潜力和局限性，强调了在工程教育中支持可靠、可扩展和情境相关数据收集的强大提示工程和验证的必要性。|
|**2025-05-02**|**Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies, Datasets, and Clinical Applications**|Jiawei He et.al.|[2505.01146](http://arxiv.org/abs/2505.01146)|null|近年来，大型语言模型（LLMs）在自然语言处理任务中展现出了非凡的能力。然而，它们在生物医学领域的应用面临着独特的挑战，尤其是在事实准确性和最新知识整合方面。检索增强生成（RAG）作为一种有前景的解决方案，通过结合LLMs的生成能力和外部知识检索来应对这些挑战。这篇全面的调查分析了RAG在生物医学领域的应用，重点关注其技术组件、可用数据集和临床应用。我们系统地分析了检索方法、排名策略和生成模型，同时也探讨了这一快速发展的领域的挑战和未来方向。我们的研究为研究人员和从业者提供了对当前生物医学RAG系统的全面理解，并确定了未来研究和发展的关键领域。|
|**2025-05-02**|**MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning**|Murtadha Ahmed et.al.|[2505.01110](http://arxiv.org/abs/2505.01110)|null|大型语言模型（LLMs）在情境学习（ICL）方面展现出惊人的能力。然而，预训练模型中固定的位置长度限制限制了演示示例的数量。最近尝试扩展情境的努力随着演示数量的增加而遭受注意力分散的问题。在本文中，我们介绍了缓解大规模ICL中注意力分散的MateICL方法，该方法使得LLMs能够在情境规模增长时保持有效的自注意力。我们首先将情境分割成多个窗口，每个窗口填充到模型情境容量，然后分别处理。接着，我们引入一个额外的层来重新校准注意力权重，随着演示数量的增加，优先考虑查询标记。我们的实验结果表明，MateICL能够有效地利用更大的情境来提高ICL性能。与基于检索的基线相比，MateICL在无需外部训练的检索模型的情况下，始终实现更好的性能。尽管推理策略（例如32k标记的情境）取得了最近进展，但我们的结果表明，MateICL在计算资源受限的环境中仍然有益。代码在https://github.com/amurtadha/MateICL上公开可用。|
|**2025-05-01**|**T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT**|Dongzhi Jiang et.al.|[2505.00703](http://arxiv.org/abs/2505.00703)|**[link](https://github.com/caraj7/t2i-r1)**|近期，大型语言模型的发展展示了思维链（CoT）和强化学习（RL）如何提升性能。然而，将这些推理策略应用于视觉生成领域仍然主要未被探索。在本文中，我们提出了T2I-R1，这是一种新型的通过具有双级CoT推理过程的RL驱动的推理增强文本到图像生成模型。具体来说，我们确定了可以用于增强生成不同阶段的两个级别的CoT：（1）语义级别的CoT用于提示的高级规划；（2）标记级别的CoT用于在分块生成过程中的低级像素处理。为了更好地协调这两个级别的CoT，我们引入了具有生成奖励集合的BiCoT-GRPO，它可以在同一个训练步骤中无缝优化两个生成CoT。通过将我们的推理策略应用于基线模型Janus-Pro，我们在T2I-CompBench上实现了13%的性能提升，在WISE基准测试上实现了19%的性能提升，甚至超越了最先进的模型FLUX。1. 代码可在以下链接获取：https://github.com/CaraJ7/T2I-R1|
|**2025-05-01**|**Steering Large Language Models with Register Analysis for Arbitrary Style Transfer**|Xinchen Yang et.al.|[2505.00679](http://arxiv.org/abs/2505.00679)|null|大型语言模型（LLMs）在跨不同风格的文本重写方面展现出强大的能力。然而，如何有效地利用这种能力进行基于示例的任意风格迁移，例如将输入文本重写以匹配给定示例的风格，仍然是一个未解的挑战。一个关键问题是如何描述示例的风格来引导LLMs进行高质量的重写。在本研究中，我们提出了一种基于语域分析的提示方法来指导LLMs完成这项任务。在多个风格迁移任务上的实证评估表明，我们的提示方法比现有的提示策略更有效地增强了风格迁移强度，同时更有效地保留了意义。|
|**2025-05-01**|**Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions**|Yiming Du et.al.|[2505.00675](http://arxiv.org/abs/2505.00675)|**[link](https://github.com/elvin-yiming-du/survey_memory_in_ai)**|记忆是人工智能系统的基本组成部分，是大型语言模型（LLMs）的基础。虽然先前的综述主要关注LLMs的记忆应用，但往往忽略了支撑记忆动态的原子操作。在这篇综述中，我们首先将记忆表示分为参数化、上下文结构化和上下文非结构化三类，然后介绍了六个基本记忆操作：巩固、更新、索引、遗忘、检索和压缩。我们系统地将这些操作映射到长期记忆、长上下文、参数化修改和多源记忆中最相关的研究主题。通过从原子操作和表示类型的视角重新审视记忆系统，这篇综述为人工智能中与记忆相关的科研、基准数据集和工具提供了结构化和动态的视角，阐明了基于LLMs的智能体中功能交互的细节，并概述了未来研究的有利方向[论文列表、数据集、方法和工具可在\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}获取]。|
|**2025-05-01**|**DeepCritic: Deliberate Critique with Large Language Models**|Wenkai Yang et.al.|[2505.00662](http://arxiv.org/abs/2505.00662)|**[link](https://github.com/rucbm/deepcritic)**|随着大型语言模型（LLMs）的快速发展，对其输出提供准确反馈和可扩展的监管成为一个紧迫且关键的问题。利用LLMs作为批判模型以实现自动化监管是一种有希望的解决方案。在本工作中，我们专注于研究和提升LLMs的数学批判能力。当前的LLM批判者提供的批判过于肤浅和表面化，导致判断准确性低，难以提供足够的反馈以帮助LLM生成器纠正错误。为了解决这一问题，我们提出了一种新颖且有效的两阶段框架，以开发能够故意批判数学解决方案每个推理步骤的LLM批判者。在第一阶段，我们利用Qwen2.5-72B-Instruct生成4.5K长篇批判作为监督微调的种子数据。每个种子批判包括精心设计的逐步批判，包括多角度验证以及每个推理步骤的初始批判的深入批判。然后，我们使用来自PRM800K的现有人类标注数据或通过基于蒙特卡洛抽样的正确性估计自动标注的数据对微调后的模型进行强化学习，以进一步激励其批判能力。我们基于Qwen2.5-7B-Instruct开发的批判模型不仅在各种错误识别基准上显著优于现有的LLM批判者（包括同规模的DeepSeek-R1-distill模型和GPT-4o），而且通过更详细的反馈更有效地帮助LLM生成器细化错误步骤。|
|**2025-05-01**|**On the generalization of language models from in-context learning and finetuning: a controlled study**|Andrew K. Lampinen et.al.|[2505.00661](http://arxiv.org/abs/2505.00661)|null|大型语言模型展现出令人兴奋的能力，但它们在从微调中泛化的表现却出人意料地有限——从无法泛化到它们训练时接触到的简单关系反转，到遗漏可以从训练信息中得出的逻辑推理。这些从微调中泛化的失败可能会阻碍这些模型的实际应用。然而，语言模型的上下文学习显示出不同的归纳偏见，并且在某些情况下可以更好地泛化。在这里，我们探讨了基于上下文学习和基于微调的学习之间泛化差异。为了做到这一点，我们构建了几个新颖的数据集来评估和改进模型从微调数据泛化的能力。这些数据集的构建目的是将数据集中的知识从预训练知识中分离出来，以创建泛化的纯净测试。我们将预训练的大型模型暴露于这些数据集信息的受控子集——要么在上下文中，要么通过微调——并评估它们在需要各种类型泛化的测试集上的表现。我们发现，在数据匹配的设置中，上下文学习比微调更灵活地泛化（尽管我们也发现了一些先前发现的条件，例如微调可以泛化到嵌入在更大知识结构中的反转）。基于这些发现，我们提出了一种方法来提高从微调中泛化的能力：向微调数据添加上下文推理。我们表明，这种方法提高了我们数据集分割和其他基准测试的泛化能力。我们的结果对理解语言模型不同学习模式的归纳偏见以及实际改进它们的性能具有重要意义。|
|**2025-05-01**|**Large Language Models Understanding: an Inherent Ambiguity Barrier**|Daniel N. Nissani et.al.|[2505.00654](http://arxiv.org/abs/2505.00654)|null|随着大型语言模型（LLMs）在理解世界和捕捉它们参与对话的意义方面的非凡出现，一场热烈的争论正在进行。人们基于思想实验、LLMs与人类之间的轶事性对话、统计分析、哲学思考等提出了论点和反论点。在这篇简短的论文中，我们提出了一种基于思想实验和半正式考虑的反论点，该论点导致了一个固有的歧义障碍，阻止LLMs对其令人惊叹的流畅对话的意义有任何理解。|
|**2025-05-01**|**Open-Source LLM-Driven Federated Transformer for Predictive IoV Management**|Yazan Otoum et.al.|[2505.00651](http://arxiv.org/abs/2505.00651)|null|在物联网（IoV）生态系统中，连接车辆的大量涌现给确保可扩展、实时和隐私保护的交通管理带来了关键挑战。现有的集中式IoV解决方案通常存在高延迟、可扩展性有限以及对专有人工智能（AI）模型的依赖，这在动态和隐私敏感的环境中造成了巨大的部署障碍。同时，在车载系统中集成大型语言模型（LLMs）仍处于探索阶段，尤其是在提示优化和联邦环境中的有效利用方面。为了解决这些挑战，我们提出了联邦提示优化交通转换器（FPoTT），这是一个新颖的框架，它利用开源LLMs进行预测性IoV管理。FPoTT引入了一种动态提示优化机制，通过迭代优化文本提示来提高轨迹预测。该架构采用双层联邦学习范式，结合轻量级边缘模型进行实时推理，以及云端的LLMs以保持全局智能。还纳入了一个由Transformer驱动的合成数据生成器，以通过NGSIM格式的多样化、高保真交通场景来增强训练。广泛的评估表明，FPoTT利用EleutherAI Pythia-1B，在真实世界数据上实现了99.86%的预测精度，同时在合成数据集上保持了高性能。这些结果突显了开源LLMs在实现安全、自适应和可扩展的IoV管理方面的潜力，为智能移动生态系统中的专有解决方案提供了有希望的替代方案。|
|**2025-05-01**|**Investigating Task Arithmetic for Zero-Shot Information Retrieval**|Marco Braga et.al.|[2505.00649](http://arxiv.org/abs/2505.00649)|**[link](https://github.com/detectivemb/task-arithmetic-for-zs-ir)**|大型语言模型（LLMs）在各种自然语言处理任务中，包括文档重排序，展现出了令人印象深刻的零样本性能。然而，在未见过的任务和领域中，它们的有效性会下降，这主要是因为词汇和词分布的变化。在本文中，我们研究了任务算术，这是一种通过简单的数学运算，如加法或减法，结合不同任务或领域上预训练的LLMs的权重，以无需额外微调来调整检索模型的技术。我们的方法能够将多种任务和领域知识合成到单个模型中，使不同检索环境中的零样本适应变得有效。在公开的科学、生物医学和双语数据集上进行的广泛实验表明，我们的方法将NDCG@10和P@10的检索性能提高了高达18%和15%。除了这些经验收益外，我们的分析还提供了关于任务算术作为零样本学习和模型适应实用策略的优缺点见解。我们将我们的代码公开提供在https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR上。|
|**2025-05-01**|**The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)**|Zihao Wang et.al.|[2505.00626](http://arxiv.org/abs/2505.00626)|null|大型语言模型（LLMs）在实践中越来越多地集成了多个输入角色（例如，系统指令、用户查询、外部工具输出）。确保模型能够准确地区分每个角色的信息——我们称之为“角色分离”——对于保持一致的多角色行为至关重要。尽管最近的研究通常针对最先进的提示注入防御措施，但这种方法是否真正教会LLMs区分角色，或者只是简单地记忆已知的触发器，仍然不清楚。在本文中，我们考察了“角色分离学习”：教会LLMs稳健地区分系统和用户标记的过程。通过一个简单、可控的实验框架，我们发现微调后的模型通常依赖于两个角色识别的代理：（1）任务类型利用，和（2）与文本开头的接近程度。尽管数据增强可以在一定程度上缓解这些捷径，但它通常导致迭代修补而不是更深入的修复。为了解决这个问题，我们提出通过调整模型输入编码中的标记级提示来加强标记角色边界的“不变信号”。特别是，操纵位置ID有助于模型学习更清晰的区分，并减少对表面代理的依赖。通过关注这种以机制为中心的视角，我们的工作阐明了LLMs如何在没有仅仅记忆已知提示或触发器的情况下，更可靠地保持一致的多角色行为。|
|**2025-05-01**|**FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation**|Chaitali Bhattacharyya et.al.|[2505.00624](http://arxiv.org/abs/2505.00624)|null|从头开始训练大型语言模型（LLMs）需要大量的计算资源，这促使人们开发更小、特定领域的LLMs，以保持效率和强大的任务性能。中等规模的模型如LLaMA、llama等已成为特定领域自适应的起点，但它们在专业数据集上测试时往往会出现准确性下降。我们引入了FineScope，这是一个从大型预训练模型中衍生出紧凑、领域优化的LLMs的框架。FineScope借鉴了稀疏自编码器（SAE）框架，受其生成可解释特征表示的能力启发，从大型数据集中提取特定领域的子集。我们应用具有领域特定约束的结构化剪枝，确保结果剪枝模型保留了针对目标领域的基本知识。为了进一步提高性能，这些剪枝模型经历了自我数据蒸馏，利用SAE整理的数据集来恢复剪枝过程中丢失的关键领域特定信息。大量的实验和消融研究表明，FineScope实现了高度竞争力的性能，在特定领域任务中优于几个大规模最先进的LLMs。此外，我们的结果表明，FineScope使剪枝模型在用SAE整理的数据集微调时能够恢复大部分原始性能。此外，将这些数据集应用于未经剪枝的预训练LLMs的微调，也提高了它们的领域特定准确性，突显了我们方法的鲁棒性。代码将予以发布。|
|**2025-04-30**|**TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments**|Sichang Tu et.al.|[2504.21851](http://arxiv.org/abs/2504.21851)|null|目标：尽管大型语言模型（LLM）已被广泛用于协助临床医生和支持患者，但现有的研究尚未探索用于标准诊断访谈和评估的对话系统。本研究旨在通过开发一个能够模拟临床医生行为的LLM驱动的对话系统，弥合心理健康服务可及性的差距。材料和方法：我们介绍了TRUST，一个能够进行创伤后应激障碍（PTSD）的正式诊断访谈和评估的协作LLM模块框架。为了指导生成适当的临床回应，我们提出了一种专门为临床访谈设计的对话行为方案。此外，我们开发了一种基于真实访谈记录的患者模拟方法，以替代临床医生耗时且昂贵的手动测试。结果：设计了一套全面的评估指标，从代理和患者模拟的角度评估对话系统。通过对话和临床专家的专业评估显示，TRUST的表现与现实生活中临床访谈相当。讨论：我们的系统在平均临床医生的水平上运行，未来在沟通方式和回应适宜性方面仍有改进空间。结论：我们的TRUST框架显示出促进心理健康服务可及性的潜力。|
|**2025-04-30**|**COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning**|Xindi Wu et.al.|[2504.21850](http://arxiv.org/abs/2504.21850)|null|多模态大型语言模型（MLLMs）在简单的视觉-语言任务上表现出色，但在需要多种能力同时识别对象、计数以及理解它们的空间关系等复杂任务上则面临挑战。这可能部分是由于视觉指令微调（VIT），这是MLLMs关键训练步骤，传统上只关注扩展数据量，而没有关注训练示例的组合复杂性。我们提出了COMPACT（COMPositional Atomic-to-complex visual Capability Tuning），它生成一个训练数据集，显式地控制训练示例的组合复杂性。COMPACT的数据允许MLLMs通过组合原子能力来更有效地学习复杂能力。在所有基准测试中，COMPACT的性能与LLaVA-665k VIT相当，而使用的数据预算不到其10%，甚至在多个任务上，尤其是涉及复杂多能力任务的那些任务上，表现优于LLaVA-665k VIT。例如，在需要四个或更多原子能力特别复杂的特定问题中，与全规模VIT相比，COMPACT在MMStar上实现了83.3%的显著提升，在MM-Vet上实现了94.0%的提升。COMPACT提供了一种可扩展、数据高效的视觉组合调优方法，以改善复杂视觉-语言任务。|
|**2025-04-30**|**An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding**|Xiuwei Shang et.al.|[2504.21803](http://arxiv.org/abs/2504.21803)|null|二进制代码分析在软件安全领域发挥着关键作用，广泛应用于软件维护、恶意软件检测、软件漏洞发现、补丁分析等任务。然而，与源代码不同，由于缺乏直观的语义信息，逆向工程师在理解二进制代码时面临重大挑战。尽管传统逆向工具可以将二进制代码转换为类似C语言的伪代码，但缺乏代码注释和诸如函数名之类的符号信息仍然使得代码理解变得困难。近年来，两组技术显示出良好的前景：（1）基于深度学习的技术在涉及二进制代码理解的任务中表现出竞争力；（2）大型语言模型（LLMs）已经在源代码级别进行了广泛的预训练，用于代码理解和生成等任务。这使得参与者对LLMs在二进制代码理解方面的能力产生了疑问。为此，本研究提出一个基准来评估LLMs在现实世界逆向工程场景中的有效性，该基准涵盖了两个关键的二进制代码理解任务，即函数名恢复和二进制代码摘要。为了更全面地评估，我们包括了具有多个目标架构以及不同优化选项的二进制文件。通过使用我们的基准对流行的LLMs进行广泛的实证研究，我们获得了宝贵的能力和局限性见解。我们的评估揭示，现有的LLMs可以在一定程度上理解二进制代码，从而提高二进制代码分析的效率。我们的结果突显了LLMs在推进二进制代码理解领域中的巨大潜力，并为二进制代码分析技术提供了新的方向。|
|**2025-04-30**|**DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition**|Z. Z. Ren et.al.|[2504.21801](http://arxiv.org/abs/2504.21801)|**[link](https://github.com/deepseek-ai/deepseek-prover-v2)**|我们介绍了DeepSeek-Prover-V2，这是一个开源的大语言模型，专为在Lean 4中进行形式化定理证明而设计，其初始化数据是通过由DeepSeek-V3驱动的递归定理证明管道收集的。冷启动训练过程首先提示DeepSeek-V3将复杂问题分解成一系列子目标。已解决的子目标的证明被合成成一个思维链过程，结合DeepSeek-V3的逐步推理，为强化学习创建一个初始冷启动。这个过程使我们能够将非正式和形式化的数学推理整合到一个统一模型中。所得到的模型DeepSeek-Prover-V2-671B在神经定理证明中达到了最先进的性能，MiniF2F-test的通过率达到了88.9%，并解决了PutnamBench中的49个问题中的658个。除了标准基准测试外，我们还引入了ProverBench，这是一个包含325个形式化问题的集合，以丰富我们的评估，其中包括从最近AIME竞赛（第24-25年）中选出的15个问题。对这些15个AIME问题的进一步评估显示，该模型成功解决了其中的6个。相比之下，DeepSeek-V3通过多数投票解决了这些问题中的8个，这突显出大型语言模型中形式化与非形式化数学推理之间的差距正在显著缩小。|
|**2025-04-30**|**MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness**|Junsheng Huang et.al.|[2504.21773](http://arxiv.org/abs/2504.21773)|null|随着大型语言模型（LLMs）的广泛应用，生成不存在事实的问题，即所谓的幻觉，受到了越来越多的关注。先前的研究主要集中于单问题设置下增强LLM置信度估计。然而，在更具有挑战性的多问题设置下，即需要同时准确回答多个问题的情况下，LLM对其内部参数化知识边界的意识尚处于未充分探索的状态。为了填补这一空白，我们提出了一种新的方法，即多答案和置信度逐步调整（MAC-Tuning），该方法在指令数据微调过程中将答案预测和置信度估计的学习过程分离。大量实验表明，我们的方法在平均精确度方面优于基线，提升了高达25%。|
|**2025-04-30**|**LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL Bugs**|Baleegh Ahmad et.al.|[2504.21770](http://arxiv.org/abs/2504.21770)|null|在检测早期硬件安全漏洞方面，静态分析非常有用，但其有效性有限，因为它需要信息来形成检查，并且通常无法解释检测到的漏洞的安全影响。大型语言模型可以用来填补这些空白，通过识别相关资产、移除静态分析工具标记的错误违规，并解释报告的违规。LASHED结合了两种方法（LLMs和静态分析）来克服各自在硬件安全漏洞检测中的局限性。我们将在四个开源SoC上针对五个常见漏洞枚举（CWEs）对我们的方法进行研究，并展示通过更好的提示工程进行改进的策略。我们发现，我们推荐方案标记的实例中有87.5%是合理的CWEs。上下文学习和要求模型“重新思考”可以提高LASHED的精确度。|
|**2025-04-30**|**LLM-based Interactive Imitation Learning for Robotic Manipulation**|Jonas Werner et.al.|[2504.21769](http://arxiv.org/abs/2504.21769)|**[link](https://github.com/tubicor/llm-iteach)**|最近在机器学习领域的进步为训练能够处理机器人中不断增长的序列决策复杂性的自主代理提供了方法。模仿学习（IL）是一种显著的方法，其中代理通过人类的示范来学习控制机器人。然而，由于在机器人任务中违反了独立同分布（i.i.d）的假设，IL通常存在显著问题。交互式模仿学习（IIL）通过允许代理从人类教师的交互式反馈中学习而提高了性能。尽管取得了这些改进，但由于需要人类参与，这两种方法都带来了重大成本。利用大型语言模型（LLMs）在推理和生成类似人类响应的涌现能力，我们引入了LLM-iTeach——一个利用LLM作为交互式教师以增强代理性能同时减轻对人力资源依赖的新的IIL框架。首先，LLM-iTeach使用分层提示策略，指导LLM生成Python代码中的策略。然后，通过设计基于相似度的反馈机制，LLM-iTeach在代理训练过程中提供交互式的纠正和评估反馈。我们使用各种机器人操作任务，将LLM-iTeach与基线方法（如行为复制（BC），一种IL方法，以及CEILing，一种使用人类教师的最先进的IIL方法）进行比较。我们的结果表明，LLM-iTeach在成功率上超越了BC，并达到了甚至超过了CEILing的水平，突显了LLMs作为低成本、类似人类的教师在交互式学习环境中的潜力。我们进一步通过在额外的任务上评估该方法来展示其泛化潜力。代码和提示可在以下链接获得：https://github.com/Tubicor/LLM-iTeach。|
|**2025-04-30**|**Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models**|Emelie Hallenberg et.al.|[2504.21742](http://arxiv.org/abs/2504.21742)|null|古希腊虚构叙事，通常被称为爱情小说或浪漫小说，从公元1世纪到15世纪中叶，长期以来在许多方面被认为相似，尤其在特定文学主题的使用上。通过应用精细调整的大型语言模型，本研究旨在调查该语料库中的文本具体有哪些共同的主题，以及它们彼此之间有何不同。结果显示，尽管有些主题在整个语料库中持续存在，但其他主题的频率有所波动，这表明了某些趋势或外部影响。最终，该方法证明能够根据既定定义充分提取文学主题，为定量和定性分析提供数据。|
|**2025-04-30**|**TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy Training**|Shengqian Wang et.al.|[2504.21735](http://arxiv.org/abs/2504.21735)|null|按摩治疗培训强调实操技术和有效的治疗师-患者沟通。然而，许多教育项目难以提供逼真的实践场景。为了解决这个问题，我们提出了TheraQuest，这是一个基于网页的、游戏化的模拟平台，它利用大型语言模型（LLM）生成具有不同症状和文化背景的虚拟患者。通过互动对话、解剖决策和即时评估，学员在低风险环境下培养诊断推理和同理心沟通技巧。与仅基于VR的解决方案不同，TheraQuest可以通过标准网页浏览器访问，从而减轻了长时间使用头戴设备的成本和不适。初步测试表明，将LLM驱动的虚拟患者与实时技能指标相结合，可以提高学员的参与度，并有助于弥合理论与实践知识之间的差距。|
|**2025-04-30**|**XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs**|Marco Arazzi et.al.|[2504.21700](http://arxiv.org/abs/2504.21700)|null|大型语言模型是现代IT领域中由AI解决方案主导的基石。然而，与之相关的安全威胁可能会阻碍它们在政府组织、医疗机构等关键应用场景中的可靠应用。因此，商业化的LLM通常经过复杂的审查机制，以消除可能产生的任何有害输出。针对这一情况，LLM越狱是一个重大的威胁，许多先前的方法已经在多个领域证明了其有效性。现有的越狱方案大多采用生成和测试策略来构建恶意输入。为了提高对审查机制的理解并设计针对性的越狱攻击，我们提出了一种可解释人工智能解决方案，该方案通过比较审查和不审查模型的运行行为，推导出独特的可利用的对齐模式。然后，我们提出了XBreaking，一种新型的越狱攻击，通过有针对性的噪声注入利用这些独特的模式来突破LLM的安全约束。我们的全面实验活动提供了关于审查机制的重要见解，并证明了我们攻击的有效性和性能。|
|**2025-04-29**|**Toward Efficient Exploration by Large Language Model Agents**|Dilip Arumugam et.al.|[2504.20997](http://arxiv.org/abs/2504.20997)|null|强化学习（RL）领域内一个迅速发展的研究方向是围绕大型语言模型（LLMs）设计的序列决策智能体。虽然由现代LLMs驱动的自主决策智能体可以促进众多现实应用，但这些成功需要能够实现数据高效RL的智能体。在RL中实现数据效率的一个关键障碍是探索，我们证明许多最近提出的LLM智能体设计方案都难以应对这一挑战。同时，RL文献中经典的、能够优雅地解决探索问题的算法，需要的技术工具在纯自然语言环境中可能难以操作。在这项工作中，我们不是依赖于微调或上下文学习来诱导LLMs隐式模仿RL算法，而是展示了如何使用LLMs明确实现一个现有的RL算法（后验采样强化学习），该算法在统计高效探索方面的能力已经被充分研究。我们提供了实证结果，证明了我们基于LLM的已知数据高效RL算法实现，在需要谨慎探索的自然语言任务中可以显著更有效。|
|**2025-04-29**|**X-Fusion: Introducing New Modality to Frozen Large Language Models**|Sicheng Mo et.al.|[2504.20996](http://arxiv.org/abs/2504.20996)|null|我们提出了一种名为X-Fusion的框架，该框架扩展了预训练的大型语言模型（LLMs）以用于多模态任务，同时保留了它们的语言能力。X-Fusion采用双塔设计，具有模态特定的权重，在冻结LLM参数的同时，整合了视觉特定的信息以用于理解和生成。我们的实验表明，X-Fusion在图像到文本和文本到图像任务上均持续优于其他替代架构。我们发现，引入以理解为导向的数据可以提升生成质量，减少图像数据噪声可以增强整体性能，而特征对齐可以加速较小模型的收敛，但对较大模型的影响最小。我们的发现为构建高效的统一多模态模型提供了有价值的见解。|
|**2025-04-29**|**ACE: A Security Architecture for LLM-Integrated App Systems**|Evan Li et.al.|[2504.20984](http://arxiv.org/abs/2504.20984)|null|LLM集成应用程序系统通过系统LLM调用第三方应用程序，利用交错规划与执行阶段来回答用户查询，从而扩展了大型语言模型（LLM）的实用性。这些系统引入了新的攻击向量，恶意应用程序可能导致规划或执行的完整性破坏、可用性崩溃或执行过程中的隐私泄露。在本工作中，我们确定了影响LLM集成应用程序规划完整性的新攻击，以及执行完整性和可用性的攻击，并在针对IsolateGPT（一种旨在减轻恶意应用程序攻击的最新解决方案）的实验中展示了这些攻击。我们提出了抽象-具体-执行（ACE）架构，这是一种新的安全架构，为LLM集成应用程序系统的规划和执行提供安全保证。具体而言，ACE通过首先使用仅包含可信信息的抽象执行计划，然后利用已安装的系统应用程序将该抽象计划映射到具体计划，将规划分解为两个阶段。我们通过在结构化计划输出上进行静态分析验证，确保由我们的系统生成的计划满足用户指定的安全信息流约束。在执行过程中，ACE在应用程序之间强制执行数据和能力屏障，并确保执行符合可信的抽象计划。实验表明，我们的系统对INJECAGENT基准测试中的攻击是安全的，INJECAGENT基准测试是针对间接提示注入攻击的控件流完整性的标准基准，以及我们新引入的攻击。我们的架构代表了朝着加强包含不同可信级别系统设施基于LLM的系统迈出的重大进步。|
|**2025-04-29**|**Real-Time Wayfinding Assistant for Blind and Low-Vision Users**|Dabbrata Das et.al.|[2504.20976](http://arxiv.org/abs/2504.20976)|null|在为盲人或视力受限者（BLV）导航不熟悉的地点继续是他们日常生活中最持久和最基本的一大障碍。现有的辅助技术，如基于GPS的导航系统、人工智能驱动的智能眼镜和配备声纳的拐杖，在实时避障、精确定位和对动态环境适应性方面往往存在局限性。为了研究潜在的解决方案，我们引入了PathFinder，这是一种新颖的无地图导航系统，它探索了理解2D图像的不同模型，包括视觉语言模型（VLMs）、大型语言模型（LLMs），并采用单目深度估计进行自由路径检测。我们的方法在深度图像上整合了深度优先搜索（DFS）算法，以确定最长无障碍路径，确保最优路线选择的同时保持计算效率。我们对现有的基于人工智能的导航方法进行了比较评估，并与BLV参与者进行了可用性研究。结果表明，PathFinder在准确性、计算效率和实时响应性之间实现了良好的平衡。值得注意的是，与基于人工智能的替代方案相比，它在户外导航中降低了平均绝对误差（MAE）并提高了决策速度。参与者反馈强调了该系统在外部情况下的可用性和有效性，但也指出了在复杂室内位置和低光条件下的问题。可用性测试显示，73%的参与者在大约一分钟内就明白了如何使用该应用，80%的人称赞了其准确性、快速响应和整体便利性的平衡。|
|**2025-04-29**|**SetKE: Knowledge Editing for Knowledge Elements Overlap**|Yifan Wei et.al.|[2504.20972](http://arxiv.org/abs/2504.20972)|null|大型语言模型（LLMs）在检索和问答等任务上表现出色，但需要更新以融入新知识和减少不精确和幻觉。传统的更新方法，如微调和增量学习，面临过拟合和高计算成本等挑战。知识编辑（KE）提供了一种有希望的替代方案，但往往忽视了知识元素重叠（KEO）现象，即多个三元组共享共同元素，导致编辑冲突。我们确定了现有KE数据集中KEO的普遍性，并展示了其对当前KE方法的重大影响，导致处理此类三元组的性能下降。为了解决这个问题，我们提出了一种新的公式，即知识集编辑（KSE），并引入了SetKE，这是一种同时编辑三元组集的方法。实验结果表明，在KEO场景下，SetKE在主流LLMs上优于现有方法。此外，我们还引入了EditSet，这是一个包含KEO三元组的数据集，提供了一个全面的基准。|
|**2025-04-29**|**OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification**|Shangyu Li et.al.|[2504.20964](http://arxiv.org/abs/2504.20964)|**[link](https://github.com/lishangyu-hkust/osvbench)**|我们引入了OSVBench，这是一个用于评估大型语言模型（LLMs）在生成与操作系统内核验证任务相关的完整规范代码的新基准。该基准首先通过提供编程模型，将规范生成问题定义为一个具有有限语法和语义范围的程序综合问题。LLMs需要理解提供的验证假设和潜在语法和语义空间，然后在操作系统的高级功能描述的指导下，生成可能存在错误的操作系统代码实现的完整规范。该基准建立在真实的操作系统内核Hyperkernel之上，总共包含245个复杂的规范生成任务，每个任务都是大约20k-30k个标记的长文本任务。我们对12个LLMs的综合评估表明，当前LLMs在操作系统验证的规范生成任务上的性能有限。它们在基准测试中的显著差异凸显了它们处理长文本代码生成任务的能力差异。评估工具包和基准可在https://github.com/lishangyu-hkust/OSVBench上找到。|
|**2025-04-29**|**Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models**|Maryna Vyshnyvetska et.al.|[2504.20951](http://arxiv.org/abs/2504.20951)|null|我们提出了一种名为“信息引力”的理论模型，用于描述大型语言模型（LLMs）中的文本生成过程。该模型利用场理论和时空几何中的物理装置来形式化用户查询与生成标记的概率分布之间的相互作用。查询被视为具有“信息质量”的对象，它弯曲模型的语义空间，形成“吸引”生成过程中标记的重力势阱。该模型提供了一种机制来解释LLM行为中观察到的几种现象，包括幻觉（源自低密度语义空洞）、对查询表述的敏感性（由于语义场曲率变化）以及采样温度对输出多样性的影响。|
|**2025-04-29**|**Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models**|Tyler McDonald et.al.|[2504.20946](http://arxiv.org/abs/2504.20946)|null|随着大型语言模型（LLMs）在日常工作中的应用不断扩展，提示工程仍然是计算语言学中的一个活跃的研究领域，尤其是在需要专门知识如算术推理的领域。虽然这些LLMs被优化用于各种任务，但它们的全面使用可能对小型团队来说在计算或财务上变得繁重。此外，完全依赖专有、封闭源代码的模型通常会限制定制和适应性，对研究和应用的扩展性造成重大挑战。相反，通过利用参数数量在70亿以下的开源模型，我们可以在资源使用上进行优化，同时仍然在标准提示方法之上观察到显著的收益。为了培养这一理念，我们引入了思维轨迹提示，这是一种简单、零样本的提示工程方法，指导LLMs使用关键问题解决方法创建可观察的子问题，专门设计用于增强算术推理能力。当将这种方法应用于与GPT-4结合的开源模型时，我们发现思维轨迹不仅允许对问题解决过程有新的洞察，而且在参数数量在70亿以下的语言模型上引入了高达125%的性能提升。这种方法强调了开源倡议在民主化人工智能研究和提高高质量计算语言学应用可及性方面的潜力。|
|**2025-04-29**|**ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification**|Ziqing Fan et.al.|[2504.20930](http://arxiv.org/abs/2504.20930)|**[link](https://github.com/MAGIC-AI4Med/ChestX-Reasoner)**|近期，推理增强的大型语言模型（LLMs）和多模态LLMs（MLLMs）在复杂任务中的性能得到了显著提升，然而，医疗AI模型往往忽略了临床实践中固有的结构化推理过程。在本研究中，我们提出了ChestX-Reasoner，这是一种旨在利用从临床报告中直接挖掘的过程监督的放射学诊断MLLM，反映了放射科医生逐步推理的过程。我们通过从常规放射学报告中提取和精炼推理链构建了一个大型数据集。我们的两阶段训练框架结合了监督微调和由过程奖励引导的强化学习，以更好地使模型推理与临床标准相一致。我们引入了RadRBench-CXR，这是一个包含59K视觉问答样本和301K临床验证推理步骤的全面基准，并提出了RadRScore，这是一个评估推理事实性、完整性和有效性的指标。与最佳医疗MLLM、最佳通用MLLM及其基础模型相比，ChestX-Reasoner在诊断准确性和推理能力方面均优于现有的医疗和通用领域MLLM，分别提高了推理能力16%、5.9%和18%，以及结果准确性3.3%、24%和27%。所有资源均已开源，以促进医疗推理MLLM的进一步研究。|
|**2025-04-29**|**An Empirical Study on the Capability of LLMs in Decomposing Bug Reports**|Zhiyuan Chen et.al.|[2504.20911](http://arxiv.org/abs/2504.20911)|null|背景：缺陷报告对于软件开发周期至关重要。它们有助于开发者追踪和解决问题，但由于其复杂性，处理起来通常很困难，这可能会延迟问题的解决并影响软件质量。  目的：本研究调查大型语言模型（LLMs）是否能够帮助开发者自动将复杂的缺陷报告分解成更小、自包含的单元，使其更容易理解和解决。  方法：我们对从Apache Jira收集的127个已解决的隐私相关缺陷报告进行了实证研究。我们使用不同的提示策略评估了ChatGPT和DeepSeek。我们首先使用零样本提示测试了这两个LLMs，然后应用改进的带有演示的提示（使用少量样本提示）来衡量它们在缺陷分解方面的能力。  结果：我们的研究结果表明，LLMs能够分解缺陷报告，但它们的整体性能仍需进一步改进，并且强烈依赖于提示的质量。使用零样本提示时，所研究的两个LLMs（ChatGPT和DeepSeek）的表现都很差。经过提示调整后，ChatGPT的真实分解率提高了140%，而DeepSeek提高了163.64%。  结论：LLMs在帮助开发者分析和分解复杂缺陷报告方面显示出潜力，但在准确性和缺陷理解方面仍需改进。|
|**2025-04-28**|**AutoJudge: Judge Decoding Without Manual Annotation**|Roman Garipov et.al.|[2504.20039](http://arxiv.org/abs/2504.20039)|null|我们介绍了AutoJudge，这是一个通过特定任务的损失性推测解码来加速大型语言模型（LLM）推理的框架。我们不再逐个匹配原始模型的输出分布，而是确定哪些生成的标记会影响生成响应的下游质量，放宽保证，以便“不重要”的标记可以更快地生成。我们的方法依赖于半贪婪搜索算法来测试哪些目标模型和草案模型之间的不匹配应该被纠正以保持质量，哪些可以跳过。然后，我们基于现有的LLM嵌入训练了一个轻量级的分类器，在推理时预测哪些不匹配的标记可以安全接受而不损害最终答案的质量。我们在零样本GSM8K推理上对Llama 3.2 1B（草案）和Llama 3.1 8B（目标）模型进行了测试，与标准推测解码相比，它每个验证周期实现了高达1.5倍的接受标记数，且答案准确率降低不到1%，并且与标准推测解码相比，在准确率略有下降的情况下实现了超过2倍的速度提升。当应用于LiveCodeBench基准时，我们的方法自动检测其他编程特定的重要标记，并显示出类似的速度提升，证明了其能够跨任务泛化。|
|**2025-04-28**|**SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning**|Wufei Ma et.al.|[2504.20024](http://arxiv.org/abs/2504.20024)|null|近期关于3D空间推理的研究探讨了数据驱动方法，并利用强化学习（RL）实现了空间推理性能的提升。然而，这些方法通常以隐式的方式进行空间推理，而且关于所获得的三维知识是否能在训练的任何阶段推广到未见过的问答类型，仍处于研究不足的状态。在这项工作中，我们引入了SpatialReasoner，这是一个新颖的大视觉语言模型（LVLM），它通过在3D感知、计算和推理阶段之间共享显式三维表示来解决3D空间推理问题。显式三维表示提供了一个连贯的接口，支持高级3D空间推理，并使我们能够研究LVLMs所犯的事实错误。结果显示，我们的SpatialReasoner在各种空间推理基准测试中实现了改进的性能，并且在评估新颖的3D空间推理问题时表现出更好的泛化能力。我们的研究将先前视觉基础模型的三维解析能力与大型语言模型的强大推理能力相结合，为3D空间推理开辟了新的方向。|
|**2025-04-28**|**Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages**|Pritika Rohera et.al.|[2504.20022](http://arxiv.org/abs/2504.20022)|null|多语言大型语言模型（LLMs）在各种语言中表现出显著的有效性，尤其是在英语等资源丰富的语言中。然而，它们在其他低资源语言，尤其是印度语系语言中的事实准确性仍然是研究的一个领域。在这项研究中，我们通过使用包含英语和19种印度语系语言的问答对数据集IndicQuest，比较了LLMs（GPT-4o、Gemma-2-9B、Gemma-2-2B和Llama-3.1-8B）在英语和印度语系语言中的表现，来评估这些模型的事实准确性。通过用英语和它们各自的印度语系翻译提出相同的问题，我们分析了这些模型在印度语系语言中的区域性问题或英语操作中的可靠性。我们的发现表明，LLMs在英语中的表现通常更好，即使对于源于印度语系语境的问题也是如此。值得注意的是，我们在低资源印度语系语言生成的回答中观察到更高的幻觉倾向，这突显了当前LLMs在多语言理解能力方面的挑战。|
|**2025-04-28**|**Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models**|Xin Wang et.al.|[2504.20020](http://arxiv.org/abs/2504.20020)|null|大型语言模型（LLMs）在自然语言处理、计算机视觉、数据挖掘等领域推动了机器学习研究的发展，但它们在推理、事实一致性和可解释性方面仍存在关键局限性。在本文中，我们提出了一种新颖的学习范式——模块化机器学习（MML），将其作为新一代LLMs的关键方法。MML将LLMs的复杂结构分解为三个相互依赖的组件：模块化表示、模块化模型和模块化推理，旨在提升LLMs的反事实推理能力、减轻幻觉，并促进公平性、安全性和透明度。具体来说，所提出的MML范式可以实现以下功能：i）通过解耦语义组件来阐明LLMs的内部工作机制；ii）允许灵活和任务自适应的模型设计；iii）实现可解释和逻辑驱动的决策过程。我们通过利用解耦表示学习、神经架构搜索和神经符号学习等先进技术，提出了一种基于MML的LLMs的可行实现方案。我们批判性地识别了关键挑战，如连续神经和离散符号过程的集成、联合优化和计算可扩展性，并提出了值得进一步探索的有希望的未来的研究方向。最终，MML范式与LLMs的结合有望弥合统计（深度）学习与形式（逻辑）推理之间的差距，从而为广泛实际应用中的稳健、适应性强和值得信赖的AI系统铺平道路。|
|**2025-04-28**|**LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation**|Beizhe Hu et.al.|[2504.20013](http://arxiv.org/abs/2504.20013)|null|在线虚假新闻的审查现在面临着由恶意使用大型语言模型（LLMs）进行虚假新闻生产带来的新挑战。尽管现有研究已经表明，LLM生成的虚假新闻从单个方面来看很难检测，但其大规模发布对新闻生态系统的影响仍然没有得到充分研究。在本研究中，我们开发了一个模拟流程和包含约56k条不同类型生成新闻的数据集，以调查LLM生成的虚假新闻在神经新闻推荐系统中的影响。我们的研究发现了一种“真相衰减”现象，即随着LLM生成的新闻参与新闻推荐，真实新闻在新闻排名中逐渐失去对虚假新闻的优势地位。我们进一步从熟悉度的角度解释了为什么会出现真相衰减，并展示了困惑度与新闻排名之间的正相关关系。最后，我们讨论了LLM生成的虚假新闻的威胁，并提出了可能的应对措施。我们敦促利益相关者应对这一新兴挑战，以维护新闻生态系统的完整性。|
|**2025-04-28**|**Towards Automated Scoping of AI for Social Good Projects**|Jacob Emmerson et.al.|[2504.20010](http://arxiv.org/abs/2504.20010)|null|人工智能用于社会公益（AI4SG）是一个新兴的努力，旨在利用人工智能系统的强大能力解决复杂的社会挑战。这些挑战范围从当地的交通网络问题到全球野生动物保护。然而，无论规模如何，许多AI4SG项目的关键瓶颈是问题界定过程的繁琐——这是一个复杂且资源密集的任务——由于缺乏既具有技术又具有领域专业知识的专业人士。鉴于大型语言模型（LLM）的显著应用，我们提出了一种问题界定代理（PSA），该代理使用LLM生成基于科学文献和现实世界知识的全面项目提案。我们通过盲审和人工智能评估证明了我们的PSA框架生成的提案与专家撰写的提案相当。最后，我们记录了现实世界问题界定的挑战，并指出几个未来工作的领域。|
|**2025-04-28**|**Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom**|Rishika Sen et.al.|[2504.20000](http://arxiv.org/abs/2504.20000)|null|知识蒸馏（KD）是减少大型语言模型（LLMs）大小的一种方法。一个具有较少模型参数（学生）的LLM被训练来模拟在特定任务上，一个较大尺寸的LLM（教师模型）的性能。对于特定领域的任务，不清楚在领域自适应中是否必须考虑教师模型、学生模型或两者。在这项工作中，我们从电信领域问答（QA）任务的视角研究这个问题。我们系统地实验了仅在教师模型上进行的监督微调（SFT）、仅在学生模型上进行的SFT以及KD之前的SFT。我们设计了实验来研究词汇（相同和不同）和KD算法（普通KD和双空间KD，DSKD）对蒸馏模型的影响。在蒸馏过程中考虑了14种不同的指标（N-gram、嵌入和LLM-based指标）的全方位评估。实验结果表明，当两个模型具有相同的词汇时，教师模型的SFT可以提高蒸馏模型的表现，无论算法和指标如何。总体而言，教师模型和学生模型的SFT在所有指标上都导致了更好的性能，尽管相同的统计显著性取决于教师模型的词汇。|
|**2025-04-28**|**TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons**|Emre Can Acikgoz et.al.|[2504.19982](http://arxiv.org/abs/2504.19982)|null|面向任务的对话（TOD）系统正受到大型语言模型（LLMs）的推动而经历一场革命，然而，这些系统日益复杂的评估方法仍然不足。虽然传统的自动指标有效地评估了早期的模块化系统，但它们仅关注对话层面，无法检测用户代理交互过程中可能出现的关键中间错误。在本文中，我们介绍了TD-EVAL（回合和对话级评估），这是一个两步评估框架，它将细粒度的回合级分析与整体对话级比较统一起来。在回合级别，我们沿着三个面向TOD的特定维度评估每个响应：对话连贯性、后端知识一致性和策略合规性。同时，我们设计了TOD Agent Arena，它通过成对比较来提供对话级质量的度量。通过在MultiWOZ 2.4和{\tau}-Bench上的实验，我们证明了TD-EVAL能够有效地识别传统指标遗漏的对话错误。此外，TD-EVAL与传统和基于LLM的指标相比，与人类判断的契合度更高。这些发现表明，TD-EVAL为TOD系统评估引入了一种新的范式，通过即插即用的框架有效地评估了回合和系统级别，为未来的研究提供了便利。|
|**2025-04-28**|**Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets**|Adam Younsi et.al.|[2504.19981](http://arxiv.org/abs/2504.19981)|null|在复杂领域如数学中，大型语言模型（LLMs）在实现准确性和多样化推理方面仍然面临挑战。一个关键瓶颈是在不花费高昂人力标注的情况下评估中间推理步骤以引导生成。为了解决这个问题，我们首先介绍了一种新颖的过程奖励模型（PRM），该模型通过蒙特卡洛树搜索与基于相似性的数据增强技术自动训练，有效地捕捉步骤级别的推理质量。利用这个PRM，我们随后将生成流网络（GFlowNets）适配到推理步骤级别。与传统的关注最大化单一奖励的强化学习不同，GFlowNets自然地根据我们的PRM测量的奖励比例采样多样化、高质量解决方案。经验评估显示，在具有挑战性的数学基准测试（例如，Llama3.2-3B在MATH Level 5上绝对准确率提高了+2.59%）上，准确率和解决方案多样性都得到了显著提升，并且对未见数据集的有效泛化（SAT MATH绝对提高了+9.4%）。我们的工作展示了PRM引导的、步骤级别的GFlowNets在LLMs中开发更稳健、更通用的数学推理的潜力。|
|**2025-04-28**|**From Concept to Practice: an Automated LLM-aided UVM Machine for RTL Verification**|Junhao Ye et.al.|[2504.19959](http://arxiv.org/abs/2504.19959)|null|验证是集成电路（IC）开发中的主要瓶颈，消耗了近70%的总开发工作量。虽然通用验证方法（UVM）在工业界广泛用于通过结构化和可重用的测试平台来提高验证效率，但构建这些测试平台和生成足够的激励信号仍然具有挑战性。这些挑战源于构建这些平台所需的相当大的手动编码工作量、重复手动执行多个电子设计自动化（EDA）工具的需要，以及深入的专业知识来处理复杂设计。在这里，我们提出了UVM^2，这是一个利用大型语言模型（LLMs）自动生成UVM测试平台，并通过覆盖率反馈迭代改进它们的验证框架，显著减少了手动工作量，同时保持了严格的验证标准。为了评估UVM^2，我们引入了一个包含最多1.6K行代码的寄存器传输级（RTL）设计基准套件。结果显示，与经验丰富的工程师相比，UVM^2将测试平台设置时间减少了高达UVM^2，并且实现了平均代码覆盖率和函数覆盖率分别为87.44%和89.58%，分别比最先进解决方案高出20.96%和23.51%。|
|**2025-04-25**|**TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation**|Gwen Yidou Weng et.al.|[2504.18535](http://arxiv.org/abs/2504.18535)|null|随着大型语言模型（LMs）的进步，控制其输出以符合人类价值观（例如，脱毒）或期望属性（例如，个性化、主题）的需求日益增加。然而，自回归模型专注于预测下一个标记，难以处理需要前瞻性的全局属性。现有的解决方案要么为每个新属性调整或后训练LM - 成本高昂且缺乏灵活性 - 要么通过采样或训练来近似未来序列的期望属性概率（EAP），这对稀有属性来说既慢又不可靠。我们引入了TRACE（可处理的概率推理以适应可控gEneration），这是一个新颖的框架，它通过可处理的概率推理和轻量级控制有效地计算EAP并适应新属性。TRACE从一个LM中提取了一个隐藏马尔可夫模型（HMM），并将其与一个小型分类器配对，以估计属性概率，从而在HMM预测的未来上实现精确的EAP计算。然后，使用这个EAP来重新加权LM的下一个标记概率，以实现全局符合的连续性。在实证研究中，TRACE在脱毒方面取得了最先进的成果，只有10%的解码开销，可在几秒钟内适应76个低资源的个性化LLMs，并无缝扩展到复合属性。|
|**2025-04-25**|**Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation**|Shivam Duggal et.al.|[2504.18509](http://arxiv.org/abs/2504.18509)|null|尽管在3D生成领域取得了前所未有的进步，但现有系统仍然往往无法生成既具有视觉吸引力，又在多个视角上保持几何和语义一致性的高质量3D资产。为了有效评估生成3D数据的质量，需要一款可靠的3D评估工具。不幸的是，现有的3D评估指标常常忽视生成资产的几何质量，或者仅仅依赖黑盒的多模态大型语言模型进行粗略评估。在本文中，我们引入了Eval3D，这是一个细粒度、可解释的评估工具，可以基于各种不同但又互补的标准忠实评估生成3D资产的质量。我们的关键观察是，许多3D生成的期望属性，如语义和几何一致性，可以通过测量不同基础模型和工具之间的不一致性来有效捕捉。因此，我们利用一系列不同的模型和工具作为探测器，来评估生成3D资产在不同方面的不一致性。与以往的工作相比，Eval3D提供了像素级的测量，实现了准确的3D空间反馈，并更紧密地与人类判断相一致。我们使用Eval3D全面评估了现有的3D生成模型，并突出了当前模型的局限性和挑战。|
|**2025-04-25**|**Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues**|Leandra Fichtel et.al.|[2504.18483](http://arxiv.org/abs/2504.18483)|null|可生成被解释者理解的解释是可解释人工智能的核心。由于理解取决于解释者的背景和需求，最近的研究集中在共构建解释对话上，其中解释者持续监控解释者的理解并动态调整解释。我们研究了大型语言模型（LLMs）在共构建解释对话中作为解释者的能力。特别是，我们进行了一项用户研究，其中解释者与LLMs互动，其中一些已被指示以共构建方式解释预定义的主题。我们在对话前后评估了解释者的理解，以及他们对LLMs共构建行为的感知。我们的结果表明，当前的LLMs显示出一些共构建行为，如提出验证问题，这有助于激发解释者的参与并可以提高对某一主题的理解。然而，他们有效监控当前理解并据此构建解释的能力仍然有限。|
|**2025-04-25**|**Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation**|Peiyuan Jing et.al.|[2504.18453](http://arxiv.org/abs/2504.18453)|null|放射学报告生成对于效率至关重要，但当前模型缺乏专家的结构化推理，这阻碍了临床信任和可解释性，因为它们无法将视觉发现与精确的解剖位置联系起来。本文介绍了一种开创性的统一训练框架BoxMed-RL，用于生成可验证和可解释的放射学报告。BoxMed-RL建立在大型视觉-语言模型之上，通过两个集成阶段革新了报告生成：1）在预训练阶段，我们通过医学概念学习来优化模型，使用思维链监督来内化类似放射科医生的流程，随后进行可验证的强化学习，将医学发现与边界框对齐。2）在下游适配器阶段，我们冻结预训练的权重，并训练一个下游适配器以确保流畅且临床可信的报告。该框架精确地模拟了放射科医生的流程，迫使模型将高级医学概念与明确的解剖证据相连接。在公共数据集上的大量实验表明，与最先进的方法相比，BoxMed-RL在METEOR和ROUGE-L指标上平均提高了7%。基于大型语言模型的指标平均提高了5%，进一步强调了BoxMed-RL在生成高质量放射学报告方面的鲁棒性。|
|**2025-04-25**|**LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection**|Rajesh Yarra et.al.|[2504.18423](http://arxiv.org/abs/2504.18423)|null|尽管人工智能（AI）在各个领域产生了变革性影响，网络安全仍然依赖于传统的静态和动态分析工具，这些工具受限于高误报率和对代码理解的肤浅。虽然生成式AI为软件开发提供了有前景的自动化能力，但利用大型语言模型（LLMs）进行漏洞检测面临着独特的挑战。本文探讨了LLMs在识别漏洞方面的潜力和局限性，承认了固有的弱点，如幻觉、有限的上下文长度和知识截止点。先前尝试使用机器学习模型进行漏洞检测的努力由于现实世界应用有限、特征工程挑战、缺乏上下文理解和训练模型以跟上不断变化的威胁格局的复杂性而证明是无效的。因此，我们提出了一种稳健的AI驱动方法，专注于缓解这些局限性，并确保基于LLMs的漏洞检测的质量和可靠性。通过结合检索增强生成（RAG）和混合代理（MoA）的创新方法，本研究旨在利用LLMs的优势，同时解决其弱点，最终为保护不断演变的软件景观铺平道路，提供可靠和高效的AI解决方案。|
|**2025-04-25**|**BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs**|Hongyu Wang et.al.|[2504.18415](http://arxiv.org/abs/2504.18415)|null|1位大语言模型（LLMs）的高效部署受到激活异常的阻碍，这增加了将其量化为低比特宽度的复杂性。我们引入了BitNet v2，这是一个新型框架，它使得1位LLMs能够实现原生4位激活量化。为了解决注意力和前馈网络激活中的异常值，我们提出了H-BitLinear模块，该模块在激活量化之前应用在线Hadamard变换。这种变换将尖锐的激活分布平滑成更类似于高斯分布的形式，适合低比特表示。实验表明，从零开始使用8位激活训练的BitNet v2的性能与BitNet b1.58相当。关键的是，当使用原生4位激活进行训练时，BitNet v2实现了最小性能下降，显著减少了批量推理时的内存占用和计算成本。|
|**2025-04-25**|**An Empirical Study of Evaluating Long-form Question Answering**|Ning Xian et.al.|[2504.18413](http://arxiv.org/abs/2504.18413)|**[link](https://github.com/bugtig6351/lfqa_evaluation)**|LFQA旨在生成对复杂问题的长篇回答。这种场景在评估方面既具有很大的灵活性，也带来了显著的挑战。大多数评估依赖于依赖于字符串或n-gram匹配的确定性指标，而基于大型语言模型的长篇回答评估的可靠性相对未被充分探索。我们通过深入研究长篇回答评估，提出以下研究问题： (i) 现有的自动评估指标在多大程度上可以替代人工评估？ (ii) 相比于人工评估，现有评估指标存在哪些局限性？ (iii) 如何提高现有评估方法的有效性和鲁棒性？我们收集了5,236条由不同大型语言模型生成的知识性和非知识性长篇回答，并对其中的2,079条进行了人工评估，重点关注正确性和信息量。随后，我们通过评估这些回答，分析了这些自动评估指标与人工评估之间的一致性。我们发现，回答的风格、长度和问题的类别可能会偏颇自动评估指标。然而，细粒度的评估有助于在某些指标上减轻这一问题。我们的发现对使用大型语言模型进行长篇问答评估具有重要意义。所有代码和数据集均可在https://github.com/bugtig6351/lfqa_evaluation上获得。|
|**2025-04-25**|**Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers**|Jared Moore et.al.|[2504.18412](http://arxiv.org/abs/2504.18412)|**[link](https://github.com/jlcmoore/llms-as-therapists)**|**大型语言模型（LLM）是否应该被用作心理治疗师？在本文中，我们调查了使用LLM来**取代**心理健康提供者的应用场景，这是在科技初创公司和研究领域中被提倡的一种用例。我们通过对主要医疗机构使用的治疗指南进行映射综述，确定了治疗关系中的关键方面，例如治疗师与客户之间治疗联盟的重要性。然后，我们通过进行多项实验来评估LLM复制和遵守这些治疗关系方面能力，实验研究了当前LLM（如“gpt-4o”）的响应。与医疗界最佳实践相反，LLM 1）对有心理健康状况的人表现出歧视，2）在自然疗法环境中对某些常见（且关键）的条件作出不当反应——例如，LLM鼓励客户的妄想思维，这可能是由于它们的谄媚。即使是在更大、更新的LLM中也是如此，这表明当前的安全实践可能无法弥补这些差距。此外，我们指出将LLM作为治疗师采用的基础性和实际障碍，例如治疗联盟需要人类特征（例如身份和利害关系）。因此，我们得出结论，LLM不应取代治疗师，并讨论了LLM在临床治疗中的替代角色。**|
|**2025-04-25**|**HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?**|Yusen Zhang et.al.|[2504.18406](http://arxiv.org/abs/2504.18406)|null|高分辨率图像（HRI）理解旨在处理具有大量像素的图像，例如病理图像和农业航空图像，这两种图像的像素数都可能超过100万。据称，视觉大型语言模型（VLMs）可以处理HRI，然而，目前缺乏一个全面的标准来评估VLMs对HRI理解的能力。为了填补这一空白，我们引入了HRScene，这是一个用于HRI理解的全新统一基准，其中包含丰富的场景。HRScene包含了25个真实世界的数据集和2个合成诊断数据集，分辨率从1,024 $\times$ 1,024到35,503 $\times$ 26,627不等。HRScene由10名研究生级别的标注者收集和重新标注，涵盖了25种场景，从显微镜图像到放射学图像、街景、长距离图片和望远镜图像。它包括真实世界物体、扫描文档和合成多图像的HRI。两个诊断评估数据集是通过将目标图像与金答案和干扰图像以不同的顺序组合而成的。这些数据集评估了模型利用HRI区域的能力。我们进行了大量实验，涉及28个VLMs，包括Gemini 2.0 Flash和GPT-4o。在HRScene上的实验表明，当前的VLMs在真实世界任务上的平均准确率约为50%，揭示了在HRI理解方面存在显著的差距。在合成数据集上的结果揭示了VLMs难以有效地利用HRI区域，显示出显著的区域差异和中间丢失，为未来的研究提供了启示。|
|**2025-04-25**|**Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization**|Kesen Zhao et.al.|[2504.18397](http://arxiv.org/abs/2504.18397)|**[link](https://github.com/kesenzhao/uv-cot)**|思维链（CoT）推理显著提高了多模态大型语言模型（MLLMs）的可解释性和问题解决能力。然而，现有方法主要关注文本CoT，限制了其利用视觉线索的能力。视觉CoT仍处于探索阶段，目前唯一的工作是基于监督微调（SFT），它依赖于大量的标注边界框数据，难以推广到未见过的案例。在本文中，我们引入了无监督视觉CoT（UV-CoT），这是一个通过偏好优化进行图像级CoT推理的新框架。UV-CoT在模型生成的边界框（一个被偏好，另一个被不偏好）之间进行偏好比较，消除了对边界框标注的需求。我们通过引入一个自动数据生成管道来获取这样的偏好数据。给定一个图像，我们的目标MLLM（例如，LLaVA-1.5-7B）使用模板提示生成种子边界框，然后使用每个边界区域作为输入来回答问题。评估MLLM（例如，OmniLLM-12B）对回答进行排名，这些排名作为监督来训练目标MLLM，通过最小化负对数似然损失。通过模拟人类感知——识别关键区域并根据它们进行推理——UV-CoT可以提高视觉理解能力，尤其是在仅凭文本描述就不足够的空间推理任务中。我们在六个数据集上的实验表明，UV-CoT与最先进的文本和视觉CoT方法相比具有优越性。我们在四个未见过的数据集上的零样本测试显示了UV-CoT的强大泛化能力。代码可在https://github.com/kesenzhao/UV-CoT上找到。|
|**2025-04-24**|**Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models**|Xu Ma et.al.|[2504.17789](http://arxiv.org/abs/2504.17789)|null|自回归（AR）模型在语言生成领域长期占据主导地位，但其在图像合成领域的应用却往往被认为不如基于扩散的模型具有竞争力。一个主要限制是AR模型需要大量的图像标记，这限制了训练和推理效率，以及图像分辨率。为了解决这个问题，我们提出了一种新颖且简单的方法——Token-Shuffle，该方法减少了Transformer中的图像标记数量。我们的关键洞察是多模态大型语言模型（MLLMs）中视觉词汇的维度冗余，其中视觉编码器产生的低维视觉代码直接映射到高维语言词汇。利用这一点，我们考虑了两个关键操作：token-shuffle，它将沿着通道维度合并空间上局部的标记以减少输入标记数量；以及token-unshuffle，它解开Transformer块之后推断出的标记，以恢复输出空间排列。与文本提示联合训练，我们的策略不需要额外的预训练文本编码器，并使MLLMs能够以统一的下一个标记预测方式支持极高分辨率的图像合成，同时保持高效的训练和推理。首次，我们将AR文本到图像生成的边界推至2048x2048的分辨率，并取得了令人满意的生成性能。在GenAI-benchmark中，我们的27亿参数模型在困难提示上的总体得分为0.77，比LlamaGen AR模型高0.18，比LDM扩散模型高0.15。大规模的人评也充分展示了我们在文本对齐、视觉瑕疵和视觉外观等方面的突出图像生成能力。我们希望Token-Shuffle能够成为MLLMs中高效高分辨率图像生成的基石设计。|
|**2025-04-24**|**Replay to Remember: Retaining Domain Knowledge in Streaming Language Models**|Sneh Pillai et.al.|[2504.17780](http://arxiv.org/abs/2504.17780)|null|在大型语言模型（LLMs）的持续学习中，通常会遇到灾难性遗忘这一关键挑战，即在新数据的暴露下，先前获得的知识会退化。虽然已经提出了诸如重放缓冲区和参数高效调整（例如低秩调整或LoRA）等技术，但很少有研究在严格的计算和数据流约束下探讨实时领域适应。在本文中，我们展示了一种轻量级方法，该方法结合了LoRA和最小重放机制，在三个不同的知识领域（医学问答、遗传学和法律）的现实中流设置中进行。使用困惑度、语义相似度和基于GPT的人类似评价标准，我们量化了模型随时间变化的适应、遗忘和恢复情况。我们的实验表明，尽管灾难性遗忘自然发生，但即使是最小重放也能显著稳定并部分恢复特定领域的知识。这项研究为在资源受限的实际情况中部署可适应的LLMs提供了实用的见解。|
|**2025-04-24**|**Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT**|Anuja Tayal et.al.|[2504.17753](http://arxiv.org/abs/2504.17753)|null|随着对话助手在医疗保健等领域的日益普及，部分原因是大型语言模型的可获得性和功能。需要进行有控制的、针对真实利益相关者的探查性评估，以突出传统架构和基于生成式AI的架构的优缺点。我们进行了一项组内用户研究，比较了两种版本的心脏衰竭患者询问食物中盐含量对话助手。一种是内部开发的具有神经符号架构的系统，另一种基于ChatGPT。评估显示，内部系统比基于ChatGPT的系统更准确，完成任务更多，且言辞更简洁；另一方面，基于ChatGPT的系统在完成任务的语音错误更少，且需要更少的澄清来完成。患者对两者没有明显的偏好。|
|**2025-04-24**|**Towards Robust LLMs: an Adversarial Robustness Measurement Framework**|Natan Levy et.al.|[2504.17723](http://arxiv.org/abs/2504.17723)|null|大型语言模型（LLMs）的兴起彻底改变了人工智能领域，然而这些模型仍然容易受到对抗性干扰的影响，这在高风险应用中削弱了它们的可靠性。尽管基于视觉的神经网络中的对抗鲁棒性已经得到了广泛研究，但LLMs的鲁棒性仍被研究不足。我们改进了鲁棒性测量与评估（RoMA）框架，以量化LLMs对对抗性输入的韧性，而无需访问模型参数。通过将RoMA的估计值与形式验证方法进行比较，我们证明了其准确性，误差幅度极小，同时保持计算效率。我们的实证评估显示，鲁棒性不仅在不同模型之间存在显著差异，而且在同一任务类别内以及不同类型的干扰之间也存在差异。这种非均匀性强调了针对特定任务进行鲁棒性评估的必要性，使得从业者能够根据特定应用的需求比较和选择模型。我们的研究提供了一种系统的方法来评估LLMs的鲁棒性，推动了更可靠的语言模型在实际部署中的发展。|
|**2025-04-24**|**Multilingual Performance Biases of Large Language Models in Education**|Vansh Gupta et.al.|[2504.17720](http://arxiv.org/abs/2504.17720)|null|大型语言模型（LLMs）在教育环境中的应用越来越广泛。尽管当前的大型语言模型仍然主要以英语为中心，但这些应用已经超越了英语。在本研究中，我们探讨在非英语语言的教育环境中使用LLMs是否合理。我们评估了在四个教育任务上流行的LLMs的性能：识别学生的错误观念、提供有针对性的反馈、交互式辅导以及在六种语言（印地语、阿拉伯语、波斯语、泰卢固语、乌克兰语、捷克语）中对翻译进行评分，这些语言包括英语。我们发现，这些任务上的性能在一定程度上与训练数据中使用的语言量相对应，资源较少的语言在任务性能上较差。尽管模型在大多数语言上的表现都相当不错，但从英语到其他语言的性能下降是显著的。因此，我们建议在部署前，实践者首先验证LLM在目标语言的教育任务中是否表现良好。|
|**2025-04-24**|**Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks**|Haru-Tada Sato et.al.|[2504.17685](http://arxiv.org/abs/2504.17685)|null|本研究探讨了小型语言模型（SLM）集成在实现与专有大型语言模型（LLM）相当准确度的潜力。我们提出了集成贝叶斯推理（EBI），这是一种新颖的方法，它将贝叶斯估计应用于结合多个SLM的判断，使它们能够超越单个模型的表现局限。我们在多种任务（日语和英语的资质评估和消费者画像分析）上的实验展示了EBI的有效性。值得注意的是，我们分析了将具有负提升值（Lift values）的模型纳入集成中如何提高整体性能的案例，并考察了该方法在不同语言中的有效性。这些发现为使用有限的计算资源构建高性能AI系统以及有效利用单个表现较低的模式提供了新的可能性。基于现有关于LLM性能评估、集成方法和开源LLM利用的研究，我们讨论了我们的方法的创新性和重要性。|
|**2025-04-24**|**INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models**|Jarne Thys et.al.|[2504.17677](http://arxiv.org/abs/2504.17677)|null|随着人工智能（AI）的兴起，尤其是大型语言模型（LLM）的出现，将这种技术整合到课堂中既带来了挑战也提供了机遇。AI有潜力通过帮助教师完成各种任务（如个性化教学方法）来革新教育，但它也引发了一些担忧，例如学生与教师互动的退化以及用户隐私问题。本文介绍了INSIGHT，这是一个将各种AI工具相结合以协助教师和学生解决练习的证明概念。INSIGHT采用模块化设计，使其能够整合到各种高等教育课程中。我们通过提取关键词分析学生对LLM提出的问题，利用这些关键词动态构建FAQ（常见问题解答），并为教师提供新的见解，以用于更个性化的面对面支持。未来的工作可以基于INSIGHT，利用收集到的数据提供适应性学习，并根据学生的学习进度和学习风格调整内容，以提供更互动和包容的学习体验。|
|**2025-04-24**|**Energy Considerations of Large Language Model Inference and Efficiency Optimizations**|Jared Fernandez et.al.|[2504.17674](http://arxiv.org/abs/2504.17674)|null|随着大型语言模型（LLMs）规模和应用的扩大，其计算和环境成本持续上升。以往的性能基准测试主要关注理想化环境下的延迟降低，往往忽略了影响能耗的多样化现实世界推理工作负载。在这项工作中，我们系统地分析了针对多样化的自然语言处理（NLP）和生成式人工智能（AI）工作负载，包括对话式AI和代码生成，常见的推理效率优化对能耗的影响。我们引入了一种建模方法，通过输入输出令牌分布的分组策略和批处理大小的变化来近似现实世界的LLM工作流程。我们的实证分析涵盖了软件框架、解码策略、GPU架构、在线和离线服务设置以及模型并行配置。我们发现，推理优化的有效性高度敏感于工作负载几何形状、软件栈和硬件加速器，这表明基于浮点运算（FLOPs）或理论GPU利用率的简单能耗估计显著低估了现实世界的能耗。我们的研究结果表明，正确应用相关的推理效率优化可以将总能耗降低高达73%，与未优化的基准相比。这些见解为可持续的LLM部署提供了基础，并为未来人工智能基础设施的节能设计策略提供了信息。|
|**2025-04-24**|**Cross-region Model Training with Communication-Computation Overlapping and Delay Compensation**|Ying Zhu et.al.|[2504.17672](http://arxiv.org/abs/2504.17672)|null|训练大型语言模型（LLMs）需要巨大的计算资源，通常需要地理上分布的数据中心（即跨区域训练）。然而，广域网络中的高通信延迟严重降低了传统分布式训练的效率。虽然像DiLoCo这样的方法可以减少通信频率，但它们遭受了阻塞同步的困扰。Streaming DiLoCo通过通信-计算重叠来缓解这一问题，但由于延迟的全局更新和部分同步，引入了更新陈旧和模型不一致的问题。这些因素阻碍了收敛，尤其是在需要激进的重叠来掩盖高延迟时。我们提出了CoCoDC，一个具有通信-计算重叠和延迟补偿的新型分布式训练框架，以明确解决这些挑战。在CoCoDC框架内，我们特别开发了一种基于泰勒展开的延迟补偿策略，以有效减轻陈旧问题，并设计了一种自适应传输策略，该策略动态安排模型片段同步，以优化带宽使用并加速收敛。大量实验突出了CoCoDC在最终准确性和训练速度方面优于DiLoCo和Streaming DiLoCo的性能。具体来说，与Streaming DiLoCo相比，CoCoDC将达到可比困惑度所需的训练步数减少了高达21.0%。我们的工作为可扩展和高效的跨区域LLM训练提供了一个有效的解决方案。|
|**2025-04-24**|**Towards a HIPAA Compliant Agentic AI System in Healthcare**|Subash Neupane et.al.|[2504.17669](http://arxiv.org/abs/2504.17669)|null|基于大型语言模型（LLMs）作为其基础推理引擎的代理人工智能系统，正在通过自主分析敏感医疗数据和以最小人工监管执行决策的方式，改变临床工作流程，如医疗报告生成和临床摘要。然而，其采用需要严格遵守如健康保险可携带性和问责制法案（HIPAA）等监管框架，尤其是在处理受保护的健康信息（PHI）时。这篇正在进行中的论文介绍了一个符合HIPAA的代理人工智能框架，该框架通过动态、上下文感知的政策执行来确保合规性。我们的框架集成了三个核心机制：（1）基于属性的访问控制（ABAC）用于细粒度的PHI治理，（2）结合正则表达式模式和基于BERT的模型混合PHI净化管道以最小化泄露，以及（3）不可变审计跟踪以进行合规性验证。|
|**2025-04-23**|**IberBench: LLM Evaluation on Iberian Languages**|José Ángel González et.al.|[2504.16921](http://arxiv.org/abs/2504.16921)|null|大型语言模型（LLMs）的全面评估仍然困难，尤其是在英语以外的语言中，高质量的数据通常有限。现有的基准和排行榜主要以英语为中心，只有少数涉及其他语言。这些基准在几个关键方面存在不足：它们忽视了语言变体的多样性，将基本自然语言处理（NLP）能力置于工业相关任务之上，并且是静态的。考虑到这些方面，我们提出了IberBench，这是一个全面且可扩展的基准，旨在评估LLMs在基础和工业相关NLP任务上的性能，涉及伊比利亚半岛和伊比利亚美洲使用的语言。IberBench集成了来自评估活动和最近基准的101个数据集，涵盖了22个任务类别，如情感和情绪分析、毒性检测和摘要。该基准通过允许持续的更新和由专家委员会监管的社区驱动的模型和数据集提交，解决了当前评估实践中的一些关键局限性，如缺乏语言多样性和静态评估设置。我们评估了从1亿到140亿参数的23个LLMs，并提供了关于它们优势和局限性的经验见解。我们的发现表明：（i）LLMs在工业相关任务上的表现不如在基础任务上；（ii）加泰罗尼亚语和巴斯克语的平均性能较低；（iii）某些任务的结果接近随机；（iv）在其他任务中，LLMs的表现高于随机但低于共享任务系统。IberBench为整个评估流程提供了开源实现，包括数据集归一化和托管、LLMs的增量评估以及公开可访问的排行榜。|
|**2025-04-23**|**Do Large Language Models know who did what to whom?**|Joseph M. Denning et.al.|[2504.16884](http://arxiv.org/abs/2504.16884)|null|大型语言模型（LLMs）常因不懂得语言而受到批评。然而，许多批评集中在人类独有的认知能力上，这些能力与语言处理不同。在这里，我们研究了一种与语言紧密相关的理解：推断句子中谁对谁做了什么（主题角色）。LLMs的核心训练目标——词预测——是否会导致句子表示捕捉到主题角色？在两个实验中，我们描述了四个LLMs中的句子表示。与人类的相似性判断不同，在LLMs中，句子对的整体表示相似性反映了句法相似性，但并未体现它们的施事和受事分配是相同还是相反。此外，我们发现几乎没有证据表明主题角色信息存在于任何隐藏单元子集中。然而，一些注意力头稳健地捕捉到了主题角色，独立于句法。因此，LLMs可以提取主题角色，但与人类相比，这种信息对其表示的影响较弱。|
|**2025-04-23**|**Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models**|Xuyang Zhu et.al.|[2504.16883](http://arxiv.org/abs/2504.16883)|null|检索增强生成（RAG）系统通过融入经过事实核查、与上下文相关的信息，为提升大型语言模型（LLM）输出提供了一种强大方法。然而，公平性和可靠性问题仍然存在，因为在检索和生成阶段都可能产生幻觉，影响用户的推理和决策。我们的研究探讨了定制警告信息（其内容取决于幻觉的具体上下文）如何在教育测验环境中塑造用户的推理和行动。初步发现表明，尽管警告提高了对高级幻觉的准确性和意识，但它们也可能引入认知摩擦，导致混淆并降低对系统的信任。通过研究这些互动，本研究为AI增强推理的更广泛目标做出了贡献：开发能够主动支持人类反思、批判性思维和基于信息决策的系统，而不是被动的信息消费。|
|**2025-04-23**|**Context-Enhanced Vulnerability Detection Based on Large Language Model**|Yixin Yang et.al.|[2504.16877](http://arxiv.org/abs/2504.16877)|null|漏洞检测是软件安全的关键方面。准确的检测对于防止潜在的安全漏洞和保护软件系统免受恶意攻击至关重要。最近，利用深度学习和大型语言模型（LLMs）的漏洞检测方法引起了越来越多的关注。然而，现有方法往往侧重于分析单个文件或函数，这限制了它们收集足够上下文信息的能力。分析整个存储库以获取上下文会引入大量的噪声和计算开销。为了解决这些挑战，我们提出了一种结合程序分析和LLMs的上下文增强漏洞检测方法。具体来说，我们使用程序分析从不同抽象级别提取上下文信息，从而过滤掉无关的噪声。将抽象的上下文与源代码一起提供给LLM进行漏洞检测。我们研究了不同级别的上下文粒度如何提高基于LLM的漏洞检测性能。我们的目标是平衡提供足够细节以准确捕获漏洞和最小化可能阻碍模型性能的不必要复杂性。基于使用GPT-4、DeepSeek和CodeLLaMA以及各种提示策略的广泛研究，我们的主要发现包括：（1）引入抽象上下文显著提高了漏洞检测的有效性；（2）不同的模型根据其代码理解能力受益于不同的抽象级别；（3）通过程序分析捕获程序行为，用于一般LLM基于代码分析任务，可能是一个需要进一步关注的方向。|
|**2025-04-23**|**Exploring How LLMs Capture and Represent Domain-Specific Knowledge**|Mirian Hipolito Garcia et.al.|[2504.16871](http://arxiv.org/abs/2504.16871)|null|我们研究大型语言模型（LLMs）是否天生能够捕捉自然语言中的领域特定细微差别。我们的实验通过检查LLMs在预填充阶段生成的隐藏状态来探究其领域敏感性，以区分不同领域的查询。我们揭示了表示模型内部识别查询领域的潜在领域相关轨迹。我们还研究了这些领域表示对提示风格和来源变化的鲁棒性。我们的方法利用这些表示进行模型选择，将最佳匹配输入查询领域轨迹的LLM（即，在类似轨迹上性能最高的模型）映射出来。我们的发现表明，LLMs可以区分相关领域的查询，并且微调的模型并不总是最准确的。与以往的工作不同，我们的解释适用于封闭和开放式生成任务。|
|**2025-04-23**|**Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification**|Alexander Shvets et.al.|[2504.16856](http://arxiv.org/abs/2504.16856)|null|大多数用于情感分析的语料库缺乏表达观点时的上下文，这对于情感理解至关重要，并且主要局限于少数几个情感类别。像GPT-4这样的基础大型语言模型（LLMs）存在过度预测情感的问题，并且资源消耗过于庞大。我们设计了一个基于LLM的数据合成流程，并利用大型模型Mistral-7b生成训练示例，以适用于更易于使用、轻量级的BERT型编码器模型。我们专注于扩大示例的语义多样性，并提议将生成基于叙事语料库，以产生不重复的以故事人物为中心的表述，涵盖28个情感类别，具有独特的上下文。通过在450个GPU小时内运行700K个推理，我们贡献了一个包含10万个上下文和30万个无上下文示例的数据集，覆盖了这两种场景。我们用它来微调预训练编码器，从而产生多个情感支柱模型。我们发现，当调整到特定任务，如GoEmotions、ISEAR、IEMOCAP和EmoContext时，情感支柱模型对新的领域高度适应，在前三个任务上达到了SOTA性能。我们还验证了我们的数据集，进行了统计分析和人评，证实了我们在表述多样化和上下文个性化方面的措施成功（尽管对中性类别的贡献较小），同时指出需要在流程中更好地处理非分类标签的必要性。|
|**2025-04-23**|**Monte Carlo Planning with Large Language Model for Text-Based Game Agents**|Zijing Shi et.al.|[2504.16855](http://arxiv.org/abs/2504.16855)|null|基于文本的游戏为语言型自主代理提供了有价值的平台。然而，由于需要大量迭代，结合蒙特卡洛树搜索（MCTS）和强化学习（RL）的“先计划后学习”范式特别耗时。此外，这些算法虽然进行基于不确定性的探索，但缺乏语言理解和推理能力。在本文中，我们介绍了蒙特卡洛规划结合动态内存引导的大语言模型（MC-DML）算法。MC-DML利用大型语言模型（LLMs）的语言理解和推理能力，以及树搜索算法的探索优势。具体来说，我们通过试验内和跨试验的记忆机制增强LLMs，使它们能够从以往的经验中学习，并在规划过程中动态调整动作评估。我们在杰里科基准的一系列基于文本的游戏上进行了实验。我们的结果表明，MC-DML算法在初始规划阶段显著提升了各种游戏的表现，超越了需要多次迭代的强大当代方法。这证明了我们算法的有效性，为在复杂环境中更高效的语言基础规划铺平了道路。|
|**2025-04-23**|**Improving Significant Wave Height Prediction Using Chronos Models**|Yilin Zhai et.al.|[2504.16834](http://arxiv.org/abs/2504.16834)|null|准确的波浪高度预测对于海事安全和海岸地区的韧性至关重要，然而传统的基于物理的模型和传统的机器学习方法在计算效率和非线性动力学建模方面面临挑战。本研究介绍了Chronos，这是第一个基于大型语言模型（LLM）的时间架构（Chronos）的实现，它针对波浪预测进行了优化。通过将高级时间模式识别应用于来自西北太平洋盆地上三个战略选定海洋区域的历 史波浪数据，我们的框架实现了多模式改进：(1)与PatchTST基线相比，训练时间减少了14.3%，推理速度提高了2.5倍，实现了0.575的均方根误差（MASE）单位；(2)在短期预测（1-24小时）方面表现优异，在全面指标上都有所提升；(3)在长期预测（1-120小时）中持续保持预测优势；(4)在零样本能力方面，与专业运营模型相比，保持了中位数性能（排名第4/12）。这种LLM增强的时间建模范式为波浪预测设定了新的标准，提供了计算效率高的解决方案，并为复杂地球物理系统建模提供了一个可转移的框架。|
|**2025-04-23**|**LRASGen: LLM-based RESTful API Specification Generation**|Sida Deng et.al.|[2504.16833](http://arxiv.org/abs/2504.16833)|null|REST表现状态转移（REST）是一种用于设计能够通过通用HTTP技术实现客户端与服务器之间可扩展、无状态的通信的Web应用架构风格。采用REST风格的Web API被称为RESTful（或REST）API。在使用或测试RESTful API时，开发者可能需要使用其规范，这些规范通常由开源标准，如OpenAPI规范（OAS）定义。然而，编写和更新这些规范可能非常耗时且易出错，这可能会对RESTful API的使用产生负面影响，尤其是在软件需求发生变化时。许多工具和方法已被提出以解决这个问题，例如Respector和Swagger Core。OAS生成可以被视为一种常见的文本生成任务，它从源代码中创建API端点的正式描述。这种潜在解决方案可能涉及使用具有强大代码理解和文本生成能力的巨型语言模型（LLMs）。受此启发，我们提出了一种使用LLMs生成RESTful API OASs的新方法：基于LLM的RESTful API规范生成（LRASGen）。据我们所知，这是首次使用LLMs和API源代码生成RESTful API的OAS。与现有工具和方法相比，LRASGen即使在实现不完整（部分代码、缺少注释/注释等）的情况下也能生成OAS。为了评估LRASGen的性能，我们在20个现实世界的RESTful API上进行了系列实证研究。结果表明，两种LLMs（GPT-4o mini和DeepSeek V3）都能支持LRASGen生成准确的规范，LRASGen生成的规范平均覆盖了开发者提供的规范中48.85%的遗漏实体。|
|**2025-04-23**|**GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning**|Luu Quy Tung et.al.|[2504.16832](http://arxiv.org/abs/2504.16832)|null|思维链（CoT）是一种强大的方法，用于解决在生成最终答案之前需要中间推理步骤的LLM任务。在本文中，我们提出了GreenMind-Medium-14B-R1，这是一个受基于组相对策略优化的微调策略启发的越南推理模型。我们还利用了一个高质量的越南合成推理数据集，并设计了两个奖励函数来解决该技术的两大主要限制：（i）语言混合，我们在采样标记的过程中显式检测到偏见语言字符的存在；（ii）我们利用基于句子转换器的模型来确保生成的推理内容保持事实正确性，并且不扭曲最终输出。在VLSP 2023挑战赛中的越南数据集上的实验结果表明，我们的模型优于先前的工作，并提高了其响应的语言一致性。此外，我们将我们的评估扩展到SeaExam——一个多语言多项选择题数据集，展示了我们的推理方法与少量提示技术相比的有效性。|
|**2025-04-22**|**TTRL: Test-Time Reinforcement Learning**|Yuxin Zuo et.al.|[2504.16084](http://arxiv.org/abs/2504.16084)|**[link](https://github.com/prime-rl/ttrl)**|本文研究了在大型语言模型（LLMs）中进行推理任务时，在无显式标签的数据上进行的强化学习（RL）。该问题的核心挑战是在推理过程中进行奖励估计，同时无法访问真实信息。虽然这种设置看似难以捉摸，但我们发现测试时缩放（TTS）中的常见做法，如多数投票，产生了令人惊讶的有效奖励，适用于驱动RL训练。在这项工作中，我们引入了测试时强化学习（TTRL），这是一种使用RL在未标记数据上训练LLMs的新方法。TTRL通过利用预训练模型中的先验知识，使LLMs能够自我进化。我们的实验表明，TTRL在各种任务和模型上都能持续提高性能。值得注意的是，TTRL在AIME 2024上仅使用未标记的测试数据，就将Qwen-2.5-Math-7B的pass@1性能提升了约159%。此外，尽管TTRL仅由Maj@N指标监督，TTRL已证明其性能可以持续超越初始模型的上限，并接近直接在带有真实标签的测试数据上训练的模型的性能。我们的实验发现验证了TTRL在各种任务上的普遍有效性，并突出了TTRL在更广泛任务和领域中的潜力。GitHub: https://github.com/PRIME-RL/TTRL|
|**2025-04-22**|**From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning**|Le Zhuo et.al.|[2504.16080](http://arxiv.org/abs/2504.16080)|null|近期，通过大量扩展训练数据和模型参数，文本到图像的扩散模型实现了令人印象深刻的视觉质量，但它们在处理复杂场景和精细细节时往往遇到困难。受大型语言模型中出现的自我反思能力的启发，我们提出了ReflectionFlow，这是一个推理时框架，使扩散模型能够迭代地反思和优化其输出。ReflectionFlow引入了三个互补的推理时扩展轴：(1) 噪声水平扩展以优化潜在初始化；(2) 提示水平扩展以提供精确的语义指导；最值得注意的是，(3) 反射水平扩展，它明确地提供可操作的反思，以迭代评估和纠正前一代。为了促进反射水平扩展，我们构建了GenRef，这是一个包含100万个三元组的的大型数据集，每个三元组包含一个反思、一个有缺陷的图像和一个增强的图像。利用这个数据集，我们在统一的框架内联合建模多模态输入，对最先进的扩散Transformer FLUX.1-dev进行了高效的反射调整。实验结果表明，ReflectionFlow在挑战性任务上的图像合成质量显著优于简单的噪声水平扩展方法，提供了一种可扩展且计算高效的解决方案。|
|**2025-04-22**|**LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities**|Thomas Schmied et.al.|[2504.16078](http://arxiv.org/abs/2504.16078)|null|大型语言模型（LLMs）的成功引发了人们对各种代理应用的兴趣。一个关键假设是，利用常识和思维链（CoT）推理，LLMs可以有效地探索和高效地解决复杂领域。然而，研究发现LLM代理在探索上存在次优问题，以及知行差距，即无法有效利用模型中存在的知识进行行动。在本工作中，我们系统地研究了为什么LLMs在决策场景中表现不佳。特别是，我们仔细考察了三种常见的失败模式：贪婪、频率偏差和知行差距。我们提出通过在自生成的CoT理由上使用强化学习（RL）进行微调来缓解这些不足。我们的实验涵盖了多臂老虎机、上下文老虎机和井字棋，表明RL微调通过增加探索和缩小知行差距来增强了LLMs的决策能力。最后，我们研究了经典探索机制，如ε-贪婪，以及LLM特定的方法，如自我纠正和自我一致性，以实现LLMs决策的更有效微调。|
|**2025-04-22**|**PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models**|Shi Qiu et.al.|[2504.16074](http://arxiv.org/abs/2504.16074)|null|我们引入了PHYBench，这是一个新颖、高质量的基准，旨在评估大型语言模型（LLMs）在物理环境中的推理能力。PHYBench包含500个精心挑选的物理问题，基于现实世界的物理场景，旨在评估模型理解和推理现实物理过程的能力。该基准涵盖了力学、电磁学、热力学、光学、现代物理和高级物理，难度级别从高中习题到本科生问题和物理奥林匹克挑战。此外，我们提出了表达式编辑距离（EED）评分，这是一种基于数学表达式之间编辑距离的新颖评估指标，它有效地捕捉了模型推理过程和结果与传统二进制评分方法之外的差异。我们在PHYBench上评估了各种LLMs，并将它们的性能与人类专家进行了比较。我们的结果表明，即使是先进的推理模型也明显落后于人类专家，突显了它们的局限性以及在复杂物理推理场景中改进的必要性。我们的基准结果和数据集可在https://phybench-official.github.io/phybench-demo/上公开获取。|
|**2025-04-22**|**Automated Static Vulnerability Detection via a Holistic Neuro-symbolic Approach**|Penghui Li et.al.|[2504.16057](http://arxiv.org/abs/2504.16057)|null|静态漏洞检测仍然是一个具有挑战性的问题，需要大量的人工努力，例如手动整理良好的漏洞模式。包括经典程序分析和基于大型语言模型（LLM）的方法在内的先前工作，都没有完全自动化地以合理的检测精度生成这样的漏洞模式。在本文中，我们设计并实现了一种名为MoCQ的新颖的神经符号框架，它结合了LLM和经典静态分析的互补优势，以实现可扩展的漏洞检测。关键洞察是MoCQ利用LLM自动提取漏洞模式并将它们转换为检测查询，然后通过静态分析在反馈循环中优化这些查询，最终执行它们以分析大型代码库并挖掘漏洞。我们在两种编程语言的七种类型漏洞上评估了MoCQ。我们发现MoCQ生成的查询至少发现了12种专家遗漏的模式。在真实数据集上，MoCQ与专家定制的查询相比，实现了相当高的精确度和召回率。此外，MoCQ在现实应用程序中发现了七个之前未知的漏洞，证明了其实际有效性。我们已经负责地通知了相应的开发者。|
|**2025-04-22**|**Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability**|Daniel Hendriks et.al.|[2504.16056](http://arxiv.org/abs/2504.16056)|null|人工智能（AI）日益影响现代社会，最近尤其是在大型语言模型（LLM）的显著进展方面。然而，LLM的高计算和存储需求仍然限制了它们在资源受限环境中的部署。知识蒸馏通过从一个较大的教师模型中训练一个较小的学生模型来应对这一挑战。先前的研究已经引入了多种用于生成训练数据和训练学生模型的知识蒸馏方法。尽管这些方法相关，但最先进蒸馏方法对模型性能和可解释性的影响尚未得到充分研究和比较。在这项工作中，我们通过应用批评-修订提示来扩展可用的方法集，并为数据生成应用，同时通过综合现有方法来训练。对于这些方法，我们基于广泛使用的常识问答（CQA）数据集提供了系统性的比较。虽然我们通过学生模型的准确率来衡量性能，但我们采用基于人类的研究来评估可解释性。我们贡献了新的蒸馏方法及其在性能和可解释性方面的比较。这应将进一步推进小型语言模型的知识蒸馏，从而有助于更广泛的应用和LLM技术的更快扩散。|
|**2025-04-22**|**Certified Mitigation of Worst-Case LLM Copyright Infringement**|Jingyu Zhang et.al.|[2504.16046](http://arxiv.org/abs/2504.16046)|null|在预训练过程中，大型语言模型（LLMs）接触到受版权保护的材料引发了关于部署后无意中侵犯版权的担忧。这推动了“版权删除”方法的发展，这是一种训练后的方法，旨在防止模型生成与受版权保护的内容实质上相似的内容。虽然当前的缓解方法对平均风险有一定效果，但我们证明它们忽略了由受版权保护来源存在的长篇逐字引用所表现出的最坏情况版权风险。我们提出了BloomScrub，这是一种非常简单但非常有效的推理时方法，可以提供有保证的版权删除。我们的方法通过反复交错引用检测和重写技术来转换可能侵犯版权的段落。通过利用高效的数据草图（Bloom过滤器），我们的方法使得对大规模现实世界语料库的版权筛选成为可扩展的。当无法删除超过长度阈值的引用时，系统可以选择不响应，从而提供有保证的风险降低。实验结果表明，BloomScrub可以降低侵权风险，保持实用性，并通过自适应拒绝适应不同的执法严格程度。我们的结果表明，轻量级的推理时方法可以出人意料地有效地用于版权预防。|
|**2025-04-22**|**LLMs meet Federated Learning for Scalable and Secure IoT Management**|Yazan Otoum et.al.|[2504.16032](http://arxiv.org/abs/2504.16032)|null|随着物联网生态系统的快速扩张，带来了可扩展性、安全性和实时决策等方面的严峻挑战。传统的集中式架构在延迟、隐私问题和资源过度消耗方面存在困难，这使得它们不适合现代大规模物联网部署。本文提出了一种基于联邦学习的全新大型语言模型（FL-LLM）框架，旨在提升物联网系统的智能化水平，同时确保数据隐私和计算效率。该框架将生成式物联网（GIoT）模型与梯度感知联邦策略（GSFS）相结合，根据实时网络条件动态优化模型更新。通过利用混合边缘-云处理架构，我们的方法在分布式物联网环境中平衡了智能、可扩展性和安全性。在IoT-23数据集上的评估表明，我们的框架提高了模型精度，减少了响应延迟，并提升了能源效率，优于传统的联邦学习技术（例如FedAvg、FedOpt）。这些发现突出了将LLM驱动的联邦学习集成到大规模物联网生态系统中的潜力，为更安全、可扩展和自适应的物联网管理解决方案铺平了道路。|
|**2025-04-22**|**LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale**|Joya Chen et.al.|[2504.16030](http://arxiv.org/abs/2504.16030)|null|近期，视频大型语言模型（Video LLMs）通常依赖于昂贵的标注数据或专有模型API（例如GPT-4o）来生成训练数据，这限制了它们的大规模训练。在本文中，我们探讨了使用低成本的自动语音识别（ASR）字幕进行视频LLM的大规模训练。具体来说，我们提出了一种新颖的流式训练方法，该方法根据时间戳将ASR单词和视频帧进行密集交错。与之前在视觉-语言表示中使用ASR的研究相比，我们的方法自然地适应了ASR的流式特性，从而使得模型能够学习到时间对齐的、精细粒度的视觉-语言建模。为了支持训练算法，我们引入了一个数据处理管道来处理YouTube视频及其字幕（CC，与ASR相同），从而产生了用于预训练的Live-CC-5M数据集和用于高质量监督微调（SFT）的Live-WhisperX-526K数据集。值得注意的是，即使在没有SFT的情况下，仅使用ASR进行预训练的LiveCC-7B-Base模型也展示了有竞争力的通用视频问答性能，并展现了一种实时视频评论的新能力。为了评估这一点，我们精心设计了一个新的LiveSports-3K基准，使用LLM作为裁判来衡量自由形式的评论。实验表明，我们的最终LiveCC-7B-Instruct模型在实时模式下甚至能够超越高级72B模型（Qwen2.5-VL-72B-Instruct，LLaVA-Video-72B）的评论质量。同时，它在VideoMME和OVOBench等流行的视频问答基准测试中达到了7B/8B规模的最先进结果，展示了我们方法广泛的泛化能力。本文的所有资源已在https://showlab.github.io/livecc发布。|
|**2025-04-22**|**Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3**|Ahmed R. Sadik et.al.|[2504.16027](http://arxiv.org/abs/2504.16027)|null|确定最有效的代码异味检测大型语言模型是一项复杂挑战。本研究引入了一种结构化的方法和评估矩阵来解决这一问题，利用了一个经过精心整理的代码样本数据集，该数据集持续性地标注了已知的异味。该数据集涵盖了四种主要的编程语言：Java、Python、JavaScript和C++，从而实现了跨语言的比较。我们以精确率、召回率和F1分数作为评估指标，对两种最先进的LLM（大型语言模型）——OpenAI GPT 4.0和DeepSeek-V3进行了基准测试。我们的分析涵盖了三个层次的细节：整体性能、类别级性能和单个代码异味类型性能。此外，我们还通过比较GPT 4.0的基于标记的检测方法与DeepSeek V3采用的模式匹配技术，探讨了成本效益。该研究还包括了与传统静态分析工具（如SonarQube）相关的成本分析。研究结果为实践者提供了在选择高效、成本效益高的自动化代码异味检测解决方案方面的宝贵指导。|
|**2025-04-21**|**VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models**|Weiye Xu et.al.|[2504.15279](http://arxiv.org/abs/2504.15279)|null|视觉推理是人类智能的核心组成部分，也是高级多模态模型的关键能力。然而，当前多模态大型语言模型（MLLMs）的推理评估往往依赖于文本描述，并允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为了解决这个问题，我们引入了VisuLogic：一个包含1000个由人类验证的问题的基准，涵盖六个类别（例如，数量变化、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估，以评估MLLMs的视觉推理能力。我们在该基准上评估了领先的MLLMs，并分析了其结果以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类实现的51.4%——揭示了视觉推理中的显著差距。此外，我们还提供了一组补充训练数据集和一个强化学习基线，以支持进一步的进展。|
|**2025-04-21**|**Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs**|Chun-Hsiao Yeh et.al.|[2504.15280](http://arxiv.org/abs/2504.15280)|**[link](https://github.com/Chenyu-Wang567/All-Angles-Bench)**|多视角理解，即在不同视角下协调视觉信息以实现有效导航、操作和3D场景理解的能力，是用于作为具身代理的多模态大型语言模型（MLLMs）中的一个基本挑战。尽管最近的MLLMs在高级推理和规划方面取得了令人印象深刻的进步，但它们在面临多视角几何一致性和跨视角对应关系时往往表现不佳。为了全面评估MLLMs在多视角场景推理中的挑战，我们提出了All-Angles Bench，这是一个包含超过2,100对人精心标注的多视角问答对基准，涵盖了90个多样化的真实场景。我们的六个任务（计数、属性识别、相对距离、相对方向、物体操作和相机姿态估计）专门测试模型的几何对应能力和在视角间一致地对齐信息的能力。我们对27个代表性MLLMs（包括Gemini-2.0-Flash、Claude-3.7-Sonnet和GPT-4o）进行了广泛实验和基准测试，与人类评估者相比，揭示了显著的性能差距，表明当前的MLLMs距离人类水平的专业能力还有很长的路要走。通过深入分析，我们发现MLLMs在两个方面表现尤为不佳：（1）部分遮挡视角的跨视角对应关系和（2）建立粗略的相机姿态。这些发现强调了特定领域优化或嵌入更强多视角意识的模块的必要性。我们相信，我们的All-Angles Bench提供了宝贵的见解，有助于弥合MLLMs与人类水平的多视角理解之间的差距。该项目和基准可在https://danielchyeh.github.io/All-Angles-Bench/上公开获取。|
|**2025-04-21**|**Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning**|Jie Cheng et.al.|[2504.15275](http://arxiv.org/abs/2504.15275)|**[link](https://github.com/cjreinforce/pure)**|**过程奖励模型（PRMs）在测试时对大型语言模型（LLMs）在具有挑战性的推理任务上进行缩放方面已被证明是有效的。然而，PRMs的奖励黑客问题限制了它们在强化微调中的成功应用。在本文中，我们确定了PRM引起的奖励黑客的主要原因：强化学习（RL）中的规范求和形式信用分配，它将价值定义为累积伽马衰减的未来奖励，容易诱导LLMs黑掉高奖励的步骤。为了解决这个问题，我们提出了PURE：过程监督强化学习（Process sUpervised Reinforcement lEarning）。PURE的关键创新是一种最小形式信用分配，将价值函数定义为未来奖励的最小值。这种方法通过限制价值函数的范围和更合理地分配优势，显著减轻了奖励黑客。通过在3个基础模型上的广泛实验，我们表明基于PRM的方法，通过实现最小形式信用分配，在仅30%的步骤内就达到了可验证奖励方法的推理性能。相比之下，规范求和形式的信用分配甚至在训练开始时就会崩溃！此外，当我们仅用10%的可验证奖励补充基于PRM的微调时，我们进一步减轻了奖励黑客，并在我们的实验中产生了基于Qwen2.5-Math-7B的最佳微调模型，实现了AMC23上的82.5%准确率和5个基准测试的平均准确率53.3%。此外，我们总结了观察到的奖励黑客案例，并分析了训练崩溃的原因。代码和模型可在https://github.com/CJReinforce/PURE上找到。**|
|**2025-04-21**|**Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning**|Ehsan Ahmadi et.al.|[2504.15263](http://arxiv.org/abs/2504.15263)|null|建筑任务本质上具有不可预测性，动态环境和安全关键的要求给工人带来了重大风险。外骨骼提供潜在的辅助作用，但在跨不同行走模式的情况下缺乏准确意图识别则效果不佳。本文提出了一种利用大型语言模型（LLMs）并辅以记忆系统的行走预测代理，旨在提高此类环境中外骨骼的辅助效果。该代理使用多模态输入——包括口头指令和来自智能眼镜的视觉数据——集成了感知模块、短期记忆（STM）、长期记忆（LTM）和细化模块，以有效地预测行走模式。评估结果显示，在没有记忆的情况下，基准加权F1分数为0.73，随着STM的使用提升到0.81，同时使用STM和LTM时达到0.90，在模糊和安全关键指令方面表现出色。校准指标，包括从0.244降至0.090的布里尔分数和从0.222降至0.044的ECE，证实了可靠性的提升。这一框架支持更安全、高级的人机外骨骼协作，对动态行业中的自适应辅助系统具有前景。|
|**2025-04-21**|**CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation**|Anirudh Khatry et.al.|[2504.15254](http://arxiv.org/abs/2504.15254)|**[link](https://github.com/anirudhkhatry/crust-bench)**|**将C代码转换为Rust的翻译至关重要，它既能使遗留的C代码现代化，又能增强与现代Rust生态系统的兼容性。然而，目前还没有用于评估系统是否能够将C代码转换为通过一系列测试用例的安全Rust代码的数据集。我们引入了CRUST-Bench，这是一个包含100个C代码仓库的数据集，每个仓库都配有一组手动编写的安全Rust接口以及可以用来验证翻译正确性的测试用例。通过考虑整个仓库而不是孤立的功能，CRUST-Bench捕捉了跨多个文件具有依赖关系的复杂项目的翻译挑战。提供的Rust接口提供了明确的规范，确保遵循惯用的、内存安全的Rust模式，而伴随的测试用例则确保了功能正确性。我们评估了最先进的大型语言模型（LLMs）在此任务上的表现，并发现生成安全和惯用的Rust代码仍然是一个对各种最先进方法和技术的挑战。我们还提供了LLMs在将C代码翻译为安全Rust代码时通常犯错的见解。表现最佳的模型OpenAI o1在单次设置中仅能解决15个任务。对CRUST-Bench的改进将导致翻译系统得到改善，这些系统能够推理复杂场景，并有助于将遗留代码库从C迁移到如Rust这样的确保内存安全的语言。您可以在https://github.com/anirudhkhatry/CRUST-bench找到数据集和代码。**|
|**2025-04-21**|**Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators**|Yilun Zhou et.al.|[2504.15253](http://arxiv.org/abs/2504.15253)|**[link](https://github.com/salesforceairesearch/jetts-benchmark)**|**在测试时扩展计算量，或者为大型语言模型（LLM）在推理过程中提供额外的计算能力，通常需要借助外部非生成式评估器（即奖励模型）。同时，LLM-裁判员，即训练用于生成自然语言评价和批评（解释）的模型，在自动评估中越来越受欢迎。尽管裁判员在实证研究中取得了成功，但它们在测试时扩展设置中的评估效果在很大程度上仍然未知。在本文中，我们引入了用于测试时扩展的裁判员评估（JETTS）基准，该基准评估了裁判员在三个领域（数学推理、代码生成和指令遵循）以及三种任务设置下的表现：响应重排序、步骤级束搜索和基于批评的响应细化。我们评估了10个不同的裁判员模型（7B-70B参数）和8个不同的基础生成器模型（6.7B-72B参数）。我们的基准测试表明，尽管在重排序方面裁判员与结果奖励模型具有竞争力，但在束搜索过程中它们始终劣于过程奖励模型。此外，尽管LLM-裁判员独特，但它们用自然语言编写的批评目前无法有效引导生成器生成更好的响应。**|
|**2025-04-21**|**MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning**|Yahan Yang et.al.|[2504.15241](http://arxiv.org/abs/2504.15241)|null|大型语言模型（LLMs）容易受到如越狱等对抗攻击，这可能导致有害或不安全的行为。在多语言环境中，这一问题更为严重，因为与安全相关的多语言数据通常有限。因此，开发一种能够检测和过滤不同语言中不安全内容的护栏对于在现实应用中部署LLMs至关重要。在本研究中，我们提出了一种构建具有推理能力的多语言护栏的方法。我们的方法包括：（1）生成包含文化和语言细微差异的合成多语言数据，（2）监督微调，以及（3）一个由课程引导的群体相对策略优化（GRPO）框架，该框架进一步提升了性能。实验结果表明，我们的多语言护栏在领域内和领域外的语言上均优于最近的基线。我们护栏的多语言推理能力使其能够生成多语言解释，这在理解多语言内容监管中的语言特定风险和歧义方面特别有用。|
|**2025-04-21**|**EvalAgent: Discovering Implicit Evaluation Criteria from the Web**|Manya Wadhwa et.al.|[2504.15219](http://arxiv.org/abs/2504.15219)|null|在结构化写作任务的评估中，通常会将一系列理想标准呈现给人类评估者或大型语言模型（LLMs）。例如，在“帮我草拟一篇关于咖啡摄入量与科研效率的学术演讲”这样的提示下，模型响应可能会根据准确性和连贯性等标准进行评估。然而，高质量的响应不应仅仅满足基本任务要求。针对此查询的有效响应应包括学术演讲的典型特征，如引人入胜的开场、明确的研究问题和总结。为了帮助识别这些隐含的标准，我们引入了EvalAgent，这是一个旨在自动发现细微和特定任务标准的创新框架。EvalAgent首先挖掘专家撰写的在线指导。然后，它利用这些证据提出基于可靠外部来源的多样化、长尾评估标准。我们的实验表明，EvalAgent产生的基于事实的准则往往是隐含的（未直接在用户的提示中说明），但非常具体（具有高度的词汇精确度）。此外，EvalAgent的准则通常不在初始响应中得到满足，但它们是可操作的，即响应可以被改进以满足这些准则。最后，我们展示了将LLM生成的准则与EvalAgent准则相结合比单独使用LLMs能够揭示更多人类价值观的准则。|
|**2025-04-21**|**Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs**|Marina Sakharova et.al.|[2504.15210](http://arxiv.org/abs/2504.15210)|null|代码生成大型语言模型（LLMs）已成为现代软件开发中不可或缺的工具，提高了生产力和加速了开发进程。本文旨在研究使用强化学习和直接偏好优化对代码生成LLMs进行微调，以进一步提高其性能。为此，我们借助符号执行技术增强了奖励模型的训练数据，确保数据更加全面和客观。通过符号执行，我们创建了一个更好地捕捉代码评估细微差异的自定义数据集。在此数据集上微调的奖励模型在估计生成代码质量方面显著优于基线模型CodeRL。借助奖励模型反馈进行训练的代码生成LLMs在性能上与CodeRL基准相当。|
|**2025-04-21**|**Compute-Optimal LLMs Provably Generalize Better With Scale**|Marc Finzi et.al.|[2504.15208](http://arxiv.org/abs/2504.15208)|null|为什么更大的语言模型具有更好的泛化能力？为了研究这个问题，我们根据Chinchilla缩放定律，在计算最优的范畴内，对大型语言模型（LLM）的预训练目标开发了一般化界限。我们引入了一种新颖的、完全经验性的Freedman型鞅收敛不等式，通过考虑损失函数的方差来缩小现有的界限。这个一般化界限可以分解为三个可解释的组成部分：每个标记的参数数量、损失方差以及固定比特率下的量化误差。随着计算最优的语言模型规模扩大，每个数据点的参数数量保持不变；然而，损失方差和量化误差都会降低，这意味着更大的模型应该具有更小的泛化差距。我们从信息论的角度探讨了为什么更大的模型更容易量化，表明它们整合新信息的能力增长速度慢于其在计算最优前沿的容量。基于这些发现，我们制定了一般化差距的缩放定律，其界限随着规模的扩大而变得可预测地更强。|
|**2025-04-18**|**Generative AI Act II: Test Time Scaling Drives Cognition Engineering**|Shijie Xia et.al.|[2504.13828](http://arxiv.org/abs/2504.13828)|**[link](https://github.com/gair-nlp/cognition-engineering)**|第一代大型语言模型——可以称为生成式AI的“第一幕”（2020-2023）——通过大规模参数和数据扩展取得了显著的成就，但在知识延迟、浅层推理和认知过程受限方面仍存在根本性的局限。在这一时期，提示工程成为我们与AI交互的主要界面，通过自然语言实现对话级别的沟通。现在，我们见证了“第二幕”（2024年至今）的兴起，模型正通过测试时间缩放技术从知识检索系统（在潜在空间中）转变为思维构建引擎。这一新范式通过基于语言的思想与AI建立了心智层面的连接。在本文中，我们阐明了认知工程的观念基础，并解释了为什么这个时刻对其发展至关重要。我们通过全面的教程和优化实现系统地剖析了这些先进方法，使认知工程变得普及，让每位从业者都能参与AI的第二次演绎。我们在GitHub仓库中提供了一个定期更新的关于测试时间缩放的论文集合：https://github.com/GAIR-NLP/cognition-engine|
|**2025-04-18**|**Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models**|Junjie Yang et.al.|[2504.13825](http://arxiv.org/abs/2504.13825)|null|知识蒸馏（KD）是一种将知识从复杂的教师模型转移到简单的学生模型的技术，显著提高了模型的效率和准确性。它在包括图像分类、目标检测、语言建模、文本分类和情感分析等各个应用中均取得了显著进步。最近在KD方法上的创新，如基于注意力的方法、块状logit蒸馏和去耦合蒸馏，显著提高了学生模型的表现。这些技术侧重于刺激复杂性、注意力机制和全局信息捕获以优化知识传递。此外，KD在压缩大型语言模型的同时保持准确性的证明，降低了计算开销并提高了推理速度。本综述综合了最新的文献，突出了知识蒸馏在人工智能和机器学习中的关键发现、贡献和未来方向，为研究人员和实践者提供了其演变角色的见解。|
|**2025-04-18**|**Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning**|Yixuan Even Xu et.al.|[2504.13818](http://arxiv.org/abs/2504.13818)|null|强化学习（RL）已成为增强大型语言模型推理能力的一种强大范式，但其在计算和内存需求上存在根本的不对称性：推理具有极高的并行性且内存占用极小，而策略更新则需要广泛的同步且内存密集。为了解决这种不对称性，我们引入了PODS（政策优化与下采样）框架，该框架通过并行生成多个回放但只更新信息丰富的子集，战略性地解耦了这些阶段。在这个框架中，我们开发了最大方差下采样方法，这是一种理论上有动机的方法，它选择具有最大多样化奖励信号的回放。我们证明这种方法具有高效的算法解决方案，并通过实验证明，使用最大方差下采样的PODS GRPO在GSM8K基准测试中比标准GRPO表现出更优越的性能。|
|**2025-04-18**|**BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models**|Zhengxian Wu et.al.|[2504.13775](http://arxiv.org/abs/2504.13775)|null|之前基于插入和改写的后门攻击在攻击效果上取得了巨大成功，但它们忽略了毒化文本与清洁文本之间的文本质量和语义一致性。尽管最近的研究引入了大型语言模型（LLMs）来生成毒化文本并提高其隐蔽性、语义一致性和文本质量，但它们的手动提示依赖于专家经验，在防御之后面临着提示适应性和攻击性能的重大挑战。在本文中，我们提出了一种基于黑盒大型语言模型（LLMs）自适应优化机制的新型后门攻击（BadApex），该机制利用黑盒LLM通过精细的提示生成毒化文本。具体来说，设计了一种自适应优化机制，通过生成和修改代理迭代地细化初始提示。生成代理基于初始提示生成毒化文本。然后，修改代理评估毒化文本的质量并细化新的提示。经过上述过程的多次迭代后，使用细化后的提示通过LLMs生成毒化文本。我们在三个数据集上进行了广泛的实验，包括六种后门攻击和两种防御方法。广泛的实验结果表明，BadApex在攻击效果上显著优于现有攻击。它提高了提示适应性、语义一致性和文本质量。此外，当应用两种防御方法时，平均攻击成功率（ASR）仍高达96.75%。|
|**2025-04-18**|**DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs**|Tamim Al Mahmud et.al.|[2504.13774](http://arxiv.org/abs/2504.13774)|**[link](https://github.com/tamimalmahmud/DP2Unlearning)**|大型语言模型（LLMs）最近在语言处理任务中实现了革命性的进步，但也带来了伦理和法律问题。LLMs倾向于记住训练数据中可能存在的私人或受版权保护的信息，这些信息可能在推理时传递给最终用户。当这种情况发生时，一个简单的解决方案是在排除不需要的数据后从头开始重新训练模型。虽然这保证了目标数据已被遗忘，但对于LLMs来说代价过高。近似无学习提供了一种更高效的替代方案，因为它涉及对训练模型本身的后续修改，以防止出现不希望的结果，但它缺乏遗忘保证，因为它仅依赖于经验证据。在这项工作中，我们提出了DP2Unlearning，这是一种新颖的LLM无学习框架，在比从头开始重新训练保留数据的成本显著降低的情况下提供正式的遗忘保证。DP2Unlearning涉及使用ε差分隐私（DP）对文本数据进行LLM训练，这随后使得能够以与所选ε相关的披露保证进行高效的无学习。我们的实验表明，DP2Unlearning在无学习后的模型性能与从头开始重新训练保留数据的LLM（无学习标准）相似，但无学习成本大约是其一半。此外，在合理的计算成本下，它既在无学习后保持模型的有用性方面，又在有效遗忘目标信息方面，都优于近似无学习方法。|
|**2025-04-18**|**Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG Come in Handy?**|Motunrayo Ibiyo et.al.|[2504.13769](http://arxiv.org/abs/2504.13769)|null|在开源生态系统中，如PyPI的恶意软件包带来了日益增长的网络安全风险。与传统的漏洞不同，这些包是故意设计来欺骗用户的，由于攻击方法不断演变和缺乏结构化数据集，这使得检测变得极具挑战性。在本研究中，我们实证评估了大型语言模型（LLMs）、检索增强生成（RAG）和少样本学习在检测恶意源代码方面的有效性。我们针对精选的数据集微调了LLMs，并整合了YARA规则、GitHub安全通告和恶意代码片段，旨在提高分类精度。我们发现了一个出人意料的成果：虽然预期RAG会提升预测性能，但在进行的评估中，它却只获得了中等水平的准确率。相比之下，少样本学习更为有效，它显著提高了恶意代码的检测率，达到了97%的准确率和95%的平衡准确率，超过了传统的RAG方法。因此，未来的工作应扩展结构化知识库，优化检索模型，并探索混合AI驱动的网络安全解决方案。|
|**2025-04-18**|**Scaling sparse feature circuit finding for in-context learning**|Dmitrii Kharlapenko et.al.|[2504.13756](http://arxiv.org/abs/2504.13756)|null|稀疏自编码器（SAEs）是用于解释大型语言模型激活的一种流行工具，但其在解决可解释性开放问题方面的效用尚不明确。在这项工作中，我们通过使用SAEs来深化我们对上下文学习（ICL）机制的理解，证明了其有效性。我们确定了抽象的SAE特征，这些特征（i）编码了模型执行哪个任务的知识，以及（ii）其潜在向量因果地引发了零样本的任务。这与先前的研究结果一致，表明ICL是通过任务向量来介导的。我们进一步证明，这些任务向量可以通过SAE潜在向量的稀疏和来很好地近似，包括这些任务执行特征。为了探索ICL机制，我们将Marks等人（2024）的稀疏特征电路方法调整为适用于更大的Gemma-1 2B模型，该模型具有30倍多的参数，并适用于更复杂的ICL任务。通过电路发现，我们发现了具有对应SAE潜在向量的任务检测特征，这些特征在提示中激活较早，用于检测任务是否已经执行。它们通过注意力和MLP子层与任务执行特征因果相关联。|
|**2025-04-18**|**Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence**|Paul K. Mandal et.al.|[2504.13730](http://arxiv.org/abs/2504.13730)|**[link](https://github.com/paulkmandal/contact)**|开源情报提供了大量非结构化文本数据，可用于评估领土控制。我们提出了CONTACT框架，该框架利用大型语言模型（LLMs）和最小监督进行领土控制预测。我们评估了两种方法：SetFit，一种基于嵌入的少样本分类器，以及应用于BLOOMZ-560m的多语言生成LLM的提示调整方法。我们的模型在涵盖叙利亚和伊拉克ISIS活动的新闻文章的小型人工标注数据集上训练，使用提示条件提取控制相关信号，如军事行动、伤亡和地点引用。我们表明，基于BLOOMZ的模型优于SetFit基线，并且基于提示的监督在低资源环境中提高了泛化能力。CONTACT证明了使用少样本方法微调的LLMs可以减少标注负担并支持从开放式OSINT流中进行结构化推理。我们的代码可在https://github.com/PaulKMandal/CONTACT/上获取。|
|**2025-04-18**|**OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation**|Yichen Wu et.al.|[2504.13707](http://arxiv.org/abs/2504.13707)|null|随着大型语言模型（LLM）的通用能力提高，智能体应用变得越来越广泛，其潜在的欺骗风险迫切需要系统评估和有效监管。与现有使用模拟游戏或提供有限选择的评估方法不同，我们引入了OpenDeception，这是一个具有开放式场景数据集的新颖欺骗评估框架。OpenDeception通过检查其内部推理过程，共同评估基于LLM的智能体的欺骗意图和能力。具体来说，我们构建了五种常见的使用场景，其中LLM与用户进行密集交互，每种场景都包含来自现实世界的十个多样化、具体的场景。为了避免与人类测试者进行高风险欺骗性交互的伦理问题和成本，我们提议通过智能体模拟来模拟多轮对话。对OpenDeception上十一种主流LLM的广泛评估突出了在基于LLM的智能体中解决欺骗风险和安全问题的紧迫性：模型间的欺骗意图比例超过80%，欺骗成功率超过50%。此外，我们还观察到能力更强的LLM确实表现出更高的欺骗风险，这需要更多努力来抑制欺骗行为。|
|**2025-04-18**|**Exploring Multimodal Prompt for Visualization Authoring with Large Language Models**|Zhen Wen et.al.|[2504.13700](http://arxiv.org/abs/2504.13700)|null|近年来，大型语言模型（LLMs）在通过简单的自然语言表述自动化可视化创作过程中展现出巨大潜力。然而，使用自然语言指令LLMs在传达可视化意图方面存在精度和表达力的限制，导致误解和耗时迭代。为了解决这些限制，我们进行了一项实证研究，以了解LLMs在可视化创作背景下如何解释模糊或不完整的文本提示，以及导致LLMs误解用户意图的条件。基于研究结果，我们引入了视觉提示作为文本提示的补充输入模式，这有助于阐明用户意图并提高LLMs的解释能力。为了探索多模态提示在可视化创作中的潜力，我们设计了VisPilot，它使用户能够轻松地使用包括文本、草图和直接操作现有可视化在内的多模态提示来创建可视化。通过两个案例研究和一项控制用户研究，我们证明了与仅文本提示的方法相比，VisPilot提供了一种更直观的创建可视化方式，同时不影响整体任务效率。此外，我们分析了文本和视觉提示在不同可视化任务中的影响。我们的发现强调了多模态提示在提高LLMs可视化创作可用性方面的重要性。我们讨论了未来可视化系统的设计启示，并提供了关于如何通过多模态提示增强创意可视化任务中的人机协作的见解。所有材料均可在https://OSF.IO/2QRAK上获取。|
|**2025-04-17**|**Sleep-time Compute: Beyond Inference Scaling at Test-time**|Kevin Lin et.al.|[2504.13171](http://arxiv.org/abs/2504.13171)|**[link](https://github.com/letta-ai/sleep-time-compute)**|**将测试时计算扩展已成为使大型语言模型（LLMs）能够解决困难问题的一个关键因素，但这也带来了高延迟和推理成本。我们引入了睡眠时间计算，允许模型在查询提出之前离线地“思考”上下文：通过预测用户可能会提出什么查询并预先计算有用量，我们可以显著减少测试时的计算需求。为了展示我们方法的有效性，我们创建了两个推理任务——有状态的GSM-Symbolic和有状态的AIME的修改版本。我们发现，睡眠时间计算可以将达到相同准确性的测试时计算量减少约5倍，并且通过扩展睡眠时间计算，我们可以在有状态的GSM-Symbolic上提高准确率高达13%，在有状态的AIME上提高18%。此外，我们引入了多查询GSM-Symbolic，它通过在每个上下文中包含多个相关查询来扩展GSM-Symbolic。通过使用多查询GSM-Symbolic将睡眠时间计算分配到关于同一上下文的相关查询，我们可以将每个查询的平均成本降低2.5倍。然后，我们进行了额外的分析，以了解何时睡眠时间计算最为有效，发现用户查询的可预测性与睡眠时间计算的有效性有很好的相关性。最后，我们进行了一项案例研究，将睡眠时间计算应用于一个现实的代理服务工程（SWE）任务。**|
|**2025-04-17**|**Exploring Expert Failures Improves LLM Agent Tuning**|Li-Cheng Lan et.al.|[2504.13145](http://arxiv.org/abs/2504.13145)|null|大型语言模型（LLMs）在作为代理方面展现出巨大的潜力，擅长处理需要多轮推理和交互的任务。拒绝采样微调（RFT）已成为微调LLMs作为代理的有效方法：它首先模仿专家生成的成功轨迹，并通过在成功的、自行生成的轨迹上进行迭代微调来进一步提高代理技能。然而，由于专家（例如，GPT-4）主要在更简单的子任务上成功，而RFT本质上偏向于简单场景，许多复杂子任务仍未解决且持续出现异常分布（OOD）。在调查这些具有挑战性的子任务时，我们发现先前失败的专家轨迹常常能提供有价值的指导，例如计划和关键动作，这些可以显著提高代理的探索效率和关键技能的获取。受这些观察的启发，我们提出了探索专家失败（EEF）方法，该方法从失败的专家轨迹中识别出有益的动作，并将它们整合到训练数据集中。潜在的有害动作被精心排除，以防止对模型学习过程的污染。通过利用专家失败中的有益动作，EEF成功解决了先前无法解决的子任务，并提高了代理调优性能。值得注意的是，我们的方法在WebShop中的胜率达到62%，超过了RFT（53.6%）和GPT-4（35.6%），并且据我们所知，成为第一个在WebShop中得分超过0.81并在SciWorld中超过81分的突破性方法。|
|**2025-04-17**|**Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo**|João Loula et.al.|[2504.13139](http://arxiv.org/abs/2504.13139)|null|在众多语言模型（LM）应用中，需要生成符合句法或语义约束的文本。对这类约束的自然处理可以视为概率条件化，但从中精确生成结果——其分布可能显著不同于LM的基础分布——通常是无法处理的。在本工作中，我们开发了一种基于顺序蒙特卡罗（SMC）的受控LM生成架构。我们的SMC框架使我们能够在推理时灵活地融入特定领域和问题的约束，并在生成过程中根据新信息有效地重新分配计算资源。通过在四个具有挑战性的领域——数据科学中的Python代码生成、文本到SQL、目标推理和分子合成——与多种替代方案和消融实验进行比较，我们证明，在我们的方法中，开销很小，小型开源语言模型能够超越超过8倍规模的大型模型，甚至优于闭源经过微调的模型。为了支持概率视角，我们展示了这些性能提升是由对后验分布的更好近似驱动的。我们的系统建立在Lew等（2023）的框架之上，并与其语言模型概率编程语言集成，为用户提供了一种简单、可编程的方式来将SMC应用于广泛的受控生成问题。|
|**2025-04-17**|**Energy-Based Reward Models for Robust Language Model Alignment**|Anamika Lochab et.al.|[2504.13134](http://arxiv.org/abs/2504.13134)|**[link](https://github.com/AnamikaLochab/EBRM)**|奖励模型（RMs）对于使大型语言模型（LLMs）与人类偏好保持一致至关重要。然而，它们通常难以捕捉复杂的人类偏好并泛化到未见数据。为了解决这些挑战，我们引入了基于能量的奖励模型（EBRM），这是一种轻量级的后处理优化框架，可以增强RMs的鲁棒性和泛化能力。EBRM明确地建模奖励分布，捕捉人类偏好的不确定性，并减轻噪声或不匹配注释的影响。它通过冲突感知的数据过滤、标签噪声感知的对比训练和混合初始化来实现这一点。值得注意的是，EBRM在不重新训练的情况下增强了RMs，使其计算效率高且适用于不同的模型和任务。在RM基准测试上的实证评估表明，在鲁棒性和泛化方面都有显著提高，与标准RMs相比，在安全关键的对齐任务上实现了高达5.97%的提升。此外，强化学习实验证实，我们优化的奖励增强了对齐质量，有效地延缓了奖励黑客行为。这些结果证明了我们的方法作为现有RMs和对齐管道的可扩展且有效的增强方案。代码可在EBRM处获取。|
|**2025-04-17**|**LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard**|Varun Rao et.al.|[2504.13125](http://arxiv.org/abs/2504.13125)|null|这篇论文研究了大型语言模型（LLMs）在金融任务中的应用。我们以Open FinLLM排行榜为基准，对基础模型进行了微调。基于Qwen2.5和Deepseek-R1，我们采用了包括监督微调（SFT）、直接偏好优化（DPO）和强化学习（RL）等技术来增强它们的金融能力。微调后的模型在广泛的金融任务中展示了显著的性能提升。此外，我们还测量了金融领域的数据缩放规律。我们的工作展示了大型语言模型（LLMs）在金融应用中的潜力。|
|**2025-04-17**|**Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training**|Xinsong Zhang et.al.|[2504.13123](http://arxiv.org/abs/2504.13123)|null|近年来，视觉-语言模型预训练领域取得了迅速发展，这主要得益于大型语言模型文本能力的持续提升。然而，现有的多模态大型语言模型训练范式高度依赖高质量的图像-文本对。随着模型和数据规模呈指数级增长，这种精心策划的数据的可获得性越来越稀缺，并趋于饱和，从而严重限制了该领域的进一步发展。本研究探讨了可扩展的视觉-语言模型预训练的标题生成技术，并证明大规模低幻觉合成标题可以发挥双重作用：1）作为预训练范式现实世界数据的可行替代品；2）通过实证验证，将其集成到视觉-语言模型中可以获得优越的性能提升。本文提出了三个关键贡献：1）一种生成高质量、低幻觉和知识丰富的合成标题的新流程。我们的连续DPO方法在减少幻觉方面取得了显著成果。具体来说，一个7B规模的模型在保留测试集中的非幻觉标题率从48.2%提高到77.9%。2）全面的实证验证显示，我们的合成标题比其对应物提供了优越的预训练优势。在35个视觉语言任务中，使用我们的数据进行训练的模型，相较于替代文本对和其他先前工作，实现了至少6.2%的性能提升。同时，在文本到图像领域也提供了相当大的支持。使用我们的数据集，在真实世界验证基准上的FID得分降低了17.1，在MSCOCO验证基准上降低了13.3。3）我们将发布一个低幻觉和高知识密集型的合成标题数据集—— Hunyuan-Recap100M。|
|**2025-04-17**|**VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models**|Haojian Huang et.al.|[2504.13122](http://arxiv.org/abs/2504.13122)|**[link](https://github.com/haroldchen19/vistadpo)**|**大型视频模型（LVMs）基于大型语言模型（LLMs）在视频理解方面展现出潜力，但往往存在与人类直觉不符和视频幻觉问题。为了解决这些挑战，我们提出了VistaDPO，这是一种新的视频分层时空直接偏好优化框架。VistaDPO在三个层次上增强了文本-视频偏好对齐：i）实例级，将整体视频内容与回应对齐；ii）时间级，将视频时间语义与事件描述对齐；iii）感知级，将空间对象与语言标记对齐。鉴于缺乏细粒度视频-语言偏好对齐的数据集，我们构建了VistaDPO-7k，这是一个包含7.2K个问答对的数据集，这些问答对附带了选中和被拒绝的回应，以及如时间戳、关键帧和边界框等时空定位信息。在视频幻觉、视频问答和字幕性能等基准任务上的大量实验表明，VistaDPO显著提高了现有LVMs的性能，有效缓解了视频-语言不匹配和幻觉问题。代码和数据可在https://github.com/HaroldChen19/VistaDPO获取。**|
|**2025-04-17**|**Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification**|Kumar Manas et.al.|[2504.13111](http://arxiv.org/abs/2504.13111)|**[link](https://github.com/kumarmanas/SHIFT)**|基于深度学习的轨迹预测模型在捕捉复杂交互方面表现出良好的能力。然而，它们在分布外泛化方面仍然面临重大挑战，尤其是由于数据不平衡以及缺乏足够的数据和多样性来确保鲁棒性和校准。为了解决这个问题，我们提出了SHIFT（用于轨迹的频谱异方差信息预测），这是一个独特的框架，它将精确校准的不确定性建模与通过自动化规则提取获得的先验信息相结合。SHIFT将轨迹预测重新定义为分类任务，并采用异方差谱归一化高斯过程有效地分解了认识论和随机不确定性。我们从训练标签中学习先验信息，这些标签自动从自然语言驾驶规则中生成，如停车规则和可驾驶性约束，使用由大型语言模型驱动的检索增强生成框架。在nuScenes数据集上进行的广泛评估，包括具有挑战性的低数据量和跨地点场景，表明SHIFT优于最先进的方法，在不确定性校准和位移指标方面实现了显著提升。特别是，我们的模型在复杂的场景中表现出色，例如交叉路口，那里的不确定性本质上更高。项目页面：https://kumarmanas.github.io/SHIFT/。|
|**2025-04-17**|**EventVAD: Training-Free Event-Aware Video Anomaly Detection**|Yihua Shao et.al.|[2504.13092](http://arxiv.org/abs/2504.13092)|null|视频异常检测（VAD）关注于识别视频中的异常情况。监督方法需要一定量的领域训练数据，并且往往难以泛化到未见过的异常。相比之下，无需训练的方法利用大型语言模型（LLMs）的内生世界知识来检测异常，但面临在定位精细的视觉过渡和多样化事件方面的挑战。因此，我们提出了EventVAD，一个事件感知的视频异常检测框架，该框架通过时间事件推理结合定制的动态图架构和多模态LLMs。具体来说，EventVAD首先采用带有时衰减约束的动态时空图建模来捕捉事件感知的视频特征。然后，它执行自适应噪声过滤，并使用信号比阈值来确定事件边界，通过无监督统计特征进行。统计边界检测模块降低了处理长视频的复杂度，并通过事件一致性提高了MLLMs的时间推理。最后，它利用分层提示策略指导MLLMs在做出最终决策之前进行推理。我们在UCF-Crime和XD-Violence数据集上进行了广泛的实验。结果表明，使用7B MLLM的EventVAD在无需训练的设置中实现了最先进的性能，优于使用7B或更大MLLMs的强大基线。|
|**2025-04-17**|**Retrieval-Augmented Generation with Conflicting Evidence**|Han Wang et.al.|[2504.13079](http://arxiv.org/abs/2504.13079)|**[link](https://github.com/hannight/ramdocs)**|**大型语言模型（LLM）代理越来越多地采用检索增强生成（RAG）来提高其回答的真实性。然而，在实际应用中，这些系统通常需要处理模糊的用户查询，同时处理来自多个来源的潜在冲突信息，并抑制来自嘈杂或不相关文档中的不准确信息。先前的工作通常孤立地研究和解决这些挑战，一次只考虑一个方面，如处理模糊性或对噪声和错误信息的鲁棒性。我们则同时考虑多个因素，提出以下两点：（i）RAMDocs（在文档中的模糊性和错误信息检索），一个新的数据集，模拟了用户查询冲突证据的复杂和现实场景，包括模糊性、错误信息和噪声；（ii）MADAM-RAG，一种多代理方法，其中LLM代理在多轮中就答案的优点进行辩论，允许聚合器收集对应于消歧实体的响应，同时丢弃错误信息和噪声，从而共同处理冲突的多种来源。我们通过在AmbigDocs（需要呈现模糊查询的所有有效答案）上使用封闭和开源模型展示了MADAM-RAG的有效性，改进了强大的RAG基线，最高提高了11.40%，在FaithEval（需要抑制错误信息）上，使用Llama3.3-70B-Instruct改进了最高15.80%（绝对值）。此外，我们发现RAMDocs对现有的RAG基线构成了挑战（Llama3.3-70B-Instruct仅获得32.60精确匹配分数）。虽然MADAM-RAG开始解决这些冲突因素，但我们的分析表明，特别是在增加支持证据和错误信息不平衡程度时，仍存在较大的差距。**|
|**2025-04-16**|**BitNet b1.58 2B4T Technical Report**|Shuming Ma et.al.|[2504.12285](http://arxiv.org/abs/2504.12285)|null|我们介绍了BitNet b1.58 2B4T，这是第一个开源、本地的1位大型语言模型（LLM），参数规模达到20亿。该模型在包含4000亿个标记的语料库上进行了训练，并在涵盖语言理解、数学推理、编码能力和对话能力等方面的基准测试中进行了严格评估。我们的结果表明，BitNet b1.58 2B4T在性能上与同类规模的领先开源全精度LLM相当，同时提供了显著的计算效率优势，包括大幅减少的内存占用、能耗和解码延迟。为了促进进一步的研究和应用，我们通过Hugging Face发布了模型权重，并提供了针对GPU和CPU架构的开源推理实现。|
|**2025-04-16**|**HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks**|Stefan Abi-Karam et.al.|[2504.12268](http://arxiv.org/abs/2504.12268)|**[link](https://github.com/stefanpie/hls-eval)**|大规模语言模型（LLM）的训练和推理速度的快速发展推动了其在学术界和工业界在半导体设计领域的应用。虽然大多数先前的工作都是在硬件描述语言（HDL）任务上评估LLM，特别是Verilog，但设计师们越来越使用高级综合（HLS）来构建特定领域的加速器和复杂的硬件系统。然而，用于全面评估LLM在HLS设计任务上的基准和工具仍然稀缺。为了解决这个问题，我们引入了HLS-Eval，这是第一个针对由LLM驱动的HLS设计的完整基准和评估框架。HLS-Eval针对两个核心任务：（1）从自然语言描述中生成HLS代码，（2）执行针对HLS特定代码的编辑以优化性能和硬件效率。该基准包括从标准HLS基准和新型来源中抽取的94个独特设计。每个案例都通过半自动流程准备，该流程产生自然语言描述和配套的测试平台，用于C模拟和综合验证，确保每个任务都是“LLM就绪”的。除了基准测试之外，HLS-Eval提供了一个模块化的Python框架，用于自动、并行评估本地和托管的LLM。它包括并行评估引擎、直接集成HLS工具，以及支持不同LLM交互范式的抽象，从而能够快速原型设计新的基准、任务和LLM方法。我们通过在Vitis HLS上对开源LLM的基准评估来展示HLS-Eval，测量了四个关键指标——可解析性、可编译性、可运行性和可综合性——反映了迭代HLS设计周期。我们还报告了pass@k指标，为LLM-for-hardware社区建立了清晰的基准和可重用的基础设施。所有基准、框架代码和结果均开源在https://github.com/stefanpie/hls-eval上。|
|**2025-04-16**|**FLIP Reasoning Challenge**|Andreas Plesner et.al.|[2504.12256](http://arxiv.org/abs/2504.12256)|**[link](https://github.com/aplesner/flip-reasoning-challenge)**|在过去的几年里，人工智能（AI）的发展展示了AI如何解决许多感知和生成任务，如图像分类和文本写作，然而推理仍然是一个挑战。本文介绍了FLIP数据集，这是一个基于人类在Idena区块链上验证任务来评估AI推理能力的基准。FLIP挑战向用户提供两组4张图像的顺序，要求他们识别出逻辑上连贯的那一组。FLIP通过强调顺序推理、视觉叙事和常识，为多模态AI系统提供了一个独特的测试平台。我们的实验评估了最先进的模型，利用了视觉语言模型（VLMs）和大型语言模型（LLMs）。结果表明，即使在零样本设置下，即使是最优秀的开源和闭源模型，其最大准确率也分别达到75.5%和77.9%，而人类的准确率为95.3%。提供图像文本描述的标题模型通过帮助推理模型提高了性能，其结果比直接使用原始图像要好，Gemini 1.5 Pro模型分别达到69.6%和75.2%。将15个模型的预测结果进行集成可以进一步提高准确率至85.2%。这些发现突显了现有推理模型的局限性以及像FLIP这样的鲁棒多模态基准的必要性。完整的代码库和数据集将在https://github.com/aplesner/FLIP-Reasoning-Challenge上提供。|
|**2025-04-16**|**AnomalyGen: An Automated Semantic Log Sequence Generation Framework with LLM for Anomaly Detection**|Xinyu Li et.al.|[2504.12250](http://arxiv.org/abs/2504.12250)|null|高质量公共日志数据集的稀缺性已成为推进基于日志的异常检测技术进步的关键瓶颈。现有的数据集存在三个基本局限性：（1）事件覆盖不完整，（2）由基于静态分析的生成框架引入的人工模式，（3）语义意识不足。为了解决这些挑战，我们提出了AnomalyGen，这是第一个专门为异常检测设计的自动化日志合成框架。我们的框架引入了一种新颖的四阶段架构，该架构将增强的程序分析与思维链推理（CoT推理）相结合，使得在不需要物理系统执行的情况下实现迭代日志生成和异常标注。在Hadoop和HDFS分布式系统上的评估表明，AnomalyGen实现了显著更广泛的日志事件覆盖（比现有数据集提高了38-95倍），同时与基于静态分析的方法相比，产生了更具有操作现实性的日志序列。当将合成的日志添加到基准数据集中时，我们观察到F1分数的最大提升为3.7%（平均提升1.8%，涵盖了三种最先进的异常检测模型）。这项工作不仅建立了一个高质量的自动化日志分析基准资源，而且还开创了在软件工程工作流程中应用大型语言模型（LLMs）的新范式。|
|**2025-04-16**|**MOS: Towards Effective Smart Contract Vulnerability Detection through Mixture-of-Experts Tuning of Large Language Models**|Hang Yuan et.al.|[2504.12234](http://arxiv.org/abs/2504.12234)|null|智能合约漏洞给区块链系统带来了重大的安全风险，可能导致严重的经济损失。现有方法存在几个局限性：（1）基于程序分析的方法依赖于预定义的模式，缺乏对新漏洞类型的灵活性；（2）基于深度学习的方法缺乏解释；（3）基于大型语言模型的方法存在高误报率。我们提出了MOS，一个基于混合专家调整（MOE-Tuning）的大型语言模型智能合约漏洞检测框架。首先，我们在大规模智能合约数据集上进行持续预训练，以提供领域增强的初始化。其次，我们通过一个多阶段管道构建了一个高质量的MOE-Tuning数据集，该管道结合了LLM生成和专家验证，以提供可靠的解释。第三，我们设计了一个漏洞感知路由机制，通过分析代码特征及其与专家的匹配度来激活最相关的专家网络。最后，我们将前馈层扩展到多个并行专家网络，每个网络专注于特定的漏洞模式。我们采用了一个双重目标损失函数：一个用于优化检测和解释性能，另一个通过熵计算确保漏洞类型在专家中的合理分布。实验表明，MOS在F1分数上平均提高了6.32%，在准确率上提高了4.80%，显著优于现有方法。漏洞解释通过人类和LLM评估获得了积极的评分（4点评分中的3-4分），正确性、完整性和简洁性的评分分别为82.96%、85.21%和94.58%。|
|**2025-04-16**|**Watermarking Needs Input Repetition Masking**|David Khachaturov et.al.|[2504.12229](http://arxiv.org/abs/2504.12229)|null|近期大型语言模型（LLMs）的进步引发了对其潜在滥用的担忧，例如用于传播虚假信息。作为回应，出现了两种对抗措施：基于机器学习的检测器，可以预测文本是否为合成，以及LLM水印，它微妙地为生成文本打上标记以便识别和归因。同时，人们已知会根据对话对象在句法和词汇上调整语言。由此推断，人类或未打上水印的LLMs可能无意中模仿LLM生成文本的特性，使得对抗措施不可靠。在这项工作中，我们研究了这种对话适应发生的程度。我们称之为“模仿”，并证明人类和LLMs最终都会模仿，包括在看似不可能的情况下模仿水印信号。这挑战了当前的学术假设，并表明为了长期水印的可靠性，误报的可能性需要显著降低，同时应该使用更长的词序列来播种水印机制。|
|**2025-04-16**|**d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning**|Siyan Zhao et.al.|[2504.12216](http://arxiv.org/abs/2504.12216)|null|近期的大型语言模型（LLMs）展现了强大的推理能力，这得益于在线强化学习（RL）。这些能力主要在从左到右的自动回归（AR）生成范式下得到体现。相比之下，基于扩散的非自动回归范式以粗到细的方式生成文本。尽管近期基于扩散的大型语言模型（dLLMs）在与它们的AR对应模型相比时取得了有竞争力的语言建模性能，但dLLMs是否也能利用LLMs推理方面的最新进展尚不清楚。为此，我们提出了d1，一个通过结合监督微调（SFT）和RL将预训练的掩码dLLMs调整为推理模型的框架。具体来说，我们开发和扩展了提高预训练dLLMs推理的技术：（a）我们利用掩码SFT技术从现有数据集中提炼知识并直接培养自我改进行为；（b）我们引入了一种新颖的无批评家、基于策略梯度的RL算法，称为diffu-GRPO。通过实证研究，我们调查了多种数学和逻辑推理基准上不同后训练方法的性能。我们发现d1取得了最佳性能，并显著提高了最先进的dLLM的性能。|
|**2025-04-16**|**What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure**|Céline Budding et.al.|[2504.12187](http://arxiv.org/abs/2504.12187)|null|有时人们认为大型语言模型（LLMs）懂得语言，或者例如知道巴黎是法国的首都。但是，LLMs实际上知道什么——如果有什么的话？在本文中，我提出LLMs可以获取Martin Davies（1990年）定义的隐性知识。尽管Davies本人否认神经网络可以获取隐性知识，但我证明了LLMs的某些架构特征符合语义描述、句法结构和因果系统性的约束。因此，隐性知识可能成为描述、解释和干预LLMs及其行为的概念框架。|
|**2025-04-16**|**SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data**|Suyoung Bae et.al.|[2504.12185](http://arxiv.org/abs/2504.12185)|null|在各个自然语言处理（NLP）任务中，微调预训练语言模型（PLM）常常导致虚假相关性的问题，这会负面影响性能，尤其是在处理分布外数据时。为了解决这个问题，我们提出了SALAD（结构感知和LLM驱动的增强数据），这是一种新颖的方法，旨在通过为对比学习生成结构感知和反事实增强数据来增强模型的鲁棒性和泛化能力。我们的方法利用基于标记的方法生成结构感知的正样本，并利用大型语言模型（LLM）生成具有多样化句子模式的反事实负样本。通过应用对比学习，SALAD使模型能够专注于学习关键句子组件之间的结构关系，同时最大限度地减少对虚假相关性的依赖。我们通过在三个任务上的实验验证了我们的方法：情感分类、性别歧视检测和自然语言推理。结果表明，SALAD不仅提高了模型在不同环境下的鲁棒性和性能，还增强了分布外数据集和跨域场景的泛化能力。|
|**2025-04-16**|**Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification**|Jaime E. Cuellar et.al.|[2504.12180](http://arxiv.org/abs/2504.12180)|null|当前社会科学领域的一个基本问题是：我们能在多大程度上信任像ChatGPT这样高度复杂的预测模型？本研究检验了这样一个假设：在提示结构的微妙变化并不会对由大型语言模型GPT-4o mini生成的情感极性分析分类结果产生显著影响。使用包含关于四位拉丁美洲总统的10万条西班牙语评论的数据集，模型在10次分类中，每次都略微改变提示，将评论分类为正面、负面或中性。实验方法包括探索性和验证性分析，以识别分类之间的显著差异。结果表明，即使是细微的提示修改，如词汇、句法或语态的变化，甚至结构上的缺失，都会影响分类。在某些情况下，模型产生了不一致的回应，例如混合类别、提供未经请求的解释或使用除西班牙语以外的语言。使用卡方检验的统计分析证实，除了在一种语言结构高度相似的情况下，大多数提示之间的比较都存在显著差异。这些发现挑战了大型语言模型在分类任务中的鲁棒性和可信度，强调了它们对指令变化的脆弱性。此外，明显的是，提示中缺乏结构化语法增加了幻觉出现的频率。讨论强调了，对大型语言模型的信任不仅基于技术性能，还基于其使用的社交和制度关系。|
|**2025-04-15**|**TextArena**|Leon Guertler et.al.|[2504.11442](http://arxiv.org/abs/2504.11442)|**[link](https://github.com/leonguertler/textarena)**|TextArena是一个开源的基于文本的竞技游戏集合，用于训练和评估大型语言模型（LLMs）的代理行为。它包含57个以上的独特环境（包括单人、双人和多人设置），并允许通过在线游戏系统（与人类和其他提交的模型对战）轻松评估模型能力，并实时显示TrueSkill分数。传统的基准测试很少评估动态社交技能，如谈判、心智理论和欺骗，而TextArena正好填补了这一空白。TextArena的设计考虑了研究、社区和可扩展性，强调添加新游戏、调整框架、测试模型、与其他模型对战以及训练模型时的便捷性。关于环境、游戏、排行榜和示例的详细文档可在https://github.com/LeonGuertler/TextArena和https://www.textarena.ai/上找到。|
|**2025-04-15**|**Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models**|Maria Teleki et.al.|[2504.11431](http://arxiv.org/abs/2504.11431)|**[link](https://github.com/mariateleki/masculine-defaults)**|男性默认是广泛认可的性别偏见的一种重要类型，但它们通常因为研究不足而看不见。男性默认包含三个关键部分：（一）文化背景，（二）男性特征或行为，（三）对这些男性特征或行为的奖励或简单接受。在这项工作中，我们研究了基于话语的男性默认，并提出了一个双重框架：（一）通过我们的性别话语相关性框架（GDCF）在大规模发现和分析口语内容中的性别化话语词汇；（二）通过我们的话语词汇嵌入关联测试（D-WEAT）测量与这些性别化话语词汇相关的性别偏见。我们将研究重点放在播客上，这是一种流行且日益增长的社会媒体形式，分析了15,117个播客剧集。我们通过LDA和BERTopic分析了性别与话语词汇之间的相关性，以自动形成性别化话语词汇表。然后，我们研究了这些性别化话语词汇在特定领域中的普及程度，发现商业、技术/政治和视频游戏领域存在基于性别话语的男性默认。接下来，我们研究了来自OpenAI的领先LLM嵌入模型中这些性别化话语词汇的表示，发现男性话语词汇的表示比女性话语词汇更稳定和健壮，这可能导致男性在下游任务上获得更好的系统性能。因此，男性通过一种最先进的语言模型获得了因他们的话语模式而获得更好的系统性能的奖励——而这种嵌入差异是一种表示上的伤害和男性默认。|
|**2025-04-15**|**A Dual-Space Framework for General Knowledge Distillation of Large Language Models**|Xue Zhang et.al.|[2504.11426](http://arxiv.org/abs/2504.11426)|null|知识蒸馏（KD）是一种通过将大型语言模型（LLMs）的知识转移到较小模型中来压缩LLMs的有前景的解决方案。在这个过程中，白盒KD方法通常通过最小化教师模型和学生模型输出分布之间的距离来转移更多信息。然而，我们发现当前的白盒KD框架存在两个局限性：a）将不同输出空间中的概率分布桥接在一起将限制教师模型和学生模型之间的相似性；b）这个框架不能应用于具有不同词汇的LLMs。这些局限性的一个根本原因是，KD的教师和学生模型的分布是由不同的预测头输出的，这导致了不同输出空间和维度的分布。因此，在本文中，我们提出了一种双空间知识蒸馏（DSKD）框架，该框架统一了教师和学生模型的预测头以进行KD。具体来说，我们首先引入了两个具有理想初始化的项目器，将教师/学生隐藏状态投影到学生/教师表示空间。之后，来自不同模型的隐藏状态可以共享相同的头并统一分布的输出空间。此外，我们开发了一种精确的标记对齐（ETA）算法，以对齐两个不同标记的序列中的相同标记。基于以上，我们的DSKD框架是一个通用的KD框架，支持离策略和在线策略KD，以及任何两个LLMs之间的KD，无论它们的词汇量如何。在指令遵循、数学推理和代码生成基准测试上的大量实验表明，DSKD在性能上显著优于基于当前白盒KD框架的现有方法，并且超越了针对具有不同词汇的LLMs的其他跨标记器KD方法。|
|**2025-04-15**|**Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts**|Quanyu Long et.al.|[2504.11420](http://arxiv.org/abs/2504.11420)|null|大型语言模型（LLMs）在众多任务中展现出了卓越的能力，但它们在处理复杂任务时通常依赖于外部上下文。虽然检索增强框架传统上关注单次遍历中选取排名靠前的文档，但许多现实场景需要组合检索，即必须以协调的方式将多个来源组合起来。在这项工作中，我们提出了一种三编码器顺序检索器，将此过程建模为马尔可夫决策过程（MDP），将检索一组元素的概率分解为一系列条件概率，并允许每个检索步骤依赖于之前选定的示例。我们分两个阶段训练检索器：首先，我们高效地构建用于初始策略训练的监督性顺序数据；然后，我们利用基于生成程序的机构对应性的奖励，来调整策略以与LLM的偏好相一致。实验结果表明，我们的方法在一致性上显著优于基线，突出了显式建模示例间依赖关系的重要性。这些发现强调了组合检索在需要多份证据或示例的任务中的潜力。|
|**2025-04-15**|**DataDecide: How to Predict Best Pretraining Data with Small Experiments**|Ian Magnusson et.al.|[2504.11393](http://arxiv.org/abs/2504.11393)|null|由于在多个数据集上预训练大型语言模型成本高昂，因此使用小规模实验来决定数据集对于降低成本至关重要。哪些基准和从观测到的小规模性能中做出决策的方法能够最准确地预测出能产生最佳大型模型的语料库？为了推动对此问题的开放式探索，我们发布了模型、数据和评估工具，构建了DataDecide——这是迄今为止最全面的开源模型集，用于比较数据和规模之间的差异。我们在25个不同来源、去重和过滤后的语料库上进行了受控的预训练实验，语料库包含多达1000亿个标记，模型大小高达10亿参数，并使用了3个随机种子。我们发现，在单一、小规模（例如1500万个参数）的模型排名是一个强大的基线，可以预测我们更大目标规模（10亿）的最佳模型（约80%的比较是正确的）。在8个基线中，没有扩展性法则方法超越单规模预测的计算决策前沿，但DataDecide可以衡量未来扩展性法则的改进。我们还发现，在小规模实验中使用连续似然度度量作为代理，可以将包括MMLU、ARC、HellaSwag、MBPP和HumanEval在内的基准在目标10亿规模上预测的准确性提高到超过80%，而所需的计算量仅为0.01%。|
|**2025-04-15**|**RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models**|Juan Diego Rodriguez et.al.|[2504.11381](http://arxiv.org/abs/2504.11381)|**[link](https://github.com/juand-r/rankalign)**|尽管大型语言模型（LLMs）在许多任务上变得更加能干和准确，但它们的行为中仍存在一些基本的不可靠来源。一个关键限制是当提示发生变化时，它们在报告相同信息上的不一致性。在本文中，我们考虑了模型生成的答案与其自身对该答案的验证之间的差异，即生成器-验证器差距。我们以比先前工作更为严格的方式定义了这个差距：我们期望生成器和验证器在整个候选答案集上的评分之间有相关性。我们发现，根据这个衡量标准，在包括问答、词汇语义任务和下一个单词预测的各种设置中，都存在很大的差距。然后，我们提出了RankAlign，一种基于排名的训练方法，并显示它平均缩小了31.8%的差距，超过了所有基线方法。此外，这种方法很好地推广到了域外任务和词汇项。|
|**2025-04-15**|**Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions**|Wang Bill Zhu et.al.|[2504.11373](http://arxiv.org/abs/2504.11373)|**[link](https://github.com/bill1235813/cancer-myth)**|癌症患者越来越多地将大型语言模型（LLMs）作为一种新的互联网搜索形式用于获取医疗信息，这使得评估这些模型处理复杂、个性化问题能力变得至关重要。然而，当前的医疗基准主要集中在医学考试或消费者搜索的问题上，并没有评估LLMs在具有详细临床背景的真实患者问题上的表现。在本文中，我们首先评估了LLMs在来自真实患者的、由三位血液肿瘤科医师审阅的癌症相关问题上的表现。虽然响应总体上准确，GPT-4-Turbo得分4.13分（满分5分），但这些模型频繁地未能识别或解决问题中存在的错误假设，这可能会对安全的医疗决策造成风险。为了系统地研究这一局限性，我们引入了Cancer-Myth，这是一个经过专家验证的包含585个具有错误假设的癌症相关问题对抗数据集。在此基准测试中，没有前沿LLM（包括GPT-4o、Gemini-1.Pro和Claude-3.5-Sonnet）能够超过30%的时间纠正这些错误假设。即使是先进的医疗智能体方法也无法防止LLMs忽视错误假设。这些发现揭示了LLMs在临床可靠性方面的一个关键差距，并强调了在医疗AI系统中需要更加稳固的安全保障的必要性。|
|**2025-04-15**|**OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution**|Lucio La Cava et.al.|[2504.11369](http://arxiv.org/abs/2504.11369)|null|开放大型语言模型（OLLMs）在生成式AI应用中越来越被利用，这给检测其输出带来了新的挑战。我们提出了OpenTuringBench，这是一个基于OLLMs的新基准，旨在训练和评估在图灵测试和作者归属问题上的机器生成文本检测器。OpenTuringBench专注于一组代表性的OLLMs，并具有一系列具有挑战性的评估任务，包括人机操控文本、域外文本以及来自先前未见模型的文本。我们还提供了OTBDetector，一个对比学习框架，用于检测和归属基于OLLMs的机器生成文本。结果显示，OpenTuringBench任务的相关性和难度各不相同，我们的检测器在各种任务中表现出显著的能力，并优于大多数现有检测器。资源可在OpenTuringBench Hugging Face仓库中获取，网址为https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench。|
|**2025-04-15**|**Teaching Large Language Models to Reason through Learning and Forgetting**|Tianwei Ni et.al.|[2504.11364](http://arxiv.org/abs/2504.11364)|**[link](https://github.com/twni2016/llm-reasoning-uft)**|利用大型语言模型在推理时的搜索功能已被证明可以进一步提高训练模型解决复杂数学和推理问题的能力。然而，这种方法显著增加了计算成本和推理时间，因为模型必须生成和评估多个候选解决方案以确定可行的推理路径。为了解决这个问题，我们提出了一种有效的方法，通过使用从各种搜索方法中得出的成功（学习）和失败（遗忘）推理路径来微调模型，将搜索能力直接集成到模型中。虽然使用这些数据微调模型可能看起来很简单，但我们发现了一个关键问题：如果天真地执行微调，模型的搜索能力往往会迅速下降。我们表明，通过采用较小的学习率可以显著减轻这种退化。在具有挑战性的24点游戏和倒计时数学推理基准上进行的广泛实验表明，我们的方法不仅优于标准微调和推理时搜索基线，而且将推理时间减少了180倍。|
|**2025-04-15**|**Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning**|Haiming Wang et.al.|[2504.11354](http://arxiv.org/abs/2504.11354)|**[link](https://github.com/moonshotai/kimina-prover-preview)**|我们介绍了Kimina-Prover Preview，这是一个大型语言模型，它开创了一种新颖的以推理驱动的探索范式，用于形式定理证明，正如在本预览版本中所展示的。通过从Qwen2.5-72B使用大规模强化学习管道进行训练，Kimina-Prover通过采用我们称之为“形式推理模式”的结构化推理模式，在Lean 4证明生成中表现出强大的性能。这种方法允许模型在Lean中模拟人类的解决问题策略，通过迭代生成和优化证明步骤。Kimina-Prover在miniF2F基准测试上设定了新的最高标准，达到80.7%的通过率（pass@8192）。除了改进基准测试性能外，我们的工作还产生了几个关键见解：（1）Kimina-Prover表现出高样本效率，即使在最小采样（pass@1）的情况下也能提供强有力的结果，并且能够随着计算预算的扩大而有效扩展，这得益于其独特的推理模式和RL训练；（2）我们展示了与模型大小明显的性能扩展趋势，这是在形式数学中以前未曾观察到的神经定理证明者的趋势；（3）学习到的推理风格，与传统的搜索算法不同，显示出弥合形式验证和非正式数学直觉之间差距的潜力。我们开源了Kimina-Prover的1.5B和7B参数的蒸馏版本。|
|**2025-04-14**|**InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models**|Jinguo Zhu et.al.|[2504.10479](http://arxiv.org/abs/2504.10479)|**[link](https://github.com/opengvlab/internvl)**|我们介绍了InternVL3，这是InternVL系列的重大进步，其特色是采用原生多模态预训练范式。InternVL3并非将仅支持文本输入的大语言模型（LLM）调整为支持视觉输入的多模态大语言模型（MLLM），而是在单一预训练阶段，从多样化的多模态数据和纯文本语料库中共同获取多模态和语言能力。这种统一的训练范式有效地解决了传统MLLM事后训练流程中常见的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3引入了可变视觉位置编码（V2PE）以支持扩展的多模态上下文，采用了先进的训练后技术，如监督微调（SFT）和混合偏好优化（MPO），并采用了测试时缩放策略以及优化的训练基础设施。广泛的实证评估表明，InternVL3在广泛的跨模态任务上表现出卓越的性能。特别是，InternVL3-78B在MMMU基准测试中取得了72.2的分数，在开源MLLM中创造了新的最高水平。其能力与领先的专有模型（包括ChatGPT-4o、Claude 3.5 Sonnet和Gemini 2.5 Pro）保持高度竞争，同时保持了强大的纯语言能力。为了追求开放科学原则，我们将公开发布训练数据和模型权重，以促进下一代MLLM的研究和发展。|
|**2025-04-14**|**MIEB: Massive Image Embedding Benchmark**|Chenghao Xiao et.al.|[2504.10471](http://arxiv.org/abs/2504.10471)|**[link](https://github.com/embeddings-benchmark/mteb)**|图像表示通常通过相互独立的、特定任务的协议进行评估，导致对模型能力的理解碎片化。例如，不清楚一个擅长聚类图像的图像嵌入模型在给定一段文本时检索相关图像的能力是否同样出色。我们引入了大规模图像嵌入基准（MIEB），以评估迄今为止最广泛的图像和图像-文本嵌入模型的性能。MIEB涵盖了130个单独的任务，这些任务分布在38种语言中，我们将它们分为8个高级类别。在我们的基准测试中，我们对50个模型进行了基准测试，发现没有任何一种方法在所有任务类别中都占主导地位。我们揭示了高级视觉模型中的隐藏能力，例如它们对文本的准确视觉表示，以及它们在交织编码中能力有限，以及在存在混淆因素的情况下匹配图像和文本的能力有限。我们还表明，在MIEB上视觉编码器的性能与其在多模态大型语言模型中使用时的性能高度相关。我们的代码、数据集和排行榜在https://github.com/embeddings-benchmark/mteb上公开可用。|
|**2025-04-14**|**Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding**|Tao Zhang et.al.|[2504.10465](http://arxiv.org/abs/2504.10465)|**[link](https://github.com/magic-research/Sa2VA)**|多模态大型语言模型（MLLMs）在精细粒度的像素级理解任务上取得了显著的成绩。然而，所有这些研究都高度依赖于额外组件，如视觉编码器（CLIP）、分割专家，导致系统复杂性高，限制了模型的扩展。在本工作中，我们的目标是探索一个无需引入额外组件的高度简化的MLLM。我们的工作受到了最近关于单Transformer作为统一视觉-语言模型（SAIL）设计的研究的启发，这些研究在Transformer中联合学习视觉标记和文本标记。我们提出了Pixel-SAIL，一个用于像素级MLLM任务的单一Transformer。特别是，我们在普通基线的基础上提出了三个技术改进。首先，我们设计了一个可学习的上采样模块来细化视觉标记特征。其次，我们提出了一种新颖的视觉提示注入策略，使单一Transformer能够理解视觉提示输入，并从视觉提示嵌入和视觉标记的早期融合中受益。第三，我们引入了一种视觉专家蒸馏策略，以有效地增强单一Transformer的细粒度特征提取能力。此外，我们收集了一个全面的像素理解基准（PerBench），通过人工检查。它包括三个任务：详细物体描述、基于视觉提示的问题回答和视觉-文本参照分割。在四个参照分割基准、一个视觉提示基准以及我们的PerBench上的大量实验表明，我们的Pixel-SAIL在更简单的管道中实现了可比甚至更好的结果。代码和模型将在https://github.com/magic-research/Sa2VA上发布。|
|**2025-04-14**|**The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer**|Weixian Lei et.al.|[2504.10462](http://arxiv.org/abs/2504.10462)|**[link](https://github.com/bytedance/sail)**|本文介绍了SAIL，这是一种单一架构的统一多模态大型语言模型（MLLM），它将原始像素编码和语言解码集成在一个架构中。与现有的模块化MLLM不同，SAIL不需要预训练的视觉变换器（ViT），从而消除了单独的视觉编码器的需求，呈现了一种更为简约的架构设计。SAIL并非引入新的架构组件，而是将混合注意力机制和多模态位置编码进行调整，以更好地适应视觉和文本模态的独特特性。我们系统地比较了SAIL的属性，包括可扩展性、跨模态信息流模式和视觉表示能力，与模块化MLLM的属性。通过扩大训练数据和模型大小，SAIL实现了与模块化MLLM相当的性能。值得注意的是，移除预训练的ViT组件增强了SAIL的可扩展性，并导致了显著不同的跨模态信息流模式。此外，SAIL展示了强大的视觉表示能力，在语义分割等视觉任务上取得了与ViT-22B相当的结果。代码和模型可在https://github.com/bytedance/SAIL获取。|
|**2025-04-14**|**GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents**|Xiaobo Xia et.al.|[2504.10458](http://arxiv.org/abs/2504.10458)|null|在构建图形用户界面（GUI）代理的现有努力中，主要依赖于在大视觉语言模型（LVLMs）上进行的监督微调训练范式。然而，这种方法不仅需要大量的训练数据，而且难以有效理解GUI截图并将知识推广到未见过的界面。这一问题显著限制了其在现实场景中的应用，尤其是在高级任务方面。受大型推理模型中的强化微调（RFT，例如DeepSeek-R1）的启发，该技术能有效增强大型语言模型在现实环境中的问题解决能力，我们提出了\name，这是第一个旨在通过统一动作空间规则建模来增强LVLMs在高级现实任务场景中GUI能力的强化学习框架。通过利用跨多个平台（包括Windows、Linux、MacOS、Android和Web）的小量精心挑选的高质量数据，并采用如组相对策略优化（GRPO）等策略优化算法来更新模型，\name在三个不同平台（移动、桌面和Web）的八个基准测试中，仅使用0.02%的数据（3K与13M相比），就实现了比以前最先进的方法（如OS-Atlas）更优的性能。这些结果展示了基于统一动作空间规则建模的强化学习在提高LVLMs执行现实世界GUI代理任务能力方面的巨大潜力。|
|**2025-04-14**|**M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models**|Junxiong Wang et.al.|[2504.10449](http://arxiv.org/abs/2504.10449)|**[link](https://github.com/jxiw/m1)**|有效的推理对于解决复杂的数学问题至关重要。最近的大型语言模型（LLMs）通过扩展长链推理来提升性能。然而，由于二次计算复杂性和线性内存需求，基于Transformer的模型在扩展上下文长度方面存在固有的限制。在本文中，我们介绍了一种基于Mamba架构的新型混合线性RNN推理模型M1，它允许内存高效的推理。我们的方法利用了现有推理模型的蒸馏过程，并通过RL训练进一步优化。在AIME和MATH基准上的实验结果表明，M1不仅优于之前的线性RNN模型，而且在相似规模下与最先进的Deepseek R1蒸馏推理模型性能相当。我们还比较了我们的生成速度与高性能通用推理引擎vLLM，观察到与相同大小的Transformer相比，速度提升了超过3倍。通过吞吐量速度提升，我们能够在固定的生成时间预算下，使用自洽投票实现比DeepSeek R1蒸馏Transformer推理模型更高的准确率。总的来说，我们引入了一种混合的Mamba推理模型，并提供了使用自洽或长链推理来扩展测试时生成的更有效的方法。|
|**2025-04-14**|**Multimodal Long Video Modeling Based on Temporal Dynamic Context**|Haoran Hao et.al.|[2504.10443](http://arxiv.org/abs/2504.10443)|**[link](https://github.com/hoar012/tdc-video)**|近期大型语言模型（LLMs）的进展在视频理解领域取得了显著突破。然而，由于LLMs的上下文长度限制和视频中的大量信息，现有模型在处理长视频时仍然存在困难。尽管一些最近的方法针对长视频理解进行了设计，但在标记压缩过程中常常丢失关键信息，并且在处理音频等额外模态时也面临挑战。在这项工作中，我们提出了一种利用帧之间时间关系的动态长视频编码方法，称为时间动态上下文（TDC）。首先，我们根据帧之间的相似性将视频分割成语义上一致的场景，然后使用视觉-音频编码器将每个帧编码成标记。其次，我们提出了一种新颖的时间上下文压缩器，以减少每个段内的标记数量。具体来说，我们采用基于查询的Transformer将视频、音频和指令文本标记聚合到一组有限的时间上下文标记中。最后，我们将静态帧标记和时间上下文标记输入到LLM中进行视频理解。此外，为了处理超长视频，我们提出了一种无需训练的思考链策略，该策略逐步从多个视频段中提取答案。这些中间答案作为推理过程的一部分，并有助于最终答案。我们在通用视频理解和音视频理解基准测试上进行了广泛实验，我们的方法表现出强大的性能。代码和模型可在https://github.com/Hoar012/TDC-Video上获取。|
|**2025-04-14**|**LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models**|Minqian Liu et.al.|[2504.10430](http://arxiv.org/abs/2504.10430)|null|近期大型语言模型（LLMs）的进步使它们接近了人类水平的说服能力。然而，这种潜力也引发了对LLM驱动说服的安全风险的担忧，尤其是它们通过操纵、欺骗、利用脆弱性以及许多其他有害手段进行不道德影响的可能性。在本工作中，我们通过对两个关键方面的系统性调查来探讨LLM说服的安全性：（1）LLMs是否适当地拒绝不道德的说服任务，并在执行过程中避免不道德的策略，包括初始说服目标看似道德中立的案例；（2）影响因素如个性特征和外部压力如何影响其行为。为此，我们引入了PersuSafety，这是第一个用于评估说服安全性的全面框架，它包含三个阶段，即说服场景创建、说服对话模拟和说服安全性评估。PersuSafety涵盖了6个不同的不道德说服主题和15种常见的非道德策略。通过在8个广泛使用的LLMs上进行的广泛实验，我们发现大多数LLMs存在显著的安全问题，包括未能识别有害的说服任务和利用各种不道德的说服策略。我们的研究呼吁更多关注改善在进步和目标驱动的对话，如说服中的安全性对齐。|
|**2025-04-14**|**Can We Edit LLMs for Long-Tail Biomedical Knowledge?**|Xinhao Yi et.al.|[2504.10421](http://arxiv.org/abs/2504.10421)|**[link](https://github.com/xinhaoyi/edit_bio_long_tail)**|知识编辑作为一种通过修改其内部知识来更新大型语言模型（LLM）的有效方法已经出现。然而，由于生物医学知识的长尾分布特性，其中罕见和低频信息普遍存在，其在生物医学领域的应用面临着独特的挑战。在本文中，我们进行了首次全面研究，以探讨知识编辑方法在编辑长尾生物医学知识方面的有效性。我们的结果表明，虽然现有的编辑方法可以提高LLM在长尾生物医学知识上的性能，但在编辑后，其在长尾知识上的表现仍然不如高频流行知识。我们的进一步分析揭示，长尾生物医学知识中包含大量的单对多知识，其中单一主题和关系链接到多个对象。这种单对多知识的普遍存在限制了知识编辑在提高LLM对长尾生物医学知识理解方面的有效性，突显了需要定制策略来弥合这一性能差距。|
|**2025-04-14**|**Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA**|Michał Turski et.al.|[2504.10419](http://arxiv.org/abs/2504.10419)|**[link](https://github.com/snowflake-labs/checkboxqa)**|复选框在现实世界的文档处理中至关重要，其勾选的有无直接影响数据提取和决策过程。尽管大型视觉和语言模型在众多任务上表现出色，但它们在解析可勾选内容方面却遇到了困难。这一挑战在单个被忽视的复选框可能导致昂贵的监管或合同疏忽的行业中尤为紧迫。为了解决这一差距，我们引入了CheckboxQA数据集，这是一个旨在评估和提升模型在复选框相关任务上表现的有针对性的资源。它揭示了当前模型的局限性，并作为提升文档理解系统的一个宝贵工具，对法律科技和金融等领域的应用具有重要意义。该数据集可在以下网址公开获取：https://github.com/Snowflake-Labs/CheckboxQA|
|**2025-04-11**|**Quantum Large Language Model Fine-Tuning**|Sang Hyub Kim et.al.|[2504.08732](http://arxiv.org/abs/2504.08732)|null|我们介绍了一种混合量子-经典深度学习架构，用于大型语言模型的微调。架构的经典部分是一个强大的句子转换器，足以在情感预测等复杂任务中表现出显著的准确性。架构的量子部分由参数化的量子电路组成，这些电路利用了量子比特之间的长距离连接。我们分析了混合模型在不同超参数设置下的性能，包括量子比特数量、量子电路的深度、学习率、重新上传步骤数量等。基于主要效应的筛选研究，我们展示了与可比的经典基线相比，预测准确性的总体提高，并且随着量子比特数量的增加，准确率呈上升趋势。我们在本研究中探索的超参数范围内观察到，与可比模型大小的经典架构相比，准确率提高了高达3.14%。我们通过消融研究展示了我们架构中每个模块的贡献。我们的研究基于有限射击次数，并包括基于有噪声量子门的模拟。|
|**2025-04-11**|**DocAgent: A Multi-Agent System for Automated Code Documentation Generation**|Dayu Yang et.al.|[2504.08725](http://arxiv.org/abs/2504.08725)|**[link](https://github.com/facebookresearch/docagent)**|高质量的代码文档对于软件开发至关重要，尤其是在人工智能时代。然而，使用大型语言模型（LLMs）自动生成它仍然具有挑战性，因为现有方法往往产生不完整、无帮助或事实错误的输出。我们引入了DocAgent，这是一个新颖的多智能体协作系统，它使用拓扑代码处理来构建增量上下文。然后，专门的智能体（阅读者、搜索者、编写者、验证者、协调者）协同生成文档。我们还提出了一种多方面的评估框架，用于评估完整性、有用性和真实性。综合实验表明，DocAgent在一致性方面显著优于基线。我们的消融研究证实了拓扑处理顺序的关键作用。DocAgent为在复杂和专有存储库中生成可靠的代码文档提供了一种稳健的方法。|
|**2025-04-11**|**SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents**|Muhammad Shihab Rashid et.al.|[2504.08703](http://arxiv.org/abs/2504.08703)|**[link](https://github.com/amazon-science/swe-polybench)**|基于大型语言模型的编码代理在软件工程任务中展现出令人印象深刻的性能，但在多种编程语言和现实场景中评估其性能仍然具有挑战性。我们引入了SWE-PolyBench，这是一个新的多语言基准，用于对编码代理进行基于执行的库级评估。SWE-PolyBench包含来自21个库的2110个实例，包括Java（165个）、JavaScript（1017个）、TypeScript（729个）和Python（199个）的任务，涵盖了错误修复、功能添加和代码重构。我们提供了一个任务和库分层子样本（SWE-PolyBench500）并发布了一个评估工具，允许完全自动化的评估。为了实现编码代理更全面的比较，这项工作还提出了一套基于语法树分析的全新指标。我们在SWE-PolyBench上评估了领先的开源编码代理，揭示了它们在语言、任务类型和复杂性类别中的优势和局限性。我们的实验表明，当前代理在不同语言上的表现不均衡，在处理复杂问题时存在困难，但在简单任务上表现出更高的性能。SWE-PolyBench旨在推动开发更通用、更稳健的AI编码助手，以适应现实世界的软件工程。我们的数据集和代码可在以下网址获取：https://github.com/amazon-science/SWE-PolyBench|
|**2025-04-11**|**Large Language Models as Span Annotators**|Zdeněk Kasner et.al.|[2504.08697](http://arxiv.org/abs/2504.08697)|null|对于高质量文本，单一得分指标很少能提供可操作的反馈。相比之下，通过标注文本中的问题区域来进行范围标注——即指出文本中的问题并标注其范围——可以指导改进并提供洞察。直到最近，范围标注仅限于人工标注员或经过微调的编码器模型。在这项研究中，我们使用大型语言模型（LLMs）自动化范围标注。我们在三个任务上比较了专家或熟练的众包标注员与开放和专有LLMs：数据到文本生成评估、机器翻译评估以及人工文本中的宣传检测。在我们的实验中，我们展示了LLMs作为范围标注器易于实现，并且与人工标注员相比具有显著的成本效益。LLMs与熟练的人类标注员达到适中的协议，在某些情况下与标注员之间的平均协议相当。定性分析表明，推理模型优于其指令调整后的对应模型，并为标注提供了更有效的解释。我们发布了超过40k个模型和人工标注的数据集，以供进一步研究。|
|**2025-04-11**|**TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning**|Hang Ni et.al.|[2504.08694](http://arxiv.org/abs/2504.08694)|null|大型语言模型（LLMs）在自动化旅行规划方面展现出潜力，但它们在处理细微的时空合理性方面往往不足。虽然现有的基准主要关注基本计划的可行性，但它们忽略了诸如路线效率、POI吸引力以及实时适应性等关键方面。本文介绍了TP-RAG，这是第一个针对检索增强、时空感知旅行规划定制的基准。我们的数据集包括2,348个真实世界的旅行查询，85,575个细粒度标注的POI，以及18,784个来自在线旅游文档的高质量旅行轨迹参考，从而实现了动态和情境感知的规划。通过广泛的实验，我们发现整合参考轨迹显著提高了旅行计划的空间效率和POI合理性，但由于参考的冲突和噪声数据，普遍性和鲁棒性方面仍存在挑战。为了解决这些问题，我们提出了EvoRAG，这是一个进化框架，它有效地将多种检索到的轨迹与LLMs的内生推理相结合。EvoRAG实现了最先进的性能，与从头开始和检索增强的基线相比，提高了时空合规性并减少了常识违规。我们的工作强调了将网络知识与LLM驱动的优化相结合的潜力，为更可靠和自适应的旅行规划代理铺平了道路。|
|**2025-04-11**|**Fast-Slow-Thinking: Complex Task Solving with Large Language Models**|Yiliu Sun et.al.|[2504.08690](http://arxiv.org/abs/2504.08690)|null|如今，大型语言模型（LLMs）逐渐被用于解决复杂任务。面对这一挑战，任务分解已成为一种有效的方法，它建议将一个复杂任务分解为多个更简单的子任务，然后分别解决它们，从而降低原始任务的难度。然而，当任务包含过于复杂的逻辑和约束时，现有任务分解方法的性能可能不尽如人意。在这种情况下，LLMs生成的解决方案可能会偏离任务的原始目的，或包含冗余甚至错误的内容。因此，受人类具有快速思考和慢速思考两种思维系统的事实启发，本文提出了一种新的任务分解方法，称为“快慢思考”（FST），该方法通过快思考（FT）和慢思考（ST）步骤的协作来激发LLMs解决问题。在快思考中，FT更关注任务的通用和简洁方面，而在慢思考中，ST则更关注任务的细节。在FT中，LLMs被提示去除原始任务的约束，从而简化为一个通用和简洁的任务。在ST中，我们回忆起FT中去除的约束，以便LLMs可以改进FT中生成的答案，以满足原始任务的要求。因此，我们的FST方法使LLMs能够通过类似人类的认知过程从粗到细地考虑复杂问题，这在三个类型任务的实验中得到了很好的验证。|
|**2025-04-11**|**Voice Interaction With Conversational AI Could Facilitate Thoughtful Reflection and Substantive Revision in Writing**|Jiho Kim et.al.|[2504.08687](http://arxiv.org/abs/2504.08687)|null|撰写优秀的文章不仅需要表达思想，还需要通过修订来完善它们，这个过程可以通过反思来促进。先前的研究表明，通过对话方式提供的反馈，如写作中心辅导课程中的反馈，可以帮助作者比静态反馈更深入地反思他们的作品。最近，多模态大型语言模型（LLM）的进步现在为支持基于语音的互动和表达性反思提供了新的可能性。特别是，我们提出将LLM生成的静态反馈重新用作对话的开端，让作者寻求澄清、请求示例和提出后续问题，从而促进对写作的更深入反思。我们认为，基于语音的交互可以自然地促进这种对话交流，鼓励作者关注更高层次的问题，促进反思的迭代完善，并比基于文本的交互减少认知负荷。为了研究这些效果，我们提出一项形成性研究，探讨文本输入与语音输入如何影响作者的反思及其随后的修订。这项研究的结果将为智能交互式写作工具的设计提供信息，揭示如何通过LLM驱动的对话代理的基于语音的交互来支持反思和修订。|
|**2025-04-11**|**Variability-Driven User-Story Generation using LLM and Triadic Concept Analysis**|Alexandre Bazin et.al.|[2504.08666](http://arxiv.org/abs/2504.08666)|null|敏捷开发中，针对需求的一个常用实践是生成一组用户故事（也称为“敏捷产品待办事项”），这大致包括一组（角色，功能）对的列表，其中角色为了某个目的处理功能。在软件产品线的背景下，一组类似系统的需求因此是一组用户故事集，每个系统一个，形成一个由（系统，角色，功能）三元组集组成的3维数据集。在本文中，我们结合三联概念分析（TCA）和大型语言模型（LLM）提示，建议开发依赖现有系统家族变异性逻辑的新系统所需的用户故事集。此过程包括以下步骤：1）计算表示为TCA蕴涵的3维变异性；2）为设计师提供可理解的设计选项；3）捕捉设计师的选项选择；4）提出与这一选择相对应的第一个用户故事集；5）根据步骤1中确定的蕴涵巩固其有效性，并在必要时完成它；6）利用LLM来获得更全面的网站。该过程通过包括67个类似用途网站的用户故事集的数据集进行评估。|
|**2025-04-11**|**Quality evaluation of Tabby coding assistant using real source code snippets**|Marta Borek et.al.|[2504.08650](http://arxiv.org/abs/2504.08650)|**[link](https://github.com/metredecoeur/tabby-testing-pipeline)**|大型语言模型已成为软件开发中的一种流行工具，提供编码辅助。由于自然语言提示，对这类工具生成的代码的准确性和可靠性进行恰当的测量是一个挑战。我们提出了一种简单的流水线，该流水线使用经典和通用类型的算法和数据结构的最先进实现。我们专注于测量TabbyML代码助手的质量，因为它具有开放许可证和在语言模型选择上的灵活性。我们的结果以环路复杂度、Halstead的缺陷与努力度以及四个基于文本的相似度矩阵呈现，展示了TabbyML在编码辅助任务中的可用性。|
|**2025-04-11**|**Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents**|Alessio Buscemi et.al.|[2504.08640](http://arxiv.org/abs/2504.08640)|null|在人工智能开发生态系统中培养信任与合作被认为是促进可信赖人工智能系统采纳的关键。通过将大型语言模型（LLM）代理嵌入进化博弈论框架中，本文研究了人工智能开发者、监管者和用户之间的复杂互动，并在不同的监管场景下模拟了他们的战略选择。进化博弈论（EGT）被用来定量地模拟每个参与者面临的困境，而LLM提供了额外的复杂性和细微差别，并使得重复博弈和纳入性格特征成为可能。我们的研究确定了战略人工智能代理的新兴行为，这些行为倾向于采取比纯粹博弈论代理更为“悲观”（不信任和存在缺陷）的立场。我们观察到，在用户完全信任的情况下，激励措施对于促进有效监管是有效的；然而，有条件的信任可能会破坏“社会契约”。因此，在用户信任和监管者声誉之间建立良性反馈循环似乎对于引导开发者创造安全的人工智能至关重要。然而，这种信任出现的水准可能取决于用于测试的具体LLM。因此，我们的结果为人工智能监管系统提供了指导，并有助于预测如果使用战略LLM代理辅助监管本身，其可能的结果。|
|**2025-04-10**|**C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing**|Zhongyang Li et.al.|[2504.07964](http://arxiv.org/abs/2504.07964)|**[link](https://github.com/tianyi-lab/c3po)**|**混合专家（MoE）大型语言模型（LLMs）在专家路径选择上存在严重不足——我们的研究揭示了从预训练中学习的朴素专家选择留下了一个令人惊讶的10-20%的准确率提升空间。受此观察的启发，我们开发了一类新的测试时优化方法，该方法针对每个测试样本，联合地重新加权或“重新混合”不同层中的专家。由于测试样本的真实标签未知，我们提议优化一个由样本的“成功邻居”定义的替代目标，这些邻居来自一组参考样本。我们引入了基于模式发现、核回归和类似参考样本/任务的平均损失的三个替代目标和算法。为了减少优化整个路径的成本，我们只将我们的算法应用于关键层中核心专家的混合权重，这些权重在性能上相似但节省了大量计算。这导致了“关键层、核心专家、协作路径优化（C3PO）”的出现。我们将C3PO应用于两种最近的MoE LLM，并在六个广泛使用的基准上对其进行测试。它始终将基准模型在准确率上提高7-15%，并且比广泛使用的测试时学习基线（如上下文学习、提示/前缀调整）大幅提升。此外，C3PO使得具有1-3B个活动参数的MoE LLM能够超越具有7-9B个参数的LLM，从而提高了MoE在效率上的优势。我们详尽的消融研究进一步为在MoE上实现测试时改进提供了新的见解。**|
|**2025-04-10**|**GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation**|Lang Lin et.al.|[2504.07962](http://arxiv.org/abs/2504.07962)|null|本文提出了一种利用多模态大型语言模型（MLLMs）进行指引用户视频目标分割（RefVOS）的新框架。以前的基于MLLM的方法通常难以解决“Ref”和“VOS”之间的困境：它们要么专注于理解少数关键帧（全局推理），要么在连续帧上跟踪物体（局部推理），并依赖外部VOS或帧选择器来缓解挑战的另一端。然而，我们的框架GLUS表明，全局和局部一致性可以统一为一个视频分割MLLM：一组稀疏的“上下文帧”提供全局信息，而一系列连续的“查询帧”进行局部目标跟踪。这进一步通过将MLLM与预训练的VOS记忆库联合训练来得到支持，以同时处理短期和长期时间信息。为了提高MLLM在有限上下文窗口内的信息效率，我们引入了目标对比学习来区分困难的重假阳性物体，并引入了一个自我改进框架来识别关键帧并执行传播。通过综合这些见解，我们的GLUS提供了一个简单而有效的基线，在MeViS和Ref-Youtube-VOS基准上实现了MLLMs的新最先进性能。我们的项目页面在https://glus-video.github.io/。|
|**2025-04-10**|**MM-IFEngine: Towards Multimodal Instruction Following**|Shengyuan Ding et.al.|[2504.07957](http://arxiv.org/abs/2504.07957)|**[link](https://github.com/syuan03/mm-ifengine)**|**指令遵循（IF）能力衡量多模态大型语言模型（MLLMs）理解用户告诉它们的内容是否准确，以及它们是否正确执行了这些指令。现有的多模态指令遵循训练数据稀缺，基准测试简单，且对于需要精确输出约束的任务，评估策略不够精确。为了解决这个问题，我们提出了MM-IFEngine，这是一种有效的流程，用于生成高质量的图像-指令对。我们的MM-IFEngine流程产生了大规模、多样化和高质量的训练数据集MM-IFInstruct-23k，适用于监督微调（SFT），并扩展为MM-IFDPO-23k以用于直接偏好优化（DPO）。我们进一步介绍了MM-IFEval，这是一个具有挑战性和多样性的多模态指令遵循基准，包括：（1）针对输出响应的组成级约束和与输入图像相关的感知级约束；（2）一个综合的评估流程，结合了基于规则的评估和裁判模型。我们进行了SFT和DPO实验，并证明在MM-IFInstruct-23k和MM-IFDPO-23k上微调MLLMs在各种IF基准测试上取得了显著的提升，例如MM-IFEval（+10.2%）、MIA（+7.6%）和IFEval（+12.3%）。完整的数据和评估代码将发布在https://github.com/SYuan03/MM-IFEngine上。**|
|**2025-04-10**|**VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning**|Yukun Qi et.al.|[2504.07956](http://arxiv.org/abs/2504.07956)|null|思维链（CoT）推理的进步显著提高了大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的能力。然而，一个严格评估视频CoT推理的框架仍然缺失。当前的视频基准测试未能充分评估推理过程，也无法揭示失败是否源于感知或推理能力的不足。因此，我们引入了VCR-Bench，这是一个新型基准，旨在全面评估LVLMs的视频思维链推理能力。VCR-Bench包含859个视频，涵盖各种视频内容和时长，以及1,034对高质量的问题-答案。每一对问题-答案都手动标注了逐步的思维链推理理由，其中每个步骤都被标记以表明其与感知或推理能力的关联。此外，我们设计了七个不同的任务维度，并提出了CoT得分，以基于逐步标记的思维链推理理由评估整个CoT过程。在VCR-Bench上的大量实验突显了当前LVLMs的显著局限性。即使是表现最好的模型o1，也仅实现了62.8%的CoT得分和56.7%的准确率，而大多数模型的得分低于40%。实验表明，大多数模型在感知步骤上的得分低于推理步骤，揭示了LVLMs在复杂视频推理中的关键瓶颈在于时空信息处理。CoT得分与准确率之间的稳健正相关证实了我们的评估框架的有效性，并强调了CoT推理在解决复杂视频推理任务中的关键作用。我们希望VCR-Bench能够作为一个标准化评估框架，揭示复杂视频推理任务中的实际不足。|
|**2025-04-10**|**Porting an LLM based Application from ChatGPT to an On-Premise Environment**|Teemu Paloniemi et.al.|[2504.07907](http://arxiv.org/abs/2504.07907)|null|考虑到机器学习（ML）系统，尤其是大型语言模型（LLM）的数据密集特性，在基于云的环境中使用它们可能会因为与数据隐私和安全相关的法规而成为挑战。考虑到这些方面意味着将LLM迁移到本地环境中，在那里可以控制隐私和安全。在本文中，我们研究了使用ChatGPT将一个实际应用从公共云迁移到本地环境的过程，ChatGPT在该公共云上运行。被迁移的应用是AIPA，一个利用大型语言模型（LLM）和复杂数据分析来增强采购招标评估的系统。迁移过程中的主要考虑因素包括开源模型的透明度和硬件成本，这些都是本地环境的核心设计选择。除了介绍迁移过程外，我们还评估了与迁移相关的利弊。|
|**2025-04-10**|**Redefining Machine Translation on Social Network Services with Large Language Models**|Hongcheng Guo et.al.|[2504.07901](http://arxiv.org/abs/2504.07901)|**[link](https://github.com/HC-Guo/RedTrans)**|随着社交互动的全球化，社交网络服务（SNS）上机器翻译（MT）的需求日益增加，但传统模型在处理像梗、俚语和流行文化引用这类文化细微差别内容时存在困难。尽管大型语言模型（LLM）在通用翻译方面取得了进步，但由于缺乏专门的训练数据和评估基准，它们在SNS特定内容上的表现仍然有限。本文介绍了RedTrans，这是一个针对SNS翻译定制的72B LLM，它通过以下三项创新开发的全新数据集进行训练：（1）使用双重LLM回译采样的监督微调，这是一种利用LLM基于回译的无监督采样方法，用于为大规模微调选择多样化的数据；（2）重写偏好优化（RePO），一种通过专家注释识别和纠正错误偏好对，构建可靠偏好语料库的算法；（3）RedTrans-Bench，这是第一个SNS翻译基准，评估幽默定位、表情符号语义和梗适应等现象。实验表明，RedTrans优于最先进的LLM。此外，RedTrans已经被部署在实际生产环境中，这表明特定领域的适应有效地弥合了通用和基于文化的翻译系统之间的差距。|
|**2025-04-10**|**How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective**|Qi Liu et.al.|[2504.07898](http://arxiv.org/abs/2504.07898)|**[link](https://github.com/liuqi6777/llm-relevance)**|近期研究表明，大型语言模型（LLMs）可以评估相关性并支持信息检索（IR）任务，如文档排名和相关性判断生成。然而，现成LLMs理解和操作相关性的内部机制仍 largely 未被探索。在本文中，我们系统地通过机制可解释性的视角研究不同LLM模块如何贡献于相关性判断。使用激活修补技术，我们分析了各种模型组件的作用，并识别出在生成点wise 或成对相关性判断中的多阶段、渐进过程。具体来说，LLMs首先在早期层提取查询和文档信息，然后在中间层根据指令处理相关性信息，最后在后期层利用特定的注意力头以所需格式生成相关性判断。我们的发现为LLMs中相关性评估的机制提供了洞察，对未来利用LLMs进行IR任务的研究具有宝贵的启示。|
|**2025-04-10**|**Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge**|Riccardo Cantini et.al.|[2504.07887](http://arxiv.org/abs/2504.07887)|**[link](https://github.com/SCAlabUnical/CLEAR-Bias_LLM_benchmark)**|**大型语言模型（LLMs）已经颠覆了人工智能领域，推动了机器翻译、摘要和对话代理等方面的进步。然而，它们越来越多地融入关键社会领域，引发了关于嵌入偏见的问题，这些偏见可能会加剧刻板印象并损害公平性。这些偏见源于多个来源，包括训练数据中的历史不平等、语言不平衡和对抗性操纵。尽管采取了缓解措施，但最近的研究表明，LLMs仍然容易受到旨在引发偏见响应的对抗性攻击。这项工作提出了一种可扩展的基准框架，用于评估LLMs对对抗性偏见引发的鲁棒性。我们的方法包括：（i）系统性地使用多任务方法对模型进行探查，以针对各种社会文化维度的偏见；（ii）通过使用LLM作为裁判的方法，利用安全分数来量化鲁棒性，以实现模型响应的自动化评估；（iii）采用越狱技术来调查安全机制中的漏洞。我们的分析检查了小型和大型最先进模型中普遍存在的偏见及其对模型安全性的影响。此外，我们评估了针对关键领域（如医学）进行微调的特定领域模型的安全性。最后，我们发布了一个关于偏见相关提示的精选数据集CLEAR-Bias，以促进系统性的漏洞基准测试。我们的发现揭示了模型大小与安全性之间的关键权衡，有助于开发更公平和更鲁棒的未来语言模型。**|
|**2025-04-10**|**Token Level Routing Inference System for Edge Devices**|Jianshu She et.al.|[2504.07878](http://arxiv.org/abs/2504.07878)|null|大型语言模型（LLM）推理的计算复杂度显著限制了它们在边缘设备上的部署效率。相比之下，小型语言模型提供更快的解码和更低的资源消耗，但往往遭受响应质量下降和易于产生幻觉的问题。为了解决这一权衡，协作解码，其中大型模型协助生成关键标记，已经成为一个有前景的解决方案。这种范式通过允许大型模型进行选择性干预以实现高质量的推理，同时保持小型模型的速度和效率，利用了这两种模型类型的优势。在本研究中，我们提出了一种新颖的协作解码推理系统，允许小型模型在设备上执行推理，同时有选择性地咨询基于云的大型模型以生成关键标记。令人印象深刻的是，该系统在 CommonsenseQA 上实现了60%的性能提升，使用仅0.5B模型在 M1 MacBook 上运行，只有不到7%的标记生成上传到云中的大型模型。|
|**2025-04-10**|**Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs**|Yichun Yin et.al.|[2504.07866](http://arxiv.org/abs/2504.07866)|null|我们介绍了Pangu Ultra，这是一个拥有1350亿参数的密集Transformer模块大型语言模型（LLM），并在Ascend神经网络处理单元（NPU）上进行了训练。尽管近年来LLM领域在推动LLM的规模和能力方面取得了前所未有的进步，但训练如此大规模的模型仍然涉及到显著的优化和系统挑战。为了稳定训练过程，我们提出了深度缩放三明治归一化，它有效地消除了深度模型训练过程中的损失峰值。我们在1320万亿个多样化和高质量标记上预先训练了我们的模型，并在后训练期间进一步增强了其推理能力。为了高效地进行如此大规模的训练，我们利用了8,192个Ascend NPU和一系列系统优化。在多个不同基准上的评估表明，Pangu Ultra显著提高了密集LLM（如Llama 405B和Mistral Large 2）的最新技术水平，甚至与DeepSeek-R1这样的稀疏模型结构（包含更多参数）相比也取得了具有竞争力的结果。我们的探索表明，Ascend NPU能够高效有效地训练超过1000亿参数的密集模型。我们的模型和系统将可供我们的商业客户使用。|
|**2025-04-09**|**Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning**|Nikhil Shivakumar Nayak et.al.|[2504.07097](http://arxiv.org/abs/2504.07097)|**[link](https://github.com/Red-Hat-AI-Innovation-Team/orthogonal-subspace-learning)**|在大型语言模型（LLMs）中，持续学习容易导致灾难性遗忘，适应新任务会显著降低对先前学习内容的性能。现有方法通常依赖于低秩、参数高效的更新，这限制了模型的表达能力，并为每个任务引入了额外的参数，从而导致可扩展性问题。为了解决这些限制，我们提出了一种利用自适应奇异值分解（SVD）的全新持续全微调方法。我们的方法动态地识别特定任务的低秩参数子空间，并限制更新与先前任务相关的关键方向正交，从而有效地最小化干扰，而无需额外的参数开销或存储先前任务的梯度。我们在标准的持续学习基准上广泛评估了我们的方法，使用了编码器-解码器（T5-Large）和仅解码器（LLaMA-2 7B）模型，涵盖了包括分类、生成和推理在内的多种任务。经验上，我们的方法达到了最先进的成果，平均准确率比最近的基线如O-LoRA高出7%，并且通过将遗忘降低到几乎可以忽略不计的水平，显著地在整个持续学习过程中保持了模型的一般语言能力、指令跟随准确性和安全性。我们的自适应SVD框架有效地平衡了模型的可塑性和知识保留，为大型语言模型中的持续学习场景提供了一个实用、理论有据和计算可扩展的解决方案。|
|**2025-04-09**|**KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs**|Elan Markowitz et.al.|[2504.07087](http://arxiv.org/abs/2504.07087)|null|知识图谱已逐渐成为向大型语言模型（LLM）注入最新、事实性知识的一种流行方法。这通常是通过将知识图谱转换为LLM可以在上下文中处理的文本来实现的。尽管已经提出了多种编码知识图谱的方法，但这一文本化过程对LLM性能的影响仍被低估。我们引入了KG-LLM-Bench，这是一个涵盖五个知识图谱理解任务的全面且可扩展的基准，并评估了不同的编码策略如何影响各种基础模型的表现。我们对七种语言模型和五种文本化策略的广泛实验为优化LLM在知识推理任务上的性能提供了见解。|
|**2025-04-09**|**DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning**|Atharva Pandey et.al.|[2504.07080](http://arxiv.org/abs/2504.07080)|null|尽管在奥林匹克级别的推理问题上有出色的表现，前沿的大型语言模型在遇到标准基准之外的 novel 问题时，在高中数学领域仍然会遇到困难。超越最终准确度，我们提出一个演绎一致性指标来分析语言模型（LMs）的思维链输出。形式上，演绎推理涉及两个子任务：理解一组输入前提，并从中推断出结论。所提出的指标研究 LMs 在这些子任务上的表现，旨在解释 LMs 在 novel 问题上的推理错误：LMs 在输入前提的上下文长度增加时理解得有多好，以及它们在多个推理跳跃中推断结论的能力有多强？由于现有的基准可能被记住，我们开发了一个流程来评估 LMs 在基准问题的 novel、扰动的版本上的演绎一致性。在 novel 小学数学问题（GSM-8k）上，我们发现 LMs 对输入前提数量的增加相当稳健，但随着推理跳跃数量的增加，准确性会显著下降。有趣的是，这些错误在原始基准中被掩盖，因为所有模型都实现了接近 100% 的准确率。随着我们使用合成数据集增加解决方案的步骤数量，与理解输入前提相比，在多个跳跃中进行预测仍然是主要的错误来源。其他因素，如语言风格的转变或早期错误的自然传播，不能解释这些趋势。我们的分析为表征 LM 推理提供了一种新观点——作为输入前提和推理跳跃窗口上的计算——这可以在不同的问题领域中提供统一的评估。|
|**2025-04-09**|**A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models**|Zhouhang Xie et.al.|[2504.07070](http://arxiv.org/abs/2504.07070)|null|个性化偏好对齐对于大型语言模型（LLMs），即根据个体用户的偏好定制LLMs的过程，是一个横跨自然语言处理（NLP）和个人化领域的研究方向。在这篇综述中，我们对LLMs的个性化对齐和建模工作进行了分析。我们介绍了一个偏好对齐技术的分类体系，包括训练时间、推理时间，以及基于用户建模的方法。我们分析了每一组技术的优缺点，并讨论了该领域的评估、基准以及开放性问题。|
|**2025-04-09**|**HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification**|Bibek Paudel et.al.|[2504.07069](http://arxiv.org/abs/2504.07069)|null|本文介绍了一种用于在企业环境中检测大型语言模型（LLM）输出中幻觉的综合系统。我们提出了一种针对企业应用中幻觉的LLM响应的新型分类法，将其分为基于上下文、常识、企业特定和无害的陈述。我们的幻觉检测模型HDM-2验证LLM响应与上下文以及普遍知晓的事实（常识）。它提供幻觉分数和词级注释，能够精确识别问题内容。为了评估其在基于上下文和常识幻觉上的性能，我们引入了一个新的数据集HDMBench。实验结果表明，HDM-2在RagTruth、TruthfulQA和HDMBench数据集上优于现有方法。这项工作解决了企业部署的具体挑战，包括计算效率、领域专业化和细粒度错误识别。我们的评估数据集、模型权重和推理代码公开可用。|
|**2025-04-09**|**TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling**|Liang-Hsuan Tseng et.al.|[2504.07053](http://arxiv.org/abs/2504.07053)|**[link](https://github.com/mtkresearch/taste-spokenlm)**|大型语言模型（LLMs）在基于文本的自然语言处理任务中表现出色，但仍然受限于对文本输入和输出的依赖。为了实现更自然的人机LLM交互，近期的研究进展集中在推导一个既能听又能生成语音的口语语言模型（SLM）。为了实现这一目标，一个有希望的方向是进行语音-文本联合建模。然而，由于模态不匹配，最近的SLM仍然落后于文本LLM。一个显著的差异可以是在语音和文本标记之间的序列长度。为了解决这个问题，我们引入了文本对齐语音标记和嵌入（TASTE）方法，这是一种在标记阶段直接通过将语音标记与相应的文本转录对齐来处理模态差距的方法。我们提出了一种通过特殊的聚合机制，并以语音重建作为训练目标来实现这一目标的方法。我们进行了广泛的实验，并表明TASTE可以在显著减少标记序列长度的同时保留重要的副语言信息。此外，通过利用TASTE，我们可以通过参数高效的微调技术，如低秩适应（LoRA），将基于文本的LLMs调整为有效的SLMs。在包括SALMON和StoryCloze在内的基准任务上的实验结果表明，基于TASTE的SLMs的表现与之前的全微调方法相似。据我们所知，TASTE是第一个利用重建目标自动学习适合口语语言建模的文本对齐语音标记和嵌入的端到端方法。我们的演示、代码和模型在https://github.com/mtkresearch/TASTE-SpokenLM上公开可用。|
|**2025-04-09**|**To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning**|Tian Qin et.al.|[2504.07052](http://arxiv.org/abs/2504.07052)|null|近期大型语言模型的进步显著提升了其推理能力，尤其是通过涉及搜索和回溯的技术。回溯通过允许通过长思维链（CoT）生成进行顺序、线性化探索，自然地扩展了测试时的计算能力。然而，这并非扩展测试时计算能力的唯一策略：使用n中选优的并行采样提供了一种同时生成多样化解决方案的替代方法。尽管顺序搜索的应用越来越广泛，但其相较于并行采样的优势，尤其是在固定的计算预算下，仍然了解不多。在本文中，我们系统地比较了这两种方法在两个具有挑战性的推理任务：CountDown和数独上的表现。令人惊讶的是，我们发现顺序搜索在CountDown上的表现不如并行采样，但在数独上优于它，这表明回溯并非总是有益。我们确定了两个可能导致回溯降低性能的因素：（1）在固定的搜索轨迹上进行训练可能导致模型陷入次优策略；（2）显式的CoT监督可能会阻碍“隐式”（非语言化）推理。将我们的分析扩展到强化学习（RL），我们展示了具有回溯能力的模型从RL微调中受益显著，而缺乏回溯的模型则看到有限的、混合的收益。总之，这些发现挑战了回溯普遍增强LLM推理的假设，反而揭示了任务结构、训练数据、模型规模和学习范式之间的复杂相互作用。|
|**2025-04-09**|**Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety**|Chad Melton et.al.|[2504.07022](http://arxiv.org/abs/2504.07022)|null|生成式大型语言模型（LLMs）的应用正迅速扩展到各个领域，承诺显著提高工作流程效率和信息检索。然而，由于准确性和可靠性方面的担忧，它们在如危险品运输等专业、高风险领域的实施具有挑战性。本研究评估了三种微调的生成模型（ChatGPT、谷歌的Vertex AI和ORNL检索增强生成（RAG）增强的LLaMA 2和LLaMA）在检索对美国危险品运输合规至关重要的监管信息方面的性能。利用约40份公开的联邦和州级监管文件，我们开发了100个与路线规划和许可要求相关的现实查询。根据准确性、详细程度和相关性对响应进行定性评估，辅以对模型输出之间语义相似性的定量评估。结果表明，RAG增强的LLaMA模型在Vertex AI和ChatGPT之上显著表现出色，提供了更详细且通常更准确的信息，尽管偶尔存在不一致之处。这项研究首次引入了RAG在交通安全领域的应用，强调了在特定领域进行微调以及采用严格的评估方法以确保可靠性和最小化高风险环境中不准确的风险的必要性。|
|**2025-04-09**|**LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware**|Nowfel Mashnoor et.al.|[2504.07015](http://arxiv.org/abs/2504.07015)|null|随着现代硬件设计在复杂性和规模上的增长，确保在保密性、完整性和可用性（CIA）三要素上的安全性变得越来越具有挑战性。信息流追踪（IFT）是一种广泛使用的方法，用于追踪数据传播，识别可能损害保密性或/和完整性的未授权活动。然而，传统的IFT方法在可扩展性和适应性方面存在困难，尤其是在高密度和互联的架构中，导致追踪瓶颈，限制了其在大规模硬件中的应用。为了解决这些限制并展示基于Transformer的模型在集成电路（IC）设计中的潜力，本文介绍了LLM-IFT，该模型集成了大型语言模型（LLM）以实现硬件中的IFT过程。LLM-IFT利用LLM驱动的结构化推理来执行分层依赖分析，系统地分解甚至是最复杂的设计。通过多步骤的LLM调用，该框架分析了模块内和模块间的依赖关系，从而实现了全面的IFT评估。通过在IP级和SoC级针对一组Trust-Hub漏洞测试用例进行实验，我们的实验证明了在硬件的保密性和完整性检查中IFT分析的准确率达到100%。|
|**2025-04-09**|**Towards LLMs Robustness to Changes in Prompt Format Styles**|Lilian Ngweta et.al.|[2504.06969](http://arxiv.org/abs/2504.06969)|null|近年来，大型语言模型（LLMs）因其在各种应用中的实用性而受到广泛关注。然而，它们对提示格式中的非语义变化敏感，提示格式的微小变化可能导致性能显著波动。在文献中，这个问题通常被称为提示脆弱性。先前关于提示工程的研究主要集中在开发识别特定任务最优提示的技术。一些研究也探讨了提示脆弱性问题，并提出了量化性能变化的方法；然而，尚未找到简单的解决方案来应对这一挑战。我们提出了混合格式（MOF），这是一种简单高效的技术，通过在提示的少样本示例中多样化使用风格来解决LLMs中的提示脆弱性问题。MOF受到了计算机视觉技术的启发，这些技术利用多样化的风格数据集来防止模型将特定风格与目标变量关联起来。实证结果表明，我们提出的技术降低了各种LLMs中由风格引起的提示脆弱性，同时也在提示变化和不同数据集上提高了整体性能。|
|**2025-04-08**|**GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning through Bayesian Optimization**|Bojana Ranković et.al.|[2504.06265](http://arxiv.org/abs/2504.06265)|**[link](https://github.com/schwallergroup/gollum)**|大型语言模型（LLMs）可以在其潜在空间中编码复杂关系，但利用它们在不确定性下的优化仍然具有挑战性。我们通过一种新颖的架构来填补这一空白，该架构将LLM微调重新定义为通过深度核方法进行高斯过程（GP）边缘似然优化。我们引入基于LLM的深度核，与GPs联合优化，以保留两者的优势——LLMs为贝叶斯优化提供丰富且灵活的输入空间，GPs则用预测不确定性来建模此空间，以便更有效地采样。应用于Buchwald-Hartwig反应优化，我们的方法将高性能反应的发现率几乎翻倍（与静态LLM嵌入相比，仅50次优化迭代就将前5%反应的覆盖率从24%提高到43%）。我们还观察到，在不需要专门特征的情况下，该方法比特定领域的表示提高了14%。在19个基准测试（从一般化学到反应和分子性质优化）中进行的大量实证评估表明，我们的方法在以下方面的稳健性、通用性和持续改进：1）任务，2）LLM架构（编码器、解码器、编码器-解码器），3）预训练领域（与化学相关或通用）和4）超参数设置（在单个数据集上一次调整）。最后，我们解释了这些改进：通过边缘似然进行的LLM-GP联合优化隐式地执行对比学习，使表示对齐，产生（1）结构更好的嵌入空间，（2）改进的不确定性校准和（3）更有效的采样——而不需要任何外部损失。这项工作在样本高效优化方面提供了实际进展，并揭示了有效贝叶斯优化的关键因素。|
|**2025-04-08**|**Hogwild! Inference: Parallel LLM Generation via Concurrent Attention**|Gleb Rodionov et.al.|[2504.06261](http://arxiv.org/abs/2504.06261)|**[link](https://github.com/eqimp/hogwild_llm)**|大型语言模型（LLMs）通过高级推理、长篇内容生成和使用工具的能力，展示了处理日益复杂任务的能力。解决这些任务通常涉及长时间的推理计算。在人类问题解决中，加快工作的常见策略是合作：通过将问题分解为子任务、同时探索不同的策略等。最近的研究表明，LLMs也可以通过实施显式的合作框架，如投票机制或显式创建可并行执行的独立子任务来实现并行操作。然而，这些框架可能并不适用于所有类型的任务，这可能会阻碍它们的适用性。在这项工作中，我们提出了不同的设计方法：我们并行运行LLM "工作者"，允许它们通过一个同时更新的注意力缓存进行同步，并提示这些工作者决定如何最佳地合作。我们的方法允许实例为当前的问题制定自己的合作策略，同时“看到”在并发缓存中的彼此部分进度。我们通过Hogwild!推理实现这一方法：一个并行LLM推理引擎，其中同一LLM的多个实例在具有相同注意力缓存的情况下并行运行，并能够即时访问彼此生成的标记。Hogwild!推理利用旋转位置嵌入（RoPE）来避免重新计算，同时提高并行硬件的利用率。我们发现，现代具有推理能力的LLMs可以不经过额外微调，直接使用共享键值缓存进行推理。|
|**2025-04-08**|**FEABench: Evaluating Language Models on Multiphysics Reasoning Ability**|Nayantara Mudur et.al.|[2504.06260](http://arxiv.org/abs/2504.06260)|**[link](https://github.com/google/feabench)**|构建对现实世界的精确模拟，并使用数值求解器来解决定量问题是工程和科学领域的基本需求。我们提出了FEABench，这是一个用于评估大型语言模型（LLMs）和LLM代理使用有限元分析（FEA）模拟和解决物理、数学和工程问题的能力的基准。我们引入了一种全面的评估方案，以研究LLMs通过推理自然语言问题描述并操作COMSOL Multiphysics®（一种FEA软件）来计算答案的能力，从而解决这些问题。此外，我们设计了一种语言模型代理，该代理具有通过其应用程序编程接口（API）与软件交互、检查其输出并使用工具在多次迭代中改进其解决方案的能力。我们表现最佳的战略有88%的时间生成可执行的API调用。能够成功与FEA软件交互并操作以解决如我们基准中问题的LLMs将推动工程自动化的前沿。获得这种能力将增强LLMs的推理能力，并使用数值求解器的精度，推进能够解决现实世界复杂问题的自主系统的开发。代码可在https://github.com/google/feabench获取。|
|**2025-04-08**|**LExT: Towards Evaluating Trustworthiness of Natural Language Explanations**|Krithi Shailya et.al.|[2504.06227](http://arxiv.org/abs/2504.06227)|null|随着大型语言模型（LLMs）在高风险领域的日益融合，已有多种方法被提出用于生成自然语言解释。这些解释对于增强模型的可解释性至关重要，尤其是在透明度和可靠性至关重要的敏感领域，如医疗保健。鉴于LLMs生成的解释及其已知的问题，评估模型生成解释的稳健评估框架的需求日益增长。自然语言生成指标如BLEU和ROUGE捕获了句法和语义准确性，但忽略了其他关键方面，如事实准确性、一致性和忠实度。为了解决这一差距，我们提出了一种量化自然语言解释可信度的通用框架，平衡可信性和忠实度，以推导出全面的语言解释可信度评分（LExT）（我们的实验代码和设置在https://github.com/cerai-iitm/LExT上公开）。将我们的领域无关框架应用于医疗保健领域并使用公共医疗数据集，我们评估了包括特定领域和通用模型在内的六个模型。我们的发现表明，它们生成可信解释的能力存在显著差异。在比较这些解释时，我们观察到一些有趣的现象，如通用模型在忠实度上的不一致性以及它们优于特定领域微调模型的趋势。这项工作进一步强调了使用定制评估框架来评估敏感领域的自然语言解释的重要性，为提高医疗保健等领域语言模型的可信度和透明度提供了基础。|
|**2025-04-08**|**Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation**|Biao Zhang et.al.|[2504.06225](http://arxiv.org/abs/2504.06225)|null|尽管仅解码器的大型语言模型（LLMs）已经显示出令人印象深刻的结果，但编码器-解码器模型由于推理效率和更丰富的编码器表示，在现实世界的应用中仍然被广泛采用。在本文中，我们研究了一个新的问题：将预训练的仅解码器LLMs适配到编码器-解码器模型，目标是利用两种方法的优势以实现更佳的质量-效率权衡。我们认为，适配不仅能够继承仅解码器LLMs的能力，而且与从头预训练相比，减少了计算需求。我们严格探索了不同的预训练目标以及参数初始化/优化技术。通过基于Gemma 2（2B和9B）和一套新预训练的mT5规模模型（高达1.6B）的广泛实验，我们证明了适配的有效性和编码器-解码器LLMs的优势。在类似的推理预算下，编码器-解码器LLMs在预训练性能上与仅解码器模型相当（通常更好），但在微调性能上则显著更优。例如，经过指令调整后，Gemma 2B-2B的性能优于Gemma 2B约7%。编码器-解码器适配还允许灵活地组合不同规模的模型，其中Gemma 9B-2B比Gemma 2B-2B高3%以上。适配的编码器表示在SuperGLUE上也取得了更好的结果。我们将发布我们的检查点以促进未来的研究。|
|**2025-04-08**|**Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs**|Dongyang Fan et.al.|[2504.06219](http://arxiv.org/abs/2504.06219)|null|随着网络内容版权持有者越来越多地采用网络爬虫豁免措施，关于数据合规性对大型语言模型（LLM）性能的影响引发了关键问题。然而，关于这些限制（以及由此产生的预训练数据集的过滤）如何影响使用这些语料库训练的模型的能力，知之甚少。在本研究中，我们将这种影响概念化为“数据合规性差距”（DCG），它量化了在遵守网络爬虫豁免措施的数据集上训练的模型与不遵守的数据集上训练的模型之间的性能差异。我们在两种设置中测量数据合规性差距：从头开始预训练模型和从现有合规模型进行持续预训练（模拟一种情况，即在预训练后期可以整合受版权保护的数据）。我们的1.5亿个模型的实验表明，截至2025年1月，遵守网络数据豁免措施并不会降低一般知识获取（接近0%的DCG）。然而，在生物医学研究等专门领域，排除主要出版商会导致性能下降。这些发现表明，尽管通用LLM可以通过使用完全开放的数据进行训练而表现相同，但在专门领域中的性能可能从训练后期获取高质量版权来源中受益。我们的研究为长期争论的数据合规性与下游模型性能之间的权衡提供了经验见解，为未来关于AI训练实践和政策决策的讨论提供了信息。|
|**2025-04-08**|**From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models**|Chejian Xu et.al.|[2504.06214](http://arxiv.org/abs/2504.06214)|null|长上下文能力对于包括文档和视频理解、情境学习和推理时间扩展等多种应用至关重要，这些应用都需要模型能够处理和推理长序列的文本和多模态数据。在本研究中，我们介绍了一种高效的训练方法，用于从对齐的指令模型构建超长上下文大型语言模型（LLMs），将上下文长度的边界从128K扩展到1M、2M和4M个标记。我们的方法利用高效的持续预训练策略来扩展上下文窗口，并采用有效的指令微调来维持指令遵循和推理能力。基于Llama3.1-Instruct并使用我们的方法构建的UltraLong-8B，在多个长上下文基准测试中实现了最先进的性能。重要的是，使用我们方法训练的模型在标准基准测试中保持了有竞争力的性能，证明了在长上下文和短上下文任务上都实现了平衡的改进。我们进一步对关键设计选择进行了深入分析，突出了扩展策略和数据组成的影响。我们的发现建立了一个稳健的框架，能够在保持模型通用能力的同时有效地扩展上下文长度。我们将在以下网址发布所有模型权重：https://ultralong.github.io/。|
|**2025-04-08**|**TxGemma: Efficient and Agentic LLMs for Therapeutics**|Eric Wang et.al.|[2504.06196](http://arxiv.org/abs/2504.06196)|null|治疗药物开发是一项成本高昂且风险极高的工作，通常面临着高失败率。为了解决这个问题，我们引入了TxGemma，这是一套高效的通用大型语言模型（LLMs），能够进行治疗特性预测，以及交互推理和可解释性。与特定任务的模型不同，TxGemma能够从不同的来源综合信息，使其在治疗药物开发流程中具有广泛的应用。该套件包括从Gemma-2微调的2B、9B和27B参数模型，基于小分子、蛋白质、核酸、疾病和细胞系等综合数据集进行微调。在66项治疗药物开发任务中，TxGemma在64项（其中45项优于）上实现了优于或相当于最先进的通用模型，在50项（其中26项优于）上优于最先进的专家模型。在治疗下游任务，如临床试验不良事件预测上对TxGemma模型进行微调，所需训练数据少于对基础LLMs的微调，这使得TxGemma适用于数据有限的应用。除了这些预测能力之外，TxGemma还具备对话模型，它弥合了通用LLMs和特定属性预测器之间的差距。这些模型允许科学家以自然语言进行交互，根据分子结构提供机制推理，并参与科学讨论。在此基础上，我们进一步介绍了Agentic-Tx，这是一个由Gemini 2.5驱动的通用治疗代理系统，它能进行推理、执行、管理多样化的工作流程，并获取外部领域知识。Agentic-Tx在“人类最后考试”基准（化学与生物学）上超越了之前的领先模型，相对于o3-mini（高）提高了52.3%，在GPQA（化学）上提高了26.7%，在ChemBench-Preference上提高了6.3%，在ChemBench-Mini上提高了2.4%。|
|**2025-04-08**|**Assessing how hyperparameters impact Large Language Models' sarcasm detection performance**|Montgomery Gole et.al.|[2504.06166](http://arxiv.org/abs/2504.06166)|null|讽刺检测对人类和机器来说都具有挑战性。这项工作探讨了模型特征如何影响OpenAI的GPT和Meta的Llama-2模型在讽刺检测中的表现，鉴于它们强大的自然语言理解能力和受欢迎程度。我们评估了不同大小、版本和超参数的微调和零样本模型。实验在流行的Self-Annotated Reddit Corpus (SARC2.0)讽刺数据集的政治和平衡（pol-bal）部分进行。在模型家族内，微调性能随着模型大小的增加而单调提高，而超参数调整也对性能有影响。在微调场景中，Llama-2-13b的全精度模型达到了最先进的准确率和 $F_1$分数，均为0.83，与平均人类表现相当。在零样本设置中，一个GPT-4模型实现了与先前尝试相当的性能，准确率为0.70，$F_1$ 分数为0.75。此外，模型的性能可能随着每个版本的发布而提高或下降，这突显了每次发布后都需要重新评估性能的必要性。|
|**2025-04-08**|**Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups**|Rijul Magu et.al.|[2504.06160](http://arxiv.org/abs/2504.06160)|null|大型语言模型（LLMs）已被证明对某些群体存在不平衡的偏见。然而，关于LLMs对风险群体发起的无端针对性攻击的研究仍然很少。我们的论文提出了三个新的贡献：（1）明确评估LLMs生成的攻击对高度脆弱的心理健康群体的效果；（2）一个基于网络的框架来研究相对偏见的传播；（3）评估这些攻击引发的相对程度的社会污名化。我们分析了一个最近发布的大规模偏见审计数据集，发现心理健康实体在攻击叙事网络中占据中心位置，这通过显著更高的平均接近度（p值=4.06e-10）和紧密聚类（基尼系数=0.7）得以体现。从污名化理论的社会学基础出发，我们的污名化分析表明，与生成链中的初始目标相比，心理健康疾病相关目标具有更多的标签成分。总的来说，这些洞察揭示了大型语言模型在增强有害言论方面的结构偏好，并强调了缓解措施所需的适当方法。|
|**2025-04-07**|**URECA: Unique Region Caption Anything**|Sangbeom Lim et.al.|[2504.05305](http://arxiv.org/abs/2504.05305)|null|区域级标题生成旨在为特定图像区域生成自然语言描述，同时突出其独特特征。然而，现有方法在跨多粒度生成独特标题方面存在困难，限制了其在现实世界中的应用。为了满足对详细区域级理解的需求，我们引入了URECA数据集，这是一个为多粒度区域标题生成量身定制的大规模数据集。与主要关注显著物体的先前数据集不同，URECA数据集通过纳入多样化的物体、部分和背景元素，确保了区域和标题之间独特且一致的映射。其核心是一个分阶段的资料整理流程，每个阶段逐步细化区域选择和标题生成。通过在每个阶段利用多模态大型语言模型（MLLMs），我们的流程产生了具有改进准确性和语义多样性的独特且上下文相关的标题。基于这个数据集，我们提出了URECA，这是一个旨在有效编码多粒度区域的创新标题模型。URECA通过简单而影响深远的修改现有MLLMs，保持了诸如位置和形状等基本空间属性，实现了精细和语义丰富的区域描述。我们的方法引入了动态掩码建模和高分辨率掩码编码器，以增强标题的独特性。实验表明，URECA在URECA数据集上实现了最先进的性能，并且很好地推广到现有的区域级标题生成基准。|
|**2025-04-07**|**Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations**|Pedro Ferreira et.al.|[2504.05294](http://arxiv.org/abs/2504.05294)|null|思维链解释被广泛用于检查大型语言模型（LLMs）的决策过程以及评估模型输出的可信度，对于LLMs与人类的有效协作至关重要。我们证明，偏好优化——对齐阶段的关键步骤——可能会无意中降低这些解释的忠实度。这是因为引导对齐的奖励模型（RM）被要求同时优化响应的预期质量和解释的适当性（例如，最小化偏差或遵守安全标准），从而产生潜在冲突。RM缺乏一种机制来评估模型内部决策过程与生成的解释之间的连贯性。因此，LLM可能会通过产生一个最终响应得分很高，而解释却是为了最大化奖励而非准确反映其推理的“奖励黑客”行为。为了解决这个问题，我们提出通过丰富RM的输入以包含预测的因果归属，使RM能够检测生成的自我解释与模型决策过程之间的差异。在控制环境中，我们展示了这种方法减少了LLM生成误导性解释的倾向。|
|**2025-04-07**|**The challenge of uncertainty quantification of large language models in medicine**|Zahra Atf et.al.|[2504.05278](http://arxiv.org/abs/2504.05278)|null|本项研究探讨了在大规模语言模型（LLMs）在医疗应用中的不确定性量化，强调技术创新和哲学意义。随着LLMs成为临床决策的不可或缺部分，准确传达不确定性对于确保可靠、安全和道德的AI辅助医疗至关重要。我们的研究将不确定性视为知识的必要部分，而非障碍，并倡导以动态和反思的方式设计AI。通过整合先进的概率方法，如贝叶斯推理、深度集成和蒙特卡洛 dropout，以及计算预测和语义熵的语言学分析，我们提出了一种综合框架，该框架管理认知和随机不确定性。该框架包括代理建模来解决专有API的限制、多源数据集成以获得更好的上下文，以及通过持续和元学习进行动态校准。通过不确定性图和置信度指标嵌入可解释性，以支持用户信任和临床可解释性。我们的方法支持与负责任和反思性AI原则一致的透明和道德决策。在哲学上，我们主张接受受控的不确定性，而不是追求绝对的可预测性，认识到医学知识的内在暂时性。|
|**2025-04-07**|**Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation**|Yucheng Chu et.al.|[2504.05276](http://arxiv.org/abs/2504.05276)|null|简答评估是科学教育的重要组成部分，它允许评估学生对复杂的三维理解的掌握情况。具备类似人类语言任务能力的大型语言模型（LLMs）在协助人工评分员减轻工作量方面越来越受欢迎。然而，LLMs在领域知识方面的局限性限制了它们对特定任务要求的理解，并阻碍了它们实现满意性能的能力。检索增强生成（RAG）作为一种有潜力的解决方案出现，它使LLMs能够在评估过程中访问相关的领域特定知识。在这项工作中，我们提出了一种自适应的RAG框架，用于自动化评分，该框架根据问题和学生答案的上下文动态检索和整合领域特定知识。我们的方法结合语义搜索和精选的教育资源来检索有价值的参考资料。在科学教育数据集上的实验结果表明，与基线LLM方法相比，我们的系统在评分准确度上有所提升。这些发现表明，RAG增强的评分系统可以作为可靠的辅助工具，带来高效的性能提升。|
|**2025-04-07**|**Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models**|Yang Yan et.al.|[2504.05262](http://arxiv.org/abs/2504.05262)|null|尽管基准测试得分很高，大型语言模型（LLMs）在解决简单问题时常常失败，这引发了一个关键问题：LLMs是学习数学原理还是仅仅记住模式？我们并没有像最近的研究那样设计越来越复杂的基准，而是使用基本的两个整数加法（从0到 $2^{64}$）来探究两个核心属性：交换律（$A+B=B+A$）和组合泛化（通过同构符号映射，例如，$7 \rightarrow y$）。尽管最先进的LLMs在数值加法上实现了73.8%-99.8%的准确率，但在符号映射下的性能下降到$\leq$7.5%，这表明LLMs未能泛化所学习的规则。随着数字数量的非单调性能缩放和频繁的交换律违反（超过1700个$A+B \neq B+A$ 的案例）进一步支持这一点。明确提供加法规则将性能平均降低了81.2%，而自我解释则保持了基准准确率，这表明LLMs的算术处理与人类定义的原理不匹配。我们的发现表明，当前的LLMs依赖于记忆模式而非真正的规则学习，突显了架构上的限制，并强调了需要新的方法来实现真正的数学推理。|
|**2025-04-07**|**Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models**|Adrián Bazaga et.al.|[2504.05258](http://arxiv.org/abs/2504.05258)|null|大型语言模型（LLMs）已成为生成连贯文本、理解上下文和执行推理任务的有力工具。然而，它们在时间推理方面存在困难，这需要处理与时间相关的信息，如事件顺序、持续时间以及时间关系。这些能力对于问答、调度和历史分析等应用至关重要。在本文中，我们介绍了一种名为TISER的新框架，通过结合时间线构建和迭代自我反思的多阶段过程，增强了LLMs的时间推理能力。我们的方法利用测试时缩放来扩展推理痕迹的长度，使模型能够更有效地捕捉复杂的时序依赖关系。这种策略不仅提高了推理的准确性，还改善了推理过程的可追溯性。实验结果表明，TISER在多个基准测试中实现了最先进的性能，包括分布外测试集，并揭示了TISER使得小型开源模型在具有挑战性的时间推理任务上超越了大型闭源模型。|
|**2025-04-07**|**Explaining Low Perception Model Competency with High-Competency Counterfactuals**|Sara Pohland et.al.|[2504.05254](http://arxiv.org/abs/2504.05254)|null|存在许多方法可以解释图像分类模型是如何做出决策的，但很少有研究探讨为什么分类器可能会对其预测缺乏信心。由于分类器可能失去信心的原因有很多，因此对于这个模型来说，仅仅表明其不确定性的程度是不够的，还需要解释其不确定的原因。反事实图像已被用于可视化可以对图像进行哪些修改以生成不同的分类决策。在这项工作中，我们探讨了使用反事实来解释低模型能力——这是一种衡量信心的预测不确定性的广义形式。为此，我们开发了五种生成高能力反事实图像的新方法，即图像梯度下降（IGD）、特征梯度下降（FGD）、自动编码器重建（Reco）、潜在梯度下降（LGD）和潜在最近邻（LNN）。我们在包含具有六个已知低模型能力原因的图像的两个独特数据集上评估了这些方法，发现Reco、LGD和LNN是生成反事实的最有前途的方法。我们进一步评估了这些三种方法如何被预训练的多模态大型语言模型（MLLMs）利用来生成低模型能力原因的语言解释。我们发现，在语言模型查询中包含反事实图像极大地提高了模型生成关于低模型能力原因准确解释的能力，从而证明了反事实图像在解释低感知模型能力方面的效用。|
|**2025-04-07**|**LLM-based Automated Grading with Human-in-the-Loop**|Hang Li et.al.|[2504.05239](http://arxiv.org/abs/2504.05239)|null|人工智能（AI）技术的兴起，特别是大型语言模型（LLMs）的崛起，为教育领域带来了显著进步。在各种应用中，自动简答题评分（ASAG），专注于评估开放式文本回答，在LLMs的引入下取得了显著的进展。这些模型不仅比传统的ASAG方法提高了评分性能，而且超越了与预定义的“黄金”答案的简单比较，使得评分场景更加复杂，如基于评分标准的评价。然而，现有的基于LLMs的方法由于依赖完全自动化的方法，在基于评分标准的评估中仍然面临着达到人类水平评分性能的挑战。在这项工作中，我们通过利用LLMs的交互能力，通过人机交互（HITL）方法探索了LLMs在ASAG任务中的潜力。我们提出的框架GradeHITL利用LLMs的生成特性向人类专家提问，结合他们的见解动态调整评分标准。这个自适应过程显著提高了评分准确性，超越了现有方法，使ASAG更接近人类水平评价。|
|**2025-04-07**|**Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG**|Hengran Zhang et.al.|[2504.05220](http://arxiv.org/abs/2504.05220)|null|检索模型通常依赖于昂贵的由人工标注的查询-文档相关性标注进行训练和评估。为了降低成本并利用大型语言模型（LLMs）在相关性判断中的潜力，我们旨在探索LLM生成的标注是否可以有效地替代人工标注来训练检索模型。检索通常强调相关性，这表明文档与查询之间的“主题相关性”，而在RAG中，文档的价值（或效用）取决于它对答案生成的贡献。认识到这种不匹配，一些研究人员使用LLM在具有文档标签的下游任务上的性能，但这种方法需要为特定任务手动提供答案，导致成本高昂且泛化能力有限。在另一条研究线上，提示LLMs选择有用的文档作为RAG参考，消除了对人工标注的需求，且不针对特定任务。如果我们利用LLMs的效用判断来标注检索数据，我们可能在大型语料库中保留跨任务的泛化能力，无需人工标注。因此，我们研究了通过LLMs进行以效用为重点的标注，用于检索和RAG任务的大规模检索器训练数据，无论是在领域内还是在领域外设置。为了减少LLMs标注的低质量正样本的影响，我们设计了一种新的损失函数，即Disj-InfoNCE。我们的实验表明：(1)在领域外设置中，基于效用标注训练的检索器在两个任务上都显著优于基于人工标注训练的检索器，证明了其优越的泛化能力。(2)在领域内设置中，LLM标注并未取代人工标注。然而，仅包含20%的人工标注数据，就能使使用效用标注训练的检索器达到完全使用人工标注训练的模型的性能。|
|**2025-04-07**|**Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling**|Hengran Zhang et.al.|[2504.05216](http://arxiv.org/abs/2504.05216)|null|密集检索是信息检索（IR）中的一个关键任务，也是诸如再排序等下游任务的基础。最近，大型语言模型（LLMs）展现出了令人信服的语义理解能力，这吸引了研究密集检索的学者。作为解码式生成模型，LLMs在语言生成方面表现出色，但由于对后续标记缺乏关注，在建模全局信息方面存在不足。受信息检索中基于词的经典语言建模方法，即查询似然（QL）模型的启发，我们通过最大化QL来充分利用LLMs的生成能力。然而，我们并非通过QL估计对文档进行排序，而是引入了一个QL最大化的辅助任务，以获得更好的对比学习判别检索器的基础。我们将我们的模型命名为LLM-QL。为了在QL建模过程中将全局文档语义浓缩为单个向量，LLM-QL有两个主要组件，即注意力停止（AS）和输入破坏（IC）。AS阻止预测标记对文档前标记的注意力，直到文档的结束标记。IC在预测过程中遮蔽输入文档中的一部分标记。在MSMARCO上的实验表明，LLM-QL的性能显著优于其他基于LLM的检索器，并且使用LLM-QL估计的QL进行排序，比基于词的QL有大幅度的提升。|
|**2025-04-04**|**Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning**|Xinyi Wang et.al.|[2504.03635](http://arxiv.org/abs/2504.03635)|null|大型语言模型（LLMs）在需要复杂推理的广泛任务中展现出令人瞩目的能力。然而，扩展对它们推理能力的影响仍未得到充分理解。在本文中，我们介绍了一个合成的多跳推理环境，旨在紧密模拟真实世界大规模知识图谱的结构和分布。我们的推理任务涉及在图中补全缺失的边，这需要高级的多跳推理，并模拟真实世界的推理场景。为了评估这一点，我们从头开始在Incomplete Graph的三元组上预训练语言模型（LMs），并评估其推断缺失边的能力。有趣的是，我们发现过参数化会由于过度记忆而损害推理性能。我们研究了影响这种U形损失曲线的不同因素，包括图结构、模型大小和训练步数。为了预测特定知识图的最佳模型大小，我们发现了一种经验缩放，该缩放将知识图谱搜索熵线性映射到最佳模型大小。这项工作为LLMs中扩展与推理之间的关系提供了新的见解，揭示了优化它们推理任务性能的可能方法。|
|**2025-04-04**|**Align to Structure: Aligning Large Language Models with Structural Information**|Zae Myung Kim et.al.|[2504.03622](http://arxiv.org/abs/2504.03622)|null|生成长篇连贯的文本对大型语言模型（LLMs）来说仍然是一个挑战，因为它们在话语生成中缺乏层次化的规划和结构化的组织。我们介绍了一种名为结构对齐（Structural Alignment）的新方法，该方法通过将LLMs与类似人类的语言结构对齐来提升长篇文本生成能力。通过将基于语言基础的语篇框架集成到强化学习中，我们的方法引导模型生成连贯且组织良好的输出。我们在近端策略优化（Proximal Policy Optimization）框架内采用密集的奖励机制，基于与人类写作相对的语篇独特性，分配细粒度、基于标记的奖励。我们评估了两种互补的奖励模型：第一种通过评分表面层面的文本特征来提高可读性，提供明确的结构化；而第二种通过分析通过层次化语篇动机的全局语篇模式，强化更深层次的连贯性和修辞复杂度，在如论文生成和长文档摘要等任务上优于标准模型和RLHF增强模型。所有训练数据和代码将在https://github.com/minnesotanlp/struct_align上公开共享。|
|**2025-04-04**|**VISTA-OCR: Towards generative and interactive end to end OCR models**|Laziz Hamdi et.al.|[2504.03621](http://arxiv.org/abs/2504.03621)|null|我们引入了\textbf{VISTA-OCR}（视觉和空间感知文本分析OCR），这是一个轻量级的架构，它将文本检测和识别统一在一个生成模型中。与需要分别分支并配备专用参数进行文本识别和检测的传统方法不同，我们的方法利用Transformer解码器在统一分支中顺序生成文本转录和它们的空间坐标。VISTA-OCR建立在编码器-解码器架构之上，通过视觉特征提取阶段逐步训练，随后进行多任务学习，实现多模态标记生成。为了解决对多功能OCR系统的需求日益增长，这些系统能够执行高级任务，如基于内容的文本定位\ref{content_based_localization}，我们在预训练期间引入了新的提示可控OCR任务。为了增强模型的能力，我们构建了一个新的数据集，该数据集由带有关联边界框注释的真实世界示例和合成样本丰富而成。尽管最近的视觉大型语言模型（VLLMs）可以高效地执行这些任务，但它们高昂的计算成本仍然是实际部署的障碍。相比之下，我们的VISTA $_{\text{omni}}$ 变体仅通过提示即可交互式地处理手写和印刷文档，只需150M参数。在多个数据集上进行的广泛实验表明，VISTA-OCR在标准OCR任务上实现了比最先进的专用模型更好的性能，并显示出在更复杂的OCR应用中的强大潜力，满足了交互式OCR系统日益增长的需求。VISTA-OCR的所有代码和注释将在接受后公开提供。|
|**2025-04-04**|**Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task**|Leonardo Ranaldi et.al.|[2504.03616](http://arxiv.org/abs/2504.03616)|null|检索增强生成（RAG）已成为当代自然语言处理（NLP）的基石，通过允许大型语言模型（LLMs）通过上下文检索访问更丰富的知识背景来增强其功能。虽然它在单语环境中特别在英语中效果显著，但其多语言任务中的应用尚未被充分探索。本文通过提出新的多语言开放域问答方法，调查了RAG在多种语言中的有效性。我们评估了各种多语言RAG策略的性能，包括问题翻译（tRAG），该方法在检索之前将问题翻译成英语，以及多语言RAG（MultiRAG），其中检索直接跨越多种语言进行。我们的研究发现，尽管tRAG有用，但其覆盖范围有限。相比之下，MultiRAG通过实现多语言检索来提高效率，但检索内容的跨语言差异引入了不一致性。为了解决这些问题，我们提出了跨语言RAG（CrossRAG），这是一种在生成响应之前将检索到的文档翻译成通用语言（例如英语）的方法。我们的实验表明，CrossRAG在知识密集型任务上的性能显著提高，既有利于资源丰富的语言，也利于资源匮乏的语言。|
|**2025-04-04**|**AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset**|Bingxiang He et.al.|[2504.03612](http://arxiv.org/abs/2504.03612)|null|偏好学习对于将大型语言模型（LLMs）与人类价值观相一致至关重要，但其成功依赖于包含三个核心组件的高质量数据集：偏好标注、指令和响应对。当前的方法混淆了这些组件，掩盖了它们各自的影响，阻碍了系统优化。在这项工作中，我们提出了AIR，这是一个组件分析框架，能够系统地隔离和优化每个组件，同时评估它们的协同效应。通过严格的实验，AIR揭示了可操作的原则：标注的简单性（点状生成评分）、指令推理的稳定性（基于LLMs的方差过滤）和响应对的质量（适中的边界 + 高绝对分数）。当这些原则结合在一起时，与基线方法相比，平均提高了+5.3，即使只有14k个高质量对。我们的工作将偏好数据集设计从偶然扩展转变为组件感知优化，为高效、可重复的校准提供了蓝图。|
|**2025-04-04**|**EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline**|Peter Baile Chen et.al.|[2504.03598](http://arxiv.org/abs/2504.03598)|null|现有的信息检索系统在目标文档语言与用户查询语言高度匹配的情况下表现出色。然而，现实中的检索系统往往需要隐式地推理文档的相关性。例如，在检索技术文本或表格时，它们与用户查询的相关性可能通过特定的术语或结构暗示，而不是在其内容中明确表达。大型语言模型（LLMs）通过利用其推理能力，在识别此类暗示的相关性方面具有巨大潜力。然而，当前的LLM增强检索受到高延迟和计算成本的阻碍，因为LLM通常会对每个查询在线重新计算查询-文档的相关性。为了解决这个问题，我们引入了EnrichIndex，这是一种检索方法，它不是在检索过程中在线使用LLM，而是通过在摄取时对所有检索语料库中的文档进行单次遍历来构建语义丰富的检索索引。此外，语义丰富的索引可以补充现有的在线检索方法，提高LLM重排器的性能。我们在五个涉及段落和表格的检索任务上评估了EnrichIndex，发现它优于强大的基于LLM的在线检索系统，与强基线相比，召回率@10的平均提高11.7点，NDCG@10提高10.6点。在LLM的在线调用方面，它处理的令牌数量减少了293.3倍，这大大降低了在线延迟和成本。总的来说，EnrichIndex是利用LLM强大的推理能力在离线构建更好的检索索引的有效方法。|
|**2025-04-04**|**Agentic Knowledgeable Self-awareness**|Shuofei Qiao et.al.|[2504.03553](http://arxiv.org/abs/2504.03553)|**[link](https://github.com/zjunlp/knowself)**|大型语言模型（LLMs）在各种代理规划任务上取得了显著性能。然而，传统的代理规划方法采用了一种“漫灌”的方法论，不加区分地将金轨迹、外部反馈和领域知识注入到代理模型中。这种做法忽视了人类在决策过程中的一项基本认知原则——情境自我意识——即动态评估情境需求并在决策中战略性地运用资源的能力。为了解决这一差距，我们提出了代理知识自我意识，这是一种新型范式，使得基于LLM的代理能够自主调节知识利用。具体来说，我们提出了KnowSelf，这是一种以数据为中心的方法，将类似于人类的具有知识自我意识的代理应用于其中。具体来说，我们设计了一种启发式情境判断标准，在代理自我探索的轨迹上标记特殊标记，以收集训练数据。通过两阶段训练过程，代理模型可以通过生成特定的特殊标记在不同的情境之间切换，以最小的成本实现最优的规划效果。我们的实验表明，KnowSelf在不同任务和模型上可以超越各种强大基线，且最小化外部知识的使用。代码可在https://github.com/zjunlp/KnowSelf上找到。|
|**2025-04-04**|**Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles**|Chen Wei Kuo et.al.|[2504.03520](http://arxiv.org/abs/2504.03520)|null|新闻报道中的偏见对公众认知产生重大影响，尤其是在犯罪、政治和社会问题方面。传统的偏见检测方法主要依赖人工监管，存在主观解读和可扩展性限制。在此，我们引入一个利用高级大型语言模型（LLMs），特别是GPT-4o、GPT-4o Mini、Gemini Pro、Gemini Flash、Llama 8B和Llama 3B的人工智能驱动框架，以系统性地识别和减轻新闻文章中的偏见。为此，我们收集了一个包含超过30,000篇与犯罪相关的文章的大规模数据集，这些文章来自五个政治上多样化的新闻来源，跨越十年（2013-2023）。我们的方法采用两阶段方法论：（1）偏见检测，其中每个LLM对段落级别的偏见内容进行评分和论证，通过人工评估验证以建立真实情况，以及（2）使用GPT-4o Mini进行迭代去偏见，通过自动重新评估和人工审查进行验证。实证结果表明，GPT-4o Mini在偏见检测方面的卓越准确性和去偏见方面的有效性。此外，我们的分析揭示了与社会科学和政治动态以及现实世界事件相关的媒体偏见在时间和地理上的变化。本研究为偏见缓解的可扩展计算方法做出了贡献，促进了新闻报道中的公平性和问责制。|
|**2025-04-04**|**LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications**|Botao Zhu et.al.|[2504.03444](http://arxiv.org/abs/2504.03444)|null|随着复合大型语言模型（LLM）应用在解决实际问题中越来越普遍，本文揭示了复合LLM应用中固有的持续时间不确定性和结构不确定性给LLM服务提供商在高效服务和调度方面带来了巨大挑战。在本文中，我们提出了LLMSched，一个针对新兴复合LLM应用的感知不确定性的调度框架。在LLMSched中，我们首先设计了一种基于DAG的新型模型来描述不确定的复合LLM应用。然后，我们采用贝叶斯网络来全面分析复合LLM应用，并识别减少不确定性的阶段，同时采用基于熵的机制来量化其不确定性减少。结合不确定性减少策略和作业完成时间（JCT）高效的方案，我们进一步提出了一种高效的调度器，以减少平均JCT。在多种代表性复合LLM应用上进行的模拟和测试平台实验评估表明，与现有的最先进调度方案相比，LLMSched可以将平均JCT降低14~79%。|
|**2025-04-04**|**Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models**|Mirko Borszukovszki et.al.|[2504.03440](http://arxiv.org/abs/2504.03440)|null|为了充分发挥大型语言模型（LLMs）的潜力，了解它们答案的不确定性至关重要。这意味着模型必须能够量化自己对给定响应正确性的确信程度。不良的不确定性估计可能导致过度自信的错误答案，从而削弱对这些模型的信任。虽然已经有很多研究针对使用文本输入并生成文本输出的语言模型，但由于最近才将这些模型添加了视觉能力，因此在视觉语言模型（VLMs）的不确定性方面进展有限。我们在受损图像数据上测试了三种最先进的VLMs。我们发现，损坏的严重程度负面对模型估计不确定性的能力产生了影响，并且模型在大多数实验中也表现出过度自信。|
|**2025-04-03**|**Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models**|Mateusz Pach et.al.|[2504.02821](http://arxiv.org/abs/2504.02821)|**[link](https://github.com/explainableml/sae-for-vlm)**|稀疏自编码器（SAEs）最近被证明可以增强大型语言模型（LLMs）的可解释性和可控性。在这项工作中，我们将SAEs的应用扩展到视觉-语言模型（VLMs），如CLIP，并介绍了一个用于评估视觉表示单义性的全面框架。我们的实验结果表明，在VLMs上训练的SAEs显著增强了单个神经元的单义性，同时展现出与专家定义的结构（例如，iNaturalist分类法）良好对齐的层次表示。最值得注意的是，我们展示了将SAEs应用于干预CLIP视觉编码器，可以直接引导多模态LLMs（例如，LLaVA）的输出，而不对底层模型进行任何修改。这些发现强调了SAEs作为一种无监督方法，在增强VLMs的可解释性和控制方面的实用性和有效性。|
|**2025-04-03**|**Generative Evaluation of Complex Reasoning in Large Language Models**|Haowei Lin et.al.|[2504.02810](http://arxiv.org/abs/2504.02810)|**[link](https://github.com/linhaowei1/kumo)**|随着强大的大型语言模型（LLMs）展现出超越人类推理能力，一个关键问题随之产生：LLMs是真正进行推理，还是仅仅从它们广泛抓取的培训数据集中回忆答案？一旦公开发布的基准被纳入后续LLMs的训练集中，它们不可避免地会受到污染，从而削弱了其作为忠实评估的可靠性。为了解决这个问题，我们引入了KUMO，这是一个专门为评估LLMs推理能力的生成评估框架。KUMO将LLMs与符号引擎协同结合，动态地生成多样化的、多轮推理任务，这些任务部分可观察且难度可调整。通过自动化流程，KUMO在开放性领域不断生成新颖的任务，迫使模型展示真正的泛化能力而非记忆能力。我们对由KUMO创建的100个领域中的5,000个任务进行了23个最先进LLMs的评估，并将它们的推理能力与大学生进行了基准测试。我们的研究结果表明，许多LLMs在简单的推理任务上超越了大学水平，而在推理规模化的LLMs上则在复杂的推理挑战中达到了大学水平。此外，LLMs在KUMO任务上的表现与在新发布的真实世界推理基准测试中的结果高度相关，这凸显了KUMO作为稳健、持久的评估工具，用于评估LLMs真实推理能力的重要性。|
|**2025-04-03**|**MegaMath: Pushing the Limits of Open Math Corpora**|Fan Zhou et.al.|[2504.02807](http://arxiv.org/abs/2504.02807)|**[link](https://github.com/llm360/megamath)**|数学推理是人类智慧的基础，也是衡量大型语言模型（LLMs）高级能力的关键标准。然而，研究界仍然缺乏一个开放、大规模、高质量的语料库，专门针对以数学为中心的LLMs预训练的需求。我们提出了MegaMath，这是一个通过以下实践从各种数学专注的来源精心制作的开放数据集：（1）回顾网络数据：我们重新提取了来自Common Crawl的数学文档，通过以数学为导向的HTML优化、基于fasttext的过滤和去重，以获取互联网上更高质量的数据。（2）回忆与数学相关的代码数据：我们从大型代码训练语料库Stack-V2中识别了高质量的数学相关代码，进一步增强了数据的多样性。（3）探索合成数据：我们从网络数据或代码数据中合成了问答风格的文本、与数学相关的代码以及交织的文本-代码块。通过整合这些策略并通过广泛的消融实验验证其有效性，MegaMath提供了371亿个标记，在现有开放数学预训练数据集中数量最多、质量最高。|
|**2025-04-03**|**A Survey of Large Language Models in Mental Health Disorder Detection on Social Media**|Zhuohan Ge et.al.|[2504.02800](http://arxiv.org/abs/2504.02800)|null|心理健康问题的检测和干预是全球研究的关键领域，社交媒体数据已被认可为心理健康研究的重要资源。然而，如何利用大型语言模型（LLMs）在社交媒体上检测心理健康问题面临着重大挑战。因此，本文旨在探讨LLMs在社交媒体数据分析中的应用潜力，不仅关注最常见的心理障碍如抑郁症和焦虑症，还纳入了精神疾病和外在化障碍，从不同维度总结LLMs的应用方法，如文本数据分析和精神疾病检测，并揭示当前研究的主要挑战和不足。此外，本文概述了流行的数据集和评估指标。本文的调查为心理健康领域的研究人员提供了一个全面的参考框架，同时展示了LLMs在心理健康检测中的巨大潜力，有助于促进LLMs在未来心理健康干预中的应用。|
|**2025-04-03**|**A Framework for Robust Cognitive Evaluation of LLMs**|Karin de Langis et.al.|[2504.02789](http://arxiv.org/abs/2504.02789)|null|大型语言模型（LLMs）的涌现认知能力已被广泛观察到，但其本质和潜在机制仍然了解不深。越来越多的研究借鉴认知科学来探究LLMs的认知，但标准方法和实验流程尚未建立。为了填补这一空白，我们开发了CognitivEval，这是一个用于系统地评估LLMs人工认知能力的框架，特别强调响应收集的鲁棒性。CognitivEval的关键特性包括：（i）自动提示排列，以及（ii）收集生成和模型概率估计的测试。我们的实验表明，这些特性导致了更稳健的实验结果。使用CognitivEval，我们复制了认知科学中的五个经典实验，展示了该框架在各个实验任务中的泛化能力，并获得了几个最先进LLMs的认知特征。CognitivEval将公开发布，以促进认知科学社区内的更广泛合作。|
|**2025-04-03**|**From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks**|Joshua Holstein et.al.|[2504.02780](http://arxiv.org/abs/2504.02780)|null|生成式人工智能的兴起，尤其是大型语言模型（LLMs），正在从根本上改变知识工作中的认知过程，引发了关于它们对人类推理和问题解决能力影响的重大问题。随着这些AI系统越来越多地融入工作流程，它们为增强人类思维提供了前所未有的机会，同时由于被动消费生成的答案而存在认知侵蚀的风险。这种紧张关系在开放式任务中尤为明显，这些任务的有效解决方案需要深入的区域知识语境化和整合。与具有既定指标的标准化任务不同，在开放式任务中衡量人-LLM交互的质量面临重大挑战，因为这些任务缺乏事实依据且解决方案开发具有迭代性。为了解决这个问题，我们提出了一种框架，该框架从两个维度分析交互模式：认知活动模式（探索与利用）和认知参与模式（建设性与破坏性）。这个框架提供了系统性的测量方法，以评估LLMs何时是有效的思维工具而不是人类认知的替代品，从而推进了对保护并增强人类认知能力的AI系统开发的理论理解和实践指导。|
|**2025-04-03**|**BT-ACTION: A Test-Driven Approach for Modular Understanding of User Instruction Leveraging Behaviour Trees and LLMs**|Alexander Leszczynski et.al.|[2504.02779](http://arxiv.org/abs/2504.02779)|**[link](https://github.com/1eggbert7/bt_llm)**|自然语言指令通常抽象且复杂，需要机器人执行多个子任务，即使对于看似简单的查询也是如此。例如，当用户要求机器人准备鳄梨吐司时，这个任务涉及到几个连续的步骤。此外，这类指令对机器人来说可能是不明确或不可行的，或者可能超出了机器人的现有知识。虽然大型语言模型（LLMs）提供了强大的语言推理能力来应对这些挑战，但有效地将它们集成到机器人系统中仍然是一个关键挑战。为了解决这个问题，我们提出了BT-ACTION，这是一种测试驱动的方法，它结合了行为树（BT）的模块化结构和LLMs，以生成遵循复杂用户指令的连贯机器人动作序列，特别是在厨房辅助环境准备食谱的背景下。我们在一项包含45名参与者的全面用户研究中评估了BT-ACTION，将其性能与直接LLM提示进行了比较。结果表明，BT-ACTION的模块化设计帮助机器人减少了错误，并增加了用户信任，参与者对利用BT-ACTION的机器人表现出明显的偏好。代码在https://github.com/1Eggbert7/BT_LLM上公开可用。|
|**2025-04-03**|**How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?**|Andres Algaba et.al.|[2504.02767](http://arxiv.org/abs/2504.02767)|**[link](https://github.com/andresalgaba/llms_scientific_literature)**|科学知识的传播取决于研究人员如何发现和引用先前的作品。在科学研究过程中采用大型语言模型（LLMs）为这些引用实践引入了新的层次。然而，目前尚不清楚LLMs在多大程度上与人类的引用实践相符，它们在各个领域的表现如何，以及可能如何影响引用动态。在这里，我们展示LLMs通过在生成参考文献时始终青睐高引用论文，系统地加强了引用中的马太效应。尽管存在率（即生成的参考文献与外部文献计量数据库中现有记录相匹配的比例）在各个领域的显著差异，这一模式在科学领域内依然持续存在。通过分析GPT-4o为10,000篇论文生成的274,951个参考文献，我们发现LLM推荐与传统引用模式不同，更倾向于选择标题较短、作者较少的近期参考文献。强调其内容层面的相关性，生成的参考文献在语义上与每篇论文的内容对齐，与真实参考文献相当的水平，并显示出类似的网络效应，同时减少了作者自我引用。这些发现说明了LLMs可能如何重塑引用实践，并通过反映和放大既定趋势来影响科学发现的轨迹。随着LLMs越来越多地融入科学研究过程，理解它们在塑造科学界如何发现和构建先前工作方面的作用变得至关重要。|
|**2025-04-03**|**Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study**|Aryan Agrawal et.al.|[2504.02733](http://arxiv.org/abs/2504.02733)|**[link](https://github.com/ary4n99/llm-robustness)**|大型语言模型（LLMs）对输入扰动非常敏感，即使是微小的提示变化也可能导致输出结果显著不同。现有的提高LLMs鲁棒性的方法主要关注扰动数据样本，而提高对任务级指令扰动的韧性则相对较少被探索。在这项工作中，我们专注于对特定任务指令进行字符和词级编辑，这会显著降低下游性能。我们尝试了多种技术来提高LLMs的鲁棒性，包括自去噪和表示对齐，测试了不同的模型（Llama 3和Flan-T5）、数据集（CoLA、QNLI、SST-2）和指令（既包括任务导向也包括角色导向）。我们发现，平均而言，自去噪——无论是通过冻结的LLM还是微调模型执行——相比于包括集成和监督方法在内的替代策略，实现了显著更高的性能提升。|
|**2025-04-03**|**Why do LLMs attend to the first token?**|Federico Barbero et.al.|[2504.02732](http://arxiv.org/abs/2504.02732)|null|大型语言模型（LLMs）往往会对序列中的第一个标记给予高度重视，从而形成所谓的“注意力陷阱”。许多研究详细探讨了这一现象，提出了各种方法来利用或减轻它。注意力陷阱与量化困难、安全问题以及流式注意力有关。然而，尽管许多研究提供了它们发生或不会发生的条件，但一个关键问题仍然没有得到深入回答：为什么LLMs会学习这样的模式，它们是如何被使用的？在这项工作中，我们从理论和实证角度论证，这种机制为LLMs提供了一种避免过度混合的方法，并将其与现有研究联系起来，这些研究从数学角度研究信息在Transformer中的传播方式。我们进行了实验来验证我们的理论直觉，并展示了上下文长度、深度和数据打包等选择如何影响陷阱行为。我们希望这项研究为LLMs中注意力陷阱的有用性提供了新的实践视角，有助于更好地理解训练过程中形成的注意力模式。|
|**2025-04-02**|**Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities**|Jing Liu et.al.|[2504.01954](http://arxiv.org/abs/2504.01954)|null|引用表达式分割（RES）旨在分割与描述性语言表达式匹配的实体掩码。而传统的RES方法主要针对对象级地面实体的定位，现实场景需要更加灵活的框架，能够处理多级目标粒度，如多对象、单对象或部分级引用。这由于用户描述目标的方式多样且细微，因此带来了巨大的挑战。然而，现有的数据集和模型主要集中于设计针对对象级目标定位的地面专家，缺乏对更实用的多粒度RES所需的数据资源和统一框架。在本文中，我们进一步朝向视觉粒度统一的RES任务迈进。为了克服数据稀缺的局限性，我们引入了一个新的多粒度引用表达式分割（MRES）任务，与RefCOCOm基准一起，包括部分级标注以推进更精细的视觉理解。此外，我们创建了MRES-32M，这是最大的视觉地面数据集，包含超过3220万掩码和标题，覆盖100万张图片，专门用于部分级视觉-语言地面实体的定位。为了解决多粒度RES的挑战，我们提出了UniRES++，这是一种统一的跨模态大型语言模型，它集成了对象级和部分级RES任务。UniRES++包含了针对精细粒度视觉特征探索的针对性设计。通过联合模型架构和参数，UniRES++在多个基准测试中取得了最先进的性能，包括用于MRES的RefCOCOm、用于通用RES的gRefCOCO，以及用于经典RES的RefCOCO、RefCOCO+和RefCOCOg。为了促进未来多粒度视觉地面研究的发展，我们的RefCOCOm基准、MRES-32M数据集和模型UniRES++将在https://github.com/Rubics-Xuan/MRES上公开发布。|
|**2025-04-02**|**The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data**|Massimiliano Luca et.al.|[2504.01951](http://arxiv.org/abs/2504.01951)|null|随着大型语言模型在广泛和跨领域的应用，评估其训练数据中统计相关性到何种程度隐藏了微妙且可能令人担忧的偏见变得至关重要。从与特定性别相关的工作、爱好和情感的角度，对LLM中的性别偏见进行了广泛的研究。在本研究中，我们引入了一个新的视角。我们研究LLM是否能够仅根据在线购物历史预测个人的性别，以及这些预测是否受到性别偏见和刻板印象的影响。使用来自美国用户的在线购买历史数据集，我们评估了六个LLM对性别进行分类的能力，然后分析了它们的推理和产品与性别的共现。结果表明，虽然模型可以以适中的准确性推断性别，但它们的决策往往根植于产品类别与性别之间的刻板联系。此外，明确指示避免偏见可以降低模型预测的确定性，但并不能消除刻板模式。我们的发现突出了LLM中性别偏见的持续性，并强调了需要强大的偏见缓解策略。|
|**2025-04-02**|**OpenCodeReasoning: Advancing Data Distillation for Competitive Coding**|Wasi Uddin Ahmad et.al.|[2504.01943](http://arxiv.org/abs/2504.01943)|null|随着基于推理的大型语言模型的问世，许多人通过将推理能力提炼到学生模型中取得了巨大成功。这些技术显著缩小了推理和标准LLM在编码任务上的差距。尽管如此，许多关于提炼推理模型的进展仍然被专有数据集所锁定，或者缺乏关于数据整理、过滤和后续训练的详细信息。为了解决这个问题，我们构建了一个高级监督微调（SFT）数据集，并使用它实现了各种规模模型的最新编码能力结果。我们的提炼模型仅使用SFT在LiveCodeBench上达到61.8%，在CodeContests上达到24.6%，超过了使用强化学习训练的替代方案。然后，我们对构建数据集所使用的数据源、代码执行过滤的影响以及指令/解决方案多样性的重要性进行了分析。我们发现执行过滤对基准准确性产生了负面影响，这使我们优先考虑指令多样性而不是解决方案的正确性。最后，我们还分析了这些模型使用的标记效率和推理模式。我们将向社区开源这些数据集和提炼模型。|
|**2025-04-02**|**Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?**|Celine Lee et.al.|[2504.01935](http://arxiv.org/abs/2504.01935)|**[link](https://github.com/celine-lee/critical_thinking)**|为了探究这个问题，我们使用确定性有限自动机（DFAs）形式化了一个框架。DFAs提供了一种形式化方法，通过它可以表征任务复杂度，通过可测量的属性，如运行长度（所需推理步骤的数量）和状态空间大小（决策复杂性）。我们首先表明，在不同任务、不同大小和训练模式的不同模型中，存在一个最优的推理标记数量，可以最大化产生正确解决方案的概率。然后，我们研究哪些复杂性的属性决定了这个关键长度：我们发现，具有更长对应底层DFA运行的任务实例（即需要更大的潜在状态跟踪需求）与更长的推理长度相关联，但出人意料的是，DFA的大小（即状态空间复杂性）并不如此。然后，我们展示了这些发现的一个推论：能够预测新问题的最优推理标记数量并过滤掉非最优长度答案，会导致一致性的准确度提高。|
|**2025-04-02**|**A thorough benchmark of automatic text classification: From traditional approaches to large language models**|Washington Cunha et.al.|[2504.01930](http://arxiv.org/abs/2504.01930)|**[link](https://github.com/waashk/atcbench)**|过去十年中，自动文本分类（ATC）取得了显著的进步，这主要体现在最近的小型和大型语言模型（SLMs和LLMs）上，它们利用了Transformer架构。尽管最近的效果有所提升，但在文献中仍缺乏对这些近期方法的有效性增益是否足以弥补其与传统文本分类方法（如SVM和逻辑回归）相比的更高成本的全面成本效益分析。在此背景下，本研究的主要贡献有两方面：（i）我们提供了对十二种传统和近期ATC解决方案（包括五种开源LLMs）的科学合理的比较分析；（ii）一个包含{22个数据集}的大规模基准，包括情感分析和主题分类，其（训练-验证-测试）分区基于折叠交叉验证程序，并附带文档和代码。代码、数据和文档的发布使社区能够以更科学的方式复制实验并推进该领域。我们的比较实验结果表明，在有效性方面，LLMs优于传统方法（平均提高26%-7.1%），而SLMs（平均提高4.9%-1.9%）。然而，由于微调，LLMs的计算成本显著增加，平均比传统方法和SLMs慢590倍和8.5倍。结果表明以下建议：（1）对于需要最佳可能有效性的应用且能够承担成本的情况，使用LLMs；（2）对于资源有限的应用或无法承担微调大型LLMs成本的情况，使用传统的逻辑回归和SVM等方法；（3）对于寻求最优效果-效率权衡的应用，使用如Roberta这样的SLMs。|
|**2025-04-02**|**Gen-C: Populating Virtual Worlds with Generative Crowds**|Andreas Panayiotou et.al.|[2504.01924](http://arxiv.org/abs/2504.01924)|null|在过去二十年中，研究人员在模拟人类群体方面取得了显著进展，但这些努力主要集中在低级任务，如碰撞避免以及路径跟随和集群等狭窄范围的行为。然而，创建引人入胜的群体场景不仅仅需要功能性的移动——它还需要捕捉到随时间推移的智能体之间、智能体与他们的环境以及彼此之间的高层交互。为了解决这个问题，我们引入了Gen-C，这是一种生成模型，用于自动化编写高层群体行为。Gen-C通过利用大型语言模型（LLM）生成有限数量的群体场景，绕过了收集和注释真实群体视频数据这一劳动密集型且具有挑战性的任务，随后通过模拟对这些场景进行扩展和泛化，构建时间扩展图来模拟虚拟智能体的行动和交互。我们的方法采用两个由条件先验网络引导的变分图自动编码器：一个用于学习图结构（智能体交互）的潜在空间，另一个用于节点特征（智能体行动和导航）。这种设置使得动态群体交互的生成变得灵活。训练好的模型可以通过自然语言进行条件化，使用户能够从文本描述中合成新的群体行为。我们在大学校园和火车站两个场景中展示了我们方法的有效性，展示了其将具有不同和动态行为的智能体填充到多样化虚拟环境中的潜力，这些智能体反映了复杂交互和高层决策模式。|
|**2025-04-02**|**Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation**|Baban Gain et.al.|[2504.01919](http://arxiv.org/abs/2504.01919)|null|大型语言模型（LLMs）的出现极大地改变了机器翻译（MT）的格局，尤其是在低资源语言和缺乏足够平行语料库、语言工具和计算基础设施的领域。本综述全面概述了利用LLMs进行MT的最新进展。我们分析了诸如少样本提示、跨语言迁移和参数高效微调等技术，这些技术能够有效地适应资源不足的环境。论文还探讨了使用LLMs生成合成数据的方法，包括回译和词汇增强。此外，我们比较了基于LLM的翻译与传统编码器-解码器模型在多种语言对上的表现，突出了每种方法的优缺点。我们讨论了持续存在的挑战，如幻觉、评估不一致性和继承的偏见，同时也评估了用于翻译质量的LLM驱动指标。本综述提供了实际见解，并概述了在大规模生成模型时代构建稳健、包容和可扩展的MT系统的未来方向。|
|**2025-04-02**|**Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning**|Yinggan Xu et.al.|[2504.01911](http://arxiv.org/abs/2504.01911)|null|大型语言模型（LLMs）通过增强推理、符号操作和数值计算，在物理学研究中扮演着越来越重要的角色。然而，确保其输出的可靠性和可解释性仍然是一个重大挑战。在我们的框架中，我们将人工智能与人类科学家之间的合作概念化为三个模块之间的动态互动：推理模块、解释模块和人工智能-科学家交互模块。我们认识到，有效的物理学推理需要严格的逻辑一致性、量化精确性和与现有理论模型的深度融合，因此我们引入了解释模块，以提高对AI生成输出的理解，这在文献中尚未被探索。该模块包括多个专业代理，如摘要生成器、模型构建者、UI构建者和测试者，它们通过在一个物理基础框架内结构化LLM输出，构建一个更可解释的科学模型，共同协作。一个案例研究显示，我们的方法提高了透明度、促进了验证，并加强了科学发现中的AI增强推理。|
|**2025-04-02**|**TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables**|Abhilash Shankarampeta et.al.|[2504.01879](http://arxiv.org/abs/2504.01879)|null|人类不断进行新的发现，理解导致这些突破的事件时间序列对于推进科学和社会至关重要。这种对时间进行推理的能力使我们能够确定未来步骤，并了解金融和政治决策对我们生活的影响。然而，大型语言模型（LLMs）通常在静态数据集上训练，这限制了它们进行有效时间推理的能力。为了评估LLMs的时间推理能力，我们提出了TRANSIENTTABLES数据集，该数据集包含从超过14,000张表格中提取的3,971个问题，跨越多个时间段的1,238个实体。我们引入了一个基于模板的问题生成管道，该管道利用LLMs来优化模板和问题。此外，我们使用最先进的LLMs建立了基线结果，以创建一个基准。我们还引入了以任务分解为中心的新型建模策略，以增强LLMs的性能。|
|**2025-04-02**|**From Code Generation to Software Testing: AI Copilot with Context-Based RAG**|Yuchen Wang et.al.|[2504.01866](http://arxiv.org/abs/2504.01866)|null|大规模软件开发的快速步伐对传统测试方法提出了越来越高的要求，这往往导致效率、准确性和覆盖率的瓶颈。我们提出了一种关于软件测试的新视角，认为缺陷检测和编写更少缺陷的代码是两个相互关联的问题，它们有一个共同的目标，即利用有限的资源减少缺陷。我们扩展了我们之前关于人工智能辅助编程的研究，该研究支持代码自动补全和由聊天机器人驱动的问答，将其应用于软件测试领域。我们引入了Copilot for Testing，这是一个自动测试系统，它将缺陷检测与代码库更新同步，利用基于上下文的检索增强生成（RAG）来提升大型语言模型（LLMs）的能力。我们的评估表明，缺陷检测准确率提高了31.2%，关键测试覆盖率提高了12.6%，用户接受率提高了10.5%，突显了人工智能驱动技术在现代软件开发实践中的变革潜力。|
|**2025-03-31**|**Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation**|Shengqiong Wu et.al.|[2503.24379](http://arxiv.org/abs/2503.24379)|null|为了解决当前视频生成领域中对用户意图准确理解存在的瓶颈，我们提出了Any2Caption，这是一个在任何条件下进行可控视频生成的创新框架。其核心思想是将各种条件解释步骤从视频合成步骤中解耦。通过利用现代的多模态大型语言模型（MLLM），Any2Caption将文本、图像、视频以及区域、运动和相机姿态等专用线索等不同输入解释为密集、结构化的字幕，为骨干视频生成器提供更好的指导。我们还引入了Any2CapIns，这是一个包含337K个实例和407K个条件的任何条件到字幕指令调优的大规模数据集。全面的评估表明，我们的系统在可控性和视频质量方面在现有视频生成模型的各个方面都取得了显著改进。项目页面：https://sqwu.top/Any2Cap/|
|**2025-03-31**|**Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models**|Rui Wang et.al.|[2503.24377](http://arxiv.org/abs/2503.24377)|**[link](https://github.com/devoallen/awesome-reasoning-economy-papers)**|**近年来，大型语言模型（LLMs）在执行复杂推理任务的能力上取得了显著进步，从快速直观思考（系统1）转变为缓慢深入推理（系统2）。虽然系统2的推理提高了任务准确性，但由于其思考速度慢以及无效或不必要的推理行为，它往往需要大量的计算成本。相比之下，系统1的推理在计算上效率高，但会导致次优性能。因此，平衡性能（好处）和计算成本（预算）之间的权衡至关重要，从而产生了推理经济性的概念。在本调查中，我们全面分析了LLMs在训练后和测试时推理阶段的推理经济性，包括i）推理低效的原因，ii）不同推理模式的行为分析，以及iii）实现推理经济性的潜在解决方案。通过提供可操作的见解并突出开放性挑战，我们旨在阐明提高LLMs推理经济性的策略，从而为该领域的研究进展提供有价值的资源。我们还提供了一个公共仓库，以持续跟踪这一快速发展的领域的进展。**|
|**2025-03-31**|**Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1**|Yi Chen et.al.|[2503.24376](http://arxiv.org/abs/2503.24376)|**[link](https://github.com/tencentarc/seed-bench-r1)**|**近期，在思维链（COT）生成方面的进步显著提升了大型语言模型（LLM）的推理能力，强化学习（RL）已成为一种有效的后训练方法。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在既需要感知又需要逻辑推理的任务中仍被研究不足。为了解决这个问题，我们引入了SEED-Bench-R1，这是一个旨在系统地评估MLLMs视频理解后训练方法的基准。它包括复杂真实世界的视频和复杂的日常规划任务，以多选题的形式呈现，需要高级感知和推理。SEED-Bench-R1通过三个级别的层次结构来评估泛化能力：分布内、跨环境和跨环境-任务场景，并配备了一个大规模的训练数据集，其中包含了易于验证的地面实况答案。我们使用Qwen2-VL-Instruct-7B作为基础模型，比较了RL与监督微调（SFT），证明了RL在分布内和分布外任务中的数据效率和卓越性能，甚至在像LongVideoBench这样的通用视频理解基准上优于SFT。我们的详细分析表明，RL增强了视觉感知，但通常产生的推理链在逻辑上不太连贯。我们确定了关键限制，如不一致的推理和忽略的视觉线索，并建议在基础模型推理、奖励模型和RL对噪声信号的抗扰性方面进行未来的改进。**|
|**2025-03-31**|**Effectively Controlling Reasoning Models through Thinking Intervention**|Tong Wu et.al.|[2503.24370](http://arxiv.org/abs/2503.24370)|null|推理增强的大型语言模型（LLMs）在生成最终答案之前会明确生成中间推理步骤，这有助于模型在复杂问题解决中表现出色。在这篇论文中，我们展示了这种新兴的生成框架为更精细地控制模型行为提供了独特的机遇。我们提出了“思维干预”这一新颖的范式，旨在通过战略性地插入或修改特定的思维标记，显式地引导LLMs的内部推理过程。我们在多个任务上进行了全面的评估，包括在IFEval上的指令遵循、在SEP上的指令层次以及在使用开源DeepSeek R1模型时的XSTest和SORRY-Bench上的安全性对齐。我们的结果表明，思维干预在基准提示方法的基础上显著提升了性能，在指令遵循场景中实现了高达6.7%的准确率提升，在指令层次推理方面提高了15.4%，在使用开放源代码DeepSeek R1模型时，对不安全的提示的拒绝率提高了40.0%。总的来说，我们的工作为控制推理LLMs开辟了一条有希望的新研究方向。|
|**2025-03-31**|**ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion**|Rana Muhammad Shahroz Khan et.al.|[2503.24354](http://arxiv.org/abs/2503.24354)|null|参数生成已成为神经网络开发的一种新型范式，它通过直接合成高质量的模型权重，为传统的神经网络训练提供了一种替代方案。在低秩适应（LoRA）应用于不断演化的（即，持续更新的）大型语言模型（LLMs）的背景下，这种方法承诺了高效的适应，而无需昂贵的重新训练。然而，现有方法在同时实现可扩展性和可控性方面面临着关键的限制。在本文中，我们引入了 $\texttt{ORAL}$，这是一种新的$\textbf{条件循环扩散}$框架，旨在解决这些挑战。$\texttt{ORAL}$包含一种新颖的条件机制，该机制整合了模型架构和文本任务规范，能够生成特定于任务的LoRA参数，这些参数可以无缝地在不断演化的基础模型之间迁移。我们的方法成功地扩展到数十亿参数的LLMs，并保持了可控性。通过在七个语言任务、四个视觉任务和三个多模态任务上使用五种预训练的LLMs进行的大量实验，我们证明了$\texttt{ORAL}$ 生成的LoRA参数具有高质量，其性能与传统的训练模型相当甚至更优。|
|**2025-03-31**|**BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models**|Alok Abhishek et.al.|[2503.24310](http://arxiv.org/abs/2503.24310)|null|在这项研究中，我们引入了BEATS，一个用于评估大型语言模型（LLMs）中的偏差、伦理、公平性和事实性的新颖框架。基于BEATS框架，我们提出一个针对LLMs的偏差基准，该基准涵盖29个不同的指标。这些指标涵盖了广泛的特点，包括人口统计、认知和社会偏差，以及伦理推理、群体公平性和与错误信息风险相关的事实性度量。这些指标使得对LLM生成响应可能持续的社会偏见以及强化或扩大系统性不平等的程度进行量化评估成为可能。为了在这个基准上获得高分，LLM必须在它们的响应中表现出非常公平的行为，使其成为负责AI评估的严格标准。基于我们实验数据的实证结果表明，37.65%的行业领先模型生成的输出包含某种形式的偏差，突显了在关键决策系统中使用这些模型的风险。BEATS框架和基准提供了一个可扩展且统计上严格的LLMs基准、诊断导致偏差的因素以及制定缓解策略的方法。通过BEATS框架，我们的目标是帮助开发更加负责任和符合伦理的AI模型。|
|**2025-03-31**|**A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG**|Arshia Kermani et.al.|[2503.24307](http://arxiv.org/abs/2503.24307)|null|本项研究对三种用于分析心理健康文本的大型语言模型（LLMs）方法进行了系统比较：提示工程、检索增强生成（RAG）和微调。利用LLaMA 3，我们在两个数据集上评估了这些方法在情感分类和心理健康状况检测任务中的表现。微调实现了最高的准确率（情感分类为91%，心理健康状况为80%），但需要大量的计算资源和训练集，而提示工程和RAG则提供了更灵活的部署方式，但性能适中（准确率为40-68%）。我们的发现为在心理健康应用中实施基于LLM的解决方案提供了实际见解，突出了准确率、计算需求和部署灵活性之间的权衡。|
|**2025-03-31**|**Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning**|Jiacheng Lin et.al.|[2503.24289](http://arxiv.org/abs/2503.24289)|**[link](https://github.com/linjc16/Rec-R1)**|**我们提出了Rec-R1，一个通用的强化学习框架，通过闭环优化将大型语言模型（LLMs）与推荐系统连接起来。与提示和监督微调（SFT）不同，Rec-R1直接通过来自固定黑盒推荐模型的反馈优化LLM生成，而不依赖于来自GPT-4o等专有模型的合成SFT数据。这避免了数据蒸馏所需的大量成本和努力。为了验证Rec-R1的有效性，我们在两个代表性任务上对其进行了评估：产品搜索和序列推荐。实验结果表明，Rec-R1不仅始终优于基于提示和SFT的方法，而且在使用简单的检索器（如BM25）时，与强大的判别性基线相比也实现了显著的提升。此外，Rec-R1保留了LLM的通用能力，而SFT通常会损害指令遵循和推理能力。这些发现表明，Rec-R1是一个很有前景的持续特定任务自适应的基础，避免了灾难性遗忘。**|
|**2025-03-31**|**Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality**|Sewoong Lee et.al.|[2503.24277](http://arxiv.org/abs/2503.24277)|**[link](https://github.com/sewoonglee/top-afa-sae)**|**稀疏自编码器（SAEs）已成为现代机制可解释性的工作马，但具有顶- $k$风格激活函数的领先SAE方法在超参数$k$的选择上缺乏理论基础。SAEs基于线性表示假设（LRH），该假设认为大型语言模型（LLMs）的表示是线性编码的，以及叠加假设（SH），该假设指出模型中可以存在比其维度更多的特征。我们表明，基于LRH和SH的正式定义，稀疏特征向量（由SAEs从LLMs的密集嵌入中学习的潜在表示）的幅度可以使用其相应的密集向量进行近似，并具有封闭形式的误差界限。为了可视化这一点，我们提出了ZF图，该图揭示了LLM隐藏嵌入和SAE特征向量之间之前未知的关系，使我们能够对预训练SAE的特征向量在给定输入下过度或不足激活的程度进行首次实证测量。相应地，我们引入了近似特征激活（AFA），它近似真实稀疏特征向量的幅度，并提出了一种基于AFA的新评估指标，用于评估输入和激活之间的对齐程度。我们还利用AFA引入了一种新的SAE架构，即顶-AFA SAE，这导致SAEs：（a）更符合理论依据；（b）无需调整SAE稀疏超参数。最后，我们通过实证证明，顶-AFA SAEs实现了与最先进的顶-k SAEs相当的重构损失，而无需调整超参数$k$ 。我们的代码可在以下网址获取：https://github.com/SewoongLee/top-afa-sae。**|
|**2025-03-31**|**Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation**|Dun Yuan et.al.|[2503.24245](http://arxiv.org/abs/2503.24245)|null|大型语言模型（LLMs）在通用自然语言处理任务上取得了显著进展。然而，当应用于电信等特定领域时，LLMs仍面临挑战，因为这些领域需要专门的专家知识和对不断变化标准的适应性。本文提出了一种新颖的框架，该框架结合了知识图谱（KG）和检索增强生成（RAG）技术，以提升LLMs在电信领域的性能。该框架利用知识图谱来捕捉有关网络协议、标准和电信相关实体的结构化、特定领域信息，全面地表示它们之间的关系。通过将KG与RAG集成，LLMs可以在生成响应时动态访问和利用最相关和最新的知识。这种混合方法弥合了结构化知识表示和LLMs生成能力之间的差距，显著提高了准确性、适应性和特定领域理解能力。我们的结果表明，KG-RAG框架在精确处理复杂技术查询方面的有效性。所提出的KG-RAG模型在常用的电信特定数据集上的问答任务中达到了88%的准确率，而仅使用RAG的方法准确率为82%，仅使用LLM的方法准确率为48%。|
|**2025-03-28**|**Q-Insight: Understanding Image Quality via Visual Reinforcement Learning**|Weiqi Li et.al.|[2503.22679](http://arxiv.org/abs/2503.22679)|**[link](https://github.com/lwq20020127/q-insight)**|**图像质量评估（IQA）关注图像的感知视觉质量，在图像重建、压缩和生成等下游任务中起着至关重要的作用。多模态大型语言模型（MLLMs）的快速发展极大地扩展了IQA的范围，使其朝着综合图像质量理解发展，这包括内容分析、退化感知和比较推理，而不仅仅是数值评分。之前基于MLLM的方法通常要么生成缺乏可解释性的数值分数，要么过度依赖使用大规模标注数据集进行的监督微调（SFT）来提供描述性评估，这限制了它们的灵活性和适用性。在本文中，我们提出了Q-Insight，这是一个基于强化学习并构建在组相对策略优化（GRPO）之上的模型，它展示了强大的视觉推理能力，同时在图像质量理解方面仅需有限数量的评分和退化标签。通过联合优化评分回归和退化感知任务，并使用精心设计的奖励函数，我们的方法有效地利用了它们的相互利益，以提升性能。大量实验表明，Q-Insight在评分回归和退化感知任务中均显著优于现有最先进的方法，同时展现出对比较推理任务的令人印象深刻的零样本泛化能力。代码将在https://github.com/lwq20020127/Q-Insight上提供。**|
|**2025-03-28**|**QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?**|Belinda Z. Li et.al.|[2503.22674](http://arxiv.org/abs/2503.22674)|**[link](https://github.com/google-deepmind/questbench)**|最近，大量研究致力于提升大型语言模型（LLMs）在推理基准测试如数学和逻辑上的表现。然而，过去的研究大多假设任务是明确定义的。在现实世界中，对LLMs的查询往往是不明确的，只能通过获取缺失信息来解决。我们将此形式化为带有缺失变量赋值的约束满足问题（CSP）。利用这种形式的一个特例，其中只有一个必要的变量赋值缺失，我们可以严格评估LLM识别最小必要问题的能力，并量化每个问题的难度水平。我们提出了QuestBench，这是一组最多只需问一个问题即可解决的模糊推理任务，包括：（1）逻辑-Q：包含一个缺失命题的逻辑推理任务，（2）规划-Q：具有部分观察初始状态的PDDL规划问题，（3）GSM-Q：带有一个缺失变量赋值的人注释的中学数学问题，以及（4）GSME-Q：GSM-Q的一个版本，其中文字问题被注释者翻译成方程式。LLM的任务是从选项列表中选择正确的澄清问题。尽管最先进的模型在GSM-Q和GSME-Q上表现出色，但它们在逻辑-Q和规划-Q上的准确率仅为40-50%。分析表明，解决明确定义的推理问题的能力可能不足以在基准测试中取得成功：模型在能够解决问题的完整版本时，也难以确定正确的问题。此外，在规划-Q领域，LLMs在明确提供预测“不确定”的选项时，也不倾向于保留意见。这突显了需要深入研究模型的信息获取能力。|
|**2025-03-28**|**Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder Re-rankers**|Francesca Pezzuti et.al.|[2503.22672](http://arxiv.org/abs/2503.22672)|**[link](https://github.com/fpezzuti/multistage-finetuning)**|**最先进的跨编码器可以被微调以在段落重排序中表现出色。作为重排序器的典型微调过程需要大量人工标注数据、一个对比学习目标以及一组基于启发式方法选择的负样本。微调的另一种新方法涉及使用提取目标来教导模型模仿一个高度有效的大型语言模型的排名。这些微调策略可以单独应用，或者按顺序应用。在本工作中，我们系统地研究了点对点跨编码器在单个阶段独立微调或两个阶段顺序微调时的有效性。我们的实验表明，使用对比学习微调的点对点跨编码器的有效性确实与使用多阶段方法的模型相当。可复现代码可在https://github.com/fpezzuti/multistage-finetuning上找到。**|
|**2025-03-28**|**Unicorn: Text-Only Data Synthesis for Vision Language Model Training**|Xiaomin Yu et.al.|[2503.22655](http://arxiv.org/abs/2503.22655)|**[link](https://github.com/yu-xm/unicorn)**|**训练视觉-语言模型（VLMs）通常需要大规模、高质量的图像-文本对，但收集或合成此类数据成本高昂。相比之下，文本数据丰富且成本低廉，因此引发了一个问题：能否仅从文本中合成高质量的多模态训练数据？为了解决这个问题，我们提出了一种跨集成三阶段的多模态数据合成框架，该框架生成了两个数据集：Unicorn-1.2M和Unicorn-471K-Instruction。在第一阶段：多样化标题数据合成中，我们利用大型语言模型（LLMs）扩展稀疏的标题种子，构建了120万个语义多样化的高质量标题。在第二阶段：指令微调数据生成中，我们将471K个标题进一步处理成多轮指令微调任务，以支持复杂推理。最后，在第三阶段：模态表示迁移中，这些文本标题表示被转换成视觉表示，从而产生了多样化的合成图像表示。这一三阶段过程使我们能够构建用于预训练的Unicorn-1.2M和用于指令微调的Unicorn-471K-Instruction，而不依赖于真实图像。通过消除对真实图像的依赖，同时保持数据质量和多样性，我们的框架为VLMs的训练提供了一种成本效益高且可扩展的解决方案。代码可在https://github.com/Yu-xm/Unicorn.git获取。**|
|**2025-03-28**|**Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users**|Antonia Karamolegkou et.al.|[2503.22610](http://arxiv.org/abs/2503.22610)|null|本文探讨了多模态大型语言模型（MLLMs）作为视觉障碍人士辅助技术的有效性。我们进行了一项用户调查，以确定采用模式和用户在使用此类技术时面临的关键挑战。尽管这些模型的采用率很高，但我们的发现突出了与情境理解、文化敏感性和复杂场景理解相关的担忧，尤其是对于可能完全依赖它们进行视觉解释的个人。根据这些结果，我们收集了五个以用户为中心的任务，包括图像和视频输入，以及一个关于光学盲文识别的新任务。我们对十二个MLLM的系统评估表明，为了克服与文化背景、多语言支持、盲文阅读理解、辅助物体识别和幻觉相关的局限性，还需要进一步的发展。这项工作为多模态AI在无障碍领域的未来方向提供了关键见解，强调了需要更多包容性、鲁棒性和值得信赖的视觉辅助技术。|
|**2025-03-28**|**On the Alignment of Post-Publication Reviews & Bibliometric and Altmetric Impact -- A Case Study on Expert Statements from the Science Media Center Germany**|Dirk Tunger et.al.|[2503.22594](http://arxiv.org/abs/2503.22594)|null|在学术出版和同行评审的背景下，本研究探讨了出版物后专家评估与其一致水平以及随后科学和公众对被评审研究的认可之间的关系。利用德国科学媒体中心的专家陈述作为数据集，我们分析了“研究背景”评论，以检查定性出版物后评估与文献计量学以及替代计量学指标之间的吻合度。我们采用大型语言模型将非结构化专家评论转化为结构化评分方案。此外，我们将这些评估与来自《科学引文索引》的引用次数以及如Altmetric关注分数、新闻报道提及和来自Altmetric Explor器的Mendeley阅读统计等替代影响指标相关联。我们研究正面或负面出版物后评论与高或低引用或替代计量计数之间的吻合度。|
|**2025-03-28**|**LLM-enabled Instance Model Generation**|Fengjunjie Pan et.al.|[2503.22587](http://arxiv.org/abs/2503.22587)|null|在基于模型的工程领域，模型是使系统能够进行设计和分析的关键组成部分。传统上，这些模型的创建是一个需要深厚建模专业知识以及针对目标系统的大量领域知识的手动过程。随着生成式人工智能的快速发展，大型语言模型（LLMs）在自动化模型生成方面显示出潜力。本研究探讨了使用LLMs生成实例模型的方法，特别关注从Ecore元模型和自然语言规范生成基于XMI的实例模型。我们观察到，当前的LLMs在直接生成有效的XMI模型方面存在困难。为了解决这个问题，我们提出了一种两步方法：首先，使用LLMs生成一个包含所有必要实例模型信息的简化结构化输出，即概念实例模型，然后将这个中间表示编译成一个有效的XMI文件。概念实例模型是格式无关的，允许通过不同的编译器将其转换为各种建模格式。该方法的可行性已在多个LLMs上进行验证，包括GPT-4o、o1-preview、Llama 3.1（8B和70B）。结果表明，所提出的方法显著提高了LLMs在实例模型生成任务中的可用性。值得注意的是，较小的开源模型Llama 3.1 70B在所提出的框架内表现与专有GPT模型相当。|
|**2025-03-28**|**Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish**|Kevin Cohen et.al.|[2503.22585](http://arxiv.org/abs/2503.22585)|**[link](https://github.com/historicalink/ironydetection)**|**本研究探讨了使用大型语言模型（LLMs）来增强数据集并提高19世纪拉丁美洲报纸中的讽刺检测能力。采用了两种策略来评估BERT和GPT-4o模型在捕捉讽刺微妙细微差别方面的有效性，通过多类和二分类任务进行评估。首先，我们实施了数据集增强，重点关注丰富情感和上下文线索；然而，这些在历史语言分析中显示出有限的影响。第二种策略，即半自动化标注过程，有效地解决了类别不平衡问题，并通过高质量标注扩充了数据集。尽管讽刺的复杂性带来了挑战，但这项工作通过两个关键贡献推动了情感分析的发展：一是引入了一个新的历史西班牙语数据集，该数据集针对情感分析和讽刺检测进行了标注；二是提出了一种半自动化标注方法，其中人类专业知识对于细化LLMs结果至关重要，并通过纳入历史和文化背景作为核心特征而得到丰富。**|
|**2025-03-28**|**Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation**|Sarubi Thillainathan et.al.|[2503.22582](http://arxiv.org/abs/2503.22582)|null|本文提出两种方法来适应这些具有挑战性的场景，以解决多语言序列到序列大型语言模型（msLLMs）在极低资源语言（LRLs）翻译中的应用问题：（1）持续预训练（CPT），即使用特定领域的单语数据进行进一步训练，以补偿LRLs的代表性不足；（2）中间任务迁移学习（ITTL），该方法通过在域内和域外并行数据上微调msLLM，以增强其在不同领域和任务中的翻译能力。在工程应用方面，这些方法被应用于斯里兰卡语、泰米尔语和英语（六个语言对）的特定领域、极低资源设置（包含少于10万个样本的数据集）的NMT系统中。我们的实验表明，与标准单阶段微调基线相比，这些方法平均提高了+1.47个双语评估（BLEU）得分，适用于所有翻译方向。此外，多模型集成进一步提高了性能，额外增加了BLEU得分。|
|**2025-03-28**|**Niyama : Breaking the Silos of LLM Inference Serving**|Kanishk Goel et.al.|[2503.22562](http://arxiv.org/abs/2503.22562)|null|大规模语言模型（LLMs）的广泛应用使得具有不同延迟要求的多样化应用成为可能。现有的LLM服务框架依赖于孤立的基础设施和粗粒度的工作负载隔离——交互式和批量——导致资源利用效率低下，对细粒度服务质量（QoS）区分的支持有限。这导致了运营效率低下、过度配置以及在流量激增时的糟糕负载管理。我们提出了Niyama，一个创新的QoS驱动推理服务系统，它能够实现共享基础设施上不同工作负载的高效协同调度。Niyama引入了细粒度的QoS分类，允许应用程序指定精确的延迟要求，并根据实时系统状态动态调整调度决策。利用LLM推理的可预测执行特性，Niyama实现了一种动态分块机制，在保持严格QoS保证的同时提高整体吞吐量。此外，Niyama采用了一种平衡公平性和效率的混合优先级策略，并采用选择性请求降级，在过载条件下实现优雅的服务降级。我们的评估表明，与当前的孤立部署相比，Niyama将服务能力提高了32%，同时保持了QoS保证。值得注意的是，在极端负载下，与当前策略相比，我们的系统将服务级别协议（SLO）违规减少了十倍。|
|**2025-03-27**|**Video-R1: Reinforcing Video Reasoning in MLLMs**|Kaituo Feng et.al.|[2503.21776](http://arxiv.org/abs/2503.21776)|**[link](https://github.com/tulerfeng/video-r1)**|**受到DeepSeek-R1通过基于规则的强化学习（RL）成功激发推理能力的影响，我们引入了Video-R1，作为第一个系统性地探索R1范式在多模态大型语言模型（MLLMs）中激发视频推理的尝试。然而，将GRPO算法直接应用于视频推理的RL训练存在两个主要挑战：（一）视频推理缺乏时间建模，以及（二）高质量视频推理数据的稀缺。为了解决这些问题，我们首先提出了T-GRPO算法，该算法鼓励模型利用视频中的时间信息进行推理。此外，我们不仅依赖于视频数据，还将高质量图像推理数据纳入训练过程。我们构建了两个数据集：Video-R1-COT-165k用于SFT冷启动和Video-R1-260k用于RL训练，两者均包含图像和视频数据。实验结果表明，Video-R1在视频推理基准（如VideoMMMU和VSI-Bench）以及一般视频基准（如MVBench和TempCompass等）上取得了显著的提升。值得注意的是，Video-R1-7B在视频空间推理基准VSI-bench上达到了35.8%的准确率，超过了商业专有模型GPT-4o。所有代码、模型和数据均已发布。**|
|**2025-03-27**|**MemInsight: Autonomous Memory Augmentation for LLM Agents**|Rana Salama et.al.|[2503.21760](http://arxiv.org/abs/2503.21760)|null|大型语言模型（LLM）智能体已经进化为能够智能地处理信息、做出决策并与用户或工具互动。一项关键能力是集成长期记忆功能，使这些智能体能够借鉴历史交互和知识。然而，随着记忆大小的增长和对语义结构化的需求，带来了显著的挑战。在本研究中，我们提出了一种自主记忆增强方法，称为MemInsight，以增强语义数据的表示和检索机制。通过利用自主增强来处理历史交互，LLM智能体被证明可以提供更准确和更具情境化的响应。我们通过三个任务场景（对话推荐、问答和事件摘要）实证验证了我们提出的方法的有效性。在LLM-REDIAL数据集上，MemInsight将推荐的说服力提高了高达14%。此外，它在LoCoMo检索的召回率方面优于RAG基线34%。我们的实证结果表明，MemInsight有潜力增强LLM智能体在多个任务中的情境性能。|
|**2025-03-27**|**GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics**|Arsham Gholamzadeh Khoee et.al.|[2503.21735](http://arxiv.org/abs/2503.21735)|null|确保软件发布决策的可靠性和有效性至关重要，尤其是在汽车系统这样的安全关键领域。对发布验证数据的精确分析，通常以表格形式呈现，在这个过程中起着关键作用。然而，传统方法依赖于对大量测试数据集和验证指标的手动分析，容易导致延误和成本高昂。大型语言模型（LLMs）提供了一种有希望的替代方案，但它们在分析推理、情境理解、处理超范围查询以及持续处理结构化测试数据方面面临挑战；这些局限性阻碍了它们在安全关键场景中的直接应用。本文介绍了一种基于LLM的工具——GateLens，用于分析汽车领域的表格数据。GateLens将自然语言查询转换为关系代数（RA）表达式，然后生成优化的Python代码。它在基准数据集上优于基线系统，实现了更高的F1分数，并以更大的鲁棒性处理复杂和模糊的查询。消融研究表明，RA模块起着至关重要的作用，当省略时，性能急剧下降。工业评估表明，GateLens将分析时间减少了80%以上，同时保持了高准确性和可靠性。正如所展示的结果所示，GateLens在不依赖少样本示例的情况下实现了高性能，展示了在不同公司角色查询类型上的强大泛化能力。与一家汽车公司合作部署GateLens的经验为将AI集成到关键工作流程（如发布验证）提供了实际指导。结果表明，通过自动化测试结果分析，GateLens实现了更快、更明智、更可靠的发布决策，从而可以推动汽车系统中软件的可扩展性和可靠性。|
|**2025-03-27**|**Effective Skill Unlearning through Intervention and Abstention**|Yongce Li et.al.|[2503.21730](http://arxiv.org/abs/2503.21730)|**[link](https://github.com/trustworthy-ml-lab/effective_skill_unlearning)**|**大型语言模型（LLMs）在各种领域展现出令人瞩目的能力。了解其能力背后的机制并对其施加控制对于开发更好的模型变得越来越重要。在这篇论文中，我们关注LLMs中的技能卸载，特别是在不损害其整体能力的前提下卸载特定技能。我们为LLMs引入了两种轻量级的、无需训练的机器技能卸载技术。首先，我们发现当模型展示不同技能时，每个前馈层（FFL）中神经元的预激活分布是不同的。此外，我们还发现，在FFL关键空间内触发相同技能集群的查询可以使用超立方体与其他查询分离。基于这些观察，我们分别通过干预和回避提出了两种轻量级、无需训练的技能卸载方法：\texttt{神经元调整}和\texttt{关键空间检测}。我们在七个不同语言的数学解题、Python编码和理解技能卸载任务上评估了我们的方法。结果表明，它们在指定技能上的卸载能力非常强。具体来说，\texttt{关键空间检测}在大多数卸载任务中，在遗忘技能上实现了超过80%的相对性能下降，而在其他技能和模型的一般知识（MMLU）上相对性能下降不到10%。我们的代码可在https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning上找到。**|
|**2025-03-27**|**Collab: Controlled Decoding using Mixture of Agents for LLM Alignment**|Souradip Chakraborty et.al.|[2503.21720](http://arxiv.org/abs/2503.21720)|null|将大型语言模型（LLMs）与人类偏好和更广泛的效用对齐对于其在应用中的安全可靠部署至关重要。从人类反馈中学习强化（RLHF）已成为一种有效对齐LLMs的技术，但它需要更新数十亿个模型参数，计算成本高昂。相比之下，受控解码提供了一种在推理时不重新训练即可对齐模型的方法。然而，由于任务本身的复杂性和可变性，单代理解码方法往往难以适应各种任务。为了提高针对目标任务的测试时性能，我们提出了一种基于代理的解码策略混合，利用现有的现成对齐LLM策略。将每个先验策略视为混合代理协作中的一个代理，我们开发了一种解码方法，通过多个代理之间的token级选择策略实现推理时的对齐。对于每个token，根据长期效用指标，从模型池中动态选择最合适的LLM。这种策略切换机制确保了每一步的最优模型选择，使LLMs在解码过程中的协作和对齐变得高效。我们提出的算法的理论分析确定了针对目标任务（通过给定现成模型的特定奖励表示）的最优性能。我们在各种任务和偏好上对开源对齐模型进行了全面的实证评估，这证明了该方法相较于单代理解码基线方法的优点。值得注意的是，Collab超越了当前的SoTA解码策略，平均奖励提高了1.56倍，基于GPT-4的胜率提高了71.89%。|
|**2025-03-27**|**Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs**|Boyang Yang et.al.|[2503.21710](http://arxiv.org/abs/2503.21710)|null|在代码库级别的软件修复中，面临着一个挑战，即如何在问题描述和代码补丁之间弥合语义差距。现有的方法大多依赖于大型语言模型（LLMs），但存在语义模糊、结构化上下文理解有限以及推理能力不足的问题。为了解决这些局限性，我们提出了KGCompass，并具有两大创新：（1）一个新颖的仓库感知知识图谱（KG），它能准确地关联仓库工件（问题与拉取请求）和代码库实体（文件、类和函数），使我们能够有效地将庞大的搜索空间缩小到仅包含20个最相关的函数，并准确地定位候选错误位置及上下文信息；（2）一种路径引导的修复机制，该机制利用从知识图谱挖掘的实体路径，通过追踪这些路径，我们可以为LLMs增加相关的上下文信息，从而生成精确的补丁及其解释。在SWE-Bench-Lite上的实验结果表明，KGCompass在开源方法中实现了最先进的修复性能（45.67%）和函数级定位精度（51.33%），每修复一次只需花费0.20美元。我们的分析显示，在成功定位的错误中，有69.7%需要通过知识图谱进行多跳遍历，没有这些遍历，基于LLM的方法难以准确地定位错误。KGCompass中构建的知识图谱是语言无关的，并且可以增量更新，这使得它成为现实世界开发环境的实用解决方案。|
|**2025-03-27**|**LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning**|Hui Wang et.al.|[2503.21683](http://arxiv.org/abs/2503.21683)|null|近年来，大型语言模型（LLMs）在自然语言处理（NLP）领域取得了显著进展，在生成、理解和推理方面表现出强大的能力。这些模型在教育、智能决策和游戏等领域得到了应用。然而，在五子棋游戏中有效地利用LLMs进行战略规划和决策仍然是一个挑战。本研究旨在开发一个基于LLMs的五子棋AI系统，模拟人类下棋的学习过程。该系统旨在理解和应用五子棋策略和逻辑，以做出合理的决策。研究方法包括使模型能够“阅读棋盘”、“理解规则”、“选择策略”和“评估位置”，并通过自我对弈和强化学习来提升其能力。结果表明，这种方法显著提高了走棋位置的选择，解决了生成非法位置的问题，并通过并行位置评估减少了处理时间。经过大量的自我对弈训练后，该模型的五子棋下棋能力得到了显著提升。|
|**2025-03-27**|**JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community**|Yunze Xiao et.al.|[2503.21679](http://arxiv.org/abs/2503.21679)|null|本文介绍了JiraiBench，这是第一个用于评估大型语言模型在检测中、日社交媒体社区中自毁性内容的双语基准。聚焦于跨国“Jirai”（地雷）在线亚文化，该亚文化包含多种自毁性行为，如药物过量、饮食失调和自残，我们提出一个综合评估框架，该框架既包含语言维度也包含文化维度。我们的数据集包括10,419篇中文帖子以及5,000篇日文帖子，沿着三个行为类别进行多维标注，实现了较高的标注者间一致性。在四个最先进的模型上的实验评估显示，基于指导性语言，性能存在显著差异，处理中文内容时，意外地发现日文提示比中文提示表现更佳。这种出现的跨文化迁移表明，在检测任务中，文化邻近性有时可以超过语言相似性。与微调模型进行的跨语言迁移实验进一步证明了这些语言系统之间知识迁移的潜力，而无需进行明确的目标语言训练。这些发现强调了在多语言内容监管中采用文化信息方法的重要性，并为开发更有效的针对脆弱在线社区检测系统的文化背景的重要性提供了实证证据。|
|**2025-03-27**|**How do language models learn facts? Dynamics, curricula and hallucinations**|Nicolas Zucchet et.al.|[2503.21676](http://arxiv.org/abs/2503.21676)|null|在预训练过程中，大型语言模型积累了大量知识，但其获取知识的动态机制仍了解甚少。本研究调查了语言模型在合成事实回忆任务中的学习动态，揭示了三个关键发现：首先，语言模型的学习分为三个阶段，在获取精确事实知识之前表现出性能平台期。从机制上看，这个平台期与支持回忆的基于注意力的电路的形成相吻合。其次，训练数据分布对学习动态有显著影响，因为不平衡的分布会导致平台期缩短。最后，幻觉与知识同时出现，通过微调将新知识整合到模型中具有挑战性，因为它会迅速破坏其现有的参数记忆。我们的结果强调了数据分布对知识获取的重要性，并提出了新的数据调度策略以加速神经网络训练。|
|**2025-03-27**|**Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base**|Satvik Verma et.al.|[2503.21674](http://arxiv.org/abs/2503.21674)|**[link](https://github.com/claudwq/Intelligent-IoT-Attack-Detection-Design-via-LLM-with-Feature-Ranking-Based-Knowledge-Base-Design)**|**随着物联网（IoT）设备的广泛应用，网络安全挑战日益突出，尤其是分布式拒绝服务（DDoS）攻击的频率和复杂度不断上升。传统的机器学习（ML）技术在检测此类攻击时往往力不从心，因为混合和演变的攻击模式复杂。为了解决这个问题，我们提出了一种新的框架，该框架利用设备端大型语言模型（ODLLMs），并通过微调和知识库（KB）集成进行智能物联网网络攻击检测。通过实施特征排序技术，并构建既长又短的针对模型容量的知识库，所提出的框架确保了高效且准确的DDoS攻击检测，同时克服了计算和隐私限制。仿真结果表明，优化的框架在多种攻击类型上实现了卓越的准确性，尤其是在使用边缘计算环境中的紧凑型模型时。这项工作为实时物联网安全提供了可扩展和安全的解决方案，推动了边缘智能在网络安全领域的应用。**|
|**2025-03-26**|**Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark**|Sondos Mahmoud Bsharat et.al.|[2503.20786](http://arxiv.org/abs/2503.20786)|**[link](https://github.com/vila-lab/mobile-mmlu)**|随着大型语言模型（LLMs）的快速发展，将它们部署在移动设备上进行设备端AI应用的兴趣日益增加。与桌面用户相比，移动用户与LLMs的互动方式不同，这产生了独特的期望和数据偏差。当前的基准数据集主要针对服务器和桌面环境，而在专门为移动环境设计的广泛数据集方面存在明显不足。此外，移动设备在存储和计算资源方面面临严格的限制，这限制了模型的大小和能力，因此需要优化的效率和优先级知识。为了解决这些挑战，我们引入了Mobile-MMLU，这是一个针对移动智能的大规模基准数据集。它包含80个与移动相关的领域中的16,186个问题，旨在评估LLMs在现实移动场景中的性能。一个具有挑战性的子集，Mobile-MMLU-Pro，提供了与MMLU-Pro相似规模的先进评估，但比我们的标准完整集难得多。这两个基准都使用多项选择题，关注于实际移动交互，如食谱建议、旅行规划和日常任务。该数据集强调关键的移动特定指标，如推理延迟、能耗、内存使用和响应质量，为模型在移动约束下的性能提供了全面的见解。此外，它优先考虑隐私和适应性，评估模型在设备端处理、维护用户隐私和适应个性化使用模式的能力。Mobile-MMLU系列为开发和比较移动优化的LLMs提供了一个标准化框架，使移动计算环境中的生产力和决策能力得到提升。我们的代码和数据可在以下网址获取：https://github.com/VILA-Lab/Mobile-MMLU。|
|**2025-03-26**|**MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams**|Yanpeng Sun et.al.|[2503.20745](http://arxiv.org/abs/2503.20745)|null|图表作为一种基本的视觉语言形式，通过结构化的符号、形状和空间排列来表示复杂的概念及其相互关系。与自然图像不同，它们的内在符号和抽象性质给多模态大型语言模型（MLLMs）带来了重大挑战。然而，当前基准混淆了感知和推理任务，使得很难评估MLLMs是否真正理解数学图表，而不仅仅是表面上的模式识别。为了填补这一空白，我们引入了MATHGLANCE，这是一个专门设计来隔离和评估MLLMs中数学感知的基准。MATHGLANCE包含1.2K张图像和1.6K个精心策划的问题，涵盖四个感知任务：形状分类、对象计数、关系识别和对象定位，涉及包括平面几何、立体几何和图形表示在内的多个领域。我们对MLLMs的评估显示，它们理解图表的能力显著有限，尤其是在细粒度定位任务中。作为回应，我们构建了GeoPeP，这是一个包含20万结构化几何图像-文本对的感知导向数据集，这些图像-文本对明确标注了几何原语和精确的空间关系。在GeoPeP上训练MLLMs导致感知准确性的显著提升，这反过来又极大地提高了数学推理能力。我们的基准和数据集为评估和推进多模态数学理解建立了关键标准，为促进未来MLLMs研究提供了宝贵的资源和见解。|
|**2025-03-26**|**Dynamic Motion Blending for Versatile Motion Editing**|Nan Jiang et.al.|[2503.20724](http://arxiv.org/abs/2503.20724)|null|运动引导动作编辑可以实现比传统关键帧动画更高层次的语义控制和迭代修改。现有方法依赖于有限的预先收集的训练三元组，这严重阻碍了它们在多样化编辑场景中的通用性。我们引入了MotionCutMix，这是一种在线数据增强技术，通过基于输入文本融合身体部位动作动态生成训练三元组。虽然MotionCutMix有效地扩展了训练分布，但其组合性质引入了更高的随机性和潜在的部位不协调。为了模拟这种丰富的分布，我们提出了MotionReFit，这是一种带有运动协调器的自回归扩散模型。自回归架构通过分解长序列促进学习，而运动协调器减轻了运动组合的伪影。我们的方法直接从高级人类指令处理空间和时间运动编辑，无需依赖额外的规格或大型语言模型。通过广泛的实验，我们表明MotionReFit在文本引导运动编辑中实现了最先进的性能。|
|**2025-03-26**|**From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models**|Nikita Neveditsin et.al.|[2503.20715](http://arxiv.org/abs/2503.20715)|null|首先，我们分析摘要中的每个部分：  1. "This study examines the performance of Large Language Models (LLMs) in Aspect-Based Sentiment Analysis (ABSA)" - 这部分说明研究的主题是评估大型语言模型（LLMs）在基于方面的情感分析（ABSA）任务中的性能。 2. "with a focus on implicit aspect extraction in a novel domain" - 这里强调研究聚焦于在新兴领域中的隐式方面提取。 3. "Using a synthetic sports feedback dataset" - 这表示使用了一个合成体育反馈数据集。 4. "we evaluate open-weight LLMs' ability to extract aspect-polarity pairs" - 这说明评估了开放权重LLMs提取方面-极性对的能力。 5. "and propose a metric to facilitate the evaluation of aspect extraction with generative models" - 提出了一个度量标准，以促进使用生成模型进行方面提取的评估。 6. "Our findings highlight both the potential and limitations of LLMs in the ABSA task" - 研究结果突出了LLMs在ABSA任务中的潜力和局限性。  现在，我们将这些部分整合成完整的中文翻译：  本研究评估了大型语言模型（LLMs）在基于方面的情感分析（ABSA）任务中的性能，特别关注于新兴领域中的隐式方面提取。使用合成体育反馈数据集，我们评估了开放权重LLMs提取方面-极性对的能力，并提出了一种度量标准，以促进使用生成模型进行方面提取的评估。我们的研究结果突出了LLMs在ABSA任务中的潜力和局限性。|
|**2025-03-26**|**Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy**|Yinan Sun et.al.|[2503.20673](http://arxiv.org/abs/2503.20673)|null|随着多模态大型语言模型的快速发展，视觉感知和理解取得了显著进步，将多个任务整合到一个视觉问答框架中。然而，这些模型容易产生幻觉，这限制了它们作为人工智能系统的可靠性。尽管这个问题在自然语言处理和图像标题生成领域得到了广泛研究，但在低级视觉感知和理解（HLPU）领域，尤其是在图像质量评估任务中，对幻觉的研究仍然不足。我们认为这些幻觉源于模型中缺乏明确的自我意识。为了解决这个问题，我们首先引入了HLPU指令数据库，这是第一个专门针对低级视觉任务中幻觉的指令数据库。该数据库包含大约20万个问答对，并分为四个子集，每个子集覆盖不同类型的指令。随后，我们提出了自我意识失败消除（SAFEQA）模型，该模型利用图像特征、显著区域特征和质量特征来提高模型在低级视觉任务中的感知和理解能力。此外，我们提出了增强自我意识偏好优化（ESA-PO）框架，以增加模型对知识边界的认识，从而减轻幻觉的发生。最后，我们在低级视觉任务上进行了全面实验，结果表明，我们提出的方法显著提高了模型在这些任务中的自我意识并减少了幻觉。值得注意的是，我们提出的方法提高了模型准确性和自我意识，并在各种评估指标上优于闭源模型。|
|**2025-03-26**|**TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews**|Huimin Xu et.al.|[2503.20666](http://arxiv.org/abs/2503.20666)|null|主题分析（TA）是一种广泛用于揭示非结构化文本数据中潜在意义的定性方法。TA在医疗保健领域提供了有价值的见解，但资源密集。大型语言模型（LLMs）已被引入进行TA，但在医疗保健领域的应用尚未得到探索。在此，我们提出了TAMA：一个使用多智能体LLMs进行临床访谈的人机协作主题分析框架。我们利用多智能体系统的可扩展性和连贯性，通过智能体之间的结构化对话来协调主题分析中心脏病专家的专业知识。使用患有异常冠状动脉起源（AAOCA）这种罕见先天性心脏病儿童的家长访谈记录，我们证明TAMA优于现有的LLM辅助TA方法，实现了更高的主题命中率、覆盖率和独特性。TAMA通过利用具有人类在环（human-in-the-loop）集成的多智能体LLM系统，在增强质量的同时显著降低人工工作量，展示了在临床环境中进行自动化主题分析的强大潜力。|
|**2025-03-26**|**Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging**|Han Wu et.al.|[2503.20641](http://arxiv.org/abs/2503.20641)|**[link](https://github.com/hahahawu/long-to-short-via-model-merging)**|从系统1推理到系统2推理的过渡在大规模语言模型（LLMs）中标志着通过深思熟虑、迭代思考处理复杂任务的重大进步。然而，这种进步通常以效率为代价，因为模型倾向于过度思考，在没有成比例提高输出质量的情况下产生冗余推理步骤。长到短（L2S）推理已成为解决这一挑战的有希望的方法，旨在平衡推理深度与实际效率。虽然现有的方法，如监督微调（SFT）、强化学习（RL）和提示工程，已显示出潜力，但它们要么计算成本高昂，要么不稳定。另一方面，模型合并提供了一种成本效益高且稳健的替代方案，通过整合系统1模型的快速思考能力和系统2模型的系统推理。在这项工作中，我们针对L2S推理的模型合并进行了全面的实证研究，探索了包括基于任务向量的、基于奇异值分解（SVD）的和基于激活信息合并的多种方法。我们的实验表明，模型合并可以将平均响应长度减少多达55%，同时保持或甚至提高基线性能。我们还发现模型规模与合并效之间存在强相关性，并在1.5B/7B/14B/32B模型上进行了广泛的评估。此外，我们调查了合并模型自我批判和自我纠正的能力，以及根据任务复杂度自适应响应长度的能力。我们的发现将模型合并突出为L2S推理的高效且有效范式，为过度思考问题提供了实际解决方案，同时保持了系统2推理的稳健性。这项工作可在GitHub上找到：https://github.com/hahahawu/Long-to-Short-via-Model-Merging。|
|**2025-03-26**|**Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions**|Alessandro Maisto et.al.|[2503.20623](http://arxiv.org/abs/2503.20623)|null|角色扮演游戏（RPG）是一种玩家之间互动以创造叙事的游戏。在RPG中，玩家的角色主要基于玩家与其角色的互动。这种新兴的共享叙事形式，主要是口头的，正越来越受到关注。特别是，许多作者研究了在游戏中将大型语言模型（LLM）作为演员的应用。在本文中，我们旨在探讨在无人干预的情况下，大型语言模型（LLM）的语言在生成RPG会话时表现出口头或书面特征的程度。我们将对生成的文本的词汇和句法特征进行语言学分析，并将结果与对话、人类RPG会话记录和书籍的分析进行比较。我们发现，LLM展现出一种与其他所有文本类别，包括口头对话、人类RPG会话和书籍都不同的模式。我们的分析展示了训练如何影响LLM的表达方式，并为这些工具的叙事能力提供了重要的指示。|
|**2025-03-26**|**What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond**|Wenchao Gu et.al.|[2503.20589](http://arxiv.org/abs/2503.20589)|null|由于复杂的代码依赖和大型语言模型（LLMs）在处理长文本时的局限性，库级别的代码生成仍然具有挑战性。尽管检索增强生成（RAG）框架被广泛采用，但不同检索信息源——如上下文代码、API和类似片段——的有效性尚未得到严格的评估。通过在两个基准测试上的实证研究，我们证明了上下文代码和潜在的API信息显著提升了LLMs的性能，而检索到的类似代码往往引入噪声，导致结果下降多达15%。基于初步结果，我们提出了AllianceCoder，这是一种新颖的上下文集成方法，它使用思维链提示将用户查询分解为实现步骤，并通过语义描述匹配检索API。在CoderEval和RepoExec上的大量实验中，AllianceCoder实现了最先进的性能，将Pass@1提升了多达20%，超过现有方法。|
|**2025-03-26**|**LLPut: Investigating Large Language Models for Bug Report-Based Input Generation**|Alif Al Hasan et.al.|[2503.20578](http://arxiv.org/abs/2503.20578)|null|导致故障的输入在诊断和分析软件错误中扮演着至关重要的角色。错误报告通常包含这些输入，开发者从中提取以辅助调试。由于错误报告是用自然语言编写的，先前的研究已经利用了各种自然语言处理（NLP）技术来自动提取输入。随着大型语言模型（LLMs）的出现，一个重要的研究问题随之产生：生成式LLMs在从错误报告中提取导致故障的输入方面能有多有效？在本文中，我们提出了一种名为LLPut的技术，用于实证评估三种开源生成式LLMs——LLaMA、Qwen和Qwen-Coder——在从错误报告中提取相关输入方面的性能。我们在包含206个错误报告的数据集上进行了实验评估，以评估这些模型的准确性和有效性。我们的发现提供了关于生成式LLMs在自动化错误诊断中能力和局限性的见解。|
|**2025-03-25**|**CoLLM: A Large Language Model for Composed Image Retrieval**|Chuong Huynh et.al.|[2503.19910](http://arxiv.org/abs/2503.19910)|**[link](https://github.com/hmchuong/CoLLM)**|**组成图像检索（CIR）是一个复杂任务，旨在根据多模态查询检索图像。典型的训练数据由包含参考图像、所需修改的文本描述和目标图像的三元组组成，获取这些数据既昂贵又耗时。CIR数据集的稀缺性导致了使用合成三元组或利用无处不在的网页爬取的图像-标题对来利用视觉-语言模型（VLMs）的零样本方法。然而，这些方法存在重大局限性：合成三元组受限于规模有限、缺乏多样性和不自然的修改文本，而图像-标题对由于缺乏三元组数据而阻碍了多模态查询的联合嵌入学习。此外，现有方法难以处理复杂和细微的修改文本，这些文本需要高级的视觉和语言模态融合和理解。我们提出了CoLLM，一个一站式框架，有效地解决了这些局限性。我们的方法从图像-标题对中动态生成三元组，使监督训练无需手动标注。我们利用大型语言模型（LLMs）生成参考图像和修改文本的联合嵌入，促进更深入的多模态融合。此外，我们引入了多文本CIR（MTCIR），一个包含340万个样本的大规模数据集，并改进了现有的CIR基准（CIRR和Fashion-IQ）以提高评估可靠性。实验结果表明，CoLLM在多个CIR基准和设置中实现了最先进的性能。MTCIR取得了具有竞争力的结果，性能提升了高达15%。我们改进的基准为CIR模型提供了更可靠的评估指标，有助于推动这一重要领域的发展。**|
|**2025-03-25**|**A Multi-Agent Framework Integrating Large Language Models and Generative AI for Accelerated Metamaterial Design**|Jie Tian et.al.|[2503.19889](http://arxiv.org/abs/2503.19889)|null|超材料因其卓越的机械、电磁和热性能而闻名，在众多应用领域具有变革潜力，但其设计仍然受限于劳动密集型的试错方法和有限的数据互操作性。在这里，我们介绍了CrossMatAgent——一个新颖的多智能体框架，它协同整合了大型语言模型与最先进的生成式AI，以革新超材料设计。通过协调一个由各司其职的智能体组成的层级团队——每个智能体专门从事图案分析、架构合成、提示工程和监督反馈等任务——我们的系统利用了GPT-4o的多模态推理能力，以及DALL-E 3的生成精度和微调后的Stable Diffusion XL模型。这种集成方法自动化了数据增强，提高了设计精度，并生成了适用于模拟和3D打印的超材料图案。包括基于CLIP的对齐、SHAP可解释性分析和在多种负载条件下的机械模拟在内的全面评估，证明了该框架生成多样、可重复且适用于应用的设计的 ability。因此，CrossMatAgent建立了一个可扩展的、AI驱动的范式，弥合了概念创新与实际实现之间的差距，为超材料发展的加速铺平了道路。|
|**2025-03-25**|**CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation**|Nengbo Wang et.al.|[2503.19878](http://arxiv.org/abs/2503.19878)|null|大型语言模型（LLMs）通过检索增强生成（RAG）技术革新了自然语言处理（NLP），尤其是通过整合外部知识提升了LLMs的能力。然而，传统的RAG系统面临着一些关键的限制，包括由于文本分块导致的上下文连续性破坏，以及对检索过程的语义相似性过度依赖。为了解决这些问题，我们提出了CausalRAG，这是一个将因果图整合到检索过程中的新颖框架。通过构建和追踪因果关系，CausalRAG保持了上下文连续性，并提高了检索精度，从而产生了更准确和可解释的响应。我们评估了CausalRAG与常规RAG和基于图的RAG方法，证明了其在多个指标上的优越性。我们的研究结果表明，将检索建立在因果推理基础上为知识密集型任务提供了一种有前途的方法。|
|**2025-03-25**|**SLA-Awareness for AI-assisted coding**|Kishanthan Thangarajah et.al.|[2503.19876](http://arxiv.org/abs/2503.19876)|null|将人工智能辅助编码工具集成到开发环境中，可以大幅缩短开发时间，并使开发者能够通过使用代码大型语言模型（CodeLLMs）更多地专注于软件工程的创意和关键方面。这些编码助手自动化了重复且耗时的编码任务，如代码生成、代码补全、代码摘要和代码翻译。响应性是这些编码助手的关键要求，以保持实时交互性，使其使用不会妨碍开发者的工作流程。不同的编码任务具有独特的特征和延迟要求：对于代码补全任务，第一次标记时间（TTFT）延迟至关重要，而对于代码翻译任务，端到端（E2E）延迟至关重要。在优化资源使用的同时管理这些不同的要求，提出了重大挑战。现有工作采用模型即服务（MaaS）范式来服务于单个CodeLLMs，但由于缺乏端到端延迟意识，无法有效管理并发编码任务和CodeLLM推理调用的延迟要求。另一个挑战是在共享集群环境中部署服务系统时保持资源利用率高。为了解决这些挑战，我们提出了编码助手任务编排器（CATO），这是一个运行时系统，旨在在满足延迟要求的同时，服务于各种编码任务并最大化资源利用。我们的实验表明，当同时服务于所有类型的编码任务时，对于TTFT关键任务，CATO将整体吞吐量和资源利用率分别提高了10%和41.1%。代码摘要任务的P95 E2E延迟也降低了18%，与现有系统相比，代码生成任务的P95 TTFT降低了14%。|
|**2025-03-25**|**Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking**|Xiaoyu Tian et.al.|[2503.19855](http://arxiv.org/abs/2503.19855)|null|近期大型语言模型（LLMs）如OpenAI-o1和DeepSeek-R1的进展展示了测试时缩放的有效性，其中扩展推理过程显著提升了模型性能。尽管如此，当前模型在处理长文本和强化学习（RL）训练效率方面仍存在限制。为了解决这些问题，我们提出了一种简单而有效的测试时缩放方法——多轮思考。这种方法通过利用先前答案作为后续轮次的提示来迭代地细化模型推理。在多个模型上进行的广泛实验，包括QwQ-32B和DeepSeek-R1，在各种基准测试中（如AIME 2024、MATH-500、GPQA-diamond和LiveCodeBench）均显示出性能提升。例如，QwQ-32B在AIME 2024数据集上的准确率从第1轮的80.3%提高到第2轮的82.1%，而DeepSeek-R1也显示出类似的增长，从79.7%增加到82.0%。这些结果证实了多轮思考是一种广泛适用的简单方法，能够实现模型性能的稳定提升，突显了其在测试时缩放技术未来发展的潜力。关键提示：{原始问题提示} 助手的上一个答案是：<answer> {上一轮答案} </answer>，请重新回答。|
|**2025-03-25**|**FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs**|Carlos Plou et.al.|[2503.19850](http://arxiv.org/abs/2503.19850)|null|在长达一小时的视频中进行信息检索是一项重大挑战，即使是目前最先进的视觉-语言模型（VLMs）也不例外，尤其是在所需信息仅存在于一小部分帧中时。长视频数据对VLMs构成了挑战，因为其上下文窗口限制和难以精确定位包含答案的帧。我们新颖的视频智能体FALCONEye结合了VLM和大型语言模型（LLM），以在视频中搜索相关信息并定位包含答案的帧。FALCONEye的创新之处在于：1）提出的元架构，与最先进技术中的短视频方法相比，更适合处理长达一小时的视频；2）一种新的高效探索算法，利用短剪辑、字幕和答案置信度来定位信息；3）我们对答案置信度的最先进VLM校准分析。我们的智能体建立在小型VLM和中等大小的LLM之上，可以在标准计算资源上运行。我们还发布了FALCON-Bench，这是一个用于评估长视频（平均超过1小时）答案搜索挑战的基准，强调了开放式问题评估的必要性。我们的实验表明，在FALCON-Bench中，FALCONEye的性能优于最先进的技术，在相关基准测试中表现与最先进的技术相似或更好。|
|**2025-03-25**|**Towards Online Multi-Modal Social Interaction Understanding**|Xinpeng Li et.al.|[2503.19851](http://arxiv.org/abs/2503.19851)|**[link](https://github.com/sampson-lee/onlinemmsi)**|多模态社交交互理解（MMSI）在人机交互系统中至关重要。在现实场景中，人工智能代理需要提供实时反馈。然而，现有的模型通常依赖于过去和未来的上下文，这阻碍了它们应用于现实世界问题。为了弥合这一差距，我们提出了一种在线MMSI设置，其中模型必须仅使用历史信息来解决MMSI任务，例如记录的对话和视频流。为了应对缺失有用未来上下文的挑战，我们开发了一个名为Online-MMSI-VLM的新框架，该框架利用两种互补策略：多党派对话预测和具有多模态大型语言模型的社会感知视觉提示。首先，为了丰富语言上下文，多党派对话预测以粗到细的方式模拟潜在的未来话语，预测即将到来的说话人轮次，然后生成细粒度的对话细节。其次，为了有效地结合视觉社交线索，如目光和手势，社会感知视觉提示使用每个个人和帧的边界框和身体关键点突出视频中的社会动态。在三个任务和两个数据集上的大量实验表明，我们的方法达到了最先进的性能，并且显著优于基线模型，表明它在在线MMSI上的有效性。代码和预训练模型将在以下网址公开发布：https://github.com/Sampson-Lee/OnlineMMSI。|
|**2025-03-25**|**A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950**|Zhao Fang et.al.|[2503.19844](http://arxiv.org/abs/2503.19844)|null|本文比较了大型语言模型（LLMs）和传统的自然语言处理（NLP）工具在对中国1900年至1950年的文本进行词性标注、命名实体识别（NER）和词语切分方面的性能。由于历史文献具有表意文字、缺乏自然词语边界和显著的语言变化等特点，因此在文本分析方面面临挑战。使用来自上海图书馆民国期刊语料库的样本数据集，将传统的Jieba和spaCy工具与LLMs，包括GPT-4o、Claude 3.5和GLM系列进行了比较。结果显示，在所有指标上，LLMs均优于传统方法，尽管计算成本显著增加，突显了准确性和效率之间的权衡。此外，LLMs在处理特定体裁的挑战，如诗歌和时间变化（即1920年前后文本）方面表现更佳，表明其上下文学习能力可以通过减少对特定领域训练数据的依赖，推动历史文本的NLP方法发展。|
|**2025-03-25**|**SemEval-2025 Task 9: The Food Hazard Detection Challenge**|Korbinian Randl et.al.|[2503.19800](http://arxiv.org/abs/2503.19800)|null|在这个挑战中，我们探讨了基于文本的食品危害预测，并针对长尾分布的类别进行了研究。任务分为两个子任务：(1) 预测网络文本是否暗示了十种食品危害类别，并识别相关的食品类别；(2) 通过为危害和产品分配特定标签，提供更精细的分类。我们的发现强调了大型语言模型生成的合成数据在长尾分布过采样方面的高度有效性。此外，我们发现仅使用编码器、编码器-解码器和仅使用解码器的微调系统在两个子任务中都实现了可比较的最大性能。在此挑战期间，我们逐步发布了（在CC BY-NC-SA 4.0许可下）一套包含6,644个手动标注的食品事件报告的新数据集。|
|**2025-03-25**|**PAVE: Patching and Adapting Video Large Language Models**|Zhuoming Liu et.al.|[2503.19794](http://arxiv.org/abs/2503.19794)|**[link](https://github.com/dragonlzm/pave)**|预先训练的视频大型语言模型（Video LLMs）表现出惊人的推理能力，然而，将这些模型适应于涉及额外模态或数据类型（如音频或3D信息）的新任务仍然具有挑战性。在本文中，我们提出了PAVE，这是一个灵活的框架，用于将预训练的Video LLMs适应具有侧通道信号的下游任务，例如音频、3D线索或多视图视频。PAVE引入了轻量级的适配器，被称为“补丁”，它们在不修改模型架构或预训练权重的情况下，为基模型添加少量参数和操作。通过这种方式，PAVE能够有效地适应预训练的基模型，以支持各种下游任务，包括视听问答、3D推理、多视图视频识别和高帧率视频理解。在这些任务中，PAVE显著提升了基模型的表现，超越了最先进的特定任务模型，同时仅增加大约0.1%的额外FLOPs和参数成本。此外，PAVE支持多任务学习，并且在不同Video LLMs上的泛化能力良好。我们的代码可在https://github.com/dragonlzm/PAVE上找到。|
|**2025-03-24**|**SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding**|Mingze Xu et.al.|[2503.18943](http://arxiv.org/abs/2503.18943)|null|我们介绍了SlowFast-LLaVA-1.5（简称SF-LLaVA-1.5），这是一系列视频大型语言模型（LLMs），为长视频理解提供了一种高效的标记解决方案。该模型家族采用了两流SlowFast机制，能够高效地建模长距离时间上下文，以满足轻量级、移动友好的视频LLMs的需求。我们提供了从10亿到70亿参数的模型，通过简化的训练流程和高质量的数据混合（由公开数据集组成）进行优化。实验结果表明，SF-LLaVA-1.5在各种视频和图像基准测试中实现了具有竞争力的性能，并且在所有模型尺寸上都表现出稳健的结果。值得注意的是，SF-LLaVA-1.5在长视频理解（例如LongVideoBench和MLVU）方面取得了最先进的成果，并且在多个视频基准测试中的小规模（10亿和30亿）上表现出色。|
|**2025-03-24**|**Video-T1: Test-Time Scaling for Video Generation**|Fangfu Liu et.al.|[2503.18942](http://arxiv.org/abs/2503.18942)|null|随着训练数据规模、模型大小和计算成本的提升，视频生成在数字创作领域取得了令人瞩目的成果，使用户能够在各个领域表达创意。最近，大型语言模型（LLMs）的研究人员将扩展应用到测试时，通过使用更多的推理时间计算，可以显著提高LLMs的性能。我们不是通过昂贵的训练成本来提升视频基础模型，而是探索测试时扩展（TTS）在视频生成中的力量，旨在回答以下问题：如果允许视频生成模型使用非平凡的推理时间计算量，在给定具有挑战性的文本提示的情况下，它能够提高生成质量多少。在这项工作中，我们将视频生成的测试时扩展重新解释为一个搜索问题，从高斯噪声空间采样到目标视频分布的更好轨迹。具体来说，我们使用测试时验证器构建搜索空间，并提供反馈和启发式算法来指导搜索过程。给定一个文本提示，我们首先通过在推理时间增加噪声候选者来探索一个直观的线性搜索策略。由于一次性对全部帧进行全步去噪需要大量的测试时间计算成本，我们进一步设计了一种更高效的TTS方法，称为帧树（ToF），该方法以自回归的方式自适应地扩展和修剪视频分支。在文本条件视频生成基准测试上的大量实验表明，增加测试时间计算量始终会导致视频质量显著提高。项目页面：https://liuff19.github.io/Video-T1|
|**2025-03-24**|**Exploring Training and Inference Scaling Laws in Generative Retrieval**|Hongru Cai et.al.|[2503.18941](http://arxiv.org/abs/2503.18941)|**[link](https://github.com/HongruCai/SLGR)**|生成式检索作为一种新型范式，利用大型语言模型（LLMs）自动回归生成文档标识符。尽管前景广阔，但其性能和可扩展性的基础机制仍不明确。我们对生成式检索的训练和推理缩放定律进行了系统性研究，探讨模型大小、训练数据规模和推理时计算量如何共同影响检索性能。为了解决缺乏合适指标的问题，我们提出了一种受对比熵和生成损失启发的全新评估度量，提供了一种连续的性能信号，使得可以稳健地对不同的生成式检索方法进行对比。我们的实验表明，基于n-gram的方法在训练和推理缩放定律方面表现出强大的一致性，尤其是在与更大的LLMs结合使用时。此外，增加推理计算量可以带来显著的性能提升，揭示了生成式检索在推理时可以从更高的计算预算中受益。在这些设置中，LLaMA模型始终优于T5模型，这表明在生成式检索中，大型仅解码器模型具有特殊优势。综合来看，我们的发现强调了模型大小、数据可用性和推理计算相互作用的协同作用，释放了生成式检索的潜力，为未来系统的设计和优化提供了新的见解。|
|**2025-03-24**|**Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training**|Brian R. Bartoldson et.al.|[2503.18929](http://arxiv.org/abs/2503.18929)|null|强化学习（RL）是大型语言模型（LLM）后训练的关键组成部分。然而，现有用于后训练的在线策略算法与使用经验重放缓冲区本质上不兼容，而分布式离线策略演员可以大规模填充这些缓冲区，以增强探索能力。我们提出通过轨迹平衡与异步（TBA），一种大规模可扩展的LLM RL系统，有效地获得重放缓冲区的这种好处。与现有方法不同，TBA将更多的计算资源用于搜索，持续生成用于中央重放缓冲区的离线数据。训练节点同时根据奖励或近期性从该缓冲区采样数据，使用轨迹平衡（TB）更新策略，TB是为GFlowNets引入的寻求多样性的RL目标。TBA提供了三个关键优势：（1）解耦训练和搜索，将训练墙钟时间加快4倍或更多；（2）通过大规模离线采样提高多样性；（3）在稀疏奖励设置中实现可扩展的搜索。在数学推理、偏好调整和自动红队（多样性和代表性的后训练任务）方面，TBA相较于强大的基线产生了速度和性能的提升。|
|**2025-03-24**|**FFN Fusion: Rethinking Sequential Computation in Large Language Models**|Akhiad Bercovich et.al.|[2503.18908](http://arxiv.org/abs/2503.18908)|null|我们介绍了FFN融合，一种通过识别和利用并行化机会来减少大型语言模型中顺序计算的架构优化技术。我们的关键洞察是，前馈网络（FFN）层的序列，尤其是去除特定注意力层后留下的序列，通常可以并行化，而精度影响最小。我们开发了一种识别和融合此类序列的原理性方法，将它们转换为并行操作，从而显著降低推理延迟，同时保持模型行为。将这些技术应用于Llama-3.1-405B-Instruct，我们创建了Llama-Nemotron-Ultra-253B-Base（Ultra-253B-Base），一个高效且即将公开发布的模型，在推理延迟上实现了1.71倍的速度提升，每令牌成本降低了35倍，同时在各个基准测试中保持了强大的性能。通过对49B至253B参数模型的广泛实验，我们证明FFN融合在更大规模上越来越有效，可以补充现有的优化技术，如量化剪枝。最引人入胜的是，我们发现即使是包含注意力和FFN层的完整变压器块有时也可以并行化，这为神经架构设计指出了新的方向。|
|**2025-03-24**|**xKV: Cross-Layer SVD for KV-Cache Compression**|Chi-Chih Chang et.al.|[2503.18893](http://arxiv.org/abs/2503.18893)|**[link](https://github.com/abdelfattah-lab/xkv)**|大型语言模型（LLMs）具有长上下文窗口，能够实现强大的应用，但代价是高内存消耗来存储键值状态（KV-Cache）。最近的研究试图将多个层的KV-cache合并为共享表示，但这些方法要么需要昂贵的预训练，要么依赖于层间高每词余弦相似性的假设，这在实践中通常不成立。我们发现，KV-Cache的多层中的主导奇异向量惊人地良好对齐。利用这一洞察，我们提出了xKV，一种简单的后训练方法，该方法对分组层的KV-Cache应用奇异值分解（SVD）。xKV将多个层的KV-Cache整合到一个共享的低秩子空间中，显著减少了KV-Cache的大小。通过在RULER长上下文基准上对广泛使用的LLMs（例如Llama-3.1和Qwen2.5）进行大量评估，xKV实现了比最先进的层间技术高达6.8倍的压缩率，同时提高了2.7%的准确性。此外，xKV与新兴的多头潜在注意力（MLA）（例如DeepSeek-Coder-V2）兼容，在编码任务上实现了显著的3倍压缩率，而性能没有下降。这些结果突出了xKV在解决长上下文LLM推理内存瓶颈方面的强大能力和多功能性。我们的代码在以下网址公开：https://github.com/abdelfattah-lab/xKV。|
|**2025-03-24**|**AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration**|Zhexuan Wang et.al.|[2503.18891](http://arxiv.org/abs/2503.18891)|**[link](https://github.com/wangzx1219/agentdropout)**|多智能体系统（MAS）基于大型语言模型（LLM）在协作问题解决中展现出巨大的潜力。然而，它们仍面临着低通信效率和次优任务性能的挑战，这使得精心设计智能体的通信拓扑变得尤为重要。受高效团队中角色动态调整的管理理论启发，我们提出了AgentDropout，通过优化通信图的邻接矩阵来识别冗余智能体和不同通信轮次的通信，并消除它们以提高令牌效率和任务性能。与最先进的方法相比，AgentDropout实现了平均减少21.6%的提示令牌消耗和18.4%的完成令牌消耗，以及任务性能提升了1.14。此外，扩展实验表明，AgentDropout实现了显著的领域迁移性和结构鲁棒性，揭示了其可靠性和有效性。我们将在https://github.com/wangzx1219/AgentDropout上发布我们的代码。|
|**2025-03-24**|**Toward building next-generation Geocoding systems: a systematic review**|Zhengcong Yin et.al.|[2503.18888](http://arxiv.org/abs/2503.18888)|null|地理编码系统在科学研究中用于空间分析以及日常生活中通过位置服务得到广泛应用。地理编码数据的质量对后续处理和应用有着重大影响，凸显了对下一代系统的需求。为满足这一需求，本综述首先考察了这些系统在不同场景下对地理编码输入和输出的演变需求。接着，它详细分析了如何构建此类系统，通过将其分解为关键功能组件，并回顾了从传统基于规则的方法到信息检索、自然语言处理和大型语言模型的高级技术的广泛方法。最后，我们根据最近的技术进步，确定了改善下一代地理编码系统的机会。|
|**2025-03-24**|**I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders**|Andrey Galichin et.al.|[2503.18878](http://arxiv.org/abs/2503.18878)|**[link](https://github.com/airi-institute/sae-reasoning)**|大型语言模型（LLMs）在自然语言处理领域取得了显著的成功。最近的进展导致了一种新型推理LLMs的发展；例如，开源的DeepSeek-R1通过整合深度思考和复杂推理实现了最先进的性能。尽管这些能力令人印象深刻，但此类模型的内部推理机制仍然未被探索。在本工作中，我们采用了稀疏自编码器（SAEs），这是一种将神经网络潜在表示学习为可解释特征稀疏分解的方法，以识别驱动DeepSeek-R1系列模型推理的特征。首先，我们提出了一种从SAE表示中提取候选“推理特征”的方法。我们通过实证分析和可解释性方法验证了这些特征，证明了它们与模型推理能力的直接相关性。关键的是，我们展示了系统地引导这些特征可以增强推理性能，为LLMs中的推理提供了第一个机制性解释。代码可在https://github.com/AIRI-Institute/SAE-Reasoning上找到。|
|**2025-03-24**|**Reimagining Memory Access for LLM Inference: Compression-Aware Memory Controller Design**|Rui Xie et.al.|[2503.18869](http://arxiv.org/abs/2503.18869)|null|大型语言模型（LLM）的推理效率通常受到大量内存带宽和容量需求的限制。现有技术，如剪枝、量化、专家混合/深度混合，在牺牲推理质量的同时，减少了内存容量和/或带宽消耗。本文介绍了一种设计解决方案，通过增强AI加速器中的片上内存控制器来进一步缓解内存瓶颈，实现两个主要目标：（1）通过无损块压缩（例如，LZ4和ZSTD）模型权重和键值（KV）缓存，在不影响推理质量的情况下显著降低内存容量和带宽使用；（2）使内存带宽和能耗能够与上下文相关的动态量化成比例地扩展。这些目标通过在片上内存控制器中配备机制来实现，这些机制通过LLM感知的内存放置和表示配置来提高权重和KV缓存的细粒度位级可访问性和可压缩性。在公开可用的LLM上的实验结果证明了这种方法的有效性，显示出模型权重减少了25.2%，KV缓存减少了46.9%。此外，我们的4GHz和32通道（7nm）硬件原型实现了8TB/s的吞吐量，且面积开销适中（小于3.8mm²），这突出了LLM感知内存控制作为高效大规模推理的关键的可行性。|
|**2025-03-21**|**Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique**|Yansi Li et.al.|[2503.17363](http://arxiv.org/abs/2503.17363)|null|增强大型语言模型（LLMs）的推理能力，尤其是对于需要多步逻辑推理的复杂任务，仍然是一个重大挑战。传统的推理时间缩放方法利用来自过程奖励模型的标量奖励信号来评估候选推理步骤，但这些标量奖励缺乏理解和证明每个步骤所必需的细微定性信息。在本文中，我们提出了一种新颖的推理时间缩放方法——逐步自然语言自我批评（PANEL），该方法使用自生成的自然语言批评作为反馈来指导步骤级搜索过程。通过为每个候选推理步骤生成丰富、易于理解的评价，PANEL保留了必要的定性信息，便于在推理过程中做出更明智的决策。这种方法绕过了需要特定任务的验证器和相关的训练开销，使其能够广泛应用于各种任务。在包括AIME和GPQA在内的具有挑战性的推理基准测试中，实验结果表明PANEL显著提高了推理性能，优于传统的基于标量奖励的方法。我们的代码可在https://github.com/puddingyeah/PANEL上获取，以支持并鼓励该有前景领域的未来研究。|
|**2025-03-21**|**OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement**|Yihe Deng et.al.|[2503.17352](http://arxiv.org/abs/2503.17352)|**[link](https://github.com/yihedeng9/openvlthinker)**|最近，DeepSeek-R1所展示的进步表明，通过具有可验证奖励的强化学习（RL），大型语言模型（LLMs）可以实现复杂的推理能力，包括诸如自我验证和自我纠正等高级行为，这显著提高了模型在AIME等具有挑战性的任务上的性能。受这些发现启发，我们的研究调查了是否可以将类似的推理能力成功集成到大型视觉语言模型（LVLMs）中，并评估其对具有挑战性的多模态推理任务的影响。我们考虑了一种方法，该方法通过迭代利用轻量级训练数据上的监督微调（SFT）和强化学习（RL）来进一步提高模型泛化能力。最初，通过使用来自不同视觉数据集的高质量图像标题生成推理步骤，从纯文本R1模型中提取推理能力。随后，通过迭代的RL训练进一步增强了推理技能，每次迭代的RL改进模型为下一轮生成精细的SFT数据集。这种迭代过程产生了OpenVLThinker，这是一个在MathVista、MathVerse和MathVision等具有挑战性的基准测试上表现出持续改进的推理性能的LVLM，展示了我们策略在稳健视觉语言推理方面的潜力。代码、模型和数据存储在https://github.com/yihedeng9/OpenVLThinker上。|
|**2025-03-21**|**Capturing Individual Human Preferences with Reward Features**|André Barreto et.al.|[2503.17338](http://arxiv.org/abs/2503.17338)|null|我们通常使用一个不区分个体的奖励模型来从人类反馈中进行强化学习，我们认为在如大型语言模型训练这样具有高度潜在分歧的场景中，这不太可能是一个好的设计选择。我们提出了一种将奖励模型专门针对个人或一组人的方法。我们的方法基于观察，即个体偏好可以作为一组通用奖励特征的线性组合来捕捉。我们展示了如何学习这样的特征，并随后使用它们快速将奖励模型适应于特定个体，即使他们的偏好没有反映在训练数据中。我们进行了在大型语言模型上的实验，比较了所提出的架构与一个非自适应奖励模型以及自适应的替代模型，包括那些进行上下文个性化的模型。根据训练数据中的分歧程度，我们的模型要么显著优于基线，要么使用更简单的架构和更稳定的训练与基线匹配其性能。|
|**2025-03-21**|**Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs**|Reem Gody et.al.|[2503.17336](http://arxiv.org/abs/2503.17336)|null|大型语言模型（LLMs）在对话式人工智能领域展示了惊人的能力，使得聊天机器人能够进行开放域的响应，并能够进行高级对话处理，如总结、意图分类和洞察生成。然而，这些模型资源密集，需要大量的内存和计算能力。为了解决这个问题，我们提出了一种经济高效的解决方案，该方案针对目标下游应用，对感兴趣的对话片段进行筛选，而不是处理每一个片段。在这项工作中，我们介绍了一种创新的方法，该方法利用LLMs的知识蒸馏来开发一个基于意图的多方对话过滤器，针对计算能力受限的环境进行了优化。我们的方法结合了不同的策略，创建了一个多样化的多方对话数据集，该数据集标注了目标意图，然后用于微调MobileBERT模型以进行多标签意图分类。该模型在效率和性能之间取得了平衡，有效地根据对话片段的意图进行筛选。通过只将相关的片段传递给LLM进行进一步处理，我们的方法显著降低了整体运行成本，这在我们的实验中得到了证明，具体取决于意图和数据分布。|
|**2025-03-21**|**CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities**|Yuxuan Zhu et.al.|[2503.17332](http://arxiv.org/abs/2503.17332)|**[link](https://github.com/uiuc-kang-lab/cve-bench)**|**大型语言模型（LLM）代理越来越能够自主进行网络攻击，对现有应用构成重大威胁。这种日益增长的风险突显了评估LLM代理利用Web应用漏洞能力的现实世界基准的迫切需求。然而，现有的基准存在不足，因为它们仅限于抽象的Capture the Flag比赛或缺乏全面覆盖。构建现实世界漏洞的基准既需要专门的知识来重现利用，又需要一种系统性的方法来评估不可预测的威胁。为了应对这一挑战，我们引入了CVE-Bench，这是一个基于关键严重性的通用漏洞和暴露（CVE）的现实世界网络安全基准。在CVE-Bench中，我们设计了一个沙箱框架，允许LLM代理在模拟现实世界条件的场景中利用易受攻击的Web应用，同时提供对其利用的有效评估。我们的评估表明，最先进的代理框架可以解决高达13%的漏洞。**|
|**2025-03-21**|**LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language**|Kun Chu et.al.|[2503.17309](http://arxiv.org/abs/2503.17309)|**[link](https://github.com/kchu/llm-map)**|双臂机器人操作提供了显著的灵活性，但也因为涉及两个手的空间和时间协调的复杂性而带来固有的挑战。现有工作主要关注实现机器人手的人级操作技能，但对长期时间尺度上的任务规划关注较少。大型语言模型（LLMs）凭借其卓越的情境学习和零样本生成能力，已被应用于各种机器人实现中，以促进任务规划。然而，LLMs在长期推理上仍然存在错误，在复杂机器人任务中存在幻觉，在生成计划时缺乏逻辑正确性的保证。之前的工作，如LLM+P，通过符号规划器扩展了LLMs。然而，这些方法尚未成功应用于双臂机器人。双臂操作不可避免地带来新的挑战，需要不仅有效分解任务，还要高效分配任务。为了应对这些挑战，本文引入了LLM+MAP，这是一种集成LLM推理和多智能体规划的机器人规划框架，自动化有效且高效的双臂任务规划。我们在各种不同复杂性的长期操作任务上进行了模拟实验。我们的方法使用GPT-4o作为后端，并将其性能与直接由LLMs（包括GPT-4o、V3以及最近的强大推理模型o1和R1）生成的计划进行了比较。通过分析规划时间、成功率、组费用和规划步骤减少率等指标，我们证明了LLM+MAP的优越性能，同时也提供了关于机器人推理的见解。代码可在https://github.com/Kchu/LLM-MAP上获取。|
|**2025-03-21**|**Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests**|John Naulty et.al.|[2503.17302](http://arxiv.org/abs/2503.17302)|null|随着软件系统日益复杂，确保开发过程中的安全性提出了重大挑战。传统的手动代码审查通常成本高昂、耗时且不适合快速的工作流程，而自动工具往往存在高误报率，限制了它们的可靠性。为了解决这些问题，我们引入了Bugdar，这是一个AI增强的代码审查系统，可以无缝集成到GitHub拉取请求中，提供近乎实时的、上下文感知的漏洞分析。Bugdar利用可微调的大型语言模型（LLMs）和检索增强生成（RAGs）来提供针对特定项目的、可操作的反馈，这些反馈与每个代码库的独特需求和开发者实践相一致。支持包括Solidity、Move、Rust和Python在内的多种编程语言，Bugdar展示了卓越的效率，平均每个拉取请求处理时间为56.4秒，或每秒处理30行代码。这比手动审查快得多，手动审查可能需要每个拉取请求数小时。通过促进主动的编码安全方法，Bugdar减少了对手动审查的依赖，加速了开发周期，并增强了软件系统的安全态势，而不会影响生产力。|
|**2025-03-21**|**CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement**|Gaifan Zhang et.al.|[2503.17279](http://arxiv.org/abs/2503.17279)|null|句子的含义往往取决于其出现的上下文。尽管句子嵌入方法取得了进展，但如何根据上下文最佳地修改句子嵌入仍然不明确。为了解决这个问题，我们提出了条件感知句子嵌入（CASE），这是一种高效且准确的方法，用于在给定条件下为句子创建嵌入。首先，CASE使用大型语言模型（LLM）为条件创建嵌入，其中句子会影响在条件池化过程中计算出的令牌的注意力得分。接下来，学习一个监督非线性投影来降低基于LLM的文本嵌入的维度。我们在现有的标准基准数据集上展示了CASE在条件语义文本相似度（C-STS）方面显著优于先前提出的C-STS方法。我们发现，减去条件嵌入可以持续提高基于LLM的文本嵌入的C-STS性能。此外，我们还提出了一种监督降维方法，它不仅降低了基于LLM的嵌入的维度，还显著提高了它们的性能。|
|**2025-03-21**|**SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging**|Aladin Djuhera et.al.|[2503.17239](http://arxiv.org/abs/2503.17239)|**[link](https://github.com/aladinD/SafeMERGE)**|在下游任务上微调大型语言模型（LLMs）可能会无意中削弱它们的安全一致性，即使是在良性的微调数据集上。我们通过提出SafeMERGE，一个在微调后保持安全性的框架来应对这一挑战。它通过选择性地合并微调和安全一致的模型层，只在它们偏离安全行为时进行，通过余弦相似度标准来衡量安全行为，从而实现这一目标。我们在GSM8K和PubMedQA任务上，对Llama-2-7B-Chat和Qwen-2-7B-Instruct模型评估了SafeMERGE与其他微调和后微调阶段的方法，同时探索了不同的合并策略。我们发现，与基线相比，SafeMERGE在降低有害输出方面始终表现出色，而不会显著牺牲性能，有时甚至还能提高性能。结果表明，我们的选择性的、子空间引导的和逐层合并方法在微调LLMs中提供了有效的保护，以防止无意中丧失安全性，同时优于简单的后微调阶段防御措施。|
|**2025-03-21**|**FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs**|Albert Sawczyn et.al.|[2503.17229](http://arxiv.org/abs/2503.17229)|null|大语言模型（LLMs）经常生成幻觉内容，这给事实性至关重要的应用带来了重大挑战。虽然现有的幻觉检测方法通常在句子级别或段落级别进行操作，我们提出了FactSelfCheck，这是一种基于黑盒采样的新颖方法，它能够实现细粒度的事实级别检测。我们的方法将文本表示为包含以三元组形式存在的知识图。通过分析多个LLM响应之间的事实一致性，我们计算了细粒度的幻觉得分，而无需外部资源或训练数据。我们的评估表明，FactSelfCheck在性能上与领先的采样方法相当，同时提供了更详细的洞察。最值得注意的是，我们的事实级别方法显著提高了幻觉修正，与基线相比，事实性内容增加了35%，而句子级别的SelfCheckGPT仅提高了8%。我们检测的细粒度特性使得对幻觉内容的识别和纠正更加精确。|
|**2025-03-20**|**Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models**|Yang Sui et.al.|[2503.16419](http://arxiv.org/abs/2503.16419)|**[link](https://github.com/eclipsess/awesome-efficient-reasoning-llms)**|大型语言模型（LLMs）在复杂任务中展现出惊人的能力。近期，大型推理模型（LRMs）如OpenAI o1和DeepSeek-R1的进步，通过利用监督微调（SFT）和强化学习（RL）技术来增强思维链（CoT）推理，进一步提升了系统-2推理领域（如数学和编程）的性能。然而，虽然更长的CoT推理序列可以提高性能，但它们也引入了由于冗长和重复输出而导致的显著计算开销，即所谓的“过度思考现象”。在本文中，我们提供了第一个结构化调查，系统地调查和探索了在LLMs中实现高效推理的当前进展。总体而言，依靠LLMs的内在机制，我们将现有工作分为几个关键方向：（1）基于模型的效率推理，考虑将完整的推理模型优化为更简洁的推理模型或直接训练高效的推理模型；（2）基于推理输出的效率推理，旨在在推理过程中动态减少推理步骤和长度；（3）基于输入提示的效率推理，旨在根据输入提示的属性（如难度或长度控制）来提高推理效率。此外，我们介绍了用于训练推理模型的效率数据的使用，探讨了小型语言模型的推理能力，并讨论了评估方法和基准测试。|
|**2025-03-20**|**The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination**|Yifan Sun et.al.|[2503.16402](http://arxiv.org/abs/2503.16402)|**[link](https://github.com/astral-group/bdc_mitigation_assessment)**|基准数据污染（BDC）——将基准测试样本包含在训练集中——在大型语言模型（LLM）评估中引起了越来越多的关注，导致性能估计虚假膨胀，损害了评估的可靠性。为了解决这个问题，研究人员提出了各种缓解策略来更新现有基准，包括修改原始问题或基于它们生成新的问题。然而，对这些缓解策略有效性的严格检验仍然缺乏。在本文中，我们设计了一个系统和受控的流程，并提出了两个新颖的指标——保真度和抗污染能力——以对现有的BDC缓解策略进行细致和全面的评估。之前的评估方法，如准确度下降和准确度匹配，仅仅关注总体准确度，往往导致结论不完整或具有误导性。我们的指标通过强调问题级别的评估结果匹配来解决这个问题。在10个LLM、5个基准、20个BDC缓解策略和2个污染场景上的大量实验表明，在所有基准中，没有现有策略显著提高了相对于原始情况（即没有基准更新）的抗污染能力，并且没有策略能够有效地平衡保真度和抗污染能力。这些发现强调了设计更有效的BDC缓解策略的紧迫需求。我们的代码仓库可在https://github.com/ASTRAL-Group/BDC_mitigation_assessment上访问。|
|**2025-03-20**|**Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them**|Guanyu Chen et.al.|[2503.16401](http://arxiv.org/abs/2503.16401)|null|大型语言模型（LLMs）和视觉语言模型（VLMs）能够在各种场景下执行各种形式的推理任务，但它们是否真的在抽象任务和基于规则的推理中超越简单的记忆和模式匹配？为了回答这个问题，我们提出了一种新颖的实验方法，误导性微调（MisFT），以检验LLMs/VLMs是否通过改变它们对基本规则的原有理解来执行抽象推理。具体来说，通过构建一个包含与正确操作原则相矛盾的数学表达式的数据集，我们将模型微调以学习那些矛盾的规则，并评估它在不同测试领域上的泛化能力。通过一系列实验，我们发现当前的LLMs/VLMs能够有效地应用矛盾的规则来解决实际数学文字问题和图像表示的数学表达式，这表明存在一个在推理之前进行抽象的内部机制。|
|**2025-03-20**|**Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation**|Yijia Luo et.al.|[2503.16385](http://arxiv.org/abs/2503.16385)|**[link](https://github.com/elena-luo/SODE)**|最近，大型语言模型（LLMs）在通过长思维链（CoT）推理展示出显著的推理能力方面取得了显著进展。R1蒸馏方案已作为一种有前途的方法出现，用于训练具有增强推理能力的低成本模型。然而，驱动其有效性的潜在机制仍然不清楚。本研究考察了蒸馏数据的普适性，并确定了在LLM蒸馏中使长链推理能力高效转移的关键组件。我们的发现显示，从类似Qwen-QwQ这样的教师模型进行长CoT推理蒸馏的有效性在非同源模型上显著降低，挑战了当前蒸馏方法普遍性的假设。为了更深入地了解长CoT推理的结构和模式，我们提出了DLCoT（解构长思维链），一种蒸馏数据增强框架。DLCoT包括三个关键步骤：（1）数据分段以分解复杂的长CoT结构，（2）通过消除不可解和冗余的解决方案进行简化，以及（3）优化中间错误状态。我们的方法显著提高了模型性能和令牌效率，促进了高性能LLMs的开发。|
|**2025-03-20**|**LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images**|Leyang Wang et.al.|[2503.16376](http://arxiv.org/abs/2503.16376)|null|现代机器学习的成功，特别是在面部转换网络中，高度依赖于高质量、成对、大规模数据集的可用性。然而，获取足够的数据往往具有挑战性且成本高昂。受近期扩散模型在高质量图像合成中的成功以及大型语言模型（LLMs）的进展所启发，我们提出了一种名为LLM辅助成对图像生成（LaPIG）的新框架。该框架能够利用LLMs生成的标题构建全面、高质量的可视和热成对图像。我们的方法包括三个部分：使用ArcFace嵌入的可视图像生成、使用潜在扩散模型（LDMs）的热图像转换以及使用LLMs的标题生成。我们的方法不仅生成了多视角的成对可视和热图像以增加数据多样性，还产生了高质量的成对数据，同时保持了它们的身份信息。我们通过将我们的方法与现有方法进行比较，在公共数据集上对其进行了评估，证明了LaPIG的优越性。|
|**2025-03-20**|**CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners**|Yunzhi Yao et.al.|[2503.16356](http://arxiv.org/abs/2503.16356)|**[link](https://github.com/zjunlp/cake)**|知识编辑（KE）使能够对大型语言模型（LLMs）中的过时或错误信息进行修改。虽然现有的KE方法可以更新孤立的事实，但它们难以将这些更新推广到依赖于修改后知识的多跳推理任务中。通过对推理电路的分析——LLMs用于基于知识的推理的神经网络路径，我们观察到当前层局部化的KE方法，如MEMIT和WISE，仅编辑单个或少数模型层，难以有效地将这些更新信息纳入这些推理路径中。为了解决这一局限性，我们提出了CaKE（电路感知知识编辑），一种新的方法，它能够更有效地整合LLMs中的更新知识。CaKE利用我们基于电路分析精心策划的数据，强制模型利用修改后的知识，刺激模型为新整合的知识发展适当的推理电路。实验结果表明，CaKE能够在相关推理任务中更准确、更一致地使用更新知识，在MQuAKE数据集上比现有KE方法提高了平均20%的多跳推理准确率。我们将在https://github.com/zjunlp/CaKE上发布代码和数据。|
|**2025-03-20**|**LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates**|Ying Shen et.al.|[2503.16334](http://arxiv.org/abs/2503.16334)|null|近期研究发现，在基于Transformer的大型语言模型（LLM）中，大量知识被编码在其前馈（FFN）层中，其中每个FFN层可以解释为子更新的总和，每个子更新对应于FFN值参数矩阵中的一个加权列向量，这些向量通常编码人类可解释的概念。鉴于此，我们假设可以通过根据这些子更新与输入或目标输出风格的相关性来调节它们的贡献，从而进一步提高和控制系统性能和行为。我们提出了LLMBRACES，这是一种新颖且高效的方法，它计算FFN层中与值向量相关联的相关性分数，并利用这些分数动态调整子更新的贡献。通过优化子更新的贡献，LLMBRACES精炼了预测过程，从而产生更准确和可靠的输出，就像一个“支架”提供支持和稳定性。此外，LLMBRACES可以扩展以支持对生成特征的条件控制，例如情感，从而实现对LLM输出的精细引导。在包括Qwen2.5-1.5B、Llama2-7B和Llama3-8B在内的各种LLM上的大量实验表明，LLMBRACES在微调和零样本设置中均优于基线方法，同时需要显著更少的可调参数，与LoRA相比最多减少75%。此外，LLMBRACES在情感控制生成和毒性降低方面表现出色，突显了其在灵活、可控文本生成方面的应用潜力。|
|**2025-03-20**|**OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence**|Long Yuan et.al.|[2503.16326](http://arxiv.org/abs/2503.16326)|null|随着多模态大型语言模型（LLMs）的快速发展，人工智能领域迎来了新的前沿，使得文本、图像和空间信息等不同大规模数据类型的整合成为可能。在本文中，我们探讨了多模态LLMs（MLLM）在地理空间人工智能（GeoAI）领域的潜力，该领域利用空间数据解决包括地理语义、健康地理、城市地理、城市感知和遥感在内的多个领域的挑战。我们提出了一种针对地理空间应用定制的MLLM（OmniGeo），能够处理和分析包括卫星图像、地理空间元数据和文本描述在内的异构数据源。通过结合自然语言理解和空间推理的优势，我们的模型增强了指令遵循的能力和GeoAI系统的准确性。结果显示，我们的模型在多种地理空间任务上优于特定任务的模型和现有的LLMs，有效地处理了多模态特性，同时在零样本地理空间任务上取得了具有竞争力的成果。我们的代码将在发表后发布。|
|**2025-03-20**|**Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1**|Peiran Gu et.al.|[2503.16304](http://arxiv.org/abs/2503.16304)|null|近年来，大型语言模型（LLMs）在自然语言处理领域取得了显著突破，并逐渐应用于人文社会科学研究领域。LLMs由于其强大的文本理解、生成和推理能力，在人文社会科学领域具有广泛的应用价值。在人文社会科学研究中，LLMs能够分析大规模文本数据并作出推断。本文从七个方面分析了大型语言模型DeepSeek-R1：低资源语言翻译、教育问答、高等教育学生写作改进、逻辑推理、教育测量和心理测量、公共卫生政策分析、艺术教育。然后，我们将DeepSeek-R1在这七个方面的回答与o1-preview的回答进行比较。DeepSeek-R1在人文社会科学领域表现良好，回答大部分问题既正确又合乎逻辑，并能给出合理的分析过程和解释。与o1-preview相比，它能够自动生成推理过程并提供更详细的解释，适合初学者或需要详细了解该知识的人群，而o1-preview则更适合快速阅读。通过分析发现，LLMs在人文社会科学领域具有广泛的应用潜力，在提高文本分析效率、语言交流等领域展现出显著优势。LLMs强大的语言理解和生成能力使其能够深入探索人文社会科学领域的复杂问题，并为学术研究和实际应用提供创新工具。|
|**2025-03-20**|**Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens**|Shuqi Lu et.al.|[2503.16278](http://arxiv.org/abs/2503.16278)|**[link](https://github.com/dptech-corp/uni-3dar)**|近期，大型语言模型及其多模态扩展的进展展示了通过自回归的下一个标记预测统一生成和理解的有效性。然而，尽管3D结构生成和理解（3D GU）在AI科学领域的核心作用至关重要，这些任务在很大程度上是独立发展的，自回归方法尚未得到充分探索。为了弥合这一差距，我们引入了Uni-3DAR，这是一个通过自回归预测无缝集成3D GU任务的统一框架。在其核心，Uni-3DAR采用了一种新颖的分层标记化方法，通过八叉树压缩3D空间，利用3D结构的固有稀疏性。然后，它应用额外的标记化来捕捉细粒度结构细节，包括原子类型和微观3D结构中的精确空间坐标等关键属性。我们进一步提出了两种优化策略来提高效率和有效性。第一种是两级子树压缩策略，可以将八叉树标记序列减少多达8倍。第二种是为动态变化的标记位置定制的掩码下一个标记预测机制，显著提升了模型性能。通过结合这些策略，Uni-3DAR成功地在单个自回归框架内统一了多种3D GU任务。在包括分子、蛋白质、聚合物和晶体在内的多个微观3D GU任务上的广泛实验验证了其有效性和多功能性。值得注意的是，Uni-3DAR在性能上超过了之前的扩散模型，实现了高达256%的相对改进，同时推理速度提升了高达21.8倍。代码已在https://github.com/dptech-corp/Uni-3DAR上公开。|
|**2025-03-19**|**SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks**|Yifei Zhou et.al.|[2503.15478](http://arxiv.org/abs/2503.15478)|**[link](https://github.com/facebookresearch/sweet_rl)**|大型语言模型（LLM）代理需要在现实任务中进行多轮交互。然而，现有的用于优化LLM代理的多轮强化学习（RL）算法未能有效地在多轮中分配信用，同时利用LLM的泛化能力，并且如何开发此类算法尚不清楚。为了研究这一问题，我们首先引入了一个新的基准，ColBench，其中LLM代理与人类合作者进行多轮交互，以解决后端编程和前端设计中的现实任务。基于这个基准，我们提出了一种新的RL算法，SWEET-RL（使用训练时信息的逐步评估的RL），该算法使用精心设计的优化目标来训练一个能够访问额外训练时信息的评判模型。评判模型为改进策略模型提供步骤级别的奖励。我们的实验表明，与现有的其他最先进的多轮RL算法相比，SWEET-RL在ColBench上的成功率和胜率提高了6%，使Llama-3.1-8B能够在现实内容创作中匹配或超越GPT4-o的性能。|
|**2025-03-19**|**Cube: A Roblox View of 3D Intelligence**|Foundation AI Team et.al.|[2503.15475](http://arxiv.org/abs/2503.15475)|**[link](https://github.com/roblox/cube)**|基于大量数据训练的基础模型在文本、图像、音频和视频领域展示了卓越的推理和生成能力。在Roblox，我们的目标是构建这样一个用于3D智能的基础模型，该模型能够支持开发者从生成3D对象和场景，到为动画绑定角色，再到生产描述对象行为的程序化脚本等各个方面。我们讨论了构建此类3D基础模型的三个关键设计要求，然后介绍了我们构建此类模型的第一步。我们预计3D几何形状将成为核心数据类型，并描述了我们的3D形状分词器解决方案。我们展示了我们的分词方案如何应用于文本到形状生成、形状到文本生成和文本到场景生成等应用。我们演示了这些应用如何与现有的大型语言模型（LLMs）协作进行场景分析和推理。最后，我们讨论了构建一个完全统一的3D智能基础模型的路径。|
|**2025-03-19**|**From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment**|Jia-Nan Li et.al.|[2503.15463](http://arxiv.org/abs/2503.15463)|**[link](https://github.com/jinaleejnl/alignx)**|大型语言模型（LLMs）传统上通过一刀切的方法进行对齐，这些方法假设人类偏好是统一的，从根本上忽视了用户价值和需求的多样性。本文介绍了一个用于可扩展个性化对齐LLMs的全面框架。我们建立了一个系统性的偏好空间，它表征了心理和行为维度，并包含了多样的角色表示，以在现实场景中实现稳健的偏好推断。在此基础上，我们引入了\textsc{AlignX}，这是一个包含超过130万个性化偏好示例的大型数据集，并开发了两种互补的对齐方法：\textit{上下文对齐}，它直接根据角色表示进行条件化，以及\textit{偏好桥接对齐}，它模拟中间偏好分布。广泛的实验表明，与现有方法相比，平均准确率在四个基准测试中提高了17.06%，同时展现出对新颖偏好的强大适应能力、对有限用户数据的鲁棒性和精确的偏好控制能力。这些结果验证了我们框架的有效性，朝着真正用户自适应的AI系统迈进。|
|**2025-03-19**|**Visual Position Prompt for MLLM based Visual Grounding**|Wei Tang et.al.|[2503.15426](http://arxiv.org/abs/2503.15426)|**[link](https://github.com/waynetomas/vpp-llava)**|尽管多模态大型语言模型（MLLMs）在处理各种与图像相关的任务方面表现出色，但在精确地将在图像中与空间信息对齐的坐标方面，它们面临挑战，尤其是在如视觉定位等位置感知任务中。这一限制源于两个关键因素。首先，MLLMs缺乏明确的时空参照，使得将文本描述与精确的图像位置相关联变得困难。其次，它们的特征提取过程优先考虑全局上下文而不是精细的空间细节，导致定位能力较弱。为了解决这一问题，我们引入了VPP-LLaVA，这是一种配备了视觉位置提示（VPP）以提升其定位能力的MLLM。VPP-LLaVA集成了两种互补机制。全局VPP在输入图像上叠加可学习的、类似轴的嵌入，以提供结构化的空间提示。局部VPP通过引入位置感知查询，专注于精细定位，建议可能的对象位置。我们还引入了一个包含0.6M样本的VPP-SFT数据集，将高质量的视觉定位数据整合为紧凑的格式，以高效地进行模型训练。在这个数据集上使用VPP进行训练增强了模型的表现，尽管与依赖更大数据集（ $\sim$ 21M样本）的其他MLLMs如MiniGPT-v2相比，训练样本较少，但仍达到了标准的视觉定位基准的最新成果。代码和VPP-SFT数据集将在接受后于https://github.com/WayneTomas/VPP-LLaVA上可用。|
|**2025-03-19**|**Probing the topology of the space of tokens with structured prompts**|Michael Robinson et.al.|[2503.15421](http://arxiv.org/abs/2503.15421)|null|本文提出了一种通用的灵活方法，用于提示大型语言模型（LLM）揭示其（隐藏的）词元输入嵌入直到同胚。此外，本文为这种方法提供了强有力的理论依据——针对通用LLM的数学证明，说明了为什么预期这种方法应该有效。掌握这种方法后，我们通过恢复Llemma-7B的词元子空间来证明其有效性。本文的结果不仅适用于LLM，也适用于一般的非线性自回归过程。|
|**2025-03-19**|**EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models**|Yinan Liang et.al.|[2503.15369](http://arxiv.org/abs/2503.15369)|null|尽管多模态大型语言模型在复杂推理任务中表现出强大的性能，但它们在部署过程中面临与模型复杂性相关的重大挑战，尤其是在资源受限的设备上。在本文中，我们提出了一种用于大型视觉语言模型的自适应剪枝方法，以提高多模态推理的效率。传统方法依赖于原始模型的训练数据来选择不同网络组件的合适剪枝比例。然而，由于大规模训练语料库导致的不可承受的搜索成本，这些方法对大型视觉语言模型来说并不实用。相比之下，我们的方法仅利用少量样本通过最大化其在未知训练数据上的泛化能力来搜索所需的剪枝策略，同时保持模型精度，这使大型视觉语言模型在精度和效率之间达到最佳平衡。具体来说，我们使用结构风险最小化原则来公式化剪枝策略的泛化差距。基于任务性能和泛化能力，我们在给定的搜索空间内迭代搜索最优剪枝策略，并优化视觉投影器以通过提高性能上限来演化搜索空间。我们针对视觉问答任务在ScienceQA、Vizwiz、MM-vet和LLaVA-Bench数据集上进行了广泛实验。仅使用64个样本进行剪枝策略搜索，EfficientLLaVA在ScienceQA上实现了83.05%的精度，与密集型LLaVA-v1.5-7B模型相比，速度提升了1.8倍。|
|**2025-03-19**|**SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation**|Thomas Pickard et.al.|[2503.15358](http://arxiv.org/abs/2503.15358)|null|在自然语言处理（NLP）中，惯用语表达提出了独特的挑战，因为它们的含义通常不能直接从其构成词中推断出来。尽管近年来大型语言模型（LLMs）取得了进展，但习语性仍然是稳健语义表示的一个重大障碍。我们为SemEval-2025任务1：AdMiRe（推进多模态习语表示）提供了数据集和任务，该任务挑战社区评估和改进模型在多模态环境和多种语言中解释惯用语表达的能力。参与者参加了两个子任务：根据图像与惯用语或字面意义的一致性进行排名，以及预测序列中的下一张图像。最有效的方法通过在专家混合设置中利用预训练的LLMs和视觉语言模型，实现了人类水平的表现，并通过多次查询来平滑这些模型在习语性表示中的弱点。|
|**2025-03-19**|**SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models**|I-Fan Lin et.al.|[2503.15351](http://arxiv.org/abs/2503.15351)|null|本文提出了一种名为“基于大型语言模型的选取与池化（SPILL）”的直观且领域自适应的意图聚类方法，该方法无需微调。现有的基于嵌入的聚类方法依赖于少量标记示例或无监督微调来优化每个新数据集的结果，这使得它们在多个数据集上的泛化能力较差。我们的目标是使这些现有的嵌入器在不进一步微调的情况下，对新的领域数据集具有更强的泛化能力。受我们关于采样和池化技术有效性的理论推导和模拟结果的启发，我们将聚类任务视为一个小规模的选取问题。解决这个问题的良好方案与更好的聚类性能相关联。因此，我们提出了一种两阶段方法：首先，对于每个话语（称为种子），我们使用现有嵌入器推导其嵌入。然后，我们应用距离度量来选取与种子相近的一组候选者。由于嵌入器未针对新数据集进行优化，在第二阶段，我们使用大型语言模型（LLM）进一步选取与种子具有相同意图的候选话语。最后，我们将这些选定的候选者与种子池化，以推导出对种子更精细的嵌入。我们发现，我们的方法通常优于直接使用嵌入器，并且与其他最先进的学术研究相比，即使那些使用更大模型并需要微调的研究，也能达到可比的结果，这展示了其强度和效率。我们的结果表明，我们的方法使现有嵌入器能够在不进行额外微调的情况下得到进一步改进，使其更能适应新的领域数据集。此外，将聚类任务视为一个小规模的选取问题，为使用LLM根据用户的任务目标定制聚类任务提供了潜在的可能性。|
|**2025-03-19**|**TruthLens:A Training-Free Paradigm for DeepFake Detection**|Ritabrata Chakraborty et.al.|[2503.15342](http://arxiv.org/abs/2503.15342)|null|随着高级AI模型生成合成图像的增多，识别和理解被操纵的视觉内容面临重大挑战。当前虚假图像检测方法主要依赖二元分类模型，这些模型侧重于准确性，但往往忽视可解释性，导致用户无法清楚地了解为什么一张图片被判定为真实或虚假。为了填补这一空白，我们引入了TruthLens，这是一个创新的无需训练的框架，它将深度伪造检测重新构想为视觉问答（VQA）任务。TruthLens利用最先进的超大视觉语言模型（LVLMs）来观察和描述视觉伪迹，并结合大型语言模型（LLMs）如GPT-4的推理能力，分析和汇总证据以做出明智的决策。通过采用多模态方法，TruthLens无缝集成了视觉和语义推理，不仅能够将图像分类为真实或虚假，还能为其决策提供可解释的解释。这种透明度增强了信任，并为识别合成内容的伪迹提供了宝贵见解。广泛的评估表明，TruthLens优于传统方法，在具有挑战性的数据集上实现了高精度，同时保持了对可解释性的强烈重视。通过将深度伪造检测重新构造成一个以推理驱动的过程，TruthLens在对抗合成媒体方面建立了一个新范式，将尖端性能与可解释性相结合，以应对视觉虚假信息的日益增长威胁。|
|**2025-03-19**|**Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs**|Yuqi Zhu et.al.|[2503.15341](http://arxiv.org/abs/2503.15341)|null|思维链（CoT）推理已被证明是提高大型语言模型（LLMs）在代码生成场景下问题解决能力的一种有效技术。然而，现有的CoT方法往往存在“过度思考”的倾向，即LLM持续应用推理策略，而没有充分考虑到任务的潜在复杂性。这导致LLM在相对简单或正确答案显而易见的问题上分配了过多的计算资源（即token）。此外，这种过度思考可能导致LLM陷入错误的推理路径，从而导致代码生成错误。在本文中，我们引入了不确定性感知思维链（UnCert-CoT），这是一种基于LLM的方法，旨在通过结合不确定性感知的CoT推理机制来增强代码生成，该机制专注于LLM更容易出错的地方。我们提出了两种基于置信度的不确定性度量方法：基于熵和基于概率差异的方法。当不确定性高时，UnCert-CoT激活CoT解码来生成多个推理路径，并选择最终展示最高正确概率的代码。相反，当不确定性低时，LLM直接生成代码。这种不确定性判断机制允许LLM优先处理复杂任务，并在简单情况下避免不必要的步骤，从而提高代码生成的整体效率和准确性。我们的实验结果表明，UnCert-CoT在具有挑战性的基准MHPP（主要是硬Python问题）上显著提高了代码生成精度，它在PassRate精度上实现了高达6.1%的提升，尤其是在传统LLM容易出错的情况下。|
|**2025-03-18**|**Aligning Multimodal LLM with Human Preference: A Survey**|Tao Yu et.al.|[2503.14504](http://arxiv.org/abs/2503.14504)|**[link](https://github.com/bradyfu/awesome-multimodal-large-language-models)**|大型语言模型（LLMs）可以通过简单的提示处理各种通用任务，无需针对特定任务进行训练。基于LLMs的多模态大型语言模型（MLLMs）在处理涉及视觉、听觉和文本数据的复杂任务方面展现出惊人的潜力。然而，与真实性、安全性、O1-like推理以及与人类偏好的对齐等关键问题仍没有得到充分解决。这一差距促使各种对齐算法的出现，每种算法都针对不同的应用场景和优化目标。最近的研究表明，对齐算法是解决上述挑战的有力方法。在本文中，我们旨在对MLLMs的对齐算法进行全面和系统的综述。具体来说，我们探讨了四个关键方面：（1）对齐算法涵盖的应用场景，包括通用图像理解、多图像、视频和音频，以及扩展的多模态应用；（2）构建对齐数据集的核心因素，包括数据来源、模型响应和偏好标注；（3）用于评估对齐算法的基准；（4）对对齐算法未来发展方向的可能讨论。本研究旨在帮助研究人员组织该领域的当前进展并激发更好的对齐方法。本文的项目页面可在https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment找到。|
|**2025-03-18**|**Engineering Scientific Assistants using Interactive Structured Induction of Programs**|Shraddha Surana et.al.|[2503.14488](http://arxiv.org/abs/2503.14488)|null|我们感兴趣的是构建能够作为领域专家科学助手的软件。预计这类助手将有助于加速识别解决需要紧急解决的复杂问题的方法。在本文中，我们的焦点不在于特定的科学问题，而是关注此类“科学加速器”的软件工程。近年来，“无代码”技术的发展似乎表明，科学家可以通过与大型语言模型（LLM）的对话简单地提出解决方案。然而，考虑到当前LLM技术的状况，对于复杂科学问题，这似乎不太可能。看起来可行的是，软件工程师可以使用LLM快速构建供领域专家使用的程序，包括用自然语言表达的专家需求。我们提出了一个交互式的“结构化”归纳编程设计，其中软件工程师和LLM合作构建一个用于科学数据分析的“助手”。本文描述了一个简单的实现，称为iStrucInd，它采用“双向可理解性”协议来实施软件工程师与LLM之间的交互。我们在这两个不同的非平凡科学数据分析任务上测试了该工具。具体来说，我们将iStrucInd构建的系统与手动构建和低代码/无代码方法构建的系统在以下方面进行了比较：（a）程序性能；（b）程序质量；（c）编程工作量。结果表明，iStrucInd允许软件工程师更快地开发出更好的程序，表明交互式结构化归纳在快速构建科学助手方面可以发挥有用作用。|
|**2025-03-18**|**Gricean Norms as a Basis for Effective Collaboration**|Fardin Saad et.al.|[2503.14484](http://arxiv.org/abs/2503.14484)|**[link](https://github.com/fardinsaad/gricean-norms)**|有效的人机协作不仅取决于AI代理执行明确指令的能力，还取决于其在沟通中处理模糊性、不完整性、无效性和不相关性的能力。格赖斯对话和推理规范通过将不明确的指令与合作关系原则对齐来促进协作。我们提出了一种规范性框架，将格赖斯规范和认知框架（共同点、关联理论和心理理论）集成到基于大型语言模型（LLM）的代理中。规范性框架采用了格赖斯的数量、质量、关系和方式等格赖斯规范，以及推理，作为解释模糊指令（模糊、不完整、无效或不相关）的规范。在这个框架中，我们引入了Lamoids，这是一种由GPT-4驱动的代理，旨在与人类协作。为了评估格赖斯规范对人类-AI协作的影响，我们评估了两种版本的Lamoid：一种有规范，另一种没有。在我们的实验中，一个Lamoid通过与人类合作，在网格世界（门、钥匙和宝石）中实现共享目标，通过解释明确和不明确的自然语言指令。我们的结果表明，具有格赖斯规范的Lamoid在任务准确度上更高，并生成更清晰、更准确和更具情境相关性的回应，而没有规范的Lamoid则不具备这些特点。这种改进源于规范性框架，它增强了代理的实用推理能力，促进了有效的人机协作，并使基于LLM的代理能够实现情境感知的通信。|
|**2025-03-18**|**Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM**|Xinyu Fang et.al.|[2503.14478](http://arxiv.org/abs/2503.14478)|**[link](https://github.com/open-compass/creation-mmbench)**|创造力是智能的一个基本方面，涉及在不同情境下生成新颖和合适的解决方案的能力。尽管大型语言模型（LLMs）在创造性能力方面已被广泛评估，但在这一领域对多模态大型语言模型（MLLMs）的评估仍然大多未涉足。为了填补这一空白，我们引入了Creation-MMBench，这是一个专门为评估MLLMs在现实世界、基于图像的任务中的创造性能力而设计的多模态基准。该基准包含765个测试案例，涵盖了51个细粒度任务。为确保严格评估，我们为每个测试案例定义了特定实例的评估标准，指导了对一般响应质量和与视觉输入的事实一致性的评估。实验结果表明，与专有模型相比，当前的开源MLLMs在创造性任务中表现显著不佳。此外，我们的分析表明，视觉微调可能会对基础LLMs的创造性能力产生负面影响。Creation-MMBench为提高MLLM的创造力提供了宝贵的见解，并为多模态生成智能未来的改进奠定了基础。完整数据和评估代码已发布在https://github.com/open-compass/Creation-MMBench。|
|**2025-03-18**|**EnvBench: A Benchmark for Automated Environment Setup**|Aleksandra Eliseeva et.al.|[2503.14443](http://arxiv.org/abs/2503.14443)|**[link](https://github.com/JetBrains-Research/EnvBench)**|**近年来，大型语言模型（LLMs）的进步使研究人员能够专注于软件工程领域的实际仓库级任务。在这项工作中，我们考虑了自动化软件仓库工作的一项基石任务——环境设置，即在一个系统上配置特定于仓库的开发环境。现有的关于环境设置的研究引入了创新的代理策略，但它们的评估通常基于小数据集，这些数据集可能无法捕捉到实践中遇到的全部配置挑战。为了解决这一差距，我们引入了一个全面的环境设置基准EnvBench。它包含了329个Python和665个基于JVM（Java、Kotlin）的仓库，重点关注那些真正具有配置挑战的仓库，排除了可以通过简单的确定性脚本完全配置的项目。为了使基准进一步扩展并用于模型调整，我们实现了两个自动指标：Python中缺失导入的静态分析检查和JVM语言的编译检查。我们通过评估三种环境设置方法，包括一个简单的零样本基线和两种代理工作流程，并使用两个强大的LLM主干GPT-4o和GPT-4o-mini进行测试，证明了我们基准的适用性。最佳方法成功配置了6.69%的Python仓库和29.47%的JVM仓库，这表明EnvBench对当前方法来说仍然具有挑战性。我们的基准套件可在https://github.com/JetBrains-Research/EnvBench上公开获取。数据集和实验轨迹可在https://jb.gg/envbench上获取。**|
|**2025-03-18**|**LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers**|Nikhil Abhyankar et.al.|[2503.14434](http://arxiv.org/abs/2503.14434)|**[link](https://github.com/nikhilsab/llmfe)**|自动化特征工程在提高表格学习任务的预测模型性能中起着关键作用。传统的自动化特征工程方法受限于其在固定、手动设计的搜索空间内依赖预定义的转换，通常忽略了领域知识。最近利用大型语言模型（LLMs）的进展使得将领域知识整合到特征工程过程中成为可能。然而，现有的基于LLMs的方法使用直接提示或仅依赖于验证分数进行特征选择，未能利用先前特征发现实验的见解或建立特征生成与数据驱动性能之间的有意义推理。为了解决这些挑战，我们提出了LLM-FE，一个结合了进化搜索和LLMs的领域知识和推理能力的创新框架，以自动发现表格学习任务的有效特征。LLM-FE将特征工程表述为程序搜索问题，其中LLMs迭代地提出新的特征转换程序，数据驱动的反馈指导搜索过程。我们的结果表明，LLM-FE在多个分类和回归基准测试中，始终优于最先进的基线，显著提升了表格预测模型的表现。|
|**2025-03-18**|**PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play**|Wei Fang et.al.|[2503.14432](http://arxiv.org/abs/2503.14432)|null|大型语言模型（LLMs）越来越多地与专用外部工具集成，然而许多任务要求在最小或含糊不清的文档下进行零样本工具使用。现有的解决方案依赖于手动重写或标记数据进行验证，这使得它们在真正的零样本设置中不适用。为了解决这些挑战，我们提出了PLAY2PROMPT，这是一个自动框架，它系统地“玩”每个工具以探索其输入输出行为。通过这种迭代试错过程，PLAY2PROMPT优化工具文档并生成使用示例，而无需任何标记数据。这些示例不仅指导LLM推理，还作为验证进一步促进工具利用。在真实世界任务上的大量实验表明，PLAY2PROMPT显著提高了开放和封闭模型在零样本工具性能方面的表现，为特定领域的工具集成提供了一个可扩展和有效的解决方案。|
|**2025-03-18**|**Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models**|Siwei Zhang et.al.|[2503.14411](http://arxiv.org/abs/2503.14411)|null|时间图神经网络（TGNNs）在时间图建模方面表现出色。然而，现实世界中的时间图通常具有丰富的文本信息，从而产生了时间文本属性图（TTAGs）。这种动态文本语义和演变图结构的结合引入了更高的复杂性。现有的TGNNs静态地嵌入文本，并且过度依赖优先考虑结构信息的编码机制，忽略了文本语义的时间演变以及语义与结构之间的重要相互作用以实现协同强化。为了解决这些问题，我们提出了一个名为\textbf{{Cross}}的新框架，该框架无缝扩展了现有的TGNNs以进行TTAG建模。关键思想是使用高级大型语言模型（LLMs）从文本空间中提取动态语义，然后生成统一语义和结构的表达性表示。具体来说，我们在{Cross}框架中提出了一种时间语义提取器，它使LLM能够提供节点文本邻域演变上下文的时间语义理解，从而促进语义动态性。随后，我们引入了语义-结构共编码器，它与合作的上面的提取器协同工作，通过同时考虑语义和结构信息并鼓励它们相互强化来合成启发性的表示。在四个公共数据集和一个实际工业数据集上的大量实验结果表明{Cross}具有显著的有效性和鲁棒性。|
|**2025-03-18**|**Large Language Models for Virtual Human Gesture Selection**|Parisa Ghanad Torshizi et.al.|[2503.14408](http://arxiv.org/abs/2503.14408)|null|共言手势传达了丰富的意义，在面对面的人际互动中扮演着重要角色。这些手势显著影响受话者的参与度、回忆、理解以及对说话者的态度。同样，它们也会影响人类与具身虚拟代理之间的互动。因此，选择和动画化有意义的共言手势已经成为这些代理设计的关键焦点。然而，自动化这个手势选择过程面临着重大挑战。之前的手势生成技术从完全自动、数据驱动的方法（这些方法往往难以产生语境相关的手势）到更手动的方法（这些方法需要制作特定的手势专业知识，耗时且缺乏通用性）各不相同。在本文中，我们利用大型语言模型的语义能力，开发了一种建议有意义的、适当的共言手势的手势选择方法。我们首先描述了如何将手势信息编码到GPT-4中。然后，我们进行了一项研究，以评估不同的提示方法在选取有意义的、语境相关的手势以及适当地将它们与共言话语对齐的能力。最后，我们详细介绍了如何在虚拟代理系统中实现这种方法，自动化选择所选手势的动画，以增强人机交互。|
|**2025-03-18**|**From "Hallucination" to "Suture": Insights from Language Philosophy to Enhance Large Language Models**|Qiantong Wang et.al.|[2503.14392](http://arxiv.org/abs/2503.14392)|null|本文通过语言哲学和心理分析的角度探讨大型语言模型（LLMs）中的幻觉现象。通过引入拉康的“符号链”和“缝合点”概念，我们提出了锚定-RAG框架作为减轻幻觉的一种新方法。与主要依赖于试错实验、不断调整数学公式或强调数量胜过质量的资源密集型方法不同，我们的方法回归语言学的基本原则，分析LLMs中幻觉的根本原因。基于稳健的理论基础，我们推导出不仅能够有效减少幻觉，还能提升LLMs性能和改善输出质量的算法和模型。本文旨在建立一个全面的理论框架来理解LLMs中的幻觉，并旨在挑战该领域普遍存在的“猜测与测试”方法和竞赛心态。我们期望为可解释性LLMs的新时代铺平道路，为基于语言的人工智能系统的内部运作提供更深入的见解。|
|**2025-03-17**|**MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**|Qin Liu et.al.|[2503.13447](http://arxiv.org/abs/2503.13447)|null|大型语言模型在进行复杂推理时面临的一个关键挑战是，它们依赖于从训练数据中匹配推理模式，而不是主动选择最适合解决给定任务的认知策略。现有方法强加了固定的认知结构，这些结构在特定任务中提高了性能，但缺乏在不同场景中的适应性。为了解决这一局限性，我们引入了METASCALE，这是一个基于元思考的测试时缩放框架——针对每个任务定制的自适应思考策略。METASCALE初始化一个候选元思考池，然后使用带有上置信界选择的多元臂老虎机算法迭代选择和评估它们，这些选择由奖励模型引导。为了进一步增强适应性，遗传算法进化高奖励的元思考，随着时间的推移完善和扩展策略池。通过在推理时间动态提出和优化元思考，METASCALE在广泛的任务中提高了准确性和泛化能力。实验结果表明，MetaScale在性能上始终优于标准推理方法，在Arena-Hard上的胜率提高了11%，在风格控制下超过了o1-mini 0.9%。值得注意的是，随着采样预算的增加，METASCALE的扩展效果更佳，并产生更多结构化、专家级别的响应。|
|**2025-03-17**|**Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance**|Noah Y. Siegel et.al.|[2503.13445](http://arxiv.org/abs/2503.13445)|null|随着大型语言模型（LLMs）能力的不断增强，确保它们自我生成的解释与其内部决策过程的一致性对于安全性和监管至关重要。在这项工作中，我们对来自8个家族的62个模型进行了全面的反事实一致性分析，包括预训练和指令调整变体，并显著扩展了先前关于反事实测试的研究。我们引入了phi-CCT，这是相关反事实测试的简化版本，它避免了需要标记概率，同时解释了原始测试的大部分方差。我们的发现揭示了明显的规模趋势：更大的模型在我们的指标上始终更加一致。然而，当比较指令调整和模仿人类的解释时，我们发现观察到的忠诚度差异往往可以归因于解释的冗长，导致沿着真正阳性/假阳性帕累托前沿的移动。虽然指令调整和提示可以影响这种权衡，但我们发现有限证据表明它们从根本上扩大了解释忠诚度的前沿，超出了与相当规模的预训练模型所能达到的范围。我们的分析突出了指令调整、冗长和模型决策过程忠实表征之间复杂的关系。|
|**2025-03-17**|**VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning**|Ye Liu et.al.|[2503.13444](http://arxiv.org/abs/2503.13444)|**[link](https://github.com/yeliudev/VideoMind)**|视频具有独特的时序维度，需要精确的基于证据的理解，其中答案直接与可视、可解释的证据相联系。尽管在大型语言模型的推理能力方面取得了重大突破，但多模态推理——尤其是针对视频——仍处于未开发状态。在这项工作中，我们引入了VideoMind，这是一种新的视频-语言智能体，旨在进行基于时序的视频理解。VideoMind包含两个关键创新：(i) 我们确定了视频时序推理的关键能力，并开发了一种基于角色的智能体工作流程，包括协调不同角色的规划器、进行时序定位的 grounding、评估时序间隔准确性的验证器和用于问答的回答者。(ii) 为了有效地整合这些不同的角色，我们提出了一个新颖的LoRA链策略，通过轻量级的LoRA适配器实现无缝的角色切换，同时避免了多个模型的额外开销，从而在效率与灵活性之间取得平衡。在14个公开基准上的大量实验表明，我们的智能体在多种视频理解任务上实现了最先进的性能，包括3个基于证据的视频问答、6个视频时序 grounding 和5个通用视频问答，凸显了其在推动视频智能体和长时序推理方面的有效性。|
|**2025-03-17**|**xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference**|Maximilian Beck et.al.|[2503.13427](http://arxiv.org/abs/2503.13427)|**[link](https://github.com/nx-ai/xlstm-jax)**|近期，通过在推理时投入大量的计算资源，在解决推理、数学和编码问题方面取得了突破性进展。因此，推理速度是大型语言模型（LLM）架构最关键的特性之一，对于高效推理的LLM的需求日益增长。最近，基于xLSTM架构的LLM已作为一种强大的Transformer替代品出现，它提供了与序列长度成线性增长的计算扩展和恒定的内存使用，这两者都是高效推理所高度期望的特性。然而，这种基于xLSTM的LLM尚未扩展到更大规模，并且还没有在推理速度和效率方面进行评估和比较。在本工作中，我们介绍了xLSTM 7B，这是一个7亿参数的LLM，它结合了xLSTM的架构优势以及针对快速和高效推理的定向优化。我们的实验表明，xLSTM 7B在下游任务上的性能与其他类似规模的LLM相当，同时与基于Llama和Mamba的LLM相比，提供了显著更快的推理速度和更高的效率。这些结果确立了xLSTM 7B作为最快和最高效的7B LLM的地位，为需要大量测试时计算的任务提供了解决方案。我们的工作突出了xLSTM作为基于LLM推理重用方法的基石架构的潜力。我们的模型权重、模型代码和训练代码是开源的。|
|**2025-03-17**|**A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives**|Weiqiang Jin et.al.|[2503.13415](http://arxiv.org/abs/2503.13415)|null|随着人工智能的快速发展，智能决策技术在各种人机竞赛中逐渐超越了人类水平，尤其是在复杂的多智能体协同任务场景中。多智能体协同决策涉及多个智能体共同工作以完成既定任务并实现特定目标。这些技术在现实世界的场景中应用广泛，如自动驾驶、无人机导航、灾害救援和模拟军事对抗。本文首先对多智能体协同决策领域使用的领先仿真环境和平台进行了全面综述。具体来说，我们从任务格式、奖励分配和所采用的技术等不同角度对这些仿真环境进行了深入分析。随后，本文对主流的多智能体系统（MAS）智能决策方法、算法和模型进行了全面概述。这些方法可以大致分为五种类型：基于规则的（主要是模糊逻辑）、基于博弈论的、基于进化算法的、基于深度多智能体强化学习（MARL）的以及基于大型语言模型（LLMs）推理的。鉴于MARL和LLMs基于的决策方法在传统规则、博弈论和进化算法方面具有显著优势，本文重点讨论了这些利用MARL和LLMs技术的多智能体方法。我们深入讨论了这些方法，强调了它们的方法分类、优势和缺点。此外，还详细介绍了未来几个突出的研究方向和多智能体协同决策的潜在挑战。|
|**2025-03-17**|**DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective**|Dengyun Peng et.al.|[2503.13413](http://arxiv.org/abs/2503.13413)|**[link](https://github.com/sfasfaffa/dlpo)**|大型语言模型（LLMs）在众多任务中取得了显著的成功，这主要得益于精心设计的提示。然而，设计和选择这些提示通常需要大量的人力，这极大地限制了其可扩展性。为了缓解这一问题，最近的研究探索了自动提示优化作为一种有前景的解决方案。尽管有这些努力，现有方法在鲁棒性、效率和泛化能力方面仍面临关键挑战。为了系统地解决这些挑战，我们首先进行实证分析，以识别当前基于反思的提示优化范式局限性。基于这些洞察，我们提出了7种受传统深度学习范式启发的创新方法，将这些概念无缝地集成到基于文本的梯度优化中。通过这些进步，我们逐步解决上述挑战，并通过广泛的实验验证了我们的方法。我们希望我们的研究不仅为未来的研究提供宝贵的指导，而且还能全面理解提示优化中的挑战和潜在解决方案。我们的代码可在https://github.com/sfasfaffa/DLPO上获取。|
|**2025-03-17**|**Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis**|Alexander Ku et.al.|[2503.13401](http://arxiv.org/abs/2503.13401)|null|现代人工智能系统，如大型语言模型，其能力日益增强，但同时也越来越难以理解。我们将这一问题比作历史上理解人类心智的困难，认为认知科学中发展出的方法对于理解大型语言模型是有用的。我们基于Marr的三层次分析框架提出了应用这些方法的方法。通过回顾与每个层次相关的已建立认知科学技术，并展示它们在揭示大型语言模型行为和内部组织方面的潜力，我们旨在为理解这些新型心智提供一套工具。|
|**2025-03-17**|**MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research**|James Burgess et.al.|[2503.13399](http://arxiv.org/abs/2503.13399)|**[link](https://github.com/jmhb0/microvqa)**|科学研究需要对多模态数据进行复杂推理，这在生物学领域尤为突出。尽管近年来在人工智能辅助研究的多模态大型语言模型（MLLMs）方面取得了进展，但现有的多模态推理基准仅针对大学水平的难度，而研究级基准则侧重于较低级别的感知，未能满足科学发现所需的复杂多模态推理。为了弥合这一差距，我们引入了MicroVQA，这是一个视觉问答（VQA）基准，旨在评估研究工作流程中至关重要的三种推理能力：专家图像理解、假设生成和实验提案。MicroVQA包含由生物学专家根据不同的显微镜模态精心挑选的1,042道多项选择题（MCQs），确保VQA样本代表真实的科学实践。在构建基准的过程中，我们发现标准的MCQ生成方法会导致语言捷径，这促使我们开发了一种新的两阶段流程：一个优化的LLM提示结构将问题-答案对组织成MCQs；然后，基于代理的“RefineBot”对它们进行更新以去除捷径。在最新的MLLMs上进行基准测试显示，最佳性能为53%；使用较小LLM的模型仅略低于顶级模型，这表明基于语言的推理比多模态推理更具挑战性；而使用科学文章进行调优可以提升性能。对思维链响应的专家分析显示，感知错误最为常见，其次是知识错误，然后是过度泛化错误。这些洞察突显了多模态科学推理的挑战，表明MicroVQA是推动人工智能驱动的生物医学研究的有价值资源。MicroVQA可在https://huggingface.co/datasets/jmhb/microvqa上获取，项目页面在https://jmhb0.github.io/microvqa。|
|**2025-03-17**|**Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning**|Mengyao Lyu et.al.|[2503.13383](http://arxiv.org/abs/2503.13383)|null|最近在数据整理和选择研究方面的进展证实了预训练大型语言模型（LLMs）在微调（SFT）阶段只需要最小监督的假设（周等，2024年）。然而，由于实验设置和验证协议的脆弱性，它们的稳定性和泛化能力受损，未能超越随机抽样（迪迪德和伊波利托，2024年；夏等，2024b）。建立在LLMs基础上的多模态LLMs（MLLMs），结合数据源的大量标记和高度异质性，放大了数据选择的重要性和复杂性。为了以稳健和高效的方式收集多模态教学数据，我们重新定义了质量指标的粒度，将其分解为14个视觉-语言相关能力，并引入多模态丰富评分器来评估每个数据候选者的能力。为了促进多样性，考虑到对齐阶段固有的目标，我们将交互风格作为多样性指标，并使用多模态丰富风格器来识别数据指令模式。通过这种方式，我们的多模态丰富评分器和风格器（mmSSR）确保高分信息以多样化的形式传达给用户。mmSSR不受基于嵌入的聚类或贪婪采样的限制，能够高效地扩展到数百万具有不同预算约束的数据，支持通用或特定能力获取的定制，并促进对新的领域进行整理的无监督泛化。在10多个实验设置中，由14个多模态基准验证，我们展示了相对于随机抽样、基线策略和最先进的筛选方法的一致性改进，仅使用2.6M数据中的30%，就实现了99.1%的完整性能。|
|**2025-03-17**|**Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning**|Hai-Long Sun et.al.|[2503.13360](http://arxiv.org/abs/2503.13360)|null|最近大型语言模型（LLMs）在推理能力方面的进步显著，从思维链（CoT）提示发展到像OpenAI o1这样的高级、以产品为导向的解决方案。在我们重新实现这个模型的过程中，我们注意到在需要视觉输入的多模态任务（例如几何问题）中，多模态LLMs（MLLMs）难以保持对视觉信息的关注，换句话说，MLLMs在推理过程中逐渐降低对视觉信息的注意力，导致过度依赖文本的输出。为了调查这一问题，我们在长链推理期间移除了图像输入。具体来说，我们在推理过程的中途截断，然后移除输入图像后重新完成推理过程。我们发现MathVista测试集的“测试难”子集上的准确率仅下降了约2%，这表明模型的文本输出主导了后续的推理过程。受此启发，我们提出了携带视觉条件化（TVC）策略，该策略将图像输入移至关键推理阶段，并通过动态剪枝压缩冗余的视觉标记。这种方法帮助模型在整个推理过程中保持对视觉组件的注意力。我们的方法在五个数学推理基准的平均性能上达到了最先进水平（比之前的sota高3.4%），证明了TVC在增强多模态推理系统中的有效性。|
|**2025-03-14**|**ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning**|Xinyi Wang et.al.|[2503.11617](http://arxiv.org/abs/2503.11617)|**[link](https://github.com/wxy3596/asma-tune)**|在逆向工程等众多应用中，汇编代码的分析和理解至关重要。然而，汇编代码信息密度低、缺乏明确的语法结构，给分析带来了重大挑战。基于掩码语言模型（MLM）的先驱方法由于促进了自然语言交互而受到限制。尽管基于解码器聚焦的大型语言模型（LLMs）的近期方法显著提高了语义表示，但它们仍然难以捕捉汇编代码中细微且稀疏的语义。在本文中，我们提出了汇编增强调整（ASMA-Tune），一个端到端的结构-语义指令调整框架。我们的方法通过投影模块将编码器架构与基于解码器的LLMs协同，以实现全面的代码理解。实验表明，ASMA-Tune优于现有基准，显著提高了汇编代码的理解能力和指令跟随能力。我们的模型和数据集可在https://github.com/wxy3596/ASMA-Tune上公开获取。|
|**2025-03-14**|**Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space**|Zhiliang Chen et.al.|[2503.11586](http://arxiv.org/abs/2503.11586)|**[link](https://github.com/chenzhiliang94/convo-plan-scope)**|大型语言模型（LLMs）被用于聊天机器人或AI助手与人类用户进行对话。在这样的应用中，对话的质量（例如，用户参与度、安全性）非常重要，只能在对话结束时才能确切知晓。为了最大化其预期质量，对话规划会根据对话中的随机转换来选择每个回合的最佳LLM响应。现有的基于模拟的对话规划算法通常在每个回合通过使用大量LLM查询来模拟未来的对话来选择最佳响应。然而，这个过程非常耗时，因此对于实时对话来说并不实用。本文提出了一种名为“语义空间高效对话规划”（SCOPE）的新方法，它利用对话的密集语义表示来高效地进行对话规划。具体来说，SCOPE对对话语义中的随机转换及其关联的奖励进行建模，以便在语义空间内进行完全规划。这使得我们能够在每个对话回合中选择最佳LLM响应，而无需进行额外的LLM查询进行模拟。因此，当应用于各种对话开头和现实世界中看到的两个奖励函数时，SCOPE的对话规划速度比传统的基于模拟的规划算法快70倍，同时在实用的规划预算内实现更高的奖励。我们的代码可以在以下网址找到：https://github.com/chenzhiliang94/convo-plan-SCOPE。|
|**2025-03-14**|**Synthesizing Access Control Policies using Large Language Models**|Adarsh Vatsa et.al.|[2503.11573](http://arxiv.org/abs/2503.11573)|null|云计算系统允许管理员编写访问控制策略来管理对私有数据的访问。虽然策略可以用方便的语言编写，如AWS身份和访问管理策略语言，但手动编写的策略往往变得复杂且容易出错。在本文中，我们研究是否以及如何有效地使用大型语言模型（LLMs）来合成访问控制策略。我们的研究集中在将访问控制请求规范与零样本提示LLMs相结合，以合成一个符合请求规范的、格式良好的访问控制策略的任务上。我们考虑了两种场景：一种是将请求规范作为允许或拒绝的具体请求列表给出，另一种是使用自然语言描述来指定允许或拒绝的请求集合。然后我们论证，对于零样本提示，使用基于语法的更精确和结构化的提示是必要的，并通过实验展示了验证我们方法初步结果。|
|**2025-03-14**|**Implicit Bias-Like Patterns in Reasoning Models**|Messi H. J. Lee et.al.|[2503.11572](http://arxiv.org/abs/2503.11572)|null|隐式偏见指的是自动或自发的心理过程，这些过程塑造了人们的感知、判断和行为。先前研究在大型语言模型（LLMs）中探讨“隐式偏见”时，往往与人类研究的方法不同，主要关注模型输出而不是模型处理过程。为了考察模型处理过程，我们提出了一种名为推理模型内隐联想测试（RM-IAT）的方法，用于研究推理模型中的隐式偏见类似模式：采用逐步推理来解决复杂任务的LLMs。使用这种方法，我们发现推理模型在处理关联不兼容信息时所需的标记比处理关联兼容信息时更多。这些发现表明，AI系统在处理信息方面存在与人类隐式偏见类似的模式。我们考虑了这些隐式偏见类似模式对它们在现实世界应用中的部署的影响。|
|**2025-03-14**|**VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity**|Jing Bi et.al.|[2503.11557](http://arxiv.org/abs/2503.11557)|null|视觉推理是人类认知的核心，使个体能够解释和抽象地理解他们的环境。尽管最近的多模态大型语言模型（MLLMs）在语言和视觉-语言任务上表现出令人印象深刻的性能，但现有的基准测试主要衡量基于识别的技能，不足以评估真正的视觉推理能力。为了弥合这一关键差距，我们引入了VERIFY，这是一个专门设计用来隔离和严格评估最先进MLLMs视觉推理能力的基准。VERIFY迫使模型主要从视觉信息中进行推理，提供最少的文本上下文以减少对特定领域知识和语言偏见的依赖。每个问题都伴随着由人类标注的推理路径，使其成为第一个提供模型决策过程深入评估的基准。此外，我们提出了新的指标来评估视觉推理的准确性，并突出了当前模型推理模式中的关键不平衡。我们对领先的MLLMs的全面基准测试揭示了显著的局限性，强调了在感知和推理方面需要平衡和整体的方法。更多信息及测试，请访问我们的项目页面（https://verify-eqh.pages.dev/）。|
|**2025-03-14**|**Potential of large language model-powered nudges for promoting daily water and energy conservation**|Zonghan Li et.al.|[2503.11531](http://arxiv.org/abs/2503.11531)|null|随着水资源和能源短缺带来的压力不断增加，培养个人节约行为的需求愈发迫切。虽然提供基于使用情况的反馈即“微推动”的概念在鼓励节约行为方面显示出希望，但其有效性常常受到缺乏有针对性且可操作内容的限制。本研究调查了使用大型语言模型（LLM）为节约意图及其依据提供定制化节约建议的影响。通过一项包含1,515名大学生参与者的调查实验，我们比较了三种虚拟微推动场景：无微推动、传统带有使用统计数据的微推动以及带有使用统计数据和个性化节约建议的LLM微推动。统计分析和因果森林模型的结果显示，微推动使得86.9%-98.0%的参与者节约意图有所增加。LLM微推动在节约意图方面实现了最大18.0%的增长，比传统微推动高出88.6%。此外，结构方程模型的结果表明，接触LLM微推动增强了自我效能感和结果预期，同时减少了对社会规范的依赖，从而提高了内在的节约动机。这些发现突显了LLM在促进个人节约水资源和能源方面的变革潜力，代表着可持续行为干预和资源管理设计的新前沿。|
|**2025-03-14**|**HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models**|Ziqin Zhou et.al.|[2503.11513](http://arxiv.org/abs/2503.11513)|null|由于视频数据固有的复杂性，它跨越时间和空间维度，文本到视频生成带来了重大挑战。在生成过程中，它引入了额外的冗余、突变的变异性以及语言和视觉标记之间的领域差距。解决这些挑战需要一种有效的视频标记器，能够在保留关键语义和时空信息的同时高效地编码视频数据，作为文本和视觉之间的关键桥梁。受VQ-VAE-2中的观察和传统动画工作流程的启发，我们提出了HiTVideo，一种用于文本到视频生成的分层标记器。它利用一个具有多层离散标记框架的3D因果VAE，将视频内容编码到分层结构的代码簿中。高层捕获语义信息，具有更高的压缩率，而低层则专注于精细的时空细节，在压缩效率和重建质量之间取得平衡。我们的方法能够高效地编码较长的视频序列（例如，8秒，64帧），与基线标记器相比，将每像素比特数（bpp）降低了约70%，同时保持有竞争力的重建质量。我们探讨了压缩和重建之间的权衡，同时强调高压缩语义标记在文本到视频任务中的优势。HiTVideo旨在解决现有视频标记器在文本到视频生成任务中的潜在局限性，力求实现更高的压缩比率，并简化在语言指导下的LLMs建模，提供一个可扩展且具有前景的框架，以推进文本到视频生成。演示页面：https://ziqinzhou66.github.io/project/HiTVideo。|
|**2025-03-14**|**V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning**|Zixu Cheng et.al.|[2503.11495](http://arxiv.org/abs/2503.11495)|null|人类在视频推理中遵循一种连续的时空推理逻辑，首先识别相关帧（“何时”），然后分析关键对象之间的空间关系（“何地”），最后利用这些关系进行推理（“何事”）。然而，视频大型语言模型（Video-LLMs）是否也能在视频中“通过连续的时空逻辑进行推理”？现有的Video-LLM基准主要关注评估物体存在，忽略了关系推理。因此，很难衡量一个模型是否真正理解视频中的物体交互（动作/事件），或者仅仅是依赖于预训练的“记忆”作为生成答案时的偏差。在这项工作中，我们引入了一个视频时空推理（Video Spatio-Temporal Reasoning，V-STaR）基准来弥补这些不足。关键思想是将视频理解分解为一个逆向时空推理（Reverse Spatio-Temporal Reasoning，RSTR）任务，该任务同时评估了物体存在、事件发生的时间以及它们的位置，同时捕捉到潜在的思维链（Chain-of-thought，CoT）逻辑。为了支持这一评估，我们构建了一个数据集来激发Video-LLMs的时空推理过程。它包含由半自动化的GPT-4驱动的管道生成的从粗到细的思维链问题，嵌入显式的推理链来模拟人类认知。14个Video-LLMs在我们的V-STaR上的实验揭示了当前Video-LLMs与稳健且一致时空推理需求之间的显著差距。|
|**2025-03-14**|**A Review of DeepSeek Models' Key Innovative Techniques**|Chengen Wang et.al.|[2503.11486](http://arxiv.org/abs/2503.11486)|null|DeepSeek-V3和DeepSeek-R1是领先的通用任务和推理开源大型语言模型（LLM），其性能与OpenAI和Anthropic等公司提供的最先进闭源模型相当，同时所需的训练成本仅为后者的一个很小比例。理解DeepSeek成功背后的关键创新技术对于推进LLM研究至关重要。在本文中，我们回顾了驱动这些模型卓越效果和效率的核心技术，包括对transformer架构的改进、多头潜在注意力、专家混合、多令牌预测等技术创新，以及算法、框架和硬件的协同设计，集团相对策略优化算法，以及使用纯强化学习和迭代训练（在监督微调和强化学习之间交替进行）的模型训练后处理。此外，我们还识别出几个开放性问题，并强调了在该快速发展的领域中的潜在研究机会。|
|**2025-03-14**|**Integrating LLMs in Gamified Systems**|Carlos J. Costa et.al.|[2503.11458](http://arxiv.org/abs/2503.11458)|null|在这项工作中，提出了一种将大型语言模型（LLMs）融入游戏化系统的全面数学框架，重点在于提升任务动态、用户参与度和奖励系统。通过整合LLMs，实现了个性化反馈、自适应学习和动态内容创作，这些对于提高用户参与度和系统性能至关重要。一个模拟环境测试了该框架的适应性，并展示了其在商业、医疗和教育等各个行业的现实应用潜力。研究结果展示了LLMs如何提供定制化体验，从而提高系统有效性和用户留存率。这项研究还考察了该框架旨在解决的问题，突出了其在最大化参与度和鼓励各行业持续行为改变中的重要性。|
|**2025-03-13**|**GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing**|Rongyao Fang et.al.|[2503.10639](http://arxiv.org/abs/2503.10639)|**[link](https://github.com/rongyaofang/got)**|**当前图像生成和编辑方法主要将文本提示作为直接输入，而不对视觉构图和显式操作进行推理。我们提出了生成思维链（GoT）这一新颖范式，它通过在输出图像之前进行显式的语言推理过程来实现生成和编辑。这种方法将传统的文本到图像生成和编辑转变为一个以推理为指导的框架，该框架分析语义关系和空间布局。我们定义了GoT的公式，并构建了包含超过900万个样本的大规模GoT数据集，这些样本包含详细的推理链，捕捉语义-空间关系。为了利用GoT的优势，我们实现了一个统一的框架，该框架集成了用于推理链生成的Qwen2.5-VL与我们的新型语义-空间引导模块增强的端到端扩散模型。实验表明，我们的GoT框架在生成和编辑任务上都取得了优异的性能，并显著优于基线方法。此外，我们的方法还实现了交互式视觉生成，使用户能够明确修改推理步骤以进行精确的图像调整。GoT开创了推理驱动视觉生成和编辑的新方向，产生的图像更符合人类意图。为了促进未来的研究，我们将数据集、代码和预训练模型在https://github.com/rongyaofang/GoT上公开发布。**|
|**2025-03-13**|**HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model**|Jiaming Liu et.al.|[2503.10631](http://arxiv.org/abs/2503.10631)|null|近年来，在视觉-语言模型（VLMs）的常识推理方面取得的进展，促使视觉-语言-动作（VLA）模型的发展，使机器人能够执行泛化操作。尽管现有的自回归VLA方法利用大规模预训练知识，但它们破坏了动作的连续性。同时，一些VLA方法通过增加一个扩散头来预测连续动作，仅依赖VLM提取的特征，这限制了它们的推理能力。在本文中，我们介绍了HybridVLA，这是一个统一的框架，它在一个大型语言模型内部无缝整合了自回归和扩散策略的优点，而不是简单地连接它们。为了弥合生成差距，提出了一种协作训练方案，将扩散模型直接注入到下一个标记的预测中。使用这个方案，我们发现这两种动作预测形式不仅相互加强，而且在不同的任务中表现出不同的性能。因此，我们设计了一种协作动作集成机制，自适应地融合这两种预测，从而实现更鲁棒的控制。在实验中，HybridVLA在各种模拟和真实世界任务中优于之前的VLA最先进方法，包括单臂和双臂机器人，同时展示了在先前未见过的配置中的稳定操作。|
|**2025-03-13**|**UniGoal: Towards Universal Zero-shot Goal-oriented Navigation**|Hang Yin et.al.|[2503.10630](http://arxiv.org/abs/2503.10630)|null|在本文中，我们提出了一种适用于通用零样本目标导向导航的通用框架。现有的零样本方法基于大型语言模型（LLM）构建特定的推理框架，这在整体流程上差异很大，并且无法泛化到不同类型的任务目标。为了实现通用零样本导航，我们提出了一种统一的图表示，以统一不同的目标，包括物体类别、实例图像和文本描述。我们还把智能体的观察转换为一个在线维护的场景图。有了这种一致的场景和目标表示，与纯文本相比，我们保留了大部分结构信息，并能够利用LLM进行基于图的显式推理。具体来说，我们在每个时间点对场景图和目标图进行图匹配，并针对不同的匹配状态提出不同的策略来生成长期探索目标。当发生零匹配时，智能体首先迭代搜索目标子图。在部分匹配的情况下，智能体随后利用坐标投影和锚点对齐来推断目标位置。最后，应用场景图校正和目标验证以实现完美匹配。我们还提出了一种黑名单机制，以实现不同阶段之间的鲁棒切换。在多个基准测试上的大量实验表明，我们的UniGoal在三个研究的导航任务上实现了最先进的零样本性能，甚至优于特定任务的零样本方法和监督的通用方法。|
|**2025-03-13**|**Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search**|Andy Zhou et.al.|[2503.10619](http://arxiv.org/abs/2503.10619)|null|我们引入了Siege，一个多轮对抗框架，它从树搜索的角度模拟了大型语言模型（LLM）安全性的逐渐侵蚀。与依赖精心设计的单个提示的单一轮次越狱不同，Siege在每个回合都以广度优先的方式扩展对话，衍生出多个对抗性提示，这些提示利用了先前响应的部分合规性。通过跟踪这些增量策略泄露并将它们重新注入后续查询中，Siege揭示了微小让步如何累积成完全禁止的输出。在JailbreakBench数据集上的评估显示，Siege在GPT-3.5-turbo上实现了100%的成功率，在GPT-4上实现了97%的成功率，且使用查询数量少于基线模型如Crescendo或GOAT。这种树搜索方法提供了对模型安全措施在连续对话回合中如何退化的深入了解，强调了对于语言模型进行稳健的多轮测试程序的紧迫性。|
|**2025-03-13**|**From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM**|Kshitij Ambilduke et.al.|[2503.10620](http://arxiv.org/abs/2503.10620)|**[link](https://github.com/utter-project/SpireLM)**|大型语言模型（LLMs）在多个语言和任务上展现出卓越的性能和泛化能力，使其成为多模态集成（例如图像或语音）极具吸引力的目标。在这项工作中，我们通过语音离散化和持续预训练，将一个现有的LLM扩展到语音模态。特别是，我们对多语言LLMs，如TOWER，感兴趣，因为它们的预训练设置允许我们将离散化的语音输入视为额外的翻译语言。所得到的开源模型SPIRE能够在保持TOWER在翻译相关任务上的原始性能的同时，转录和翻译英语语音输入，展示了在LLM适应过程中将离散化语音输入集成作为额外语言是可行的。我们将我们的代码和模型提供给社区。|
|**2025-03-13**|**Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models**|Andy Zhou et.al.|[2503.10617](http://arxiv.org/abs/2503.10617)|null|将大型语言模型应用于多个任务可能会导致跨技能干扰，即对一个技能的改进会损害另一个技能。虽然像LoRA这样的方法在权重级别上施加了正交约束，但它们并没有完全解决隐藏状态表示中的干扰问题。我们提出了一种基于表示的全新方法，称为组合子空间表示微调（CS-ReFT），它学习多个正交子空间变换，每个变换专门针对一种不同的技能，并通过一个轻量级路由器将它们组合起来。通过在隐藏状态中隔离这些子空间编辑，而不是权重矩阵，CS-ReFT更有效地防止了跨任务冲突。在AlpacaEval基准测试中，将CS-ReFT应用于Llama-2-7B，取得了93.94%的胜率，超过了GPT-3.5 Turbo（86.30%），同时模型参数仅增加了0.0098%。这些发现表明，通过简单路由器组合的专用表示编辑可以显著提高多任务指令遵循能力，而额外开销最小。|
|**2025-03-13**|**R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization**|Yi Yang et.al.|[2503.10615](http://arxiv.org/abs/2503.10615)|**[link](https://github.com/Fancy-MLLM/R1-onevision)**|**大型语言模型在复杂文本任务中展现了卓越的推理能力。然而，多模态推理，即需要整合视觉和文本信息，仍然是一个重大挑战。现有的视觉-语言模型往往难以有效地分析和推理视觉内容，导致在复杂推理任务上的表现不佳。此外，缺乏全面的基准测试阻碍了对多模态推理能力的准确评估。在本文中，我们介绍了R1-Onevision，一个旨在弥合视觉感知与深度推理之间差距的多模态推理模型。为了实现这一目标，我们提出了一种跨模态推理流程，该流程将图像转换为形式化的纹理表示，从而实现精确的语言推理。利用这一流程，我们构建了R1-Onevision数据集，该数据集提供了跨多个领域的详细、逐步的多模态推理标注。我们进一步通过监督微调和强化学习开发R1-Onevision模型，以培养高级推理和强大的泛化能力。为了全面评估不同等级的多模态推理性能，我们引入了R1-Onevision-Bench，这是一个与人类教育阶段一致的基准，涵盖了从初中到大学及以上的考试。实验结果表明，R1-Onevision实现了最先进的性能，在多个具有挑战性的多模态推理基准测试中优于GPT-4o和Qwen2.5-VL等模型。**|
|**2025-03-13**|**CoSTA $\ast$ : Cost-Sensitive Toolpath Agent for Multi-turn Image Editing**|Advait Gupta et.al.|[2503.10613](http://arxiv.org/abs/2503.10613)|**[link](https://github.com/tianyi-lab/CoSTAR)**|**文本到图像模型如稳定扩散和DALLE-3在多轮图像编辑方面仍存在困难。我们将此任务分解为一个使用工具的代理工作流程（路径），通过不同成本的人工智能工具解决一系列子任务。传统的搜索算法需要昂贵的探索来找到工具路径。虽然大型语言模型（LLMs）拥有子任务规划的先验知识，但它们可能缺乏对工具能力和成本的准确估计，以确定在每个子任务中应用哪种工具。我们能否结合LLMs和图搜索的优点来找到成本效益高的工具路径？我们提出了一种三阶段方法“CoSTA*”，该方法利用LLMs创建一个子任务树，帮助剪枝给定任务的人工智能工具图，然后在该小子图上进行A*搜索以找到工具路径。为了更好地平衡总成本和质量，CoSTA*将每个工具在每个子任务上的两个指标结合起来，以引导A*搜索。然后，每个子任务的输出由视觉-语言模型（VLM）进行评估，如果出现失败，将触发更新该子任务中工具的成本和质量。因此，A*搜索可以快速从失败中恢复以探索其他路径。此外，CoSTA*可以在子任务之间自动切换模态，以实现更好的成本-质量权衡。我们建立了一个具有挑战性的多轮图像编辑的新基准，CoSTA*在成本和质量方面均优于最先进的图像编辑模型或代理，并在用户偏好上进行多方面的权衡。**|
|**2025-03-13**|**TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention**|Jinhao Duan et.al.|[2503.10602](http://arxiv.org/abs/2503.10602)|**[link](https://github.com/jinhaoduan/truthprint)**|**对象幻觉（OH）已被认为是大型视觉语言模型（LVLMs）中一个主要的可靠挑战。最近大型语言模型（LLMs）的进步表明，内部状态，如隐藏状态，编码了生成的响应的“整体真实性”。然而，关于LVLMs中的内部状态如何运作以及它们是否能作为“逐词”幻觉指标的问题，目前还研究不足，这对于缓解OH至关重要。在本文中，我们首先深入探讨了LVLM内部状态与OH问题之间的关系，并发现：（1）LVLM内部状态是幻觉行为的逐词高特异性指标。此外，（2）不同的LVLM在共同的潜在子空间中编码了幻觉的通用模式，表明存在由各种LVLM共享的“通用真实方向”。基于这些发现，我们提出了真理引导预干预（TruthPrInt），它首先学习LVLM解码的真实方向，然后在LVLM解码过程中应用真理引导的推理时间干预。我们进一步提出了ComnHallu，通过构建和对齐幻觉潜在子空间来增强跨LVLM和跨数据幻觉检测的可迁移性。我们在广泛的实验设置中评估了TruthPrInt，包括领域内和领域外场景，覆盖了流行的LVLMs和OH基准。实验结果表明，TruthPrInt显著优于最先进的方法。代码将在https://github.com/jinhaoduan/TruthPrInt上提供。**|
|**2025-03-13**|**Unlock the Power of Unlabeled Data in Language Driving Model**|Chaoqun Wang et.al.|[2503.10586](http://arxiv.org/abs/2503.10586)|null|近期，基于视觉的大型语言模型（VisionLLMs）在自动驾驶领域取得了快速进展。然而，这种进步极其依赖于大规模高质量标注数据，而这些数据成本高昂且劳动密集。为了解决这个问题，我们提出利用大量未标注数据的价值，以半监督学习的方式提高语言驾驶模型。具体来说，我们首先引入一系列基于模板的提示，以提取场景信息，生成基于有限标注数据训练的模型产生的针对未标注数据的假答案。接着，我们提出了一种自我一致性优化方法来提高这些假标注的质量，这些标注随后用于进一步训练。通过利用预训练的VisionLLM（例如InternVL），我们构建了一个强大的语言驾驶模型（LDM），用于驾驶场景问答，并优于先前最先进的方法。在DriveLM基准上的大量实验表明，我们的方法仅需5%的标注数据就能表现良好，在性能上与使用完整数据集训练的模型相媲美。特别是，我们的LDM在有限标注数据的情况下达到了44.85%的性能，在使用未标注数据时提高至54.27%，而使用完整数据集训练的模型在DriveLM基准上的性能达到了60.68%。|
|**2025-03-12**|**MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System**|Jihao Zhao et.al.|[2503.09600](http://arxiv.org/abs/2503.09600)|**[link](https://github.com/IAAR-Shanghai/Meta-Chunking)**|检索增强生成（RAG）虽然可以作为大型语言模型（LLMs）的有效补充，但往往忽略了其流程中文本分块的关键方面。本文首先介绍了一种双指标评估方法，包括边界清晰度和块粘性，以实现对分块质量的直接量化。利用这种方法，我们突出了传统分块和语义分块在处理复杂语境细微差别方面的固有局限性，从而证实了将LLMs整合到分块过程中的必要性。为了解决基于LLM的方法中计算效率与分块精度之间的固有权衡，我们设计了粒度感知混合分块器（MoC）框架，该框架包含三个阶段的处理机制。值得注意的是，我们的目标是引导分块器生成一个结构化的分块正则表达式列表，这些表达式随后用于从原始文本中提取块。大量实验表明，我们提出的指标和MoC框架有效地解决了分块任务中的挑战，揭示了分块核心，同时提高了RAG系统的性能。|
|**2025-03-12**|**How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation**|Ruohao Guo et.al.|[2503.09598](http://arxiv.org/abs/2503.09598)|**[link](https://github.com/octaviaguo/EchoMist)**|随着大型语言模型（LLMs）在各个场景中得到广泛应用，它们在不知不觉中传播错误信息的程度成为一个关键的安全问题。目前的研究主要评估LLMs对明确错误陈述的处理，却忽视了错误信息通常以未经质疑的前提在现实世界的用户互动中微妙地表现出来的情况。我们编制了ECHOMIST，这是第一个关于隐含错误信息的全面基准，其中错误的前提被嵌入到用户对LLMs的查询中。ECHOMIST基于严格的选择标准，并从包括现实世界的人机对话和社交媒体互动在内的多种来源精心挑选数据。我们还引入了一个新的评估指标，以衡量LLMs是否能够识别和反驳错误信息，而不是放大用户的错误观念。通过对包括GPT-4、Claude和Llama在内的广泛LLMs进行广泛的实证研究，我们发现当前模型在这个任务上表现惊人地差，常常无法检测到错误的前提，并生成误导性的解释。我们的发现强调了在LLM安全研究中更加关注隐含错误信息的迫切需要。|
|**2025-03-12**|**SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment**|Katrin Renz et.al.|[2503.09594](http://arxiv.org/abs/2503.09594)|null|将大型语言模型（LLMs）集成到自动驾驶领域引起了广泛关注，希望以此提高泛化能力和可解释性。然而，现有方法往往只关注驾驶或视觉-语言理解，但要同时实现高驾驶性能和广泛的语言理解仍然具有挑战性。此外，解决视觉-语言理解的主要方法是使用视觉问答。但对于自动驾驶来说，这只有在与动作空间相一致时才有用。否则，模型的回答可能与它的行为不一致。因此，我们提出了一种能够处理三个不同任务的新模型：（1）闭环驾驶，（2）视觉-语言理解，（3）语言-动作对齐。我们的模型SimLingo基于视觉语言模型（VLM），仅使用摄像头工作，不包括像激光雷达这样的昂贵传感器。SimLingo在广泛使用的CARLA模拟器上的Bench2Drive基准测试中取得了最先进的性能，并在CARLA挑战2024中获得了冠军。此外，我们在各种与语言相关的任务中取得了优异的成绩，同时保持了高驾驶性能。|
|**2025-03-12**|**BIMBA: Selective-Scan Compression for Long-Range Video Question Answering**|Md Mohaiminul Islam et.al.|[2503.09590](http://arxiv.org/abs/2503.09590)|**[link](https://github.com/md-mohaiminul/BIMBA)**|视频问答（VQA）在长视频中面临的主要挑战是从众多冗余帧中提取相关信息并建模长距离依赖关系。自注意力机制为序列建模提供了一般解决方案，但当应用于长视频中的大量时空标记时，其成本过高。大多数先前的方法依赖于压缩策略来降低计算成本，例如通过稀疏帧采样减少输入长度或通过时空池化压缩传递给大型语言模型（LLM）的输出序列。然而，这些简单的方法过度表示冗余信息，往往错过显著事件或快速发生的时空模式。在这项工作中，我们引入了BIMBA，这是一种高效的时态模型，用于处理长视频。我们的模型利用选择性扫描算法，学习从高维视频中有效选择关键信息，并将其转换为用于高效LLM处理的简化的标记序列。大量实验表明，BIMBA在多个长视频VQA基准测试中实现了最先进的准确性，包括PerceptionTest、NExT-QA、EgoSchema、VNBench、LongVideoBench和Video-MME。代码和模型在https://sites.google.com/view/bimba-mllm上公开提供。|
|**2025-03-12**|**Cost-Optimal Grouped-Query Attention for Long-Context LLMs**|Yingfa Chen et.al.|[2503.09579](http://arxiv.org/abs/2503.09579)|**[link](https://github.com/thunlp/cost-optimal-gqa)**|**构建有效且高效的基于Transformer的大型语言模型（LLMs）最近成为研究焦点，需要最大化模型的语言能力并最小化训练和部署成本。现有工作主要描述了模型性能、参数大小和数据大小之间的复杂关系，以及寻找训练LLMs的最佳计算分配。然而，它们忽略了上下文长度和注意力头配置（分组查询注意力中的查询和键值头数量）对训练和推理的影响。在本文中，我们系统地比较了不同参数大小、上下文长度和注意力头配置的模型，从模型性能、计算成本和内存成本等方面进行评估。然后，我们将现有的仅基于参数大小和训练计算的扩展方法，用于指导在训练和推理过程中构建成本最优的LLMs。我们的定量扩展研究表明，在处理足够长的序列时，具有较少注意力头的较大模型可以实现更低的损失，同时产生更低的计算和内存成本。我们的发现为开发实用的LLMs提供了宝贵的见解，尤其是在长上下文处理场景中。我们将公开发布我们的代码和数据。**|
|**2025-03-12**|**Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks**|Lutfi Eren Erdogan et.al.|[2503.09572](http://arxiv.org/abs/2503.09572)|null|大型语言模型（LLMs）在使语言代理处理简单任务方面取得了显著进步。然而，将它们应用于复杂、多步骤、长期任务仍然是一个挑战。近期的研究通过将高级规划与低级执行分离取得了成功，这使得模型能够有效平衡高级规划目标和低级执行细节。然而，由于LLMs并非天生就训练用于这项任务，生成准确计划仍然很困难。为了解决这个问题，我们提出了一个名为“计划-行动”（Plan-and-Act）的新框架，该框架将明确的规划纳入基于LLM的代理，并引入了一种通过新颖的合成数据生成方法来增强计划生成的可扩展方法。“计划-行动”包括一个规划器模型，该模型生成结构化、高级的计划以实现用户目标，以及一个执行器模型，该模型将这些计划转换为特定环境的行动。为了有效地训练规划器，我们引入了一种合成数据生成方法，该方法将真实轨迹标注为可行的计划，并附加了多样化和广泛示例以增强泛化能力。我们使用网络导航作为代表性的长期规划环境对“计划-行动”进行了评估，在WebArena-Lite基准测试中实现了最先进的54%成功率。|
|**2025-03-12**|**Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models**|Qiguang Chen et.al.|[2503.09567](http://arxiv.org/abs/2503.09567)|null|近年来，在大型语言模型（RLLMs）推理方面的最新进展，如OpenAI-O1和DeepSeek-R1，展示了它们在数学和编码等复杂领域的强大能力。它们成功的关键因素在于应用了长链式思维（Long CoT）的特性，这增强了推理能力，并使得解决复杂问题成为可能。然而，尽管有了这些进展，关于长链式思维（Long CoT）的全面调查仍然不足，这限制了我们对它与传统短链式思维（Short CoT）区别的理解，并使得关于“过度思考”和“测试时缩放”等问题的争论变得更加复杂。本调查旨在填补这一空白，提供对Long CoT的统一视角。（1）我们首先区分了Long CoT与Short CoT，并引入了一种新的分类法来归类当前的推理范式。（2）接下来，我们探讨了Long CoT的关键特性：深度推理、广泛探索和可行的反思，这些特性使得模型能够处理更复杂的任务，并比浅层次的Short CoT产生更高效、更连贯的结果。（3）然后，我们研究了诸如Long CoT的这些特性的出现等关键现象，包括过度思考和测试时缩放，并提供了这些过程在实际中如何表现的认识。（4）最后，我们确定了重大的研究空白，并突出了有希望的未来方向，包括多模态推理的整合、效率改进和增强的知识框架。通过提供结构化的概述，本调查旨在激发未来的研究，并进一步推动人工智能中逻辑推理的发展。|
|**2025-03-12**|**Large Language Models for Multi-Facility Location Mechanism Design**|Nguyen Thach et.al.|[2503.09533](http://arxiv.org/abs/2503.09533)|null|设计基于代理偏好的多设施位置优化社会成本的策略证明机制一直具有挑战性，这主要是因为需要广泛的领域知识和较差的最坏情况保证。最近，深度学习模型被提出作为替代方案。然而，这些模型需要一些领域知识，以及广泛的超参数调整，并且缺乏可解释性，这在实践中是至关重要的，因为当学习到的机制透明度是强制性的。在本文中，我们介绍了一种新颖的方法，称为LLMMech，通过将大型语言模型（LLMs）纳入进化框架来生成可解释的、无超参数、经验上策略证明和几乎最优的机制，从而解决这些局限性。我们的实验结果，在评估了各种问题设置，在这些设置中，社会成本在代理之间任意加权，并且代理的偏好可能不是均匀分布的情况下，表明LLM生成的机制通常优于现有的手工制作的基线和深度学习模型。此外，这些机制在处理分布外代理偏好和更大实例（具有更多代理）方面表现出令人印象深刻的泛化能力。|
|**2025-03-12**|**Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**|Bowen Jin et.al.|[2503.09516](http://arxiv.org/abs/2503.09516)|**[link](https://github.com/petergriffinjin/search-r1)**|高效获取外部知识和最新信息对于大型语言模型（LLMs）中的有效推理和文本生成至关重要。检索增强和工具使用训练方法，其中搜索引擎被视为工具，缺乏复杂的多轮检索灵活性或需要大规模的监督数据。在推理过程中向具有推理能力的先进LLMs提示使用搜索引擎并非最佳方案，因为LLM没有学会如何最佳地与搜索引擎互动。本文介绍了Search-R1，这是DeepSeek-R1模型的扩展，其中LLM通过强化学习（RL）仅通过逐步推理中的实时检索来学习自主生成（多个）搜索查询。Search-R1通过多轮搜索交互优化LLM的展开，利用检索到的标记掩码进行稳定的RL训练和简单的基于结果的奖励函数。在七个问答数据集上的实验表明，Search-R1相对于SOTA基线提高了26%（Qwen2.5-7B）、21%（Qwen2.5-3B）和10%（LLaMA3.2-3B）的性能。本文还进一步提供了关于RL优化方法、LLM选择和检索增强推理中的响应长度动态的实证见解。代码和模型检查点可在https://github.com/PeterGriffinJin/Search-R1上获取。|
|**2025-03-12**|**ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning**|Ziyu Wan et.al.|[2503.09501](http://arxiv.org/abs/2503.09501)|**[link](https://github.com/ziyuwan/rema-public)**|近期关于大型语言模型（LLMs）推理的研究致力于通过整合元思考来进一步提高其性能——使模型能够监控、评估和控制其推理过程，以实现更适应性和有效的解决问题。然而，当前的单代理工作缺乏获取元思考的专门设计，导致效果低下。为了应对这一挑战，我们引入了强化元思考代理（ReMA），这是一个新颖的框架，它利用多智能体强化学习（MARL）来激发元思考行为，鼓励LLMs思考思考。ReMA将推理过程解耦为两个层次化的代理：一个负责生成战略监督和计划的顶层元思考代理，以及一个负责详细执行的底层推理代理。通过具有一致目标的迭代强化学习，这些代理探索并学习协作，从而提高了泛化能力和鲁棒性。实验结果表明，ReMA在复杂推理任务上优于单代理RL基线，包括竞争级别的数学基准和LLM作为裁判的基准。全面的消融研究进一步阐述了每个不同代理的演变动态，为元思考推理过程如何增强LLMs的推理能力提供了宝贵的见解。|
|**2025-03-11**|**Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs**|Ariba Khan et.al.|[2503.08688](http://arxiv.org/abs/2503.08688)|**[link](https://github.com/ariba-k/llm-cultural-alignment-evaluation)**|针对理解多元利益相关者之间代表性日益增长的兴趣，对大型语言模型（LLMs）的“文化契合度”研究应运而生。目前评估文化契合度的方法借鉴了社会科学方法论，但往往忽视了系统性的稳健性检验。在这里，我们识别并测试了当前评估方法背后的三个假设：（1）稳定性：文化契合度是LLMs的特性，而不是评估设计的结果；（2）可外推性：在一个狭窄的问题集上与某种文化的契合度可以预测在该文化上的其他契合度；（3）可引导性：LLMs可以被可靠地引导以表达特定的文化视角。通过考察领先LLMs的显性和隐性偏好的实验，我们发现跨展示格式的稳定性很高，评估的文化维度与保留的文化维度之间缺乏一致性，以及在提示引导下的行为异常。我们表明，这些不一致性可能导致评估结果对方法的小幅变化非常敏感。最后，我们在评估设计的一个案例研究中表明，狭窄的实验和对证据的选择性评估可能无法完整地描绘LLMs的文化契合度特性。总的来说，这些结果突出了当前评估LLMs文化契合度方法的重要局限性。|
|**2025-03-11**|**Self-Taught Self-Correction for Small Language Models**|Viktor Moskvoretskii et.al.|[2503.08681](http://arxiv.org/abs/2503.08681)|null|尽管大型语言模型（LLMs）在各种任务上取得了显著的成绩，但它们仍然容易出错。一个关键挑战是使它们能够自我纠正。虽然先前的研究依赖于外部工具或大型专有模型，但这项工作通过仅使用自生成数据进行迭代微调，探索了小型语言模型（SLMs）的自我纠正。我们引入了自我教学自我纠正（STaSC）算法，该算法结合了多个算法设计选择。在问答任务上的实验结果表明，STaSC有效地学习了自我纠正，从而显著提高了性能。我们的分析进一步揭示了自我纠正的机制以及不同设计选择对学习动态和整体性能的影响。为了支持未来的研究，我们发布了用户友好的代码库和轻量级模型。|
|**2025-03-11**|**Exploring the Word Sense Disambiguation Capabilities of Large Language Models**|Pierpaolo Basile et.al.|[2503.08662](http://arxiv.org/abs/2503.08662)|null|词义消歧（WSD）是计算语言学中的一个历史性任务，多年来一直受到广泛关注。然而，随着大型语言模型（LLMs）的出现，对这个任务（在其经典定义下）的兴趣有所下降。在这项研究中，我们评估了各种LLMs在WSD任务上的性能。我们将一个先前的基准（XL-WSD）扩展，重新设计了两个适合LLMs的子任务：1）给定句子中的一个词，LLM必须生成正确的定义；2）给定句子中的一个词和一组预定义的意义，LLM必须选择正确的一个。扩展的基准使用XL-WSD和BabelNet构建。结果表明，LLMs在零样本学习方面表现良好，但无法超越当前最先进的方法。然而，一个参数数量适中的微调模型优于所有其他模型，包括最先进的模型。|
|**2025-03-11**|**LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization**|Xianfeng Wu et.al.|[2503.08619](http://arxiv.org/abs/2503.08619)|**[link](https://github.com/xianfengwu01/lightgen)**|近年来，文本到图像生成的发展主要依赖于大量数据集和参数繁重的架构。这些需求严重限制了缺乏大量计算资源的学者和从业者的可及性。在本文中，我们介绍了\模型，这是一种高效训练范式，用于图像生成模型，它使用了知识蒸馏（KD）和直接偏好优化（DPO）。受多模态大型语言模型（MLLMs）中广泛采用的数据KD技术成功的启发，LightGen将来自最先进（SOTA）文本到图像模型的知识蒸馏到一个只有 $0.7B$参数的紧凑的掩码自回归（MAR）架构中。使用仅由$2M$ 高质量图像组成的紧凑合成数据集，这些图像由各种标题生成，我们证明在确定模型性能方面，数据多样性远远超过数据量。这种策略大大降低了计算需求，将预训练时间从可能成千上万的GPU天数缩短到仅仅88 GPU天数。此外，为了解决合成数据固有的不足，尤其是高频细节和空间不准确性，我们集成了DPO技术，该技术可以精细图像真实性和位置准确性。全面实验证实，LightGen在降低计算资源的同时，实现了与SOTA模型相当的形象生成质量，并扩大了资源受限环境的可及性。代码可在https://github.com/XianfengWu01/LightGen上获得。|
|**2025-03-11**|**EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments**|Dongping Li et.al.|[2503.08604](http://arxiv.org/abs/2503.08604)|**[link](https://github.com/silence143/EMMOE)**|长期以来，开发受自然语言控制的自主家庭机器人一直是人类的追求。虽然大型语言模型（LLMs）和具身智能的发展使这一目标更接近，但仍然存在一些挑战：缺乏更复杂机器人任务的统一基准，有限的评估方法和指标，LLMs与移动操作轨迹之间的数据不兼容。为了解决这些问题，我们引入了开放式环境中的具身移动操作（EMMOE），它要求代理解释用户指令并在连续空间中执行长期日常任务。EMMOE无缝地将高级和低级具身任务集成到一个统一的框架中，并引入了三个新的评估指标。此外，我们收集了EMMOE-100数据集，它包含各种任务属性、详细的过程注释、失败后的重新规划以及用于LLM训练的两个子数据集。此外，我们设计了HomieBot，这是一个复杂的代理系统，由具有直接偏好优化（DPO）的LLM、轻量级导航和操作模型以及多个错误检测机制组成。最后，我们展示了HomieBot的性能以及不同模型和策略的评估。|
|**2025-03-11**|**NSF-SciFy: Mining the NSF Awards Database for Scientific Claims**|Delip Rao et.al.|[2503.08600](http://arxiv.org/abs/2503.08600)|null|我们提出了NSF-SciFy，这是一个从国家科学基金会（NSF）资助数据库中提取的科学主张提取大规模数据集，包含超过40万篇跨越五十年时间的资助摘要。虽然之前的数据集依赖于已发表的文献，但我们利用了资助摘要这一独特的优势：它们在发表之前捕捉到研究生命周期的早期主张。我们还引入了一个新的任务，用于区分提案中现有的科学主张和抱负性研究意图。通过使用前沿大型语言模型的零样本提示，我们从16,000篇资助摘要中联合提取了11.4万个科学主张和14.5万个研究提案，创建了名为NSF-SciFy-MatSci的聚焦子集。我们使用这个数据集评估了三个关键任务：(1) 技术摘要到非技术摘要的生成，模型实现了高BERTScore（0.85+ F1）；(2) 科学主张提取，微调模型相对于基础模型提高了100%的相对改进；(3) 研究提案提取，通过微调显示90%以上的改进。我们引入了基于大型语言模型的创新评估指标，以对主张/提案提取质量进行稳健评估。作为迄今为止最大的科学主张数据集——预计包含NSF资助的所有STEM学科中约280万个主张——NSF-SciFy为主张验证和元科学研究提供了新的机会。我们将所有数据集、训练模型和评估代码公开发布，以促进进一步的研究。|
|**2025-03-11**|**HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding**|Shehreen Azad et.al.|[2503.08585](http://arxiv.org/abs/2503.08585)|null|尽管多模态大型语言模型（MLLMs）取得了进展，但由于帧和上下文长度的限制，当前方法在中等至长视频理解方面存在困难。因此，这些模型通常依赖于帧采样，这可能导致在长时间内遗漏关键信息，且缺乏特定任务的关联性。为了解决这些挑战，我们引入了HierarQ，这是一个基于层次Q-Former的任务感知框架，它可以依次处理帧以避开帧采样的需求，同时避免大型语言模型（LLM）的上下文长度限制。我们引入了一种轻量级的双流语言引导特征调节器，将任务感知性引入视频理解中，实体流捕捉短期上下文中的帧级对象信息，而场景流则识别较长时间范围内的更广泛交互。每个流都有专门的记忆库支持，这使得我们提出的层次查询变压器（HierarQ）能够有效捕获短期和长期上下文。在视频理解、问答和字幕任务上的10个视频基准数据集上的广泛评估表明，HierarQ在大多数数据集上实现了最先进的性能，证明了其在全面视频分析方面的鲁棒性和效率。|
|**2025-03-11**|**RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video Understanding**|Xichen Tan et.al.|[2503.08576](http://arxiv.org/abs/2503.08576)|null|多模态大型语言模型（MLLMs）在视频理解方面的能力正在迅速发展。为了有效地评估它们的视频理解能力，提出了长视频理解基准，如Video-MME和MLVU。然而，这些基准直接使用统一的帧采样进行测试，这导致信息损失严重，影响了评估在反映MLLMs真实能力方面的准确性。为了解决这个问题，我们提出了RAG-Adapter，这是一个即插即用的框架，通过采样与给定问题最相关的帧来减少测试过程中的信息损失。此外，我们引入了一种分组监督对比学习（GCL）方法，通过在我们的MMAT数据集上进行微调，进一步增强了RAG-Adapter的采样有效性。最后，我们在各种视频理解基准上测试了多个基线MLLMs，发现RAG-Adapter采样始终优于均匀采样（例如，GPT-4o在Video-MME上的准确率提高了9.3%），为长视频基准提供了一种更准确的测试方法。|
|**2025-03-11**|**DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process**|Minjun Zhu et.al.|[2503.08569](http://arxiv.org/abs/2503.08569)|null|大型语言模型（LLMs）在科学研究评估中越来越被广泛应用，尤其是在自动论文评审方面。然而，现有的基于LLM的评审系统面临着一些重大挑战，包括领域专业知识有限、推理幻觉以及缺乏结构化评估。为了解决这些局限性，我们引入了DeepReview，这是一个多阶段框架，通过整合结构化分析、文献检索和基于证据的论证来模拟专家评审员。使用DeepReview-13K，一个带有结构化注释的精选数据集，我们训练了DeepReviewer-14B，其性能超过了CycleReviewer-70B，且使用的标记更少。在其最佳模式下，DeepReviewer-14B在对GPT-o1和DeepSeek-R1的评估中分别达到了88.21%和80.20%的胜率。我们的工作为基于LLM的论文评审设定了新的基准，所有资源均公开可用。代码、模型、数据集和演示已发布在http://ai-researcher.net。|
|**2025-03-11**|**Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs**|Wanyong Feng et.al.|[2503.08551](http://arxiv.org/abs/2503.08551)|null|选择题（MCQs）的难度是教育评估中的一个关键因素。预测MCQ难度具有挑战性，因为它需要理解达到正确选项的复杂性和干扰项（即错误选项）的可信度。在本文中，我们提出了一种新颖的两阶段方法来预测MCQ的难度。首先，为了更好地估计每个MCQ的复杂性，我们使用大型语言模型（LLMs）来增强达到每个选项所需的推理步骤。我们不仅使用MCQ本身，还使用这些推理步骤作为预测难度的输入。其次，为了捕捉干扰项的可信度，我们从分布中采样知识水平，以考虑对MCQ作出反应的学生之间的差异。这种由项目反应理论（IRT）启发的设置使我们能够估计学生选择每个选项（包括正确和错误选项）的可能性。我们使用基于Kullback-Leibler（KL）散度的正则化目标将这些预测与它们的真实值对齐，并使用估计的可能性来预测MCQ的难度。我们在两个带有使用IRT估计的真实难度值的现实世界数学MCQ和响应数据集上评估了我们的方法。实验结果表明，我们的方法优于所有基线，平均平方误差降低了28.3%，决定系数提高了34.6%。我们还定性讨论了我们的新颖方法如何导致预测MCQ难度时准确性更高。|
|**2025-03-10**|**Talking to GDELT Through Knowledge Graphs**|Audun Myers et.al.|[2503.07584](http://arxiv.org/abs/2503.07584)|null|在这项工作中，我们研究了各种检索增强再生（RAG）方法，以了解每种方法在问答分析中的优缺点。为了获得这种理解，我们使用了全球事件、语言和调（GDELT）数据集的一个案例研究子集以及从在线新闻文章中抓取的原始文本语料库。为了从文本语料库中检索信息，我们实施了一个传统的向量存储RAG以及基于最先进的大语言模型（LLM）的方法，用于自动构建知识图谱（KG）和检索相关子图。除了这些语料库方法之外，我们还开发了一个基于本体论的新框架，用于直接从GDELT构建知识图谱（KG），该框架利用GDELT的底层模式来创建全球事件的有序表示。为了从基于本体论的知识图谱中检索相关信息，我们实现了直接的图查询和最先进的图检索方法。我们在问答任务中比较了每种方法的性能。我们发现，虽然我们的基于本体论的知识图谱对问答很有价值，但相关子图的自动提取具有挑战性。相反，LLM生成的知识图谱虽然捕捉了事件摘要，但往往缺乏一致性和可解释性。我们的发现表明，本体论和基于LLM的知识图谱构建之间的协同方法具有优势，并为此提出了相应的途径。|
|**2025-03-10**|**Junior Software Developers' Perspectives on Adopting LLMs for Software Engineering: a Systematic Literature Review**|Samuel Ferino et.al.|[2503.07556](http://arxiv.org/abs/2503.07556)|null|近年来，许多研究探讨了初级开发者采用基于大型语言模型（LLM）的工具进行软件开发的情况。这些研究旨在理解开发者使用这些工具的看法，这对于在软件工程中成功采用LLM工具是一个基本支柱。本文的目的是概述初级软件开发者对软件工程中LLM工具（LLM4SE）的看法和使用情况。我们遵循Kitchenham等人提出的指南，对56篇主要研究进行了系统文献综述（SLR），将具有等于或少于五年经验的软件开发者（包括计算机科学/软件工程学生）定义为初级软件开发者。我们发现，大部分研究集中于理解在软件工程中整合AI工具的不同方面。只有8.9%的研究为初级软件开发者提供了一个明确的定义，且定义不统一。使用LLM工具进行信息搜索是最常见的任务。ChatGPT是研究中最常见的一种LLM工具（以及实验）。大多数研究（83.9%）报告了对采用LLM工具的正面和负面看法。我们还发现并分类了关于LLM采用的优点、挑战和建议。我们的结果表明，开发者不仅使用LLM进行代码生成，还用它来提高他们的开发技能。关键的是，他们不仅体验到了采用LLM工具的好处，还至少意识到一些LLM的限制，如生成错误建议、潜在的数据泄露和AI幻觉。我们的发现对软件工程研究人员、教育者和开发者具有启示意义。|
|**2025-03-10**|**KSOD: Knowledge Supplement for LLMs On Demand**|Haoran Li et.al.|[2503.07550](http://arxiv.org/abs/2503.07550)|null|大型语言模型（LLMs）在各种任务中展现了非凡的能力，但在特定领域的任务中仍然会产生错误。为了进一步提高它们的性能，我们提出了KSOD（按需为LLMs补充知识），一个新颖的框架，它通过基于知识的监督微调（SFT）使LLMs能够提升其能力。KSOD从知识不足的角度分析错误的原因，通过识别可能导致错误的LLM中可能缺失的知识。随后，KSOD在知识数据集上调整知识模块，并基于此验证LLM是否缺少已识别的知识。如果知识得到验证，KSOD使用知识模块将已识别的知识补充到LLM中。在特定知识而非特定任务上调整LLM，将任务与知识解耦，我们的实验在两个特定领域基准和四个通用基准上实证表明，KSOD增强了LLM在需要补充知识的任务上的性能，同时保持了它们在其他任务上的性能。我们的发现揭示了利用基于知识的SFT提高LLM能力的潜力。|
|**2025-03-10**|**Bi-Directional Mental Model Reconciliation for Human-Robot Interaction with Large Language Models**|Nina Moorman et.al.|[2503.07547](http://arxiv.org/abs/2503.07547)|null|在人类与机器人的交互中，人类和机器人代理维持着对环境、共同任务以及彼此的内部心理模型。这些表示的准确性取决于每个代理进行心灵理论的能力，即理解其队友的知识、偏好和意图。当心理模型出现分歧并影响到任务执行时，需要进行调和以防止交互质量的下降。我们提出了一种双向心理模型调和框架，利用大型语言模型通过半结构化自然语言对话促进对齐。我们的框架放宽了先前心理模型调和工作的假设，即人类或机器人代理开始时必须有一个正确的模型以与对方对齐。通过我们的框架，人类和机器人都能在交互过程中识别并沟通缺失的任务相关上下文，迭代地朝着共享的心理模型进步。|
|**2025-03-10**|**Queueing, Predictions, and LLMs: Challenges and Open Problems**|Michael Mitzenmacher et.al.|[2503.07545](http://arxiv.org/abs/2503.07545)|null|排队系统为应用机器学习预测（如估计服务时间）以改善系统性能提供了许多机会。这种集成引发了许多关于如何有效利用预测来改善调度决策的开放性问题。最近的研究探索了具有预测服务时间的队列，通常旨在最小化系统中的作业时间。我们回顾了这些工作，强调了预测的有效性，并提出了关于队列性能的开放性问题。然后，我们转向考虑一个重要的实际调度预测应用示例，即大型语言模型（LLM）系统，这提出了新的调度挑战，并突出了预测改善性能的潜力。特别是，我们考虑了执行推理的LLM。LLM系统中的推理请求（作业）本质上很复杂；它们具有可变的服务时间，动态内存占用受键值（KV）存储内存限制，以及多种可能的抢占方法，这些方法以不同的方式影响性能。我们提供了LLM系统中调度重要方面的背景信息，并介绍了由此产生的新的模型和开放性问题。我们认为，将排队理论中的洞察和分析应用于LLM系统的调度中存在重大机遇。|
|**2025-03-10**|**XIFBench: Evaluating Large Language Models on Multilingual Instruction Following**|Zhenyu Li et.al.|[2503.07539](http://arxiv.org/abs/2503.07539)|null|大型语言模型（LLMs）在各种应用中展示了卓越的指令遵循能力。然而，在多语言环境中的表现仍理解不足，因为现有的评估缺乏细致的约束分析。我们引入了XIFBench，这是一个基于约束的综合基准，用于评估LLMs的多语言指令遵循能力，它包含五个约束类别的分类和涵盖六种语言的465条平行指令，这些语言跨越不同的资源水平。为确保一致的跨语言评估，我们开发了一种基于需求的协议，该协议利用英语需求作为语义锚点。然后，这些需求被用来验证跨语言的翻译。与各种LLMs的广泛实验揭示，在资源水平之间存在显著的指令遵循性能差异，识别出关键影响因素，如约束类别、指令复杂性和文化特异性。|
|**2025-03-10**|**TokenButler: Token Importance is Predictable**|Yash Akhauri et.al.|[2503.07518](http://arxiv.org/abs/2503.07518)|**[link](https://github.com/abdelfattah-lab/tokenbutler)**|大型语言模型（LLMs）依赖于键值（KV）缓存来存储标记历史，从而实现标记的高效解码。然而，随着KV-Cache的扩大，它成为主要的内存和计算瓶颈。但是，有缓解这一瓶颈的机会，尤其是因为先前的研究表明，只有一小部分标记对每个解码步骤有实质性贡献。寻找这些关键标记的一个关键挑战是，它们是动态的，并且高度依赖于输入查询。现有方法要么通过永久删除标记而冒着质量风险，要么保留完整的KV-Cache，但在生成时依赖于检索标记块（页面），从而在密集、内容丰富的任务中失败。此外，许多现有的KV-Cache稀疏化方法依赖于不准确的标记重要性代理。为了解决这些局限性，我们引入了TokenButler，这是一个高粒度、查询感知的预测器，它学会了识别这些关键标记。通过训练一个参数开销不到1.2%的轻量级预测器，TokenButler根据标记的上下文预测重要性进行优先排序。与用于估计标记重要性的SoTA方法相比，这提高了困惑度和下游准确度超过8%。我们在一个新颖的合成小上下文共指检索任务上评估了TokenButler，证明了接近Oracle的准确性。代码、模型和基准：https://github.com/abdelfattah-lab/TokenButler|
|**2025-03-10**|**Language Models Fail to Introspect About Their Knowledge of Language**|Siyuan Song et.al.|[2503.07513](http://arxiv.org/abs/2503.07513)|**[link](https://github.com/siyuansong2004/language-introspection)**|近期人们对大型语言模型（LLMs）能否内省自身内部状态产生了兴趣。这种能力将使LLMs更具可解释性，并验证在语言学中使用标准内省方法评估模型中的语法知识（例如，询问“这个句子是否语法正确？”）的合理性。我们系统地调查了21个开源LLMs中出现的内省现象，在两个对理论有意义的领域进行了研究：语法知识和词预测。关键的是，在两个领域，模型的内部语言知识可以从直接测量字符串概率中得到理论上的依据。然后，我们评估模型的响应是否忠实反映了其内部知识。我们提出了一种新的内省度量：模型被提示的响应预测其自身字符串概率的程度，超出另一个几乎具有相同内部知识的模型所能预测的范围。虽然元语言提示和概率比较都导致了高任务准确率，但我们没有发现LLMs具有特权“自我访问”的证据。我们的发现使最近关于模型可以内省的结果变得复杂，并为论证提示响应不应与模型的言语概括混淆提供了新的证据。|
|**2025-03-10**|**Plume: Scaffolding Text Composition in Dashboards**|Maxim Lisnic et.al.|[2503.07512](http://arxiv.org/abs/2503.07512)|null|仪表盘中的文本扮演着多重关键角色，包括提供背景信息、提供洞察、指导交互和总结关键信息。尽管其重要性不容忽视，但大多数仪表盘工具都侧重于可视化，对文本创作的支持有限。为了填补这一空白，我们开发了Plume系统，旨在帮助作者创作有效的仪表盘文本。通过形成性评估示例仪表盘，我们创建了一种文本参数分类法，并阐述了视觉位置与语义联系之间的关系，这些内容为Plume的设计提供了依据。Plume利用大型语言模型（LLMs）生成与上下文相适应的内容，并提供了撰写清晰、易读文本的指南。一项针对12名仪表盘作者的初步评估探讨了辅助文本创作如何融入工作流程，揭示了LLM生成文本的优势和局限性，以及我们人机协作方法的价值。我们的研究结果表明，通过更好地支持文本在传达洞察中扮演的多种角色，可以改善仪表盘创作工具。|
|**2025-03-10**|**Sometimes the Model doth Preach: Quantifying Religious Bias in Open LLMs through Demographic Analysis in Asian Nations**|Hari Shankar et.al.|[2503.07510](http://arxiv.org/abs/2503.07510)|**[link](https://github.com/harishankar08/llmopinions)**|大型语言模型（LLMs）在不知情的情况下能够生成观点并传播偏见，这些偏见源于代表性不足和非多样化的数据收集。先前的研究已经分析了这些观点，尤其是针对西方，特别是美国的观点。然而，由此产生的见解可能无法推广到非西方人群。随着LLM系统在各个不同领域的广泛使用，每个生成输出的文化敏感性变得至关重要。我们的工作提出了一种新的方法，该方法从定量分析LLMs生成的观点出发，改进了之前关于提取模型社会人口特征的工作。我们的方法通过汉明距离测量LLM的响应与调查受访者之间的距离，以推断模型输出中反映出的人口特征。我们在全球南部的多个国家进行了调查，重点关注印度和其他亚洲国家，特别是评估模型在涉及宗教宽容和身份的调查中的表现。我们的分析表明，大多数开放LLMs符合单一的同质化特征，在不同国家/地区之间有所变化，这反过来又引发了关于LLMs推广霸权世界观、损害不同少数派观点的风险。我们的框架也可能对未来的研究有用，这些研究将调查训练数据、模型架构以及LLM输出中反映出的复杂交叉，特别是关于宗教宽容和身份等敏感话题。|
|**2025-03-10**|**V2Flow: Unifying Visual Tokenization and Large Language Model Vocabularies for Autoregressive Image Generation**|Guiwei Zhang et.al.|[2503.07493](http://arxiv.org/abs/2503.07493)|**[link](https://github.com/zhangguiwei610/v2flow)**|我们提出了V2Flow，这是一种新型的分词器，能够生成离散的可视化标记，这些标记能够进行高保真重建，同时确保与大型语言模型（LLMs）词汇空间的结构和潜在分布对齐。利用这种紧密的视觉-词汇耦合，V2Flow能够在现有的LLMs之上实现自回归视觉生成。我们的方法将可视化分词定义为流匹配问题，旨在学习从标准正态先验到连续图像分布的映射，该映射依赖于嵌入在LLMs词汇空间中的标记序列。V2Flow的有效性源于两个核心设计。首先，我们提出了一种可视化词汇重采样器，它将可视化数据压缩成紧凑的标记序列，每个序列被表示为LLMs词汇上的软分类分布。这允许将可视化标记无缝集成到现有的LLMs中，以实现自回归视觉生成。其次，我们提出了一种带掩码的自回归修正流解码器，使用带掩码的变换器编码器-解码器来细化可视化标记，使其成为语境丰富的嵌入。这些嵌入随后条件化一个专用速度场，以实现精确重建。此外，还引入了一种自回归修正流采样策略，确保灵活的序列长度同时保持有竞争力的重建质量。广泛的实验表明，V2Flow优于主流的基于VQ的分词器，并促进了在现有LLMs之上的自回归视觉生成。https://github.com/zhangguiwei610/V2Flow|
|**2025-03-10**|**LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?**|Bangyan Li et.al.|[2503.07487](http://arxiv.org/abs/2503.07487)|null|近期，多模态大型模型（MLLMs）在视觉理解与推理方面的能力表现出色，涵盖各种视觉-语言任务。然而，MLLMs在零样本医学疾病识别方面通常表现不佳，因为它们没有充分利用捕获的特征和可用的医学知识。为了解决这一挑战，我们提出了LLaVA-RadZ，一个简单而有效的零样本医学疾病识别框架。具体来说，我们设计了一种端到端训练策略，称为解码侧特征对齐训练（DFAT），以利用MLLM解码器架构的特性，并融入针对不同模态量身定制的模态特定标记，从而有效利用图像和文本表示，促进稳健的跨模态对齐。此外，我们引入了一个领域知识锚定模块（DKAM），以利用大型模型内生的医学知识，从而缓解图像-文本对齐中的类别语义差距。DKAM提高了类别级别的对齐，使得疾病识别更加准确。在多个基准测试上的大量实验表明，我们的LLaVA-RadZ在零样本疾病识别方面显著优于传统的MLLMs，并且与成熟的、高度优化的基于CLIP的方法相比，表现出最先进的性能。|
|**2025-03-10**|**GenAIReading: Augmenting Human Cognition with Interactive Digital Textbooks Using Large Language Models and Image Generation Models**|Ryugo Morita et.al.|[2503.07463](http://arxiv.org/abs/2503.07463)|null|认知增强是推进教育的重要基石，尤其是在个性化学习方面。然而，由于大量文本材料（如叙事和学术教科书）的使用，个性化这些材料仍然具有挑战性，因为它们可能会阻碍学习者的参与和理解。本研究基于双编码理论等认知理论——该理论认为，结合文本和视觉信息可以增强理解和记忆——探讨了生成式人工智能（GenAI）丰富教育材料的潜力。我们利用大型语言模型（LLMs）生成简明的文本摘要，以及图像生成模型（IGMs）从文本输入中创建视觉上协调的内容。招募了24名参与者后，我们验证了整合AI生成的补充材料显著提高了学习成果，阅读后测试分数提高了7.50%。这些发现强调了GenAI在创建能够增强认知增强的适应性学习环境中的变革潜力。|
|**2025-03-10**|**MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning**|Xiangru Tang et.al.|[2503.07459](http://arxiv.org/abs/2503.07459)|**[link](https://github.com/gersteinlab/medagents-benchmark)**|大型语言模型（LLMs）在现有的医学问答基准测试中表现出令人印象深刻的性能。这种高性能使得对高级方法进行有意义的评估和区分变得越来越困难。我们提出了MedAgentsBench，这是一个专注于需要多步临床推理、诊断制定和治疗计划等具有挑战性的医学问题的基准。从七个已建立的医学数据集中汲取灵感，我们的基准针对现有评估中的三个关键局限性进行了处理：（1）简单问题的普遍存在，即使基础模型也能实现高性能；（2）不同研究之间采样和评估协议的不一致；（3）缺乏对性能、成本和推理时间之间相互作用的系统性分析。通过使用各种基础模型和推理方法的实验，我们证明了最新的思维模型DeepSeek R1和OpenAI o3在复杂的医学推理任务中表现出卓越的性能。此外，与传统的搜索型智能体方法相比，高级搜索型智能体方法提供了有前景的性能与成本比。我们的分析揭示了模型家族在复杂问题上的性能差距，并确定了针对不同计算约束的最优模型选择。我们的基准和评估框架可在https://github.com/gersteinlab/medagents-benchmark上公开获取。|
|**2025-03-10**|**LLMs syntactically adapt their language use to their conversational partner**|Florian Kandra et.al.|[2503.07457](http://arxiv.org/abs/2503.07457)|null|在论文中，首先提到了一个观察现象：人们在对话中经常将自己的语言使用与对方对齐。接下来，作者提出了一个研究问题：大型语言模型（LLMs）是否表现出与人类类似的对话适应行为。为了验证这一点，作者构建了一个LLMs之间的对话语料库，并发现随着对话的进行，两个LLM代理的句法选择越来越相似，这证实了现代LLMs至少在初步层面上会根据对话伙伴调整自己的语言使用。以下是摘要的中文翻译：  人们经常观察到，在对话中，人类说话者会将自己的语言使用与对方对齐。在本研究中，我们通过实证方法探讨了大型语言模型（LLMs）是否展现出与人类相似的对话适应行为。我们构建了一个LLMs之间的对话语料库，发现随着对话的进行，两个LLM代理最终会做出更加相似的句法选择，这证实了现代LLMs至少在基础层面上会根据对话伙伴调整自己的语言使用。|
|**2025-03-10**|**From Idea to Implementation: Evaluating the Influence of Large Language Models in Software Development -- An Opinion Paper**|Sargam Yadav et.al.|[2503.07450](http://arxiv.org/abs/2503.07450)|null|1. 首先识别摘要中的关键信息，包括主题（transformer架构在自然语言处理中的转折点）、具体模型（BERT、GPT）、应用领域（软件开发和教育）、大语言模型（ChatGPT、Bard）、研究目的（收集和分析专家对LLM在软件开发中的使用经验）、专家意见（积极、生产力提升、编码时间减少、潜在担忧和挑战）。  2. 确定翻译的顺序，从介绍transformer架构的重要性开始，然后是LLMs的普及及其在软件开发中的应用，接着是研究方法和结果，最后是专家意见的总结。  3. 开始逐句翻译：     - The introduction of transformer architecture was a turning point in Natural Language Processing (NLP).      > transformer架构的引入是自然语言处理（NLP）领域的转折点。     - Models based on the transformer architecture such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-Trained Transformer (GPT) have gained widespread popularity in various applications such as software development and education.      > 基于transformer架构的模型，如BERT（双向编码器表示从Transformer）和GPT（生成预训练Transformer），在软件开发和教育等众多领域得到了广泛应用。     - The availability of Large Language Models (LLMs) such as ChatGPT and Bard to the general public has showcased the tremendous potential of these models and encouraged their integration into various domains such as software development for tasks such as code generation, debugging, and documentation generation.      > 大语言模型（LLMs）如ChatGPT和Bard对公众的可用性展示了这些模型的巨大潜力，并促进了它们在软件开发等领域（如代码生成、调试和文档生成）的应用。     - In this study, opinions from 11 experts regarding their experience with LLMs for software development have been gathered and analysed to draw insights that can guide successful and responsible integration.      > 在这项研究中，收集并分析了11位专家关于他们在软件开发中使用LLMs的经验，以获取指导成功和负责任整合的见解。     - The overall opinion of the experts is positive, with the experts identifying advantages such as increase in productivity and reduced coding time.      > 专家的整体意见是积极的，他们识别出诸如生产力提升和编码时间减少等优势。     - Potential concerns and challenges such as risk of over-dependence and ethical considerations have also been highlighted.      > 也强调了潜在的担忧和挑战，如过度依赖的风险和伦理考量。  4. 将翻译的句子整合成完整的中文摘要：  transformer架构的引入是自然语言处理（NLP）领域的转折点。基于transformer架构的模型，如BERT（双向编码器表示从Transformer）和GPT（生成预训练Transformer），在软件开发和教育等众多领域得到了广泛应用。大语言模型（LLMs）如ChatGPT和Bard对公众的可用性展示了这些模型的巨大潜力，并促进了它们在软件开发等领域（如代码生成、调试和文档生成）的应用。在这项研究中，收集并分析了11位专家关于他们在软件开发中使用LLMs的经验，以获取指导成功和负责任整合的见解。专家的整体意见是积极的，他们识别出诸如生产力提升和编码时间减少等优势。也强调了潜在的担忧和挑战，如过度依赖的风险和伦理考量。|
|**2025-03-10**|**From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector Graphics**|Jaewook Lee et.al.|[2503.07429](http://arxiv.org/abs/2503.07429)|null|大型语言模型（LLMs）的进步为通过自动化支持教师和学生来提升数学教育提供了新的可能性。虽然先前的研究主要集中在生成数学问题和高质量干扰项上，但可视化在数学学习中的作用仍被低估。图表对于数学思维和问题解决至关重要，但手动创建它们既耗时又需要特定领域的专业知识，这限制了其可扩展性。最近关于使用LLMs生成可缩放矢量图形（SVG）的研究为自动化图表创建提供了一种有希望的方法。与基于像素的图像不同，SVG使用XML来表示几何图形，允许无缝缩放和适应性。教育平台如可汗学院和IXL已经使用SVG来显示数学问题和提示。在本文中，我们探讨了使用LLMs通过中间SVG表示生成与文本提示相关的数学图表。我们针对三个研究问题进行了探讨：（1）如何自动生成问题解决提示中的数学图表并评估其质量，（2）SVG是否是数学图表的有效中间表示形式，（3）LLMs生成基于SVG的准确图表所需的提示策略和格式。我们的贡献包括定义自动生成基于SVG的数学提示图表的任务，开发基于LLM提示的管道，并确定改进图表生成的关键策略。此外，我们引入了一个基于视觉问答（VQA）的评估设置，并进行了消融研究以评估不同的管道变体。通过自动化数学图表的创建，我们旨在为学生和教师提供准确、概念相关的视觉辅助工具，以增强问题解决和学习体验。|
|**2025-03-10**|**RePO: ReLU-based Preference Optimization**|Junkang Wu et.al.|[2503.07426](http://arxiv.org/abs/2503.07426)|**[link](https://github.com/junkangwu/repo)**|将大型语言模型（LLMs）与人类偏好对齐对于实际部署至关重要，然而，现有的方法如RLHF面临着计算和稳定性挑战。尽管DPO通过单个超参数 $\beta$建立了一个离线范式，但后续的方法如SimPO通过双重参数（$\beta$，$\gamma$）重新引入了复杂性。我们提出了基于ReLU的偏好优化（RePO）算法，这是一种简化算法，通过以下两个进步消除了$\beta$：（1）保留SimPO的无参考边距但通过梯度分析去除$\beta$；（2）采用基于ReLU的最大边距损失，它自然地过滤掉平凡对。从理论上讲，RePO被定义为SimPO的极限情况（$\beta \to \infty$ ），其中逻辑加权崩溃为二进制阈值，形成0-1损失的凸包。在AlpacaEval 2和Arena-Hard上的实证结果表明，RePO在多个基础模型上优于DPO和SimPO，并且只需要调整一个超参数。|
|**2025-03-10**|**REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding**|Yan Tai et.al.|[2503.07413](http://arxiv.org/abs/2503.07413)|**[link](https://github.com/MacavityT/REF-VLM)**|**多模态大型语言模型（MLLMs）在经过大规模数据集训练后，在多种视觉-语言任务上表现出强大的零样本能力。然而，将密集预测任务（如语义分割和关键点检测）仅以文本输出的形式表示时，这些任务对MLLMs来说提出了重大挑战。同时，当前利用潜在嵌入进行视觉任务解码的MLLMs在多任务学习和多粒度场景下的适应性有限。在本工作中，我们提出了REF-VLM，这是一个用于统一训练各种视觉解码任务的全端框架。为了解决复杂的视觉解码场景，我们引入了基于三元组的引用范式（TRP），通过三元结构明确地将视觉解码任务中的三个关键维度解耦：概念、解码类型和目标。TRP采用符号分隔符来强制进行结构化表示学习，增强了模型输出的可解析性和可解释性。此外，我们构建了视觉任务指令遵循数据集（VTInstruct），这是一个包含超过1亿个跨25种任务类型的多模态对话样本的大规模多任务数据集。VT-Instruct除了文本输入和输出外，还包含了各种视觉提示，如点、框、涂鸦和遮罩，并生成由文本和视觉单元（如框、关键点、深度和遮罩）组成的输出。不同视觉提示和视觉单元的组合产生了广泛多样的任务类型，显著扩大了REF-VLM的应用范围。定性和定量实验表明，我们的REF-VLM在各种标准基准测试中优于其他MLLMs。代码、数据集和演示可在https://github.com/MacavityT/REF-VLM找到。**|
|**2025-03-10**|**Revisiting Noise in Natural Language Processing for Computational Social Science**|Nadav Borenstein et.al.|[2503.07395](http://arxiv.org/abs/2503.07395)|null|计算社会科学（CSS）是一个由研究人员利用人类生成内容前所未有的可用性所推动的新兴领域。然而，由于该领域所探讨的理论和数据集的性质，包括高度主观的任务和复杂的非结构化文本语料库，它呈现了一系列独特的挑战。在这些挑战中，一个研究较少的话题是噪声的普遍存在。本论文旨在通过提出一系列相互关联的案例研究来填补文献中的这一空白，这些案例研究考察了CSS中噪声的不同表现形式。这包括历史记录OCR处理后的字符级错误、古语、主观和模糊任务注释的不一致性，以及在大规模语言模型内容生成过程中引入的噪声和偏见。本论文挑战了CSS中的噪声本质上是有害或无用的传统观念。相反，它认为某些形式的噪声可以编码有意义的信息，这些信息对于推进CSS研究至关重要，例如个人的独特沟通风格或数据集和任务的文化依赖性。此外，本论文强调了处理噪声的细微差别的重要性，以及CSS研究人员在遇到噪声时必须考虑的因素，表明不同类型的噪声需要不同的策略。|
|**2025-03-07**|**Understanding the Limits of Lifelong Knowledge Editing in LLMs**|Lukas Thede et.al.|[2503.05683](http://arxiv.org/abs/2503.05683)|null|保持大型语言模型在事实上的实时更新对于部署至关重要，但昂贵的重新训练仍然是一个挑战。知识编辑提供了一种有前景的替代方案，但现有方法仅在小型或合成编辑基准上进行测试。在本研究中，我们旨在将终身知识编辑的研究与实际相关规模的现实世界编辑相连接。我们首先介绍了WikiBigEdit；这是一个构建用于自动扩展以实现未来基准的大规模现实世界WikiData编辑基准。在其首次实例中，它包括超过50万个问答对用于知识编辑，并附带一个全面的评估流程。最后，我们利用WikiBigEdit来研究现有知识编辑技术整合大量现实世界事实的能力，并对比它们的性能与检索增强和持续微调等通用修改技术，以全面了解当前终身知识编辑的实际范围。|
|**2025-03-07**|**A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval**|Yu Zhang et.al.|[2503.05659](http://arxiv.org/abs/2503.05659)|**[link](https://github.com/tsinghua-fib-lab/llm-agent-for-recommendation-and-search)**|信息技术深刻改变了人类与信息互动的方式。在线上创造的、共享和传播的大量内容使得获取相关信息变得越来越困难。在过去二十年里，搜索和推荐系统（统称为信息检索系统）已经显著发展，以应对这些挑战。最近大型语言模型（LLMs）的进展展示了超越人类在多种语言相关任务上的表现，并展现出一般理解、推理和决策能力。本文探讨了大型语言模型代理在增强搜索和推荐系统中的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来阐述现有研究。我们强调了LLM代理在解决搜索和推荐当前挑战方面的巨大潜力，并对未来研究方向提供了洞见。本文首次系统地回顾和分类了这些领域中LLM代理的研究，为利用这一先进的AI技术进行信息检索提供了新颖的视角。为了帮助理解现有工作，我们列出了关于基于代理的模拟和大型语言模型的相关论文，链接如下：https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search。|
|**2025-03-07**|**Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings**|Xuanqing Liu et.al.|[2503.05620](http://arxiv.org/abs/2503.05620)|null|大型语言模型（LLMs）在处理复杂对话任务时展现出令人瞩目的能力，无需针对特定用例进行微调。然而，实时分析实时对话需要低延迟处理系统，由于延迟限制，因此不切实际地部署具有数十亿参数的模型。因此，从业者通常更倾向于使用参数数量以百万计的较小模型，这些模型是在高质量、人工标注的数据集上训练的。然而，制作这样的数据集既耗时又昂贵。因此，越来越有必要将LLM生成的标签的可扩展性与人工标注的精确性相结合，使微调后的较小模型既能实现更高的速度，又能达到与较大模型相当精度。在本文中，我们介绍了一个简单而有效的框架来解决这一挑战。我们的方法专门针对逐句分类问题，包括意图检测、对话状态跟踪等任务。为了减轻LLM标注错误的影响——这是学生模型不准确的主要来源——我们提出了一个噪声减少的偏好学习损失。实验结果表明，我们的方法显著提高了句子级别对话任务的准确性，包括情感检测（超过2%）、对话动作分类（超过1.5%）等。|
|**2025-03-07**|**A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models**|Dong Shu et.al.|[2503.05613](http://arxiv.org/abs/2503.05613)|null|大型语言模型（LLMs）彻底改变了自然语言处理，但它们的内部机制仍然很大程度上不透明。最近，机制可解释性作为理解LLMs内部工作原理的一种手段，引起了研究界的广泛关注。在各种机制可解释性方法中，稀疏自编码器（SAEs）因其能够将LLMs内部复杂叠加的特征分解为更可解释的组成部分而成为特别有前景的方法。本文全面审视了SAEs作为解释和理解LLMs的有前景方法。我们提供了对SAE原理、架构和应用的系统概述，这些内容专门针对LLM分析，涵盖了理论基础、实现策略和稀疏机制的最新进展。我们还探讨了如何利用SAEs来解释LLMs的内部工作原理，引导模型行为朝向期望的方向，并为未来的模型开发更透明的训练方法。尽管SAE的实现和扩展仍存在挑战，但它们继续为理解大型语言模型的内部机制提供了有价值的工具。|
|**2025-03-07**|**R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning**|Huatong Song et.al.|[2503.05592](http://arxiv.org/abs/2503.05592)|null|现有的大型推理模型（LRMs）展示了强化学习（RL）增强大型语言模型（LLMs）复杂推理能力的潜力。虽然它们在数学和编码等具有挑战性的任务上取得了显著的成绩，但它们通常依赖内部知识来解决问题，这可能在时间敏感或知识密集型问题中不足，导致不准确和幻觉。为了解决这个问题，我们提出了\textbf{R1-Searcher}，一种新颖的两阶段基于结果的RL方法，旨在增强LLMs的搜索能力。这种方法允许LLMs在推理过程中自主调用外部搜索系统以获取额外的知识。我们的框架完全依赖于RL，无需冷启动时的过程奖励或蒸馏。%有效地泛化到域外数据集，并支持Base和Instruct模型。我们的实验表明，我们的方法在性能上显著优于之前的强大RAG方法，甚至与闭源的GPT-4o-mini相比也是如此。|
|**2025-03-07**|**Evaluating open-source Large Language Models for automated fact-checking**|Nicolo' Fontana et.al.|[2503.05565](http://arxiv.org/abs/2503.05565)|null|随着在线虚假信息的日益普遍，对自动化事实核查解决方案的需求不断上升。大型语言模型（LLMs）已作为协助这项任务的潜在工具出现，但它们的有效性尚不确定。本研究评估了各种开源LLMs的事实核查能力，重点关注它们评估不同程度语境信息的能力。我们进行了三个关键实验：(1)评估LLMs是否能够识别一个断言与事实核查文章之间的语义关系；(2)评估模型在给定相关事实核查文章时验证断言的准确性；(3)测试LLMs利用来自外部知识源（如Google和Wikipedia）的数据进行事实核查的能力。我们的结果表明，LLMs在识别断言与文章的联系以及验证经过事实核查的故事方面表现良好，但在确认事实新闻方面表现不佳，被如RoBERTa等传统微调模型超越。此外，引入外部知识并未显著提高LLMs的性能，这要求更定制化的方法。我们的发现突出了LLMs在自动化事实核查中的潜力和局限性，强调了在它们能够可靠地取代人工核查员之前，需要进一步优化。|
|**2025-03-07**|**Revitalizing Saturated Benchmarks: A Weighted Metric Approach for Differentiating Large Language Model Performance**|Bryan Etzine et.al.|[2503.05551](http://arxiv.org/abs/2503.05551)|null|现有的基准测试正变得饱和，并且由于数据污染和大型语言模型（LLM）能力的提升等因素，难以区分模型性能。本文介绍了一种新的加权指标EMDM（增强模型区分度度量），通过增强模型区分能力来激活基准测试。EMDM结合了最终答案和思维链（CoT）推理的正确性，根据解决评估数据中给定样本所需的复杂性和推理深度分配权重。使用基线LLM在两种设置中——无指导，即模型对测试样本没有先前的接触，和有指导，即模型对期望答案有先验知识——EMDM区分了不同难度的实例。这些设置中的CoT和答案正确性为权重分配的优化目标提供了信息，从而实现了对模型性能的更细致评估。与在ARC-Challenge上实现17%区分度的精确匹配（EM）指标相比，EMDM实现了46%的区分度，证明了其在根据推理和知识需求区分模型方面的有效性。|
|**2025-03-07**|**Leveraging Approximate Caching for Faster Retrieval-Augmented Generation**|Shai Bergman et.al.|[2503.05530](http://arxiv.org/abs/2503.05530)|null|检索增强生成（RAG）通过整合外部知识提高了大型语言模型（LLM）答案的可靠性。然而，RAG由于从大型向量数据库中查找相关文档的计算成本高昂，增加了端到端推理时间。为了解决这个问题，我们引入了Proximity，这是一个近似键值缓存，通过利用用户查询中的相似性来优化RAG工作流程。Proximity不是独立地处理每个查询，当出现相似查询时，它会重用之前检索到的文档，从而减少对昂贵的向量数据库查找的依赖。我们在MMLU和MedRAG基准测试上评估了Proximity，证明它显著提高了检索效率，同时保持了响应的准确性。Proximity将检索延迟减少了高达59%，同时保持了准确性和降低了向量数据库的计算负担。我们还实验了不同的相似性阈值，并量化了速度和召回率之间的权衡。我们的研究表明，近似缓存是优化基于RAG系统的可行且有效的策略。|
|**2025-03-07**|**PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs**|Roberto Cerina et.al.|[2503.05529](http://arxiv.org/abs/2503.05529)|null|本文介绍了一种名为PoSSUM的开源协议，该协议通过多模态大型语言模型（LLMs）对社交媒体用户进行无干扰式调查。PoSSUM利用用户的实时帖子、图片和其他数字痕迹来创建硅样本，以捕捉LLMs训练数据中不存在的信息。为了获得具有代表性的估计值，PoSSUM采用具有结构化先验的分层回归和后分层（MrP）方法来抵消社交媒体平台可观察到的选择偏差。该协议在2024年美国总统选举期间进行了验证，期间进行了五次PoSSUM调查，并在GitHub和X上发布。在最后一次调查中，于10月17日至26日进行，使用1054名X用户的合成样本，PoSSUM准确预测了51个州中的50个州的结果，并将共和党候选人获胜的概率定为0.65。值得注意的是，它还表现出比大多数现有调查机构更低的州级偏差。这些结果证明了PoSSUM作为传统调查方法全自动、无干扰替代品的潜力。|
|**2025-03-07**|**Cognitive Bias Detection Using Advanced Prompt Engineering**|Frederic Lemieux et.al.|[2503.05516](http://arxiv.org/abs/2503.05516)|null|认知偏差，即判断中的理性偏离，在生成客观内容时带来了重大挑战。本文介绍了一种使用大型语言模型（LLMs）和高级提示工程技术的实时认知偏差检测的新方法。所提出的系统通过分析文本数据来识别常见的认知偏差，如确认偏差、循环推理和隐藏假设。通过设计定制化的提示，系统有效地利用LLMs的能力来识别和减轻这些偏差，从而提高人类生成内容的质量（例如新闻、媒体、报告）。实验结果表明，我们的方法在识别认知偏差方面具有高准确性，为增强内容客观性和减少偏见决策风险提供了一种有价值的工具。|
|**2025-03-06**|**L $^2$M: Mutual Information Scaling Law for Long-Context Language Modeling**|Zhuo Chen et.al.|[2503.04725](http://arxiv.org/abs/2503.04725)|**[link](https://github.com/LSquaredM/mutual_info_scaling_law)**|我们严格地建立了一个自然语言中关于长距离依赖的双边互信息缩放定律。这个缩放定律，我们证明了它与传统的两点互信息不同，并且独立缩放，是理解长上下文语言模型的关键。利用这个缩放定律，我们提出了长上下文语言建模（L$^2$ M）条件，它将模型有效建模长上下文长度的能力与其存储过去信息的潜在状态大小缩放联系起来。我们的结果通过在Transformer和状态空间模型上的实验得到了验证。这项工作为大型语言模型向更长上下文长度的发展奠定了理论基础。|
|**2025-03-06**|**Shifting Long-Context LLMs Research from Input to Output**|Yuhao Wu et.al.|[2503.04723](http://arxiv.org/abs/2503.04723)|null|近期在长上下文大型语言模型（LLMs）方面的进步主要集中在处理扩展输入上下文上，从而在长上下文理解方面取得了显著进步。然而，生成长文本这一同样关键方面相对较少受到关注。本文倡导在自然语言处理（NLP）研究中实现范式转变，以解决长输出生成方面的挑战。如新颖写作、长期规划和复杂推理等任务需要模型理解广泛上下文并产生连贯、上下文丰富且逻辑一致的扩展文本。这些需求凸显了当前LLMs能力中存在的关键差距。我们强调这一未被充分探索领域的的重要性，并呼吁集中精力开发针对生成高质量长文本的基础LLMs，这对于现实世界的应用具有巨大潜力。|
|**2025-03-06**|**Enough Coin Flips Can Make LLMs Act Bayesian**|Ritwik Gupta et.al.|[2503.04722](http://arxiv.org/abs/2503.04722)|null|大型语言模型（LLMs）展现出在输入提示中根据少量示例进行泛化的能力，这种新兴能力被称为情境学习（ICL）。我们研究了LLMs是否利用ICL以与贝叶斯框架一致的方式或依赖于模式匹配来进行结构化推理。使用有偏硬币抛掷的受控环境，我们发现：（1）LLMs通常具有有偏先验，导致零样本设置中的初始偏差，（2）情境证据比显式的偏差指令更为重要，（3）LLMs普遍遵循贝叶斯后验更新，偏差主要源于未校准的先验而不是更新错误，以及（4）注意力大小对贝叶斯推理的影响可以忽略不计。通过足够的情境学习有偏硬币抛掷的演示，LLMs以贝叶斯方式更新其先验。|
|**2025-03-06**|**Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining**|Houyi Li et.al.|[2503.04715](http://arxiv.org/abs/2503.04715)|null|大型语言模型（LLMs）在众多任务上的强大能力现已得到充分证实，但它们的有效部署需要仔细的超参数优化。通过广泛的实证研究，包括对各种配置进行网格搜索，我们发现这些超参数遵循普遍的缩放定律：最佳学习率与模型参数和数据大小都呈幂律关系，而最佳批量大小主要与数据大小相关。我们的分析揭示了在固定模型和数据大小条件下，超参数存在凸优化景观。这种凸性意味着存在一个最佳超参数平台期。我们为社区贡献了一个通用、即插即用的最佳超参数工具。其在测试集上的估计值仅比通过穷举搜索找到的全局最佳LLM性能低0.07%。这些定律在各种模型稀疏性、训练数据分布和模型形状的变化中表现出显著的鲁棒性。据我们所知，这是第一个统一不同模型形状和结构的工作，如专家混合模型和密集型变压器，并在各种数据分布中建立了最佳超参数缩放定律。这一全面的优化过程需要大量的计算资源，使用了近一百万个NVIDIA H800 GPU小时从头开始训练了3700个不同大小和超参数的LLMs，总共消耗了大约1000万亿个token。为了促进可重复性和进一步的研究，我们将逐步通过我们的指定仓库https://step-law.github.io/发布所有损失测量和模型检查点。|
|**2025-03-06**|**Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size**|Alireza Behtash et.al.|[2503.04704](http://arxiv.org/abs/2503.04704)|null|我们提出了一种新颖的模型量化方法，该方法超越了针对特定架构和大小依赖的压缩方法对大型语言模型（LLMs）的限制，采用熵加权量化（EWQ）。通过分析Transformer块之间的熵分布，EWQ确定哪些块可以在不造成显著性能下降的情况下安全地进行量化，而与模型架构或大小无关。我们的方法优于均匀量化方法，在保持与未量化模型0.5%以内的MMLU准确率的同时，将内存使用量减少高达18%。我们展示了EWQ在多个架构上的有效性——从16亿到700亿参数——并展示了在模型规模或架构设计方面的一致的质量-压缩权衡改进。EWQ的一个令人惊讶的发现是，与未量化模型相比，它能够降低困惑度，这表明通过选择性精度降低存在有益的正则化。这种改进适用于不同的模型家族，表明层级熵与最佳精度需求之间存在基本关系。此外，我们引入了FastEWQ，这是一种快速进行熵分布分析的方法，消除了加载模型权重的需求。这项技术利用了熵分布的通用特性，这些特性在各种架构和规模中持续存在，使得量化决策几乎瞬间完成，同时保持80%的分类准确率并进行全面熵分析。我们的结果表明，有效的量化策略可以独立于特定的架构选择或模型大小进行开发，为高效的LLM部署开辟了新的可能性。|
|**2025-03-06**|**UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets**|Wenyu Wang et.al.|[2503.04693](http://arxiv.org/abs/2503.04693)|null|大型语言模型（LLMs）在训练过程中不可避免地会获取有害信息。LLM 的反学习旨在消除这些有害信息的影响，同时保持模型的总体性能。现有的反学习方法，以基于梯度上升的方法为代表，主要关注遗忘目标数据，而忽视了逻辑相关知识对反学习有效性的关键影响。在本文中，通过理论和实验分析，我们首先证明反学习性能不佳的关键原因是模型可以通过与逻辑相关知识的推理来重建目标内容。为了解决这个问题，我们提出了通过参数外推改进反学习（UIPE）的方法，该方法移除与遗忘目标高度相关的知识。实验结果表明，UIPE显著提高了TOFU基准上各种主流LLM反学习方法的性能。|
|**2025-03-06**|**Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases**|Pengcheng Qiu et.al.|[2503.04691](http://arxiv.org/abs/2503.04691)|null|最新的推理增强大型语言模型（推理LLM），如DeepSeek-R1和OpenAI-o3，已经取得了显著的成功。然而，将这些推理增强应用于高度专业的医学领域尚未得到明确评估，尤其是不仅评估最终生成的结果，还要检查其推理过程的质量。在本研究中，我们提出了MedR-Bench，这是一个以推理为重点的医学评估基准，包含从病例报告中挖掘的1,453个结构化病例以及推理参考。我们的基准涵盖了13个身体系统和10种专业疾病，包括常见和罕见疾病。在我们的评估中，我们引入了一个通用的框架，包括三个关键的临床阶段：评估推荐、诊断决策和治疗计划，全面捕捉LLM在整个患者医疗旅程中的表现。对于指标，我们提出了一种新的代理系统，即推理评估器，旨在以可扩展的方式自动化和客观量化自由文本推理响应，从效率、事实性和完整性等方面动态搜索和执行交叉引用检查。因此，我们评估了包括DeepSeek-R1、OpenAI-o3-mini在内的五项最先进的推理LLM。我们的结果表明，当前的LLM可以处理相对简单的诊断任务，并取得了足够的批判性评估结果，准确率一般超过85%。然而，它们在更复杂的任务上，如评估推荐和治疗计划方面，仍然存在困难。在推理方面，它们的推理过程通常可靠，事实性得分超过90%，尽管它们经常省略关键的推理步骤。我们的研究清楚地揭示了当前临床LLM的进一步发展方向。|
|**2025-03-06**|**LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue**|Sangyeop Kim et.al.|[2503.04675](http://arxiv.org/abs/2503.04675)|null|理解用户对对话系统的满意度，即用户满意度估计（USE），对于评估对话质量和提升用户体验至关重要。然而，现有用户满意度估计方法由于对用户不满意原因的理解有限以及标注用户意图的高成本而面临挑战。为了解决这些挑战，我们提出了PRAISE（用于可解释满意度估计的计划与检索对齐），这是一个用于有效用户满意度预测的可解释框架。PRAISE通过三个关键模块运作。策略规划器开发策略，这些策略是用于分类用户满意度的自然语言标准。特征检索器随后结合大型语言模型（LLMs）对用户满意度的知识，并从话语中检索相关特征。最后，评分分析器评估策略预测并分类用户满意度。实验结果表明，PRAISE在三个USE任务基准上实现了最先进的性能。除了卓越的性能外，PRAISE还提供了额外的好处。它通过有效地将话语与策略对齐，提供了实例级解释，从而增强了可解释性。此外，PRAISE比现有方法更高效，因为它在推理阶段无需使用LLMs。|
|**2025-03-06**|**Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment**|Wen Yang et.al.|[2503.04647](http://arxiv.org/abs/2503.04647)|**[link](https://github.com/znlp/implicit-cross-lingual-rewarding)**|直接偏好优化（DPO）已成为将大型语言模型（LLMs）与人类偏好对齐的一种突出方法。虽然DPO使英语LLMs的对齐取得了显著进展，但多语言偏好对齐受到数据稀缺性的阻碍。为了解决这个问题，我们提出了一种新的方法，通过隐式奖励从对齐良好的英语模型中捕获学习到的偏好，并通过迭代训练将这些偏好转移到其他语言。具体来说，我们从英语DPO对齐模型的logits及其对应参考模型的logits中推导出一个隐式奖励模型。然后，利用这个奖励模型在跨语言指令遵循对中标注偏好关系，使用英语指令评估多语言响应。随后，这些标注数据用于多语言DPO微调，促进从英语到其他语言的偏好知识迁移。对Llama3进行两次迭代微调后，在X-AlpacaEval排行榜上，所有训练语言的平均胜率提高了12.72%，长度控制胜率提高了5.97%。我们的研究结果表明，利用现有的英语对齐模型可以实现高效的多语言偏好对齐，显著减少了大量多语言偏好数据的需要。代码可在https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding上找到。|
|**2025-03-06**|**Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking**|Yijie Xu et.al.|[2503.04636](http://arxiv.org/abs/2503.04636)|null|随着开源大型语言模型（LLMs）如Llama3的能力增强，开发水印技术以检测其潜在滥用变得至关重要。现有的水印方法要么在LLM推理期间添加水印，这不适合开源LLMs，要么主要针对分类LLMs而不是最近的生成LLMs。将这些水印适应开源LLMs以进行滥用检测仍然是一个开放性的挑战。本研究为开源LLMs定义了两种滥用场景：知识产权（IP）侵权和LLM使用违规。然后，我们探讨了推理时水印蒸馏和后门水印在这些场景中的应用。我们提出了全面的评估方法来评估各种现实世界的进一步微调场景对水印的影响以及这些水印对LLM性能的影响。我们的实验表明，后门水印可以有效地检测IP侵权，而推理时水印蒸馏适用于这两种场景，但相对于后门水印，对进一步微调的鲁棒性较低，并且对LLM性能的影响更大。探索更先进的水印方法以检测开源LLMs的滥用应该是未来一个重要的研究方向。|
|**2025-03-05**|**The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems**|Richard Ren et.al.|[2503.03750](http://arxiv.org/abs/2503.03750)|null|随着大型语言模型（LLMs）的能力增强和具有代理性，对其输出的信任需求显著增加，同时，人们越来越担心模型可能会为了追求目标而学会说谎。为了解决这些担忧，围绕LLMs中“诚实”概念的研究成果逐渐出现，同时也有旨在减轻欺骗行为的干预措施。然而，目前对诚实的评估非常有限，没有结合大规模和适用于所有模型的基准。此外，许多声称测量诚实的基准实际上只是以伪装的形式测量了准确性——即模型信念的正确性。在这项工作中，我们引入了一个大规模的人收集数据集，用于直接测量诚实，使我们第一次能够将准确性从诚实中分离出来。在一系列不同的LLMs中，我们发现，虽然较大的模型在我们的基准测试中获得了更高的准确性，但它们的诚实度并没有提高。令人惊讶的是，尽管大多数前沿LLMs在真实性基准测试中获得了高分，但我们发现，当被迫说谎时，前沿LLMs有很强的倾向性，导致在我们的基准测试中诚实度得分较低。我们发现，简单的方法，如表示工程干预措施，可以提高诚实度。这些结果强调了确保LLMs保持可信的稳健评估和有效干预措施的日益增长的需求。|
|**2025-03-05**|**Process-based Self-Rewarding Language Models**|Shimao Zhang et.al.|[2503.03746](http://arxiv.org/abs/2503.03746)|**[link](https://github.com/shimao-zhang/process-self-rewarding)**|大型语言模型在各种下游任务中表现出卓越的性能，并被广泛应用于多个场景。为了进一步提高LLMs的性能，使用人工标注的偏好数据进行训练，但这种训练受限于人类性能的极限。因此，提出了自我奖励方法，其中LLMs通过奖励自己的输出生成训练数据。然而，现有的自我奖励范式在数学推理场景中并不有效，甚至可能导致性能下降。在这项工作中，我们提出了基于过程的自我奖励流程，用于语言模型，该方法在自我奖励范式内引入了长期推理、逐步LLM作为评判者和逐步偏好优化。通过迭代的过程式自我奖励，我们的新范式成功提高了LLMs在多个数学推理基准测试上的性能，展示了自我奖励在实现可能超越人类能力的LLM推理方面的巨大潜力。|
|**2025-03-05**|**Towards Understanding Distilled Reasoning Models: A Representational Approach**|David D. Baek et.al.|[2503.03730](http://arxiv.org/abs/2503.03730)|null|本文研究了模型蒸馏对大型语言模型（LLMs）推理特征发展的影响。为了探索这一点，我们在Qwen系列模型及其微调变体上训练了一个交叉编码器。我们的结果表明，交叉编码器学习到了对应于各种推理类型的特征，包括自我反思和计算验证。此外，我们发现蒸馏模型包含独特的推理特征方向，这些方向可以被用来引导模型进入过度思考或敏锐思考模式。特别是，我们对四种特定的推理类别进行了分析：（a）自我反思，（b）演绎推理，（c）替代推理和（d）对比推理。最后，我们考察了蒸馏过程导致的特征几何变化，并发现迹象表明，更大的蒸馏模型可能发展出更结构化的表示，这与增强的蒸馏性能相关。通过揭示蒸馏如何改变模型，我们的研究有助于提高AI系统的透明度和可靠性。|
|**2025-03-05**|**Improving LLM Safety Alignment with Dual-Objective Optimization**|Xuandong Zhao et.al.|[2503.03710](http://arxiv.org/abs/2503.03710)|**[link](https://github.com/wicai24/door-alignment)**|现有的针对大型语言模型（LLMs）的训练时安全性对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO），一种广泛部署的对齐方法，在实验和理论环境中都表现出局限性，因为其损失函数对于拒绝学习来说不够优。通过基于梯度的分析，我们识别出这些不足，并提出了一种改进的安全性对齐方法，将DPO目标分解为两个部分：（1）鲁棒的拒绝训练，即使在产生部分不安全生成时也鼓励拒绝，以及（2）有针对性的有害知识遗忘。这种方法显著提高了LLMs对各种越狱攻击的鲁棒性，包括在分布内和分布外场景中的预填充、后缀和多轮攻击。此外，我们引入了一种方法，通过结合基于奖励的标记级加权机制来强调关键拒绝标记，从而进一步提高对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的标记分布变化以及拒绝和有害标记的内部表示相关，为LLMs安全性对齐的未来研究提供了有价值的方向。代码可在https://github.com/wicai24/DOOR-Alignment上找到。|
|**2025-03-05**|**Effective LLM Knowledge Learning via Model Generalization**|Mingkang Zhu et.al.|[2503.03705](http://arxiv.org/abs/2503.03705)|null|大型语言模型（LLMs）是在包含广泛世界知识的庞大文档上训练的。然而，关于知识如何通过自回归预训练获得，仍未得到充分理解。这种缺乏理解极大地阻碍了有效的知识学习，尤其是在对最新信息进行持续预训练时，因为这种不断变化的信息通常缺乏像基础知识那样的多样化重复。在本文中，我们专注于理解和改进LLMs的知识学习。我们发现并验证了LLMs的知识学习可以被视为自回归预训练目标中隐藏的隐式监督任务。我们的研究结果表明，LLMs的知识学习将受益于旨在提高监督任务泛化能力的方法。基于我们的分析，我们提出了基于格式的数据增强方法来增长分布内的样本，这种方法不会像文本释义那样改变文档中嵌入的事实。我们还引入了敏锐度感知最小化作为有效的优化算法，以更好地提高泛化能力。此外，我们的分析和方法可以很容易地扩展到指令微调。大量的实验结果验证了我们的发现，并证明了我们的方法在持续预训练和指令微调中的有效性。本文为解释和设计有效的LLMs知识学习策略提供了新的视角和见解。|
|**2025-03-05**|**A Practical Memory Injection Attack against LLM Agents**|Shen Dong et.al.|[2503.03704](http://arxiv.org/abs/2503.03704)|null|基于大型语言模型（LLMs）的智能体在众多复杂、现实世界的应用中展现出强大的能力。然而，当用于演示的历史记录被恶意篡改时，具有受损记忆库的LLM智能体可能会轻易产生有害输出。在本文中，我们提出了一种新颖的记忆注入攻击（MINJA），通过仅通过查询和输出观察与智能体交互，即可将恶意记录注入记忆库。这些恶意记录被设计成在执行受害者用户的查询时，引发一系列导致不希望智能体行为的恶意推理步骤。具体来说，我们引入了一系列桥梁步骤，将受害者查询与恶意推理步骤联系起来。在注入恶意记录的过程中，我们提出了一种指示提示，引导智能体自主生成我们设计的桥梁步骤。我们还提出了一种渐进式缩短策略，逐步移除指示提示，使得恶意记录在处理后续的受害者查询时容易被检索。我们针对不同智能体进行的广泛实验证明了MINJA在破坏智能体记忆方面的有效性。MINJA对执行的要求极低，使得任何用户都能影响智能体记忆，突显了LLM智能体的实际风险。|
|**2025-03-05**|**Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models**|Jiyue Jiang et.al.|[2503.03702](http://arxiv.org/abs/2503.03702)|null|高质量的数据资源在大型语言模型（LLMs）的学习中起着至关重要的作用，尤其是在像粤语这样的低资源语言中。尽管粤语有超过8500万母语使用者，但由于普通话的统治地位、粤语社区内部的缺乏凝聚力、字符编码和输入方法的多样性，以及海外粤语使用者倾向于使用英语的倾向，粤语在自然语言处理（NLP）领域仍被视为低资源语言。此外，粤语的丰富口语词汇、英语借词和语码转换特性增加了语料库收集和处理的复杂性。为了应对这些挑战，我们从各种来源收集粤语文本，包括开源语料库、香港特定论坛、维基百科和Common Crawl数据。我们通过语言过滤、质量过滤、内容过滤和去重步骤进行严格的数据处理，成功构建了一个超过20亿个标记的高质量粤语语料库，用于训练大型语言模型。我们进一步通过在精选的粤语任务上进行监督微调（SFT）来细化模型，增强了其处理特定应用的能力。训练完成后，该模型在四个粤语基准测试中实现了最先进的（SOTA）性能。在训练我们的数据集后，该模型在其他主流语言任务上也表现出改进的性能。|
|**2025-03-05**|**Addressing Overprescribing Challenges: Fine-Tuning Large Language Models for Medication Recommendation Tasks**|Zihao Zhao et.al.|[2503.03687](http://arxiv.org/abs/2503.03687)|**[link](https://github.com/zzhustc2016/lamo)**|药物治疗推荐系统因其能够根据患者的临床数据提供个性化的有效药物组合而在医疗保健领域受到关注。然而，现有方法在适应不同的电子健康记录（EHR）系统以及有效利用非结构化数据方面面临挑战，导致泛化能力有限和性能不佳。最近，利用大型语言模型（LLM）在医学领域的兴趣日益增长，以支持医疗保健专业人员和提高患者护理。尽管医学LLM已经出现，并在医学问答等任务中显示出有希望的结果，但它们在临床环境中的实际应用，特别是在药物治疗推荐方面，通常仍被低估。在本研究中，我们评估了通用和医学特定LLM在药物治疗推荐任务中的应用。我们的发现表明，LLM经常面临过度处方的挑战，导致临床风险增加和药物推荐准确性下降。为了解决这个问题，我们提出了语言辅助药物治疗推荐（LAMO），它采用参数高效的微调方法来调整开源LLM，以在药物治疗推荐场景中实现最佳性能。LAMO利用临床记录中的丰富临床信息，这是传统方法中经常未充分利用的资源。由于我们的方法，LAMO在内部验证准确性上优于先前最先进的方法超过10%。此外，时间和外部验证展示了LAMO在各种时间和医院环境中的强大泛化能力。此外，一个分布外的药物治疗推荐实验表明，即使是在训练数据之外的药物，LAMO的准确性也非常出色。|
|**2025-03-05**|**Attentive Reasoning Queries: A Systematic Method for Optimizing Instruction-Following in Large Language Models**|Bar Karov et.al.|[2503.03669](http://arxiv.org/abs/2503.03669)|**[link](https://github.com/emcie-co/parlant)**|我们提出了注意力推理查询（ARQs），这是一种新颖的结构化推理方法，通过领域专业化的推理蓝图显著提升了大型语言模型（LLMs）的指令遵循能力。虽然LLMs在众多任务上表现出卓越的能力，但它们在多轮对话中往往难以持续遵循复杂、特定用例的指令，这给业务关键应用带来了挑战。ARQs通过引导LLMs通过有针对性的查询进行系统化推理步骤，恢复关键指令并促进完成过程中的中间推理，来解决这一限制。在Parlant的广泛测试中，我们的框架为可靠的客户面向代理提供了可靠保障，其中ARQs因需而生，它们在87个测试场景中实现了90.2%的成功率，超过了思维链推理（86.1%）和直接响应生成（81.5%）。ARQs在处理持续失败的模式，如指南重新应用和幻觉预防方面表现出特别的优势。我们的分析还显示，精心设计的ARQs可能比自由形式推理在计算上更有效率。这些发现表明，结构化推理方法为控制LLMs在复杂场景中处理信息和做出决策提供了有效的机制。|
|**2025-03-05**|**Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction**|Gustaw Opiełka et.al.|[2503.03666](http://arxiv.org/abs/2503.03666)|**[link](https://github.com/gucioopielka/concept_vectors)**|类比推理依赖于概念抽象，但尚不清楚大型语言模型（LLMs）是否拥有这样的内部表示。我们探索了LLM激活的蒸馏表示，并发现函数向量（FVs；Todd等人，2024年）——适用于情境学习（ICL）任务的紧凑表示——对简单的输入变化（例如，开放式问题与多选题）不保持不变，这表明它们捕捉到的不仅仅是纯粹的概念。使用表征相似性分析（RSA），我们定位了一组编码不变概念向量（CVs）的注意力头，这些CVs针对如“反义词”等口头概念。这些CVs作为特征检测器独立于最终输出运作——这意味着模型可能形成正确的内部表示，但仍然产生错误的输出。此外，CVs可用于因果引导模型行为。然而，对于如“之前”和“之后”等更抽象的概念，我们没有观察到不变线性表示，这一发现与我们关联到LLMs在这些领域表现出的泛化问题。|
|**2025-03-04**|**Wikipedia in the Era of LLMs: Evolution and Risks**|Siming Huang et.al.|[2503.02879](http://arxiv.org/abs/2503.02879)|**[link](https://github.com/hsm316/llm_wikipedia)**|本文全面分析了大型语言模型（LLMs）对维基百科的影响，通过现有数据研究维基百科的演变，并使用模拟来探讨潜在风险。我们首先分析页面浏览量和文章内容，以研究维基百科的最新变化并评估LLMs的影响。随后，我们评估LLMs对与维基百科相关的各种自然语言处理（NLP）任务的影响，包括机器翻译和检索增强生成（RAG）。我们的发现和模拟结果表明，维基百科文章受到了LLMs的影响，某些类别的影响约为1%-2%。如果基于维基百科的机器翻译基准受到LLMs的影响，模型的得分可能会膨胀，模型之间的比较结果也可能发生变化。此外，如果知识库被LLM生成的内容污染，RAG的有效性可能会降低。虽然LLMs尚未完全改变维基百科的语言和知识结构，但我们相信我们的实证发现表明需要仔细考虑潜在的未来风险。|
|**2025-03-04**|**The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models**|Ke Ji et.al.|[2503.02875](http://arxiv.org/abs/2503.02875)|null|提高大型语言模型（LLMs）的推理能力通常需要使用标记数据的有监督微调或计算成本高昂的采样。我们引入了无监督前缀微调（UPFT），它利用了前缀自洽性的观察——即不同解决方案轨迹中共享的初始推理步骤——来提高LLM的推理效率。通过仅在初始前缀子串（多达8个标记）上训练，UPFT消除了对标记数据或穷尽采样的需求。在推理基准测试上的实验表明，UPFT的性能与有监督方法如拒绝采样微调相当，同时将训练时间减少了75%，采样成本减少了99%。进一步的分析显示，错误往往出现在推理过程的后期阶段，而基于前缀的训练保留了模型的结构知识。这项工作展示了最小无监督微调如何在大型语言模型中解锁显著的推理收益，为传统方法提供了一种可扩展且资源高效的替代方案。|
|**2025-03-04**|**Prompting Generative AI with Interaction-Augmented Instructions**|Leixian Shen et.al.|[2503.02874](http://arxiv.org/abs/2503.02874)|null|生成式人工智能（GenAI）模型的出现，包括大型语言模型和文本到图像模型，不仅在能力上出类拔萃，更重要的是，它们通过文本提示实现了直观的沟通方式。尽管这种方式直观，但基于文本的指令却受到自然语言模糊和冗余特性的困扰。为了解决这个问题，研究人员探索了通过促进精确和有效的人类意图表达来增强文本指令的交互方式，例如直接操作。然而，交互增强指令的设计策略缺乏系统性的研究，这阻碍了我们对它们的理解和应用。为了提供一个交互增强指令的全景，我们提出一个框架来分析相关工具，从为什么、何时、谁、什么以及如何应用交互来增强文本指令。值得注意的是，我们确定了应用交互的四个目的，包括限制、扩展、组织和细化文本指令。每个目的的设计范例也进行了总结，以帮助未来的研究人员和从业者。|
|**2025-03-04**|**FairSense-AI: Responsible AI Meets Sustainability**|Shaina Raza et.al.|[2503.02865](http://arxiv.org/abs/2503.02865)|null|本文介绍FairSense-AI：一个旨在检测和减轻文本和图像中偏见的跨模态框架。通过利用大型语言模型（LLMs）和视觉-语言模型（VLMs），FairSense-AI揭示了内容中可能出现的微妙偏见或刻板印象，为用户提供偏见得分、解释性高亮和自动化公平性改进建议。此外，FairSense-AI集成了与MIT AI风险存储库和国家标准与技术研究院（NIST）AI风险管理框架等框架相一致的AI风险评估组件，从而实现结构化识别伦理和安全问题。该平台通过模型剪枝和混合精度计算等技术优化能源效率，从而减少其环境影响。通过一系列案例研究和应用，我们展示了FairSense-AI如何通过解决公平性的社会维度以及大规模AI部署中对可持续性的迫切需求，来促进负责任的AI使用。https://vectorinstitute.github.io/FairSense-AI，https://pypi.org/project/fair-sense-ai/|
|**2025-03-04**|**Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt Aggregation Framework**|Ziang Zhou et.al.|[2503.02863](http://arxiv.org/abs/2503.02863)|null|大型语言模型（LLMs）通常表现出信心分数不匹配，通常高估其预测的可靠性。尽管对大型语言模型（LLMs）的口头信心已经引起了关注，但先前的研究在是否可以通过提示系统地引导信心分数上存在分歧。最近的研究甚至认为这种提示引起的信心转变是可以忽略的，暗示LLMs的信心校准对语言干预的响应是刚性的。与这些说法相反，我们首先通过在7个基准上对三个模型（包括GPT3.5、LLAMA3-70b、GPT4）进行探查，严格证实了方向性信心转变的存在，证明明确的指令可以以受控的方式增加或减少信心分数。基于这一观察，我们提出了一种包含三个组件的新框架：信心引导、引导信心聚合和引导答案选择，命名为SteeringConf。我们的方法SteeringConf利用一种信心操纵机制引导LLMs的信心分数向几个期望的方向转变，随后是一个汇总模块，它将引导的信心分数聚合起来以生成最终的预测。我们在7个基准上评估了我们的方法，它在信心校准和故障检测任务中的校准指标上始终优于基线。|
|**2025-03-04**|**Privacy and Accuracy-Aware AI/ML Model Deduplication**|Hong Guan et.al.|[2503.02862](http://arxiv.org/abs/2503.02862)|null|随着隐私保护机器学习算法，如差分隐私随机梯度下降（DP-SGD）的日益普及，在私有数据集上训练或微调模型变得越来越普遍。这种转变导致了对提供不同隐私保证和效用水平的模型的需求，以满足多样化的用户需求。然而，管理大量模型的多个版本带来了重大的运营挑战，包括增加推理延迟、更高的资源消耗和成本上升。模型去重是许多模型服务和数据库系统广泛使用的一种技术，以支持高性能和低成本推理查询和模型诊断查询。然而，现有的模型去重工作都没有考虑隐私问题，导致某些去重模型的隐私成本无限累积，并且当应用于去重DP训练的模型时效率低下。我们首次形式化了去重DP训练模型的难题，并提出了一种新颖的隐私和精度感知去重机制来解决这些问题。我们开发了一种贪婪策略，用于选择和分配基础模型给目标模型，以最小化存储和隐私成本。在去重目标模型时，我们动态安排精度验证，并应用稀疏向量技术来减少与私有验证数据相关的隐私成本。与不提供隐私保证的基线相比，我们的方法将单个模型（包括大型语言模型和视觉变压器）的压缩比提高了高达35倍。我们还观察到由于I/O操作的减少，推理速度提高了高达43倍。|
|**2025-03-04**|**Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers**|Zicong He et.al.|[2503.02851](http://arxiv.org/abs/2503.02851)|**[link](https://github.com/ziconghe2002/hcl-spark)**|大型语言模型（LLMs）被认为会出现幻觉，这种现象通常与创造力有关。虽然以往的研究主要通过理论或定性视角来探讨这种联系，但我们的工作采取了一种定量方法，系统地研究LLMs中幻觉与创造力之间的关系。鉴于创造力的复杂性质，我们提出了一个针对LLMs的狭义定义，并引入了一个评估框架HCL，该框架量化了在LLMs解码的不同层次上的幻觉和创造力。我们的实证分析揭示了幻觉与创造力之间在层深度、模型类型和模型大小上的一致性权衡。值得注意的是，在不同的模型架构中，我们识别出每个模型大小中一个特定的层，该层能够优化这种权衡。此外，这种最优层通常出现在大型模型的早期层，并且在此层模型的可信度也显著更高。这些发现提供了一个定量视角，为LLMs创造力和幻觉之间的相互作用提供了新的见解。我们的实验代码和数据可在https://github.com/ZicongHe2002/HCL-Spark上获取。|
|**2025-03-04**|**Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs**|Yuzhe Gu et.al.|[2503.02846](http://arxiv.org/abs/2503.02846)|**[link](https://github.com/open-compass/anah)**|大型语言模型（LLMs）在作为各个领域的AI助手时，会表现出幻觉（即不忠实或无意义的信息）。由于LLMs的回答中总是伴随着真实内容，因此之前的基于响应级偏好学习的事实性对齐方法在训练过程中不可避免地引入了噪声。因此，本文提出了一种基于直接偏好优化（DPO）的细粒度事实性对齐方法，称为Mask-DPO。Mask-DPO将句子级事实性作为掩码信号，仅从偏好样本中的事实性句子中学习，并防止对非偏好样本中的事实内容进行惩罚，从而解决了偏好学习中的歧义。大量的实验结果表明，Mask-DPO可以显著提高LLMs对域内和域外数据集问题的回答的事实性，尽管这些问题及其对应的话题在训练期间并未见过。仅在ANAH训练集上训练，Llama3.1-8B-Instruct在ANAH测试集上的得分从49.19%提高到77.53%，甚至超过了Llama3.1-70B-Instruct（53.44%）的得分，而在域外传记数据集上的FactScore也从30.29%提高到39.39%。我们进一步研究了Mask-DPO的泛化特性，使用了不同的训练样本缩放策略，发现缩放数据集中的主题数量比问题数量更有效。我们提出了关于事实性对齐如何作用于LLMs的假设，对其现象的含义进行了探讨，并进行了概念验证实验来验证它。我们希望该方法和研究结果为未来关于事实性对齐扩展的研究铺平道路。|
|**2025-03-04**|**AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation**|Songming Zhang et.al.|[2503.02832](http://arxiv.org/abs/2503.02832)|null|在现代大型语言模型（LLM）中，LLM的对齐至关重要，通常通过强化学习从人类反馈（RLHF）和直接偏好优化（DPO）等方法实现。然而，在大多数现有的LLM对齐方法中，响应中的所有标记都使用稀疏的、响应级别的奖励或偏好标注进行优化。忽视标记级别的奖励可能会导致错误地惩罚高质量标记或鼓励低质量标记，从而造成性能不佳和收敛速度慢。为了解决这个问题，我们提出了AlignDistil，这是一种用于标记级别奖励优化的RLHF等效蒸馏方法。具体来说，我们将DPO学习到的奖励引入RLHF目标，并从理论上证明了该目标与标记级别蒸馏过程的等价性，其中教师分布线性组合了DPO模型和参考模型的logits。在此基础上，我们通过构建一个正常和反向DPO模型的对比DPO奖励，进一步缩小了DPO模型奖励与纯奖励模型之间的精度差距。此外，为了避免对不同标记的欠优化和过优化，我们设计了一种标记自适应logit外推机制，为每个标记构建适当的教师分布。实验结果表明，我们的AlignDistil在性能上优于现有方法，并因其标记级别的分布性奖励优化而展示了快速收敛。|
|**2025-03-04**|**RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration**|Alicia Russell-Gilbert et.al.|[2503.02800](http://arxiv.org/abs/2503.02800)|null|在复杂工业环境中进行异常检测面临着独特的挑战，特别是在数据稀疏和运营条件不断变化的环境中。在这样的环境中进行预测性维护（PdM）需要适应性、可迁移性以及能够整合特定领域知识的方法论。在本文中，我们提出了一种名为RAAD-LLM的新颖框架，用于自适应异常检测，该框架利用了与检索增强生成（RAG）相结合的大型语言模型（LLMs）。这种方法解决了上述PdM挑战。通过有效利用特定领域知识，RAAD-LLM在不要求针对特定数据集进行微调的情况下，增强了时间序列数据中异常的检测。框架的自适应机制使其能够动态调整对正常运营条件的理解，从而提高检测精度。我们通过一个塑料制造厂的真实应用和Skoltech异常基准（SKAB）验证了这一方法。结果显示，在真实世界数据集上，与我们的先前模型相比，准确率从70.7提高到了89.1。通过允许输入序列数据与语义相结合，RAAD-LLM集成了多模态能力，这有助于模型和工厂操作员之间进行更协作的决策。总的来说，我们的发现支持了RAAD-LLM在PdM中革命化异常检测方法的能力，可能引领异常检测在各种行业中的实施方式发生范式转变。|
|**2025-02-28**|**LLM Post-Training: A Deep Dive into Reasoning Large Language Models**|Komal Kumar et.al.|[2502.21321](http://arxiv.org/abs/2502.21321)|**[link](https://github.com/mbzuai-oryx/awesome-llm-post-training)**|大型语言模型（LLMs）已经改变了自然语言处理领域，并带来了多样化的应用。在庞大的网络规模数据上的预训练为这些模型奠定了基础，但研究界现在越来越关注后训练技术以实现进一步的突破。虽然预训练提供了广泛的语言基础，但后训练方法使LLMs能够细化其知识、提高推理能力、增强事实准确性，并更有效地与用户意图和伦理考量保持一致。微调、强化学习和测试时缩放已成为优化LLMs性能、确保鲁棒性和提高各种实际任务适应性的关键策略。本文综述系统地探讨了后训练方法，分析了它们在超越预训练对LLMs进行细化中的作用，并解决了诸如灾难性遗忘、奖励黑客攻击和推理时间权衡等关键挑战。我们突出了模型对齐、可扩展适应和推理时间推理方面的新兴方向，并概述了未来的研究方向。我们还提供了一个公共仓库，以持续跟踪这个快速发展的领域的进展：https://github.com/mbzuai-oryx/Awesome-LLM-Post-training。|
|**2025-02-28**|**FANformer: Improving Large Language Models Through Effective Periodicity Modeling**|Yihong Dong et.al.|[2502.21309](http://arxiv.org/abs/2502.21309)|**[link](https://github.com/yihongdong/fanformer)**|周期性作为最重要的基本特征之一，为在人类学习范式内促进结构化知识获取和系统化认知过程奠定了基础。然而，Transformer中周期性建模的潜在缺陷影响了基于其构建的大语言模型（LLMs）的学习效率和从数据中建立基本原理的能力。在本文中，我们证明了整合有效的周期性建模可以提升LLMs的学习效率和性能。我们引入了FANformer，该模型将傅里叶分析网络（FAN）集成到注意力机制中，通过修改注意力机制的特征投影过程来实现高效的周期性建模。在语言建模上的大量实验结果表明，FANformer在模型规模扩大和训练标记增加时，始终优于Transformer，凸显了其卓越的学习效率。为进一步验证FANformer的有效性，我们在1万亿标记上预训练了一个FANformer-1B。与具有相似模型参数或训练标记的开源LLMs相比，FANformer-1B在下游任务上表现出显著改进。这些结果将FANformer定位为提升LLMs的有效且具有潜力的架构。|
|**2025-02-28**|**Contextualizing biological perturbation experiments through language**|Menghua Wu et.al.|[2502.21290](http://arxiv.org/abs/2502.21290)|**[link](https://github.com/genentech/perturbqa)**|高内涵扰动实验使科学家能够以前所未有的分辨率探测生物分子系统，但实验和分析成本构成了显著的障碍，限制了其广泛应用。机器学习有潜力指导高效地探索扰动空间并从这些数据中提取新的见解。然而，当前的方法忽略了相关生物学的语义丰富性，并且它们的目标与下游生物分析不一致。在本文中，我们假设大型语言模型（LLMs）是表示复杂生物关系和合理化实验结果的自然介质。我们提出了PerturbQA，这是一个关于扰动实验的结构化推理基准。与主要调查现有知识的现有基准不同，PerturbQA受到扰动建模开放问题的启发：预测未见扰动中的差异表达和方向变化，以及基因集富集。我们评估了建模扰动的最先进的机器学习和统计方法，以及标准的LLM推理策略，并发现当前方法在PerturbQA上的表现不佳。作为可行性的证明，我们引入了Summer（SUMMarize, retrievE, and answeR，一个简单、领域信息化的LLM框架，其性能与当前最先进水平相当或超过。我们的代码和数据在https://github.com/genentech/PerturbQA上公开可用。|
|**2025-02-28**|**Adaptive Keyframe Sampling for Long Video Understanding**|Xi Tang et.al.|[2502.21271](http://arxiv.org/abs/2502.21271)|null|多模态大型语言模型（MLLMs）通过将视觉输入作为额外标记注入大型语言模型（LLMs）作为上下文，实现了开放世界的视觉理解。然而，当视觉输入从单张图像变为长视频时，上述范式会遇到困难，因为大量视频标记已显著超过MLLMs的最大容量。因此，现有的基于视频的MLLMs大多建立在从输入数据中采样一小部分标记的基础上，这可能导致关键信息丢失，从而产生错误的答案。本文提出了一种简单而有效的算法，称为自适应关键帧采样（AKS）。它插入一个名为关键帧选择的即插即用模块，旨在通过固定数量的视频标记最大化有用信息。我们将关键帧选择定义为涉及（1）关键帧与提示之间的相关性以及（2）关键帧对视频的覆盖率的优化问题，并提出了一种自适应算法来近似最佳解决方案。在两个长视频理解基准测试上的实验验证了自适应关键帧采样在选择信息丰富的关键帧时提高了视频问答的准确性（优于强基线）。我们的研究揭示了信息预过滤在基于视频的MLLMs中的重要性。代码可在https://github.com/ncTimTang/AKS上获取。|
|**2025-02-28**|**RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete**|Yuheng Ji et.al.|[2502.21257](http://arxiv.org/abs/2502.21257)|null|近年来，多模态大型语言模型（MLLMs）在多种多模态环境中的能力显著提高。然而，它们在机器人场景中的应用，特别是在长期操作任务中，暴露出显著的局限性。这些局限性源于当前MLLMs缺乏三个关键的机器人大脑能力：规划能力，涉及将复杂操作指令分解成可管理的子任务； affordance感知，即识别和解释交互对象的属性；以及轨迹预测，即预测成功执行所需的完整操作轨迹的预见能力。为了从抽象到具体地增强机器人大脑的核心能力，我们引入了ShareRobot，这是一个高质量异构数据集，它对任务规划、对象affordance和末端执行器轨迹等多维信息进行标记。ShareRobot的多样性和准确性已由三位人类注释员精心调整。基于这个数据集，我们开发了RoboBrain，这是一个基于MLLM的模型，它结合了机器人和通用多模态数据，采用多阶段训练策略，并融合了长视频和高分辨率图像以提高其机器人操作能力。广泛的实验表明，RoboBrain在各种机器人任务中实现了最先进的性能，突显了其提升机器人大脑能力的潜力。|
|**2025-02-28**|**Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs**|Xiaomin Li et.al.|[2502.21239](http://arxiv.org/abs/2502.21239)|null|大型语言模型（LLMs）通过编码大量事实知识，在各种任务中展现出令人瞩目的性能。然而，它们仍然容易产生幻觉，生成错误或误导性信息，通常伴随着高不确定性。现有的幻觉检测方法主要关注量化模型内部的不确定性，这种不确定性源于模型内部缺失或冲突的知识。然而，幻觉也可能源于外部不确定性，即模糊的用户查询导致多种可能的解释。在本工作中，我们引入了语义体积（Semantic Volume），这是一种用于量化LLMs中外部和内部不确定性的新颖数学度量。我们的方法通过对查询和响应进行扰动，将它们嵌入到语义空间中，并计算嵌入向量的Gram矩阵的行列式，以此捕捉它们的分散性作为不确定性的度量。我们的框架提供了一种可泛化和无监督的不确定性检测方法，无需对LLMs进行白盒访问。我们在外部和内部不确定性检测方面进行了大量实验，证明我们的语义体积方法在这两项任务中均持续优于现有基线。此外，我们还提供了将我们的度量与微分熵联系起来的理论见解，统一并扩展了以前的基于采样的不确定性度量，如语义熵。语义体积被证明是一种稳健且可解释的方法，通过系统地检测用户查询和模型响应中的不确定性来提高LLMs的可靠性。|
|**2025-02-28**|**Transforming Tuberculosis Care: Optimizing Large Language Models For Enhanced Clinician-Patient Communication**|Daniil Filienko et.al.|[2502.21236](http://arxiv.org/abs/2502.21236)|null|推理步骤： 1. 确定翻译的领域是计算机科学，特别是涉及人工智能和医疗保健。 2. 识别摘要中的关键术语和概念，如“Tuberculosis”、“Large Language Model”、“digital adherence technology”、“patient engagement”等。 3. 保留专业术语的原意，同时确保翻译符合中文的表达习惯。 4. 将原文的句子结构和逻辑关系转换为中文表达。  中文翻译结果： 结核病（TB）是全球因传染病导致死亡的主要原因，在低收入和中等收入国家负担最重。在这些地区，有限的医疗保健获取和患者与提供者的高比例阻碍了有效的患者支持、沟通和治疗完成。为了填补这一差距，我们提出将一个专业的大型语言模型集成到一个有效的数字依从性技术中，以增强与治疗支持者的互动沟通。这种人工智能驱动的方案，在人工介入的框架下运行，旨在提高患者参与度并改善结核病治疗结果。|
|**2025-02-28**|**ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs**|Hao Ge et.al.|[2502.21231](http://arxiv.org/abs/2502.21231)|null|长上下文能力对大型语言模型（LLM）至关重要。为了在长上下文训练中将内存消耗分摊到多个设备上，通常会使用跨数据分区（即数据并行）和跨内部数据分区（即上下文并行）。当前的训练框架主要将这两种技术视为正交的，并建立静态通信组来组织设备作为静态网格（例如，二维网格）。然而，LLM训练的序列长度通常因文本、多模态或强化学习而异。数据异构性与静态网格之间的不匹配会导致冗余通信和不平衡的计算，从而降低训练效率。在本工作中，我们引入了ByteScale，这是一个高效、灵活且可扩展的LLM训练框架，用于大规模混合训练长序列和短序列。ByteScale的核心是一种新颖的并行策略，即混合数据并行（HDP），它通过动态网格设计将跨数据分区和跨内部数据分区统一。具体来说，我们构建了一个通信优化器，通过数据感知的分片和动态通信消除短序列的冗余通信，并通过选择性卸载进一步压缩长序列的通信成本。此外，我们还开发了一个平衡调度器，通过并行感知的数据分配来缓解不平衡的计算。我们使用7B到141B的模型大小、256K到2048K的上下文长度，在一个拥有超过12,000个GPU的生产集群上评估了ByteScale。实验结果表明，ByteScale的性能优于最先进的训练系统，最高可达7.89倍。|
|**2025-02-28**|**ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer**|Omer Goldman et.al.|[2502.21228](http://arxiv.org/abs/2502.21228)|null|为了在多种语言中实现公平的性能，多语言大型语言模型（LLMs）必须能够超越所学语言来抽象知识。然而，当前文献中缺乏可靠的方法来衡量LLMs跨语言知识迁移的能力。为此，我们提出了ECLeKTic，一个多语言闭卷问答（CBQA）数据集，以简单、黑盒的方式评估跨语言知识迁移。我们通过控制12种语言中维基百科文章的存在与否，检测到信息在语言间的覆盖不均。我们使用源语言生成寻求知识的问题，答案出现在相关的维基百科文章中，并将它们翻译成所有其他11种语言，这11种语言的维基百科缺乏相应文章。假设维基百科反映了LLMs训练数据中的突出知识，为了解决ECLeKTic的CBQA任务，模型需要在语言间迁移知识。通过实验8个LLMs，我们发现即使SOTA模型可以很好地预测在知识获取语言中的查询答案，它们仍然难以在语言间有效共享知识。|
|**2025-02-28**|**Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought**|Jianhao Huang et.al.|[2502.21212](http://arxiv.org/abs/2502.21212)|null|思维链（CoT）提示已被证明可以通过指导模型产生中间推理步骤，显著提高了大型语言模型（LLMs）的性能，尤其是在算术和推理任务中。尽管CoT在增强表达性方面取得了显著的实证成功和理论优势，但CoT训练背后的机制仍然在很大程度上未被探索。在本文中，我们研究了在用于线性回归的上下文权重预测任务上，Transformer在CoT目标下的训练动态。我们证明，一个没有CoT的单层线性Transformer只能实现梯度下降（GD）的单步，并且无法恢复真实的权重向量，而具有CoT提示的Transformer可以学习自主执行多步GD，实现近乎精确的恢复。此外，我们还表明，训练好的Transformer在未见数据上也能有效地泛化。使用我们的技术，我们还展示了循环Transformer在线性回归的上下文学习中的最终性能比没有循环的Transformer有显著提升。从实证角度来看，我们证明了CoT提示带来了实质性的性能提升。|
|**2025-02-27**|**R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts**|Zhongyang Li et.al.|[2502.20395](http://arxiv.org/abs/2502.20395)|**[link](https://github.com/tianyi-lab/R2-T2)**|在大型多模态模型（LMMs）中，对非语言模态（例如，视觉表示）的感知通常不如大型语言模型（LLMs）强大的推理能力，这阻碍了LMMs在具有挑战性的下游任务上的表现。这种弱点最近通过用专家混合（MoE）代替视觉编码器得到了缓解，MoE为各种下游任务提供了丰富、多粒度和多样化的表示。多模态MoE的性能很大程度上取决于其路由器，该路由器为每个输入重新加权并混合不同专家的表示。然而，我们发现端到端训练的路由器并不总是为每个测试样本产生最优的路由权重。为了弥合这一差距，我们提出了一种新颖且高效的方法“测试时重路由（R2-T2）”，该方法通过将路由权重向量移动到测试样本邻域内正确预测样本的向量，在测试时局部优化路由权重向量。我们提出了三种具有不同优化目标和邻域搜索空间的R2-T2策略。R2-T2在具有不同任务的挑战性基准测试中，持续且大幅提高了最先进的LMMs的性能，而不需要训练任何基础模型参数。|
|**2025-02-27**|**Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis**|Jeffrey Yang Fan Chiang et.al.|[2502.20383](http://arxiv.org/abs/2502.20383)|null|近年来，Web人工智能代理在处理复杂网络导航任务方面展现了令人瞩目的能力。然而，新兴研究表明，尽管这些代理和独立的超大语言模型（LLMs）都建立在相同的安全模型之上，但Web人工智能代理的脆弱性却比LLMs更大。考虑到Web人工智能代理相较于独立的LLMs具有更大的灵活性，这可能会导致它们面临更广泛的有敌意的用户输入。为了解决这些担忧，本研究调查了导致Web人工智能代理脆弱性增加的潜在因素。值得注意的是，这种差异源于Web人工智能代理与独立LLMs之间的多方面差异，以及复杂信号——通常是简单的评估指标，如成功率，所无法捕捉的细微差别。为了应对这些挑战，我们提出了一种组件级分析和更细致、系统的评估框架。通过这种精细的调查，我们确定了三个加剧Web人工智能代理脆弱性的关键因素；（1）将用户目标嵌入到系统提示中，（2）多步骤动作生成，以及（3）观察能力。我们的发现强调了在人工智能代理设计中增强安全和鲁棒性的紧迫需求，并为针对性的防御策略提供了可操作的见解。|
|**2025-02-27**|**Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers**|Shalev Lifshitz et.al.|[2502.20379](http://arxiv.org/abs/2502.20379)|null|通过在测试时利用更多的计算资源，大型语言模型（LLMs）可以在不额外训练的情况下提升性能。一种常见策略是使用验证器来评估候选输出。在这项工作中，我们提出了一个关于测试时计算的新尺度维度：调整验证器的数量。我们引入了多智能体验证（MAV）作为一种测试时计算的范式，该范式结合多个验证器来提升性能。我们提出使用方面验证器（AVs），这是现成的LLMs被提示来验证输出不同方面的选择，作为MAV系统中验证器的一种可能选择。AVs是MAV的一个方便的构建块，因为它们可以容易地组合而无需额外训练。此外，我们引入了BoN-MAV，这是一种简单的多智能体验证算法，它将n个最佳采样与多个验证器相结合。BoN-MAV显示出比自洽性和奖励模型验证更强的尺度模式，我们展示了从弱到强的泛化，即组合弱验证器甚至可以提升更强的LLMs，以及自我提升，即使用相同的基模型来生成和验证输出。我们的结果将调整验证器的数量确立为改善语言模型测试时性能的一个有希望的全新维度。|
|**2025-02-27**|**PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation**|Albert Gong et.al.|[2502.20377](http://arxiv.org/abs/2502.20377)|**[link](https://github.com/kilian-group/phantom-wiki)**|高质量的基准对于评估大型语言模型（LLMs）的推理和检索能力至关重要。然而，为这一目的收集数据集并非长久之计，因为它们容易发生数据泄露和夸大性能结果。为了解决这些挑战，我们提出了PhantomWiki：一个生成独特、事实一致的文档语料库以及多样化的问答对的工作流程。与先前的工作不同，PhantomWiki既不是一个固定的数据集，也不基于任何现有数据。相反，每个评估都会根据需要生成一个新的PhantomWiki实例。我们通过改变问题难度和语料库大小来分别分离推理和检索能力，并发现PhantomWiki数据集对前沿LLMs来说具有意想不到的挑战性。因此，我们贡献了一个可扩展且具有抗数据泄露能力的框架，用于分离评估推理、检索和工具使用能力。我们的代码可在https://github.com/kilian-group/phantom-wiki上获取。|
|**2025-02-27**|**Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization**|Ryan C. Barron et.al.|[2502.20364](http://arxiv.org/abs/2502.20364)|**[link](https://github.com/lanl/t-elf)**|代理生成式人工智能，通过检索增强生成（RAG）、知识图谱（KG）和向量存储（VS）等大型语言模型（LLMs）的支持，代表了一种在法律体系、研究、推荐系统、网络安全以及全球安全（包括扩散研究）等专门领域具有变革性的技术。该技术在推断巨大非结构化或半结构化数据集中的关系方面表现卓越。在此，法律领域包括具有广泛、相互关联且半结构化知识系统的复杂数据，包括宪法、法律、法规和案例法。从法律文件及其关系中提取见解和导航复杂网络对于有效的法律研究至关重要。在此，我们介绍了一种集成了RAG、VS和KG的生成式人工智能系统，这些KG是通过非负矩阵分解（NMF）构建的，以增强法律信息检索和人工智能推理，并最小化幻觉。在法律体系中，这些技术使人工智能代理能够识别和分析案件、法律和先例之间的复杂联系，揭示隐藏的关系并预测法律趋势——这些任务是确保正义和提高运营效率所必需的。我们的系统采用网络抓取技术，系统地从Justia等公开可访问的平台收集法律文本，如法律、宪法规定和案例法。通过利用高级语义表示、层次关系和潜在主题发现，它弥合了传统基于关键词的搜索和上下文理解之间的差距。这个框架支持法律文档聚类、摘要和交叉引用，为半结构化数据提供可扩展、可解释和准确的检索，同时推进计算法学和人工智能。|
|**2025-02-27**|**Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs**|Kuan Lok Zhou et.al.|[2502.20356](http://arxiv.org/abs/2502.20356)|null|大型语言模型（LLMs）在理解创意内容方面表现出显著的局限性，正如Hessel等人（2023年）在《纽约客》卡通标题竞赛（NYCCC）上的影响性研究所示。他们的研究揭示了LLMs与人类在幽默理解方面的巨大差距，并确立了理解和评估创意内容是AI发展中的一项关键挑战。我们通过将幽默理解分解为三个组成部分并系统地改进每个部分来重新审视这一挑战：通过改进标注来增强视觉理解，利用LLM生成的幽默推理和解释，以及实施与人类偏好数据的针对性对齐。我们的改进方法在标题排名上达到了82.4%的准确率，显著提高了之前的67%基准，并达到了该领域世界知名人类专家的表现水平。值得注意的是，尽管通过各种角色提示尝试模仿子群体偏好显示出最小的影响，但使用群体偏好进行模型微调却证明非常有效。这些发现揭示了通过针对特定子群体和个人的对齐，可以有效地解决LLMs在创意判断方面的局限性。最后，我们提出，实现人工通用智能需要系统地收集创意领域的人类偏好数据。我们主张，正如人类创造力深受个人和文化偏好的影响一样，用多样化的人类偏好数据训练LLMs可能是发展真正创意理解所必需的。|
|**2025-02-27**|**KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model**|Kai Zhang et.al.|[2502.20350](http://arxiv.org/abs/2502.20350)|null|药物发现是生物医学自然语言处理（NLP）中的关键任务，然而可解释的药物发现仍被研究不足。同时，大型语言模型（LLMs）在自然语言理解和生成方面表现出惊人的能力。利用LLMs进行可解释药物发现有望提高下游任务和实际应用。在本研究中，我们利用开源药物知识图谱、临床试验数据和PubMed出版物，构建了一个用于可解释药物发现任务的综合数据集，命名为expRxRec。此外，我们引入了KEDRec-LM，这是一种指令调整的LLM，能够从丰富的医学知识语料库中提炼知识，用于药物推荐和理由生成。为了鼓励该领域的进一步研究，我们将公开发布（附在此提交中）该数据集和KEDRec-LM。|
|**2025-02-27**|**Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models**|Yi Jing et.al.|[2502.20344](http://arxiv.org/abs/2502.20344)|null|大型语言模型（LLMs）在需要复杂语言能力的任务上表现出色，如指称消歧和隐喻识别/生成。尽管LLMs拥有令人印象深刻的性能，但它们处理和表示语言知识的内部机制仍然在很大程度上是不透明的。先前关于语言机制的研究受限于粗糙的粒度、不足的因果分析和狭窄的焦点。在本研究中，我们使用稀疏自动编码器（SAEs）进行了一次系统且全面的因果调查。我们从六个维度提取了广泛的语言特征：语音学、音韵学、形态学、句法、语义和语用学。我们通过构建最小对比数据集和反事实句子数据集来提取、评估和干预这些特征。我们引入了两个指标——特征表示置信度（FRC）和特征干预置信度（FIC）——以衡量语言特征捕捉和控制语言现象的能力。我们的结果表明，LLMs中存在固有的语言知识表示，并展示了控制模型输出的潜力。这项工作提供了强有力的证据，表明LLMs拥有真正的语言知识，并为未来研究中更具可解释性和可控性的语言建模奠定了基础。|
|**2025-02-27**|**Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners**|Daniele Paliotta et.al.|[2502.20339](http://arxiv.org/abs/2502.20339)|null|近期的研究表明，通过在测试时扩展计算资源，大型语言模型（LLMs）的性能可以得到显著提升。一种常见的策略是生成多个思维链（CoT）轨迹，并通过各种选择机制对这些轨迹的输出进行聚合。这引发了一个基本问题：具有较低复杂度的模型能否利用其优越的生成吞吐量，在固定的计算预算下超越同样规模的Transformer？为了回答这个问题并克服缺乏强亚二次推理器的问题，我们从预训练的Transformer中提炼出纯和混合的Mamba模型。我们的提炼模型仅训练了80亿个标记，在数学推理数据集上表现出强大的性能和可扩展性，同时在推理大批次和长序列时速度更快。尽管由于提炼导致的无样本性能下降，但纯和混合的Mamba模型在固定的时间预算下，其覆盖率和准确性性能仍可以超越其Transformer教师模型，为扩展推理计算开辟了新的方向。|
|**2025-02-27**|**Expertise Is What We Want**|Alan Ashworth et.al.|[2502.20335](http://arxiv.org/abs/2502.20335)|null|临床决策依赖于专家推理，这种推理受到标准化、基于证据的指南的指导。然而，将这些指南转化为自动化临床决策支持系统可能会出现不准确的情况，更重要的是，会失去细节的细微差别。我们分享了一个应用架构，即大型语言专家（LLE），它结合了大型语言模型（LLMs）的灵活性和强大功能，以及专家系统的可解释性、可说明性和可靠性。LLMs有助于解决专家系统的一些关键挑战，如整合和编纂知识以及数据标准化。反过来，类似于专家系统的方法有助于克服LLMs的挑战，包括幻觉、原子和低成本更新以及可测试性。  为了突出大型语言专家（LLE）系统的能力，我们构建了一个LLE系统，以帮助处理新诊断为癌症的患者的检查工作。及时开始癌症治疗对于最佳患者结果至关重要。然而，诊断建议的日益复杂使得初级保健医生难以确保在患者首次访问肿瘤学家之前，患者已经完成了必要的检查。与许多现实世界的临床任务一样，这些检查需要分析非结构化健康记录和运用细微的临床决策逻辑。在本研究中，我们描述了设计、评估一个旨在快速识别和推荐正确诊断检查的LLE系统。该系统展示了高度的 临床级别准确性（>95%）并有效地解决了来自大型学术中心乳腺癌和结直肠癌患者现实世界数据中识别出的差距。|
|**2025-02-26**|**Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing**|Akshat Gupta et.al.|[2502.19416](http://arxiv.org/abs/2502.19416)|null|这项研究探讨了大型语言模型（LLMs）局部更新对模型的影响，特别是在知识编辑的背景下——这是一项旨在不改变模型更广泛能力的情况下添加或修改特定事实的任务。首先，我们表明，在不同后训练干预措施中，如连续预训练、完全微调和基于LORA的微调，更新矩阵的Frobenius范数总是增加。这种增加的范数对局部知识编辑特别有害，因为在模型中只更新了部分矩阵。我们发现，包括微调、基于超网络的途径和定位-编辑方法在内的各种编辑技术都存在一个一致的现象：更新矩阵的范数随着连续更新而始终增加。这种增长扰乱了模型平衡，尤其是在孤立矩阵被更新而其余模型保持静态的情况下，可能导致潜在的稳定性和下游性能下降。在对中间激活向量进行更深入的研究后，我们发现内部激活的范数减小，并伴随着这些激活占据的子空间的转变，这表明这些激活向量现在在表示空间中占据的区域与未编辑模型完全不同。在本文中，我们突出了持续和局部顺序知识编辑的技术挑战及其对维持模型稳定性和有用性的影响。|
|**2025-02-26**|**Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs**|Dayu Yang et.al.|[2502.19411](http://arxiv.org/abs/2502.19411)|**[link](https://github.com/dayuyang1999/awesome-code-reasoning)**|在大型语言模型（LLMs）中，代码和推理相互强化：代码提供了一种抽象的、模块化的、以逻辑驱动的结构，支持推理；而推理将高级目标转化为更小、可执行的步骤，推动更高级的代码智能。在本研究中，我们考察了代码如何作为增强推理的结构化媒介：它提供了可验证的执行路径，强制执行逻辑分解，并使运行时验证成为可能。我们还探讨了推理的改进如何将代码智能从基本补全转变为高级能力，使模型能够通过规划和调试解决复杂的软件工程任务。最后，我们确定了关键挑战，并提出了未来研究方向以加强这种协同作用，最终提高LLMs在这两个领域的性能。|
|**2025-02-26**|**Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices**|Xinru Wang et.al.|[2502.19410](http://arxiv.org/abs/2502.19410)|null|大型语言模型（LLMs）在推荐日常行为作为个人人工智能助手方面展现出巨大的潜力，而可解释人工智能（XAI）技术正越来越多地被用于帮助用户理解为何给出某个推荐。然而，今天的个人人工智能助手通常位于超小设备上，如智能手表，这些设备屏幕空间有限。然而，LLM生成的解释的冗长性使得在如此超小的设备上提供可浏览的LLM解释变得具有挑战性。为了解决这个问题，我们探索了以下两个方面：1）在提示过程中，使用定义好的上下文组件对LLM的解释文本进行空间结构化；2）根据置信度水平向用户提供时间适应性解释。我们进行了一项用户研究，以了解这些方法如何影响用户在与超小设备上的LLM推荐和解释交互时的体验。结果显示，结构化解释减少了用户阅读解释时的时间和认知负荷。始终显示的结构化解释增加了用户对AI推荐的接受度。然而，由于缺乏足够和可读的细节，用户对结构化解释的满意度低于非结构化解释。此外，与始终显示的结构化解释相比，适应性呈现结构化解释在提高用户对AI的认识方面效果较差。结合用户的访谈反馈，这些结果导致了以下设计启示：在个性化超小设备上显示的LLM解释的内容和时机时，需要谨慎考虑。|
|**2025-02-26**|**ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models**|Danae Sánchez Villegas et.al.|[2502.19409](http://arxiv.org/abs/2502.19409)|null|推理图像序列对于多模态大型语言模型（MLLMs）来说仍然是一个挑战。尽管最近的一些模型在预训练期间纳入了多图像数据，但它们仍然难以识别序列结构，通常将图像视为独立处理。这项工作引入了ImageChain框架，通过将视觉序列建模为多轮对话来增强MLLMs在图像数据上的序列推理能力。在ImageChain中，图像与相应的文本描述交织在一起，形成一个受控对话，明确捕捉时间依赖和叙事进展。我们的方法针对下一个场景描述任务进行优化，即模型根据先前的视觉和文本提示生成对即将到来的场景的上下文感知描述。我们证明，我们的方法在下一个场景描述任务上的性能得到了提升——在SimRate指标上平均提高了3.7%至19%，该指标量化了与人工标注的地面真实情况的语义相似度。此外，ImageChain在从漫画到机器人学的各种应用中实现了鲁棒的零样本跨领域性能。广泛的实验验证了在多模态、多轮对话设计中进行指令调整是弥合静态图像理解与时间感知推理之间差距的关键。|
|**2025-02-26**|**Learning Code-Edit Embedding to Model Student Debugging Behavior**|Hasnain Heickal et.al.|[2502.19407](http://arxiv.org/abs/2502.19407)|null|在计算机科学教育中，为编程作业提供有效的反馈具有挑战性：学生通过迭代提交代码、执行代码和使用编译器或自动评分器的有限反馈进行调试来解决问题。分析学生在这一过程中的调试行为，可以揭示他们知识的重要见解，并告知更好的个性化支持工具。在这项工作中，我们提出了一种基于编码器-解码器的模型，该模型学习连续学生代码提交之间的有意义的代码编辑嵌入，以捕捉他们的调试行为。我们的模型利用有关学生代码提交是否通过每个测试用例的信息来微调大型语言模型（LLMs），以学习代码编辑表示。它能够提供个性化的下一步代码建议，在保持学生的编码风格的同时提高测试用例的正确性。我们的模型还使我们能够分析学生的代码编辑模式，使用聚类技术揭示常见的错误和调试行为。在真实世界学生代码提交数据集上的实验结果表明，我们的模型在代码重构和个性化代码建议方面表现出色，同时揭示了学生调试行为中的有趣模式。|
|**2025-02-26**|**General Reasoning Requires Learning to Reason from the Get-go**|Seungwook Han et.al.|[2502.19402](http://arxiv.org/abs/2502.19402)|null|大型语言模型（LLMs）展示了令人印象深刻的实际应用价值，体现了人工智能的有效性。然而，它们在自适应和鲁棒推理方面的能力——这是通用人工智能（AGI）的标志——仍然很脆弱。尽管LLMs在常识推理、编程和数学方面似乎取得了成功，但它们在将算法理解推广到新情境方面遇到了困难。我们通过在神秘编程语言中的算法任务进行的实验表明，LLM的推理过度拟合于训练数据，并且其在迁移性方面有限。我们认为，这种有限迁移性的根本问题是LLM中推理与知识的耦合。为了从AUI过渡到AGI，我们提出了通过三个关键方向来解开知识和推理的提议：（1）通过从头开始使用强化学习（RL）来训练推理，而不是广泛使用的下一个标记预测预训练；（2）使用合成任务的课程来减轻学习强化学习的推理先验，然后将其迁移到自然语言任务；（3）使用较小的上下文窗口来学习更通用的推理函数，以减少利用标记之间虚假的相关性。这样一个推理系统，结合一个训练好的检索系统以及作为知识存储的大型外部记忆库，可以在学习在新颖场景中进行推理时克服现有架构的几个限制。|
|**2025-02-26**|**TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding**|Max Ku et.al.|[2502.19400](http://arxiv.org/abs/2502.19400)|null|理解特定领域的定理往往需要不仅仅是基于文本的推理；通过结构化的视觉解释进行有效沟通对于深入理解至关重要。虽然大型语言模型（LLMs）在基于文本的定理推理方面表现出强大的性能，但它们生成连贯且具有教学意义的视觉解释的能力仍然是一个未解的挑战。在这项工作中，我们引入了TheoremExplainAgent，这是一种通过Manim动画生成长篇定理解释视频（超过5分钟）的代理方法。为了系统地评估多模态定理解释，我们提出了TheoremExplainBench，这是一个涵盖多个STEM学科240个定理的基准，包括5个自动评估指标。我们的结果表明，代理规划对于生成详细的长期视频至关重要，o3-mini代理的成功率为93.8%，总体得分为0.77。然而，我们的定量和定性研究表明，大多数生成的视频在视觉元素布局上存在一些小问题。此外，多模态解释揭示了基于文本的解释未能揭示的更深层次的推理缺陷，突出了多模态解释的重要性。|
|**2025-02-26**|**DataMan: Data Manager for Pre-training Large Language Models**|Ru Peng et.al.|[2502.19363](http://arxiv.org/abs/2502.19363)|null|大型语言模型（LLMs）的性能随着数据规模法则的出现而涌现，这使得选择预训练数据的重要性日益增加。然而，现有方法依赖于有限的启发式和人类直觉，缺乏全面和明确的指南。为了解决这个问题，我们受到“逆向思维”的启发——提示LLMs自我识别哪些标准有利于其性能。由于其预训练能力与困惑度（PPL）相关，我们从文本困惑度异常的原因中推导出14个质量标准，并引入15个常见应用领域以支持领域混合。在这篇论文中，我们训练了一个数据管理器（DataMan），使其能够从点对点评分中学习质量评分和领域识别，并使用它对447B个令牌的预训练语料库进行14个质量评分和领域类型的标注。我们的实验验证了我们的方法，使用DataMan选择30B个令牌来训练一个13亿参数的语言模型，这在与最先进的基线相比，在情境学习（ICL）、困惑度和指令遵循能力方面取得了显著的改进。表现最佳的模式，基于总评分l=5，超过了使用均匀采样训练的数据量多50%的模式。我们继续使用DataMan标注的高质量、特定领域的数据进行预训练，以增强特定领域的ICL性能，从而验证DataMan的领域混合能力。我们的发现强调了质量排名的重要性，质量标准的互补性及其与困惑度低的相关性，分析了PPL与ICL性能之间的不一致性。我们还对预训练数据集进行了彻底的分析，考察了其组成、质量评分的分布和原始文档来源。|
|**2025-02-26**|**Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?**|Yancheng He et.al.|[2502.19361](http://arxiv.org/abs/2502.19361)|**[link](https://github.com/openstellarteam/deltabench)**|近期，o1-like模型受到了广泛关注，这些模型通过生成长链式思维（CoT）推理步骤来提升现有大型语言模型（LLM）的推理能力。在本文中，为了理解这些长CoT的品质并衡量现有LLM对这些长CoT的批判能力，我们引入了DeltaBench，包括来自不同o1-like模型（例如，QwQ，DeepSeek-R1）为不同推理任务（例如，数学、代码、通用推理）生成的长CoT，以衡量检测长CoT推理中错误的能力。基于DeltaBench，我们首先对生成的长CoT进行细粒度分析，以发现不同o1-like模型的有效性和效率。然后，我们对现有过程奖励模型（PRMs）和批判模型进行了广泛的评估，以检测每个注释过程的错误，旨在研究现有PRMs和批判模型的边界和局限性。最后，我们希望DeltaBench能够引导开发者更好地理解他们模型的长期CoT推理能力。|
|**2025-02-26**|**Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets**|Tohida Rehman et.al.|[2502.19339](http://arxiv.org/abs/2502.19339)|null|文本摘要技术在自然语言处理中扮演着至关重要的角色，它可以将大量文本压缩成简洁且连贯的摘要。随着数字内容的快速增长和对有效信息检索需求的增加，文本摘要近年来已成为研究的热点。本研究对四种领先的预训练和开源大型语言模型进行了全面评估：BART、FLAN-T5、LLaMA-3-8B和Gemma-7B，评估数据集包括CNN/DM、Gigaword、新闻摘要、XSum和BBC新闻等五个多样化的数据集。评估采用了广泛认可的自动指标，包括ROUGE-1、ROUGE-2、ROUGE-L、BERTScore和METEOR，以评估模型生成连贯且信息丰富的摘要的能力。结果显示了这些模型在处理不同文本类型时的相对优势和局限性。|
|**2025-02-25**|**DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers**|Xueguang Ma et.al.|[2502.18460](http://arxiv.org/abs/2502.18460)|**[link](https://github.com/facebookresearch/dpr-scale)**|大型语言模型（LLMs）在作为稠密检索器进行微调时展现出了强大的效果和鲁棒性。然而，它们庞大的参数规模带来了显著的推理时间计算挑战，包括对大规模语料库的高编码成本和增加的查询延迟，限制了它们的实际部署。虽然较小的检索器提供了更好的效率，但它们在有限的监督微调数据下通常无法有效泛化。在本研究中，我们引入了DRAMA，这是一个利用LLMs来训练较小且可泛化的稠密检索器的训练框架。具体来说，我们采用修剪的LLMs作为骨干，并在单阶段对比学习设置中对LLM增强的多样化数据进行训练。实验表明，DRAMA比传统的基于编码器的检索器在多语言和长上下文方面提供了更好的能力，并在多个任务和语言上取得了强大的性能。这些结果突显了将较小检索器的训练与LLMs日益增长的进步联系起来，弥合效率与泛化之间的差距的潜力。|
|**2025-02-25**|**LLM-Based Design Pattern Detection**|Christian Schindler et.al.|[2502.18458](http://arxiv.org/abs/2502.18458)|null|检测不熟悉代码库中的设计模式实例是提高软件质量和可维护性的一个具有挑战性但至关重要的任务。传统的静态分析工具往往难以应对现实世界中模式实现所具有的复杂性、可变性和缺乏明确标注的特点。在本文中，我们提出了一种利用大型语言模型自动识别不同代码库中设计模式实例的新方法。我们的方法侧重于识别类在模式实例中所扮演的角色。通过提供对软件结构和意图的更清晰洞察，这项研究旨在支持开发者，提高理解力，并简化重构、维护和遵循最佳实践等任务。|
|**2025-02-25**|**FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response**|Mollie Shichman et.al.|[2502.18452](http://arxiv.org/abs/2502.18452)|null|大型语言模型（LLMs）具有强大的常识推理能力。然而，这些能力通常在大规模模型中才会出现。这意味着可以本地运行的较小模型在特定推理任务上的帮助和能力有限。为了满足我们的问题空间需求，我们将较小的LLMs微调到灾害领域，因为这些领域涉及复杂且低频的物理常识知识。我们引入了一个创建现场就绪指令解码代理（FRIDA）模型的流程，其中领域专家和语言学家结合他们的知识来制作高质量的种子数据，这些数据用于生成用于微调的合成数据。我们为合成生成创建了一套130条种子指令，一个包含25000条指令的合成数据集，以及119条与一般和地震特定物体功能相关的评估指令。我们微调了几个LLaMa和Mistral指令微调模型，并发现FRIDA模型在各种规模上都优于其基础模型。然后我们进行了一项消融研究，以了解哪种类型的合成数据最影响性能，并发现仅训练物理状态和物体功能常识知识就能超过基于所有数据的FRIDA模型。我们得出结论，FRIDA流程能够注入一般常识，但需要通过信息检索来增强特定领域知识。|
|**2025-02-25**|**SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution**|Yuxiang Wei et.al.|[2502.18449](http://arxiv.org/abs/2502.18449)|null|近期DeepSeek-R1的发布展示了强化学习（RL）在提升大型语言模型（LLM）的通用推理能力方面的巨大潜力。虽然DeepSeek-R1和其他后续工作主要关注将RL应用于竞争性编码和数学问题，但本文介绍了SWE-RL，这是第一个将基于RL的LLM推理扩展到现实世界软件工程的方案。利用轻量级的基于规则的奖励（例如，真实解决方案和LLM生成的解决方案之间的相似度得分），SWE-RL使LLM能够通过从广泛的开放源代码软件演化数据中学习，自主恢复开发者的推理过程和解决方案——这些数据记录了软件的整个生命周期，包括代码快照、代码变更以及问题、拉取请求等事件。在Llama 3之上训练的我们的推理模型，Llama3-SWE-RL-70B，在SWE-bench Verified（由人类验证的真实世界GitHub问题集合）上实现了41.0%的解决率。据我们所知，这是迄今为止中等规模（<100B）LLM的最佳性能，甚至可以与GPT-4o等领先专有LLM相媲美。令人惊讶的是，尽管仅在软件演化数据上执行RL，Llama3-SWE-RL甚至出现了泛化推理能力。例如，它在五个非领域任务上显示出改进的结果，即函数编码、库使用、代码推理、数学和通用语言理解，而监督微调基线甚至导致平均性能下降。总的来说，SWE-RL通过在大量软件工程数据上应用强化学习，为提高LLM的推理能力开辟了新的方向。|
|**2025-02-25**|**MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning**|Chanwoo Park et.al.|[2502.18439](http://arxiv.org/abs/2502.18439)|null|利用多个大型语言模型（LLMs）构建协作的多智能体工作流程已经显示出巨大的潜力。然而，大多数先前的研究集中在提示即插即用的LLMs，依赖于它们固有的协作能力，但最近的研究表明这可能不会提高LLMs的性能。在本文中，我们介绍了一种新的后训练范式MAPoRL（基于强化学习的多智能体后协同训练协作LLMs），以明确激发协作行为并进一步释放多智能体LLM框架的潜力。在MAPoRL中，多个LLMs首先独立生成自己的响应，并参与多轮讨论以协作改进最终答案。最后，MAPoRL验证器评估答案和讨论，通过分配一个分数来验证答案的正确性，同时增加激励以鼓励纠正和有说服力的讨论。这个分数作为协同训练奖励，然后通过多智能体强化学习来最大化。与现有的LLM后训练范式不同，MAPoRL提倡使用强化学习共同训练多个LLMs以实现更好的泛化。伴随着分析洞察，我们的实验表明，仅训练单个LLMs不足以诱导有效的协作。相反，多智能体协同训练可以提高跨基准的协作性能，并泛化到未见过的领域。|
|**2025-02-25**|**TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning**|Frederikus Hudi et.al.|[2502.18431](http://arxiv.org/abs/2502.18431)|**[link](https://github.com/fhudi/textgames)**|推理是大语言模型（LLMs）的一项基本能力，使其能够理解、分析和解决复杂问题。在这篇论文中，我们介绍了TextGames，这是一个创新的基准测试，专门用于通过要求高水平的模式识别、空间意识、算术和逻辑推理的文本游戏来评估LLMs。我们的分析探讨了LLMs在单轮和多轮推理中的表现，以及它们利用反馈通过自我反思来纠正后续答案的能力。我们的研究发现，尽管LLMs在处理大多数简单和中等难度的问题上表现出色，但在更难的任务上面临重大挑战。相比之下，在给予足够的时间的情况下，人类能够解决所有任务。此外，我们观察到LLMs在通过自我反思进行多轮预测时表现出改进，但它们仍然在序列、计数和持续遵循复杂规则方面存在困难。此外，针对推理进行优化的模型优于优先考虑指令遵循的预训练LLMs，这突出了推理技能在解决高度复杂问题中的关键作用。|
|**2025-02-25**|**OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference**|Xiangyu Zhao et.al.|[2502.18411](http://arxiv.org/abs/2502.18411)|**[link](https://github.com/phoenixz810/omnialign-v)**|近期开源的多模态大型语言模型（MLLMs）的进步主要集中在提升基础能力上，但在与人类偏好的对齐上仍存在较大差距。本文介绍OmniAlign-V，这是一个包含20万高质量训练样本的全面数据集，其中包括多样化的图像、复杂的问答以及不同的回答格式，以提升MLLMs与人类偏好的对齐度。我们还提出了MM-AlignBench，这是一个专门为评估MLLMs与人类价值观对齐度而设计的人标注基准。实验结果表明，使用OmniAlign-V对MLLMs进行微调，采用监督式微调（SFT）或直接偏好优化（DPO），可以显著提高与人类偏好的对齐度，同时在标准VQA基准测试中保持或提升性能，保留其基本能力。我们的数据集、基准、代码和检查点已在https://github.com/PhoenixZ810/OmniAlign-V发布。|
|**2025-02-25**|**Monte Carlo Temperature: a robust sampling strategy for LLM's uncertainty quantification methods**|Nicola Cecere et.al.|[2502.18389](http://arxiv.org/abs/2502.18389)|null|在大型语言模型（LLMs）中的不确定性量化（UQ）对于其安全可靠部署至关重要，尤其是在错误输出可能产生严重后果的关键应用中。当前UQ方法通常依赖于使用非零温度采样多次查询模型，以生成多样化的输出以进行不确定性估计。然而，选择给定温度参数的影响研究不足，我们的分析揭示温度在不确定性估计质量中起着基本作用。传统的识别最佳温度值的方法需要昂贵的超参数优化（HPO），并且必须为每个新的模型-数据集组合重复进行。我们提出了蒙特卡洛温度（MCT），这是一种稳健的采样策略，消除了对温度校准的需求。我们的分析揭示：1）MCT在广泛温度范围内提供更稳健的不确定性估计；2）MCT通过替换不依赖HPO的固定温度策略来提高UQ方法的表现；3）MCT在统计上与代表理想结果的、经过良好调整但计算成本高昂的HPO过程的占位温度相当。这些发现表明，无需温度参数校准的计算负担，即可实现有效的UQ。|
|**2025-02-25**|**How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities**|Minhua Lin et.al.|[2502.18387](http://arxiv.org/abs/2502.18387)|null|搜索在各个领域的解决问题中扮演着基础角色，大多数现实世界的决策问题都可通过系统搜索来解决。从最近关于搜索和学习的讨论中汲取灵感，我们从三个角度系统地探讨了搜索和大型语言模型（LLMs）之间的互补关系。首先，我们分析了学习如何提高搜索效率，并提出了通过学习进行搜索（SeaL）的框架，该框架利用LLMs进行有效且高效的搜索。其次，我们将SeaL进一步扩展为SeaL-C，以确保搜索过程中的严格完备性。我们在三个现实世界规划任务上的评估表明，与传统的搜索方法相比，SeaL在实现近乎完美准确率的同时，将搜索空间减少了高达99.1%。最后，我们通过调查LLMs是否能够独立发展搜索能力来探索它们离现实搜索有多远。我们的分析显示，虽然当前LLMs在复杂问题中的高效搜索存在困难，但融入系统搜索策略显著提高了它们的解决问题的能力。这些发现不仅验证了我们的方法的有效性，而且突出了提高LLMs搜索能力以适应现实应用的需求。|
|**2025-02-25**|**MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning**|Sepehr Asgarian et.al.|[2502.18371](http://arxiv.org/abs/2502.18371)|null|在竞争激烈的广告领域中，成功取决于有效地应对和利用消费者、广告商和广告平台之间复杂的互动。这些多方面的互动迫使广告商优化模拟消费者行为、提升品牌记忆力和定制广告内容策略。为了应对这些挑战，我们提出了MindMem，这是一种用于广告记忆力的多模态预测模型。通过整合文本、视觉和听觉数据，MindMem实现了最先进的性能，在LAMBDA数据集上获得了0.631的Spearman相关系数，在Memento10K数据集上获得了0.731的系数，持续超越现有方法。此外，我们的分析确定了影响广告记忆力的关键因素，例如视频节奏、场景复杂性和情感共鸣。在此基础上，我们引入了MindMem-ReAd（MindMem驱动的广告再生），它使用基于大型语言模型的模拟来优化广告内容和投放，从而在广告记忆度上实现了高达74.12%的提高。我们的结果突出了人工智能在广告中的变革潜力，为广告商提供了一款强大的工具，以驱动参与度、增强竞争力并最大化在快速变化的市场中的影响。|
|**2025-02-24**|**Introducing Visual Perception Token into Multimodal Large Language Model**|Runpeng Yu et.al.|[2502.17425](http://arxiv.org/abs/2502.17425)|**[link](https://github.com/yu-rp/visualperceptiontoken)**|**为了利用视觉信息，多模态大型语言模型（MLLM）依赖于其视觉编码器的感知过程。视觉感知的完整性和准确性显著影响空间推理、精细理解等任务的精度。然而，MLLM仍然缺乏控制自身视觉感知过程的能力，例如选择性地查看图像的特定区域或关注与特定物体类别相关的信息。在本工作中，我们提出了视觉感知令牌的概念，旨在赋予MLLM控制其视觉感知过程的机制。我们设计了两种类型的视觉感知令牌，称为区域选择令牌和视觉重新编码令牌。MLLM像生成文本一样自主生成这些令牌，并使用它们来触发额外的视觉感知动作。区域选择令牌明确识别需要进一步感知的图像中的特定区域，而视觉重新编码令牌则使用其隐藏状态作为控制信号来引导额外的视觉感知过程。大量的实验表明，这些令牌在处理空间推理、提高精细理解等方面具有优势。平均而言，引入视觉感知令牌将2B模型的性能提高了23.6%，将其分数从0.572提高到0.708，甚至比7B参数模型的表现高出13.4%（从0.624）。请查看我们的代码库：https://github.com/yu-rp/VisualPerceptionToken**|
|**2025-02-24**|**MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs**|Jiarui Zhang et.al.|[2502.17422](http://arxiv.org/abs/2502.17422)|**[link](https://github.com/saccharomycetes/mllms_know)**|**近年来，多模态大型语言模型（MLLMs）在视觉识别任务上取得了快速进步。鉴于它们有潜力集成到许多关键应用中，了解它们视觉感知的局限性是很重要的。在这项工作中，我们研究了MLLMs在回答有关图像的问题时，是否能够像处理大视觉细节一样有效地感知小视觉细节。我们观察到，它们的性能对问题的视觉主体大小非常敏感，并通过进行干预研究进一步表明这种效应实际上是因果的。接下来，我们研究了MLLMs在回答视觉问题时注意力的模式，并有趣地发现，即使它们提供了错误的答案，它们也能始终知道该看向哪里。基于这些发现，我们提出了无训练的视觉干预方法，该方法利用任何MLLM自身的内部知识，以注意力和梯度图的形式，来增强其对小视觉细节的感知。我们在两个广泛使用的MLLM和七个视觉问答基准上评估了我们的方法，并表明它们可以显著提高MLLMs的准确率，而不需要任何训练。我们的结果阐明了将MLLMs应用于涉及小细节的视觉识别任务的风险，并指出使用模型内部状态进行视觉干预是一个有前景的方向来减轻这种风险。**|
|**2025-02-24**|**LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification**|Penghui Yang et.al.|[2502.17421](http://arxiv.org/abs/2502.17421)|**[link](https://github.com/sail-sg/longspec)**|**推测性解码已成为缓解大型语言模型（LLMs）自回归解码高推理延迟的有前景的技术。尽管前景广阔，但在LLMs中有效应用推测性解码仍面临三个关键挑战：草案模型的内存需求不断增加、短训练语料库与长上下文推理之间的分布偏移，以及注意力实现的低效。在本工作中，我们通过解决这些挑战来提升长上下文设置中推测性解码的性能。首先，我们提出了一种具有固定大小键值（KV）缓存的内存高效草案模型。其次，我们为短训练数据引入了新的位置索引，使得从短上下文训练无缝适应到长上下文推理成为可能。最后，我们提出了一种创新性的注意力聚合方法，该方法结合了快速的前缀计算实现与标准的注意力机制以处理树掩码，有效解决了树解码的延迟和内存低效问题。我们的方法在各种长上下文任务上取得了显著成果，包括代码库级别的代码补全、长上下文摘要以及类似o1的长推理任务，证明了在降低延迟方面的显著改进。代码可在https://github.com/sail-sg/LongSpec上找到。**|
|**2025-02-24**|**The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence**|Tom Wollschläger et.al.|[2502.17420](http://arxiv.org/abs/2502.17420)|null|大型语言模型（LLMs）的安全对齐可以通过对抗性构造的输入来规避，然而这些攻击如何绕过安全屏障的机制仍然了解甚少。先前的研究表明，模型激活空间中的一个拒绝方向决定了LLM是否会拒绝一个请求。在本研究中，我们提出了一种新颖的基于梯度的表示工程方法，并使用它来识别拒绝方向。与先前工作相反，我们发现存在多个独立的拒绝方向，甚至还有多维度概念锥来调节拒绝。此外，我们表明，在干预下，正交性本身并不意味着独立性，这激发了考虑线性和非线性效应的表示独立性的概念。使用这个框架，我们确定了机制上独立的拒绝方向。我们发现LLM中的拒绝机制受复杂空间结构的控制，并确定了功能上独立的拒绝方向，证实了多个不同的机制驱动拒绝行为。我们的基于梯度的方法揭示了这些机制，并可作为未来关于理解LLMs工作的基础。|
|**2025-02-24**|**From System 1 to System 2: A Survey of Reasoning Large Language Models**|Zhong-Zhi Li et.al.|[2502.17419](http://arxiv.org/abs/2502.17419)|**[link](https://github.com/zzli2022/awesome-slow-reason-system)**|**实现人类水平智能需要改进从快速直观的系统1到更慢、更谨慎的系统2推理的过渡。虽然系统1在快速启发式决策方面表现出色，但系统2依赖于逻辑推理以实现更准确的判断和减少偏见。基础大型语言模型（LLMs）在快速决策方面表现出色，但缺乏复杂推理的深度，因为它们尚未完全接受系统2思维特有的逐步分析步骤。最近，像OpenAI的o1/o3和DeepSeek的R1这样的推理LLMs在数学和编码等领域展示了专家级的表现，紧密模仿系统2的谨慎推理，并展示了类似人类的认知能力。本综述首先简要概述了基础LLMs的进展和系统2技术的早期发展，探讨了它们结合如何为推理LLMs铺平道路。接下来，我们讨论了如何构建推理LLMs，分析了它们的特征、支持高级推理的核心方法以及各种推理LLMs的演变。此外，我们概述了推理基准，提供了代表性推理LLMs性能的深入比较。最后，我们探讨了推进推理LLMs的具有前景的方向，并维护一个实时GitHub仓库（https://github.com/zzli2022/Awesome-Slow-Reason-System）以跟踪最新发展。我们希望这份综述能成为宝贵的资源，激发创新并推动这一快速发展的领域的进步。**|
|**2025-02-24**|**Reasoning with Latent Thoughts: On the Power of Looped Transformers**|Nikunj Saunshi et.al.|[2502.17416](http://arxiv.org/abs/2502.17416)|null|大型语言模型展现了卓越的推理能力，而规模法则表明，尤其是沿着深度轴的参数数量是主要的驱动因素。在这项工作中，我们提出了一个更强烈的论断——许多推理问题需要较大的深度，但不一定需要很多参数。这为循环模型在推理方面的应用开辟了新的途径。首先，我们表明，对于许多合成推理问题，如加法、p-跳归纳和数学问题，一个k层变换器循环L次几乎与一个kL层非循环模型的表现相匹配，并且显著优于一个k层模型。这一观点得到了理论结果的进一步证实，这些结果显示许多此类推理问题可以通过迭代算法解决，因此，可以使用几乎最优深度的循环模型有效地解决。也许令人惊讶的是，这些优势也转化为语言模型的实际应用——在许多下游推理任务中，一个k层循环L次的语言模型可以与kL层语言模型相竞争，甚至更好。事实上，我们的实证分析揭示了一个有趣的现象：循环和非循环模型表现出与其有效深度相关的规模行为，类似于思维链（CoT）推理的推理时间规模。我们进一步阐明了与CoT推理的联系，通过证明循环模型隐式生成潜在思维，并且可以用T个循环模拟T步的CoT。受这些发现启发，我们还提出了推理和记忆之间的一个有趣的对立，并设计了一种在两方面都有效的基于循环的正则化方法。|
|**2025-02-24**|**COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs**|Liming Liu et.al.|[2502.17410](http://arxiv.org/abs/2502.17410)|**[link](https://github.com/lliu606/cosmos)**|**大型语言模型（LLMs）在各种领域展现出显著的成功，然而由于它们所处的复杂且高维度的损失景观，其优化仍然是一个重大挑战。虽然自适应优化器如AdamW被广泛使用，但它们存在一些关键局限性，包括无法捕捉坐标之间的相互依赖性以及高内存消耗。随后研究，以SOAP为例，试图更好地捕捉坐标之间的相互依赖性，但付出了更大的内存开销，限制了大规模LLMs的可扩展性。一种替代方法试图通过低维投影来减少内存消耗，但这导致了大量的近似误差，从而降低了优化效果（例如，每token效率）。在本文中，我们提出了COSMOS，这是一种新型的混合优化器，它利用梯度矩阵中特征子空间的不同重要性来实现内存效率，同时不牺牲优化性能。COSMOS的设计是基于我们的经验洞察和实际考虑。具体来说，COSMOS将SOAP应用于主导特征子空间，该子空间捕捉了主要的优化动态，而将MUON应用于剩余的特征子空间，该子空间不太关键但使用SOAP处理起来计算成本较高。这种混合策略显著减少了内存消耗，同时保持了稳健的优化性能，使其特别适合于大规模LLMs。我们在各种数据集和Transformer架构上提供了数值实验，以证明COSMOS的有效性。我们的代码可在https://github.com/lliu606/COSMOS上获取。**|
|**2025-02-24**|**Large Language Models are Powerful EHR Encoders**|Stefan Hegselmann et.al.|[2502.17403](http://arxiv.org/abs/2502.17403)|**[link](https://github.com/stefanhgm/ehrshot-benchmark)**|电子健康记录（EHRs）在临床预测方面具有巨大的潜力，但其固有的复杂性和异质性为传统的机器学习方法带来了重大挑战。基于大量未标记EHR数据的特定领域EHR基础模型在预测准确性和泛化能力方面展现了有希望的改进；然而，其训练受到对多样化和高质量数据集访问有限以及编码标准和医疗实践不一致性的限制。在本研究中，我们探讨了使用基于嵌入的通用大型语言模型（LLMs）作为EHR编码器的可能性。通过将患者记录序列化为结构化的Markdown文本，将代码转换为可读描述符，我们利用了在广泛公共语料库上预训练的LLMs的广泛泛化能力，从而绕过了对专有医疗数据集的需求。我们系统地评估了两种最先进的LLM嵌入模型GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct，在EHRSHOT基准的15个不同临床预测任务中，将它们的性能与特定于EHR的基础模型CLIMBR-T-Base和传统的机器学习基线进行比较。我们的结果表明，基于LLM的嵌入通常与专用模型的表现相匹配或超过，甚至在少量样本设置中，其有效性随着底层LLM的大小和可用上下文窗口的规模而增加。总的来说，我们的发现表明，将LLMs重新用于EHR编码为临床预测提供了一种可扩展且有效的方法，能够克服传统EHR建模的限制，并促进更可互操作和更具普遍性的医疗应用。|
|**2025-02-24**|**On Relation-Specific Neurons in Large Language Models**|Yihong Liu et.al.|[2502.17355](http://arxiv.org/abs/2502.17355)|**[link](https://github.com/cisnlp/relation-specific-neurons)**|**在大规模语言模型（LLMs）中，某些神经元可以存储在预训练期间学习到的特定知识。虽然知识通常以关系和实体的组合形式出现，但尚不清楚是否有些神经元专注于关系本身——独立于任何实体。我们假设这样的神经元能够检测输入文本中的关系，并指导涉及这种关系的生成。为了研究这个问题，我们使用基于统计的方法研究了Llama-2系列模型在所选关系集上的表现。我们的实验证明了存在特定于关系的神经元。我们测量了选择性地关闭特定于关系r的候选神经元对LLM处理以下两方面的影响：（1）关系为r的事实以及（2）关系为不同于r的不同关系r'的事实。关于它们编码关系信息的能力，我们为特定于关系的神经元提供了以下三个特性的证据。 $\textbf{(i) 神经元累积性。}$ r的神经元表现出累积效应，因此关闭其中较大一部分会导致r中更多事实的退化。$\textbf{(ii) 神经元多用途。}$ 神经元可以在多个密切相关以及不太相关的关系中共享。一些关系神经元可以在语言间转移。$\textbf{(iii) 神经元干扰。}$ 关闭特定于某一关系的神经元可以提高LLM在处理其他关系事实时的生成性能。我们将在https://github.com/cisnlp/relation-specific-neurons上公开我们的代码。**|
|**2025-02-24**|**How Scientists Use Large Language Models to Program**|Gabrielle O'Brien et.al.|[2502.17348](http://arxiv.org/abs/2502.17348)|null|跨学科科学家编写代码以进行关键活动，如数据收集和生成、统计分析建模和可视化。随着能够生成代码的大型语言模型变得广泛可用，科学家在研究软件开发过程中可能越来越多地使用这些模型。我们研究了早期采用代码生成模型的科学家的特征，并对一所公立、以研究为重点的大学的科学家进行了访谈。通过访谈和用户交互日志的审查，我们发现科学家通常将代码生成模型用作一种信息检索工具，以导航不熟悉的编程语言和库。我们提出了关于他们的验证策略的发现，并讨论了可能从代码生成实践中无意中影响科学分析参数的潜在漏洞。|
|**2025-02-21**|**Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training**|Jaydeep Borkar et.al.|[2502.15680](http://arxiv.org/abs/2502.15680)|**[link](https://github.com/jaydeepborkar/Assisted-Memorization)**|由于个人身份信息（PII）的敏感性，其所有者可能有权控制其在大型语言模型（LLM）训练中的包含或请求其删除。除此之外，由于数据集编目技术的不断发展，PII可能被添加或从训练数据集中删除，因为它们是新抓取的以供重新训练，或者因为它们被纳入新的下游微调阶段。我们发现，PII记忆的数量和容易程度是模型在整个训练流程中不断演变的动态属性，这取决于经常改变的设计选择。我们描述了三种新颖的现象：（1）在训练后期出现的类似外观的PII可以引起我们称之为辅助记忆的早期看到的序列的记忆，这在我们的设置中是一个重要因素（高达1/3）；（2）添加PII可以显著增加其他PII的记忆（在我们的设置中，高达约7.5倍）；（3）删除PII可能导致其他PII被记忆。模型创建者在训练模型时应该考虑这些一级和二级隐私风险，以避免新的PII重复的风险。|
|**2025-02-21**|**FLEKE: Federated Locate-then-Edit Knowledge Editing**|Zongkai Zhao et.al.|[2502.15677](http://arxiv.org/abs/2502.15677)|**[link](https://github.com/zongkaiz/fleke)**|定位后编辑知识编辑（LEKE）是更新大型语言模型（LLMs）而不进行完全重新训练的关键技术。然而，现有方法假设单用户环境，在现实世界的多客户端场景中变得效率低下，其中去中心化组织（例如，医院、金融机构）独立更新重叠的知识，导致冗余的中介知识向量（MKV）计算和隐私问题。为了解决这些挑战，我们引入了联邦定位后编辑知识编辑（FLEKE），这是一个新的任务，它使多个客户端能够协作执行LEKE，同时保护隐私并减少计算开销。为了实现这一点，我们提出了FedEdit，这是一个两阶段框架，它优化了MKV的选择和重用。在第一阶段，客户端本地应用LEKE并将计算出的MKVs上传。在第二阶段，FLEKE不再仅仅依赖于基于服务器的MKV共享，而是允许客户端根据余弦相似度检索相关的MKVs，从而实现知识的重新编辑并最小化冗余计算。在两个基准数据集上的实验结果表明，FedEdit保持了非联邦LEKE超过96%的性能，同时比基于FedAvg的基线提高了大约两倍。此外，我们发现，在我们的FedEdit框架中，MEMIT在FLEKE任务中的表现比PMET更稳定。我们的代码可在https://github.com/zongkaiz/FLEKE上找到。|
|**2025-02-21**|**AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind**|Zhining Zhang et.al.|[2502.15676](http://arxiv.org/abs/2502.15676)|**[link](https://github.com/SCAI-JHU/AutoToM)**|心智理论（ToM），即基于人们的行为理解其心理变量的能力，是开发社交智能代理的关键。目前的心智理论推理方法要么依赖于提示大型语言模型（LLMs），这容易产生系统错误，要么使用刚性、手工制作的贝叶斯心智理论（BToM）模型，这些模型更稳健，但不能跨不同领域进行泛化。在这项工作中，我们引入了AutoToM，这是一种自动贝叶斯心智理论方法，旨在实现开放式的机器心智理论。AutoToM可以在任何领域运行，推断任何心理变量，并执行任何阶数的稳健心智理论推理。对于一个心智理论推理问题，AutoToM首先提出一个初始的BToM模型。然后，它基于提出的模型进行自动贝叶斯逆规划，利用LLM作为后端。根据推理的不确定性，它通过引入额外的心理变量和在上下文中引入更多的时间步长来迭代地改进模型。在多个心智理论基准上的实证评估表明，AutoToM始终实现最先进的性能，为机器心智理论提供了一种可扩展、稳健且可解释的方法。|
|**2025-02-21**|**Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing**|Shoumik Saha et.al.|[2502.15666](http://arxiv.org/abs/2502.15666)|**[link](https://github.com/ShoumikSaha/ai-polished-text)**|随着大型语言模型（LLMs）在文本生成中的应用日益广泛，人们普遍对AI生成内容的检测产生了担忧。然而，一个被忽视的挑战是AI润色的文本，即人类撰写的文本通过AI工具进行细微的改进。这引发了一个关键问题：是否应该将微调后的文本归类为AI生成？误分类可能导致虚假的抄袭指控和关于AI在线内容普及的误导性说法。在这项研究中，我们使用我们的AI润色文本评估（APT-Eval）数据集，系统地评估了11个最先进的AI文本检测器。该数据集包含11.7K个样本，这些样本在AI参与程度不同的情况下进行了润色。我们的发现表明，检测器经常将微调后的文本错误地归类为AI生成，难以区分AI参与的程度，并对较老和较小的模型存在偏见。这些局限性突显了更加细腻的检测方法迫切需要的紧迫性。|
|**2025-02-21**|**Machine-generated text detection prevents language model collapse**|George Drayson et.al.|[2502.15654](http://arxiv.org/abs/2502.15654)|**[link](https://github.com/georgedrayson/model_collapse)**|随着大型语言模型（LLMs）的日益普及，它们生成的输出正遍布网络，未来可能会出现机器生成内容稀释人类创作文本的情况。由于网络数据是LLM预训练的主要资源，未来的模型将在未知比例的合成数据上进行训练。这可能导致模型崩溃，这是一种导致模型强化自身错误并导致模型性能下降的退化过程。在本研究中，我们调查了解码策略对模型崩溃的影响，其中我们分析了递归训练期间生成的数据特征、其与人类参考的相似性以及由此产生的模型性能。使用导致模型退化最严重的解码策略，我们探讨了当训练数据的来源（人类或合成）未知时如何避免模型崩溃的问题。我们基于从我们的机器生成文本检测器中提取的重要性权重对数据分布进行重采样的新型方法。我们的方法在两个LLM变体（GPT-2和SmolLM2）的开放式文本生成任务上得到验证，表明我们可以成功地防止模型崩溃，并且在训练数据集中有足够的人类创作数据时，我们的方法可以提升模型性能。|
|**2025-02-21**|**Empowering LLMs with Logical Reasoning: A Comprehensive Survey**|Fengxiang Cheng et.al.|[2502.15652](http://arxiv.org/abs/2502.15652)|null|大型语言模型（LLMs）在各种自然语言任务上取得了显著的成就。然而，近期的研究发现，LLMs在逻辑推理能力上仍存在重大挑战。本文总结了主要挑战，并将其分为两个方面：（1）逻辑问答，LLMs往往无法在复杂的逻辑问题中生成正确答案，这些问题需要根据一系列前提和约束进行复杂的演绎、归纳或类比推理。（2）逻辑一致性，LLMs容易在不同问题中产生自相矛盾的回答。例如，一个最先进的Macaw问答LLM对“喜鹊是鸟吗？”和“鸟有翅膀吗？”这两个问题都回答“是”，但对“喜鹊有翅膀吗？”这个问题回答“否”。为了促进这一研究方向，我们全面调查了最前沿的方法，并提出了这些方法的详细分类。具体来说，为了准确回答复杂的逻辑问题，先前的方法可以根据对外部求解器、提示、预训练和微调的依赖进行分类。为了避免逻辑矛盾，我们讨论了各种逻辑一致性的概念和解决方案，包括蕴涵、否定、传递性、事实一致性及其复合体。此外，我们回顾了常用的基准数据集和评估指标，并讨论了有希望的研究方向，例如扩展模态逻辑以处理不确定性，以及满足多个逻辑一致性的高效算法。|
|**2025-02-21**|**Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models**|Anirudh Sundar et.al.|[2502.15639](http://arxiv.org/abs/2502.15639)|null|多语言大型语言模型（mLLMs）中跨语言对齐是理想属性，因为对齐可以提升跨语言任务的表现。通常，对齐需要微调模型，这计算成本高昂，并且需要大量的语言数据，而这些数据往往难以获得。微调的数据高效替代方法是模型干预——一种通过操纵模型激活来引导生成向期望方向发展的方法。我们分析了这种流行干预措施（寻找专家）对mLLMs中跨语言表示对齐的影响。我们确定了给定语言中需要操纵的神经元，并分析了干预前后的mLLMs嵌入空间。我们表明，修改mLLM的激活可以改变其嵌入空间，从而增强跨语言对齐。进一步地，我们发现嵌入空间的变化转化为检索任务下游性能的提升，在跨语言检索中，最高可达2倍的top-1准确率提升。|
|**2025-02-21**|**The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer**|Marthe Ballon et.al.|[2502.15631](http://arxiv.org/abs/2502.15631)|**[link](https://github.com/MartheBallon/analysis_o3-mini_thinks_harder_not_longer)**|大型语言模型在数学推理方面取得了显著的进步，利用思维链和测试时计算扩展。然而，关于推理标记使用与准确性提升之间的相互作用，仍存在许多未解问题。特别是，在比较不同代际的模型时，不清楚改进的性能是由于更长的推理链还是更有效的推理。我们系统地分析了在Omni-MATH基准测试中o1-mini和o3-mini变体上的思维链长度，发现o3-mini（m）在不需要比o1-mini更长的推理链的情况下实现了更高的准确性。此外，我们表明，随着推理链的增长，所有模型和计算设置下的准确性通常会下降，即使在控制问题难度的情况下也是如此。这种准确性下降在更熟练的模型中显著较小，这表明新一代推理模型在测试时计算的使用上更加有效。最后，我们强调，虽然o3-mini（h）在准确性上略高于o3-mini（m），但它通过在所有问题上分配更多的推理标记来实现，即使是o3-mini（m）已经能解决的问题。这些发现为模型能力与推理长度之间的关系提供了新的见解，对效率、扩展和评估方法具有影响。|
|**2025-02-21**|**Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing**|Qi Le et.al.|[2502.15618](http://arxiv.org/abs/2502.15618)|**[link](https://github.com/qi-le1/probe_pruning)**|我们引入了探针剪枝（PP），这是一种用于在线、动态、结构化剪枝大型语言模型（LLMs）的新框架，采用批量方式应用。PP利用了这样一个见解：并非所有样本和标记对模型输出的贡献是相等的，探测每个批次的一小部分样本可以有效识别关键权重，从而实现针对不同批次的定制动态剪枝。它包括三个主要阶段：探测、历史信息剪枝和完整推理。在探测阶段，PP根据剩余重要性选择一小部分关键隐藏状态，以运行几个模型层。在历史信息剪枝阶段，PP策略性地将探测状态与历史状态相结合。随后，它根据集成状态和PP重要性分数（一个专门开发来评估每个权重通道在维持性能方面重要性的指标）进行结构化剪枝。在最后阶段，对剩余权重进行完整推理。PP的一个主要优点是它与现有模型兼容，因为它运行时无需额外的神经网络模块或微调。在LLaMA-2/3和OPT模型上对PP的全面评估显示，即使使用仅占1.5%的FLOPs的最小探测，也能显著提高LLMs结构化剪枝的效率。例如，在WikiText2上评估LLaMA-2-7B时，PP在40%的剪枝率下，与最先进的方法相比，实现了每单位运行时间降低2.56倍的性能下降比率。我们的代码可在https://github.com/Qi-Le1/Probe_Pruning上找到。|
|**2025-02-21**|**On the Robustness of Transformers against Context Hijacking for Linear Classification**|Tianle Li et.al.|[2502.15609](http://arxiv.org/abs/2502.15609)|null|基于Transformer的大型语言模型（LLMs）展示了强大的上下文学习能力。然而，它们的预测可能会受到事实正确的上下文的干扰，这种现象称为上下文劫持，揭示了显著的鲁棒性问题。为了从理论上理解这一现象，我们基于线性Transformer的最新进展，探索了一个基于上下文的线性分类问题。在我们的设置中，上下文标记被设计为事实正确的查询-答案对，其中查询与最终查询相似，但标签相反。然后，我们对线性Transformer的鲁棒性进行了通用的理论分析，将其表述为模型深度、训练上下文长度和劫持上下文标记数量的函数。一个关键发现是，经过良好训练的更深层次的Transformer可以实现更高的鲁棒性，这与经验观察结果一致。我们表明，这种改进是由于更深层次允许更多的细粒度优化步骤，有效地减轻了上下文劫持的干扰。这一点也由我们的数值实验得到了很好的支持。我们的发现为更深层次架构的益处提供了理论见解，并有助于增强对Transformer架构的理解。|
|**2025-02-20**|**LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention**|Shang Yang et.al.|[2502.14866](http://arxiv.org/abs/2502.14866)|**[link](https://github.com/mit-han-lab/omniserve)**|大型语言模型（LLMs）在处理长序列方面展现出巨大的潜力，然而，由于预填充阶段的注意力计算具有二次复杂性和解码阶段KV缓存的庞大内存占用，高效地服务于这些长上下文模型仍然具有挑战性。为了解决这些问题，我们引入了LServe，这是一个通过混合稀疏注意力加速长序列LLM服务的有效系统。这种方法将适用于预填充和解码阶段的不同硬件友好型、结构化稀疏模式统一到一个框架中，在该框架中，会跳过对不那么重要的标记的计算，以块为单位进行。LServe展示了静态和动态稀疏在长上下文LLM注意力中的兼容性。这种设计通过结合这些优化实现了乘法加速。具体来说，我们在预填充和解码阶段将一半的注意力头转换为几乎免费的流式头。此外，我们发现无论上下文长度如何，都只需要一个常数的KV页面来保留长上下文能力。然后，我们设计了一个基于查询中心相似度的分层KV页面选择策略，根据查询动态修剪KV页面。平均而言，LServe将LLM预填充速度提高了高达2.9倍，解码速度提高了1.3-2.1倍，同时保持了长上下文的准确性。代码已发布在https://github.com/mit-han-lab/omniserve。|
|**2025-02-20**|**Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning**|Shuyue Stella Li et.al.|[2502.14860](http://arxiv.org/abs/2502.14860)|**[link](https://github.com/stellalisy/alfa)**|大型语言模型（LLMs）在不确定性下往往无法提出有效的问题，这使得它们在决策中需要主动收集信息的关键领域变得不可靠。我们提出了一种名为ALFA的框架，通过以下方式改进LLM的提问能力：（i）将“好问题”的概念分解为一系列基于理论的属性（例如，清晰度、相关性），（ii）可控地合成特定属性的提问变体，（iii）通过基于偏好的优化对模型进行对齐，以显式地学习在细粒度属性上提出更好的问题。以临床推理作为案例研究，我们引入了MediQ-AskDocs数据集，该数据集由17k个现实世界的临床交互组成，并附加了80k对后续问题的特定属性偏好对，以及一个新颖的专家注释的交互式医疗问答任务，用于评估提问能力。与ALFA对齐的模型在MediQ-AskDocs上诊断错误减少了56.6%，与SOTA指令调整的LLMs相比，问题级别的胜率为64.4%，且具有良好的泛化能力。我们的研究结果表明，通过结构化和细粒度的属性显式地引导提问，为提高LLMs提供了一种可扩展的途径，尤其是在专家应用领域。|
|**2025-02-20**|**FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling**|Weilin Zhao et.al.|[2502.14856](http://arxiv.org/abs/2502.14856)|null|推测采样已成为加速大型语言模型（LLMs）自回归生成过程的重要技术，它通过利用草稿-验证机制在每次正向传递中产生多个标记。虽然最先进的推测采样方法仅使用单层和语言建模（LM）头作为草稿模型以实现令人印象深刻的层压缩，但对于像Llama-3-8B这样的大词汇量LLMs（词汇量为128k个标记），它们的效率提升大幅减少。为了解决这个问题，我们提出了FR-Spec，这是一种频率排序的推测采样框架，通过词汇空间压缩优化草稿候选选择。通过将草稿搜索限制在频率优先的标记子集中，我们的方法将LM头计算开销减少了75%，同时确保最终输出分布的等价性。在多个数据集上的实验表明，与最先进的推测采样方法EAGLE-2相比，平均速度提升了1.12倍。|
|**2025-02-20**|**Prompt-to-Leaderboard**|Evan Frick et.al.|[2502.14855](http://arxiv.org/abs/2502.14855)|**[link](https://github.com/lmarena/p2l)**|大型语言模型（LLM）的评估通常依赖于如准确率或人类偏好这样的汇总指标，这些指标平均了用户和提示的差异。这种平均化掩盖了模型性能在用户和提示方面的特定变化。为了解决这个问题，我们提出了“提示到排行榜”（P2L）方法，该方法为特定提示生成排行榜。核心思想是训练一个LLM，它以自然语言提示为输入，输出一个布拉德利-特里系数向量，然后使用这些系数来预测人类的偏好投票。由此产生的提示依赖型排行榜允许进行无监督的任务特定评估、查询到模型的最佳路由、个性化以及模型优势和劣势的自动评估。来自聊天机器人竞技场的数据显示，P2L比平均排行榜更好地捕捉了语言模型性能的细微差异。此外，我们的发现表明，P2L产生特定提示评估的能力遵循与LLM本身观察到的幂律缩放相似。到2025年1月，基于这种方法训练的路由器在聊天机器人竞技场排行榜上取得了第一名。我们的代码可在以下GitHub链接找到：https://github.com/lmarena/p2l。|
|**2025-02-20**|**GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks**|Jianwen Luo et.al.|[2502.14848](http://arxiv.org/abs/2502.14848)|**[link](https://github.com/ayanami2003/gate)**|大型语言模型（LLMs）在工具制作方面展现出巨大的潜力，但现有的框架往往难以高效地构建可靠的工具集，且局限于单一任务场景。为了解决这些挑战，我们提出了GATE（基于图的适应性工具进化），这是一个自适应框架，能够动态地构建和进化跨多个场景的可重复使用工具的分层图。我们在开放式任务（Minecraft）、基于代理的任务（TextCraft，DABench）和代码生成任务（MATH，Date，TabMWP）上评估了GATE。我们的结果表明，与之前的SOTA相比，GATE在Minecraft中实现了高达4.3倍的里程碑完成速度，在代码生成任务中平均提高了9.23%，在代理任务中提高了10.03%。GATE展示了自适应进化的力量，在保持高效率的同时，平衡了工具的数量、复杂性和功能性。代码和数据可在\url{https://github.com/ayanami2003/GATE}获取。|
|**2025-02-20**|**Red-Teaming LLM Multi-Agent Systems via Communication Attacks**|Pengfei He et.al.|[2502.14847](http://arxiv.org/abs/2502.14847)|null|基于大型语言模型的多智能体系统（LLM-MAS）通过实现通过消息通信的复杂智能体协作，彻底改变了复杂问题解决能力。虽然通信框架对于智能体协调至关重要，但也引入了一个关键且尚未探索的安全漏洞。在这项工作中，我们引入了中间智能体（AiTM），这是一种新型攻击，通过拦截和操纵智能体间的消息来利用LLM-MAS的基本通信机制。与现有攻击不同，这些攻击会损害单个智能体，而AiTM展示了攻击者如何仅通过操纵智能体间传递的消息就能破坏整个多智能体系统。为了在有限的控制和角色受限的通信格式挑战下实现攻击，我们开发了一个具有反射机制的LLM驱动的对抗性智能体，该机制可以生成上下文感知的恶意指令。我们在各种框架、通信结构和现实世界应用中的全面评估表明，LLM-MAS容易受到基于通信的攻击，突显了在多智能体系统中采取强大安全措施的需求。|
|**2025-02-20**|**Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation**|Yue Yang et.al.|[2502.14846](http://arxiv.org/abs/2502.14846)|null|利用富文本（如图表和文档）进行图像推理是视觉-语言模型（VLMs）的关键应用。然而，由于丰富的视觉-语言数据种类稀少，VLMs在这些领域往往难以应对。为了解决这一挑战，我们提出了CoSyn框架，该框架利用仅文本的大型语言模型（LLMs）的编码能力，自动创建合成富文本多模态数据。给定描述目标领域（例如，“营养成分标签”）的输入文本，CoSyn会提示LLM生成用于渲染合成图像的代码（Python、HTML、LaTeX等）。以底层代码作为合成图像的文本表示，CoSyn可以生成高质量的指令微调数据，再次依赖于仅文本的LLM。使用CoSyn，我们构建了一个包含40万张图像和270万行视觉-语言指令微调数据的语料库。在七个基准测试上的全面实验表明，在合成数据上训练的模型在包括Llama 3.2在内的竞争性开源模型中取得了最先进的性能，并超过了GPT-4V和Gemini 1.5 Flash等专有模型。此外，CoSyn还可以生成合成指向数据，使VLMs能够在输入图像中定位信息，展示了其在开发能够在现实世界环境中行动的多模态智能体方面的潜力。|
|**2025-02-20**|**Revealing and Mitigating Over-Attention in Knowledge Editing**|Pinzheng Wang et.al.|[2502.14838](http://arxiv.org/abs/2502.14838)|**[link](https://github.com/PinzhengWang322/Reveal_Attention_Drift)**|大型语言模型在众多任务中展现了卓越的性能，但它们仍然由于从训练数据中学习到的错误知识而表现出不可取的错误。为了避免这种情况，知识编辑方法应运而生，通过高效地修改极小比例的参数来精确编辑特定模型知识。然而，这些方法可能导致特定性失败的问题：当与编辑知识相关的内容出现在上下文中时，可能会无意中破坏其他现有的知识。然而，这些方法可能导致特定性失败，由于编辑，现有知识和能力严重退化。我们的初步研究表明，特定性失败主要源于模型注意力头对与编辑知识相关的实体分配过高的注意力分数，从而不当地关注上下文中的特定片段，我们将其称为注意力漂移现象。为了减轻这种注意力漂移问题，我们提出了一种简单而有效的方法——选择性注意力漂移限制（SADR），在知识编辑过程中引入一个额外的正则化项，以限制注意力权重分布的变化，从而防止对编辑实体的不当关注。在五个常用强大型语言模型上的实验表明，我们的方法非常有效，SADR可以显著减轻主要知识编辑任务中的特定性失败。|
|**2025-02-20**|**Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs**|Danni Liu et.al.|[2502.14830](http://arxiv.org/abs/2502.14830)|**[link](https://github.com/dannigt/mid-align)**|在通过微调在特定任务应用中展现出色能力的同时，将这些优势扩展到多种语言对于广泛的可访问性至关重要。然而，有效的跨语言迁移受到语言模型在不同语言间的性能差距以及许多语言中微调数据稀缺的限制。通过对超过1,000对语言的大型语言模型内部表示的分析，我们发现中间层具有最强的跨语言对齐潜力。基于这一发现，我们提出了一种中间层对齐目标，并将其集成到特定任务训练中。在槽填充、机器翻译和结构化文本生成上的实验表明，该方法在跨语言迁移方面有持续的提升，尤其是在低资源语言方面。该方法对对齐语言的选择具有鲁棒性，并能推广到对齐过程中未见过的语言。此外，我们展示了单独训练的对齐模块可以与现有的特定任务模块合并，从而在不进行完整重新训练的情况下提高跨语言能力。我们的代码已公开提供（https://github.com/dannigt/mid-align）。|
|**2025-02-20**|**A Survey of Model Architectures in Information Retrieval**|Zhichao Xu et.al.|[2502.14822](http://arxiv.org/abs/2502.14822)|null|本调查探讨了信息检索（IR）中模型架构的演变，重点关注两个关键方面：用于特征提取的骨干模型和用于相关性估计的端到端系统架构。综述有意将架构考虑与训练方法分离，以提供对IR系统中结构创新的有针对性分析。我们追溯了从传统基于词的方法到现代神经网络方法的发展，特别强调了基于transformer的模型以及随后的大型语言模型（LLM）的影响。最后，我们讨论了新兴的挑战和未来方向，包括针对性能和可扩展性的架构优化、处理多模态、多语言数据，以及适应传统搜索范式之外的全新应用领域。|
|**2025-02-19**|**Where's the Bug? Attention Probing for Scalable Fault Localization**|Adam Stein et.al.|[2502.13966](http://arxiv.org/abs/2502.13966)|null|确保代码正确性仍然是一个具有挑战性的问题，尽管大型语言模型（LLMs）在代码相关任务上的能力不断增强。虽然基于LLM的程序修复系统可以使用用户的错误报告来提出修复方案，但它们的有效性本质上受到故障定位（FL）能力的限制，这对于人类和LLMs都是一个具有挑战性的问题。现有的FL方法依赖于可执行测试用例，需要训练成本高昂且通常噪声很大的行级标注，或者需要资源密集型的LLMs。在本文中，我们提出了Bug Attention Probe（BAP）方法，该方法在不使用任何直接定位标签的情况下学习最先进的故障定位，优于传统的FL基线和大规模LLM的提示。我们在各种代码设置中评估了我们的方法，包括来自标准Defects4J数据集的真实世界Java错误以及涵盖多种错误类型和语言的七个其他数据集。在所有八个数据集的平均值中，BAP相比最强的基线提高了34.6%的top-1准确率，比零样本提示GPT-4o提高了93.4%。BAP在提示方面也显著更高效，以极小的计算成本超过了大型开放权重模型。|
|**2025-02-19**|**Autellix: An Efficient Serving Engine for LLM Agents as General Programs**|Michael Luo et.al.|[2502.13965](http://arxiv.org/abs/2502.13965)|null|大型语言模型（LLM）的应用正从简单的聊天机器人演变为动态、通用的代理程序，这些程序通过扩展LLM调用和输出令牌来帮助AI代理推理、探索和解决复杂任务。然而，现有的LLM服务系统忽略了程序和调用之间的依赖关系，错失了重要的优化机会。我们的分析揭示，提交给LLM服务引擎的程序经历了长时间的累积等待时间，主要原因是LLM请求和程序层面的首部阻塞。为了解决这个问题，我们引入了Autellix，这是一个将程序视为一等公民的LLM服务系统，以最小化它们的端到端延迟。Autellix拦截程序提交的LLM调用，为调度器提供程序级别的上下文。我们提出了两个调度算法——针对单线程和分布式程序——它们根据程序之前完成的调用来抢占和优先处理LLM调用。我们的评估表明，在多样化的LLM和代理工作负载中，与最先进的系统（如vLLM）相比，Autellix在相同的延迟下将程序的吞吐量提高了4-15倍。|
|**2025-02-19**|**MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads**|Weihao Liu et.al.|[2502.13963](http://arxiv.org/abs/2502.13963)|**[link](https://github.com/NeosKnight233/MuDAF)**|大型语言模型（LLMs）由于输入中的无关信息而经常表现出注意力分散，这严重影响了它们的长期上下文能力。受近期关于检索头在长期上下文事实性研究中的有效性启发，我们旨在通过直接改进此类检索头来解决这一问题。我们提出了多文档注意力聚焦（MuDAF）这一新方法，它通过对比学习显式优化头部级别的注意力分布。根据实验结果，MuDAF可以显著提高LLMs的长期上下文问答性能，特别是在多文档问答中。在检索分数和注意力可视化方面的广泛评估表明，MuDAF具有使注意力头部更加关注相关信息并减少注意力分散的巨大潜力。|
|**2025-02-19**|**Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering**|William Jurayj et.al.|[2502.13962](http://arxiv.org/abs/2502.13962)|null|在扩展大型语言模型在测试时的计算规模方面，已在推理基准测试中展现出令人印象深刻的性能。然而，现有的测试时扩展评估假设推理系统应该对任何提出的问题都给出答案。这忽略了关于模型对其答案是否自信以及是否总是提供回答是否适当的问题。为了解决这些担忧，我们在推理过程中提取置信度分数以进行阈值处理模型响应。我们发现，在推理时增加计算预算不仅有助于模型正确回答更多问题，还增加了对正确响应的置信度。然后，我们通过考虑具有非零响应风险水平的设置来扩展当前在评估期间实现零风险响应的范式，并建议在这些设置下报告评估的方法。|
|**2025-02-19**|**LIDDIA: Language-based Intelligent Drug Discovery Agent**|Reza Averly et.al.|[2502.13959](http://arxiv.org/abs/2502.13959)|null|药物发现是一个漫长、昂贵且复杂的过程，严重依赖人类药物化学家，他们可能需要花费数年时间去探索潜在的疗法空间。近年来，人工智能在化学领域的进步试图加速个体药物发现任务；然而，仍然迫切需要一种能够导航药物发现过程的智能代理。为此，我们引入了LIDDiA，这是一个能够在虚拟环境中智能导航药物发现过程的自主代理。通过利用大型语言模型的推理能力，LIDDiA成为了一个低成本且高度可适应的自主药物发现工具。我们对LIDDiA进行了全面考察，证明了以下三点：（1）它能够在超过70%的30个临床相关靶点上生成符合关键药物标准的分子，（2）它在化学空间中智能地平衡了探索和利用，（3）它能够在EGFR（癌症的关键靶点）上识别出有希望的全新药物候选分子。|
|**2025-02-19**|**Neurosymbolic artificial intelligence via large language models and coherence-driven inference**|Steve Huntsman et.al.|[2502.13953](http://arxiv.org/abs/2502.13953)|null|我们设计了一种算法，用于生成一组命题，这些命题客观地实例化了支持以连贯性为驱动的推理的图。然后，我们评估了大型语言模型（LLMs）从自然语言表达（的简单转换）中重建连贯性图的能力，并得到了从单一提示到针对推理优化的模型的有希望的结果。将连贯性驱动的推理与神经模型的一致性评估相结合，可能推动机器认知领域的最新进展。|
|**2025-02-19**|**Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region**|Chak Tou Leong et.al.|[2502.13946](http://arxiv.org/abs/2502.13946)|null|大型语言模型（LLMs）的安全对齐仍然存在漏洞，因为它们的初始行为很容易被相对简单的攻击破解。由于在输入指令和初始模型输出之间填充固定模板是现有LLMs的常见做法，我们假设这个模板是它们脆弱性的关键因素：LLMs与安全相关的决策过度依赖模板区域的聚合信息，这很大程度上影响了这些模型的安全行为。我们将这个问题称为模板锚定安全对齐。在本文中，我们进行了广泛的实验，并验证了模板锚定安全对齐在各个对齐的LLMs中普遍存在。我们的机制分析展示了它是如何导致模型在遇到推理时间破解攻击时的脆弱性。此外，我们表明将安全机制从模板区域分离出来在减轻破解攻击的漏洞方面具有前景。我们鼓励未来的研究开发更稳健的安全对齐技术，以减少对模板区域的依赖。|
|**2025-02-19**|**LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization**|Guanzheng Chen et.al.|[2502.13922](http://arxiv.org/abs/2502.13922)|**[link](https://github.com/DAMO-NLP-SG/LongPO)**|**大型语言模型（LLMs）通过预训练和对齐展现了非凡的能力。然而，由于长语境对齐不足，在长语境场景中，优越的短语境LLMs可能会表现不佳。由于对长语境进行人工标注不切实际以及平衡短语境和长语境性能的困难，这一对齐过程仍然具有挑战性。为了解决这些挑战，我们引入了LongPO，它使短语境LLMs能够通过内部转移短语境能力来自我进化，从而在长语境任务上表现出色。LongPO利用LLMs从自生成的短到长偏好数据中学习，这些数据包括为相同指令生成的具有长语境输入和它们各自的压缩短语境对应答案的配对响应。这种偏好揭示了在短语境对齐期间培养的LLMs的能力和潜力，这些能力和潜力在长语境对齐不足的情况下可能会被削弱。此外，LongPO还纳入了短到长的KL约束，以减轻在长语境对齐过程中短语境性能的下降。当将LongPO应用于从128K到512K语境长度的Mistral-7B-Instruct-v0.2时，LongPO完全保留了短语境性能，并在长语境和短语境任务中在很大程度上优于简单的SFT和DPO。具体来说，我们的方法训练的模型在长语境基准测试中的结果可以与，甚至超过那些涉及大量长语境标注和更大参数规模的优越LLMs（例如GPT-4-128K）的结果。**|
|**2025-02-19**|**Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis**|Jiahao Gai et.al.|[2502.13921](http://arxiv.org/abs/2502.13921)|null|近期代码生成技术的进展揭示了利用大型语言模型（LLMs）在通用编程语言如Python和C++中的应用潜力，为自动化软件开发和提升程序员生产力开辟了新的机遇。LLMs在软件编程中的潜力引发了探索自动化硬件生成和自动化的广泛关注。尽管初步尝试已将LLMs应用于生成硬件描述语言（HDLs），但在这一方向上仍存在一些挑战。首先，可用于HDL训练的数据量与软件编程语言相比显著较小。其次，主要针对软件代码进行预训练的LLMs往往产生更容易出错的HDL设计。第三，生成HDL所需的标记数量远高于软件编程，导致成本和能耗效率低下。为了应对这些挑战，本文探讨了利用LLMs生成基于高级综合（HLS）的硬件设计。尽管在文献中，针对特定领域编程语言的代码生成并不新颖，但我们旨在提供实验结果、见解、基准和评估基础设施，以研究HLS相对于低级HDLs在LLM辅助的硬件设计生成中的适用性。为了实现这一目标，我们首先针对基于HLS的硬件生成微调预训练模型，使用包含文本提示和相应参考HLS设计的收集数据集。随后，提出了一种LLM辅助框架，以自动化端到端硬件代码生成，并研究思维链和反馈循环促进技术在HLS设计生成中的影响。由于研究时间限制，我们计划在未来评估更先进的推理模型。|
|**2025-02-19**|**Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health**|Xingbo Wang et.al.|[2502.13920](http://arxiv.org/abs/2502.13920)|**[link](https://github.com/xingbow/sleephealthLLM)**|尽管睡眠追踪设备普及，但许多人难以将数据转化为改善睡眠健康的实际措施。现有方法通常提供基于数据的建议，但这些方法可能不符合现实生活的限制和个体环境。我们提出了HealthGuru，这是一种由大型语言模型驱动的聊天机器人，通过数据驱动的、理论指导的、适应性强的建议以及对话式行为改变支持来提高睡眠健康。HealthGuru的多智能体框架整合了可穿戴设备数据、上下文信息和上下文多臂老虎机模型，以提出定制的助眠活动。该系统在促进自然对话的同时，融入了基于数据的见解和理论行为改变技术。我们的16名参与者的八周野外部署研究将HealthGuru与基线聊天机器人进行了比较。结果显示，使用HealthGuru改善了睡眠时长和活动评分等指标，提高了回答质量，并增加了用户改变行为的动机。我们还确定了个性化设计和健康聊天机器人用户参与方面的挑战。|
|**2025-02-18**|**Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation**|Bencheng Liao et.al.|[2502.13145](http://arxiv.org/abs/2502.13145)|**[link](https://github.com/hustvl/mmmamba)**|近期，多模态大型语言模型（MLLMs）取得了显著的成绩，但由于其二次方计算复杂度、不断增长的键值缓存需求以及对独立视觉编码器的依赖，其部署面临挑战。我们提出了mmMamba，这是一个通过使用适度的学术计算资源，从现有的MLLMs中逐步蒸馏以开发线性复杂度原生多模态状态空间模型的框架。我们的方法使得训练好的仅解码器MLLM可以直接转换为线性复杂度架构，而无需预训练基于RNN的LLM或视觉编码器。我们提出了一种种子策略，用于从训练好的Transformer中提取Mamba，以及一个三阶段蒸馏配方，能够有效地将Transformer的知识转移到Mamba上，同时保留多模态能力。我们的方法还支持灵活的混合架构，可以结合Transformer和Mamba层，以实现可定制的效率-性能权衡。从基于Transformer的仅解码器HoVLE蒸馏而来的mmMamba-linear，在与现有的线性复杂度和二次方复杂度VLMs的竞争中表现出竞争力，而mmMamba-hybrid进一步显著提升了性能，接近HoVLE的能力。在103K个标记处，mmMamba-linear相比HoVLE实现了20.6倍的加速和75.8%的GPU内存减少，而mmMamba-hybrid实现了13.5倍的加速和60.2%的内存节省。代码和模型已发布在https://github.com/hustvl/mmMamba。|
|**2025-02-18**|**Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization**|Shuo Xing et.al.|[2502.13146](http://arxiv.org/abs/2502.13146)|**[link](https://github.com/taco-group/re-align)**|大型视觉语言模型（VLMs）的出现，通过整合视觉模态，拓宽了单模态大型语言模型（LLMs）的范畴和能力，从而在各种实际场景中解锁了变革性的跨模态应用。尽管它们的性能令人印象深刻，但VLMs容易产生重大幻觉，尤其是跨模态不一致的形式。基于人类反馈强化学习（RLHF）在调整LLMs方面的成功，最近的研究进展集中在将直接偏好优化（DPO）应用于精心挑选的数据集以减轻这些问题。然而，这些方法通常以暴力方式引入偏好信号，忽视了视觉信息在调整过程中的关键作用。在本文中，我们引入了Re-Align，这是一种新的调整框架，它利用图像检索来构建一个双偏好数据集，有效地结合了文本和视觉偏好信号。我们进一步引入了rDPO，这是标准直接偏好优化的一种扩展，在微调过程中增加了额外的视觉偏好目标。我们的实验结果表明，Re-Align不仅比先前的方法更有效地减轻了幻觉，而且在一般的视觉问答（VQA）任务中也取得了显著的性能提升。此外，我们表明Re-Align在各种VLM大小和架构上保持了鲁棒性和可扩展性。这项工作在调整多模态LLMs方面迈出了重要一步，为更可靠和有效的跨模态应用铺平了道路。我们在https://github.com/taco-group/Re-Align上发布了所有代码。|
|**2025-02-18**|**UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models**|Huawei Lin et.al.|[2502.13141](http://arxiv.org/abs/2502.13141)|**[link](https://github.com/huawei-lin/uniguardian)**|大型语言模型（LLMs）容易受到如提示注入、后门攻击和对抗攻击等攻击，这些攻击通过操纵提示或模型来生成有害输出。在本文中，我们跳出传统的深度学习攻击范式，探索它们之间的内在联系，并将它们统称为提示触发攻击（PTA）。这引发了一个关键问题：我们能否确定一个提示是良性的还是被污染的？为了解决这个问题，我们提出了UniGuardian，这是第一个旨在检测LLM中的提示注入、后门攻击和对抗攻击的统一防御机制。此外，我们引入了一种单前向策略来优化检测流程，使得在单次前向传递中能够同时进行攻击检测和文本生成。我们的实验证实，UniGuardian能够准确且高效地在LLMs中识别恶意提示。|
|**2025-02-18**|**AIDE: AI-Driven Exploration in the Space of Code**|Zhengyao Jiang et.al.|[2502.13138](http://arxiv.org/abs/2502.13138)|**[link](https://github.com/wecoai/aideml)**|机器学习，作为现代人工智能的基础，推动了世界性的创新，彻底改变了我们的世界。然而，在这些进步的背后，隐藏着一个复杂且往往繁琐的过程，需要大量的人力和计算密集型迭代和实验。开发机器学习模型的工程师和科学家们大部分时间都花在试错任务上，而不是构思创新解决方案或研究假设。为了应对这一挑战，我们引入了AI驱动的探索（AIDE），这是一个由大型语言模型（LLMs）驱动的机器学习工程代理。AIDE将机器学习工程视为一个代码优化问题，并将试错过程定义为潜在解决方案空间中的树搜索。通过战略性地重用和改进有希望的解决方案，AIDE有效地用计算资源换取了性能提升，在多个机器学习工程基准测试中取得了最先进的结果，包括我们的Kaggle评估、OpenAI MLE-Bench和METRs RE-Bench。|
|**2025-02-18**|**Theorem Prover as a Judge for Synthetic Data Generation**|Joshua Ong Jun Leang et.al.|[2502.13137](http://arxiv.org/abs/2502.13137)|null|由于合成数据在数学推理中具有提升大型语言模型（LLMs）数学能力潜力，对其需求不断增加。然而，确保中间推理步骤的有效性仍然是一个重大挑战，影响了数据质量。虽然通过定理证明器进行形式验证可以有效地验证LLMs的推理，但数学证明的自动形式化仍然存在错误。为此，我们引入了迭代自动形式化方法，该方法通过迭代优化定理证明器的形式化来减轻错误，从而将Lean证明器的执行率从60%提高到87%。在此基础上，我们提出了定理证明器作为裁判（TP-as-a-Judge）的方法，该方法利用定理证明器的形式化来严格评估LLMs的中间推理，有效地将自动形式化与合成数据生成相结合。最后，我们提出了基于定理证明器反馈的强化学习（RLTPF）框架，该框架在强化学习从人类反馈（RLHF）中用定理证明器反馈取代人工标注。在多个LLMs上，应用TP-as-a-Judge和RLTPF仅使用3,508个样本就提高了基准测试，在Mistral-7B的MultiArith上实现了5.56%的准确率提升，在Llama-2-7B的SVAMP上实现了6.00%的提升，在Llama-3.1-8B的AQUA上实现了3.55%的提升。|
|**2025-02-18**|**Learning to Defer for Causal Discovery with Imperfect Experts**|Oscar Clivio et.al.|[2502.13132](http://arxiv.org/abs/2502.13132)|null|将专家知识，例如来自大型语言模型的，整合到因果发现算法中可能会很具挑战性，因为不能保证这些知识是正确的。专家的建议可能与数据驱动的结果相矛盾，并且其可靠性会根据领域或特定查询而有很大差异。基于软约束或预测因果关系中不一致性的现有方法未能考虑到这些专业知识的差异。为了解决这个问题，我们提出了L2D-CD，这是一种衡量专家建议正确性和最佳地将它们与数据驱动因果发现结果相结合的方法。通过将学习到延迟（L2D）算法应用于成对因果发现（CD），我们学习到一个延迟函数，该函数选择是否依赖于使用数值数据的经典因果发现方法或基于文本元数据的专家建议。我们在著名的图宾根成对数据集上评估了L2D-CD，并展示了其相对于独立使用的因果发现方法和专家的优越性能。此外，我们的方法还识别了专家表现强或弱的专业领域。最后，我们概述了一种将此方法推广到具有超过两个变量的图的因果发现中的策略，为该领域的进一步研究铺平了道路。|
|**2025-02-18**|**Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning**|Jingyang Lin et.al.|[2502.13127](http://arxiv.org/abs/2502.13127)|null|近期大型语言模型（LLMs）在处理越来越长的序列方面取得了进展，序列长度从2K到2M个标记，甚至更长。然而，仅仅延长输入序列长度并不一定能导致有效的长上下文理解。在本研究中，我们以监督方式将思维链（CoT）推理集成到LLMs中，以促进有效的长上下文理解。为此，我们引入了LongFinanceQA，这是一个金融领域的合成数据集，旨在提高长上下文推理能力。与现有的长上下文合成数据不同，LongFinanceQA在最终结论之前包含了中间CoT推理，这鼓励LLMs进行显式推理，提高了长上下文理解中的准确性和可解释性。为了生成合成的CoT推理，我们提出了属性驱动的代理推理（PAI），这是一个模拟人类推理步骤的代理框架，包括属性提取、检索和总结。我们通过在Loong基准测试上评估GPT-4o-mini w/ PAI的推理能力来评估PAI的推理能力，其性能比标准GPT-4o-mini提高了20.0%。此外，我们在LongFinanceQA上微调了LLaMA-3.1-8B-Instruct，在Loong的金融子集上实现了24.6%的性能提升。|
|**2025-02-18**|**RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises**|Zenan Zhai et.al.|[2502.13125](http://arxiv.org/abs/2502.13125)|**[link](https://github.com/LibrAIResearch/ruozhibench)**|近年来，大型语言模型（LLMs）在复杂推理问题的回答方面取得了显著进展。然而，它们在识别和回应包含逻辑谬误或故意误导性前提的文本方面的能力研究相对较少。为了填补这一空白，我们引入了RuozhiBench，这是一个包含677个经过精心挑选的问题的双语数据集，这些问题包含各种形式的欺骗性推理，通过大量的人工努力和专家评审精心制作。我们使用开放式和两选一的形式，对17个来自5个系列的LLMs在RuozhiBench上的表现进行了全面评估，并对评估协议和结果模式进行了广泛的分析。尽管这些模型在传统基准测试中得分很高，但它们在检测和正确推理逻辑谬误方面的能力有限，即使是表现最好的模型Claude-3-haiku，其准确率也只有62%，而人类准确率超过90%。|
|**2025-02-18**|**Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context**|Marion Bartl et.al.|[2502.13120](http://arxiv.org/abs/2502.13120)|null|性别包容性语言通常被用于确保所有个体，无论性别如何，都能与某些概念相关联。虽然心理语言学研究表明了它在人类认知方面的作用，但关于大型语言模型（LLMs）如何处理性别包容性语言仍然不清楚。鉴于商业LLMs在日常应用中越来越占据重要地位，检查LLMs是否真的对性别包容性语言进行中性解读至关重要，因为它们生成的语言有可能影响用户的语言。本研究考察了LLM生成的指代词是否与特定的性别表达一致或反映了模型偏见。我们将心理语言学方法从法语调整为英语和德语，发现英语中，LLMs通常保持先行词的性别，但表现出潜在的男性偏见。在德语中，这种偏见更为强烈，甚至盖过了所有测试过的性别中性化策略。|
|**2025-02-18**|**STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models**|Narun Raman et.al.|[2502.13119](http://arxiv.org/abs/2502.13119)|null|如何判断一个给定的大语言模型（LLM）是否能够可靠地执行经济推理？大多数现有的LLM基准主要关注特定应用，无法向模型提供丰富的经济任务。一个值得注意的例外是Raman等人[2024]，他们提出了一种全面基准化战略决策的方法；然而，这种方法未能解决微观经济学中普遍存在的非战略环境，如供需分析。我们通过将微观经济推理分类为58个不同的元素，重点关注供需的逻辑，每个元素基于多达10个不同的领域、5个视角和3种类型，来填补这一空白。通过一个新颖的LLM辅助数据生成协议auto-STEER生成基准数据，该协议通过调整手写的模板来针对新的领域和视角生成一系列问题。由于它提供了一种自动生成新问题的方法，auto-STEER减轻了LLM可能被训练以过度拟合评估基准的风险；因此，我们希望它能成为未来多年评估和微调模型的的有用工具。我们通过一个对27个LLM的案例研究展示了我们基准的有用性，这些LLM从小型开源模型到当前的技术前沿不等。我们检查了每个模型解决整个分类中微观经济问题的能力，并使用各种提示策略和评分指标呈现了结果。|
|**2025-02-17**|**Idiosyncrasies in Large Language Models**|Mingjie Sun et.al.|[2502.12150](http://arxiv.org/abs/2502.12150)|**[link](https://github.com/locuslab/llm-idiosyncrasies)**|在这项工作中，我们揭示了并研究了大型语言模型（LLMs）的独特之处——它们输出中的独特模式，这些模式可以用来区分模型。为此，我们考虑了一个简单的分类任务：给定特定的文本输出，目标是预测生成该文本的源LLM。我们在各种LLM组别中评估了这个合成任务，并发现仅将现有的文本嵌入模型微调到LLM生成的文本上就能达到极高的分类准确率。值得注意的是，在涉及ChatGPT、Claude、Grok、Gemini和DeepSeek的五种分类问题中，我们在保留的验证数据上达到了97.1%的准确率。我们的进一步研究揭示了这些独特之处根植于词级分布。即使在文本被外部LLM重写、翻译或总结的情况下，这些模式仍然持续存在，这表明它们也编码在语义内容中。此外，我们利用LLM作为评委，为每个模型的独特之处生成详细的、开放式描述。最后，我们讨论了我们的发现更广泛的影响，特别是对于在合成数据上的训练和推断模型相似性。代码可在https://github.com/locuslab/llm-idiosyncrasies上找到。|
|**2025-02-17**|**HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation**|Ling Yang et.al.|[2502.12148](http://arxiv.org/abs/2502.12148)|**[link](https://github.com/gen-verse/hermesflow)**|自回归范式的显著成功在多模态大型语言模型（MLLMs）领域取得了重大进展，像Show-o、Transfusion和Emu3等强大模型在统一图像理解和生成方面取得了显著进步。首次，我们发现了一个共同现象：MLLMs的理解能力通常强于它们的生成能力，两者之间存在显著差距。基于这一洞察，我们提出了HermesFlow，这是一个简单但通用的框架，旨在无缝地弥合MLLMs中理解和生成之间的差距。具体来说，我们以同源数据作为输入，来定制理解和生成方面的同源偏好数据。通过成对-DPO和自我玩耍迭代优化，HermesFlow有效地利用同源偏好数据对齐多模态理解和生成。大量实验表明，我们的方法在缩小多模态理解和生成之间的差距方面显著优于先前的方法。这些发现突出了HermesFlow作为下一代多模态基础模型通用对齐框架的潜力。代码：https://github.com/Gen-Verse/HermesFlow|
|**2025-02-17**|**Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control**|Jinyan Su et.al.|[2502.12145](http://arxiv.org/abs/2502.12145)|**[link](https://github.com/jinyansu1/flare-aug)**|检索增强生成（RAG）已成为缓解大型语言模型（LLM）幻觉的一种有效方法，通过结合外部知识检索。然而，现有的RAG框架往往不加区分地应用检索，导致在不需要时过度检索，或在需要复杂推理时无法迭代检索。虽然最近出现的自适应检索策略可以自适应地导航这些检索策略，但它们仅基于查询复杂性进行预测，缺乏用户驱动的灵活性，这使得它们难以满足多样化的用户应用需求。在本文中，我们介绍了一种新颖的用户可控RAG框架，该框架允许动态调整准确度-成本权衡。我们的方法利用两个分类器：一个训练用于优先考虑准确度，另一个用于优先考虑检索效率。通过一个可解释的控制参数 $\alpha$ ，用户可以根据他们的具体需求在最小成本检索和高精度检索之间无缝切换。我们通过实证研究证明了我们的方法有效地平衡了准确度、检索成本和用户可控性，使其成为实际且可适应的解决方案，适用于现实世界应用。|
|**2025-02-17**|**Small Models Struggle to Learn from Strong Reasoners**|Yuetai Li et.al.|[2502.12143](http://arxiv.org/abs/2502.12143)|null|大型语言模型（LLMs）在复杂推理任务中表现出色，将它们的推理能力提炼到更小的模型中已经显示出前景。然而，我们发现了一个有趣的现象，我们将其称为“小模型可学习性差距”：小模型（参数量≤3B）并不总是从长链式思维（CoT）推理或从更大模型中提炼中受益。相反，当它们在更短、更简单的推理链上微调时，表现更好，这些推理链与其内在的学习能力更相匹配。为了解决这个问题，我们提出了混合提炼，这是一种简单而有效的策略，通过结合长和短CoT示例或从更大和更小模型中进行推理来平衡推理复杂性。我们的实验表明，与仅使用数据训练相比，混合提炼显著提高了小模型的推理性能。这些发现突出了直接强模型提炼的局限性，并强调了适应推理复杂性以实现有效的推理能力转移的重要性。|
|**2025-02-17**|**SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs**|Yige Xu et.al.|[2502.12134](http://arxiv.org/abs/2502.12134)|**[link](https://github.com/xuyige/softcot)**|思维链（CoT）推理通过生成中间推理步骤，使大型语言模型（LLMs）能够解决复杂的推理任务。然而，大多数现有方法都集中在硬性标记解码上，这限制了推理在离散词汇空间内进行，并且可能并不总是最优。尽管最近的研究探索了连续空间推理，但它们往往遭受灾难性遗忘，限制了它们在零样本设置中表现良好的最先进LLMs的应用。为了应对这一挑战，我们提出了一种新的连续空间推理方法，该方法不需要修改底层LLM。具体来说，我们采用一个轻量级辅助模型来生成实例特定的软性思维标记，作为初始的思维链，然后通过一个投影模块将这些标记映射到LLM的表示空间。在五个推理基准测试上的实验结果表明，我们的方法通过监督和参数高效的微调，增强了LLM的推理性能。|
|**2025-02-17**|**Transformer Dynamics: A neuroscientific approach to interpretability of large language models**|Jesseba Fernando et.al.|[2502.12131](http://arxiv.org/abs/2502.12131)|null|随着人工智能模型在规模和功能上的激增，对其内部机制的理解仍然是一个关键的挑战。受神经科学中动态系统方法成功的启发，在此我们提出了一种研究深度学习系统计算的新框架。我们关注变压器模型中的残差流（RS），将其概念化为跨越层级的动态系统。我们发现，尽管RS不是一个特权的基，但其各个单元的激活在层间表现出强烈的连续性。在RS中的激活随着层级的增加而加速并变得更加密集，而单个单元追踪不稳定的周期轨道。在低层，降低维度的空间中，RS遵循一个具有类似吸引子动态的曲线轨迹。这些见解将动态系统理论与机制可解释性相结合，为结合理论严谨性和大规模数据分析以深化我们对现代神经网络的理解的“人工智能神经科学”奠定了基础。|
|**2025-02-17**|**Scaling Autonomous Agents via Automatic Reward Modeling And Planning**|Zhenfang Chen et.al.|[2502.12130](http://arxiv.org/abs/2502.12130)|null|大型语言模型（LLMs）在多种文本生成任务中展现了非凡的能力。然而，LLMs在需要多步骤决策和环境反馈的问题上仍存在困难，如在线购物、科学推理和数学问题解决。与纯文本数据不同，收集大规模决策数据具有挑战性。此外，许多强大的LLMs只能通过API访问，这由于成本和复杂性阻碍了它们在代理任务中的微调。为了解决LLM代理的限制，我们提出了一种框架，该框架能够从环境中自动学习奖励模型而不需要人工标注。该模型可用于评估LLM代理的动作轨迹并提供任务规划的经验法则。具体来说，我们的方法包括使用一个基于LLM的代理随机导航环境，生成多样化的动作轨迹。随后，利用另一个LLM为每个轨迹分配任务意图并合成正确的响应和负响应。这些三元组（任务意图、正确响应和负响应）随后被用作训练数据，以优化能够评分动作轨迹的奖励模型。通过在不同代理基准上的评估，我们证明了我们框架的有效性和泛化能力。总之，我们提出的框架在增强LLM代理决策能力方面取得了重大进展。通过自动化奖励模型的学习，我们克服了数据稀缺和API限制的挑战，有可能彻底改变LLMs在复杂和交互式环境中的应用。这项研究为更复杂的AI代理铺平了道路，这些代理能够解决需要多步骤决策的广泛现实世界问题。|
|**2025-02-17**|**Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA**|Patryk Marszałek et.al.|[2502.12122](http://arxiv.org/abs/2502.12122)|**[link](https://github.com/gmum/b-lora-xs)**|低秩自适应（LoRA）通过将权重更新分解为低秩矩阵，实现了对大型语言模型的参数高效微调，显著降低了存储和计算开销。虽然这种方法有效，但标准LoRA缺乏不确定性量化机制，导致模型过度自信且校准不佳。LoRA的贝叶斯变体解决了这一限制，但代价是可训练参数数量显著增加，部分抵消了原始的效率提升。此外，这些模型更难以训练，可能遭受不稳定收敛的问题。在本研究中，我们提出了一种新的参数高效的贝叶斯LoRA，证明了在非常低维的参数空间中可以实现有效的不确定性量化。所提出的方法在保持计算效率的同时，实现了强大的性能、改进的校准和泛化能力。我们的实验结果表明，通过适当的权重空间投影：1）不确定性可以在低维空间中有效建模，2）权重协方差表现出低秩特征。|
|**2025-02-17**|**LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws**|Prasanna Mayilvahanan et.al.|[2502.12120](http://arxiv.org/abs/2502.12120)|null|通过提供模型大小、标记和计算之间最佳平衡的估计，规模定律指导着大型语言模型（LLMs）的开发。最近，损失到损失的比例定律，它将预训练数据集和下游任务之间的损失联系起来，已经成为理解和改进LLM性能的有力工具。在这项工作中，我们研究了哪些因素对损失到损失的比例影响最大。我们的实验表明，预训练数据和分词器决定了比例趋势。相比之下，模型大小、优化超参数，甚至是像Llama这样的基于transformer的模型和像Mamba这样的状态空间模型之间的重大架构差异，影响有限。因此，从业者应仔细挑选合适的预训练数据集以实现最佳下游性能，而架构和其他设置则可以自由优化以提高训练效率。|
|**2025-02-17**|**PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection**|Jinhe Bi et.al.|[2502.12119](http://arxiv.org/abs/2502.12119)|null|视觉指令微调通过改进预训练的多模态大型语言模型（MLLMs）来提升其在现实任务中的性能。然而，视觉指令数据集的快速扩展导致了显著的数据冗余，进而增加了过高的计算成本。现有的数据选择方法主要依赖于代理模型或基于损失的指标，这两种方法由于需要模型推理和反向传播，因此都带来了大量的计算开销。为了解决这一挑战，我们提出了PRISM，这是一种新型的无需训练的多模态数据选择方法。与现有方法不同，PRISM消除了对代理模型的依赖，去除了预热预训练和基于梯度的优化。相反，它利用皮尔逊相关分析来量化MLLMs的内在视觉编码特性，计算一个特定于任务的关联度评分来识别高价值实例。这不仅实现了数据高效的选择，还保持了原始的性能。在多个MLLMs上的实证评估表明，PRISM将视觉指令微调和数据选择所需的总时间缩短到传统方法的30%，同时在八个多模态和三个语言理解基准测试中超越了完全微调的模型，最终性能提升了101.7%。|
|**2025-02-14**|**MM-RLHF: The Next Step Forward in Multimodal LLM Alignment**|Yi-Fan Zhang et.al.|[2502.10391](http://arxiv.org/abs/2502.10391)|null|尽管在多模态大型语言模型（MLLMs）方面取得了显著的进展，但大多数最先进的模型尚未与人类偏好进行彻底对齐。这种差距存在的原因是，当前的对齐研究主要在特定领域（例如幻觉减少）取得了进展，而将模型与人类偏好对齐是否可以系统地提高MLLM能力这一更广泛的问题仍基本未得到探索。为此，我们引入了MM-RLHF，一个包含120k个细粒度、人工标注的偏好比较对的数据集。这个数据集在规模、多样性、标注粒度和质量方面都超过了现有资源。利用这个数据集，我们提出了几个关键创新，以提高奖励模型的质量和对齐算法的效率。值得注意的是，我们引入了一种基于批评的奖励模型，在分配分数之前对模型输出进行批评，与传统的标量奖励机制相比，提供了更好的可解释性和更有信息量的反馈。此外，我们还提出了动态奖励缩放，这是一种根据奖励信号调整每个样本损失权重的的方法，从而优化了高质量比较对的使用。我们的方法在10个不同的维度和27个基准上进行了严格评估，结果表明模型性能有显著和一致性的提升。具体来说，使用MM-RLHF和我们的对齐算法微调LLaVA-ov-7B，使对话能力提高了19.5%，安全性提高了60%。我们已经开源了偏好数据集、奖励模型、训练和评估代码，以及奖励建模和安全基准。更多详情请访问我们的项目页面：https://mm-rlhf.github.io。|
|**2025-02-14**|**Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction**|WonJin Yoon et.al.|[2502.10388](http://arxiv.org/abs/2502.10388)|null|近年来，大型语言模型（LLMs）在无需针对特定数据集进行监督训练的情况下，已能够自动处理长篇文档。然而，与简单的信息提取任务相比，它们在复杂任务中的零样本性能仍然不够理想。对于具有长、复杂数据输入的任务，一个可行的方法是首先总结文档，然后对总结进行监督微调。然而，总结过程不可避免地会导致一些信息损失。在本研究中，我们提出了一种处理长文档总结的方法，旨在捕捉原始文档的不同重要方面。我们假设使用不同方面提示生成的LLM总结包含不同的“信息信号”，并提出方法来测量这些差异。我们引入了有效集成这些不同总结信号的途径，以用于监督训练transformer模型。我们使用来自四家医院的真实世界数据，在具有高度影响力的任务——30天精神科出院后的再次入院预测——上验证了我们的假设，并表明我们提出的方法提高了预测患者结果的复杂任务的预测性能。|
|**2025-02-14**|**Enhancing Multilingual LLM Pretraining with Model-Based Data Selection**|Bettina Messmer et.al.|[2502.10361](http://arxiv.org/abs/2502.10361)|null|数据集整理已成为强大大型语言模型（LLM）性能的基础。虽然存在各种基于规则的过滤启发式方法用于英语和多语言数据集，但基于模型的过滤技术主要关注英语。为了解决因对非英语语言研究有限而导致的差异，我们提出了一种针对多语言数据集的基于模型的过滤框架，旨在识别多样化的结构化和知识丰富的样本。我们的方法强调透明度、简单性和效率，利用基于Transformer和FastText的分类器确保我们的技术和数据具有广泛的可访问性。我们在FineWeb-2网络爬虫数据集上进行了全面的消融研究，涵盖了不同的语言家族、书写系统和资源可用性，以证明我们方法的有效性。训练一个1B参数的Llama模型，对于70B和119B个标记，我们的方法只需15%的训练标记即可匹配基线MMLU分数，同时也在其他基准测试中有所改进。这些发现为我们方法对其他语言的泛化能力提供了强有力的证据。因此，我们将我们的框架扩展到20种语言，并为这些语言发布了精炼的预训练数据集。|
|**2025-02-14**|**Organize the Web: Constructing Domains Enhances Pre-Training Data Curation**|Alexander Wettig et.al.|[2502.10341](http://arxiv.org/abs/2502.10341)|null|现代语言模型是在包含数万亿个标记的大规模、非结构化数据集上训练的，这些数据集是通过网络爬虫获取的。非结构化的特性使得难以对其内容进行推理，并发展出数据整理的系统方法。在本文中，我们通过对内容进行分类并组织到各个领域来解构单一的网页语料库。我们介绍了WebOrganizer，这是一个从主题和格式两个方面组织网页的框架。利用这两个互补的概念，我们通过将大型语言模型的标注精炼为高效的分类器来自动标注预训练数据。这使得我们能够研究不同领域的数据应该如何混合以改进下游任务中的模型，并且我们展示了我们可以结合关于有效主题和格式的见解来进一步提升性能。我们证明了我们的领域混合方法也提高了基于质量选择数据的方法。此外，我们还研究了并比较了基于质量的方法如何隐性地改变领域混合。总的来说，我们的工作表明构建和混合领域为基于质量的数据整理方法提供了宝贵的补充，为有效且富有洞察力的预训练数据整理开辟了新的途径。|
|**2025-02-14**|**Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering**|Nick Ferguson et.al.|[2502.10338](http://arxiv.org/abs/2502.10338)|null|大型语言模型（LLMs）在自然语言任务上表现出色，但在需要复杂、多步骤推理的问答（QA）任务中仍面临挑战。我们概述了这些任务中所需推理的类型，并将它们重新表述为元级推理（类似于高级战略推理或规划）和对象级推理（体现在数学推理等低级任务中）。我们介绍了Franklin，这是一个需要元级和对象级推理的新数据集，并将其与另外三个数据集一起用于评估四个LLMs在需要多步骤推理的问答任务中的表现。来自人工标注研究的结果表明，LLMs在高频次地展现出元级推理，但在某些数据集中，它们在对象级推理任务上遇到了困难。此外，证据表明，LLMs发现Franklin数据集中问题的对象级推理具有挑战性，但它们在元级推理要求方面表现出色。|
|**2025-02-14**|**LLM-Powered Preference Elicitation in Combinatorial Assignment**|Ermis Soumalias et.al.|[2502.10308](http://arxiv.org/abs/2502.10308)|null|我们研究了大型语言模型（LLMs）作为人类代理简化组合分配中的偏好获取（PE）的潜力。虽然传统的PE方法依赖于迭代查询来捕捉偏好，但LLMs提供了一种减少人力投入的一次性替代方案。我们提出了一种LLM代理框架，可以与最先进的ML驱动的偏好获取方案协同工作。我们的框架处理了LLMs引入的新挑战，如响应变异性和增加的计算成本。我们通过在研究广泛的课程分配领域中对LLM代理的效率进行实验评估，并调查了成功所需的模型能力。我们发现，我们的方法将分配效率提高了高达20%，并且这些结果在不同LLMs以及报告质量和准确性的差异中都是稳健的。|
|**2025-02-14**|**Open-Source AI-Powered Optimization in Scalene: Advancing Python Performance Profiling with DeepSeek-R1 and LLaMA 3.2**|Saem Hasan et.al.|[2502.10299](http://arxiv.org/abs/2502.10299)|null|Python的灵活性和易用性是以性能低效为代价的，这要求开发者依赖分析器来优化执行。SCALENE是一款高性能的CPU、GPU和内存分析器，它能够在运行速度上显著快于传统分析器的同时，为Python应用程序提供细致的洞察。最初，SCALENE集成了OpenAI的API以生成基于AI的优化建议，但其对专有API的依赖限制了其可访问性。本研究探讨了在SCALENE中使用开源大型语言模型（LLMs），如DeepSeek-R1和Llama 3.2，来生成优化建议的可行性。我们的评估显示，DeepSeek-R1提供的代码优化效果与专有模型相当。我们将DeepSeek-R1集成到SCALENE中，以自动分析性能瓶颈并提出改进建议，从而增强了SCALENE的实用性，同时保持了其开源特性。这项研究证明了开源LLMs可以作为AI驱动的代码优化的可行替代品，为更易于访问和成本效益更高的性能分析工具铺平了道路。|
|**2025-02-14**|**Are Large Language Models the future crowd workers of Linguistics?**|Iris Ferrazzo et.al.|[2502.10266](http://arxiv.org/abs/2502.10266)|null|从人类参与者中获取数据是实证语言学研究中使用的一种核心数据收集策略。这类研究中的参与者数量可能差异很大，从少数到众包规模不等。即使它们提供了资源丰富的数据，这两种设置也伴随着许多缺点，例如在完成任务过程中对参与者注意力的控制不足、众包环境中的工作条件不稳定，以及耗时的实验设计。因此，这项研究旨在回答一个问题：如果将大型语言模型（LLMs）纳入实证语言学流程，它们是否能够克服这些障碍。进行了两个复制案例研究，以澄清这一问题：Cruz（2023）和Lombard等人（2021）。两个原本为人类参与者设计的强制提取任务在所提出的框架中通过OpenAI的GPT-4o-mini模型进行了复制。其与我们的零样本提示基线相结合的性能显示了LLMs的有效性和高度灵活性，这些模型在语言任务中往往优于人类信息提供者。第二次复制的发现进一步突出了探索额外提示技术（如思维链（CoT）提示）的必要性，在第二次后续实验中，这些技术在对关键项目和填充项目的人类性能上表现出更高的对齐度。鉴于这项研究的规模有限，进一步探索LLMs在实证语言学以及其他未来人文学科应用中的性能是值得的。|
|**2025-02-14**|**Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers**|Aivin V. Solatorio et.al.|[2502.10263](http://arxiv.org/abs/2502.10263)|**[link](https://github.com/worldbank/ai4data-use)**|通过追踪数据在研究论文中的提及和使用情况，可以为改善数据的可发现性、质量和生产提供关键见解。然而，手动识别和分类大量学术文献中的数据集提及是一项资源密集型任务，且不具备可扩展性。本文提出了一种机器学习框架，通过利用大型语言模型（LLMs）、合成数据和两阶段微调过程来自动化研究领域的数据集提及检测。我们采用零样本提取研究论文、将LLM作为质量评估的评判者以及推理代理进行细化，以生成一个弱监督的合成数据集。Phi-3.5-mini instruct模型在此数据集上进行预微调，随后在手动标注的子集上进行微调。在推理阶段，基于ModernBERT的分类器高效地过滤数据集提及，降低了计算开销同时保持了高召回率。在保留的手动标注样本上评估，我们的微调模型在数据集提取准确性上优于NuExtract-v1.5和GLiNER-large-v2.1。我们的结果表明，LLM生成的合成数据可以有效解决训练数据稀缺问题，提高低资源环境下的泛化能力。本框架为可扩展监测数据集使用提供了一条途径，增强透明度，并支持研究人员、资助者和政策制定者在识别数据差距和加强数据可访问性方面做出明智决策。|
|**2025-02-14**|**VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models**|Gokul Karthik Kumar et.al.|[2502.10250](http://arxiv.org/abs/2502.10250)|null|视觉-语言模型（VLMs）在各种视觉基准测试中表现出色，但通常受限于缺乏高质量的视觉微调数据。为了解决这一挑战，我们引入了VisCon-100K，这是一个从交错图像-文本网络文档中派生出的新数据集。我们的方法将OBELICS数据集中的45K个网络文档转换成100K个图像对话样本。我们利用GPT-4V生成图像上下文描述，并使用OpenChat 3.5模型将这些描述转换为多样化的自由形式和多项选择题-答案对。将此数据集用于微调显著提高了VLM在多个基准测试中的性能。与仅关注细粒度视觉内容的方法不同，我们的方法利用了伴随的网页上下文，从而获得了更优的结果。我们还发现，一种“泄漏模态混合”方法，其中对话样本包含可以从图像及其上下文描述中回答的问题，比非泄漏的描述和问答对组合表现更佳。VisCon-100k数据集在两种流行的VLM方法中表现出强大的性能：仅文本的大型语言模型（LLM）与使用图像描述数据的视觉编码器对齐（ShareGPT4V-7b）和多模态预训练LLM（IDEFICS2-8b）使用交错图像-文本数据。除了发布VisCon-100K数据集外，我们还提供了一个在此数据集上训练的上下文描述器，便于未来研究和开源应用中可扩展的微调数据生成。我们使用相同的流程，但用我们训练的上下文描述器替换GPT-4V，还发布了更大的VisCon-1M数据集。|
|**2025-02-13**|**MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency**|Dongzhi Jiang et.al.|[2502.09621](http://arxiv.org/abs/2502.09621)|null|在回答问题时利用思维链（CoT）显著提升了大型语言模型（LLMs）的推理能力，然而其对于大型多模态模型（LMMs）的影响尚未得到系统评估和深入探讨。在本文中，我们介绍了MME-CoT，这是一个专门用于评估LMMs CoT推理性能的基准，涵盖了六个领域：数学、科学、OCR、逻辑、时空和通用场景。作为该领域的首次全面研究，我们提出了一套详尽的评估方案，其中包括三个新颖的指标，从细粒度层面评估推理质量、鲁棒性和效率。利用精心挑选的高质量数据和独特的评估策略，我们对最先进的LMMs进行了深入研究，揭示了几个关键发现：1）具有反思机制的模型显示出更优越的CoT质量，其中Kimi k1.5优于GPT-4o，并展现出最高质量的成果；2）CoT提示往往降低LMM在感知密集型任务上的性能，暗示了可能存在的有害的过度思考行为；3）尽管CoT质量较高，但具有反思机制的LMM在正常响应和自我纠正阶段都表现出明显的低效率。我们希望MME-CoT能够成为推动LMMs多模态推理发展的基石。项目页面：https://mmecot.github.io/|
|**2025-02-13**|**Exploring the Potential of Encoder-free Architectures in 3D LMMs**|Yiwen Tang et.al.|[2502.09620](http://arxiv.org/abs/2502.09620)|**[link](https://github.com/ivan-tang-3d/enel)**|在二维视觉领域，无编码器架构已被初步探索，但它们能否有效地应用于三维理解场景仍然是一个未解之谜。在本文中，我们首次全面研究了无编码器架构克服基于编码器的3D大型多模态模型（LMMs）挑战的潜力。这些挑战包括无法适应不同的点云分辨率以及编码器产生的点特征无法满足大型语言模型（LLMs）的语义需求。我们确定了3D LMMs去除编码器并使LLM承担3D编码器角色的关键方面：1）我们在预训练阶段提出了LLM内嵌语义编码策略，探讨了各种点云自监督损失的影响，并提出了混合语义损失以提取高级语义。2）我们在指令微调阶段引入了分层几何聚合策略。这将在LLM的早期层中引入归纳偏差，以关注点云的局部细节。最终，我们提出了第一个无编码器3D LMM，ENEL。我们的70亿参数模型在分类、标题生成和VQA任务上分别达到了55.0%、50.92%和42.7%，与当前最先进的模型ShapeLLM-13B相当。我们的结果表明，无编码器架构在三维理解领域替代基于编码器的架构具有极高的潜力。代码已发布在https://github.com/Ivan-Tang-3D/ENEL。|
|**2025-02-13**|**Human-LLM Coevolution: Evidence from Academic Writing**|Mingmeng Geng et.al.|[2502.09606](http://arxiv.org/abs/2502.09606)|null|通过对arXiv论文摘要的统计分析，我们发现，自2024年初这些词汇被指出过度使用之后，几个ChatGPT之前频繁使用的词汇，如“深入探究”，其出现频率明显下降。相反，ChatGPT偏爱的某些其他词汇，如“显著的”，其出现频率仍在持续增加。这些现象表明，一些学术论文作者已经调整了他们对大型语言模型（LLMs）的使用方式，例如通过选择输出或对LLM生成的内容进行修改。因此，人类与LLMs的这种协同进化和合作为在现实场景中检测机器生成文本带来了额外的挑战。通过检查词汇频率来评估LLMs对学术写作的影响仍然是可行的，应该更多地关注那些已经频繁使用的词汇，包括那些频率有所下降的词汇。|
|**2025-02-13**|**SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models**|Yung-Sung Chuang et.al.|[2502.09604](http://arxiv.org/abs/2502.09604)|**[link](https://github.com/voidism/selfcite)**|我们介绍了SelfCite，这是一种新颖的自监督方法，旨在将大型语言模型（LLM）与生成高质量、细粒度、句子级引用的语句对齐。SelfCite不是仅仅依赖于昂贵且劳动密集型的标注，而是利用LLM自身通过上下文消除提供的奖励信号：如果引用是必要的，从上下文中移除被引用的文本应该防止生成相同的响应；如果引用是充分的，仅保留被引用的文本应该保留相同的响应。这种奖励可以引导推理时的最佳N个采样策略，显著提高引用质量，同时也可用于偏好优化，直接微调模型以生成更好的引用。SelfCite的有效性通过在LongBench-Cite基准测试中，将五个长篇问答任务的引用F1值提高至5.3个百分点得到验证。|
|**2025-02-13**|**Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs**|Siyan Zhao et.al.|[2502.09597](http://arxiv.org/abs/2502.09597)|**[link](https://github.com/amazon-science/PrefEval)**|大型语言模型（LLMs）越来越多地被用作聊天机器人，但它们在个性化回应用户偏好方面的能力仍然有限。我们介绍了PrefEval，这是一个用于评估LLMs在长语境对话场景中推断、记忆和遵守用户偏好的基准。PrefEval包含3000对人工编纂的用户偏好和查询，覆盖20个主题。PrefEval包含用户个性化或偏好信息，既有显性也有隐性形式，并使用生成和分类任务来评估LLMs的性能。借助PrefEval，我们在多会话对话中评估了10个开源和专有LLMs在上下文长度高达10万个标记时的上述偏好跟踪能力。我们与各种提示、迭代反馈和检索增强生成方法进行了基准测试。我们的基准测试努力显示，最先进的LLMs在主动跟踪用户偏好方面面临重大挑战。特别是，在零样本设置中，大多数评估模型的偏好跟踪准确率在仅10轮（约3k个标记）时低于10%。即使在高级提示和检索方法下，偏好跟踪在长语境对话中仍然恶化。此外，我们展示了在PrefEval上进行微调可以显著提高性能。我们相信，PrefEval可以作为衡量、理解和增强LLMs偏好跟踪能力的重要资源，为个性化对话代理铺平道路。我们的代码和数据集可在https://prefeval.github.io/获取。|
|**2025-02-13**|**KIMAs: A Configurable Knowledge Integrated Multi-Agent System**|Zitao Li et.al.|[2502.09596](http://arxiv.org/abs/2502.09596)|null|基于大型语言模型（LLMs）的知识密集型对话已成为最受欢迎和最有帮助的应用之一，可以在不同方面帮助人们。许多当前的知识密集型应用都集中在检索增强生成（RAG）技术上。虽然许多开源的RAG框架促进了基于RAG的应用开发，但它们在处理由主题和格式中的异构数据、会话上下文管理和低延迟响应时间要求所复杂化的实际场景时往往力不从心。本技术报告提出了一种可配置的知识集成多代理系统，KIMAs，以解决这些挑战。KIMAs具有一个灵活且可配置的系统，用于整合多样化的知识来源，包括1）上下文管理和查询重写机制，以提高检索准确性和多轮对话的连贯性，2）高效的知识路由和检索，3）简单但有效的过滤和引用生成机制，以及4）优化的可并行化多代理管道执行。我们的工作为在现实世界环境中推进LLMs的部署提供了一个可扩展的框架。为了展示KIMAs如何帮助开发者构建不同规模和重点的知识密集型应用，我们演示了如何配置系统以运行在实践中的三个具有可靠性能的应用程序。|
|**2025-02-13**|**Logical forms complement probability in understanding language model (and human) performance**|Yixuan Wang et.al.|[2502.09589](http://arxiv.org/abs/2502.09589)|null|随着对使用大型语言模型（LLMs）进行自然语言规划的兴趣日益增加，理解其行为成为了一个重要的研究问题。这项工作对LLMs在自然语言中进行逻辑推理的能力进行了系统性的研究。我们引入了一个关于命题逻辑和模态逻辑中假设和选言三段论的受控数据集，并将其作为理解LLMs性能的测试平台。我们的研究结果在预测LLMs行为方面提供了新的见解：除了输入概率（Gonen等人，2023年；McCoy等人，2024年）之外，还应将逻辑形式视为正交因素。此外，我们通过比较LLMs和人类的行为结果，展示了人类和LLMs在逻辑推理性能上的相似之处和不同之处。|
|**2025-02-13**|**Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks**|Qian Wan et.al.|[2502.09577](http://arxiv.org/abs/2502.09577)|null|在初稿之前进行构思和组织想法的过程称为预写作。这包括一系列非正式、迭代和半结构化的策略，如视觉图示，这对以轮流对话方式与大型语言模型（LLMs）协作构成挑战。我们提出了Polymind，这是一个利用多个LLM驱动代理支持预写作的视觉图示工具。该系统采用并行协作工作流程，取代了轮流对话交互。它定义了多个“微任务”，以模拟协作写作和小组头脑风暴等群体协作场景。Polymind不仅能够同时编排多个微任务，避免了反复向聊天机器人提出各种目的的提示。用户可以配置和委托定制化的微任务，并通过指定任务要求、切换可见性和主动性来管理他们的微任务。我们的评估显示，与ChatGPT相比，用户在Polymind上的协作定制性更高，因此能够在预写作过程中快速扩展个性化的写作想法。|
|**2025-02-13**|**Zero-shot generation of synthetic neurosurgical data with large language models**|Austin A. Barr et.al.|[2502.09566](http://arxiv.org/abs/2502.09566)|**[link](https://github.com/aabarr/Synthetic-Neurosurgical-Data)**|临床数据对于神经外科研究至关重要，但其获取往往受到数据可用性、样本量小、隐私法规以及资源密集型的预处理和去识别化程序的制约。合成数据为解决获取和使用真实世界数据（RWD）的相关挑战提供了潜在解决方案。本研究旨在通过与条件表格生成对抗网络（CTGAN）进行基准测试，评估大型语言模型（LLM）GPT-4o生成合成神经外科数据的零样本生成能力。合成数据集与真实世界神经外科数据进行比较，以评估其真实性（均值、比例、分布和双变量相关）、实用性（在RWD上机器学习分类器的性能）和隐私性（RWD中记录的重复）。GPT-4o生成的数据集在未进行微调或访问RWD进行预训练的情况下，其性能与CTGAN相当或更优。数据集显示出对RWD的高单变量和双变量真实性，且未直接暴露任何真实患者记录，即使在样本量放大的情况下。在GPT-4o生成的数据上训练机器学习分类器并在RWD上进行测试以进行二元预测任务，结果显示F1分数为0.706，与在CTGAN数据上训练（0.705）预测术后功能状态恶化的性能相当。GPT-4o显示出生成高真实性合成神经外科数据的潜力。这些发现还表明，使用GPT-4o合成的数据可以有效地增加小样本量的临床数据，并训练用于预测神经外科结果的机器学习模型。为进一步提高分布特性保留和提升分类器性能，需要进一步研究。|
|**2025-02-13**|**MDCrow: Automating Molecular Dynamics Workflows with Large Language Models**|Quintina Campbell et.al.|[2502.09565](http://arxiv.org/abs/2502.09565)|**[link](https://github.com/ur-whitelab/MDCrow)**|分子动力学（MD）模拟对于理解生物分子系统至关重要，但其自动化仍然具有挑战性。最近，大型语言模型（LLM）在利用LLM代理自动化复杂科学任务方面取得了成功。在本文中，我们介绍了MDCrow，这是一个能够自动化MD工作流程的智能代理型LLM助手。MDCrow利用40多个专家设计的工具进行文件处理、设置模拟、分析模拟输出以及从文献和数据库中检索相关信息。我们对MDCrow在25个不同难度和所需子任务的工作任务中的表现进行了评估，并评估了该代理对难度和提示风格的鲁棒性。\texttt{gpt-4o}能够以低方差完成复杂任务，紧随其后的是具有说服力的开源模型\texttt{llama3-405b}。虽然提示风格不会影响最佳模型的表现，但它对较小模型有显著影响。|
|**2025-02-12**|**Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples**|Andrianos Michail et.al.|[2502.08638](http://arxiv.org/abs/2502.08638)|null|评估模型跨语言语义搜索能力通常仅限于信息检索和语义文本相似度等任务中的现有数据集。为了允许特定领域的评估，我们引入了一种新的跨语言语义搜索任务——跨语言语义区分（CLSD），它只需要目标领域中感兴趣的语言对的一组平行句子对。该任务侧重于模型跨语言将真实平行句子排在由大型语言模型生成的硬负例之上的能力。我们为新闻领域中的德语-法语语言对创建了四个CLSD任务实例。在本案例研究中，我们发现同时针对检索任务进行微调的模型（例如，多语言E5）在以英语作为枢纽语言时受益，而像LaBSE这样的双语文本挖掘模型在直接跨语言的情况下表现最佳。我们还展示了由我们的干扰生成策略启用的细粒度相似度分析，这表明不同的嵌入模型对不同的扰动类型敏感。|
|**2025-02-12**|**Ensemble based approach to quantifying uncertainty of LLM based classifications**|Srijith Rajamohan et.al.|[2502.08631](http://arxiv.org/abs/2502.08631)|null|首先，我们需要理解摘要中的关键术语和概念：  1. **Large Language Models (LLMs)**：大型语言模型。 2. **internal model's parameters**：模型内部参数。 3. **context window**：上下文窗口。 4. **greedy sampling strategy**：贪婪采样策略。 5. **variance**：方差。 6. **conceptual certainty**：概念确定性。 7. **parametric knowledge**：参数知识。 8. **lexical variance**：词汇方差。 9. **Finetuning**：微调。 10. **classification problem**：分类问题。 11. **probabilistic method**：概率方法。 12. **certainties of the predicted classes**：预测类别的确定性。  接下来，我们将摘要逐步翻译为中文：  - The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window.   大型语言模型（LLMs）的输出是内部模型参数和输入到上下文窗口中的输入的函数。  - The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input.   这里提出的假设是，在贪婪采样策略下，LLMs输出的方差是模型参数知识中嵌入的概念确定性以及输入中的词汇方差的一个函数。  - Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations.   微调模型会导致模型输出对词汇输入变化的敏感性降低。  - This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes.   然后将此应用于分类问题，并提出了一种概率方法来估计预测类别的确定性。  最终的中文翻译结果为：  大型语言模型（LLMs）的输出是内部模型参数和输入到上下文窗口中的输入的函数。这里提出的假设是，在贪婪采样策略下，LLMs输出的方差是模型参数知识中嵌入的概念确定性以及输入中的词汇方差的一个函数。微调模型会导致模型输出对词汇输入变化的敏感性降低。然后将此应用于分类问题，并提出了一种概率方法来估计预测类别的确定性。|
|**2025-02-12**|**Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks**|Ang Li et.al.|[2502.08586](http://arxiv.org/abs/2502.08586)|null|近期大量机器学习安全文献聚焦于针对对齐的大型语言模型（LLMs）的攻击。这些攻击可能提取私人信息或迫使模型生成有害输出。在实际部署中，LLMs 通常是大型的智能管道的一部分，包括记忆系统、检索、网络访问和API调用。这些额外的组件引入了漏洞，使得这些由LLM驱动的代理比孤立的LLMs更容易受到攻击，但相对较少的研究关注LLM代理的安全性。在本文中，我们分析了LLM代理独有的安全和隐私漏洞。我们首先提供了一个按威胁行为者、目标、入口点、攻击者可观测性、攻击策略和代理管道固有问题性进行分类的攻击分类法。然后，我们对流行的开源和商业代理进行了一系列示范性攻击，展示了其漏洞的即时实际影响。值得注意的是，我们的攻击易于实现，并且不需要对机器学习有任何了解。|
|**2025-02-12**|**QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval**|Wonduk Seo et.al.|[2502.08557](http://arxiv.org/abs/2502.08557)|null|查询扩展在信息检索（IR）中广泛用于通过丰富查询以增加额外的上下文信息来提高搜索结果。尽管基于大型语言模型（LLM）的最近方法通过多个提示生成伪相关内容和扩展术语，但它们往往产生重复且狭窄的扩展，缺乏检索所有相关信息所需的多样化上下文。在本文中，我们介绍了一种新颖且有效的查询扩展框架QA-Expand。它首先从初始查询生成多个相关问题，然后产生相应的伪答案作为代理文档。一个反馈模型进一步重写和过滤这些答案，以确保仅包含最富有信息量的增强。在BEIR和TREC等基准上的大量实验表明，QA-Expand比最先进的方法提高了高达13%的检索性能，为现代检索挑战提供了一个稳健的解决方案。|
|**2025-02-12**|**Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies**|Sunnie S. Y. Kim et.al.|[2502.08554](http://arxiv.org/abs/2502.08554)|null|大型语言模型（LLMs）可能会生成听起来流畅且令人信服的错误回应，这增加了用户将这些回应当作正确信息的风险。减轻这种过度依赖是一个关键挑战。通过一项参与者使用LLM增强的应用程序来回答客观问题的有声思维研究，我们确定了LLM回应的几个特征，这些特征塑造了用户的依赖性：解释（答案的支持性细节）、解释的不一致性以及来源。通过一项大规模、预先注册、受控的实验（N=308），我们隔离并研究了这些特征对用户依赖性、准确性及其他指标的影响。我们发现，解释的存在增加了对正确和错误回应的依赖。然而，当提供来源或解释出现不一致性时，我们发现对错误回应的依赖性降低。我们讨论了这些发现对培养对LLMs适当依赖性的影响。|
|**2025-02-12**|**LLMs can implicitly learn from mistakes in-context**|Lisa Alazraki et.al.|[2502.08550](http://arxiv.org/abs/2502.08550)|null|从错误中学习是人类智能的基本特征。先前的研究表明，大型语言模型（LLMs）在提供全面解释的情况下，即详细说明为什么答案错误或如何纠正它，也能从错误的答案中学习。在本工作中，我们探讨了当没有提供这种解释时，LLMs是否能在数学推理任务中从错误中学习。我们研究了LLMs是否能够仅通过观察错误和正确答案来隐式推断这种理由。令人惊讶的是，我们发现当从上下文中消除理由时，LLMs的平均表现更好，错误答案只是简单地与正确答案一起展示。这种方法在我们的评估中也显著优于思维链提示。我们表明，这些结果在不同大小和不同推理能力的LLMs中是一致的。此外，我们进行了深入分析，并表明与将更多样化的问答对引入上下文相比，同时提示错误和正确答案会导致更好的性能和更好的泛化。最后，我们表明，仅观察到错误和正确答案的模型生成的新理由，在人类评分上与那些通过示例理由辅助产生的理由得分相当。我们的结果表明，LLMs确实能够进行上下文隐式学习。|
|**2025-02-12**|**LLM Pretraining with Continuous Concepts**|Jihoon Tack et.al.|[2502.08524](http://arxiv.org/abs/2502.08524)|null|接下来，我们将逐步推理并给出摘要的中文翻译。  1. **Next token prediction**：下一个标记预测，在大型语言模型预训练中作为标准训练目标。    - 翻译：下一个标记预测是大型语言模型预训练中使用的标准训练目标。  2. **Representations are learned as a result of optimizing for token-level perplexity**：通过优化标记级困惑度来学习表示。    - 翻译：通过优化标记级困惑度来学习表示。  3. **We propose Continuous Concept Mixing (CoCoMix)**：我们提出了连续概念混合（CoCoMix）。    - 翻译：我们提出了连续概念混合（CoCoMix）。  4. **a novel pretraining framework that combines discrete next token prediction with continuous concepts**：一个将离散的下一个标记预测与连续概念相结合的新型预训练框架。    - 翻译：一个将离散的下一个标记预测与连续概念相结合的新型预训练框架。  5. **Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations**：具体来说，CoCoMix预测从预训练的稀疏自动编码器学习到的连续概念，并通过与标记隐藏表示交织将其混合到模型的隐藏状态中。    - 翻译：具体来说，CoCoMix预测从预训练的稀疏自动编码器学习到的连续概念，并通过与标记隐藏表示交织将其混合到模型的隐藏状态中。  6. **Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks**：通过在多个基准测试中的实验，包括语言建模和下游推理任务。    - 翻译：通过在多个基准测试中的实验，包括语言建模和下游推理任务。  7. **we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens**：我们表明，CoCoMix更高效地利用样本，并且始终优于标准的下一个标记预测、知识蒸馏和插入暂停标记。    - 翻译：我们表明，CoCoMix更高效地利用样本，并且始终优于标准的下一个标记预测、知识蒸馏和插入暂停标记。  8. **We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains**：我们发现，在一个端到端框架中将概念学习和交织相结合对于性能提升至关重要。    - 翻译：我们发现，在一个端到端框架中将概念学习和交织相结合对于性能提升至关重要。  9. **Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept**：此外，CoCoMix通过允许直接检查和修改预测的概念，增强了可解释性和可控性。    - 翻译：此外，CoCoMix通过允许直接检查和修改预测的概念，增强了可解释性和可控性。  10. **offering a transparent way to guide the model's internal reasoning process**：提供了一种透明的引导模型内部推理过程的方法。     - 翻译：提供了一种透明的引导模型内部推理过程的方法。  综合以上步骤，以下是摘要的中文翻译：  下一个标记预测一直是大型语言模型预训练中的标准训练目标。通过优化标记级困惑度来学习表示。我们提出了连续概念混合（CoCoMix）这一新型预训练框架，它将离散的下一个标记预测与连续概念相结合。具体来说，CoCoMix预测从预训练的稀疏自动编码器学习到的连续概念，并通过与标记隐藏表示交织将其混合到模型的隐藏状态中。通过在多个基准测试中的实验，包括语言建模和下游推理任务，我们表明CoCoMix更高效地利用样本，并且始终优于标准的下一个标记预测、知识蒸馏和插入暂停标记。我们发现，在一个端到端框架中将概念学习和交织相结合对于性能提升至关重要。此外，CoCoMix通过允许直接检查和修改预测的概念，增强了可解释性和可控性，提供了一种透明的引导模型内部推理过程的方法。|
|**2025-02-12**|**The Paradox of Stochasticity: Limited Creativity and Computational Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data**|Evgenii Evstafev et.al.|[2502.08515](http://arxiv.org/abs/2502.08515)|null|本研究探讨了温度设置和模型架构如何影响三个大型语言模型（LLMs）生成结构化虚构数据（如姓名、出生日期）。研究对象包括 llama3.1:8b、deepseek-r1:8b 和 mistral:latest。通过系统地测试从0.0到1.0的温度值，以0.1为增量进行330次试验，生成了889个结构化实体，并验证了它们的句法一致性。主要发现表明，模型架构显著影响了计算效率，mistral:latest 和 llama3.1:8b 的数据处理速度比 deepseek-r1:8b 快8倍。与预期相反，温度与处理时间无相关性，挑战了关于随机采样成本的假设。输出多样性仍然有限，因为所有温度下模型都持续默认使用常见的姓名原型（例如，“John Doe”和“Jane Smith”），尽管罕见名称在中间值（0.3-0.7）聚集。这些结果表明，在结构化生成任务中，架构优化而非温度调整主导了性能。这些发现强调了在选择模型时优先考虑效率，并建议在合成数据管道中需要显式多样性约束来减轻默认输出偏差。|
|**2025-02-12**|**Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation**|Mahnaz Koupaee et.al.|[2502.08514](http://arxiv.org/abs/2502.08514)|**[link](https://github.com/amazon-science/madisse)**|基于大型语言模型（LLMs）的忠实度评估器常常被文本的流畅性所迷惑，难以识别摘要中的错误。我们提出了一种摘要忠实度评估方法，其中多个基于LLMs的代理被分配初始立场（无论他们的信念可能是什么），并被迫提出理由来证明所强加的信念，从而进行多轮辩论以达成一致。均匀分布的初始分配导致立场更加多样化，从而产生更有意义的辩论，并最终识别出更多错误。此外，通过分析最近的忠实度评估数据集，我们发现，摘要要么忠实于源文档，要么不忠实，并不总是如此。因此，我们引入了一个新的维度——模糊性，以及一个详细的分类法来识别这些特殊情况。实验表明，我们的方法有助于识别模糊性，并在非模糊摘要上表现出更强的性能。|
|**2025-02-12**|**Measuring Diversity in Synthetic Datasets**|Yuchang Zhu et.al.|[2502.08512](http://arxiv.org/abs/2502.08512)|**[link](https://github.com/bluewhalelab/dcscore)**|大型语言模型（LLMs）被广泛用于为各种自然语言处理（NLP）任务生成合成数据集，如文本分类和摘要。然而，准确测量这些合成数据集的多样性——这对于稳健的模型性能至关重要——仍然是一个重大挑战。在本文中，我们引入了DCScore，这是一种从分类角度测量合成数据集多样性的新方法。具体来说，DCScore将多样性评估定义为样本分类任务，利用样本之间的相互关系。我们进一步对DCScore满足的与多样性相关的公理进行了理论验证，突出了其作为原则性多样性评估方法的作用。在合成数据集上的实验结果表明，DCScore与评估数据集的多个多样性伪真实值的相关性更强，凸显了其有效性。此外，实证和理论证据都表明，与现有方法相比，DCScore大幅降低了计算成本。代码可在以下网址获取：https://github.com/BlueWhaleLab/DCScore。|
|**2025-02-11**|**DarwinLM: Evolutionary Structured Pruning of Large Language Models**|Shengkun Tang et.al.|[2502.07780](http://arxiv.org/abs/2502.07780)|**[link](https://github.com/IST-DASLab/DarwinLM)**|大型语言模型（LLMs）在各个自然语言处理（NLP）任务上取得了显著的成功。然而，它们庞大的计算成本限制了它们的广泛应用，尤其是在实时应用中。结构化剪枝通过压缩模型并直接提供端到端速度提升，为解决这个问题提供了一个有效的解决方案，且与硬件环境无关。同时，模型的各个组成部分对剪枝的敏感性各不相同，这要求进行非均匀模型压缩。然而，剪枝方法不仅应识别出有能力的子结构，还应考虑压缩后的训练。为此，我们提出了一个名为 \sysname 的 \emph{训练感知}结构化剪枝方法。\sysname 建立在进化搜索过程之上，通过变异在每个代中生成多个子模型，并选择最适应的模型进行生存。为了评估训练后的效果，我们在子模型群体中纳入了一个轻量级的多步骤训练过程，在每次选择阶段逐步增加标记数，并淘汰表现不佳的模型。我们通过在 Llama-2-7B、Llama-3.1-8B 和 Qwen-2.5-14B-Instruct 上进行广泛的实验来验证我们的方法，实现了结构化剪枝的最先进性能。例如，\sysname 在压缩后训练阶段所需的训练数据量比 ShearedLlama 少 $5\times$ ，同时超越了 ShearedLlama。|
|**2025-02-11**|**Auditing Prompt Caching in Language Model APIs**|Chenchen Gu et.al.|[2502.07776](http://arxiv.org/abs/2502.07776)|**[link](https://github.com/chenchenygu/auditing-prompt-caching)**|在大规模语言模型（LLMs）中，提示缓存会导致数据依赖的时序变化：缓存过的提示比未缓存的提示处理速度更快。这些时序差异引入了旁路时序攻击的风险。例如，如果缓存被多个用户共享，攻击者可以通过快速API响应时间来识别缓存过的提示，从而获取有关其他用户提示的信息。由于提示缓存可能导致隐私泄露，因此API提供商的缓存策略透明度非常重要。为此，我们开发并进行了统计审计，以检测现实世界LLM API提供商中的提示缓存。我们在包括OpenAI在内的七个API提供商中检测到跨用户的全局缓存共享，可能导致用户提示的潜在隐私泄露。提示缓存引起的时序变化也可能导致模型架构信息的泄露。具体来说，我们发现证据表明OpenAI的嵌入模型是一个仅解码器的Transformer，这一信息之前并未公开。|
|**2025-02-11**|**Automatic Robot Task Planning by Integrating Large Language Model with Genetic Programming**|Azizjon Kobilov et.al.|[2502.07772](http://arxiv.org/abs/2502.07772)|null|准确的任务规划对于控制自主系统，如机器人、无人机和自动驾驶车辆至关重要。行为树（BTs）因其模块化、灵活性和可重用性，被认为是在任务规划中最突出的控制策略定义框架之一。为机器人系统生成可靠且准确的行为树（BT）控制策略仍然具有挑战性，通常需要领域专业知识。在本文中，我们提出了LLM-GP-BT技术，该技术利用大型语言模型（LLM）和遗传编程（GP）来自动化行为树的生成和配置。LLM-GP-BT技术处理以人类自然语言表达的机器人任务命令，并以计算高效和用户友好的方式将它们转换为准确可靠的行为树任务计划。所提出的技术通过仿真实验系统性地开发和验证，展示了其在简化自主系统任务规划方面的潜力。|
|**2025-02-11**|**Great Power Brings Great Responsibility: Personalizing Conversational AI for Diverse Problem-Solvers**|Italo Santos et.al.|[2502.07763](http://arxiv.org/abs/2502.07763)|null|新加入开源软件（OSS）项目的新手面临许多挑战。大型语言模型（LLMs），如ChatGPT，已成为回答问题和提供指导的潜在资源，许多开发者现在更倾向于使用ChatGPT而不是传统的问答网站如Stack Overflow。然而，LLMs在呈现信息时可能存在偏见，这可能会对解决问题风格可能未被广泛代表的新手产生特别影响。这引发了关于AI驱动支持对OSS项目新手的可访问性的重要问题。这篇愿景论文概述了根据不同的解决问题风格调整AI响应的潜力，以避免偏袒特定的子群体。我们讨论了基于AI角色的提示工程作为与AI互动的策略的潜力。这项研究邀请进一步研究，以改进基于AI的工具，更好地支持对OSS项目的贡献。|
|**2025-02-11**|**Scalable Fingerprinting of Large Language Models**|Anshul Nasery et.al.|[2502.07760](http://arxiv.org/abs/2502.07760)|null|模型指纹识别已成为模型所有者识别其共享模型的有力工具。然而，为了降低误检率、抵御指纹泄露以及防御模型用户联盟试图绕过检测的行为，我们认为{\em 可扩展性}是关键，即扩大模型中可嵌入指纹的数量。因此，我们将可扩展性视为指纹识别方案的至关重要要求。我们在比以往考虑的规模大得多的范围内进行指纹设计实验，并提出了一种新的方法，称为核仁采样，以生成可扩展、持久且无害的指纹。我们证明，该方案可以将24,576个指纹添加到Llama-3.1-8B模型中——比现有方案多两个数量级——而不会降低模型的效用。我们插入的指纹在经过标准训练数据上的监督微调后仍然存在。我们进一步讨论了指纹识别的安全风险，并从理论和实证上展示了像我们这样的可扩展指纹识别方案如何减轻这些风险。|
|**2025-02-11**|**Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension**|Wenbo Gong et.al.|[2502.07752](http://arxiv.org/abs/2502.07752)|null|设计具有低内存需求和快速收敛的大型语言模型（LLM）的优化器是一个重要且具有挑战性的问题。本文通过结构化Fisher信息矩阵（FIM）近似的角度，朝着系统设计此类优化器迈出了第一步。我们表明，许多最先进的效率优化器可以被视为在特定结构假设下FIM近似（在Frobenius范数下）的解决方案。基于这些见解，我们提出了两个针对LLM的实用高效优化器设计建议，包括谨慎选择结构假设以平衡通用性和效率，以及通过一种新颖的低秩扩展框架增强具有通用结构的优化器的内存效率。我们通过推导新的内存高效优化器：行和列缩放随机梯度下降（RACS）和自适应低维子空间估计（Alice）来展示如何使用每种设计方法。在LLaMA预训练（高达10亿参数）上的实验验证了其有效性，显示出比现有内存高效基线和Adam更快的收敛速度和更好的性能，同时几乎没有内存开销。值得注意的是，Alice的收敛速度比Adam快2倍以上，而RACS在10亿参数模型上表现出与SGD类似的内存性能。|
|**2025-02-11**|**WHODUNIT: Evaluation benchmark for culprit detection in mystery stories**|Kshitij Gupta et.al.|[2502.07747](http://arxiv.org/abs/2502.07747)|**[link](https://github.com/kjgpta/WhoDunIt-Evaluation_benchmark_for_culprit_detection_in_mystery_stories)**|我们提出一个新颖的数据集，名为WhoDunIt，用于评估大型语言模型（LLM）在叙事情境中的演绎推理能力。该数据集由开放域的悬疑小说和短篇小说构建而成，挑战LLM在阅读和理解故事后识别罪犯。为了评估模型的鲁棒性，我们应用了一系列以角色名字为单位的增强方法，包括原始名字、名字交换以及用来自流行话语中的知名真实或虚构实体进行替换。此外，我们使用不同的提示风格来研究提示对演绎推理准确性的影响。我们使用最先进的模型进行了评估研究，具体为GPT-4o、GPT-4-turbo和GPT-4o-mini，通过多次试验和多数响应选择来确保可靠性。结果表明，虽然LLM在未更改的文本上表现可靠，但准确性在特定的名字替换下有所下降，尤其是那些广为人知的名字。此数据集在此处公开可用。|
|**2025-02-11**|**The Economics of Large Language Models: Token Allocation, Fine-Tuning, and Optimal Pricing**|Dirk Bergemann et.al.|[2502.07736](http://arxiv.org/abs/2502.07736)|null|我们建立了一个经济框架来分析大型语言模型（LLM）的最优定价和产品设计。我们的框架捕捉了LLM的几个关键特征：处理输入和输出标记的可变运营成本；通过微调定制模型的能力；以及用户在任务需求和错误敏感性方面的多维异质性。在我们的模型中，垄断卖家通过产品菜单提供多个版本的LLM。最优定价结构取决于任务间标记分配是否可协商以及用户是否面临规模约束。具有相似的总价值-规模特征的用户会选择类似的微调和标记消费水平。最优机制可以通过两步收费菜单实现，对使用更频繁的用户收取更高的加成。我们的结果合理化了观察到的行业实践，例如基于模型定制和用量的分层定价。|
|**2025-02-11**|**Economics of Sourcing Human Data**|Sebastin Santy et.al.|[2502.07732](http://arxiv.org/abs/2502.07732)|null|人工智能的进步依赖于人类生成数据，从标注者市场到更广泛的互联网。然而，大型语言模型的广泛应用现在威胁到了这些平台上人类生成数据的质量和完整性。我们认为，这个问题不仅仅在于过滤人工智能生成内容的直接挑战——它揭示了数据收集系统设计的更深层次缺陷。现有系统往往以牺牲人类内在动机为代价，优先考虑速度、规模和效率，导致参与度下降和数据质量下降。我们提出，重新思考数据收集系统，使其与贡献者的内在动机相一致——而不是仅仅依赖外部激励——可以帮助在规模上维持高质量数据来源，同时保持贡献者的信任和长期参与。|
|**2025-02-11**|**Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK**|Marcos Cramer et.al.|[2502.07728](http://arxiv.org/abs/2502.07728)|null|大型语言模型（LLMs）展示了卓越的代码生成能力，但生成的代码的正确性无法天然信赖。本文探讨了使用形式化软件验证（特别是Ada语言的SPARK框架）来确保LLM生成代码可靠性的可行性。我们提出了Marmaragan工具，该工具利用LLM为现有程序生成SPARK注释，从而实现代码的形式化验证。该工具在一组精心挑选的SPARK程序上进行了基准测试，通过有选择地移除注释来测试特定功能。Marmaragan与GPT-4o在基准测试中的表现令人鼓舞，其中50.7%的基准案例已生成正确的注释。这些结果为将LLM的力量与形式化软件验证的可靠性相结合的未来工作奠定了基础。|
|**2025-02-10**|**Rationalization Models for Text-to-SQL**|Gaetano Rossiello et.al.|[2502.06759](http://arxiv.org/abs/2502.06759)|null|我们提出了一种生成思维链（CoT）理由的框架，以增强文本到SQL模型的微调。这些理由包括中间SQL语句和解释，作为构建最终SQL查询的逐步步骤。该过程从手动标注一小部分示例开始，然后使用这些示例通过迭代、动态的少量知识蒸馏程序从教师模型中提示大型语言模型。随后，在验证后的分解查询上训练一个合理化模型，从而为文本到SQL数据集生成大量的合成CoT标注。为了评估该方法，我们在BIRD数据集上使用带有和不带有这些理由的小型语言模型进行微调。结果表明，逐步查询生成提高了执行精度，特别是对于中等和高度复杂的查询，同时也增强了可解释性。|
|**2025-02-10**|**Gradient Multi-Normalization for Stateless and Scalable LLM Training**|Meyer Scetbon et.al.|[2502.06742](http://arxiv.org/abs/2502.06742)|null|训练大型语言模型（LLMs）通常依赖于如Adam（Kingma & Ba，2015）这样的自适应优化器，这些优化器存储额外的状态信息以加速收敛，但会带来显著的内存开销。最近的研究，如SWAN（Ma等，2024），通过消除对优化器状态的需求，并通过对瞬时梯度应用多步预处理程序，实现了与Adam相当的性能。受SWAN成功的影响，我们引入了一种新的框架来设计无状态优化器，该框架根据多个范数对随机梯度进行归一化。为了实现这一点，我们提出了一种简单的交替方案来强制执行对这些范数的梯度归一化。我们表明，我们的过程可以产生，直到任意精度，问题的固定点，并且SWAN是我们方法的一个特例，具有精心选择的范数，从而提供了对其设计的更深入理解。然而，SWAN计算成本高昂的白化/正交化步骤限制了其在大型LLMs中的实用性。利用我们的原则性视角，我们开发了一种更高效、可扩展和实用的无状态优化器。我们的算法放宽了SWAN的性质，显著降低了其计算成本，同时保持了其内存效率，使其适用于大规模模型的训练。在预训练LLaMA模型（参数量高达10亿）的实验中，我们的算法比Adam快3倍，同时内存需求显著降低，优于其他内存高效的基线。|
|**2025-02-10**|**VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data**|Thomas Zeng et.al.|[2502.06737](http://arxiv.org/abs/2502.06737)|null|过程奖励模型（PRMs）通过利用增加的推理时间计算，被证明可以有效地提高大型语言模型（LLMs）的数学推理能力。然而，它们主要在数学数据上进行训练，其泛化到非数学领域的表现尚未得到严格的研究。为此，这项工作首先表明，当前的PRMs在其他领域表现不佳。为了解决这一局限性，我们引入了VersaPRM，这是一种多领域PRM，它在利用我们新颖的数据生成和标注方法生成的合成推理数据上进行了训练。VersaPRM在多个领域实现了持续的性能提升。例如，在法律领域的MMLU-Pro类别中，通过加权多数投票，VersaPRM实现了7.9%的性能提升，超过了多数投票基线的提升——超过了Qwen2.5-Math-PRM的1.3%提升。此外，我们还向社区贡献，开源了VersaPRM的所有数据、代码和模型。|
|**2025-02-10**|**Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining**|Daouda Sow et.al.|[2502.06733](http://arxiv.org/abs/2502.06733)|null|在大量和异构数据集上预训练大型语言模型（LLMs）对于在多样化的下游任务中实现最先进的性能至关重要。然而，当前的训练范式平等地对待所有样本，忽略了单个样本在整个训练过程中的重要性和相关性。现有的重新加权策略主要关注组级数据的重要性，未能利用细粒度的实例级信息，并且没有根据训练过程中单个样本的重要性动态调整。在本文中，我们引入了旨在提高LLM预训练效率和效果的新型动态、实例级数据重新加权算法。我们的方法以在线方式根据每个训练样本的损失值调整其权重，使模型能够根据当前训练阶段动态关注更信息丰富或更重要的样本。特别是，我们的框架使我们能够系统地制定重新加权策略，优先考虑冗余或不具有信息量的数据，我们发现这种方法效果最好。此外，我们开发了一个新的理论框架来分析基于损失的重新加权对基于梯度的优化收敛的影响，为这些策略如何影响收敛界限提供了第一个形式化的描述。我们通过一系列任务实证验证了我们的方法，从预训练70亿和14亿参数的LLMs到较小规模的语言模型和线性回归问题，证明了我们的基于损失的重新加权方法可以导致更快的收敛和显著提高的性能。|
|**2025-02-10**|**Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling**|Runze Liu et.al.|[2502.06703](http://arxiv.org/abs/2502.06703)|**[link](https://github.com/RyanLiu112/compute-optimal-tts)**|测试时间缩放（TTS）是通过在推理阶段使用额外计算来提高大型语言模型（LLM）性能的重要方法。然而，目前的研究没有系统地分析策略模型、过程奖励模型（PRM）和问题难度如何影响TTS。这种分析的缺乏限制了人们对TTS方法的理解和实际应用。在本文中，我们关注两个核心问题：（1）在不同策略模型、PRM和问题难度级别上缩放测试时间计算的最佳方法是什么？（2）扩展计算在多大程度上可以提高LLM在复杂任务上的性能，并且较小的语言模型能否通过这种方法超越较大的模型？通过在MATH-500和具有挑战性的AIME24任务上的全面实验，我们得出以下观察结果：（1）计算最优的TTS策略高度依赖于策略模型、PRM和问题难度的选择。（2）使用我们的计算最优TTS策略，极小的策略模型可以超越较大的模型。例如，一个1B LLM可以在MATH-500上超过一个405B LLM。此外，在MATH-500和AIME24上，一个0.5B LLM优于GPT-4o，一个3B LLM超越一个405B LLM，一个7B LLM击败了o1和DeepSeek-R1，同时具有更高的推理效率。这些发现表明，根据每个任务和模型的具体特征调整TTS策略的重要性，并指出TTS是增强LLM推理能力的一种有前途的方法。|
|**2025-02-10**|**Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations**|Rui Chen et.al.|[2502.06669](http://arxiv.org/abs/2502.06669)|null|本文观察到大型语言模型（LLMs）的无监督能力有了显著提升。由于它们对输入的高度敏感性，研究越来越集中于通过直接和简单的提示工程来提高LLMs的性能，而不是复杂的领域适应。研究表明，LLMs表现出情感智力，正面和负面的情绪都可能潜在地增强任务表现。然而，先前的研究主要集中在单一刺激类型上，忽视了比较不同刺激效果、检验不同任务难度的影响或探索潜在机制。受社会认知理论中自我效能感和任务表现之间正相关关系的启发，本文引入了言语效能刺激（VES）。我们的VES包括三种类型的言语提示：鼓励性、挑衅性和批判性，涵盖六个方面，如有用性和能力。我们进一步将任务难度进行分类，旨在广泛研究不同难度的VES如何影响语言模型的自效能感和任务成就。实验结果表明，三种类型的VES在大多数任务上提高了LLMs的表现，而最有效的VES因模型而异。在大量实验中，我们获得了一些与心理理论一致的研究结果，为未来研究提供了新的见解。|
|**2025-02-10**|**Automatic Evaluation of Healthcare LLMs Beyond Question-Answering**|Anna Arias-Duart et.al.|[2502.06666](http://arxiv.org/abs/2502.06666)|null|当前大型语言模型（LLM）的基准测试通常基于开放式或封闭式问答评估，以避免对人工劳动的需求。封闭式测量评估响应的事实性，但缺乏表达性。开放式评估捕捉模型产生话语响应的能力，但更难以评估其正确性。这两种方法通常独立或结合使用，尽管它们之间的关系仍不太清楚。本研究聚焦于医疗保健领域，在该领域中事实性和话语都非常重要。它引入了一套全面的、多轴的医疗LLM评估方案，探索开放式和封闭式基准及指标之间的相关性。研究发现，现有方法存在盲点和重叠。作为一个更新的理智检查，我们发布了一个新的医学基准——CareQA——包括开放和封闭两种版本。最后，我们提出了一个用于开放式评估的新指标——放宽的困惑度——以缓解所识别的限制。|
|**2025-02-10**|**EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models**|Xingrun Xing et.al.|[2502.06663](http://arxiv.org/abs/2502.06663)|null|现代大型语言模型（LLMs）通过扩展规律驱动，在大型模型尺寸上实现了智能的突破。最近，对云成本、延迟和隐私的日益关注，使得开发紧凑型边缘语言模型成为一项紧迫的需求。与受扩展规律限制的直接预训练不同，这项工作提出了修剪感知预训练，专注于保留更大优化模型的性能。它具有以下特点：1）数据可扩展性：我们在LLM中引入了最小参数组，并持续优化结构修剪，将LLM-Pruner和SparseGPT等后训练修剪方法扩展到预训练阶段。2）架构无关性：使用显著性驱动的修剪自动设计LLM架构，这是现代预训练中首次超越人类设计的SoTA LLM。我们发现，通过扩展LLM压缩并扩展其边界，它实现了高质量的边缘语言模型，称为EfficientLLM。EfficientLLM在常识基准测试中显著优于具有 $100M \sim 1B$ 参数的SoTA基线，如MobileLLM、SmolLM、Qwen2.5-0.5B、OLMo-1B、Llama3.2-1B。作为首次尝试，EfficientLLM弥合了传统LLM压缩和直接预训练方法之间的性能差距，我们将在https://github.com/Xingrun-Xing2/EfficientLLM上全面开源。|
|**2025-02-10**|**Unbiased Evaluation of Large Language Models from a Causal Perspective**|Meilin Chen et.al.|[2502.06655](http://arxiv.org/abs/2502.06655)|null|基准污染已成为LLM（大型语言模型）评估领域的一个重要关注点。之前的“代理作为评估者”方法通过让代理参与问题的生成来解决这一问题。尽管它们取得了成功，但“代理作为评估者”方法中的偏差仍然在很大程度上未被探索。在本文中，我们提出了评估偏差的理论公式，为设计无偏差的评估协议提供了有价值的见解。此外，我们通过对最小“代理作为评估者”设置精心设计的探测任务，确定了两种类型的偏差。为了解决这些问题，我们提出了无偏差评估者，这是一种提供更全面、无偏差和可解释的LLM评估协议。大量的实验表明，当前LLM还有很大的改进空间。此外，我们证明了无偏差评估者不仅提供了基准污染的强有力证据，而且还提供了可解释的评估结果。|
|**2025-02-10**|**In-Context Learning (and Unlearning) of Length Biases**|Stephanie Schoch et.al.|[2502.06653](http://arxiv.org/abs/2502.06653)|null|大型语言模型在情境学习方面展现出强大的能力，其中示例输入输出对被附加到提示中以供演示。然而，现有研究已经证明了模型能够学习情境中的词汇和标签偏差，这对模型的表现和鲁棒性产生负面影响。其他统计数据偏差的影响尚未得到充分探索，这正是本研究旨在解决的问题。我们特别研究了长度偏差对情境学习的影响。我们证明了模型确实在预测的上下文窗口中学习了长度偏差，并进一步实证分析了调节模型所表现出的偏差水平的因素。此外，我们展示了在情境中学习长度信息可以用来抵消模型中已经编码的长度偏差（例如，通过微调）。这揭示了情境学习在去偏模型预测行为方面的力量，而不需要昂贵的参数更新。|
|**2025-02-07**|**Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray**|Yunhang Shen et.al.|[2502.05177](http://arxiv.org/abs/2502.05177)|**[link](https://github.com/vita-mllm/long-vita)**|建立大型视觉-语言模型的长上下文能力对于视频理解、高分辨率图像理解、多模态代理和推理至关重要。我们引入了Long-VITA，这是一个简单而有效的用于长上下文视觉-语言理解任务的大型多模态模型。它擅长同时处理和分析图像、视频和文本的模态，这些模态跨越4K帧或1M个标记，同时在短上下文多模态任务上表现出色。我们提出了一种有效的多模态训练方案，该方案从大型语言模型开始，通过视觉-语言对齐、通用知识学习，以及两个连续阶段的长期序列微调。我们进一步实现了上下文并行分布式推理和logits掩码语言模型头部，以在模型推理期间将Long-VITA扩展到无限长的图像和文本输入。关于训练数据，Long-VITA仅基于来自公共数据集的1700万个样本构建，与最近具有内部数据的尖端模型相比，在各种多模态基准测试中表现出最先进的性能。Long-VITA完全可重现，并支持NPU和GPU平台进行训练和测试。我们希望Long-VITA可以作为有竞争力的基线，并为开源社区在推进长上下文多模态理解方面提供有价值的见解。|
|**2025-02-07**|**NoLiMa: Long-Context Evaluation Beyond Literal Matching**|Ali Modarressi et.al.|[2502.05167](http://arxiv.org/abs/2502.05167)|**[link](https://github.com/adobe-research/NoLiMa)**|近期的大型语言模型（LLMs）支持长达128K到1M个token的上下文。评估这些能力的一种流行方法是“大海捞针”（NIAH）测试，它涉及从“草堆”（长篇无关上下文）中检索“针”（相关信息）。这种方法的扩展包括增加干扰项、事实链和上下文推理。然而，在这些基准测试中，模型可以利用针和草堆之间现有的直接匹配来简化任务。为了解决这个问题，我们引入了NoLiMa，这是一个扩展NIAH的基准，它包含精心设计的针集，其中问题和针之间具有最小的词汇重叠，要求模型推断潜在的关联来在草堆中定位针。我们评估了12个声称支持至少128K个token上下文的流行LLMs。虽然它们在短上下文（<1K）中表现良好，但随着上下文长度的增加，性能显著下降。例如，在32K时，10个模型的性能低于其强大的短长度基线的一半。即使是表现最好的GPT-4o，其基线从几乎完美的99.3%下降到69.7%。我们的分析表明，这些下降源于在长上下文中，当缺乏直接匹配时，注意力机制面临的难度增加，这使得检索相关信息变得更加困难。|
|**2025-02-07**|**DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails**|Yihe Deng et.al.|[2502.05163](http://arxiv.org/abs/2502.05163)|**[link](https://github.com/yihedeng9/duoguard)**|大型语言模型（LLMs）的快速发展增加了对护栏模型的需求，以确保其负责任的使用，尤其是在检测不安全和非法内容方面。虽然英语中的安全数据非常丰富，但由于其他语言的开放源代码安全数据稀缺，多语言护栏建模仍被研究不足。为了填补这一空白，我们提出了一种新颖的双玩家强化学习（RL）框架，其中生成器和护栏模型通过对抗性协同进化，生成高质量合成数据以用于多语言护栏训练。我们将这种交互理论化为一个双玩家博弈，并证明其收敛到纳什均衡。实证评估显示，我们的模型在英语基准测试中优于现有模型，与LlamaGuard3（8B）相比，提高了近10%，同时在推理速度上快4.5倍，且模型规模更小（0.5B）。我们在多语言安全任务上取得了实质性进步，尤其是在解决收集的实数据集中低资源语言的失衡问题。消融研究强调了合成数据生成在弥合英语与其他语言之间开源数据失衡中的关键作用。这些发现建立了一种可扩展和高效的合成数据生成方法，为改进的多语言护栏模型增强LLM安全性铺平了道路。代码、模型和数据将在https://github.com/yihedeng9/DuoGuard上开源。|
|**2025-02-07**|**A Lightweight Method to Disrupt Memorized Sequences in LLM**|Parjanya Prajakta Prashant et.al.|[2502.05159](http://arxiv.org/abs/2502.05159)|null|大型语言模型（LLMs）在许多任务上展现出令人印象深刻的能力，但同时也存在复制受版权保护内容的危险，引发了法律和伦理方面的担忧。尽管差分隐私或神经元编辑等方法可以减少记忆，但它们通常需要昂贵的重新训练或直接访问模型权重，可能会降低性能。为了解决这些挑战，我们提出了TokenSwap，一种轻量级、事后处理的方法，该方法用小型辅助模型（例如DistilGPT-2）的概率替换与语法相关的标记的概率。我们在Pythia-6.9b和LLaMA-3-8b等商业级模型上进行了广泛的实验，并证明我们的方法有效地将已知的记忆生成案例减少了多达10倍，同时对下游任务的影响微乎其微。我们的方法为真实世界系统的用户提供了一种独特且有效的解决方案。|
|**2025-02-07**|**Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation**|Steffen Eger et.al.|[2502.05151](http://arxiv.org/abs/2502.05151)|**[link](https://github.com/nl2g/transformingsciencellms)**|随着大型多模态语言模型的兴起，科学正处于基于人工智能的技术变革的门槛上。最近，提出了大量新的AI模型和工具，承诺将赋予全球研究人员和学者更有效地进行研究的权力。这包括研究周期的各个方面，特别是（1）寻找相关文献；（2）生成研究想法和进行实验；（3）生成基于文本的内容；（4）生成多模态内容（例如，科学图表和图解）；以及（5）基于AI的自动同行评审。在这篇综述中，我们深入概述了这些激动人心的最新发展，这些发展有望从根本上改变科学研究过程，带来长期积极影响。我们的综述涵盖了上述五个方面，指出了相关数据集、方法和结果（包括评估）以及未来研究的局限性和范围。关于这些工具的不足之处和滥用（伪造科学、剽窃、损害研究诚信）的伦理担忧在我们的讨论中占据了特别突出的位置。我们希望我们的综述不仅能够成为该领域新手的参考指南，也能成为“AI4Science”领域新AI项目的催化剂。|
|**2025-02-07**|**CodeSCM: Causal Analysis for Multi-Modal Code Generation**|Mukur Gupta et.al.|[2502.05150](http://arxiv.org/abs/2502.05150)|**[link](https://github.com/nb15/codeSCM-naacl25)**|本文提出了一种名为CodeSCM的结构因果模型（SCM），用于分析使用大型语言模型（LLMs）的多模态代码生成。通过对CodeSCM进行干预，我们测量了不同提示模态（如自然语言、代码和输入输出示例）对模型的影响。CodeSCM引入了潜在中介变量，以区分多模态代码生成提示中的代码和自然语言语义。通过在这些中介变量上应用因果中介分析原理，我们量化了直接效应，代表了模型的虚假倾向。我们发现，除了自然语言指令外，输入输出示例对代码生成也有显著影响。|
|**2025-02-07**|**An Annotated Reading of 'The Singer of Tales' in the LLM Era**|Kush R. Varshney et.al.|[2502.05148](http://arxiv.org/abs/2502.05148)|null|首先，我们需要理解摘要中的关键术语和概念：  1. **Parry-Lord oral-formulaic theory**：帕里-洛德口头公式理论，这是一种关于无文盲游吟诗人如何学习、创作和传播口头叙事诗歌的理论。 2. **annotated reading**：注释阅读，即对某个理论或文本进行详细的解释和分析。 3. **large language models (LLMs)**：大型语言模型，这是一种能够理解和生成自然语言的计算机模型。 4. **generative artificial intelligence (AI)**：生成式人工智能，这是一种能够自主生成内容的人工智能技术。  接下来，我们将摘要的每个部分翻译成中文：  - The Parry-Lord oral-formulaic theory was a breakthrough in understanding how oral narrative poetry is learned, composed, and transmitted by illiterate bards.   帕里-洛德口头公式理论在理解无文盲游吟诗人如何学习、创作和传播口头叙事诗歌方面是一个突破。  - In this paper, we provide an annotated reading of the mechanism underlying this theory from the lens of large language models (LLMs) and generative artificial intelligence (AI).   在本文中，我们从大型语言模型（LLMs）和生成式人工智能（AI）的角度对这一理论背后的机制进行注释阅读。  - We point out the similarities and differences between oral composition and LLM generation, and comment on the implications to society and AI policy.   我们指出口头创作与LLM生成之间的相似之处和不同之处，并就其对社会和AI政策的影响进行评论。  最后，我们将整个摘要翻译为中文：  帕里-洛德口头公式理论在理解无文盲游吟诗人如何学习、创作和传播口头叙事诗歌方面是一个突破。在本文中，我们从大型语言模型（LLMs）和生成式人工智能（AI）的角度对这一理论背后的机制进行注释阅读。我们指出口头创作与LLM生成之间的相似之处和不同之处，并就其对社会和AI政策的影响进行评论。|
|**2025-02-07**|**Refining Integration-by-Parts Reduction of Feynman Integrals with Machine Learning**|Matt von Hippel et.al.|[2502.05121](http://arxiv.org/abs/2502.05121)|null|在理论粒子物理和引力波物理领域的先进计算中，积分部分的减少常常成为瓶颈，并且依赖于选择积分部分恒等式时的启发式方法，其质量极大地影响了性能。在这篇论文中，我们探讨了使用机器学习技术来寻找改进的启发式方法。我们使用funsearch，这是一种基于大型语言模型代码生成遗传编程的变种，来探索可能的方法，然后使用强类型遗传编程来专注于有用的解决方案。这两种方法都成功地重新发现了最近被纳入积分部分求解器中的最先进的启发式方法，并在一个例子中找到了在这方面的小幅进步。|
|**2025-02-07**|**Flexible and Efficient Grammar-Constrained Decoding**|Kanghee Park et.al.|[2502.05111](http://arxiv.org/abs/2502.05111)|null|大型语言模型（LLMs）经常被要求生成遵循精确语法规则的输出，例如代码片段或格式化数据。语法约束解码（GCD）可以通过屏蔽那些会导致输出不属于指定上下文无关文法（CFG）的标记来保证LLM输出符合这些规则。为了保证正确性，GCD算法必须计算给定LLM子词标记器如何与给定上下文无关文法使用的标记对齐，并基于此信息计算标记掩码。这样做既具有挑战性，现有的GCD算法需要花费数十分钟来预处理常见的语法。我们提出了一种新的GCD算法及其实现，与现有方法相比，它提供了17.71倍的离线预处理速度，同时在在线掩码计算中保持了最先进的效率。|
|**2025-02-07**|**Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs**|Rohit Saxena et.al.|[2502.05092](http://arxiv.org/abs/2502.05092)|null|从视觉表征中理解时间是基本认知技能，但对于多模态大型语言模型（MLLMs）来说，这仍然是一个挑战。在这项工作中，我们调查了MLLMs通过模拟时钟和年历来解释时间和日期的能力。为了实现这一目标，我们精心制作了一个结构化数据集，包括两个子集：1） $\textit{ClockQA}$，它包含各种类型的时钟样式——标准时钟、黑表盘时钟、无秒针时钟、罗马数字时钟和箭头指针时钟——以及与时间相关的问题；2）$\textit{CalendarQA}$ ，它由年历图像组成，问题范围从众所周知的日子（例如，圣诞节、新年）到通过计算得出的日子（例如，一年中的第100天或第153天）。我们的目标是分析当MLLMs面对与时间相关的视觉数据时，它们如何执行视觉识别、数值推理和时间推断。我们的评估表明，尽管最近取得了进展，但可靠地理解时间对于MLLMs来说仍然是一个重大挑战。|
|**2025-02-06**|**Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**|Zuyan Liu et.al.|[2502.04328](http://arxiv.org/abs/2502.04328)|**[link](https://github.com/ola-omni/ola)**|近期大型语言模型的发展，尤其是GPT-4o的推出，引发了人们对开发能够理解更多模态的全模态模型的兴趣。尽管一些开源替代方案已经出现，但与专业的单模态模型相比，性能仍存在明显差距。在本文中，我们介绍了Ola，这是一个在图像、视频和音频理解方面与专业模型相比达到竞争水平的全模态语言模型。Ola的核心设计在于其渐进式模态对齐策略，该策略逐步扩展语言模型的支持模态。我们的训练流程从最独特的模态开始：图像和文本，然后通过连接语言和音频知识的语音数据以及连接所有模态的视频数据，逐步扩展模型的能力。渐进式学习流程还使我们能够保持跨模态对齐数据的相对较小规模，使得从现有的视觉-语言模型开发全模态变得既容易又成本较低。此外，为了解锁类似GPT-4o的高级交互体验，我们进一步设计了一种基于句子的解码解决方案，用于流式语音生成。广泛的实验表明，Ola在所有模态上超越了现有的开源全模态LLM，与类似规模的最新专业模型相比，也实现了高度竞争的性能。我们的目标是使Ola成为一个完全开源的全模态理解解决方案，以推进该新兴领域未来的研究。模型权重、代码和数据已在https://github.com/Ola-Omni/Ola上开源。|
|**2025-02-06**|**Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions**|Yik Siu Chan et.al.|[2502.04322](http://arxiv.org/abs/2502.04322)|**[link](https://github.com/yiksiu-chan/SpeakEasy)**|尽管进行了广泛的安全对齐努力，大型语言模型（LLMs）仍然容易受到引发有害行为的越狱攻击。虽然现有研究主要关注需要技术专长的攻击方法，但仍有两个关键问题尚未得到充分探索：（1）越狱后的响应是否真正有助于普通用户执行有害行为？（2）在更常见、简单的人-LLM交互中是否存在安全漏洞？在本文中，我们证明当LLM的响应既可操作又具有信息性时，它们最有效地促进有害行为——这两个属性在多步骤、多语言交互中很容易引发。利用这一见解，我们提出了HarmScore，这是一个越狱度量指标，用于衡量LLM响应使有害行为得以有效执行的程度，以及Speak Easy，这是一个简单的多步骤、多语言攻击框架。值得注意的是，通过将Speak Easy纳入直接请求和越狱基线，我们在四个安全基准测试中观察到，在开源和专有LLMs中，攻击成功率平均绝对提高了0.319，HarmScore提高了0.426。我们的工作揭示了一个关键但往往被忽视的漏洞：恶意用户可以轻易利用常见的交互模式来实现有害目的。|
|**2025-02-06**|**ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters**|Kamer Ali Yuksel et.al.|[2502.04315](http://arxiv.org/abs/2502.04315)|**[link](https://github.com/kayuksel/ChamaleonLLM)**|最近在大语言模型（LLMs）领域取得了显著进展，这些模型在多种任务上表现出色。然而，这些模型通常使用固定的权重进行部署，这限制了它们在推理过程中动态适应现实世界数据中固有的可变性的能力。本文介绍了一种名为ChamaleonLLM的新框架，通过利用批感知聚类和即时生成低秩更新，实现了LLMs在推理时的自适应调整。与传统的微调方法（如低秩调整LoRA）或依赖于一组固定预学习的均匀（可变掩码）的方法不同，我们的方法根据聚类批次的聚合统计信息动态生成对解码器权重的自适应修改。通过智能地分组相似输入，并通过超网络计算上下文感知的低秩更新，ChamaleonLLM实现了显著的性能提升，优于传统的LoRA方法，同时消除了维护多个专家模型的开销。我们的实验突出了我们的方法作为语言模型推理的通用且高度自适应解决方案的潜力。ChamaleonLLM已开源，以确保实验的可重复性：https://anonymous.4open.science/r/ChamaleonLLM/|
|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306](http://arxiv.org/abs/2502.04306)|**[link](https://github.com/gen-verse/scoreflow)**|近期研究利用大型语言模型多智能体系统解决复杂问题，同时试图减少构建它们的手动工作量，推动了自动化智能体工作流程优化方法的发展。然而，由于表示限制、缺乏适应性和在依赖离散优化技术时的扩展性差，现有方法仍然不够灵活。我们通过ScoreFlow框架解决了这些挑战，这是一个简单但性能高效的框架，它在连续空间中利用了高效的基于梯度的优化。ScoreFlow集成了Score-DPO，这是一种直接偏好优化方法的新变体，它考虑了定量反馈。在涵盖问答、编码和数学推理的六个基准测试中，ScoreFlow相对于现有基线实现了8.2%的提升。此外，它还使小型模型能够在较低的推理成本下超越大型模型。项目：https://github.com/Gen-Verse/ScoreFlow|
|**2025-02-06**|**Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization**|Yuanye Liu et.al.|[2502.04295](http://arxiv.org/abs/2502.04295)|**[link](https://github.com/henrylau7/cfpo)**|大型语言模型（LLMs）在各项任务中展现出了显著的能力，其现实世界的有效性往往由提示设计所驱动。虽然最近的研究集中在优化提示内容上，但提示格式，这一关键但常被忽视的维度，却得到了有限的系统研究。在本文中，我们提出了内容-格式集成提示优化（CFPO）方法，这是一种创新的方法，通过迭代精炼过程联合优化提示内容和格式。CFPO利用自然语言变异来探索内容变化，并采用动态格式探索策略，系统地评估不同的格式选项。我们在多个任务和开源LLMs上的广泛评估表明，与仅内容优化的方法相比，CFPO展现了可测量的性能提升。这突出了集成内容-格式优化的重要性，并提供了提升LLMs性能的一种实用且模型无关的方法。代码将在https://github.com/HenryLau7/CFPO上提供。|
|**2025-02-06**|**PILAF: Optimal Human Preference Sampling for Reward Modeling**|Yunzhen Feng et.al.|[2502.04270](http://arxiv.org/abs/2502.04270)|null|随着大型语言模型在现实应用中的不断推广，使它们与人类价值观保持一致变得至关重要。从人类反馈中学习强化学习（RLHF）已成为一项关键技术，在无法直接访问专家人类价值观的情况下，将偏好数据转化为奖励模型。在实践中，RLHF主要依赖于近似的奖励模型，这些模型可能无法始终如一地引导策略最大化潜在的人类价值观。我们提出了策略插值学习用于对齐反馈（PILAF），这是一种新颖的响应采样策略，用于偏好标签，它明确地将偏好学习与最大化潜在的专家奖励对齐。PILAF在理论和统计两个方面都得到了证明，显示出其最优性。该方法易于实现，并在反馈整理至关重要的迭代和在线RLHF环境中展现出强大的性能。|
|**2025-02-06**|**MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion**|Xintong Hao et.al.|[2502.04235](http://arxiv.org/abs/2502.04235)|null|尽管大型语言模型在各种任务中表现出色，但它们持续的扩展面临一个关键挑战：高质量预训练数据的稀缺。虽然模型架构不断进化，自然语言数据难以实现规模增长。为了解决这个瓶颈，我们提出了MAssive Genre-Audience（MAGA）重定义方法，该方法系统地从现有语料库中合成多样化、语境丰富的预训练数据。这项工作主要有三个贡献：（1）我们提出了MAGA重定义方法，这是一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含770亿个标记的MAGACorpus。（2）我们使用不同的数据预算扩展策略评估了MAGACorpus，证明了在各种模型大小（134M-13B）上的一致改进，确立了下一代大规模合成预训练语言模型的必要性。（3）通过全面分析，我们研究了提示工程对合成训练崩溃的影响，并揭示了使用验证损失的传统崩溃检测指标的限制。我们的工作表明，MAGA可以在保持质量的同时大幅扩展训练数据集，为超越数据限制的模型扩展提供了一条可靠途径。|
|**2025-02-06**|**Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks**|Andreas Happe et.al.|[2502.04227](http://arxiv.org/abs/2502.04227)|**[link](https://github.com/andreashappe/cochise)**|我们探讨了在企业网络中利用由大型语言模型（LLM）驱动的自主系统进行假设性入侵渗透测试的可行性和有效性。我们介绍了一个由LLM驱动的原型，该原型能够在现实生活中的活动目录测试环境中攻击账户。我们的研究全面评估了原型的能力，在执行攻击过程中同时突出了其优势和局限性。评估使用了一个现实模拟环境（活动目录游戏，GOAD），以捕捉到代表实时网络场景的复杂交互、随机结果和时间依赖性。研究得出结论，自主的LLM能够进行假设性入侵模拟，这可能使面临预算限制的组织能够民主化渗透测试的访问。原型源代码、跟踪记录和已分析的日志已作为开源发布，以增强集体网络安全并促进LLM驱动的网络安全自动化方面的未来研究。|
|**2025-02-06**|**Keep It Light! Simplifying Image Clustering Via Text-Free Adapters**|Yicen Li et.al.|[2502.04226](http://arxiv.org/abs/2502.04226)|null|许多具有竞争力的聚类流程具有多模态设计，利用大型语言模型（LLMs）或其他文本编码器以及文本-图像对，但这些在现实世界的下游应用中往往不可用。此外，这些框架通常难以训练且需要大量的计算资源，使得其广泛应用变得具有挑战性。在这项工作中，我们表明在深度聚类中，可以通过一个无文本且高度简化的训练流程实现与更复杂的最先进方法的竞争力。特别是，我们的方法——通过预训练模型进行简单聚类（SCP），仅训练一个小型聚类头，同时利用预训练的视觉模型特征表示和正数据对。在包括 CIFAR-10、CIFAR-20、CIFAR-100、STL-10、ImageNet-10 和 ImageNet-Dogs 等基准数据集上的实验表明，SCP 实现了高度竞争力的性能。此外，我们还提供了一个理论结果，解释了为什么，至少在理想条件下，额外的基于文本的嵌入可能不是在视觉中实现强大聚类性能所必需的。|
|**2025-02-06**|**Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents**|Ilia Karmanov et.al.|[2502.04223](http://arxiv.org/abs/2502.04223)|null|光学字符识别（OCR）技术被广泛用于从文档图像中提取文本，从而实现高效的数字化和数据检索。然而，当处理复杂文档时，仅仅提取文本是不够的。要全面理解这些文档，需要了解它们的结构——包括格式、公式、表格以及多页文档中多个块和列的阅读顺序——以及语义信息，以检测脚注和图像标题等元素。这种全面的理解对于下游任务，如检索、文档问答以及为训练大型语言模型（LLMs）和视觉语言模型（VLMs）进行数据整理至关重要。为了解决这个问题，我们引入了“Eclair”，这是一个专为处理各种文档类型而设计的通用文本提取工具。给定一个图像，“Eclair”能够提取按照阅读顺序的格式化文本，以及它们的边界框和相应的语义类别。为了彻底评估这些新颖的功能，我们引入了我们的多个人工标注的文档级OCR和语义分类基准。在基准测试中，“Eclair”实现了最先进的准确率，在关键指标上优于其他方法。此外，我们在现有的基准测试中评估了“Eclair”，展示了它在多个评估标准下的灵活性和优势。|
|**2025-02-05**|**Do Large Language Model Benchmarks Test Reliability?**|Joshua Vendrow et.al.|[2502.03461](http://arxiv.org/abs/2502.03461)|**[link](https://github.com/MadryLab/platinum-benchmarks)**|在部署大型语言模型（LLMs）时，确保这些模型不仅能力强，而且可靠至关重要。已经创建了众多基准来追踪LLMs能力的增长，然而，在衡量它们的可靠性方面却缺乏类似的关注。为了了解这一差距可能带来的潜在影响，我们研究了当前基准在量化模型可靠性方面的效果。我们发现，普遍存在的标签错误可能会损害这些评估，掩盖模型残留的故障并隐藏不可靠的行为。受可靠性评估这一差距的启发，我们随后提出了所谓的“铂金基准”的概念，即经过精心策划以最大限度地减少标签错误和模糊性的基准。作为构建此类基准的第一步尝试，我们对十五个现有流行基准中的示例进行了修订。我们在这些铂金基准上评估了广泛的各种模型，并发现，前沿的LLMs在诸如基础数学单词问题等简单任务上仍然存在失败。进一步分析这些失败揭示了前沿模型在持续努力解决的问题上的先前未识别的模式。我们已在https://github.com/MadryLab/platinum-benchmarks上提供代码。|
|**2025-02-05**|**Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training**|Boyao Wang et.al.|[2502.03460](http://arxiv.org/abs/2502.03460)|null|小型语言模型（SLMs）因其广泛的应用前景，受到了学术界和工业界的广泛关注。为了获得性能强大的SLMs，传统的做法要么是从头开始预训练模型，这会带来巨大的计算成本，要么是压缩/剪枝现有的大型语言模型（LLMs），但这会导致性能下降，与从头预训练相比有所不足。在本文中，我们研究了涉及结构化剪枝和模型训练的加速方法家族。我们发现：1）层自适应剪枝（Adapt-Pruner）在LLMs中非常有效，并比现有的剪枝技术带来了显著的改进；2）具有进一步训练的自适应剪枝导致模型与从头开始预训练的模型相当；3）增量剪枝通过交错剪枝与训练，每次仅去除少量神经元（约5%），从而带来非微小的性能提升。在LLaMA-3.1-8B上的实验结果表明，Adapt-Pruner在常识基准测试上的平均准确率优于传统的剪枝方法，如LLM-Pruner、FLAP和SliceGPT，提高了1%-7%。此外，Adapt-Pruner通过剪枝其更大的版本，将MobileLLM-125M在MMLU基准测试上的性能恢复到600M，通过剪枝减少了200倍的token数，并发现了一个新的1B模型，在多个基准测试中超越了LLaMA-3.2-1B。|
|**2025-02-05**|**A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)**|Yiye Chen et.al.|[2502.03450](http://arxiv.org/abs/2502.03450)|null|场景图已成为大型语言模型（LLMs）进行有据空间推理的结构化和可序列化环境表示。在这项工作中，我们提出了SG-RwR，一个用于场景图推理和规划的方案引导检索-推理框架。我们的方法采用两个协作的代码编写LLM智能体：（1）推理智能体用于任务规划和生成信息查询，以及（2）检索智能体用于根据查询提取相应的图信息。两个智能体迭代协作，实现了对图信息的顺序推理和自适应关注。与先前工作不同，两个智能体仅以场景图方案而不是完整图数据为提示，这通过限制输入令牌减少了幻觉，并促使推理智能体抽象地生成推理轨迹。在跟踪轨迹后，检索智能体根据对方案的理解，程序化地查询场景图数据，从而实现对图动态和全局的关注，增强了推理与检索之间的对齐。通过在多个模拟环境中的实验，我们表明我们的框架在数值问答和规划任务中优于现有的基于LLM的方法，并且可以从任务级别的少量示例中受益，即使在没有智能体级别的演示的情况下。项目代码将发布。|
|**2025-02-05**|**BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving**|Ran Xin et.al.|[2502.03438](http://arxiv.org/abs/2502.03438)|null|近年来，大型语言模型（LLMs）的进步促使人们对使用Lean4进行自动定理证明产生了越来越大的兴趣，其中有效的树搜索方法对于导航证明搜索空间至关重要。虽然现有的方法主要依赖于值函数和蒙特卡洛树搜索（MCTS），但像最佳优先搜索（BFS）这样简单的方法的潜力仍未得到充分探索。本文研究了BFS是否能够在大规模定理证明任务中实现有竞争力的性能。我们提出了\texttt{BFS-Prover}，一个可扩展的专家迭代框架，具有三个关键创新。首先，我们在每个专家迭代轮次中实施战略数据过滤，排除可以通过束搜索节点扩展解决的问题，以关注更难的情况。其次，我们通过直接偏好优化（DPO）提高BFS的样本效率，该优化应用于自动标注有编译器错误反馈的状态策略对，从而改进LLM的策略，优先考虑生产性扩展。第三，我们在BFS中采用长度归一化，以鼓励探索更深层次的证明路径。\texttt{BFS-Prover}在MiniF2F测试集上取得了71.31的分数，因此挑战了复杂树搜索方法必要性的看法，证明了当适当扩展时，BFS可以实现有竞争力的性能。|
|**2025-02-05**|**On Fairness of Unified Multimodal Large Language Model for Image Generation**|Ming Liu et.al.|[2502.03429](http://arxiv.org/abs/2502.03429)|null|统一的多模态大型语言模型（U-MLLMs）在端到端流程中的视觉理解和生成方面表现出令人印象深刻的性能。与仅生成模型（例如，Stable Diffusion）相比，U-MLLMs可能在其输出中引发关于偏见的新问题，这些问题可能受到其统一能力的影响。考虑到传播有害刻板印象的风险尚未充分探索，这一差距尤其令人担忧。在本文中，我们对最新的U-MLLMs进行了基准测试，并发现其中大多数都表现出明显的群体偏见，如性别和种族偏见。为了更好地理解和缓解这个问题，我们提出了一种“定位然后修复”策略，其中我们审计并展示了单个模型组件如何受到偏见的影响。我们的分析表明，偏见主要源于语言模型。更有趣的是，我们在U-MLLMs中观察到一种“部分对齐”现象，即对偏见的理解似乎很小，但生成偏见仍然很大。因此，我们提出了一种新颖的平衡偏好模型，以平衡合成数据中的群体分布。实验表明，我们的方法可以减少群体偏见，同时保持语义忠实度。我们希望我们的发现强调了在未来对U-MLLMs进行更全面解释和去偏见策略的必要性。|
|**2025-02-05**|**Harnessing Large Language Models for Curated Code Reviews**|Oussama Ben Sghaier et.al.|[2502.03425](http://arxiv.org/abs/2502.03425)|**[link](https://github.com/OussamaSghaier/CuREV)**|在代码审查中，生成结构化且相关的评论对于识别代码问题和促进准确、高效的代码更改至关重要。精心制作的评论不仅使代码审查过程更加流畅，而且在代码精炼等后续任务中也至关重要，在这些任务中，代码会被修改以满足输入的审查评论。尽管有各种基于AI的自动化评论生成方法，但它们的有效性受到训练数据质量的影响。现有的代码审查数据集通常嘈杂且未经过精炼，限制了AI模型的学习潜力，并阻碍了自动化过程。为了解决这些挑战，我们提出了一种策划流程，旨在提高最大公开代码审查数据集的质量。我们首先建立了一个评估框架，包括特定的标准和类别，以实证研究数据集的初始质量。然后，我们采用基于大型语言模型（LLM）的方法，应用我们的策划流程来精炼数据集。基于相同的评估框架，对全新策划的数据集进行了比较分析，显示出评论的清晰度和简洁度有了显著提高。此外，我们还评估了策划数据集对自动化下游任务的影响，特别是评论生成和代码精炼。我们的研究发现，策划数据集导致模型在生成更准确的评论方面性能提升。策划的评论也更加有用，因为它们能导致更准确的代码精炼。|
|**2025-02-05**|**Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts**|Nikta Gohari Sadr et.al.|[2502.03418](http://arxiv.org/abs/2502.03418)|null|零样本提示技术显著提高了大型语言模型（LLMs）的性能。然而，我们对零样本提示为何如此有效缺乏清晰的理解。例如，在提示“让我们一步一步思考”中，“思考”或“一步一步”对它的成功更为关键？现有的可解释性方法，如基于梯度和基于注意力的方法，计算量大且仅限于开源模型。我们引入了ZIP分数（零样本扰动重要性分数），这是一个通用的指标，适用于开源和闭源模型，基于系统的输入词扰动。我们在四个最近的LLMs、七个广泛使用的提示和几个任务上的实验揭示了词重要性中的有趣模式。例如，尽管“一步一步”和“思考”都显示出高的ZIP分数，但哪个更有影响力取决于模型和任务。我们通过控制实验验证了我们的方法，并将我们的结果与人类判断进行了比较，发现专有模型在关于词重要性的直觉上与人类更接近。这些发现增强了我们对LLM行为的理解，并有助于开发更有效的零样本提示和改进的模型分析。|
|**2025-02-05**|**SPRI: Aligning Large Language Models with Context-Situated Principles**|Hongli Zhan et.al.|[2502.03397](http://arxiv.org/abs/2502.03397)|null|将大型语言模型与人类价值观对齐，尤其是在需要复杂人工监督的任务中，由于依赖人类专业知识进行特定情境指导既资源密集又耗时，因此是一项艰巨的任务。先前的研究已经利用预定义的规则或原则来引导模型的行为（Bai等，2022；Sun等，2023）。然而，这些原则往往过于通用，使得它们难以适应每个单独的输入查询或情境。在这项工作中，我们提出了Situated-PRInciples（SPRI）框架，该框架需要最少或不需要人工努力，旨在为每个输入查询实时生成指导原则，并利用这些原则来对齐每个响应。我们在三个任务上评估了SPRI，结果表明：1）SPRI能够在复杂的专业领域任务中推导出原则，其性能与专家手工制作的原则相当；2）SPRI生成的原则导致针对特定实例的评分标准，优于先前LLM作为评判框架的方法；3）使用SPRI生成合成SFT数据可以提高真实性。我们将在https://github.com/honglizhan/SPRI-public上发布我们的代码和模型生成。|
|**2025-02-05**|**LIMO: Less is More for Reasoning**|Yixin Ye et.al.|[2502.03387](http://arxiv.org/abs/2502.03387)|**[link](https://github.com/gair-nlp/limo)**|我们提出了一项基本发现，挑战了我们对复杂推理在大规模语言模型中如何产生的理解。虽然传统观点认为，复杂的推理任务需要大量的训练数据（>100,000个示例），但我们证明，复杂的数学推理能力可以通过令人惊讶的少量示例有效地激发。通过全面实验，我们提出的模型LIMO在数学推理方面展现了前所未有的性能。仅使用817个精心挑选的训练样本，LIMO在AIME上的准确率达到57.1%，在MATH上达到94.8%，分别比之前的基于SFT的模型提高了6.5%和59.2%，而只使用了之前方法所需训练数据的1%。LIMO展示了出色的分布外泛化能力，在10个不同的基准测试中实现了40.5%的绝对改进，超过了在100倍更多数据上训练的模型，挑战了SFT导致记忆而非泛化的观点。基于这些结果，我们提出了“少即是多推理假说”（LIMO假说）：在预训练期间已全面编码领域知识的基座模型中，复杂的推理能力可以通过最小但精确编排的认知过程演示而出现。这个假说认为，复杂推理的激发阈值由两个关键因素决定：（1）模型在预训练期间编码的知识基础的完整性；（2）后训练示例作为“认知模板”的有效性，展示模型如何利用其知识库来解决复杂的推理任务。为了促进数据高效推理的可重复性和未来研究，我们将LIMO作为一个综合的开源套件发布在https://github.com/GAIR-NLP/LIMO。|
|**2025-02-05**|**Demystifying Long Chain-of-Thought Reasoning in LLMs**|Edward Yeo et.al.|[2502.03373](http://arxiv.org/abs/2502.03373)|**[link](https://github.com/eddycmu/demystify-long-cot)**|将论文摘要逐步翻译为中文如下：  1. Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction.    - 增加推理计算量可以提升大型语言模型（LLMs）的推理能力，长思维链（CoTs）使得回溯和错误纠正等策略成为可能。  2. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices.    - 强化学习（RL）已成为开发这些能力的关键方法，但长CoTs出现的条件仍然不明确，RL训练需要谨慎的设计选择。  3. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories.    - 在本研究中，我们系统地研究了长CoT推理的机制，确定了使模型能够生成长CoT轨迹的关键因素。  4. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings:    - 通过广泛的监督微调（SFT）和RL实验，我们提出了四个主要发现：  5. (1) While SFT is not strictly necessary, it simplifies training and improves efficiency;    - （1）虽然SFT并非严格必要，但它简化了训练并提高了效率；  6. (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth;    - （2）推理能力往往随着训练计算量的增加而出现，但其发展并非必然，因此奖励塑造对于稳定CoT长度增长至关重要；  7. (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning;    - （3）扩展可验证的奖励信号对于RL至关重要。我们发现，利用带过滤机制的噪声、网络提取的解决方案具有强大的潜力，特别是在STEM推理等分布外的（OOD）任务中；  8. (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach.    - （4）错误纠正等核心能力在基础模型中固有存在，但通过RL有效地激励这些技能以完成复杂任务需要大量的计算，而衡量其出现则需要一种细微的方法。  9. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs.    - 这些见解为优化训练策略以增强LLMs中的长CoT推理提供了实际指导。  10. Our code is available at: https://github.com/eddycmu/demystify-long-cot.     - 我们的代码可在以下链接获取：https://github.com/eddycmu/demystify-long-cot。|
|**2025-02-04**|**A comparison of translation performance between DeepL and Supertext**|Alex Flückiger et.al.|[2502.02577](http://arxiv.org/abs/2502.02577)|**[link](https://github.com/supertext/evaluation_deepl_supertext)**|随着强大的机器翻译（MT）系统越来越多地基于大型语言模型（LLMs），可靠的品质基准需要能够捕捉它们利用扩展上下文能力的方法。本研究通过评估未分段的文本，比较了两个商业机器翻译系统——DeepL和Supertext。我们使用专业翻译人员评估具有完整文档级上下文的段落，评估了四种语言方向的翻译质量。虽然段落级评估在大多数情况下没有显示出对系统有强烈的偏好，但文档级分析显示，在四个语言方向中有三个偏好Supertext，这表明在较长的文本中具有更高的连贯性。我们提倡更多上下文敏感的评估方法，以确保机器翻译质量评估反映现实世界的可用性。我们发布所有评估数据和脚本，以便进一步分析和复制，网址为https://github.com/supertext/evaluation_deepl_supertext。|
|**2025-02-04**|**Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement**|Soheil Abbasloo et.al.|[2502.02573](http://arxiv.org/abs/2502.02573)|null|大型语言模型（LLMs）在众多领域展示了令人印象深刻的能力，为优化问题求解这一关键、普遍且复杂的领域带来了革命性的机遇。本文探讨了LLMs在处理顺序优化问题（SOPs）方面的能力。我们引入了WorldGen，这是一个动态框架，用于生成具有可控复杂度的未见SOPs，以评估LLMs的性能。我们的初步观察表明，尽管LLMs在简单SOPs上表现良好，但随着复杂度的增加，其性能显著下降。受此启发，我们重新审视了关于推理的哲学假设，以提升LLMs的性能。受赫格尔辩证法框架的影响，我们提出了ACE，展示了如何在无需重新训练或进一步微调的情况下，显著提高LLMs在SOP环境中的性能。|
|**2025-02-04**|**Learning the RoPEs: Better 2D and 3D Position Encodings with STRING**|Connor Schenck et.al.|[2502.02562](http://arxiv.org/abs/2502.02562)|null|我们引入了STRING：可分离的平移不变位置编码。STRING通过一个统一的理论框架扩展了最近提出并被广泛用于大型语言模型的旋转位置编码。重要的是，STRING仍然提供了精确的平移不变性，包括任意维度的标记坐标，同时保持低计算开销。这些特性在机器人领域尤为重要，因为在机器人中，高效的3D标记表示至关重要。我们将STRING集成到具有RGB(-D)输入（颜色加可选深度）的视觉变换器中，展示了显著的改进，例如在开放词汇对象检测和机器人控制器方面。我们通过严格的数学分析补充了我们的实验，证明了我们方法的普适性。|
|**2025-02-04**|**LLMs for Generation of Architectural Components: An Exploratory Empirical Study in the Serverless World**|Shrikara Arun et.al.|[2502.02539](http://arxiv.org/abs/2502.02539)|null|近期，大型语言模型（LLMs）的能力和普及度呈指数级增长，这导致了代码生成领域的显著研究。然而，这种生成仅限于代码片段。更进一步，我们的目标是自动生成架构组件。这不仅会加快开发时间，而且最终使我们能够完全跳过开发阶段，直接从设计决策过渡到部署。为此，我们开展了一项探索性研究，探讨LLMs生成函数即服务（FaaS，又称无服务器函数）架构组件的能力。与单体和微服务等其他架构风格相比，它们架构组件的小型化使得这种架构风格更适合使用当前LLMs进行生成。我们通过系统地选择开源无服务器存储库，隐藏一个无服务器函数，并利用包含关于整体系统不同层次上下文信息的最新LLMs来生成被隐藏的函数，来进行这项研究。我们通过存储库中现有的测试来评估正确性，并使用软件工程（SE）和自然语言处理（NLP）领域的指标来评估代码质量和人类生成代码与LLM生成代码之间的相似度。除了我们的发现之外，我们还讨论了在架构组件生成中使用生成人工智能（GenAI）的前进道路。|
|**2025-02-04**|**Adaptive Self-improvement LLM Agentic System for ML Library Development**|Genghan Zhang et.al.|[2502.02534](http://arxiv.org/abs/2502.02534)|**[link](https://github.com/zhang677/pcl-lite)**|机器学习库通常是用针对特定架构的架构特定编程语言（ASPL）编写的，这些库对于高效的机器学习系统至关重要。然而，编写这些高性能的机器学习库具有挑战性，因为这需要掌握机器学习算法和ASPL的专业知识。另一方面，大型语言模型（LLMs）已显示出一般的编码能力。但是，使用LLMs通过ASPL生成机器学习库仍然存在挑战，因为1）即使是经验丰富的程序员，这个任务也相当复杂，2）由于ASPL的晦涩和不断变化，代码示例有限。因此，LLMs需要有限的资料进行复杂推理才能完成这项任务。为了解决这些挑战，我们介绍了一种自适应自我改进的智能体系统。为了评估我们系统的有效性，我们在一个典型的机器学习库基准上构建了一个基准，并使用开源和闭源LLMs生成ASPL代码。我们的结果表明，与基线单一LLM相比，性能提高了高达3.9倍。|
|**2025-02-04**|**Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies**|Han Zhou et.al.|[2502.02533](http://arxiv.org/abs/2502.02533)|null|大型语言模型作为相互交互和协作的多个智能体，在解决复杂任务方面表现出色。这些智能体被编程为具有声明其功能的提示，以及协调智能体之间交互的拓扑结构。为多智能体系统（MAS）设计提示和拓扑结构本质上很复杂。为了自动化整个设计过程，我们首先对设计空间进行深入研究，旨在了解构建有效MAS背后的因素。我们发现，提示与拓扑结构在实现更有效的MAS设计中发挥着关键作用。基于这些洞察，我们提出了多智能体系统搜索（MASS），这是一个MAS优化框架，通过交织其从局部到全局、从提示到拓扑结构的优化阶段，在三个阶段中高效地利用复杂的MAS设计空间：1）块级（局部）提示优化；2）工作流程拓扑优化；3）工作流程级（全局）提示优化，其中每个阶段都依赖于从前一阶段迭代优化后的提示/拓扑结构。我们表明，MASS优化后的多智能体系统在性能上显著优于现有的各种替代方案。基于MASS找到的系统，我们最终提出了构建有效多智能体系统的设计原则。|
|**2025-02-04**|**Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search**|Maohao Shen et.al.|[2502.02508](http://arxiv.org/abs/2502.02508)|null|大型语言模型（LLMs）在多个领域展现了惊人的推理能力。近期研究表明，增加测试时的计算量可以提升LLMs的推理能力。这通常涉及在推理时通过外部LLM验证器引导的大量采样，从而形成一个两人系统。尽管有外部指导，但该系统的有效性展示了单个LLM处理复杂任务的潜力。因此，我们提出了一个新的研究问题：我们能否将搜索能力内化，从而从根本上提升单个LLMs的推理能力？这项工作探索了一个不同的方向，专注于针对自回归搜索（即具有自我反思和探索新策略的扩展推理过程）的LLMs后训练。为了实现这一目标，我们提出了动作-思维链（COAT）推理和两阶段训练范式：1）一个小型格式调整阶段，以内部化COAT推理格式；2）一个利用强化学习的大型自我改进阶段。我们的方法产生了Satori，这是一个基于开源模型和数据训练的70亿参数LLMs。大量的实证评估表明，Satori在数学推理基准测试中实现了最先进的性能，同时表现出对域外任务的强大泛化能力。代码、数据和模型将全部开源。|
|**2025-02-04**|**EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization**|Yize Wu et.al.|[2502.02493](http://arxiv.org/abs/2502.02493)|null|推测解码是一种有效且无损的大语言模型（LLM）推理加速方法。它使用一个较小的模型生成草稿标记序列，然后由原始基础模型进行验证。在多GPU系统中，通过张量并行（TP）可以进一步降低推理延迟，但草稿模型的最佳TP大小通常小于基础模型，导致在草稿阶段GPU空闲。为了解决这个问题，我们提出了EasySpec，这是一种层并行推测策略，旨在优化多GPU的利用率。EasySpec打破了草稿模型中层的顺序执行顺序，使得可以在设备间实现多层并行化，尽管会引入一些近似误差。在每个草稿和验证迭代之后，草稿模型的关键值（KV）缓存通过单次前向传递进行校准，以最小的额外延迟防止长期误差累积。我们在几个主流开源LLM上评估了EasySpec，使用同一系列模型的小版本作为草稿模型。结果表明，与传统的解码相比，EasySpec可以实现高达4.17倍的峰值加速，同时保持基础LLM的原有分布。具体来说，草稿阶段可以加速至1.62倍，最大精度下降仅为7%，且无需在草稿模型上进行训练或微调。|
|**2025-02-04**|**Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study**|Menglong Cui et.al.|[2502.02481](http://arxiv.org/abs/2502.02481)|null|大型语言模型（LLMs）在多语言能力方面不断展现出改进，即使是小型开源模型也展示了快速的性能提升。在本文中，我们系统地探讨了参数少于十亿的开放LLMs在处理多语言机器翻译（MT）任务方面的能力。我们对六种流行的LLMs进行了全面评估，并发现像Gemma2-9B这样的模型展示了令人印象深刻的跨语言翻译能力。随后，我们在持续预训练阶段引入了并行优先、单语其次（PFMS）数据混合策略，以进一步提高MT性能，并介绍了GemmaX2-28，这是一个在28种语言中实现顶级多语言翻译性能的9B模型。具体来说，GemmaX2-28在持续超越最先进（SOTA）模型如TowerInstruct和XALMA的同时，与Google Translate和GPT-4-turbo等模型实现了具有竞争力的性能。|
|**2025-02-04**|**SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency**|Qianhao Yuan et.al.|[2502.02458](http://arxiv.org/abs/2502.02458)|**[link](https://github.com/icip-cas/saisa)**|多模态大型语言模型（MLLMs）主要分为两种架构，每种架构都涉及到训练和推理效率之间的权衡：嵌入空间对齐（例如，LLaVA-1.5）在推理过程中效率低下，而交叉注意力空间对齐（例如，Flamingo）在训练过程中效率低下。在本文中，我们比较了这两种架构，并确定了构建高效MLLMs的关键因素。它们之间一个主要的不同点在于如何将注意力应用于视觉标记，特别是在它们之间的交互中。为了研究视觉标记之间的注意力是否必要，我们提出了一种新的自注意力机制，NAAViT（无视觉标记注意力），该机制消除了这种类型的注意力。我们在LLaVA-1.5上的初步实验表明，视觉标记之间的注意力高度冗余。基于这些见解，我们引入了SAISA（自注意力输入空间对齐），这是一种新型架构，可以增强训练和推理效率。SAISA直接将视觉特征与NAAViT自注意力块的输入空间对齐，减少了自注意力块和前馈网络（FFNs）的计算开销。使用与LLaVA-1.5相同的配置，SAISA将推理FLOPs减少了66%，将训练预算减少了26%，同时在准确率方面实现了更优的性能。全面的消融研究进一步验证了SAISA在各种LLMs和视觉编码器上的有效性。代码和模型将公开提供在https://github.com/icip-cas/SAISA上。|
|**2025-01-31**|**Vintix: Action Model via In-Context Reinforcement Learning**|Andrey Polubarov et.al.|[2501.19400](http://arxiv.org/abs/2501.19400)|**[link](https://github.com/dunnolab/vintix)**|**在上下文强化学习（ICRL）中，代表了一种有前景的范式，用于开发在推理时通过试错交互学习的通用代理，类似于大型语言模型如何上下文自适应，但专注于奖励最大化。然而，将ICRL扩展到玩具任务和单一领域设置之外的可扩展性仍然是一个未解决的挑战。在本工作中，我们通过引入一个固定、跨领域的模型，该模型能够通过上下文强化学习学习行为，迈出了将ICRL扩展的第一步。我们的结果表明，算法蒸馏（Algorithm Distillation）这一旨在促进ICRL的框架，为构建多才多艺的动作模型提供了一种引人注目且具有竞争力的替代方案。这些发现突出了ICRL作为通用决策系统可扩展方法的可能性。代码将在https://github.com/dunnolab/vintix发布。**|
|**2025-01-31**|**Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game**|Mustafa O. Karabag et.al.|[2501.19398](http://arxiv.org/abs/2501.19398)|**[link](https://github.com/mustafakarabag/llmchameleon)**|**基于大型语言模型（LLM）的智能体在包含非合作方的环境中变得普遍。在这样的环境中，智能体的决策需要隐藏信息以避免对手得知，向合作伙伴透露信息，以及推断信息以识别其他智能体的特征。为了调查LLM是否具备这些信息控制和决策能力，我们让LLM智能体参与基于语言的隐蔽身份游戏《变色龙》。在这个游戏中，一群彼此不认识的非变色龙智能体试图在不泄露秘密的情况下识别变色龙智能体。游戏需要上述信息控制能力，无论是作为变色龙还是非变色龙。实证结果表明，虽然非变色龙LLM智能体能够识别变色龙，但它们未能从变色龙那里隐藏秘密，并且它们的获胜概率远低于最简单的策略水平。为了正式解释这种行为，我们对从隐藏到揭示的一系列策略进行了理论分析，并给出了非变色龙获胜概率的上限。基于实证结果和对不同策略的理论分析，我们推断基于LLM的非变色龙智能体向未知身份的智能体透露了过多的信息。我们的结果表明，包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet在内的当代LLM在战略互动中存在弱点。**|
|**2025-01-31**|**Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models**|Alina Shutova et.al.|[2501.19392](http://arxiv.org/abs/2501.19392)|**[link](https://github.com/goodevening13/aquakv)**|高效的现实世界部署大型语言模型（LLMs）依赖于键值（KV）缓存来处理和生成长输出，从而减少重复计算的需求。对于大型上下文，键值缓存可能占用数十吉字节设备内存，因为它们存储每个标记和层的向量表示。最近的研究表明，缓存的向量可以通过量化、剪枝或合并进行压缩，但这些技术通常在提高压缩率的同时牺牲质量。在这项工作中，我们旨在通过利用两个观察结果来改进键值压缩：1）不同层之间键和值之间的固有依赖关系；2）内部网络状态的高压缩机制。我们提出了AQUA-KV，这是一种针对键值缓存的自适应量化方法，它依赖于紧凑的适配器来利用键和值之间现有的依赖关系，并旨在“最优”地压缩无法预测的信息。AQUA-KV显著提高了压缩率，同时在最先进的LLM系列上保持了高精度。在Llama 3.2 LLMs上，我们实现了接近无损推理，每个值的位数为2-2.5位，在困惑度和LongBench得分上相对误差低于1%。AQUA-KV是一次性的、简单的和高效的：它可以在1-6小时内在一个GPU上校准，即使是70B模型也是如此。|
|**2025-01-31**|**Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models**|Wenzhi Fang et.al.|[2501.19389](http://arxiv.org/abs/2501.19389)|**[link](https://github.com/wenzhifang/Federated-Sketching-LoRA-Implementation)**|近年来，在设备上微调大型语言模型（LLMs）越来越受到关注。近期的研究将低秩自适应（LoRA）技术与联邦微调相结合，以缓解与设备模型大小和数据稀缺相关的问题。然而，计算资源的不一致性仍然是一个关键瓶颈：虽然高秩模块通常能提升性能，但不同的设备能力限制了LoRA的可实现秩范围。现有的试图解决这个问题的方法要么缺乏分析上的依据，要么增加了额外的计算开销，留下了一个对高效且理论上有根据的解决方案的巨大差距。为了应对这些挑战，我们提出了联邦草图LoRA（FSLoRA），它利用草图机制使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比率，即确定设备上子矩阵的秩，FSLoRA灵活地适应了特定于设备的通信和计算约束。我们对FSLoRA提供了严格的收敛性分析，描述了草图比率如何影响收敛速度。通过在多个数据集和LLM模型上的综合实验，我们证明了FSLoRA相对于各种基线具有优越的性能。|
|**2025-01-31**|**SELMA: A Speech-Enabled Language Model for Virtual Assistant Interactions**|Dominik Wagner et.al.|[2501.19377](http://arxiv.org/abs/2501.19377)|null|在本文中，我们提出并评估了SELMA，这是一个用于虚拟助手交互的语音启用语言模型，它将音频和文本作为输入整合到大型语言模型（LLM）中。SELMA旨在同时在一个端到端模型中处理与虚拟助手交互相关的三个主要任务和两个辅助任务。我们采用低秩自适应模块来高效训练音频编码器和LLM。此外，我们还实现了一种特征池化策略，使系统能够识别全局模式并提高对依赖于单个序列元素的任务准确性。在语音触发（VT）检测、设备指向性语音检测（DDSD）和自动语音识别（ASR）实验结果中，我们的方法显著简化了虚拟助手典型的输入处理流程，并且与针对每个单独任务的专用模型相比，性能也得到了提升。在VT检测任务中，SELMA实现了相对误码率的64%提升，在DDSD任务中提升了22%，同时单词错误率也接近基线。|
|**2025-01-31**|**We're Different, We're the Same: Creative Homogeneity Across LLMs**|Emily Wenger et.al.|[2501.19361](http://arxiv.org/abs/2501.19361)|null|众多强大的大型语言模型（LLMs）现在可用作写作辅助工具、创意生成器等。尽管这些LLMs被宣传为有助的创意助手，但多项研究显示，将LLM作为创意伙伴会导致创意输出范围变窄。然而，这些研究仅考虑了与单个LLM交互的影响，从而引发了这样的疑问：这种创意范围的缩小是否源于使用特定LLM——它可能具有有限的输出范围——还是源于一般性地将LLMs作为创意助手。为了研究这个问题，我们通过标准化创意测试从人类和广泛的大型语言模型中收集创意回应，并比较了这些回应在人群层面的多样性。我们发现，即使控制了回应结构和其他关键变量，LLM的回应与其他LLM的回应之间也更加相似，而人类回应之间则不太相似。这一发现在我们评估的LLMs中创意输出同质化显著的发现，为关于创意和LLMs的持续对话增添了新的维度。如果今天的LLMs表现相似，无论使用哪种模型，将它们作为创意伙伴可能会驱使用户走向有限的“创意”输出集合。|
|**2025-01-31**|**Mechanical Properties of the Meninges: Large Language Model Assisted Systematic Review of over 25,000 Studies**|Brandon P. Chelstrom et.al.|[2501.19359](http://arxiv.org/abs/2501.19359)|null|为了预测由于脑外伤导致的脑组织机械损伤，准确的大脑膜本构模型及其相应的机械性能值至关重要。由于大脑膜的复杂解剖结构和空间变异性机械行为，它们在当前的有限元（FE）头部模型中通常被过度简化。本研究对大脑膜每一层的机械性能进行了系统综述（SR），以获取有限元建模的基准数据，并确定现有文献中的空白。相关研究通过三个阶段进行筛选：广泛的初始搜索过滤器、大型语言模型分类器和人工审核。在最初考虑的超过25,000项研究中，这项综述最终包括了47项关于硬脑膜的研究、8项关于蛛网膜的研究和7项关于软脑膜的研究，代表了迄今为止关于大脑膜机械性能最全面和最系统的研究。研究发现，每一层都表现出非线性速率依赖性，这种依赖性随物种、年龄、位置和方向而变化。这项研究揭示了在简化线性弹性FE模型中最常使用的软脑膜弹性模量可能被低估了一个数量级，并且未能考虑方向依赖性。未来的研究应关注大脑膜的机械性能，研究范围应更广泛，包括蛛网膜和软脑膜的加载速率以及年龄效应，因为这些特征相对研究较少，并且预期会影响有限元预测的准确性。|
|**2025-01-31**|**The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking**|Yuchun Miao et.al.|[2501.19358](http://arxiv.org/abs/2501.19358)|null|这项工作识别了从人类反馈的强化学习（RLHF）中的能量损失现象及其与奖励黑客行为的关联。具体来说，在大型语言模型（LLM）的最后一层，能量损失在强化学习过程中逐渐增加，能量损失的过度增加是奖励黑客行为的特征。除了实证分析之外，我们还提供了理论依据，通过证明在温和的条件下，增加的能量损失会降低LLM中上下文相关性的上限，这是奖励黑客行为的关键方面，因为降低的上下文相关性通常表明在强化学习中过度拟合了奖励模型青睐的模式。为了解决这个问题，我们提出了一种能量损失感知的PPO算法（EPPO），该算法在奖励计算过程中惩罚LLM最后一层能量损失的增加，以防止过度能量损失，从而减轻奖励黑客行为。我们从理论上证明，EPPO可以概念上解释为熵正则化的强化学习算法，这为其有效性提供了更深入的见解。在多个LLM和任务上的大量实验表明，能量损失现象的普遍性，以及EPPO在减轻奖励黑客行为和提高RLHF性能方面的有效性。|
|**2025-01-31**|**Towards Adaptive Self-Improvement for Smarter Energy Systems**|Alexander Sommer et.al.|[2501.19340](http://arxiv.org/abs/2501.19340)|null|该论文介绍了一种基于大型语言模型（LLMs）的决策和优化分层框架，通过自适应代码生成来利用LLMs。不是直接进行决策，LLMs通过一个引导任务生成的元策略和一个用于操作动作的基础策略，生成并完善可执行的控制策略。应用于简化的微电网场景，该方法通过迭代改进电池控制策略，实现了高达15%的成本节约。所提出的方法为将基于LLM的工具集成到规划和控制任务中奠定了基础，为复杂系统提供可适应和可扩展的解决方案，同时应对不确定性和可复现性的挑战。|
|**2025-01-31**|**Homogeneity Bias as Differential Sampling Uncertainty in Language Models**|Messi H. J. Lee et.al.|[2501.19337](http://arxiv.org/abs/2501.19337)|null|先前的研究表明，大型语言模型（LLMs）和视觉语言模型（VLMs）在与边缘化群体相关的文本生成中，比与主导群体相关的文本生成表现出更同质化的代表。然而，导致这种同质化偏差背后的机制仍然相对未被充分探索。我们提出，这种偏差源于在推理时从概率分布中采样标记的系统差异。通过分析标记采样分布中的三个不确定性度量——熵、困惑度和区分概率——我们发现，在某些模型中，特别是在GPT-4 Turbo和Llama-3.2中，当生成关于边缘化群体（例如黑人美国人和女性）的文本时，与关于其主导群体（例如白人美国人和男性）的文本相比，标记的采样更具确定性。尽管这些发现可能有助于解释某些模型中的同质化偏差，但这些模式并未在所有测试的VLMs中复现，这表明可能有多种机制共同导致AI中的同质化偏差。|
|**2025-01-30**|**Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs**|Yue Wang et.al.|[2501.18585](http://arxiv.org/abs/2501.18585)|null|大型语言模型（LLMs）如OpenAI的o1通过扩展测试时的计算能力并展现出类似人类的深度思考，在复杂的推理任务中展现出了惊人的能力。然而，我们识别出一种我们称之为“浅思”的现象，其中类似于o1的LLMs在达到正确解决方案之前，经常在不同推理思路之间切换，而没有充分探索有希望的路径。这种行为导致推理深度不足，性能下降，尤其是在具有挑战性的数学问题上。为了系统地分析这个问题，我们在三个具有挑战性的测试集和两个代表性的开源o1类似模型上进行了实验，发现频繁的思路切换与错误回答相关。我们引入了一个新的指标来量化“浅思”，通过测量错误答案中的标记效率。为了解决“浅思”问题，我们提出了一种带有思路切换惩罚TIP的解码策略，该策略旨在阻止过早的思维转换，鼓励对每个推理路径进行更深入的探索。实验结果表明，我们的方法在不要求模型微调的情况下，提高了具有挑战性的数据集上的准确率。我们的发现有助于理解类似于o1的LLMs中的推理低效问题，并为提高其解决问题的能力提供了一个实用的解决方案。|
|**2025-01-30**|**Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for Multi-Step Reasoning Over Speed in MATH**|Evgenii Evstafev et.al.|[2501.18576](http://arxiv.org/abs/2501.18576)|null|本项研究调查了DeepSeek R1语言模型在30个来自MATH数据集的具有挑战性的数学问题上的性能，这些问题在其他模型在时间限制下已经证明无法解决。与以往的研究不同，本研究去除了时间限制，以探索DeepSeek R1的架构，该架构以其基于token的推理能力而闻名，是否可以通过多步骤过程实现准确的解决方案。该研究将DeepSeek R1与其他四种模型（gemini-1.5-flash-8b、gpt-4o-mini-2024-07-18、llama3.1:8b和mistral-8b-latest）在11个温度设置下进行了比较。结果表明，DeepSeek R1在这些复杂问题上实现了更高的准确率，但生成的token数量比其他模型显著更多，证实了其token密集型方法。这些发现突显了在大型语言模型进行数学问题解决时准确性和效率之间的权衡：虽然DeepSeek R1在准确率方面表现出色，但其对大量token生成的依赖可能并不适用于需要快速响应的应用。该研究强调了在选择LLM时考虑特定任务需求的重要性，并强调了温度设置在优化性能中的作用。|
|**2025-01-30**|**BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos**|Lehao Lin et.al.|[2501.18565](http://arxiv.org/abs/2501.18565)|null|近年来，人工智能（AI）特别是多模态大型语言模型（MLLMs）的快速发展，使得AI能够理解文本、图像、视频和其他多媒体数据，允许AI系统根据人类提供的提示执行各种任务。然而，AI驱动的机器人越来越多地能够绕过大多数现有的CAPTCHA系统，对网络应用构成了重大的安全威胁。这使得设计新的CAPTCHA机制成为一项紧迫的任务。我们观察到，人类对视频中的位移和突然变化非常敏感，而当前的AI系统在理解和有效应对这些情况方面仍然存在困难。基于这一观察，我们设计和实现了BounTCHA，这是一种利用人类对视频转换和中断中边界感知的CAPTCHA机制。通过利用AI扩展原始视频的能力，我们引入了意想不到的转折和变化，为生成用于CAPTCHA目的的短视频创建了一个管道。我们开发了一个原型并进行了实验，收集了人类在边界识别上的时间偏差数据。这些数据作为区分人类用户和机器人的依据。此外，我们对BounTCHA进行了详细的安全分析，证明了它对各种类型攻击的抵抗能力。我们希望BounTCHA能够作为一项强大的防御措施，在AI驱动时代保护数百万网络应用。|
|**2025-01-30**|**Semantic Web and Creative AI -- A Technical Report from ISWS 2023**|Raia Abu Ahmad et.al.|[2501.18542](http://arxiv.org/abs/2501.18542)|null|国际语义网研究学校（ISWS）是一个为期一周的密集项目，旨在让参与者深入该领域。本文报告了由十支学生团队在ISWS 2023期间进行的一项协作努力，每个团队都由一位资深研究员作为导师指导。每个团队都从不同的角度探讨了创意人工智能的主题，并以一组研究问题作为他们研究的主要对象。2023年ISWS的焦点是语义网技术和创意人工智能的交汇点。ISWS 2023探讨了语义网技术和创意人工智能之间的各种交汇点。重点关注领域是LLMs作为知识工程支持工具的潜力。参与者还深入探讨了LLMs的多方面应用，包括创意内容生产的法律方面、人机交互、多模态生成AI模型的去中心化方法、纳米出版物、AI个人科学知识图谱、自动故事和叙事完成中的常识知识、艺术批评的生成AI、提示工程、自动音乐创作、常识原型和概念融合以及隐性知识的激发。随着大型语言模型和语义技术的不断发展，新的激动人心的前景正在出现：一个创意表达和事实知识之间的界限越来越模糊的世界，这将导致一个既具有信息性又具有启发性的知识世界。|
|**2025-01-30**|**Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges**|Manveer Singh Tamber et.al.|[2501.18536](http://arxiv.org/abs/2501.18536)|**[link](https://github.com/manveertamber/content_injection_attacks)**|**考虑这样一种场景：用户在搜索信息时，却遭遇了充斥着误导性或无关内容的文本。这种场景体现了神经网络信息检索（IR）管道中的一个简单但强大的漏洞：内容注入攻击。我们发现，用于检索、重排序和大型语言模型（LLM）相关度判断的嵌入模型都容易受到这些攻击，攻击者通过在段落中插入误导性文本来操纵模型判断。我们确定了两个主要威胁：（1）在看似具有欺骗性“相关”的段落中插入无关或有害内容，以及（2）将整个查询或关键查询术语插入段落以提升其感知的相关性。虽然第二种策略在先前的研究中已被探讨，但据我们所知，我们首次对第一种威胁进行了实证分析，展示了最先进的模型如何轻易被误导。我们的研究系统地考察了影响攻击成功因素，例如注入内容的位置以及相关和非相关材料的平衡。此外，我们还探讨了各种防御策略，包括对抗性段落分类器、检索器微调以降低操纵内容的影响以及提示LLM判断者采取更加谨慎的方法。然而，我们发现这些对策往往涉及权衡，为了攻击的鲁棒性牺牲了有效性，有时甚至惩罚了合法文档。我们的发现强调了对抗这些不断发展的对抗策略，以保持IR系统可信性的必要性。我们发布了我们的代码和脚本，以促进进一步的研究。**|
|**2025-01-30**|**Differentially Private Steering for Large Language Model Alignment**|Anmol Goel et.al.|[2501.18532](http://arxiv.org/abs/2501.18532)|**[link](https://github.com/ukplab/iclr2025-psa)**|**将大型语言模型（LLMs）与人类价值观对齐并避免不良行为（如幻觉）变得越来越重要。最近，通过激活编辑引导LLMs走向期望的行为已成为缓解推理时有害生成的有效方法。激活编辑通过保留正面演示（例如，真实）的信息并最小化负面演示（例如，幻觉）的信息来修改LLMs的表示。当这些演示来自私有数据集时，对齐后的LLM可能会泄露包含在那些私有样本中的私人信息。在本工作中，我们首次研究了将LLM行为与私有数据集对齐的研究。我们的工作提出了名为“私有引导大型语言模型对齐（PSA）”的算法，以具有差分隐私（DP）保证的方式来编辑LLM激活。我们在七个不同的基准上进行了大量实验，使用了不同大小（0.5B到7B）和模型家族（LlaMa、Qwen、Mistral和Gemma）的开源LLMs。我们的结果表明，PSA在性能损失最小的情况下实现了LLM对齐的DP保证，包括对齐指标、开放式文本生成质量和通用推理。我们还开发了第一个用于评估和审计通过激活编辑引导LLM问题的实证隐私的成员推断攻击（MIA）。我们的攻击针对激活编辑，仅依赖于生成的文本而不依赖于其相关的概率。我们的实验通过显示我们的PSA算法相对于几种现有非隐私技术改进了保证来支持理论保证。**|
|**2025-01-30**|**Learn from the Past: Language-conditioned Object Rearrangement with Large Language Models**|Guanqun Cao et.al.|[2501.18516](http://arxiv.org/abs/2501.18516)|null|物体重组是协作机器人的一项重要任务，它们被引导操纵物体达到指定的目标状态。确定物体的放置位置是影响重组过程效率的主要挑战。大多数当前方法严重依赖预先收集的数据集来训练模型以预测目标位置，并且受到特定指令的限制，这限制了它们的更广泛适用性和有效性。在本文中，我们提出了一种基于大型语言模型（LLM）的语言条件物体重组框架。特别是，我们的方法通过使用过去的成功经验作为参考来推断所需的最终目标位置，模仿人类的推理过程。基于LLM强大的自然语言理解和推理能力，我们的方法可以无监督地泛化处理各种日常物体和自由形式的语言指令。实验结果表明，我们的方法可以有效地执行机器人重组任务，甚至包括涉及长序列顺序的任务。|
|**2025-01-30**|**Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch**|Arthur Douillard et.al.|[2501.18512](http://arxiv.org/abs/2501.18512)|null|大型语言模型（LLMs）的训练通常分布在大量的加速器上以缩短训练时间。由于在每个梯度步骤中都需要交换内部状态和参数梯度，所有设备都需要使用低延迟高带宽的通信链路进行集中，以支持所需的大量交换比特。最近，像DiLoCo这样的分布式算法放松了这种集中放置的限制：加速器可以被分组为“工作者”，工作者之间的同步仅在不频繁的情况下发生。这反过来意味着工作者可以由更低带宽的通信链路连接，而不会影响学习质量。然而，在这些方法中，工作者之间的通信仍然需要与之前相同的峰值带宽，因为同步需要所有参数在所有工作者之间进行交换。在本文中，我们从三个方面改进了DiLoCo。首先，我们按顺序同步参数的子集，而不是一次性同步所有参数，这大大降低了峰值带宽。其次，我们允许工作者在同步时继续训练，这减少了实际时间。第三，我们对工作者之间交换的数据进行量化，这进一步减少了工作者之间的带宽。通过适当组合这些修改，我们通过实验表明，我们可以分配数十亿参数规模的训练，并达到类似的质量，但所需的带宽降低了两个数量级。|
|**2025-01-30**|**CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction**|Peter J. Bentley et.al.|[2501.18504](http://arxiv.org/abs/2501.18504)|null|大语言模型（LLM）图像识别是一种从图像中提取数据的有力工具，但其准确性取决于在提示中提供足够的线索——需要领域专家来完成专业任务。我们引入了Cue Learning using Evolution for Accurate Recognition（CLEAR），它结合了LLM和进化计算来生成和优化线索，从而提高图像中特定特征的识别。它通过自动生成新颖的领域特定表示，然后使用遗传算法来优化合适的文本线索来实现这一目标。我们将CLEAR应用于从建筑内外部图像中识别可持续性数据的现实世界任务。我们研究了使用可变长度表示与固定长度表示相比的效果，并展示了如何通过从分类估计重构为实值估计来提高LLM的一致性。我们表明，CLEAR在所有任务中都能实现比专家人类识别和人类撰写的提示更高的准确性，错误率降低了两个数量级，并且消融研究表明了解决方案的高度简洁性。|
|**2025-01-30**|**A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models**|Changshu Liu et.al.|[2501.18482](http://arxiv.org/abs/2501.18482)|null|代码执行推理正成为评估大型语言模型（LLMs）在编程任务能力的新非功能性指标。最先进的框架（如CodeMind或REval）和基准测试（如CruxEval）通常关注LLMs在有限程序中对给定代码的输入/输出或中间变量状态/值的预测。然而，目前还没有工具能够进行更深入的结果分析。没有这样的工具，关于LLMs代码执行推理的观察就无法推广到更多数据集，这将阻碍研究社区和从业者设计出具有更好代码执行推理能力的下一代LLMs。本文介绍了ExeRScope，这是一系列工具和启发式方法，用于分析代码执行推理框架的结果，以更好地理解研究基准中代码属性对代码执行推理的影响。有了这样的工具，分析可以推广到具有相似属性的代码，而无需迫切地设计更多基准测试，这本身就是一项繁琐的工作。|
|**2025-01-29**|**Learning Beyond the Surface: How Far Can Continual Pre-Training with LoRA Enhance LLMs' Domain-Specific Insight Learning?**|Pouya Pezeshkpour et.al.|[2501.17840](http://arxiv.org/abs/2501.17840)|**[link](https://github.com/megagonlabs/insight_miner)**|**大语言模型（LLMs）在各种任务上展现出卓越的性能，但它们从特定领域数据集中提取和吸收更深入见解的能力仍未得到充分探索。在这项研究中，我们探讨了持续预训练如何提升LLMs在三种不同形式的见解学习上的能力：陈述性、统计性和概率性见解。聚焦于两个关键领域：医学和金融，我们使用LoRA对LLMs进行训练，基于两个现有数据集。为了评估每种见解类型，我们创建了基准来衡量持续预训练如何帮助模型超越表面知识。我们还评估了文档修改对捕捉见解的影响。结果表明，虽然对原始文档进行持续预训练有一定的影响，但修改文档以保留关键信息显著增强了LLMs的见解学习能力。**|
|**2025-01-29**|**Leveraging Multimodal LLM for Inspirational User Interface Search**|Seokhyeon Park et.al.|[2501.17799](http://arxiv.org/abs/2501.17799)|**[link](https://github.com/spark-damian/s-ui)**|**灵感搜索，即探索设计以启发新的创意工作的过程，在移动用户界面（UI）设计中至关重要。然而，探索庞大的UI参考空间仍然是一个挑战。现有的基于AI的UI搜索方法往往忽略了关键的语义，如目标用户或应用程序的情绪。此外，这些模型通常需要诸如视图层次结构等元数据，限制了其实际应用。我们使用多模态大型语言模型（MLLM）从移动UI图像中提取和解释语义。通过形式研究，我们确定了关键的UI语义，并开发了一个基于语义的UI搜索系统。通过计算和人工评估，我们证明了我们的方法显著优于现有的UI检索方法，为UI设计师提供了更加丰富和与上下文相关的搜索体验。我们增强了移动UI设计语义的理解，并突出了MLLM在灵感搜索中的潜力，为未来的研究提供了一个丰富的UI语义数据集。**|
|**2025-01-29**|**BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights**|Chan-Jan Hsu et.al.|[2501.17790](http://arxiv.org/abs/2501.17790)|null|我们提出了BreezyVoice，一个专为台湾普通话设计的文本到语音（TTS）系统，强调了其音素控制能力，以解决该语言中多音节的歧义问题。基于CosyVoice，我们引入了 $S^{3}$ 分词器、一个大语言模型（LLM）、最优传输条件流匹配模型（OT-CFM）和字符到音素预测模型，以生成接近人类口语的逼真语音。我们的评估显示了BreezyVoice在一般和代码转换情境下的卓越性能，突显了其在生成高保真语音方面的鲁棒性和有效性。此外，我们还解决了在建模长尾说话者和多音节歧义方面的挑战。我们的方法显著提升了性能，并为神经编解码器TTS系统的工作原理提供了宝贵的见解。|
|**2025-01-29**|**AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing**|Peter Pak et.al.|[2501.17784](http://arxiv.org/abs/2501.17784)|null|在这项工作中，我们研究了大型语言模型在给定一组工艺参数输入的情况下预测增材制造缺陷区域的能力。为此，我们利用工艺参数缺陷数据集微调了一系列模型，命名为AdditiveLLM，以预测潜在的缺陷区域，包括键孔、熔合不足和球化。我们比较了不同的输入格式化方法，以评估模型在稀疏的基线数据集和自然语言提示数据集上正确预测缺陷区域的能力。该模型表现出强大的预测能力，在要求提供与一组工艺参数相关的缺陷区域时，准确率达到93%。自然语言输入的引入进一步简化了工艺参数选择的过程，使用户能够识别出针对其构建的最佳设置。|
|**2025-01-29**|**2SSP: A Two-Stage Framework for Structured Pruning of LLMs**|Fabrizio Sandri et.al.|[2501.17771](http://arxiv.org/abs/2501.17771)|**[link](https://github.com/fabriziosandri/2ssp)**|我们提出了一种新颖的两阶段框架（2SSP）用于结构化剪枝，用于剪枝大型语言模型（LLMs），该框架结合了两种不同的剪枝策略，即宽度剪枝和深度剪枝。第一阶段（宽度剪枝）通过移除整个神经元及其对应的行和列，旨在保留在各个Transformer块中前馈网络中间状态中被剪枝结构的连通性。这是基于一个重要性分数来衡量每个神经元对输出幅度的冲击。第二阶段（深度剪枝）则移除整个注意力子模块。这是通过应用一个迭代过程来实现的，该过程移除对特定指标（在我们的案例中是困惑度）影响最小的注意力子模块。我们还提出了一种新颖的机制来平衡两个阶段的稀疏率，以符合期望的全局稀疏率。我们在四个LLM家族和三种稀疏率（25%、37.5%和50%）上测试了2SSP，测量了在三个语言建模数据集上的困惑度以及六个下游任务上的性能。我们的方法在三个语言建模和六个下游任务上始终优于五种最先进的竞争对手，在剪枝时间上最多提高了两个数量级。代码可在以下网址获取：\url{https://github.com/FabrizioSandri/2SSP}。|
|**2025-01-29**|**Hybrid Graphs for Table-and-Text based Question Answering using LLMs**|Ankush Agarwal et.al.|[2501.17767](http://arxiv.org/abs/2501.17767)|null|解决需要在对结构化（表格）和未结构化（原始文本）数据源进行推理和汇总的问题带来了重大挑战。现有方法依赖于微调和高质量、人工编辑的数据，而这些数据的获取是困难的。近期在大语言模型（LLMs）方面的发展已显示出在单源文本数据零样本设置下进行多跳问答（QA）的潜力，然而对于多源表格-文本问答的探索仍然有限。在本文中，我们提出了一种基于混合图的表格-文本问答新型方法，该方法无需微调即可利用LLMs。我们的方法通过从文本和表格数据构建统一的混合图，基于输入问题修剪信息，以简明扼要的方式为LLMs提供相关上下文。我们在具有挑战性的混合QA和OTT-QA数据集上使用最先进的LLMs，包括GPT-3.5、GPT-4和LLaMA-3，对我们的方法进行了评估。我们的方法在这两个数据集上都取得了最佳的无样本性能，在Hybrid-QA上的精确匹配得分提高了最多10%，在OTT-QA上提高了5.4%。此外，与原始上下文相比，我们的方法将标记的使用减少了高达53%。|
|**2025-01-29**|**On the Partitioning of GPU Power among Multi-Instances**|Tirth Vamja et.al.|[2501.17752](http://arxiv.org/abs/2501.17752)|null|高效管理云数据中心中的电力对于降低成本、提高性能和最小化环境影响至关重要。GPU对于机器学习（ML）和通用人工智能（GenAI）等任务至关重要，是电力消耗的主要贡献者。NVIDIA的多实例GPU（MIG）技术通过实现具有分区级资源跟踪的独立分区来提高GPU利用率，促进了多个租户之间的GPU共享。然而，由于缺乏硬件支持，在MIG实例之间准确分配GPU电力消耗仍然具有挑战性。本文通过开发软件方法来估算每个MIG分区的电力使用量来应对这一挑战。我们分析了NVIDIA GPU利用率指标，发现具有良好准确性的轻量级方法难以构建。因此，我们探索了基于机器学习的电力模型的使用，以实现准确的分区级电力估算。我们的发现表明，单一的通用离线电力模型或建模方法不适用于各种工作负载，尤其是在并发使用MIG的情况下，并且使用正在执行的工作负载的分区级利用率指标构建的在线模型可以显著提高准确性。我们使用NVIDIA A100 GPU，为包括矩阵乘法和大型语言模型推理在内的工作负载展示了这种准确的分区级电力估算方法，有助于实现透明和公平的碳排放报告。|
|**2025-01-29**|**Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation**|Aitor Arrieta et.al.|[2501.17749](http://arxiv.org/abs/2501.17749)|null|大型语言模型（LLMs）已成为我们日常生活的integral（核心）部分。然而，它们带来了一些风险，包括可能损害个人隐私、延续偏见和传播错误信息等。这些风险突显了对强大的安全机制、道德规范和全面测试的必要性，以确保它们负责任地部署。LLMs的安全是其需要在模型部署和向普通用户开放前彻底测试的关键特性。本文报告了蒙东大学和塞维利亚大学的学者在OpenAI早期安全测试项目中对外测试OpenAI的新款o3-mini LLM的经历。具体而言，我们应用我们的工具ASTRAL，自动和系统性地生成最新的不安全测试输入（即提示），帮助我们测试和评估LLMs的不同安全类别。我们在早期的o3-mini beta版本上自动生成了和执行了总计10,080个不安全的测试输入。在手动验证由ASTRAL归类为不安全的测试用例后，我们识别出总计87个实际的不安全LLM行为实例。我们强调了在OpenAI最新LLMs部署前测试阶段所揭露的关键洞察和发现。|
|**2025-01-29**|**Using Code Generation to Solve Open Instances of Combinatorial Design Problems**|Christopher D. Rosin et.al.|[2501.17725](http://arxiv.org/abs/2501.17725)|**[link](https://github.com/constructive-codes/cpro1)**|**《组合设计手册》收录了许多类型的组合设计，并列出了尚未确定存在性的开放实例。我们开发了一种构造性协议CPro1，该协议使用大型语言模型（LLMs）生成构建组合设计的代码，并解决了一些这些开放实例。该协议从特定类型设计的定义和一个可靠地确认所提议设计是否有效的验证器开始。LLM选择策略并在代码中实现它们，而脚手架则使用验证器提供自动超参数调整和执行反馈。大多数生成的代码都失败了，但通过生成许多候选方案，该协议自动化了各种标准方法（例如模拟退火、遗传算法）的探索，并尝试了各种变化（例如成本函数）以找到成功的方法。在16种不同类型的设计中测试，CPro1为其中6种构建了解决开放实例的方案：对称和斜对称权重矩阵、等距排列数组、打包数组、平衡三值设计和佛罗伦萨矩形。**|
|**2025-01-29**|**RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts**|Eujeong Choi et.al.|[2501.17715](http://arxiv.org/abs/2501.17715)|**[link](https://github.com/boychaboy/ricota)**|**在高度监管的大型语言模型（LLMs）时代，用户与对话代理（CAs）的互动正在演变。随着用户超越程序边界去探索和建立与这些系统的关系，人们越来越关注未经授权的访问或操纵的可能性，这通常被称为“越狱”。此外，对于具有高度人类特征的CAs，用户表现出发起亲密性互动或试图驯服其聊天机器人的倾向。为了捕捉和反映这些野外交互到聊天机器人设计中，我们提出了RICoTA，这是一个由609个挑战LLMs的野外用户制作的对话组成的韩国红队数据集，这些对话捕捉了越狱尝试。我们利用在韩国类似Reddit的社区中自行发布的用户-聊天机器人对话，这些对话包含针对社交聊天机器人的特定测试和游戏意图。通过这些提示，我们旨在评估LLMs识别对话类型和用户测试目的的能力，以推导出减轻越狱风险的聊天机器人设计启示。我们的数据集将通过GitHub公开提供。**|
|**2025-01-28**|**FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data**|Deren Lei et.al.|[2501.17144](http://arxiv.org/abs/2501.17144)|**[link](https://github.com/derenlei/factcg)**|**在之前关于训练基于事实性的分类模型以检测大型语言模型（LLM）中的幻觉的研究中，研究者们主要依赖于公共的自然语言推理（NLI）数据和合成数据。然而，传统的NLI数据集并不适合文档级别的推理，这对于检测LLM幻觉至关重要。近期的一些文档级别合成数据生成方法涉及迭代地从文档中删除句子，并使用基于LLM的提示进行事实性标注。虽然这种方法有效，但对于长文档来说计算成本很高，并且受到LLM能力的限制。在这项工作中，我们分析了现有合成训练数据与真实LLM输出陈述之间的差异。基于我们的发现，我们提出了一种新的合成数据生成方法CG2C，该方法利用从文档中提取的上下文图进行多跳推理。我们的事实核查模型FactCG，在使用相同的骨干模型的基础上，通过更紧密的推理展示了改进的性能。实验表明，它在LLM-Aggrefact基准测试中甚至超过了GPT-4-o，同时模型规模要小得多。**|
|**2025-01-28**|**ASTRAL: Automated Safety Testing of Large Language Models**|Miriam Ugarte et.al.|[2501.17132](http://arxiv.org/abs/2501.17132)|null|大型语言模型（LLMs）因其理解和生成复杂的人类似内容的能力而近期受到关注。然而，确保其安全性至关重要，因为它们可能会提供有害和不安全的回应。现有的LLM测试框架解决了各种与安全相关的问题（例如，毒品、恐怖主义、动物虐待），但往往由于数据集不平衡和过时而面临挑战。在本文中，我们提出了一种名为ASTRAL的工具，该工具自动生成和执行测试用例（即提示），以测试LLMs的安全性。首先，我们引入了一种新的黑盒覆盖标准，以生成平衡且多样化的不安全测试输入，涵盖多样化的安全类别以及语言写作特征（即不同的风格和说服性写作技巧）。其次，我们提出了一种基于LLM的方法，该方法利用检索增强生成（RAG）、少量提示策略和网页浏览来生成最新的测试输入。最后，类似于当前的LLM测试自动化技术，我们利用LLM作为测试或然性来区分安全和不安全的测试输出，从而实现完全自动化的测试方法。我们在知名的LLMs上进行了广泛的评估，揭示了以下关键发现：i）当作为测试或然性时，GPT3.5优于其他LLMs，能够准确地检测到不安全响应，甚至超过了更近期的LLMs（例如，GPT-4）以及专门用于检测不安全LLM输出的LLMs（例如，LlamaGuard）；ii）结果表明，与目前使用的静态数据集相比，我们的方法可以揭露近两倍的不安全LLM行为，使用相同的测试输入数量；iii）我们的黑盒覆盖标准与网页浏览相结合，可以有效地引导LLM生成最新的不安全测试输入，显著增加不安全LLM行为数量。|
|**2025-01-28**|**Optimizing Large Language Model Training Using FP4 Quantization**|Ruizhe Wang et.al.|[2501.17116](http://arxiv.org/abs/2501.17116)|null|随着训练大型语言模型（LLMs）的计算需求不断增长，需要更高效的方法。量化训练通过启用低比特算术运算来降低这些成本，提供了一种有希望的解决方案。虽然FP8精度已经证明了可行性，但利用FP4仍然是一个挑战，因为量化误差显著且表示能力有限。这项工作为LLMs引入了第一个FP4训练框架，通过以下两个关键创新来应对这些挑战：一个用于精确权重更新的可微量化估计器和一种异常值钳位及补偿策略，以防止激活崩溃。为确保稳定性，该框架集成了混合精度训练方案和向量量化。实验结果表明，我们的FP4框架在准确度上与BF16和FP8相当，且退化最小，能够有效扩展到在最多100B个标记上训练的13B参数LLMs。随着支持FP4的下一代硬件的出现，我们的框架为高效的超低精度训练奠定了基础。|
|**2025-01-28**|**Unlocking Transparent Alignment Through Enhanced Inverse Constitutional AI for Principle Extraction**|Carl-Leander Henneking et.al.|[2501.17112](http://arxiv.org/abs/2501.17112)|null|传统的对齐大型语言模型（LLM）的方法，如基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO），依赖于隐含原则，限制了可解释性。宪法人工智能（CAI）提供了一种明确的、基于规则的框架来指导模型输出。在此基础上，我们改进了逆向宪法人工智能（ICAI）算法，该算法从偏好数据集中提取宪法。通过改进原则生成、聚类和嵌入过程，我们的方法提高了从合成数据和真实世界数据集中提取的原则的准确性和泛化能力。虽然上下文对齐产生了适度改进，但我们的结果突出了这些原则促进更透明和适应性对齐方法的潜力，为超越传统微调的未来进步提供了有希望的方向。|
|**2025-01-28**|**Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on Advanced Mathematical Problem-Solving**|Evgenii Evstafev et.al.|[2501.17084](http://arxiv.org/abs/2501.17084)|null|大型语言模型（LLMs）在许多自然语言任务上表现出色，但在复杂数学问题解决、尤其是符号推理和保持输出一致性方面存在困难。本研究评估了10个参数在70亿到80亿之间的LLMs，使用MATH数据集中的945个竞赛级别问题进行测试。重点在于评估它们在推理过程中生成可执行Python代码的能力，涉及超过9,450次代码执行。研究引入了一个使用mistral-large-2411的评价框架，对答案进行5点评分，有助于解决数学符号的不一致性。研究还考察了逐个生成输出对结果精炼的影响。结果显示，在顶级商业模型（gpt-4o-mini，得分83.7%）和最无效的开源模型（open-codestral-mamba:v0.1，得分49.2%）之间存在着显著的34.5%的性能差距。这种差异在数论等复杂领域尤为明显。虽然逐个生成输出略微提高了lama3.1:8b模型的准确性（+0.8%），但也将其代码执行时间减少了36.7%，这突显了效率和精度之间的权衡。研究还注意到，所有模型在难度较大的问题上都表现出准确性较低的一致趋势。尽管使用了受控执行环境，但生成的代码中不到1%是不安全的，3.17%的问题在尝试了10次后仍未解决，这表明混合推理方法可能是有益的。|
|**2025-01-28**|**Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models**|Minghan Li et.al.|[2501.17039](http://arxiv.org/abs/2501.17039)|null|近年来，大型语言模型（LLMs）在包括信息检索在内的多个领域展现了卓越的能力。大多数以往的做法都是利用这些模型为每个查询、每段文本或每份文档单独创建一个嵌入表示，这种策略在检索增强生成（RAG）框架中得到体现并得到应用。尽管这种方法已被证明是有效的，但我们认为，由于它依赖于相对粗糙的表示，因此它在充分捕捉文档级文本的细微复杂性和层次结构方面存在不足。为了解决这一局限性，我们提出了一种新颖的细粒度方法，旨在提高长文档的相关性评分准确性。我们的方法首先将长文档划分为块，每个块都使用LLM进行嵌入以与查询表示进行匹配。在计算相关性评分时，我们通过加权求和方法汇总查询-块相关性评分，从而为整个文档生成一个全面的查询评分。尽管这种方法看似简单，但我们的实验发现，这种方法优于标准表示方法，并且实现了嵌入生成延迟的显著降低。此外，通过精心优化成对损失函数，实现了更优越的性能。|
|**2025-01-28**|**Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies**|Manojkumar Parmar et.al.|[2501.17030](http://arxiv.org/abs/2501.17030)|null|大型语言模型（LLMs）在推理、对齐和特定任务性能方面取得了显著的进步。然而，确保这些系统的无害性仍然是一个关键挑战，尤其是在DeepSeek-R1这样的高级模型中。本文考察了强化学习（RL）作为减少DeepSeek-R1有害输出的主要方法的局限性，并将其与监督微调（SFT）进行了比较。虽然RL提高了推理能力，但它面临奖励黑客攻击、泛化失败、语言混合和高计算成本等挑战。我们提出了结合RL和SFT的混合训练方法，以实现稳健的无害性降低。还提出了DeepSeek-R1的安全部署的使用建议和未来方向。|
|**2025-01-28**|**Automated Refactoring of Non-Idiomatic Python Code: A Differentiated Replication with LLMs**|Alessandro Midolo et.al.|[2501.17024](http://arxiv.org/abs/2501.17024)|**[link](https://github.com/alemidolo/gptidiomrefactoring)**|在Python生态系统中，由于惯用构造的表达能力、提高生产力和甚至效率，其采用得到了促进，尽管关于熟悉度或可理解性的争议。最近的研究贡献提出了基于静态代码分析和转换的方法，以自动识别并将非惯用代码重构为惯用代码的机会。鉴于大型语言模型（LLMs）最近在代码相关任务中提供的潜力，在本文中，我们展示了复制研究的结果，其中我们调查了GPT-4在推荐和建议惯用重构操作方面的有效性。我们的结果表明，GPT-4不仅有效地识别了惯用构造，而且在提出重构操作方面经常超越基准，而现有的基线未能实现。对随机样本的手动分析显示了获得的建议的正确性。我们的发现强调了LLMs在实现以往需要基于复杂代码分析的推荐器任务中的潜力。|
|**2025-01-28**|**Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement**|Kei Katsumata et.al.|[2501.17022](http://arxiv.org/abs/2501.17022)|**[link](https://github.com/keio-smilab24/mmig)**|我们考虑了根据目标物体图像和容器图像生成自由形式移动操作指令的问题。传统的图像标题模型无法生成适当的指令，因为它们的架构通常是针对单张图像进行优化的。在本研究中，我们提出了一种模型，该模型能够同时处理目标物体和容器，以生成用于移动操作任务的自由形式指令句子。此外，我们引入了一种新颖的训练方法，该方法有效地将基于学习型和基于n-gram的自动评估指标得分作为奖励。这种方法使得模型能够学习词语之间的共现关系和适当的释义。结果显示，我们提出的方法在标准自动评估指标上优于基线方法，包括代表性的多模态大型语言模型。此外，物理实验表明，使用我们的方法增强语言指令数据可以提高现有多模态语言理解模型在移动操作方面的性能。|
|**2025-01-28**|**Large Language Models for Code Generation: The Practitioners Perspective**|Zeeshan Rasheed et.al.|[2501.16998](http://arxiv.org/abs/2501.16998)|**[link](https://github.com/gpt-laboratory/llm-evaluation)**|大型语言模型（LLMs）已成为编码助手，能够根据自然语言提示生成源代码。随着LLMs在软件开发中的广泛应用，学术界和行业项目正在开发各种工具、基准和指标来评估LLM生成代码的有效性。然而，缺乏通过实证方法评估、结合从业者视角来评估实际应用中功能、语法和准确性的解决方案。为了解决这一差距，我们提出并开发了一个多模型统一平台，该平台基于自然语言提示生成和执行代码。我们进行了一项调查，调查了来自四大洲11个国家的60名软件从业者，他们在不同的专业角色和领域工作，以评估每个模型的可用性、性能、优势和局限性。结果呈现了从业者的反馈和见解，包括LLMs在软件开发中的优缺点、基准和指标忽视的关键方面，以及对其实际应用的更广泛理解。这些发现可以帮助研究人员和从业者就系统性地选择和使用LLMs在软件开发项目中做出明智的决定。未来的研究将专注于将更多样化的模型集成到所提出的系统中，纳入额外的案例研究，并开展开发者访谈，以深入了解LLM驱动的软件开发。|
|**2025-01-27**|**Evaluating The Performance of Using Large Language Models to Automate Summarization of CT Simulation Orders in Radiation Oncology**|Meiyun Cao et.al.|[2501.16309](http://arxiv.org/abs/2501.16309)|null|目的：本研究旨在利用大型语言模型（LLM）来自动生成CT模拟医嘱的摘要并评估其性能。 材料与方法：从我国机构的Aria数据库中收集了607份患者的CT模拟医嘱。使用本地托管的Llama 3.1 405B模型，通过应用程序编程接口（API）服务提取CT模拟医嘱中的关键词并生成摘要。下载的CT模拟医嘱根据治疗方式和疾病部位分为七个组。对于每个组，与治疗师合作开发了一个定制的指令提示，以指导Llama 3.1 405B模型生成摘要。通过仔细审查每个CT模拟医嘱，手动导出相应摘要的基准，并由治疗师进行验证。使用验证的基准作为参考，由治疗师评估LLM生成的摘要的准确性。 结果：约98%的LLM生成的摘要与手动生成的基准在准确性方面一致。我们的评估显示，与相应治疗师生成的摘要相比，LLM生成的摘要在格式上的一致性和可读性方面有所提高。这种自动方法在所有组中表现出一致的性能，无论治疗方式或疾病部位如何。 结论：本研究证明了Llama 3.1 405B模型在提取关键词和总结CT模拟医嘱方面的精度和一致性，表明LLM有巨大的潜力帮助这项任务，减轻治疗师的工作负担并提高工作流程效率。|
|**2025-01-27**|**RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based Video Event Retrieval**|Long Nguyen et.al.|[2501.16303](http://arxiv.org/abs/2501.16303)|null|由于多媒体内容的快速增长，使用文本查询从视频中检索事件变得越来越具有挑战性。现有的基于文本的视频事件检索方法通常过于关注对象级描述，忽视了上下文信息的重要作用。这种局限性在查询缺乏足够上下文时尤为明显，例如缺少位置细节或背景元素模糊。为了应对这些挑战，我们提出了一种名为RAPID（检索增强并行推理草稿）的新型系统，该系统利用大型语言模型（LLMs）和基于提示的学习的进步来对用户查询进行语义修正和丰富，添加相关的上下文信息。这些丰富的查询随后通过并行检索进行处理，然后通过评估步骤根据与原始查询的匹配度选择最相关的结果。通过对我们自行开发的数据库进行的大量实验表明，RAPID显著优于传统的检索方法，尤其是在处理上下文不完整的查询时。我们的系统在速度和准确性方面得到了验证，通过参加2024年胡志明市人工智能挑战赛，成功地从超过300小时的视频中检索了事件。与比赛组织者提出的基线方法进行比较的进一步评估，显示了RAPID卓越的有效性，突显了我们方法的优势和鲁棒性。|
|**2025-01-27**|**Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With Configurable Depth and Width**|Zheng Liu et.al.|[2501.16302](http://arxiv.org/abs/2501.16302)|null|大型语言模型（LLMs）为执行细粒度文本重排序提供了强大的基础。然而，由于计算带宽的限制，它们在现实中往往难以承受。在这项工作中，我们提出了一种名为“套娃重排序器”（Matroyshka Re-Ranker）的灵活架构，该架构旨在根据用户的配置在运行时定制模型层和每层的序列长度。因此，基于LLM的重排序器可以应用于各种现实情况。这种增加的灵活性可能会以精度损失为代价。为了解决这个问题，我们引入了一套优化性能的技术。首先，我们提出了级联自蒸馏，其中每个子架构学习从其超级组件中保留精确的重排序性能，这些组件的预测可以作为平滑且信息丰富的教师信号被利用。其次，我们设计了一种因子化的补偿机制，其中两个协作的低秩自适应模块，垂直和水平，共同用于补偿由层和序列压缩的任意组合引起的精度损失。我们在MSMARCO的段落和文档检索数据集以及BEIR基准的所有公开数据集上进行了全面的实验。在我们的实验中，Matryoshka Re-Ranker在多种压缩形式和不同应用场景中显著优于现有方法，同时有效地保持了其优越的性能。|
|**2025-01-27**|**Large Models in Dialogue for Active Perception and Anomaly Detection**|Tzoulio Chamiti et.al.|[2501.16300](http://arxiv.org/abs/2501.16300)|**[link](https://github.com/Tzoulio/Large_Models_Dialogue_for_Active_Perception)**|自主空中监控是一个旨在从人类难以到达的区域收集信息的重要任务。同时，这项任务通常需要从相当远的距离或过去未曾遇到的情况下识别异常。在本文中，我们提出了一种新颖的框架，该框架利用大型语言模型（LLMs）提供的先进能力来主动收集信息并在新场景中执行异常检测。为此，我们提出了一种基于LLM的模型对话方法，其中两个深度学习模型进行对话以主动控制无人机，提高感知和异常检测的准确性。我们在一个高保真模拟环境中进行了实验，在该环境中，LLM被提供了一组预定义的自然语言移动命令，这些命令映射为可执行的代码函数。此外，我们还部署了一个多模态视觉问答（VQA）模型，该模型负责视觉问答和图像描述。通过让两个模型进行对话，LLM在同时驾驶无人机飞入场景的不同部分时提出探索性问题，从而提供了一种实现主动感知的新方法。通过利用LLMs的推理能力，我们输出了一种改进的场景详细描述，超越了现有的静态感知方法。除了信息收集之外，我们的方法还用于异常检测，我们的结果证明了所提出的方法在通知和警报潜在危险方面的有效性。|
|**2025-01-27**|**FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers**|Renshan Zhang et.al.|[2501.16297](http://arxiv.org/abs/2501.16297)|null|将高分辨率视觉输入纳入，使多模态大型语言模型（MLLMs）具备了增强的视觉感知能力，以应对现实世界的任务。然而，大多数现有的高分辨率MLLMs都依赖于基于裁剪的方法来处理图像，这导致视觉编码碎片化以及冗余标记数量急剧增加。为了解决这些问题，我们提出了FALCON模型。FALCON引入了一种新颖的视觉寄存器技术，同时实现以下目标：1）在视觉编码阶段消除冗余标记。为了直接解决视觉编码器输出中存在的视觉冗余，我们提出了一种基于寄存器的表示压缩（ReCompact）机制。该机制引入了一组可学习的视觉寄存器，旨在自适应地聚合关键信息，同时丢弃冗余。它使得编码器能够以最小数量的输出标记生成更紧凑的视觉表示，从而消除了额外压缩模块的需求。2）确保视觉编码的连续性。为了解决由碎片化视觉输入引起的潜在编码错误，我们开发了一个寄存器交互注意力（ReAtten）模块。该模块通过允许视觉寄存器之间的交互，促进子图像间有效且高效的信息交换。它确保了整个编码过程中的视觉语义连续性。我们在广泛场景下的高分辨率基准测试中对FALCON进行了全面的实验。FALCON展示了卓越的性能，视觉标记数量实现了显著的9倍和16倍减少。|
|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282](http://arxiv.org/abs/2501.16282)|null|理解脑部疾病对于准确的临床诊断和治疗至关重要。近期在多模态大型语言模型（MLLMs）方面的进展为在文本描述的支持下解读医学图像提供了一种有前景的方法。然而，以往的研究主要集中于2D医学图像，对3D图像丰富的空间信息挖掘不足，基于单一模态的方法则因忽视其他模态中包含的关键临床信息而受限。为了解决这一问题，本文提出了一种名为Brain-Adapter的新方法，该方法通过增加一个额外的瓶颈层来学习新知识并将其注入到原始预训练的知识中。主要思想是在训练过程中引入一个轻量级的瓶颈层，以减少参数数量同时捕捉关键信息，并利用对比语言-图像预训练（CLIP）策略在统一表征空间内对多模态数据进行对齐。大量的实验表明，我们的方法在整合多模态数据方面非常有效，显著提高了诊断准确性，同时没有高计算成本，突显了在增强现实世界诊断工作流程中的潜力。|
|**2025-01-27**|**Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation**|Jiayi Hong et.al.|[2501.16277](http://arxiv.org/abs/2501.16277)|**[link](https://github.com/vaderasu/llm4viz-experiments)**|在本文中，我们评估了两种杰出的大型语言模型（LLMs）的可视化素养：OpenAI的生成预训练变压器（GPT），即ChatGPT的后端，以及Google的Gemini（之前被称为Bard），以建立评估其可视化能力的基准。尽管LLMs在生成图表描述、标题和设计建议方面显示出潜力，但它们在评估可视化方面的潜力仍被低估。收集人类数据用于评估在时间和金钱方面一直是可视化研究的瓶颈，如果LLMs能够在某些有限的范围内充当评估者，它们将是一个重要的资源。为了研究在可视化评估过程中使用LLMs的可行性，我们探讨了LLMs拥有可视化素养的程度——这是它们在领域中有效利用的关键因素。我们使用修改后的53项可视化素养评估测试（VLAT）对GPT-4和Gemini进行了一系列实验。我们的发现表明，与我们探索的LLMs相比，目前它们在可视化素养方面未能达到与VLAT中报告的普通公众数据相同的水准，LLMs在回答问题时过度依赖其预先存在的知识，而不是利用可视化提供的信息。|
|**2025-01-27**|**URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT**|Long Nguyen et.al.|[2501.16276](http://arxiv.org/abs/2501.16276)|null|随着人工智能的快速发展，特别是在自然语言处理领域，大型语言模型（LLMs）在教育问答系统中变得至关重要，尤其是在大学招生聊天机器人中。检索增强生成（RAG）等概念和其他高级技术已被开发出来，通过整合特定大学数据来增强这些系统，使LLMs能够提供有关招生和学术咨询的知情回答。然而，这些增强的RAG技术往往涉及高昂的运营成本，并需要训练复杂的专业模块，这给实际部署带来了挑战。此外，在教育环境中，提供准确答案以防止错误信息至关重要，而基于LLM的系统在没有适当策略和方法的情况下发现这项任务具有挑战性。在本文中，我们介绍了统一RAG（URAG）框架，这是一种混合方法，显著提高了回答的准确性，尤其是在关键查询方面。实验结果表明，URAG增强了我们内部轻量级模型，使其能够与最先进的商业模型相媲美。此外，为了验证其实际适用性，我们在我们的教育机构进行了一项案例研究，获得了积极的反馈和赞誉。这项研究不仅证明了URAG的有效性，还突出了其在教育环境中实际实施的可行性。|
|**2025-01-27**|**A foundation model for human-AI collaboration in medical literature mining**|Zifeng Wang et.al.|[2501.16255](http://arxiv.org/abs/2501.16255)|null|系统文献综述对循证医学至关重要，需要全面分析临床试验出版物。然而，由于在广泛的诊疗领域和多样化的任务中缺乏训练和评估，人工智能（AI）模型在医学文献挖掘中的应用受到了限制。在此，我们提出了LEADS，这是一种用于医学文献研究搜索、筛选和数据分析的AI基础模型。该模型在LEADSInstruct的633,759条指令数据点上进行了训练，这些数据点是从21,335篇系统综述、453,625篇临床试验出版物和27,015个临床试验登记处精心挑选的。我们展示了LEADS在六个任务上对四个最前沿的通用大型语言模型（LLM）的一致性改进。此外，LEADS通过在专家请求后提供支持性参考文献，简化了流程，同时保持了高质量的结果，从而增强了专家工作流程。一项包含14个不同机构16位临床医生和医学研究者的研究显示，与单独工作的专家相比，与LEADS协作的专家在研究选择方面的召回率达到了0.81，而单独工作的专家召回率为0.77，节省了22.6%的时间。在数据提取任务中，使用LEADS的专家达到了0.85的准确率，而没有使用LEADS的准确率为0.80，同时节省了26.9%的时间。这些发现突出了专业医学文献基础模型超越通用模型的可能性，当将其集成到医学文献挖掘的专家工作流程中时，可以带来显著的质量和效率收益。|
|**2025-01-27**|**Multi-Agent Geospatial Copilots for Remote Sensing Workflows**|Chaehong Lee et.al.|[2501.16254](http://arxiv.org/abs/2501.16254)|null|我们提出了GeoLLM-Squad，这是一个地理空间协同工作助手，它将新颖的多智能体范式引入遥感（RS）工作流程。与现有的依赖单一大型语言模型（LLM）的单智能体方法不同，GeoLLM-Squad通过将RS任务委派给专门的子智能体，将智能体编排与地理空间任务解决分离。基于开源的AutoGen和GeoLLM-Engine框架，我们的工作实现了不同应用的模块化集成，涵盖了城市监测、林业保护、气候分析和农业研究等领域。我们的结果表明，当RS任务的复杂性增加时，单智能体系统难以扩展，而GeoLLM-Squad保持了稳健的性能，在智能体正确性方面比最先进的基础模型提高了17%。我们的发现突出了多智能体AI在推进RS工作流程中的潜力。|
|**2025-01-24**|**HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation**|Xin Zhou et.al.|[2501.14729](http://arxiv.org/abs/2501.14729)|**[link](https://github.com/lmd0311/hermes)**|**驾驶世界模型（DWMs）已成为自动驾驶的关键，因为它能够实现未来场景预测。然而，现有的DWMs仅限于场景生成，未能包含场景理解，这涉及到对驾驶环境的解释和推理。在本文中，我们提出了一种名为HERMES的统一驾驶世界模型。我们通过一个统一的框架在驾驶场景中无缝集成3D场景理解和未来场景演变（生成）。具体来说，HERMES利用鸟瞰图（BEV）表示来整合多视图空间信息，同时保留几何关系和交互。我们还引入了世界查询，通过大型语言模型（LLM）中的因果注意力将世界知识融入BEV特征，从而为理解和生成任务提供上下文丰富性。我们在nuScenes和OmniDrive-nuScenes数据集上进行了全面的研究，以验证我们方法的有效性。HERMES实现了最先进的性能，将生成错误减少了32.4%，并将理解指标如CIDEr提高了8.0%。模型和代码将在https://github.com/LMD0311/HERMES上公开发布。**|
|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719](http://arxiv.org/abs/2501.14719)|null|公平获取可靠的卫生信息对公共卫生至关重要，但在线卫生资源的质量因语言而异，这引发了关于大型语言模型（LLMs）在医疗保健中不一致性的担忧。在这项研究中，我们检查了LLMs针对英语、德语、土耳其语和中国语的卫生相关问题的回答的一致性。我们主要通过按疾病类型对卫生相关问题进行分类，并扩大其多语言范围（添加土耳其语和中文翻译），大幅度扩展了HealthFC数据集。我们发现回答中存在显著的差异，可能会传播医疗保健的错误信息。我们的主要贡献包括：1）一个包含疾病类别元信息的多语言卫生相关问题数据集；2）一个基于提示的新型评估工作流程，通过解析实现两种语言之间的子维度比较。我们的发现突出了在多语言环境中部署基于LLM的工具的关键挑战，并强调了改进跨语言对齐以确保准确和公平的卫生信息的需求。|
|**2025-01-24**|**Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models**|Naihao Deng et.al.|[2501.14717](http://arxiv.org/abs/2501.14717)|null|近年来，自然语言处理领域利用指令调整技术来提升大型语言模型（LLMs）在表格相关任务上的性能。然而，先前的研究使用了不同的基础模型和训练数据，导致在结果表格LLMs之间缺乏直接的比较。为了解决这个问题，我们在现有的公共训练数据集上对Mistral、OLMo和Phi系列的基础模型进行了微调。我们的复现实验达到了与现有表格LLMs相当甚至更好的性能，在Hitab表格问答数据集上建立了新的最先进性能。更重要的是，通过系统性的跨领域评估，我们解耦了训练数据和基础模型的贡献，揭示了它们各自的影响。此外，我们还评估了针对表格的指令调整对通用基准测试的影响，揭示了专业化和泛化之间的权衡。|
|**2025-01-24**|**FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing**|James Seale Smith et.al.|[2501.14713](http://arxiv.org/abs/2501.14713)|null|在自然语言处理（NLP）领域，大型语言模型（LLMs）的快速普及对内存受限设备上的高效部署提出了迫切需求，同时又不牺牲性能。我们提出了一种剪枝LLMs的方法，该方法根据重要性分数选择性地剪枝模型块，并用低参数替换策略替换它们。具体来说，我们提出了一种基于权重共享机制的原理性度量方法，该机制利用未剪枝的模型块和块特定的低秩适配器来替换每个剪枝块。此外，我们通过输出特征归一化和基于低秩奇异值分解重建的适配器初始化方案，促进了这些替换块的训练。实证评估表明，与现有方法相比，该方法取得了显著的性能提升，在30%压缩率下在5/6基准测试中达到最先进的性能，在40%压缩率下在6/6基准测试中达到最先进的性能。我们还证明了我们的方法可以扩展较小的模型，通过仅使用大约0.3%的扩展训练令牌，在6/6基准测试中提升了性能，同时额外参数成本最小。|
|**2025-01-24**|**The Karp Dataset**|Mason DiCicco et.al.|[2501.14705](http://arxiv.org/abs/2501.14705)|null|理解大型语言模型（LLMs）的数学推理能力是人工智能研究中的一个核心主题。这个新领域需要创建用于训练和评估LLMs性能的推理任务数据集。为此，我们引入了Karp数据集：这是第一个由NP完备性归约的详细证明组成的数据集。这些归约的难度各异，从本科课程的基本练习到更具挑战性的来自学术论文的归约。我们比较了最先进模型在此任务上的表现，并展示了使用Karp数据集进行微调对推理能力的影响。|
|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693](http://arxiv.org/abs/2501.14693)|null|近期在表格理解领域的研究主要集中在针对表格相关任务对大型语言模型（LLMs）进行指令微调。然而，现有研究忽视了超参数选择的影响，并且缺乏对这些表格LLMs的领域外表格理解能力和一般能力的全面评估。在本文中，我们评估了现有表格LLMs的这些能力，并揭示了与基线模型相比，领域外表格理解能力和一般能力都有显著下降。通过系统分析，我们表明超参数，如学习率，可以显著影响表格特定能力和一般能力。与现有的表格指令微调工作相反，我们证明了较小的学习率和较少的训练实例可以增强表格理解能力，同时保持一般能力。基于我们的发现，我们引入了TAMA，这是一个从LLaMA 3.1 8B Instruct指令微调的表格LLM，它在表格任务上的性能与GPT-3.5和GPT-4相当甚至更好，同时保持了强大的领域外泛化能力和一般能力。我们的发现强调了通过仔细选择超参数来降低数据标注成本和更高效地开发模型的可能性。|
|**2025-01-24**|**An Empirical Study on LLM-based Classification of Requirements-related Provisions in Food-safety Regulations**|Shabnam Hassani et.al.|[2501.14683](http://arxiv.org/abs/2501.14683)|null|随着工业4.0对食品行业的转型，软件在实现食品安全法规合规性方面的作用变得越来越关键。食品安全法规，就像许多法律领域一样，主要以技术无关的方式阐述，以确保其长期性和广泛适用性。然而，这种做法在法规与现代系统和软件之间留下了差距，这些系统和软件被越来越多地用于实施法规。在本文中，我们追求两个主要目标。首先，我们通过对食品安全法规进行扎根理论研究，发展出与系统和软件需求密切相关的食品安全概念的概念性描述。其次，我们检验了两大类大型语言模型（LLM）——BERT和GPT——在根据与要求相关的食品安全概念自动分类法律条文方面的有效性。我们的结果表明：（a）当进行微调时，BERT和GPT家族中表现最佳模型的准确率差异相对较小。然而，在我们的实验中，最强大的模型GPT-4o仍然达到了最高的准确率，平均精确度为89%，平均召回率为87%；（b）使用GPT-4o进行少量样本学习将召回率提高到97%，但将精确度降低到65%，表明了微调和少量样本学习之间的权衡；（c）尽管我们的训练示例仅来自加拿大法规，但基于LLM的分类在来自美国的测试条文中表现始终如一，表明在监管司法管辖范围内具有一定的泛化性；（d）对于我们的分类任务，LLM在性能上显著优于使用长短期记忆（LSTM）网络和自动关键词提取构建的简单基线。|
|**2025-01-24**|**Diffusion based Text-to-Music Generationwith Global and Local Text based Conditioning**|Jisi Zhang et.al.|[2501.14680](http://arxiv.org/abs/2501.14680)|null|基于扩散的文本到音乐（TTM）模型能够根据文本描述生成音乐。通常，基于UNet的扩散模型通过预训练的大型语言模型或跨模态音频语言表示模型生成的文本嵌入进行条件化。本研究提出了一种基于扩散的TTM模型，其中UNet同时依赖于（i）单模态语言模型（例如，T5）通过交叉注意力，以及（ii）跨模态音频语言表示模型（例如，CLAP）通过特征线性调制（FiLM）。该扩散模型被训练以利用来自T5的局部文本表示和来自CLAP的全局表示。此外，我们提出了修改方案，通过我们称之为平均池化和自注意力池化的池化机制从T5中提取全局和局部表示。这种方法减轻了对额外编码器（例如，CLAP）的需求以提取全局表示，从而减少了模型参数的数量。我们的结果表明，将CLAP的全局嵌入结合到T5的局部嵌入中，与仅依赖T5局部嵌入的基线模型（KL=1.54）相比，增强了文本一致性（KL=1.47）。另一方面，通过所提出的平均池化方法直接从T5的局部嵌入中提取全局文本嵌入，在生成质量（FAD=1.89）方面表现出优异表现，而文本一致性（KL=1.51）略低于同时基于CLAP和T5文本嵌入的模型（FAD=1.94和KL=1.47）。我们提出的解决方案不仅高效，而且在所需参数数量方面也是紧凑的。|
|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654](http://arxiv.org/abs/2501.14654)|**[link](https://github.com/stanfordmlgroup/medagentbench)**|**近期，大型语言模型（LLMs）取得了显著的进步，尤其是在作为智能代理方面的能力，从而超越了它们作为聊天机器人的传统角色。这些代理可以利用其规划和工具利用能力来处理高层次指定的任务。然而，目前缺乏一个标准化的数据集来评估LLMs在医疗应用中的智能代理能力，这使得在交互式医疗环境中对LLMs进行复杂任务的评估变得具有挑战性。为了填补这一空白，我们引入了MedAgentBench，这是一个广泛的评估套件，旨在评估大型语言模型在医疗记录环境中的智能代理能力。MedAgentBench包含100个由人类医生编写的、针对特定患者的、来自10个类别的临床任务，以及100个具有超过70万个数据元素的、现实的患者档案，一个符合FHIR标准的交互式环境，以及相应的代码库。该环境使用现代电子病历系统中使用的标准API和通信基础设施，因此可以轻松迁移到实际运行的电子病历系统中。MedAgentBench提供了一个未饱和的智能代理基准，当前最先进的LLMs在其中表现出一定的成功能力。最佳模型（GPT-4o）的成功率为72%。然而，仍有很大的改进空间，为社区提供了下一步优化的方向。此外，不同任务类别之间的性能存在显著差异。MedAgentBench确立了这一点，并在https://github.com/stanfordmlgroup/MedAgentBench上公开发布，为模型开发者提供了一个有价值的框架，以跟踪进展并推动医疗领域大型语言模型智能代理能力的持续改进。**|
|**2025-01-24**|**Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion**|Ziyao Xu et.al.|[2501.14649](http://arxiv.org/abs/2501.14649)|**[link](https://github.com/xzy-xzy/dedc)**|**为了实现通用的、鲁棒的从自然语言到形式语言的转换（N2F），大型语言模型（LLMs）在面对不熟悉的正式语言时需要具备强大的分解和组合能力，并且能够应对组合间隙和反直觉的符号名称。为了调查LLMs是否具备这一组基本能力，我们提出了DEDC框架。该框架半自动地进行样本和任务构建，允许独立评估LLMs在N2F中的分解和组合能力集合。基于此框架，我们评估和分析最先进的LLMs，主要发现包括：（1）LLMs在分解和组合方面都存在不足；（2）LLMs表现出广泛的错误类型，这些错误可以归因于自然语言理解以及符号系统的学习和使用方面的缺陷；（3）组合间隙和反直觉的符号名称都影响了LLMs的分解和组合。我们的工作为研究LLMs在N2F中的分解和组合基本能力提供了新的视角。对不足和归因的详细分析有助于LLMs后续的改进。**|
|**2025-01-23**|**CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation**|Guofeng Cui et.al.|[2501.13927](http://arxiv.org/abs/2501.13927)|null|大型语言模型（LLMs）在自然语言处理任务中展现出巨大潜力，但它们在机器翻译（MT）中的应用仍然面临挑战，这主要归因于在以英语为中心的数据上进行的预训练以及从人类反馈中进行强化学习（RLHF）的复杂性。直接偏好优化（DPO）已成为一种更简单、更高效的替代方案，但其性能高度依赖于偏好数据的质量。为了解决这个问题，我们提出了基于置信度-奖励驱动的偏好优化（CRPO），这是一种新颖的方法，它将奖励分数与模型置信度相结合，以改善用于微调的数据选择。CRPO选择模型不确定或表现不佳的具有挑战性的句子对，从而实现更有效的学习。虽然CRPO主要设计用于LLMs，但它也推广到编码器-解码器模型如NLLB，展示了其多功能性。实证结果表明，CRPO在翻译准确性和数据效率方面均优于现有的RS-DPO、RSO和MBR评分等方法。|
|**2025-01-23**|**Analysis of Indic Language Capabilities in LLMs**|Aatman Vaidya et.al.|[2501.13912](http://arxiv.org/abs/2501.13912)|null|本报告评估了文本输入文本输出的大型语言模型（LLMs）在理解和生成印度语族语言方面的性能。此次评估用于识别和优先考虑适合纳入安全基准的印度语族语言。我们通过回顾现有的评估研究和数据集，以及支持印度语族语言的28个LLMs，进行这项研究。我们根据训练数据、模型和数据的许可、访问类型和模型开发者对LLMs进行分析。我们还比较了评估数据集中印度语族语言的性能，并发现印度语族语言在性能上存在显著差异。印地语是模型中最广泛代表的语言。虽然模型性能与前五种语言的讲者数量大致相关，但之后的评估则有所不同。|
|**2025-01-23**|**Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models**|Linh Tran et.al.|[2501.13904](http://arxiv.org/abs/2501.13904)|null|多模态大型语言模型（LLMs）在通过整合文本、图像和音频等多模态信息以革新客户支持和运营方面起着关键作用。联邦提示学习（FPL）是一种最近提出的方法，它结合了预训练的多模态LLMs（如视觉语言模型）和联邦学习，以创建个性化、保护隐私的AI系统。然而，平衡个性化、泛化性和隐私之间的竞争目标仍然是一个重大挑战。过度个性化可能导致过拟合，降低泛化能力，而严格的隐私措施，如差分隐私，可能会阻碍个性化和泛化。在本文中，我们提出了一种差分隐私联邦提示学习（DP-FPL）方法来应对这一挑战，通过利用低秩适应方案来捕捉泛化，同时保持一个残留项以保留个性化的表达能力。为确保隐私，我们引入了一种新颖的方法，对局部提示的两个低秩组件应用局部差分隐私，对全局提示应用全局差分隐私。我们的方法减轻了隐私噪声对模型性能的影响，同时平衡了个性化与泛化之间的权衡。广泛的实验证明了我们的方法在其他基准上的有效性。|
|**2025-01-23**|**Exploring Finetuned Audio-LLM on Heart Murmur Features**|Adrian Florea et.al.|[2501.13884](http://arxiv.org/abs/2501.13884)|null|大型音频语言模型（LLMs）在识别和分析人类语音、音乐和环境声音方面表现出色。然而，尽管科学界对其产生了浓厚兴趣，但它们理解其他类型声音的潜力，尤其是生物医学声音，仍很大程度上未被充分探索。在这项研究中，我们聚焦于使用心音图（即心脏声音）来诊断心血管疾病。大多数现有的深度神经网络（DNN）方法仅限于心脏杂音分类（健康与非健康），并未预测杂音的其他声学特征，如时间、等级、粗糙度、音调和质量，这些特征对于帮助医生诊断潜在的心脏状况至关重要。我们提出对音频LLM Qwen2-Audio在PhysioNet CirCor DigiScope心音图（PCG）数据集上进行微调，并评估其在分类11个专家标注的杂音特征方面的性能。此外，我们旨在通过探索使用音频表示模型SSAMBA的前处理分割算法，实现更具噪声鲁棒性和泛化能力的系统。我们的结果表明，基于LLM的模型在11个特征中的8个上优于现有最佳方法，在剩下的3个特征上表现相当。此外，LLM成功地分类了具有有限训练数据的长期尾部杂音特征，这是一个所有先前方法都未能分类的任务。这些发现突显了音频LLMs作为辅助人类心脏病学家的潜力，以增强心脏病诊断。|
|**2025-01-23**|**The machine learning platform for developers of large systems**|Alexey Naikov et.al.|[2501.13881](http://arxiv.org/abs/2501.13881)|null|自2021年以来，以检索增强生成（RAG）形式的机器学习系统稳步发展。RAG可以被视为一种知识迁移的版本。在所研究的案例中，大型计算系统被视为RAG的应用点，其中包括大型语言模型（LLM），作为开发团队的合作伙伴。这种方法在开发过程中以及进一步的运营时间中都具有优势。|
|**2025-01-23**|**A RAG-Based Institutional Assistant**|Gustavo Kuratomi et.al.|[2501.13880](http://arxiv.org/abs/2501.13880)|null|尽管大型语言模型（LLMs）在文本生成方面表现出强大的能力，但在需要访问结构化知识库或特定文档的场景中却面临困难，这限制了它们在知识密集型任务中的有效性。为了解决这一局限性，检索增强生成（RAG）模型已被开发出来，使生成模型能够将其输入中纳入相关的文档片段。在本文中，我们设计并评估了一种基于RAG的虚拟助手，专门针对圣保罗大学。我们的系统架构包含两个关键模块：一个检索器和一个生成模型。我们对这两个组件进行了不同类型的模型实验，调整了超参数，如块大小和检索文档的数量。我们最优的检索器模型达到了30%的Top-5准确率，而我们的最有效的生成模型在对比真实答案时得分为22.04%。值得注意的是，当向LLMs提供正确的文档片段时，准确率显著提高至54.02%，提高了超过30个百分点。相反，没有上下文输入时，性能下降至13.68%。这些发现突出了数据库访问在提高LLM性能中的关键作用。它们还揭示了当前语义搜索方法在准确识别相关文档方面的局限性，并强调了LLMs在生成精确响应方面所面临的持续挑战。|
|**2025-01-23**|**On the Reasoning Capacity of AI Models and How to Quantify It**|Santosh Kumar Radha et.al.|[2501.13833](http://arxiv.org/abs/2501.13833)|null|近年来，大型语言模型（LLMs）的进展加剧了关于其推理能力本质的讨论。尽管在GPQA和MMLU等基准测试中取得了高成绩，但这些模型在更复杂的推理任务中表现出局限性，突显了需要更严格的评估方法。我们提出了一种新颖的现象学方法，该方法超越了传统的准确率指标，以探究模型行为的潜在机制，并建立了一个可能广泛影响我们分析和理解AI系统的框架。以多选题推理任务中的位置偏差作为案例研究，我们展示了系统性的扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两种互补的现象学模型：一种是将模型响应分解为推理、记忆和猜测成分的概率混合模型（PMM），另一种是量化模型置信度与策略选择之间关系的信度理论一致性（ITC）分析。通过对推理基准测试的控制实验，我们表明，对于当前模型来说，真正的推理仍然具有挑战性，表面的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明，仅凭准确率往往高估了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。这个框架为现实世界的部署提供了定量标准，允许应用根据策略分布而不是总体性能指标来指定可靠性阈值。|
|**2025-01-23**|**Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing**|Hao Zhang et.al.|[2501.13831](http://arxiv.org/abs/2501.13831)|null|大型语言模型（LLMs）在文本风格转换和语法错误纠正等重写任务中表现出色。尽管这些任务中输入和输出之间存在相当大的重叠，但解码成本仍然随着输出长度的增加而增加，无论重叠程度如何。通过利用输入和输出之间的重叠，Kaneko和Okazaki（2023）提出了模型无关的编辑跨度表示，以压缩重写内容以节省计算。他们在四个重写任务中报告了输出长度减少率接近80%，同时最小化了对准确性的影响。在这篇论文中，我们提出了受短语统计机器翻译启发的替代编辑短语表示。我们系统地比较了我们的短语表示与它们的跨度表示。我们将LLM重写模型应用于自动语音识别（ASR）后编辑任务，并表明我们的仅针对目标短语的编辑表示具有最佳的效率-准确性权衡。在LibriSpeech测试集上，我们的方法缩小了编辑跨度模型与完整重写模型之间的50-60%的词错误率（WER）差距，同时仅损失了编辑跨度模型长度减少率的10-20%。|
|**2025-01-23**|**Hallucinations Can Improve Large Language Models in Drug Discovery**|Shuzhou Yuan et.al.|[2501.13824](http://arxiv.org/abs/2501.13824)|null|研究人员对大型语言模型（LLMs）中出现的幻觉问题表示担忧，然而它们在创意至关重要的领域，如药物发现中的潜力值得探讨。在本文中，我们提出了一个假设：幻觉可以改善LLMs在药物发现中的应用。为了验证这一假设，我们使用LLMs用自然语言描述分子的SMILES字符串，并将这些描述作为提示的一部分，以解决药物发现中的特定任务。在七个LLMs和五个分类任务上进行评估，我们的发现证实了这一假设：包含幻觉的文本可以使LLMs实现更好的性能。值得注意的是，Llama-3.1-8B相比没有幻觉的基线，在ROC-AUC上实现了18.35%的提升。此外，由GPT-4o生成的幻觉在模型中提供了最一致的性能提升。此外，我们还进行了实证分析和案例研究，以调查影响性能的关键因素及其背后的原因。我们的研究为LLMs中幻觉的潜在应用提供了启示，并为未来在药物发现中利用LLMs的新研究方向提供了新的视角。|
|**2025-01-23**|**Large Language Model driven Policy Exploration for Recommender Systems**|Jie Wang et.al.|[2501.13816](http://arxiv.org/abs/2501.13816)|null|最近推荐系统（RS）的发展中融入了强化学习（RL），将推荐视为马尔可夫决策过程（MDP）。然而，在静态用户数据上训练的离线RL策略在动态在线环境中部署时容易受到分布变化的影响。此外，过度关注短期相关项目可能会阻碍探索，导致推荐不佳并负面影响长期用户收益。基于在线RL的RS在生产部署中也面临挑战，因为暴露用户于未训练或不稳定的策略存在风险。大型语言模型（LLM）为模仿用户目标和偏好，在离线预训练策略中提供了有希望的解决方案，以增强在线环境中的初始推荐。有效管理分布变化和平衡探索对于改进基于RL的RS至关重要，尤其是在利用LLM预训练的情况下。为了解决这些挑战，我们提出了一种交互增强学习策略（iALP），该策略利用从LLM中提取的用户偏好。我们的方法包括用用户状态提示LLM以提取项目偏好，根据反馈学习奖励，并使用演员-评论家框架更新RL策略。此外，为了在在线场景中部署iALP，我们引入了一种自适应变体A-iALP，该变体实施了一种简单的微调策略（A-iALP $_{ft}$）和一种旨在减轻策略受损和探索受限问题的自适应方法（A-iALP$_{ap}$ ）。在三个模拟环境中的实验表明，A-iALP带来了显著的性能提升。|
|**2025-01-22**|**Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment**|Melissa Kazemi Rad et.al.|[2501.13080](http://arxiv.org/abs/2501.13080)|null|大型语言模型（LLMs）在包括对话人工智能产品在内的不同应用中展现出了强大的能力，因此确保这些产品的安全和可靠性至关重要。通过减轻它们对恶意用户交互的脆弱性，可以避免重大风险和声誉损害。在本研究中，我们全面探讨了不同LLMs的思维链（CoT）响应微调和对齐的有效性，这些响应作为输入监管的护栏。我们系统地通过利用一小部分训练数据来探索各种调整方法，将这些模型作为代理防御机制来检测恶意输入并提供其判断的推理，从而防止对话代理被滥用。我们严格评估了不同调整策略的有效性和鲁棒性，以泛化到各种对抗性和恶意查询类型。我们的实验结果表明，即使是有限的资源，针对不同有害输入查询的定制对齐过程也具有潜力。这些技术显著提高了对话人工智能系统的安全性，并为部署更安全、更值得信赖的AI驱动交互提供了一个可行的框架。|
|**2025-01-22**|**Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning**|Bohao Yang et.al.|[2501.13042](http://arxiv.org/abs/2501.13042)|**[link](https://github.com/bernard-yang/mmsci_table)**|**近期，大型语言模型（LLMs）在表格理解能力上取得了显著进步，但它们依赖于将表格转换为文本序列。虽然多模态大型语言模型（MLLMs）能够直接进行视觉处理，但由于输入图像分辨率的固定和数值推理能力的不足，它们在处理科学表格方面面临限制。我们提出了一种全面的多模态科学表格理解和推理框架，该框架能够处理具有动态输入图像分辨率的表格。我们的框架由三个关键组件组成：（1）MMSci-Pre，一个包含52K个科学表格结构识别样本的特定领域表格结构学习数据集；（2）MMSci-Ins，一个包含12K样本的指令微调数据集，涵盖了三个基于表格的任务；（3）MMSci-Eval，一个包含3,114个测试样本的基准，专门用于评估数值推理能力。大量的实验表明，与150K个通用领域表格相比，我们的52K个科学表格图像的特定领域方法取得了更好的性能，突出了数据质量相对于数量的重要性。我们提出的基于表格的MLLMs，具有动态输入分辨率，在一般表格理解和数值推理能力方面都有显著提高，并具有良好的泛化能力，适用于未参与训练的数据集。我们的代码和数据在https://github.com/Bernard-Yang/MMSci_Table上公开。**|
|**2025-01-22**|**Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament**|Yantao Liu et.al.|[2501.13007](http://arxiv.org/abs/2501.13007)|**[link](https://github.com/thu-keg/pairwiserm)**|**最佳N个（Best-of-N，BoN）采样是测试时对大型语言模型（LLMs）进行缩放的一种常见策略，它依赖于奖励模型从多个生成中选择最佳候选解决方案。然而，传统的奖励模型通常分配任意且不一致的分数，限制了其有效性。为了解决这个问题，我们提出了一种成对奖励模型（Pairwise RM）与淘汰赛相结合的BoN采样方法。不同于分配绝对分数，针对一个数学问题，Pairwise RM同时评估两个候选解决方案的正确性。这种方法消除了任意评分的需求，并通过并行比较实现了解决方案的交叉验证。在淘汰赛中，Pairwise RM对候选解决方案进行成对比较，并迭代地淘汰错误的解决方案。我们构建了我们的数据集（\ourdataset），这是一个包含443K个成对比较的大规模数据集，来源于NumiaMath，并使用\texttt{gemini-1.5-flash}进行标注，并通过监督微调训练Pairwise RM。在MATH-500和奥林匹克竞赛基准上的实验表明，与传统判别性奖励模型相比，这种方法取得了显著的改进。在最具挑战性的前50%问题上，实现了40%到60%的相对改进。**|
|**2025-01-22**|**Large Language Model-Based Semantic Communication System for Image Transmission**|Soheyb Ribouh et.al.|[2501.12988](http://arxiv.org/abs/2501.12988)|null|大型语言模型（LLMs）在理解和生成各种数据类型（如图像和文本）方面的显著成功，展示了它们处理和提取跨不同领域语义信息的能力。这种变革性能力为语义通信奠定了基础，使得高效和智能的通信系统成为可能。在本工作中，我们提出了一种基于OFDM的图像传输语义通信框架。我们提出了一种创新的语义编码器设计，它利用LLMs提取传输数据意义的能力，而不是关注其原始表示。在接收端，我们设计了一种基于LLM的语义解码器，能够理解上下文并生成最合适的表示以适应给定上下文。我们将在不同场景下评估我们的系统，包括具有不同速度范围的城区宏蜂窝环境。评估指标显示，我们提出的系统将数据量减少了4250倍，同时比传统通信方法实现了更高的数据速率。这种方法为解锁6G连接的潜力提供了一种稳健且可扩展的解决方案。|
|**2025-01-22**|**LLM4WM: Adapting LLM for Wireless Multi-Tasking**|Xuanyu Liu et.al.|[2501.12983](http://arxiv.org/abs/2501.12983)|null|无线信道是通信的基础，包括众多统称为信道相关任务的子任务。这些任务可以通过基于信道特性的联合学习来共享表示并提升系统设计。为了利用这一优势，提出了针对信道相关任务的LLM4WM——一种大型语言模型（LLM）多任务微调框架。该框架采用混合专家与低秩自适应（MoE-LoRA）方法进行多任务微调，使得预训练的LLM的通用知识能够转移到这些任务中。考虑到无线信道数据的独特特性，设计了预处理模块、适配器模块和多任务输出层，以使信道数据与LLM的语义特征空间相匹配。在信道相关多任务数据集上的实验表明，LLM4WM在全样本和少样本评估中均优于现有方法，这归功于其鲁棒的多任务联合建模和迁移学习能力。|
|**2025-01-22**|**OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models**|Chongren Sun et.al.|[2501.12975](http://arxiv.org/abs/2501.12975)|**[link](https://github.com/sunchongren/onioneval)**|**大型语言模型（LLMs）能力强大，但无论是训练还是推理都需要大量的计算资源。在LLM家族中，小型模型（参数少于100亿的模型）在各种任务上也能表现出色。然而，这些小型模型与它们的较大模型类似，存在一些共同的局限性，包括容易产生幻觉。尽管存在许多基准来评估LLM中的幻觉，但很少有专门针对小型LLM（SLLMs）的。此外，SLLMs在不同基准上的表现差异很大。在本文中，我们引入了OnionEval，这是一个多层结构化框架，包含一个称为上下文影响分数（CI）的特定指标，旨在有效评估小型LLM在不同上下文水平上的事实冲突幻觉倾向。我们的实验结果表明，SLLMs的一个关键特征是它们在事实分析方面表现出色，但在上下文推理方面面临挑战。进一步的研究表明，一个简单的思维链策略可以显著减少这些局限性，提高SLLMs在实际应用中的实用性。**|
|**2025-01-22**|**Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs**|Jan Corazza et.al.|[2501.12972](http://arxiv.org/abs/2501.12972)|**[link](https://github.com/informalsystems/fuzzmo_use_case2)**|当提到区块链系统是无信任的，这实际上意味着所有的信任都寄托在软件上。因此，确保区块链软件正确性具有强烈的动机——这里的漏洞可能导致数百万美元的损失并摧毁企业。建立软件正确性的一种最强大的方法是通过使用形式化方法。然而，基于形式化方法的方案在时间和专业知识方面产生了显著的开销，以成功应用它们。我们的工作通过自动化创建形式化模型——软件系统的数学抽象——来解决这一关键缺点，这在使用形式化方法时通常是一个核心任务。我们进行模型综合分为三个阶段：首先将代码转换为模型占位符；然后使用大型语言模型（LLM）“填补空白”；最后，我们在语法和语义层面迭代修复生成的模型。通过这种方式，我们显著减少了创建形式化模型所需的时间，并提高了依赖于它们的宝贵软件验证方法的可及性。我们工作的实际背景是减少使用形式化模型对智能合约正确性进行审计的价值实现时间。|
|**2025-01-22**|**It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act**|Kristof Meding et.al.|[2501.12962](http://arxiv.org/abs/2501.12962)|null|构成公平决策的因素是什么？这个问题不仅对人类来说很难，当使用人工智能（AI）模型时，挑战更大。鉴于歧视性算法行为，欧盟最近通过了《人工智能法案》，该法案要求对AI模型制定特定规则，结合了传统的非歧视法律规范和基于机器学习的算法公平性概念。本文旨在通过以下方式弥合《人工智能法案》中这两种不同概念之间的差距：首先，对法律和计算机科学方向的学者进行高级别的概念介绍；其次，对《人工智能法案》中法律非歧视规范与算法公平性之间的关系进行深入分析。我们的分析揭示了三个关键发现：（1）大多数非歧视性规定仅针对高风险AI系统。（2）高风险系统的监管包括数据输入要求和输出监控，尽管这些规定往往不一致，并引发计算可行性的问题。（3）针对通用AI模型的规定，例如不被同时归类为高风险系统的大型语言模型，与其他规定相比，目前缺乏具体性。基于这些发现，我们建议为AI系统开发更具体的审计和测试方法。本文旨在为研究AI系统中歧视问题的法律学者和计算机科学方向的机器学习研究人员之间的未来跨学科合作奠定基础。|
|**2025-01-22**|**Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference**|Weizhi Fei et.al.|[2501.12959](http://arxiv.org/abs/2501.12959)|null|尽管涉及长文本输入的应用对于大型语言模型（LLM）的有效利用至关重要，但它们也导致了计算成本的增加和性能的降低。为了应对这一挑战，我们提出了一种高效、无需训练的提示压缩方法，该方法能在压缩提示中保留关键信息。我们识别出基于Transformer的LLM中特定的注意力头，将其指定为评估头，这些评估头能够选择长输入中对于推理最关键的标记。基于这一发现，我们开发了EHPC（基于评估头的提示压缩方法），它通过在预填充阶段仅利用带有评估头的前几层，使LLM能够快速“浏览”输入提示，随后仅将重要标记传递给模型进行推理。EHPC在两个主流基准测试中实现了最先进的结果：提示压缩和长文本推理加速。因此，它有效地降低了与商业API调用相关的复杂性和成本。我们进一步证明了EHPC与基于键值缓存加速的方法相比具有竞争力，从而突出了其在提高LLM长文本任务效率方面的潜力。|
|**2025-01-22**|**GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models**|Pengxiang Zhao et.al.|[2501.12956](http://arxiv.org/abs/2501.12956)|null|大型语言模型（LLMs）由于资源需求庞大，面临着显著的部署挑战。虽然低比特量化权重可以减少内存使用并提高推理效率，但当前硬件缺乏对混合精度通用矩阵乘法（mpGEMM）的原生支持，导致基于解量化的实现效率低下。此外，均匀量化方法通常无法充分捕捉权重分布，导致性能下降。我们提出了GANQ（GPU自适应非均匀量化），这是一种针对硬件高效的基于查找表优化的层间后训练非均匀量化框架。GANQ通过利用无训练、GPU自适应优化算法，有效地减少了层间量化误差，从而实现了卓越的量化性能。大量实验表明，与最先进的3位和4位量化方法相比，GANQ能够将FP16基线下的困惑度差距减少。此外，当部署在单个NVIDIA RTX 4090 GPU上时，GANQ的量化模型相较于基线实现了高达2.57倍的加速，进一步提升了LLMs部署中的内存和推理效率。|
|**2025-01-21**|**InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling**|Yi Wang et.al.|[2501.12386](http://arxiv.org/abs/2501.12386)|**[link](https://github.com/opengvlab/internvideo)**|**本文旨在通过长而丰富的上下文（LRC）建模来提高视频多模态大型语言模型（MLLM）的性能。为此，我们开发了一个新的InternVideo2.5版本，着重于增强原始MLLM感知视频中的细粒度细节和捕捉长时序结构的能力。具体来说，我们的方法通过直接偏好优化将密集视觉任务标注融入MLLM，并通过自适应分层标记压缩开发紧凑的时空表示。实验结果表明，这种独特的LRC设计极大地提高了视频MLLM在主流视频理解基准（短时与长时）中的表现，使得MLLM能够记住显著更长的视频输入（至少比原始版本长6倍），并掌握如物体跟踪和分割等专业的视觉能力。我们的工作强调了多模态上下文丰富性（长度和精细度）在赋予MLLM内在能力（关注力和记忆力）中的重要性，为未来视频MLLM研究提供了新的见解。代码和模型可在https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5获取。**|
|**2025-01-21**|**Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists**|Thomas F. Eisenmann et.al.|[2501.12374](http://arxiv.org/abs/2501.12374)|**[link](https://github.com/andreskarjus/genaiexperiment)**|**新型生成式AI在分析和生成文化物品方面的能力引发了关于艺术教育和人类专业性质与价值的必然问题。AI是否已经拉平了专业艺术家与业余人士之间的竞争场地，或者训练有素的艺术家表达力、策展技能和经验反而增强了使用这些新工具的能力？在这项预先注册的研究中，我们对50位活跃的艺术家和与他们在人口统计学上相匹配的业余人士样本进行了实验比较。我们设计了两个任务来模拟艺术实践，以测试他们在忠实和创造性图像创作方面的能力：复制一个参考图像，以及尽可能远离它。我们开发了一个定制的平台，参与者使用现代文本到图像模型来完成这两个任务。我们还收集并比较了参与者对AI的情感。平均而言，艺术家比他们的业余同行产生了更忠实和更具创造性的输出，尽管差距很小。尽管AI可能简化了内容创作，但专业知识仍然有价值——即使在生成式AI的局限空间内也是如此。最后，我们还探讨了如果让一个代表性的具有视觉能力的大型语言模型（GPT-4o）担任图像生成代理，它将如何完成同样的任务，并发现它在复制方面表现相当，但在创造性任务中甚至超过了艺术家。在这两个任务中，最好的结果仍然是由人类产生的。这些结果突出了将艺术技能与AI培训相结合的重要性，以帮助艺术家和其他视觉专业人士为技术不断发展的环境做好准备。我们看到了与生成式AI协作协同的潜力，这可能会重塑创意产业和艺术教育。**|
|**2025-01-21**|**Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL**|Yeounoh Chung et.al.|[2501.12372](http://arxiv.org/abs/2501.12372)|**[link](https://github.com/yeounoh/lc_nl2sql)**|大型语言模型（LLMs）在自然语言处理的各种任务中展现出令人印象深刻的能力。特别是推理能力的提升和上下文窗口的扩展，为利用这些强大模型开辟了新的途径。NL2SQL（自然语言到SQL）的挑战在于，自然语言问题本身具有固有的歧义性，而SQL生成则需要精确理解复杂的数据模式和语义。解决这种语义歧义问题的一种方法是提供更多和足够的上下文信息。在本研究中，我们探讨了Google最先进的LLM（gemini-1.5-pro）提供的扩展上下文窗口（也称为长上下文）的性能和延迟之间的权衡。我们研究了各种上下文信息的影响，包括列示例值、问题和SQL查询对、用户提供的提示、SQL文档和模式。据我们所知，这是第一个研究扩展上下文窗口和额外上下文信息如何从准确性和延迟成本两个方面帮助NL2SQL生成的作品。我们表明，长上下文LLMs具有鲁棒性，不会在扩展的上下文信息中迷失方向。此外，我们基于Google的gemini-pro-1.5的长上下文NL2SQL管道在BIRD基准（开发）上取得了优异的性能，达到67.41%，无需微调和昂贵的自洽性技术。|
|**2025-01-21**|**Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration**|Thomas Walshe et.al.|[2501.12332](http://arxiv.org/abs/2501.12332)|null|在现实世界的机器学习项目中，获取标记的训练数据一直是一项成本高昂的任务，以满足数量和质量的要求。最近，大型语言模型（LLMs），尤其是GPT-4，在以高精度标记数据方面展现出巨大的潜力。然而，隐私和成本问题阻碍了GPT-4的广泛应用。在这项工作中，我们探索了有效利用开源模型进行自动标记的方法。我们确定将标签模式集成是一项有希望的技术，但发现仅用标签描述进行分类会导致在高基数任务上性能不佳。为了解决这个问题，我们提出了检索增强分类（RAC）方法，其中LLM一次对单个标签进行推理，使用相应的标签模式；我们从最相关的标签开始，迭代直到LLM选择一个标签。我们表明，我们的方法，通过动态集成标签描述，在标记任务中提高了性能。我们进一步表明，通过仅关注最有希望的标签，RAC可以在标签质量和覆盖范围之间进行权衡——这是我们利用来自动标记我们内部数据集的特性。|
|**2025-01-21**|**VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model**|Xianwei Zhuang et.al.|[2501.12327](http://arxiv.org/abs/2501.12327)|**[link](https://github.com/VARGPT-family/VARGPT)**|**我们提出了一种名为VARGPT的新型多模态大型语言模型（MLLM），该模型在单个自回归框架内统一了视觉理解和生成。VARGPT采用下一个标记预测范式进行视觉理解，采用下一个尺度预测范式进行视觉自回归生成。VARGPT创新性地扩展了LLaVA架构，在MLLM中实现了高效的尺度自回归视觉生成，同时在一个模型框架内无缝地容纳了混合模态的输入和输出。我们的VARGPT在特别策划的数据集上进行了三阶段的统一训练过程，包括预训练阶段和两个混合视觉指令调整阶段。统一的训练策略旨在实现视觉和文本特征的对齐，增强理解和生成方面的指令遵循，以及提高视觉生成质量。尽管其基于LLAVA的多模型理解架构，VARGPT在视觉问答和推理等各个视觉中心基准测试中，显著优于LLaVA-1.5。值得注意的是，VARGPT自然支持自回归视觉生成和指令到图像合成的功能，展示了其在视觉理解和生成任务中的多功能性。项目页面为：\url{https://vargpt-1.github.io/}**|
|**2025-01-21**|**LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations**|Hasan Abu-Rasheed et.al.|[2501.12300](http://arxiv.org/abs/2501.12300)|null|在学习个性化为学习者提供了巨大潜力的情况下，现代高等教育实践需要更深入地考虑领域模型和学习环境，以开发有效的个性化算法。本文介绍了一种创新的高等教育课程建模方法，该方法利用大型语言模型（LLMs）进行知识图谱（KG）补全，旨在创建个性化的学习路径推荐。我们的研究侧重于对大学学科进行建模，并将它们的主题与相应的领域模型联系起来，使学生学习路径中能够整合来自不同学院和机构的课程模块。我们方法的核心是一个协作过程，其中LLMs协助人类专家从讲座材料中提取高质量、细粒度的主题。我们为大学模块和利益相关者开发了领域、课程和用户模型。我们将此模型应用于从两个研究模块（嵌入式系统和基于FPGA的嵌入式系统开发）创建知识图谱。所得知识图谱结构化了课程并将其与领域模型相联系。我们通过定性专家反馈和定量图质量指标来评估我们的方法。领域专家验证了模型的相关性和准确性，而图质量指标衡量了我们知识图谱的结构属性。我们的结果表明，LLM辅助的图补全方法增强了将相关课程连接起来以实现个性化学习体验的能力。专家反馈还表明，对于概念提取和分类，提出的协作方法得到了高度接受。|
|**2025-01-21**|**MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks**|Qishen Zhou et.al.|[2501.12281](http://arxiv.org/abs/2501.12281)|**[link](https://github.com/youxiaotu/MoGERNN)**|**在给定部分观测到的道路网络的情况下，我们如何预测未观测位置的交通状况？虽然深度学习方法在交通预测方面表现出卓越的性能，但大多数方法都假设在所有感兴趣的地点都有传感器，这在经济上是不切实际的。此外，这些方法通常在传感器配置发生变化时需要昂贵的重新训练。为了解决这些挑战，我们提出了MoGERNN，一个归纳时空图表示模型。受大型语言模型中混合专家方法（Mixture of Experts）的启发，我们引入了一个混合图专家（MoGE）模块，通过多个图消息聚合器和稀疏门控网络来建模复杂的空间依赖关系。该模块估计未观测位置的初始状态，然后这些状态由一个基于GRU的编码器-解码器处理，该解码器集成了一个图消息聚合器来捕捉时空依赖关系并预测未来状态。在两个真实世界数据集上的实验表明，MoGERNN在观测和未观测位置上都一致优于基线方法。MoGERNN甚至可以在没有传感器的区域准确预测交通拥堵的发展，为交通管理提供有价值的信息。此外，MoGERNN能够适应动态传感网络，即使与重新训练的版本相比，也能保持具有竞争力的性能。使用不同数量的可用传感器进行的测试证实了其持续的优越性，而消融研究验证了其关键模块的有效性。**|
|**2025-01-21**|**Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement**|Maosong Cao et.al.|[2501.12273](http://arxiv.org/abs/2501.12273)|**[link](https://github.com/internlm/condor)**|监督微调（SFT）数据的质量在提升大型语言模型（LLMs）的对话能力方面起着关键作用。然而，随着LLMs的不断发展，高质量人工标注的SFT数据获取已成为一个重要的瓶颈，这导致了对合成训练数据的更大依赖。在这项工作中，我们引入了Condor，这是一个新颖的两阶段合成数据生成框架，它结合了世界知识树和自我反思优化，以大规模生成高质量的SFT数据。我们的实验结果表明，仅在20K个Condor生成的样本上进行微调的基础模型，其性能优于同类模型。Condor中的额外优化阶段进一步使得LLMs能够在不同规模（高达720亿）上进行迭代自我改进，验证了我们的方法的有效性。此外，我们对训练后合成数据扩展的研究揭示了大量未开发的性能提升潜力，为未来的研究开辟了有希望的道路。|
|**2025-01-21**|**FOCUS: First Order Concentrated Updating Scheme**|Yizhou Liu et.al.|[2501.12243](http://arxiv.org/abs/2501.12243)|null|大型语言模型（LLMs）展现出惊人的性能，而改进它们的预训练过程似乎是其能力进一步提升的关键。基于Adam、学习率衰减和权重衰减已记录的成功，我们假设预训练损失景观具有狭窄的谷地结构。通过合成损失函数的实验，我们发现当梯度查询噪声相对于谷地的尖锐度较高时，Adam的性能落后于Signum，因为Adam会急剧减少有效步长。这一观察结果促使我们开发了FOCUS，这是一种优化器，通过引入对移动平均参数的吸引力来增强Signum，使其在处理噪声的同时保持更大的步长。在训练GPT-2时，FOCUS证明比Signum更稳定，比Adam更快。这些结果表明，梯度噪声可能是LLM训练中被低估的限制因素，而FOCUS提供了有希望的解决方案。|
|**2025-01-21**|**InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models**|Pha Nguyen et.al.|[2501.12231](http://arxiv.org/abs/2501.12231)|null|通过观察人类执行多步骤任务，可以构建具有情境感知的助手，这些助手能够根据对动作和任务的了解提供帮助。在本文中，我们开发了一个基于多模态大型语言模型的情境感知指导任务助手（InsTALL），它利用在线视觉流（例如用户的屏幕共享或视频录制）并实时响应用户有关当前任务的查询。为了实现有用的帮助，InsTALL 1）在任务视频和配对文本数据上训练一个多模态模型，2）自动从视频数据中提取任务图，并在训练和推理时利用它。我们表明，InsTALL在多模态活动理解所考虑的子任务中实现了最先进的性能——任务识别（TR）、动作识别（AR）、下一步动作预测（AP）和计划预测（PP），并且在两个与自动错误识别相关的创新子任务上优于现有基线。|
|**2025-01-17**|**FaceXBench: Evaluating Multimodal LLMs on Face Understanding**|Kartik Narayan et.al.|[2501.10360](http://arxiv.org/abs/2501.10360)|**[link](https://github.com/kartik-3004/facexbench)**|**多模态大型语言模型（MLLMs）在广泛的任务和领域中展示了令人印象深刻的解决问题的能力。然而，它们在面部理解方面的能力尚未得到系统研究。为了填补这一空白，我们引入了FaceXBench，这是一个综合性的基准，旨在评估MLLMs在复杂面部理解任务上的表现。FaceXBench包含来自25个公共数据集和新建的FaceXAPI数据集的5,000个多模态多选题。这些问题涵盖了6个广泛类别中的14个任务，评估了MLLMs在偏见与公平性、面部认证、识别、分析、定位和工具检索方面的面部理解能力。使用FaceXBench，我们对26个开源MLLMs以及2个专有模型进行了广泛的评估，揭示了复杂面部理解任务中的独特挑战。我们分析了模型在三个评估设置中的表现：零样本、上下文任务描述和思维链提示。我们的详细分析表明，当前的MLLMs，包括像GPT-4o和GeminiPro 1.5这样的高级模型，仍有很大的改进空间。我们相信FaceXBench将成为开发能够执行复杂面部理解的MLLMs的关键资源。代码：https://github.com/Kartik-3004/facexbench**|
|**2025-01-17**|**Agent4Edu: Generating Learner Response Data by Generative Agents for Intelligent Education Systems**|Weibo Gao et.al.|[2501.10332](http://arxiv.org/abs/2501.10332)|**[link](https://github.com/bigdata-ustc/agent4edu)**|个性化学习是智能教育系统中一种有潜力的教育策略，旨在提高学习者的实践效率。然而，离线指标与在线表现之间的差异严重阻碍了他们的进步。为了应对这一挑战，我们引入了Agent4Edu，这是一种利用大型语言模型（LLMs）在人类智能领域最新进展的个性化学习模拟器。Agent4Edu具有由LLM驱动的生成型智能体，这些智能体配备了针对个性化学习算法的学习者档案、记忆和动作模块。学习者档案使用现实世界的响应数据初始化，捕捉实践风格和认知因素。受人类心理学理论的启发，记忆模块记录实践事实和高级摘要，并整合反思机制。动作模块支持各种行为，包括练习理解、分析和响应生成。每个智能体都可以与个性化学习算法（如计算机自适应测试）交互，从而实现定制服务的多方面评估和提升。通过全面评估，我们探讨了Agent4Edu的优缺点，强调了智能体和人类学习者之间在回答的一致性和差异。代码、数据和附录可在https://github.com/bigdata-ustc/Agent4Edu上公开获取。|
|**2025-01-17**|**Large language models for automated scholarly paper review: A survey**|Zhenzhen Zhuang et.al.|[2501.10326](http://arxiv.org/abs/2501.10326)|null|大型语言模型（LLMs）对人类社会产生了重大影响，涉及多个领域。其中，学术界不仅是受到LLMs影响的领域，也是LLMs发展的关键力量。在学术出版物中，这一现象体现在将LLMs纳入同行评审机制来审阅稿件的过程中。我们在之前的研究论文中提出了自动学术论文评审（ASPR）的概念。随着这种纳入的扩展，现在进入了ASPR与同行评审共存阶段，这在之前的论文中有所描述。LLMs对于ASPR的全面实施具有变革潜力，但同时也带来了需要解决的新问题和挑战。在这篇综述论文中，我们旨在为LLMs时代的ASPR提供一个全面的视角。我们首先调查了哪些LLMs被用于进行ASPR。然后，我们回顾了随着LLM技术的融入，ASPR相关技术瓶颈得到了哪些解决。接着，我们探讨了LLMs为ASPR带来的新方法、新数据集、新源代码和新在线系统。此外，我们总结了LLMs在ASPR中的性能和问题，并调查了出版商和学术界对ASPR的态度和反应。最后，我们讨论了与LLMs在ASPR开发中相关的挑战。我们希望这篇综述能为研究人员提供启发性的参考，并促进ASPR在实际应用中的进展。|
|**2025-01-17**|**HiMix: Reducing Computational Complexity in Large Vision-Language Models**|Xuange Zhang et.al.|[2501.10318](http://arxiv.org/abs/2501.10318)|null|近年来，得益于大型语言模型和模态对齐技术的进步，现有的大型视觉-语言模型（LVLMs）在众多场景中取得了显著的性能。然而，过高的计算复杂度限制了这些模型在实际应用中的广泛应用。我们认为，计算复杂度的主要瓶颈之一是由于模型计算中涉及了冗余的视觉序列。这一观点源于对LVLMs语言解码器中视觉和语言信息传输效率的重新评估。随后，我们提出了一种新的分层视觉-语言交互机制，称为“混合注意的分层视觉注入”（HiMix）。在HiMix中，只有语言序列经历完整的正向传播，而视觉序列在每个语言解码器层中与语言在特定阶段进行交互。值得注意的是，我们的方法显著降低了计算复杂度，同时性能损失最小。具体来说，HiMix在多个LVLM模型中将语言解码器的计算成本降低了10倍，同时保持了可比较的性能。这突显了我们方法的优势，我们希望我们的研究为视觉-语言理解领域带来新的视角。  项目页面：https://xuange923.github.io/HiMix|
|**2025-01-17**|**Addressing Popularity Bias in Third-Party Library Recommendations Using LLMs**|Claudio Di Sipio et.al.|[2501.10313](http://arxiv.org/abs/2501.10313)|null|软件工程推荐系统（RSSE）在通过根据开发者的上下文提供相关建议来自动化开发任务中发挥着关键作用。然而，它们遭受了所谓的流行度偏差，即推荐可能对当前任务不相关的流行物品的现象。特别是，长尾效应可能会损害系统在准确性方面的性能，从而导致推荐中的假阳性。基础模型是当前最先进的基于生成式AI的模型，在多个软件工程（SE）任务中实现了相关结果。本文旨在研究大型语言模型（LLMs）解决第三方库（TPLs）推荐系统中流行度偏差的能力。我们进行了一项消融研究，实验了缓解流行度偏差的最先进技术，包括微调和流行度惩罚机制。我们的发现表明，所考虑的LLMs无法解决TPL推荐器中的流行度偏差，尽管微调和后处理惩罚机制有助于提高提供的推荐的整体多样性。此外，我们讨论了LLMs在此背景下的局限性，并提出了解决TPL推荐器中流行度偏差的潜在改进建议，从而为这一方向上的进一步实验铺平了道路。|
|**2025-01-17**|**Computational Protein Science in the Era of Large Language Models (LLMs)**|Wenqi Fan et.al.|[2501.10282](http://arxiv.org/abs/2501.10282)|null|考虑到蛋白质的重要性，计算蛋白质科学一直是一个关键的科学研究领域，致力于揭示蛋白质序列-结构-功能范式中的知识并开发应用。在过去的几十年里，人工智能（AI）对计算蛋白质科学产生了重大影响，导致在特定蛋白质建模任务中取得了显著的成就。然而，之前的AI模型仍然存在局限性，例如难以理解蛋白质序列的语义，以及在广泛的蛋白质建模任务中无法进行泛化。最近，由于它们前所未有的语言处理和泛化能力，大型语言模型（LLMs）已成为AI的一个里程碑。它们可以促进整个领域的全面进步，而不仅仅是解决个别任务。因此，研究人员积极地将LLM技术引入计算蛋白质科学，开发出能够巧妙地掌握蛋白质基础知识的蛋白质语言模型（pLMs），并且可以有效地泛化来解决各种序列-结构-功能推理问题。在见证繁荣发展的同时，有必要对LLM技术赋能的计算蛋白质科学进行系统概述。首先，我们根据所掌握的蛋白质知识将现有的pLMs分类，即潜在序列模式、明确的结构和功能信息以及外部科学语言。其次，我们介绍pLMs的利用和适应性，突出它们在促进蛋白质结构预测、蛋白质功能预测和蛋白质设计研究方面的显著成就。然后，我们描述pLMs在抗体设计、酶设计和药物发现中的实际应用。最后，我们具体讨论这个快速发展的领域的有希望的未来方向。|
|**2025-01-17**|**Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation**|Azat Abdullin et.al.|[2501.10200](http://arxiv.org/abs/2501.10200)|null|自动生成测试是软件工程研究中的一个关键且持续关注的领域。大型语言模型（LLMs）的出现为执行广泛任务的能力带来了新的机遇。然而，与基于搜索的软件测试（SBST）和符号执行等传统技术相比，基于LLM的方法的有效性仍然不确定。在本文中，我们对基于三个工具的自动测试生成方法进行了广泛的研究：EvoSuite用于SBST，Kex用于符号执行，TestSpark用于基于LLM的测试生成。我们在GitBug Java数据集上评估了这些工具的性能，并使用各种基于执行和基于特征的指标进行比较。我们的结果表明，尽管基于LLM的测试生成具有潜力，但在覆盖率方面仍然落后于传统方法。然而，它在突变得分上显著优于它们，这表明LLM提供了对代码的更深层次语义理解。基于LLM的方法在故障检测能力方面也劣于SBST和基于符号执行的方法。此外，我们的基于特征的分析表明，所有工具主要受所测试类（CUT）的复杂性和内部依赖性的影响，而基于LLM的方法对CUT的大小特别敏感。|
|**2025-01-17**|**Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education**|William Hersh et.al.|[2501.10186](http://arxiv.org/abs/2501.10186)|null|生成式人工智能在生物医学和健康领域产生了深远的影响，这既体现在专业工作中，也体现在教育中。基于大型语言模型（LLMs），生成式人工智能在模拟情况下（如参加医学考试、回答临床问题、解决临床案例、应用临床推理和总结信息）的表现与人类相当。生成式人工智能在教育领域也被广泛应用，在学术课程及其评估中表现良好。本文综述了LLMs在教育领域的成功案例，并突出了其在教育中面临的挑战，尤其是可能损害专业工作知识和技能获取的方面。然后，本文提供了关于如何克服LLMs在教育中使用的不足的最佳实践建议。尽管在教育中使用生成式人工智能存在挑战，但所有学生和教师，不仅在生物医学和健康领域，还应在其他领域，都必须理解和掌握其使用。|
|**2025-01-17**|**Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval**|Vera Pavlova et.al.|[2501.10175](http://arxiv.org/abs/2501.10175)|null|本研究探讨了在伊斯兰领域内自然语言处理（NLP）技术的应用，重点关注开发一个伊斯兰神经检索模型。通过利用强大的XLM-R模型，研究采用语言缩减技术创建了一个轻量级双语大型语言模型（LLM）。我们的领域自适应方法针对伊斯兰领域面临的独特挑战，即在域内语料库大量存在阿拉伯语，而在其他语言（包括英语）中则有限的问题。该工作采用多阶段训练过程来提高检索模型的表现，结合了大型检索数据集（如MS MARCO）和较小的域内数据集。此外，我们通过数据增强技术和可靠的伊斯兰来源，创建了一个英语域内检索数据集。这种方法增强了特定领域的检索数据集，从而进一步提升了性能。研究发现，将领域自适应和多阶段训练方法结合用于双语伊斯兰神经检索模型，使其在下游检索任务上优于单语模型。|
|**2025-01-17**|**Exploring the Impact of Generative Artificial Intelligence in Education: A Thematic Analysis**|Abhishek Kaushik et.al.|[2501.10134](http://arxiv.org/abs/2501.10134)|null|近年来，生成式人工智能（GenAI）技术的进步对教育领域产生了变革性影响。大型语言模型（LLMs）如ChatGPT和Bard可以用于自动化常规任务，创建个性化教学内容，以及处理重复性任务，从而为学生提供更多创造性思考的时间。然而，在教育领域，制定指导方针、政策和评估方法以确保这些工具的负责任整合至关重要。本文对从教育领域专业人士那里收集到的七篇论文进行了主题分析，以了解在教育中使用ChatGPT和Bard等GenAI模型的优势和不足。对论文进行了探索性数据分析（EDA），以从文本中提取更多见解。研究发现，有几个主题突出了GenAI工具的利弊，以及克服这些局限性的建议，并确保学生在负责任和道德的方式下使用这些工具。|
|**2025-01-16**|**Distilling Multi-modal Large Language Models for Autonomous Driving**|Deepti Hegde et.al.|[2501.09757](http://arxiv.org/abs/2501.09757)|null|自动驾驶需要安全的运动规划，尤其是在关键的“长尾”场景中。最近的全端到端自动驾驶系统利用大型语言模型（LLM）作为规划器来提高对罕见事件的泛化能力。然而，在测试时使用LLM引入了高计算成本。为了解决这个问题，我们提出了DiMA，一个端到端自动驾驶系统，它保持了无LLM（或基于视觉）规划器的效率，同时利用了LLM的世界知识。DiMA通过一系列专门设计的代理任务，将来自多模态LLM的信息提炼为基于视觉的端到端规划器。在联合训练策略下，两个网络共有的场景编码器产生结构化的表示，这些表示在语义上是扎根的，并且与最终的规划目标相一致。值得注意的是，LLM在推理时是可选的，这使规划既稳健又不会牺牲效率。使用DiMA进行训练，使得L2轨迹误差减少了37%，基于视觉的规划器的碰撞率降低了80%，以及在长尾场景中的轨迹误差减少了44%。DiMA还在nuScenes规划基准测试中实现了最先进的性能。|
|**2025-01-16**|**Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues**|Youngjoon Jang et.al.|[2501.09754](http://arxiv.org/abs/2501.09754)|null|我们的目标是把连续的手语翻译成口语文本。受人类译员依赖上下文进行准确翻译的启发，我们将额外的上下文线索与手语视频结合到一个新的翻译框架中。具体来说，除了编码输入视频的视觉手语识别特征外，我们还整合了以下互补文本信息：(i)描述背景节目的字幕，(ii)先前句子的翻译，以及(iii)记录手语的伪注释。这些信息被自动提取并与视觉特征一起输入到一个预训练的大型语言模型（LLM）中，我们对该模型进行微调以生成文本形式的口语翻译。通过广泛的消融研究，我们展示了每个输入线索对翻译性能的积极贡献。我们在目前最大的英国手语数据集BOBSL上训练和评估了我们的方法。我们发现，与在BOBSL上之前报道的结果相比，以及与我们作为基线实施的最新方法相比，我们的上下文方法显著提高了翻译质量。此外，我们还通过将其应用于How2Sign——一个美国手语数据集，证明了我们方法的一般性，并取得了具有竞争力的结果。|
|**2025-01-16**|**OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking**|Zekun Xi et.al.|[2501.09751](http://arxiv.org/abs/2501.09751)|**[link](https://github.com/zjunlp/omnithink)**|机器写作使用大型语言模型时通常依赖于检索增强生成。然而，这些方法仍然局限于模型预定义的范围之内，限制了生成丰富信息的内容。具体来说，传统的检索信息往往缺乏深度、实用性和重复性，这会负面影响生成文章的质量，导致内容浅薄、重复且缺乏原创性。为了解决这些问题，我们提出了OmniThink，这是一个模仿人类迭代扩展和反思过程的机器写作框架。OmniThink背后的核心思想是模拟学习者在逐步深化对主题知识的过程中所表现出的认知行为。实验结果表明，OmniThink在提高生成文章的知识密度的同时，并未损害如连贯性和深度等指标。人类评估和专家反馈进一步突出了OmniThink在解决生成长篇文章的实际挑战中的潜力。|
|**2025-01-16**|**Enhancing Lexicon-Based Text Embeddings with Large Language Models**|Yibin Lei et.al.|[2501.09749](http://arxiv.org/abs/2501.09749)|null|近期，大型语言模型（LLMs）在通用文本嵌入任务上展现了卓越的性能。尽管密集嵌入在相关研究中占主导地位，但我们首次引入了基于词库的嵌入模型（LENS），它利用LLMs在这些任务上实现了具有竞争力的性能。针对传统因果LLMs中固有的分词冗余问题和单向注意力限制，LENS通过分词嵌入聚类来整合词汇空间，并研究双向注意力和各种池化策略。具体来说，LENS通过将每个维度分配给特定的分词簇来简化词库匹配，其中语义相似的词被分组在一起，并通过双向注意力释放LLMs的潜力。大量实验表明，LENS在大型文本嵌入基准（MTEB）上优于密集嵌入，提供了与密集嵌入相当大小的紧凑特征表示。值得注意的是，将LENS与密集嵌入相结合在MTEB的检索子集（即BEIR）上实现了最先进的性能。|
|**2025-01-16**|**Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models**|Bihui Jin et.al.|[2501.09745](http://arxiv.org/abs/2501.09745)|null|机器学习开发者经常使用交互式计算笔记本，如Jupyter笔记本，来托管数据处理和模型训练的代码。Jupyter笔记本提供了编写机器学习管道和交互式观察输出的便利工具，然而，由于笔记本的长度和复杂性，维护Jupyter笔记本，例如添加新功能或修复错误，可能会很具挑战性。此外，目前还没有与Jupyter笔记本开发者编辑相关的现有基准。为了解决这个问题，我们提出了第一个由GitHub上792个机器学习仓库的20,095次修订中提取的48,398个Jupyter笔记本编辑的数据集，并进行了第一个使用大型语言模型（LLMs）预测Jupyter笔记本代码编辑的研究。我们的数据集捕捉了单元格级别和行级别的修改的细节，为理解机器学习工作流程中的实际维护模式提供了基础。我们观察到，Jupyter笔记本的编辑非常局部化，更改平均只涉及存储库中的166行代码。虽然较大模型在代码编辑方面优于较小模型，但所有模型在微调后在我们的数据集上的准确性仍然很低，这表明现实世界机器学习维护任务的复杂性。我们的发现强调了上下文信息在提高模型性能中的关键作用，并为提升大型语言模型在工程机器学习代码方面的能力指出了有前景的方向。|
|**2025-01-16**|**Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps**|Nanye Ma et.al.|[2501.09732](http://arxiv.org/abs/2501.09732)|null|生成模型在各个领域产生了重大影响，这主要得益于它们在训练过程中通过增加数据、计算资源和模型规模来扩展的能力，这一现象被称为扩展定律。最近的研究开始探索大型语言模型（LLMs）在推理时间内的扩展行为，揭示了性能如何随着推理过程中额外计算的增加而进一步提高。与LLMs不同，扩散模型本质上是灵活的，可以通过调整去噪步骤的数量来调整推理时间的计算，尽管性能提升通常在几十步之后就会趋于平坦。在这项工作中，我们探索了扩散模型在增加去噪步骤之外的推理时间扩展行为，并研究如何通过增加计算来进一步提高生成性能。具体来说，我们考虑了一个搜索问题，旨在识别扩散采样过程中更好的噪声。我们沿着两个轴来构建设计空间：用于提供反馈的验证器和用于寻找更好噪声候选者的算法。通过对类条件化和文本条件化图像生成基准的大量实验，我们的发现表明，增加推理时间计算可以显著提高扩散模型生成的样本质量，并且由于图像的复杂性质，框架中组件的组合可以具体选择以符合不同的应用场景。|
|**2025-01-16**|**CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education**|Tianyu Wang et.al.|[2501.09709](http://arxiv.org/abs/2501.09709)|**[link](https://github.com/tisage/cybermentor)**|许多非传统学生在网络安全专业中往往缺乏来自同伴、家庭成员和教授的建议，这可能会阻碍他们的学习体验。此外，由于内容相关性、建议的地域性、最低专业知识和时间问题等因素，这些学生可能无法充分利用各种基于大型语言模型（LLM）的人工智能助手。本文通过介绍一个旨在为这些学生的需求提供全面支持的、解答与知识、技能和职业准备建议相关问题的应用程序来解决这些挑战。我们开发了一个学习工具平台——CyberMentor，以解决网络安全专业学生的多样需求和痛点。该平台由代理工作流和生成式大型语言模型（LLMs）驱动，利用检索增强生成（RAG）实现准确和上下文相关的信息检索，以达到可访问性和个性化。我们展示了它在解决网络安全教育知识需求、应对职业市场竞争力、解决分析性和编程作业的技能需求以及提供实时按需学习支持方面的价值。通过三个使用场景，我们展示了CyberMentor在促进知识获取和职业准备、提供无缝的基于技能的指导和支持方面的作用。我们还采用了基于LangChain提示的评估方法来评估平台的影响，证实了其在帮助性、正确性和完整性方面的出色表现。这些结果强调了系统在支持学生发展实用网络安全技能的同时，改善高等教育中的公平性和可持续性的能力。此外，CyberMentor的开源设计允许其在其他学科中进行调整，促进教育创新并扩大其潜在影响。|
|**2025-01-16**|**Domain Adaptation of Foundation LLMs for e-Commerce**|Christian Herold et.al.|[2501.09706](http://arxiv.org/abs/2501.09706)|null|我们介绍了e-Llama模型：8亿和70亿参数的大规模语言模型，这些模型针对电子商务领域进行了调整。这些模型被视为具有电子商务深度知识的基座模型，为指令和微调提供基础。e-Llama模型是通过在1万亿个特定领域数据上持续预训练Llama 3.1基座模型获得的。我们讨论了我们的方法，并通过一系列消融研究来解释我们选择超参数的动机。为了量化模型在电子商务领域的适应程度，我们定义并实施了一系列多语言、针对电子商务的评估任务。我们表明，在仔细选择训练设置的情况下，Llama 3.1模型可以适应新领域，而不会在通用领域任务上牺牲显著性能。我们还探讨了将调整后的模型与基座模型合并的可能性，以更好地控制领域间的性能权衡。|
|**2025-01-16**|**Simulated Interactive Debugging**|Yannic Noller et.al.|[2501.09694](http://arxiv.org/abs/2501.09694)|null|软件调试，即故障定位及其修复，是软件工程中的主要活动。因此，有效的、高效的调试是软件工程师必须培养的核心技能之一。然而，调试技术的教学通常非常有限，或者只是以间接的方式教授，例如在软件项目中进行。结果，大多数计算机科学（CS）学生以零散和无组织的方式学习调试。在这项工作中，我们提出了一种称为模拟交互式调试的方法，该方法以交互式的方式引导学生进行调试过程。这种指导旨在赋予学生修复他们解决方案的能力，并拥有适当的“学习”体验。我们设想，这种引导调试技术可以集成到计算机科学教育课程中的编程课程早期阶段。为了进行初步评估，我们开发了一个原型实现，使用传统的故障定位技术和大型语言模型。学生可以使用诸如自动设置断点或交互式聊天机器人等特性。我们设计和执行了一个包括这个集成到IDE中的工具的受控实验，实验对象为八名本科生。基于反馈，我们得出结论，参与者喜欢辅助调试器的系统指导。特别是，他们将自动设置断点评为最有效，其次是交互式调试和聊天，以及设置断点的解释。在我们未来的工作中，我们将改进我们的概念和实现，添加新功能，并进行更深入的用户研究。|
|**2025-01-16**|**Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models**|Fengli Xu et.al.|[2501.09686](http://arxiv.org/abs/2501.09686)|null|语言长期以来被视为人类推理的必要工具。大型语言模型（LLMs）的突破引发了研究人员利用这些模型解决复杂推理任务的重大兴趣。研究人员已经超越了简单的自回归标记生成，通过引入“思维”的概念——代表推理过程中中间步骤的标记序列。这一创新范式使得LLMs能够模仿复杂的人类推理过程，如树状搜索和反思性思考。最近，一种新兴的学习推理趋势将强化学习（RL）应用于训练LLMs掌握推理过程。这种方法通过试错搜索算法自动生成高质量的推理轨迹，通过提供大量训练数据显著扩展了LLMs的推理能力。此外，最近的研究表明，鼓励LLMs在测试时推理过程中使用更多标记进行“思考”可以进一步提高推理准确性。因此，训练时间和测试时间的扩展共同展示了一个新的研究前沿——通向大型推理模型的路径。OpenAI的o1系列产品的推出标志着这一研究方向的重大里程碑。在本综述中，我们全面回顾了LLMs推理方面的最新进展。我们首先介绍了LLMs的基础背景，然后探讨了推动大型推理模型发展的关键技术组件，重点关注自动化数据构建、学习推理技术和测试时间扩展。我们还分析了构建大型推理模型的流行开源项目，并以开放挑战和未来研究方向结束。|
|**2025-01-15**|**Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails**|Shaona Ghosh et.al.|[2501.09004](http://arxiv.org/abs/2501.09004)|null|随着大型语言模型（LLMs）和生成式人工智能的日益普及，对内容安全性的担忧也在同步增长。目前，针对LLM相关安全风险的全面范围，缺乏高质量、人工标注的数据集，这些数据集可用于商业应用。为了填补这一空白，我们提出了一种全面且可适应的安全风险分类法，该分类法分为12个顶级危害类别，并扩展到9个细粒度子类别。这个分类法旨在满足下游用户的多样化需求，提供更细致和灵活的工具来管理各种风险类型。我们使用一种混合数据生成流程，结合人工标注和多LLM“陪审团”系统来评估响应的安全性，从而获得了Aegis 2.0，这是一组精心挑选的34,248个人工-LLM交互样本集合，按照我们提出的分类法进行标注。为了验证其有效性，我们展示了几个使用参数高效技术训练的轻量级模型，在Aegis 2.0上的性能与在更大、非商业数据集上完全微调的领先安全模型相当。此外，我们引入了一种新的训练混合方法，将安全性与主题跟随数据相结合。这种方法增强了守护模型的适应性，使其能够在推理过程中泛化到新的风险类别。我们计划将Aegis 2.0数据和模型开源给研究社区，以帮助LLMs的安全防护。|
|**2025-01-15**|**Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models**|Emma Croxford et.al.|[2501.08977](http://arxiv.org/abs/2501.08977)|null|随着大型语言模型（LLMs）被整合到电子健康记录（EHR）工作流程中，在实施前对它们进行验证的工具有助于评估其性能。现有的用于评估医生文档质量的工具通常不适用于LLM生成的文本的复杂性，且缺乏对真实世界数据的验证。为了评估LLM生成的临床摘要，开发了医生文档摘要质量工具（PDSQI-9）。使用多个LLMs（GPT-4o、Mixtral 8x7b和Llama 3-8b）从多个专业领域的真实世界EHR数据中生成多文档摘要。验证包括皮尔逊相关系数用于内容效度、因素分析和Cronbach's alpha用于结构效度、评分者间可靠性（ICC和Krippendorff's alpha）用于普遍性、半德尔菲过程用于内容效度，以及高质量与低质量摘要的比较用于区分效度。七位医生评分者评估了779个摘要并回答了8,329个问题，实现了超过80%的评分者间可靠性效力。PDSQI-9显示出强烈的内部一致性（Cronbach's alpha = 0.879；95% CI：0.867-0.891）和高评分者间可靠性（ICC = 0.867；95% CI：0.867-0.868），支持结构效度和普遍性。因素分析确定了4个因素模型，解释了58%的方差，代表了组织、清晰度、准确性和实用性。实质性效度得到了笔记长度与简洁性（rho = -0.200，p = 0.029）和组织性（rho = -0.190，p = 0.037）评分之间的相关性支持。区分效度区分了高质量与低质量摘要（p < 0.001）。PDSQI-9显示出稳健的构念效度，支持其在临床实践中用于评估LLM生成的摘要，并促进LLMs更安全地融入医疗工作流程。|
|**2025-01-15**|**Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models**|Karukriti Kaushik Ghosh et.al.|[2501.08974](http://arxiv.org/abs/2501.08974)|null|基于方面的情感分析（ASBA）是一种精细化的情感分析方法，旨在根据产品、服务或实体的特定方面或特征提取和分类情感。与传统情感分析不同，后者将整体评论或文本分配一个总体情感得分，而ASBA专注于将文本分解为单个组件或方面（例如，质量、价格、服务）并对每个方面进行情感评估。这使客户意见的理解更加细致，使企业能够确定具体的优势和改进领域。该过程包括几个关键步骤，包括方面提取、情感分类以及针对评论段落或其他用户提供的任何形式的方面级情感聚合。ASBA在产品评论、社交媒体监控、客户反馈分析和市场研究等领域具有重大应用。通过利用自然语言处理（NLP）和机器学习技术，ASBA促进了有价值见解的提取，使公司能够做出基于数据的决策，从而提高客户满意度和优化产品。随着ASBA的发展，它有潜力通过提供对各种产品方面的更深入的情感理解，大大改善个性化客户体验。在本研究中，我们分析了LLMs在完全跨领域的基于方面的情感分析中的强度，旨在为某些产品定义框架并将其用于其他类似情况。我们认为，在SemEval-2015任务12的基于方面的情感分析数据集上，可以达到92%的准确率。|
|**2025-01-15**|**Analyzing the Ethical Logic of Six Large Language Models**|W. Russell Neuman et.al.|[2501.08951](http://arxiv.org/abs/2501.08951)|null|本研究考察了六种著名的大型生成语言模型的伦理推理：OpenAI GPT-4o、Meta LLaMA 3.1、Perplexity、Anthropic Claude 3.5 Sonnet、Google Gemini 和 Mistral 7B。研究探讨了这些模型如何阐述和应用伦理逻辑，特别是在应对道德困境，如电车问题和海因茨困境时的表现。不同于传统的对齐研究，本研究采用了可解释性-透明度框架，促使模型解释其伦理推理。这种方法通过三种已建立的伦理类型进行分析：后果主义-义务论分析、道德基础理论和科尔伯格道德发展阶段模型。研究发现，大型语言模型展现出高度一致的伦理逻辑，以理性主义和后果主义为重点，决策往往优先考虑伤害最小化和公平。尽管在预训练和模型架构上存在相似之处，但在伦理推理方面，模型之间出现了细微和显著的差异，这反映了微调和后训练过程中的变化。这些模型始终展现出博学、谨慎和自我意识，其伦理推理类似于道德哲学研究生级别的讨论。令人印象深刻的是，这些系统都宣称自己的伦理推理比典型的人类道德逻辑更复杂。|
|**2025-01-15**|**Applying General Turn-taking Models to Conversational Human-Robot Interaction**|Gabriel Skantze et.al.|[2501.08946](http://arxiv.org/abs/2501.08946)|null|轮流发言是对话的基本要素，但当前的人机交互（HRI）系统通常依赖于简单基于沉默的模型，导致不自然的停顿和中断。本文首次探讨了通用轮流发言模型，特别是TurnGPT和语音活动投影（VAP）的应用，以改善HRI中的对话动态。这些模型使用自监督学习目标在人类-人类对话数据上训练，无需特定领域的微调。我们提出了将这些模型结合使用的方法，以预测机器人何时开始准备回应、轮流发言和处理潜在的干扰。我们在一个受试者内研究中评估了所提出的系统，与传统的基线系统进行了比较，使用了Furhat机器人，与39名成年人在对话环境中进行，并结合了一个大型语言模型进行自主回应生成。结果显示，参与者显著更喜欢所提出的系统，并且它显著减少了回应延迟和中断。|
|**2025-01-15**|**Disentangling Exploration of Large Language Models by Optimal Exploitation**|Tim Grams et.al.|[2501.08925](http://arxiv.org/abs/2501.08925)|null|探索是自我提升和开放式问题解决的关键技能。然而，大型语言模型是否能够有效探索状态空间仍存在疑问。现有的评估主要关注探索和利用之间的权衡，通常在多臂老虎机问题中进行评估。相比之下，这项工作将探索作为唯一目标，要求代理提供增强未来回报的信息。为了评估，我们提出通过测量已探索状态的最优可实现回报来将缺失奖励分解为探索和利用两个部分。我们使用各种大型语言模型进行的实验表明，大多数模型难以充分探索状态空间，而弱探索是不够的。我们观察到模型大小与探索性能之间存在正相关关系，大型模型表现出更优越的能力。此外，我们还表明，我们的分解可以揭示在提示工程中由代理指令驱动的行为差异，为优化探索任务中的LLM性能提供了一个有价值的工具。|
|**2025-01-15**|**GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge**|Liam Dugan et.al.|[2501.08913](http://arxiv.org/abs/2501.08913)|**[link](https://github.com/liamdugan/raid)**|近期，许多共享任务都针对从大型语言模型（LLMs）检测生成的文本。然而，这些共享任务往往侧重于以下两种情况之一：一是文本局限于特定领域；二是文本可以来自许多领域，其中一些在测试时可能看不到。在这个共享任务中，我们使用新发布的RAID基准，旨在回答模型是否能够检测从大量且固定的领域和LLMs生成的文本，这些领域和LLMs在训练过程中都是可见的。在三个月的时间里，9个团队尝试了这个任务，提交了23个检测器。我们发现，多个参与者能够在RAID的机器生成文本上获得超过99%的准确率，同时保持5%的误报率——这表明检测器能够稳健地同时检测来自许多领域和模型的文本。我们讨论了这一结果的可能解释，并为未来的研究提供了方向。|
|**2025-01-15**|**Leveraging Large Language Models as Knowledge-Driven Agents for Reliable Retrosynthesis Planning**|Qinyu Ma et.al.|[2501.08897](http://arxiv.org/abs/2501.08897)|**[link](https://github.com/qinyuma316/retrosynthesisagent)**|在材料化学中识别可靠的合成途径是一项复杂的任务，尤其是在聚合物科学中，这是因为大分子具有复杂且往往非唯一的命名。为了应对这一挑战，我们提出了一种集成大型语言模型（LLMs）和知识图谱（KGs）的智能体系统。通过利用LLMs强大的提取和识别化学物质名称的能力，并将提取的数据存储在结构化的知识图谱中，我们的系统完全自动化了相关文献的检索、反应数据的提取、数据库查询、逆合成途径树的构建，以及通过检索额外文献和推荐最佳反应途径的进一步扩展。一种新颖的多分支反应途径搜索（MBRPS）算法能够探索所有途径，特别关注多分支途径，帮助LLMs克服在多分支路径上的推理弱点。这项工作代表了首次尝试开发一个由LLMs驱动的专门针对大分子的完全自动化的逆合成规划智能体。应用于聚酰亚胺合成，我们的新方法构建了一个包含数百条途径的逆合成途径树，并推荐了优化路线，包括已知和新型途径，证明了其有效性和更广泛应用的潜力。|
|**2025-01-15**|**How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering**|Christoph Treude et.al.|[2501.08774](http://arxiv.org/abs/2501.08774)|null|人工智能（AI），包括大型语言模型和生成式AI，正成为软件开发领域的一股重要力量，为开发者提供了贯穿整个开发生命周期的强大工具。尽管软件工程研究已经广泛研究了AI工具在软件开发中的应用，但开发者与这些AI工具之间的具体交互类型直到最近才开始受到关注。理解和改进这些交互有望提高AI驱动工作流程的生产力、信任和效率。在本文中，我们提出了一种开发者与AI工具之间交互类型的分类法，确定了十一种不同的交互类型，例如自动补全代码建议、命令驱动操作和对话式辅助。基于这一分类法，我们概述了一个研究议程，重点关注优化AI交互、提高开发者控制和解决AI辅助开发中的信任和可用性挑战。通过为研究开发者-AI交互建立一个结构化的基础，本文旨在激发对创建更有效、适应性更强的AI工具进行软件开发的科研活动。|
|**2025-01-15**|**Enhanced Large Language Models for Effective Screening of Depression and Anxiety**|June M. Liu et.al.|[2501.08769](http://arxiv.org/abs/2501.08769)|null|抑郁和焦虑障碍广泛存在，需要及时识别和管理。近期大型语言模型（LLMs）的进步为解决这一问题提供了潜在方案，但高昂的成本和关于训练数据的伦理担忧仍然是挑战。本文介绍了一种合成临床访谈的流程，生成了1,157个交互式对话（PsyInterview），并提出了基于LLM的情绪障碍筛查系统EmoScan。EmoScan能够区分粗略的疾病（例如焦虑或抑郁障碍）和精细的疾病（例如重度抑郁障碍），并进行高质量的访谈。评估结果显示，EmoScan在筛查情绪障碍方面的表现超过了基础模型和其他LLMs（如GPT-4），其F1分数为0.7467。它还提供了优越的解释（BERTScore为0.9408），并在外部数据集上表现出强大的泛化能力（F1分数为0.67）。此外，EmoScan在访谈技巧方面优于基线，这一点通过自动评分和人工评估得到了验证。这项工作强调了可扩展的数据生成管道对于开发有效的心理健康LLM工具的重要性。|
|**2025-01-14**|**PokerBench: Training Large Language Models to become Professional Poker Players**|Richard Zhuang et.al.|[2501.08328](http://arxiv.org/abs/2501.08328)|**[link](https://github.com/pokerllm/pokerbench)**|**我们介绍了PokerBench——一个用于评估大型语言模型（LLM）打扑克能力的基准。随着LLM在传统NLP任务上的出色表现，它们在复杂策略游戏如扑克中的应用带来了新的挑战。扑克是一种不完整信息游戏，需要多种技能，如数学、推理、计划、策略，以及对博弈论和人类心理的深入了解。这使得扑克成为大型语言模型的理想下一个前沿领域。PokerBench由11,000个最重要的场景组成，分为开牌和翻牌阶段，并与经验丰富的扑克玩家合作开发。我们评估了包括GPT-4、ChatGPT 3.5以及各种Llama和Gemma系列模型在内的显要模型，发现所有最先进的LLM在打最优扑克方面表现不佳。然而，经过微调后，这些模型显示出明显的改进。我们通过让不同分数的模型相互竞争来验证PokerBench，结果表明在PokerBench上的高分对应着实际扑克游戏中的更高胜率。通过我们微调后的模型与GPT-4之间的游戏，我们还识别了简单监督微调在学习最优游戏策略方面的局限性，这表明需要更高级的方法来有效地训练语言模型以在游戏中表现出色。因此，PokerBench为LLM的扑克打能力提供了一个独特的快速可靠评估基准，以及一个全面基准来研究LLM在复杂游戏场景中的进展。数据集和代码将在以下网址提供：\url{https://github.com/pokerllm/pokerbench}。**|
|**2025-01-14**|**Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks**|Miran Heo et.al.|[2501.08326](http://arxiv.org/abs/2501.08326)|null|我们提出了Omni-RGPT，这是一种多模态大型语言模型，旨在促进图像和视频的区域级理解。为了在时空维度上实现一致的区域表示，我们引入了Token Mark，一套突出视觉特征空间中目标区域的标记。这些标记通过区域提示（例如，框或掩码）直接嵌入到空间区域中，并同时纳入文本提示以指定目标，从而在视觉和文本标记之间建立直接联系。为了进一步支持无需tracklets的鲁棒视频理解，我们引入了一个辅助任务，该任务利用标记的一致性指导Token Mark，使视频中的区域解释保持稳定。此外，我们还引入了一个大规模的区域级视频指令数据集（RegVID-300k）。Omni-RGPT在基于图像和视频的常识推理基准测试中取得了最先进的成果，同时在字幕和指代表达理解任务中表现出色。|
|**2025-01-14**|**ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations**|Ziyuan Huang et.al.|[2501.08324](http://arxiv.org/abs/2501.08324)|null|阿尔茨海默病分析模型生成1（ADAM-1）是一个多智能体大型语言模型（LLM）框架，旨在整合和分析多模态数据，包括微生物组特征、临床数据集和外部知识库，以增强对阿尔茨海默病（AD）的理解和检测。通过利用检索增强生成（RAG）技术以及其多智能体架构，ADAM-1综合来自不同数据源的观点，并利用文献驱动的证据对发现进行语境化。与XGBoost的比较评估显示，ADAM-1的平均F1分数与XGBoost相似，但方差显著降低，突显了其鲁棒性和一致性，特别是在小型实验室数据集中。虽然目前针对二元分类任务进行优化，未来的迭代旨在纳入额外的数据模态，如神经影像学和生物标志物，以拓宽其在阿尔茨海默病研究和诊断中的可扩展性和适用性。|
|**2025-01-14**|**Exploring Robustness of Multilingual LLMs on Real-World Noisy Data**|Amirhossein Aliakbarzadeh et.al.|[2501.08322](http://arxiv.org/abs/2501.08322)|**[link](https://github.com/caisa-lab/llms-real-world-noise-robustness)**|**大型语言模型（LLMs）在可能包含人类造成的拼写错误的网络数据上进行了训练。但它们是否对类似的现实世界噪声具有鲁棒性？在本文中，我们研究了现实世界拼写错误对9个语言模型性能的影响，这些模型的参数从0.2B到13B不等，在3个不同的自然语言处理（NLP）任务上，即自然语言推理（NLI）、命名实体识别（NER）和意图分类（IC）上。我们在6种不同的语言上进行了实验，并使用维基百科编辑历史为它们构建了一个现实世界噪声词典。我们发现，在所有数据集和语言的平均值中，所研究模型在干净和噪声测试数据上的性能差距从2.3到4.3个百分点不等。此外，与BLOOM、Falcon和Bert-like模型相比，mT5模型通常表现出更强的鲁棒性。特别是，mT5（13B）在平均意义上在3个任务和6种语言中的4种上表现最为鲁棒。**|
|**2025-01-14**|**Enhancing Automated Interpretability with Output-Centric Feature Descriptions**|Yoav Gur-Arieh et.al.|[2501.08319](http://arxiv.org/abs/2501.08319)|**[link](https://github.com/yoavgur/feature-descriptions)**|**自动可解释性管道为大型语言模型（LLMs）中由特征表示的概念生成自然语言描述，例如植物或句子中的第一个单词。这些描述是通过激活特征的输入得到的，这些输入可能是模型表示空间中的某个维度或方向。然而，识别激活输入是代价高昂的，特征在模型行为中的机制作用既取决于输入如何使特征激活，也取决于特征激活如何影响输出。通过使用引导评估，我们揭示了当前管道提供的描述未能捕捉到特征对输出的因果效应。为了解决这个问题，我们提出了高效、以输出为中心的方法来自动生成特征描述。这些方法使用特征刺激后权重更高的标记，或者直接将词汇“反嵌入”头应用于特征后的最高权重标记。我们的以输出为中心的描述比以输入为中心的描述更好地捕捉了特征对模型输出的因果效应，但结合两者在输入和输出评估上都取得了最佳性能。最后，我们表明，以输出为中心的描述可用于找到先前被认为是“失效”的特征的激活输入。**|
|**2025-01-14**|**HALoGEN: Fantastic LLM Hallucinations and Where to Find Them**|Abhilasha Ravichander et.al.|[2501.08292](http://arxiv.org/abs/2501.08292)|null|尽管生成式大型语言模型（LLMs）在生成高质量、流畅文本方面表现出色，但它们也产生了幻觉：与既定世界知识或提供输入背景不符的陈述。然而，测量幻觉具有挑战性，因为让人类即时验证模型生成内容既昂贵又耗时。在这项工作中，我们发布了HALoGEN，这是一个全面的幻觉基准，包括：（1）涵盖编程、科学归属和摘要等九个领域的10923个用于生成模型的提示；（2）针对每个用例的自动高精度验证器，将LLM生成内容分解为原子单元，并针对高质量知识源验证每个单元。我们使用这个框架评估了14个语言模型的大约150,000个生成内容，发现即使是表现最好的模型也充满了幻觉（有时根据领域不同，生成的原子事实中高达86%是幻觉）。我们进一步基于幻觉是否可能源自对训练数据的错误回忆（A类错误）、训练数据中的错误知识（B类错误）或伪造（C类错误）来定义了一种新型LLM幻觉错误分类。我们希望我们的框架为研究为什么生成模型会产生幻觉提供基础，并推进可信赖的大型语言模型的发展。|
|**2025-01-14**|**LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding**|Hongyu Li et.al.|[2501.08282](http://arxiv.org/abs/2501.08282)|**[link](https://github.com/appletea233/llava-st)**|**近期在多模态大型语言模型（MLLMs）方面的进步显示出很有希望的结果，但现有方法在同时有效处理时间和空间定位方面仍存在困难。这一挑战源于两个关键问题：首先，引入时空定位会带来大量的坐标组合，这使得语言和视觉坐标表示的对齐变得复杂；其次，在视频特征压缩期间编码细粒度的时间和空间信息本质上是困难的。为了解决这些问题，我们提出了LLaVA-ST，这是一种用于细粒度时空多模态理解的MLLM。在LLaVA-ST中，我们提出了语言对齐位置嵌入，它将文本坐标特殊标记嵌入到视觉空间中，简化了细粒度时空对应的对齐。此外，我们设计了时空打包器，它将时间和空间分辨率的特征压缩解耦为两个独立的点对区域注意力处理流。此外，我们提出了包含430万个训练样本的ST-Align数据集，用于细粒度时空多模态理解。通过ST-align，我们提出了一个渐进式训练流程，通过序列的粗到细阶段对齐视觉和文本特征。此外，我们引入了一个ST-Align基准，用于评估时空交织的细粒度理解任务，包括时空视频定位（STVG）、事件定位和描述（ELC）以及空间视频定位（SVG）。LLaVA-ST在需要细粒度时间、空间或时空交织多模态理解的11个基准上取得了出色的性能。我们的代码、数据和基准将发布在https://github.com/appletea233/LLaVA-ST。**|
|**2025-01-14**|**Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing**|Pulkit Arora et.al.|[2501.08276](http://arxiv.org/abs/2501.08276)|null|大型语言模型（LLMs）在各种自然语言处理任务中表现出令人印象深刻的性能。然而，人们对其在不同语言变体领域的可靠性表示担忧。许多研究提出了针对局部对抗攻击的鲁棒性评估措施，但我们需要不受不同语言风格偏见影响的全球鲁棒模型。我们采取更广泛的方法，探索跨社会人口统计维度的更广泛变化，以对语言模型的推理能力进行结构化可靠性测试。我们将SocialIQA数据集扩展，创建基于社会人口统计风格的多样化释义集。评估旨在更深入地了解LLMs在以下方面的能力：（a）使用工程提示生成人口统计释义的能力；（b）在现实世界、复杂语言场景中的推理能力。我们还探索了诸如困惑度、可解释性和释义的ATOMIC性能等指标，以对这些集合进行细粒度的LLMs可靠性分析。我们发现，针对特定人群的释义对语言模型的性能有显著影响，这表明语言变化的微妙之处仍然是一个重大挑战。代码和数据集将公开发布，以供重复研究和未来研究使用。|
|**2025-01-14**|**Addressing the sustainable AI trilemma: a case study on LLM agents and RAG**|Hui Wu et.al.|[2501.08262](http://arxiv.org/abs/2501.08262)|**[link](https://github.com/huiwxing/llmagent_trilemma)**|大型语言模型（LLMs）展示了显著的能力，但它们的广泛应用和更高级的应用提出了关键的可持续性挑战，尤其是在推理能耗方面。我们提出了可持续AI三元悖论的概念，突出了AI能力、数字公平和环境可持续性之间的紧张关系。通过系统地研究LLM代理和检索增强生成（RAG），我们分析了嵌入在内存模块设计中的能源成本，并引入了新的指标来量化能源消耗与系统性能之间的权衡。我们的实验结果表明，当前的内存增强框架存在显著的能源效率低下，并证明了资源受限环境面临着不成比例的效率惩罚。我们的发现挑战了目前以LLM为中心的代理设计范式，并为开发更可持续的AI系统提供了实用的见解。|
|**2025-01-14**|**Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models**|Yifu Qiu et.al.|[2501.08248](http://arxiv.org/abs/2501.08248)|null|近期长上下文语言模型（LCLMs）的进步有望通过简化流程来改变检索增强生成（RAG）。凭借其扩展的上下文窗口，LCLMs可以处理整个知识库并直接进行检索和推理——我们将这种能力定义为上下文检索与推理（ICR^2）。然而，现有的基准如LOFT常常通过提供过于简化的上下文来高估LCLM的性能。为了解决这个问题，我们引入了ICR^2，这是一个通过包含使用强大检索器检索的混杂段落来评估LCLM在更现实场景中的基准。随后，我们提出了三种提高LCLM性能的方法：（1）检索后生成微调，（2）检索注意力探查，该方法使用注意力头来在解码过程中过滤和去噪长上下文，（3）与生成头联合训练检索头。我们在LOFT和ICR^2上对五种知名的LCLM进行评估，结果表明，应用我们的最佳方法到Mistral-7B上，与标准RAG和监督微调相比，在LOFT上精确匹配分数提高了17和15分，在ICR^2上分别提高了13和2分。即便是一个规模较小的模型，它在大多数任务上甚至超越了GPT-4-Turbo。|
|**2025-01-13**|**Imagine while Reasoning in Space: Multimodal Visualization-of-Thought**|Chengzu Li et.al.|[2501.07542](http://arxiv.org/abs/2501.07542)|null|思维链（CoT）提示已被证明在增强大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的复杂推理中非常有效。然而，它在复杂的空间推理任务中遇到了困难。尽管如此，人类的认知超越了语言本身，使其能够以文字和图像两种方式思考。受此启发，我们提出了一种新的推理范式，即多模态可视化思维（MVoT）。它通过生成推理痕迹的图像可视化，使MLLMs能够进行视觉思考。为确保高质量的可视化，我们引入了标记差异损失到自回归MLLMs中。这一创新显著提高了视觉的连贯性和准确性。我们通过几个动态空间推理任务验证了这种方法。实验结果表明，MVoT在各项任务中表现出竞争性的性能。此外，它在CoT失败的最具挑战性场景中显示出稳健和可靠的改进。最终，MVoT为复杂推理任务开辟了新的可能性，在这些任务中，视觉思维可以有效地补充语言推理。|
|**2025-01-13**|**ML Mule: Mobile-Driven Context-Aware Collaborative Learning**|Haoxiang Yu et.al.|[2501.07536](http://arxiv.org/abs/2501.07536)|null|人工智能已经融入了日常生活的几乎每个方面，从计算机视觉中的物体检测到用于撰写电子邮件的大型语言模型，再到智能家居中的紧凑型模型。这些机器学习模型针对个人用户，但通常与他们分离，因为它们通常存储和处理在集中的数据中心。这种集中式方法引发了隐私担忧，产生了高昂的基础设施成本，并且难以实现个性化。为了解决这些问题，我们提出了ML Mule，一种利用个人移动设备作为“驮马”的方法，在它们穿越物理空间时训练和传输模型快照，并将这些模型与它们所占据的物理“空间”共享。这种方法在设备之间隐式形成关联群体，这些设备与共享特定空间的用户相关联，从而实现协作模型进化，并保护用户隐私。我们的方法解决了传统、联邦和完全去中心化学习系统的一些主要不足。所提出的框架代表了一类新的机器学习方法，它们更稳健、分布式和个性化，使该领域更接近实现智能、自适应和真正情境感知的智能环境原始愿景。结果表明，与现有方法相比，ML Mule收敛速度更快，模型精度更高。|
|**2025-01-13**|**Investigating Large Language Models in Inferring Personality Traits from User Conversations**|Jianfeng Zhu et.al.|[2501.07532](http://arxiv.org/abs/2501.07532)|null|大型语言模型（LLMs）在多个领域展现出了与人类相似的能力，包括心理评估。本研究评估了LLMs，特别是GPT-4o和GPT-4o mini，在零样本提示条件下是否能够推断五大人格特质并生成五大人格问卷-10（BFI-10）项目得分。我们的发现表明，在计算特质之前引入一个中间步骤——提示BFI-10项目得分——可以提高准确性，并且比直接推断特质更接近黄金标准。这种结构化方法强调了利用心理框架提高预测精度的意义。此外，基于抑郁症状存在的群体比较揭示了模型性能的差异。参与者被分为两组：至少有一种抑郁症状的参与者和无症状的参与者。GPT-4o mini在症状存在组中对抑郁相关特质（如神经质和尽责性）的变化表现出更高的敏感性，而GPT-4o在跨组的细微解释上展现出优势。这些发现强调了LLMs有效分析现实世界心理数据的潜力，为人工智能与心理学交叉领域的研究提供了宝贵的基石。|
|**2025-01-13**|**RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**|Difei Gu et.al.|[2501.07525](http://arxiv.org/abs/2501.07525)|**[link](https://github.com/difeigu/radalign)**|**自动胸部X光片解读需要准确的地病分类和详细的放射学报告生成，这在临床工作中是一个重大挑战。当前的方法要么侧重于分类精度而牺牲可解释性，要么通过图像字幕技术生成详细但可能不可靠的报告。在这项研究中，我们提出了RadAlign，一个结合了视觉语言模型（VLM）的预测精度和大型语言模型（LLM）推理能力的创新框架。受放射科医生工作流程的启发，RadAlign首先使用一个专门的VLM将视觉特征与关键医学概念对齐，实现了平均AUC为0.885的优越疾病分类。这些识别出的医学状况，以对齐的视觉-语言空间中的基于文本的概念表示，然后被用于基于LLM的报告生成。通过检索增强的生成机制，将输出基于类似的历史案例，RadAlign提供了具有0.678的GREEN评分的优越报告质量，优于最先进方法的0.634。我们的框架在保持强大的临床可解释性的同时减少了幻觉，通过集成的预测和生成AI推进了自动医学影像和报告分析。代码可在https://github.com/difeigu/RadAlign获取。**|
|**2025-01-13**|**Parallel Key-Value Cache Fusion for Position Invariant RAG**|Philhoon Oh et.al.|[2501.07523](http://arxiv.org/abs/2501.07523)|null|近期大型语言模型（LLMs）的进展凸显了检索增强生成（RAG）利用外部信息的重要性。然而，LLMs对相关信息在上下文中的位置敏感，当此类信息置于中间位置时，容易产生错误的响应，这种现象被称为“迷失在中间”现象。在本文中，我们提出一个框架，该框架为仅解码器模型生成一致的输出，无论输入上下文顺序如何。三个开放域问答任务的实验结果表明，该模型对输入上下文顺序不敏感，即位置不变性，并且与现有的RAG管道方法相比，对无关段落具有更强的鲁棒性。|
|**2025-01-13**|**Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards**|Yangsibo Huang et.al.|[2501.07493](http://arxiv.org/abs/2501.07493)|null|现在，人们通常通过让人类手动投票来评估大型语言模型（LLMs）的输出，这与评估特定任务中的知识或技能的典型基准不同。聊天机器人竞技场是这类基准中最受欢迎的，它通过让用户在两个随机选择的模型之间选择更好的回应来对模型进行排名（而不透露哪个模型负责生成）。这些平台被广泛信赖为衡量LLMs能力公平且准确的手段。在本文中，我们表明，如果没有实施机器人保护和其它防御措施，基于投票的基准可能容易受到对抗性操纵。具体来说，我们展示了攻击者可以通过花费大约一千票（在模拟的、离线的聊天机器人竞技场版本中验证）来改变排行榜（以推广他们喜欢的模型或降低竞争对手的排名）。我们的攻击分为两个步骤：首先，我们展示了攻击者如何以超过95%的准确率确定用于生成特定回复的模型；然后，攻击者可以利用这些信息持续地对目标模型进行投票（或反对）。与聊天机器人竞技场开发者合作，我们识别、提出并实施了缓解措施，以增强聊天机器人竞技场对对抗性操纵的鲁棒性，根据我们的分析，这大幅增加了此类攻击的成本。其中一些防御措施在我们合作之前就已经存在，例如使用Cloudflare的机器人保护、恶意用户检测和速率限制。其他一些措施，包括reCAPTCHA和登录，正在被集成到聊天机器人竞技场中以加强其安全性。|
|**2025-01-13**|**TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language Models**|Thales Sales Almeida et.al.|[2501.07482](http://arxiv.org/abs/2501.07482)|**[link](https://github.com/timelyeventsbenchmark/tiebe)**|在快速发展的知识景观和大型语言模型的日益普及的背景下，出现了一种需求，即持续更新这些模型以反映当前事件。虽然现有的基准评估了普遍的事实回忆能力，但它们通常忽略了两个关键方面：模型通过持续学习整合演变知识的能力以及它们在性能上的显著区域差异。为了解决这些差距，我们引入了及时事件基准（TiEBe），这是一个包含超过11,000个问题-答案对的数据库，专注于全球和区域重要事件。TiEBe利用来自维基百科的结构化回溯数据，能够进行持续更新，以评估大型语言模型对演变全球事务的知识以及它们对不同地区事件的理解。我们的基准表明，大型语言模型在事实回忆方面表现出显著的地理差异，强调了需要更加平衡的全球知识表示。此外，TiEBe作为评估持续学习策略的工具，提供了关于模型获取新信息而不忘记过去知识的能力的见解。|
|**2025-01-13**|**A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**|Yihao Liu et.al.|[2501.07468](http://arxiv.org/abs/2501.07468)|null|全球医疗体系在效率、可及性和个性化方面面临着持续的挑战。依托现代人工智能技术，如多模态大型语言模型和世界模型，具身人工智能（EmAI）代表了一个变革性的前沿领域，它提供了增强自主性和与物理世界互动的能力，以应对这些挑战。作为一个跨学科且快速发展的研究领域，“医疗领域的EmAI”涵盖了算法、机器人技术和生物医学等多个领域。这种复杂性强调了及时回顾和分析的重要性，以跟踪进展、解决挑战并促进跨学科合作。在本文中，我们提供了EmAI在医疗领域“大脑”的全面概述，其中我们介绍了感知、行动、规划和记忆的基础AI算法，并重点介绍了跨越临床干预、日常护理与陪伴、基础设施支持和生物医学研究等医疗应用。尽管具有巨大潜力，但EmAI在医疗领域的开发受到诸如安全担忧、模拟平台与实际应用之间的差距、缺乏标准化基准以及在跨学科领域进展不均等关键挑战的阻碍。我们讨论了技术障碍并探讨了伦理考量，提出了对未来EmAI在医疗领域发展的前瞻性观点。还介绍了一个EmAI系统智能层次结构的框架，以指导进一步的发展。通过提供系统性的见解，本研究旨在激发创新和实际应用，为智能、以患者为中心的医疗新时代铺平道路。|
|**2025-01-13**|**Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is Not AGI**|Rolf Pfister et.al.|[2501.07458](http://arxiv.org/abs/2501.07458)|null|基于对ARC-AGI的区分，即由ARC-AGI的创造者François Chollet提出的技能与智能的区别，引入了一种对智能的新理解：一个智能体越能以更少的知识在更多样化的世界中更有效地实现更多样化的目标，那么它就越智能。对ARC-AGI基准的分析显示，其任务代表了一种非常特定的问题类型，这种问题可以通过大量尝试预定义操作的组合来解决。这种方法也被o3所应用，通过广泛使用计算能力实现了高分。然而，对于物理世界和人类领域中的大多数问题，解决方案无法预先测试，且预定义的操作不可用。因此，像o3那样进行大量预定义操作的尝试不能作为AGI的基础——相反，需要新的方法，这些方法可以可靠地解决广泛的各种问题，而无需现有技能。为了支持这种发展，概述了一个新的智能基准，它涵盖了要解决的未知任务的更高多样性，从而能够全面评估智能以及向通用人工智能（AGI）的进展。|
|**2025-01-13**|**Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection**|Xin Yin et.al.|[2501.07425](http://arxiv.org/abs/2501.07425)|null|尽管许多基于学习的单元测试生成方法已经提出并取得了显著性能，但它们仍然依赖于特定任务的数据库，存在局限性。最近，由于能够处理包括单元测试生成在内的广泛任务，由提示工程引导的大型语言模型（LLMs）受到了关注。尽管它们取得了成功，但LLMs在为焦点方法或函数生成单元测试时可能会出现幻觉，因为它们缺乏对项目全局上下文的了解。这些幻觉可能表现为调用不存在的方法，以及错误的参数或返回值，例如参数类型或数量不匹配。尽管许多研究已经探讨了上下文的作用，但它们通常为不同的模型和焦点方法提取固定的上下文模式，这可能不适合所有生成过程（例如，过多的无关上下文可能导致冗余，阻止模型关注关键信息）。为了克服这一局限性，我们提出了RATester，它通过注入全局上下文信息来增强LLM生成更具有仓库意识的单元测试的能力。为了使LLMs具备类似于人类测试者的全局知识，我们集成了语言服务器gopls，它提供必要的功能（例如，查找定义）以协助LLM。当RATester遇到不熟悉的标识符（例如，不熟悉的结构体名称）时，它首先利用gopls获取相关定义和文档注释，然后使用这些全局知识来引导LLM。通过利用gopls，RATester丰富了LLM对项目全局上下文的知识，从而在单元测试生成过程中减少了幻觉。|
|**2025-01-10**|**LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs**|Omkar Thawakar et.al.|[2501.06186](http://arxiv.org/abs/2501.06186)|**[link](https://github.com/mbzuai-oryx/llamav-o1)**|**推理是解决复杂多步问题的关键能力，尤其是在视觉环境中，顺序逐步理解至关重要。现有方法缺乏评估视觉推理的全面框架，并且不强调逐步解决问题。为此，我们通过三个关键贡献提出一个全面框架，以推进大型语言模型（LMMs）中的逐步视觉推理。首先，我们引入一个专门设计的视觉推理基准，用于评估多步推理任务。该基准提供了从复杂视觉感知到科学推理的八个不同类别，总共有超过4k个推理步骤，能够对LMMs在多个步骤中执行准确和可解释的视觉推理能力进行稳健评估。其次，我们提出一个新颖的指标，用于评估单个步骤的视觉推理质量，强调正确性和逻辑一致性。与传统的最终任务准确率指标相比，该指标对推理性能的洞察更深。第三，我们提出一个新的多模态视觉推理模型，名为LlamaV-o1，采用多步课程学习方法进行训练，任务逐步组织以促进增量技能获取和问题解决。所提出的LlamaV-o1旨在进行多步推理，并通过结构化训练范式逐步学习。大量实验表明，我们的LlamaV-o1优于现有的开源模型，并在与闭源专有模型相比时表现良好。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中平均得分达到67.3，绝对增益为3.8%，同时在推理扩展时快5倍。我们的基准、模型和代码均公开可用。**|
|**2025-01-10**|**PEACE: Empowering Geologic Map Holistic Understanding with MLLMs**|Yangyu Huang et.al.|[2501.06184](http://arxiv.org/abs/2501.06184)|null|地质图作为地质科学的基本图表，为地球地下和地表的结构和组成提供了关键见解。这些地图在各种领域，包括灾害检测、资源勘探和土木工程中，都是不可或缺的。尽管它们的重要性不言而喻，但当前的多元模态大型语言模型（MLLMs）在地质图理解方面往往存在不足。这种差距主要源于地图概括化的挑战性，这包括处理高分辨率地图、管理多个相关组件以及需要专业知识。为了量化这一差距，我们构建了GeoMap-Bench，这是首个用于评估MLLMs在地质图理解方面的基准，它评估了提取、引用、定位、推理和分析的全面能力。为了弥合这一差距，我们引入了GeoMap-Agent，这是首个专为地质图理解设计的智能体，它具有三个模块：分层信息提取（HIE）、领域知识注入（DKI）和提示增强问答（PEQA）。受人类科学家跨学科合作启发，一个AI专家小组充当顾问，利用多样化的工具库全面分析问题。通过全面实验，GeoMap-Agent在GeoMap-Bench上的总得分达到0.811，显著优于GPT-4o的0.369。我们的工作，通过MLLMs增强地质图全面理解（PEACE），为地质领域高级AI应用铺平了道路，提高了地质调查的效率和准确性。|
|**2025-01-10**|**Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories**|Gerd Kortemeyer et.al.|[2501.06143](http://arxiv.org/abs/2501.06143)|null|我们研究了一个基于大型语言模型的AI系统GPT-4o在多个语言和学科领域广泛分布的物理概念题库上的多语言和多模态性能。我们从PhysPort网站获取的题库涵盖了经典物理学主题，如力学、电磁学、光学和热力学，以及相对论、量子力学、天文学、数学和实验技能。与之前仅限于文本的研究不同，我们将题库上传为图片，以模拟学生在纸上看到的情景，评估系统的多模态功能。AI系统用英语提问，并自主选择其回答的语言——要么保持测试的官方语言，要么完全切换到英语，或者混合使用语言——这揭示了其适应语言复杂性和数据可用性的行为。我们的结果表明，各学科领域的性能存在一些差异，其中实验技能领域表现最差。此外，AI在需要视觉解释图像的问题上的表现不如纯文本问题。AI难以回答的问题往往总是与题库语言有关。我们还发现，不同语言之间的性能存在较大差异，一些语言似乎从语言切换中受益很大，这种现象类似于人类说话者的代码切换。总体而言，将所获得的AI结果与现有文献进行比较，我们发现该AI系统在所有学科领域（但实验技能除外）的测试后均优于普通本科生。|
|**2025-01-10**|**Supervision policies can shape long-term risk management in general-purpose AI models**|Manuel Cebrian et.al.|[2501.06137](http://arxiv.org/abs/2501.06137)|**[link](https://github.com/manuelcebrianramos/llm_supervision_policies)**|**随着通用人工智能（GPAI）模型，包括大型语言模型（LLMs）的快速扩散和部署，为人工智能监管机构带来了前所未有的挑战。我们假设这些机构将需要在一个新兴的风险和事件报告生态系统中进行导航，其规模可能超出它们的监管能力。为了研究这一问题，我们开发了一个模拟框架，该框架由从风险、事件或危害报告生态系统（包括社区驱动的平台、众包倡议和专家评估）中提取的特征进行参数化。我们评估了四种监管政策：非优先级（先到先得）、随机选择、基于优先级（首先解决最高优先级的风险）和多样性优先级（平衡高风险与全面覆盖风险类型）。我们的结果表明，虽然基于优先级和多样性优先级的政策在减轻高风险方面更为有效，尤其是那些由专家识别出的风险，但它们可能会无意中忽视更广泛社区报告的系统性问题。这种疏忽可能导致某些类型报告的反馈循环被放大，同时抑制其他类型的报告，从而扭曲整体风险景观的感知。我们通过几个现实世界的数据集验证了我们的模拟结果，包括一个包含超过一百万次ChatGPT交互的数据集，其中超过15万个对话被识别为有风险。这种验证强调了人工智能风险监管中固有的复杂权衡，并突出了风险管理政策选择如何塑造社会中使用各种GPAI模型的人工智能风险未来景观。**|
|**2025-01-10**|**Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational AI**|Yuya Asano et.al.|[2501.06129](http://arxiv.org/abs/2501.06129)|null|通用型自动语音识别（ASR）系统在目标导向对话中并不总是表现良好。现有的ASR纠错方法依赖于先前用户数据或命名实体。我们将纠错扩展到没有先前用户数据且具有语言灵活性（如词汇和句法变化）的任务。我们提出了一种新颖的上下文增强方法，结合大型语言模型和一种排名策略，该策略结合了目标导向对话AI及其任务的对话状态中的上下文信息。我们的方法通过以下两方面进行排名：（1）根据与上下文的词汇和语义相似性对n-best ASR假设进行排名；（2）根据与ASR假设的音位对应关系对上下文进行排名。该方法在家居改善和烹饪领域与真实用户进行了评估，与纠错方法相比，我们的方法将召回率和F1值分别提高了34%和16%，同时保持了精确率和误报率。当我们的纠错方法正确工作时，用户对其的评价从0.8到1分（满分5分）提高，没有因为误报而降低。|
|**2025-01-10**|**Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding**|Fabian David Schmidt et.al.|[2501.06117](http://arxiv.org/abs/2501.06117)|**[link](https://github.com/fdschmidt93/fleurs-slu)**|尽管最近的多语言自动语音识别模型声称支持数千种语言，但由于双模态语音和文本训练数据有限，低资源语言的语音识别仍然非常不可靠。更好的多语言口语理解（SLU）可以通过利用语言语义来补偿稀缺的训练数据，如通过上下文消除歧义或利用跨语言的语义相似性，从而大大增强多语言语音识别的鲁棒性。更重要的是，SLU对于大约一半没有正式书写系统的活语言的无障碍语音技术来说是必不可少的。然而，多语言SLU的评估仍然局限于像意图分类或语言识别这样的浅层任务。为了解决这个问题，我们提出了Fleurs-SLU，这是一个多语言SLU基准，包括102种语言的专题语音分类和92种语言的听力理解选择题回答。我们在Fleurs-SLU上广泛评估了端到端的语音分类模型以及将语音转文本转录与后续的大语言模型分类相结合的级联系统。我们的结果表明，级联系统在多语言SLU任务中表现出更大的鲁棒性，尽管当适当预训练时，语音编码器在专题语音分类中可以实现有竞争力的性能。我们还发现，鲁棒的多语言语音识别、有效的语音转文本翻译和强大的多语言SLU之间存在强烈的关联，突显了声学和语义语音表示之间的相互益处。|
|**2025-01-10**|**From Conversation to Automation: Leveraging Large Language Models to Analyze Strategies in Problem Solving Therapy**|Elham Aghakhani et.al.|[2501.06101](http://arxiv.org/abs/2501.06101)|null|问题解决疗法（PST）是一种结构化的心理治疗方法，通过引导个体识别问题、进行解决方案头脑风暴、决策和结果评估来帮助他们管理压力和解决个人问题。随着心理健康护理越来越多地整合如聊天机器人和大语言模型（LLM）等科技，了解如何有效地自动化PST变得至关重要。本研究利用匿名化治疗记录，利用各种LLM和基于Transformer的模型分析和分类治疗干预措施。我们的结果显示，GPT-4o在识别PST策略方面取得了最高的准确率（0.76），超过了其他模型。此外，我们引入了一种新的沟通策略维度，增强了当前的PST框架，为治疗师与患者互动提供了更深入的见解。这项研究展示了LLM自动化复杂治疗对话分析的可能性，为心理健康干预提供了可扩展、高效的工具。我们的标注框架可以增强PST的易用性、有效性和个性化，为治疗师提供实时支持，以更精确、有针对性的干预措施。|
|**2025-01-10**|**Addressing speaker gender bias in large scale speech translation systems**|Shubham Bansal et.al.|[2501.05989](http://arxiv.org/abs/2501.05989)|null|本项研究针对语音翻译（ST）系统中的说话人性别偏见问题，这一问题可能导致冒犯和不准确的翻译。在大规模ST系统中常见的男性偏见通常是通过从机器翻译（MT）系统获得的数据进行训练而持续存在的。我们的方法包括两个关键步骤。首先，我们采用大型语言模型（LLMs）以经济高效的方式根据说话人的性别更正翻译。其次，我们使用更正后的数据微调ST模型，使模型能够直接从音频线索生成针对特定性别的翻译，无需明确的性别输入。此外，我们还提出了一种适用于说话人性别预先定义或不应从语音线索中推断的三个模式的微调模型。在MuST-SHE测试集上，与基线以及其他大规模ST系统（如Seamless M4T和Canary）相比，我们的方法在女性说话人的翻译上提高了70%。|
|**2025-01-10**|**Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys**|Divya Mani Adhikari et.al.|[2501.05985](http://arxiv.org/abs/2501.05985)|null|设计适合ICTD研究的文化相关问卷具有挑战性，尤其是在将调查适应非西方背景的人群时。先前的工作通过专家评审和试点研究来调整问卷，这些方法既资源密集又耗时。为了解决这些挑战，我们提出使用大型语言模型（LLMs）来自动化跨文化环境下的问卷预测试过程。我们的研究使用LLMs将一个以美国为中心的气候意见调查调整为针对南非受众。然后，我们通过Prolific对116名南非参与者进行了测试，让他们对两个版本都提供反馈。参与者认为LLM调整的问题略优于传统版本。我们的研究开启了关于LLMs在调整调查和促进跨文化问卷设计中的潜在作用的讨论。|
|**2025-01-10**|**Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study of LLM Hallucination on North Korea**|Eunjung Cho et.al.|[2501.05981](http://arxiv.org/abs/2501.05981)|null|在大型语言模型（LLMs）中出现的幻觉问题仍然是对其安全部署的一个重大挑战，尤其是由于其可能导致虚假信息的传播。大多数现有的解决方案通过聚焦于使模型与可信来源对齐或改进模型在输出中表达自信（或缺乏自信）的方式来解决这一挑战。尽管这些措施在大多数情况下可能有效，但在需要更细致方法的场景中可能不足，尤其是在获取准确数据有限或确定可信来源具有挑战性的情况下。在本研究中，我们将朝鲜——一个以极端缺乏可靠来源和耸人听闻的虚假信息普遍存在的国家——作为一个案例研究。我们探索和评估了表现最佳的一些多语言LLMs和特定语言模型在三个具有重大地缘政治利益的国家使用的语言中生成关于朝鲜的信息：英语（美国、英国）、韩语（韩国）和普通话（中国）。我们的发现揭示了显著的差异，表明模型和语言的选择可能导致对朝鲜的截然不同的理解，这对于该国对全球安全构成的挑战具有重要意义。|
|**2025-01-09**|**ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding**|Xingyu Fu et.al.|[2501.05452](http://arxiv.org/abs/2501.05452)|null|结构化图像理解，如解释表格和图表，需要在对图像中各种结构和文本进行战略性地重新聚焦，形成一个推理序列以得出最终答案。然而，当前的多模态大型语言模型（LLMs）缺乏这种多跳选择性注意能力。在这项工作中，我们引入了ReFocus，这是一个简单而有效的框架，它通过代码对输入图像进行视觉编辑，使多模态LLMs能够通过生成“视觉思维”来调整和细化他们的视觉焦点。具体来说，ReFocus使多模态LLMs能够生成Python代码来调用工具并修改输入图像，依次绘制框、突出显示部分和遮蔽区域，从而增强视觉推理过程。我们在涉及表格和图表的广泛结构化图像理解任务上进行了实验。ReFocus在所有任务上相对于没有视觉编辑的GPT-4o大幅提升了性能，在表格任务上平均提高了11.0%，在图表任务上提高了6.8%。我们深入分析了不同视觉编辑效果，以及ReFocus在不引入额外信息的情况下提高性能的原因。此外，我们使用ReFocus收集了一个14k的训练集，并证明这种带有中间信息的视觉思维链提供了比标准VQA数据更好的监督，相对于使用QA对训练的同一模型平均提高了8.0%，相对于CoT提高了2.6%。|
|**2025-01-09**|**Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark**|Yunzhuo Hao et.al.|[2501.05444](http://arxiv.org/abs/2501.05444)|**[link](https://github.com/hychaochao/EMMA)**|人类智能的基石之一是能够在文本和图像之间进行有机推理，然而，多模态大型语言模型（MLLMs）在执行此类多模态推理方面的能力仍被低估。现有的基准测试往往强调以文本为主的推理或依赖于浅层视觉线索，未能充分评估综合视觉和文本推理。我们引入了EMMA（增强多模态推理），这是一个针对数学、物理、化学和编码等领域有机多模态推理的基准测试。EMMA任务要求进行高级跨模态推理，这种推理不能通过独立于每个模态的推理来解决，为MLLMs的推理能力提供了一个增强的测试套件。我们对最先进的MLLMs在EMMA上的评估显示，在处理复杂的多模态和多步骤推理任务时存在重大局限性，即使是在采用思维链提示和测试时计算扩展等高级技术的情况下表现不佳。这些发现强调了改进多模态架构和训练方法的需求，以缩小人类推理与模型在多模态推理能力之间的差距。|
|**2025-01-09**|**A survey of textual cyber abuse detection using cutting-edge language models and large language models**|Jose A. Diaz-Garcia et.al.|[2501.05443](http://arxiv.org/abs/2501.05443)|null|社交媒体平台的成功促进了数字社区中各种形式的在线滥用出现。这种滥用以多种方式体现，包括仇恨言论、网络欺凌、情感虐待、性侵和色情信息。在本文中，我们全面分析了社交媒体中普遍存在的滥用形式，特别关注新兴技术，如语言模型（LMs）和大型语言模型（LLMs），如何重塑这些网络中滥用内容的检测和生成。我们深入探讨了社交媒体滥用得以持续传播的机制，并探讨了其心理和社会影响。此外，我们考察了高级语言模型的二元角色——一方面强调其增强自动化检测系统以识别滥用行为的潜力，另一方面也承认其生成有害内容的能力。本文旨在为关于在线安全和伦理的持续讨论做出贡献，提供对网络滥用不断演变的格局以及减轻和加剧这一问题的技术创新的见解。|
|**2025-01-09**|**Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese Micro-bloggers**|Jerry Chongyi Hu et.al.|[2501.05423](http://arxiv.org/abs/2501.05423)|null|在危机期间研究公众情绪对于理解观点和情感如何转变、导致社会两极分化至关重要。我们以微博——中国最受欢迎的微博客网站为例，研究在COVID-19疫情爆发期间发布的帖子。研究时段包括COVID-19疫情前阶段、爆发阶段和疫情预防早期阶段。我们使用Llama 3 8B这个大型语言模型，通过将用户情感分类为积极、消极、讽刺和中立等类别来分析平台上的用户情感。分析微博上的情感转变有助于了解社会事件和政府行动如何影响公众舆论。本研究有助于理解健康危机期间社会情感的变化动态，填补了针对中国平台情感分析的空白。通过考察这些动态，我们旨在提供有价值的观点，探讨数字通信在塑造社会应对前所未有的全球挑战中所起的作用。|
|**2025-01-09**|**FairCode: Evaluating Social Bias of LLMs in Code Generation**|Yongkang Du et.al.|[2501.05396](http://arxiv.org/abs/2501.05396)|**[link](https://github.com/yongkdu/faircode)**|**大型语言模型（LLMs）在代码生成方面展现出显著的能力，引起了对其输出质量和安全性的评估越来越多的关注。然而，关于代码生成中偏差的研究仍然有限。现有研究通常通过应用恶意提示或重新应用任务和数据集来评估歧视性模型的偏差。鉴于LLMs通常与人类价值观相一致，且先前数据集并未完全优化用于代码相关任务，迫切需要专门为评估代码模型设计的基准。在本研究中，我们引入了FairCode，这是一种用于评估代码生成偏差的新颖基准。FairCode包含两个任务：函数实现和测试用例生成，每个任务通过不同的场景评估社会偏差。此外，我们提出了一种新的指标，FairScore，用于评估模型在此基准上的性能。我们在广泛使用的LLMs上进行了实验，并对结果进行了全面分析。研究发现，所有测试的LLMs都存在偏差。代码可在https://github.com/YongkDu/FairCode上找到。**|
|**2025-01-09**|**Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models**|Kristian G. Barman et.al.|[2501.05382](http://arxiv.org/abs/2501.05382)|null|本文探讨了发展物理特定的大型人工智能模型（我们称之为大型物理模型，LPMs）的想法，并为其提供了一条可能的路线图。这些模型基于如大型语言模型（LLMs）这样的基础模型——在广泛的数据上训练而成——针对物理学研究的需求进行了定制。LPMs可以独立运行或作为集成框架的一部分。该框架可以纳入专业工具，包括用于数学操作的符号推理模块、分析特定实验和模拟数据的框架，以及合成理论和科学文献的机制。我们首先探讨物理界是否应该积极开发和完善专用模型，而不是仅仅依赖商业LLMs。然后，我们概述了LPMs如何通过物理学、计算机科学和科学哲学专家之间的跨学科合作来实现。为了有效地整合这些模型，我们确定了三个关键支柱：开发、评估和哲学反思。开发侧重于构建能够处理物理文本、数学表述和多种物理数据的模型。评估通过测试和基准测试来评估准确性和可靠性。最后，哲学反思包括分析LLMs在物理学中的更广泛影响，包括其产生新的科学理解和可能出现的新的研究合作动态。受粒子物理学实验合作组织结构的启发，我们提出了一个类似的跨学科和协作方法来构建和改进大型物理模型。这条路线图提供了具体目标，定义了实现这些目标的途径，并确定了实现物理特定大规模AI模型必须解决的问题。|
|**2025-01-09**|**Accelerated Diffusion Models via Speculative Sampling**|Valentin De Bortoli et.al.|[2501.05370](http://arxiv.org/abs/2501.05370)|null|推理步骤：  1. 理解摘要内容：摘要介绍了名为“推测采样”的技术，用于加速大型语言模型的推理。该技术使用快速草稿模型生成候选标记，并根据目标模型的分布接受或拒绝这些标记。之前推测采样仅限于离散序列，现在扩展到扩散模型，这些模型通过连续的、向量值马尔可夫链生成样本。  2. 确定翻译重点：翻译时应注意术语的准确性，如“推测采样”、“草稿模型”、“扩散模型”、“马尔可夫链”等。  3. 翻译摘要：  推测采样是一种流行的技术，通过使用快速草稿模型生成候选标记，并根据目标模型的分布接受或拒绝它们来加速大型语言模型的推理。虽然推测采样之前仅限于离散序列，但我们将其扩展到扩散模型，这些模型通过连续的、向量值马尔可夫链生成样本。在这种情况下，目标模型是一个计算成本高但质量高的扩散模型。我们提出了各种草稿策略，包括一种简单而有效的方法，该方法不需要训练草稿模型，并且可以直接应用于任何扩散模型。我们的实验在各种扩散模型上展示了显著的生成速度提升，将函数评估次数减半，同时生成来自目标模型的精确样本。|
|**2025-01-09**|**Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction**|Hantao Lou et.al.|[2501.05336](http://arxiv.org/abs/2501.05336)|**[link](https://github.com/htlou/stream-aligner)**|**随着大型语言模型（LLMs）的快速发展，其在能力上的显著提升也带来了与人类价值观和意图相符合的担忧。当前的对齐策略，包括自适应训练和推理时方法，在这一领域显示出潜力。然而，这些方法仍然难以在各种任务和难度上平衡部署复杂性和能力。在本研究中，我们引入了流式分布诱导对齐器（Stream Aligner），这是一种新颖的对齐范式，它将效率与在整个生成过程中各种任务上的增强性能相结合。Stream Aligner通过使用一个小模型来学习后缀句子的偏好，迭代地纠正上游模型输出的后缀句子，然后使用纠正后的句子替换后续生成中的后缀句子，实现了动态的句子级纠正。与Aligner相比，我们的实验表明，Stream Aligner减少了对外部模型能力的依赖，增强了LLMs的推理能力，并降低了用户交互时的延迟。具体来说，Stream Aligner-2B模型在测试的Llama2-70B-chat模型上实现了76.1%的有用性提升、36.0%的无害性提升，而Stream Aligner-8B在测试的Llama3-70B-Instruct模型上的数学能力提升了3.5%。**|
|**2025-01-09**|**"What's Happening"- A Human-centered Multimodal Interpreter Explaining the Actions of Autonomous Vehicles**|Xuewen Luo et.al.|[2501.05322](http://arxiv.org/abs/2501.05322)|null|随着公众对自动驾驶汽车的信任度下降，研究强调了有必要向乘客解释这些车辆的行为以促进对自主系统的信任。解释者可以通过提高透明度和降低感知风险来增强信任。然而，现有的解决方案往往缺乏以人为本的多模态解释集成方法。本文介绍了一种新颖的人中心多模态解释器（HMI）系统，该系统利用人类偏好提供视觉、文本和听觉反馈。该系统结合了带有鸟瞰图（BEV）、地图和文本显示的视觉界面，以及使用微调的大语言模型（LLM）进行语音交互。我们的用户研究，涉及多样化的参与者，表明HMI系统显著提高了乘客对自动驾驶汽车的信任度，平均信任水平提高了超过8%，普通环境中的信任度甚至上升了高达30%。这些结果突显了HMI系统通过提供清晰、实时且具有情境敏感性的车辆行为解释，提高自动驾驶汽车的可接受性和可靠性的潜力。|
|**2025-01-09**|**CallNavi: A Study and Challenge on Function Calling Routing and Invocation in Large Language Models**|Yewei Song et.al.|[2501.05255](http://arxiv.org/abs/2501.05255)|null|通过聊天机器人与软件系统交互可能会遇到挑战，尤其是当聊天机器人需要按照正确的顺序和参数生成API调用以与系统通信时。在聊天机器人系统中进行API调用面临重大挑战，尤其是在需要准确选择和执行API的复杂、多步骤任务中。我们在这一领域做出以下三方面的贡献：首先，引入一个旨在评估API函数选择、参数生成和嵌套API调用的新数据集；其次，对最先进的语言模型在不同复杂程度下进行基准测试，以评估其在API函数生成和参数准确性方面的性能；第三，提出一种增强的API路由方法，该方法结合了通用的大语言模型进行API选择，以及针对参数生成和某些提示工程方法的微调模型。这些方法在处理复杂API任务方面取得了实质性改进，为现实世界的API驱动聊天机器人系统提供了实用的进步。|
|**2025-01-08**|**Re-ranking the Context for Multimodal Retrieval Augmented Generation**|Matin Mortaheb et.al.|[2501.04695](http://arxiv.org/abs/2501.04695)|null|检索增强生成（RAG）通过结合外部知识来提高大型语言模型（LLMs）在特定语境下生成响应的准确性和减少幻觉。然而，多模态RAG系统面临独特的挑战：（i）检索过程可能会选择与用户查询不相关的条目（例如，图像、文档），以及（ii）在处理这些条目以生成RAG输出时，视觉-语言模型或多模态语言模型如GPT-4o可能会产生幻觉。在本文中，我们旨在解决第一个挑战，即改进多模态RAG检索阶段中从知识库中选择相关语境的能力。具体来说，我们利用我们在之前工作中设计的用于评估RAG性能的相关性得分（RS）度量来选择检索过程中的更多相关条目。基于嵌入的检索，如基于CLIP的嵌入和余弦相似度，通常在多模态数据上表现不佳。我们表明，通过使用更先进的关联度量，可以通过从知识库中选择更多相关片段来增强检索过程，并通过自适应选择最多 $k$ 个条目而不是固定数量的条目来消除语境中的不相关片段。我们使用COCO数据集进行的评估显示了在选择相关语境和生成响应的准确性方面的重要提升。|
|**2025-01-08**|**URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**|Ruilin Luo et.al.|[2501.04686](http://arxiv.org/abs/2501.04686)|**[link](https://github.com/URSA-MATH/URSA-MATH)**|思维链（CoT）推理在大型语言模型（LLMs）的数学推理中得到了广泛应用。最近，在CoT轨迹上引入导数过程监督引发了关于测试时增强扩展能力的讨论，从而提升了这些模型潜力。然而，在多模态数学推理中，高质量CoT训练数据的稀缺阻碍了现有模型实现高精度CoT推理，并限制了测试时的推理潜力。在本工作中，我们提出了一种三模块合成策略，该策略集成了CoT蒸馏、轨迹格式重写和格式统一。它产生了一个高质量的多模态数学CoT推理指令微调数据集，即MMathCoT-1M。我们全面验证了训练的URSA-7B模型在多个多模态数学基准上的最先进（SOTA）性能。为了测试时的扩展，我们引入了一种数据合成策略，自动生成过程注释数据集，称为DualMath-1.1M，专注于解释和逻辑。通过在DualMath-1.1M上进一步训练URSA-7B，我们实现了从CoT推理能力到稳健监督能力的转变。训练后的URSA-RM-7B充当验证器，有效地提升了URSA-7B在测试时的性能。URSA-RM-7B还展示了优秀的分布外（OOD）验证能力，展示了其泛化能力。模型权重、训练数据和代码将开源。|
|**2025-01-08**|**Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations**|Archita Srivastava et.al.|[2501.04675](http://arxiv.org/abs/2501.04675)|null|图表解读对于可视化数据分析至关重要，但准确从图表中提取信息对自动化模型来说是一个重大挑战。本研究调查了DEPLOT模块的微调，该模块将图表或图像转换为线性化表格，在一个包含50,000个条形图的定制数据集上进行了研究。该数据集包括简单、堆叠和分组条形图，旨在针对这些可视化的独特结构特征。微调后的DEPLOT模型使用1,000张图像的测试集和两个指标对其基础版本进行了评估：相对映射相似度（RMS），它衡量分类映射的准确性；以及相对数字集合相似度（RNSS），它评估数值解释的准确性。为了进一步探索大型语言模型（LLMs）的推理能力，我们整理了一个额外的100张条形图图像集，配对问题答案集。我们的发现表明，与直接查询图像相比，在图像旁边提供结构化的中间表格可以显著提高LLMs的推理性能。|
|**2025-01-08**|**Assessing Language Comprehension in Large Language Models Using Construction Grammar**|Wesley Scivetti et.al.|[2501.04661](http://arxiv.org/abs/2501.04661)|null|大型语言模型尽管能力显著，但它们以令人惊讶和不可预测的方式失败。由于它们训练的数据规模庞大，评估其对语言的真正“理解”尤其具有挑战性。因此，我们构建了一个评估体系，利用构造语法（CxG）系统地评估大型语言模型（LLM）的自然语言理解（NLU）。CxG非常适合这个目的，因为它提供了一个理论基础来构建针对性的评估集。这些数据集被精心构建，包括不太可能出现在预训练数据中的例子，同时对于人类来说是直观且易于理解的，这使得评估更加针对性和可靠。我们的实验聚焦于下游自然语言推理和推理任务，通过比较LLM对通过8个独特的构造（Cxns）传达的潜在意义的理解与人类理解之间的差异。结果显示，尽管LLM展示了某些构造信息的知识，即使是包括GPT-o1在内的最新模型，在处理这些构造传达的抽象意义时也遇到了困难，正如测试句子与它们的预训练数据不相似的情况所显示的那样。我们认为，这些情况提供了对真正语言理解更准确的测试，突显了LLM在语义能力方面的关键局限性。我们将我们新颖的数据集以及相关的实验数据（包括提示和模型响应）公开。|
|**2025-01-08**|**Multi-task retriever fine-tuning for domain-specific and efficient RAG**|Patrice Béchard et.al.|[2501.04652](http://arxiv.org/abs/2501.04652)|null|检索增强生成（RAG）在部署大型语言模型（LLMs）时变得无处不在，因为它可以解决诸如生成幻觉或过时信息等典型限制。然而，当构建现实世界的RAG应用时，会出现实际问题。首先，检索到的信息通常是特定领域的。由于微调LLMs在计算上非常昂贵，因此更可行的是微调检索器以提高LLM输入中包含的数据质量。其次，随着更多应用在同一现实世界系统中部署，我们无法负担为每个检索器部署单独的应用。此外，这些RAG应用通常检索不同类型的数据。我们的解决方案是在各种特定领域任务上微调一个小型检索器编码器，使我们能够部署一个可以服务于多种用例的编码器，从而实现低成本、可扩展性和速度。我们展示了该编码器如何泛化到领域外设置，以及在实际企业用例中如何泛化到一个未见过的检索任务。|
|**2025-01-08**|**FlairGPT: Repurposing LLMs for Interior Designs**|Gabrielle Littlefair et.al.|[2501.04648](http://arxiv.org/abs/2501.04648)|null|室内设计涉及精心挑选和布置物品，以创造一个符合客户设计要求的、美观的、功能性强且和谐的空間。这项任务尤其具有挑战性，因为成功的设计不仅要将所有必要的物品以统一风格融合在一起，还要确保它们以最大化可达性的方式排列，同时遵循多种经济性和使用考虑因素。虽然已提出基于数据的方法，但这些方法通常针对特定房间或领域，且在产生最终布局时缺乏可解释性的设计设计考虑。在本文中，我们探讨了大型语言模型（LLMs）是否可以直接用于室内设计。我们发现，LLMs目前还不能生成完整的布局，但它们可以以结构化的方式有效利用，灵感来源于室内设计师的工作流程。通过系统地探测LLMs，我们可以可靠地生成一个物品列表以及指导其放置的相关约束。我们将这些信息转化为设计布局图，然后使用现成的约束优化设置来解决它，以生成最终的布局。我们将我们的算法与现有的基于LLM的方法和人类设计在各种设计配置中进行基准测试，并使用多种定量和定性指标以及用户研究来评估结果。总之，我们证明了当以结构化的方式使用时，LLMs可以有效地生成多样化的高质量布局，使它们成为创建大规模虚拟场景的可行解决方案。项目网页：https://flairgpt.github.io/|
|**2025-01-08**|**Knowledge Retrieval Based on Generative AI**|Te-Lun Yang et.al.|[2501.04635](http://arxiv.org/abs/2501.04635)|null|本研究开发了一个基于检索增强生成（RAG）的问答系统，利用中文维基百科和Lawbank作为检索来源。使用TTQA和TMMLU+作为评估数据集，该系统采用BGE-M3进行密集向量检索以获取高度相关的搜索结果，并使用BGE-reranker根据查询相关性对这些结果进行排序。最相关的检索结果作为大型语言模型（LLM）的参考知识，增强其回答问题和建立基于生成人工智能的知识检索系统的能力。系统通过两阶段评估来评估其有效性：自动和辅助性能评估。自动评估通过比较模型的自动生成标签与真实答案来计算准确率，在无需人类干预的标准化条件下测量性能。辅助性能评估包括20个由20位无金融背景的参与者回答的与金融相关的多项选择题。最初，参与者独立回答。随后，他们收到系统生成的参考信息以帮助回答，检验系统在提供辅助时是否提高了准确性。本研究的贡献主要包括：（1）增强LLM能力：通过集成BGE-M3和BGE-reranker，系统检索并重新排序高度相关的结果，减少幻觉，并动态访问授权或公共知识来源。（2）提高数据隐私：定制的RAG架构使得LLM能够本地运行，无需将私有数据发送到外部服务器。这种方法增强了数据安全性，减少了对商业服务的依赖，降低了运营成本，并缓解了隐私风险。|
|**2025-01-08**|**"Can you be my mum?": Manipulating Social Robots in the Large Language Models Era**|Giulio Antonio Abbo et.al.|[2501.04633](http://arxiv.org/abs/2501.04633)|null|近年来，由大型语言模型驱动的机器人取得了进步，增强了它们的对话能力，使交互更加接近人类对话。然而，这些模型在HRI（人机交互）中引入了安全和安全方面的担忧，因为它们容易受到绕过内置安全措施的操纵。设想一个部署在家庭中的社交机器人，这项工作旨在了解普通用户如何试图利用语言模型来违反道德原则，例如提示机器人表现得像一个生活伴侣。我们进行了一项试点研究，涉及21名大学生与Misty机器人互动，试图在基于特定HRI道德原则的三个场景中绕过其安全机制：依恋、自由和同理心。我们的结果显示，参与者使用了五种技术，包括使用情感语言侮辱和呼吁怜悯。我们希望这项工作能为未来研究提供信息，以设计强大的保障措施，确保道德和安全的人机交互。|
|**2025-01-08**|**Quantum-inspired Embeddings Projection and Similarity Metrics for Representation Learning**|Ivan Kankeu et.al.|[2501.04591](http://arxiv.org/abs/2501.04591)|**[link](https://github.com/ivpb/qiepsm)**|在过去十年中，表示学习作为一种将大量数据中提取的复杂信息嵌入到密集向量空间的关键技术，已经在机器学习中崭露头角。在其他应用中，它已成为大型语言模型和基于对比学习的高级计算机视觉系统的一个关键构建块。表示学习系统的核心组件是投影头，它将原始嵌入映射到不同的、通常是压缩的空间，同时保留向量之间的相似性关系。在本文中，我们提出了一种受量子启发的投影头，包括相应的受量子启发的相似性度量。具体来说，我们将经典嵌入映射到希尔伯特空间中的量子态，并引入一个基于量子电路的投影头来降低嵌入维度。为了评估这种方法的有效性，我们通过集成我们的投影头用于嵌入压缩，扩展了BERT语言模型。我们使用TREC 2019和TREC 2020深度学习基准在信息检索任务中比较了使用我们受量子启发的投影头压缩的嵌入与使用经典投影头压缩的嵌入的性能。结果表明，我们的受量子启发的方 法相对于经典方法实现了具有竞争力的性能，同时参数减少了32倍。此外，当从头开始训练时，它在较小的数据集上尤其表现出色。这项工作不仅突出了受量子启发的有效方法，还强调了在神经网络中利用高效、专用的低纠缠电路模拟作为强大的受量子启发的技术的实用性。|
|**2025-01-08**|**InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection**|Yuhang Liu et.al.|[2501.04575](http://arxiv.org/abs/2501.04575)|**[link](https://github.com/reallm-labs/infiguiagent)**|**图形用户界面（GUI）代理，借助多模态大型语言模型（MLLM），在计算机和手机等计算设备上的任务自动化方面展现出巨大潜力。然而，现有的代理在多步推理和依赖文本注释方面面临挑战，限制了其有效性。我们介绍了\textit{InfiGUIAgent}，这是一种基于MLLM的GUI代理，采用两阶段监督微调管道进行训练。第一阶段提升基本技能，如GUI理解和定位，而第二阶段利用合成数据集成分层推理和预期反思推理技能，以赋予代理原生推理能力。\textit{InfiGUIAgent}在多个GUI基准测试中取得了有竞争力的表现，突出了原生推理技能在增强GUI自动化任务交互方面的作用。相关资源可在\url{https://github.com/Reallm-Labs/InfiGUIAgent}找到。**|
|**2025-01-07**|**Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos**|Haobo Yuan et.al.|[2501.04001](http://arxiv.org/abs/2501.04001)|**[link](https://github.com/magic-research/Sa2VA)**|这项工作介绍了Sa2VA，这是第一个用于密集地理解图像和视频的统一模型。与现有的多模态大型语言模型不同，这些模型通常仅限于特定的模态和任务，Sa2VA支持广泛的图像和视频任务，包括指称分割和对话，且仅需最小的一次性指令调整。Sa2VA将SAM-2（一个基础视频分割模型）与LLaVA（一个高级视觉语言模型）相结合，并将文本、图像和视频统一到一个共享的大型语言模型（LLM）标记空间中。利用LLM，Sa2VA生成指令标记，引导SAM-2生成精确的掩码，从而实现对静态和动态视觉内容的基于情境的多模态理解。此外，我们引入了Ref-SAV，一个包含超过72k个复杂视频场景中物体表情的自动标记数据集，旨在提升模型性能。我们还在Ref-SAV数据集中手动验证了2k个视频物体，以在复杂环境中建立指称视频物体分割的基准。实验表明，Sa2VA在多个任务中达到了最先进的水平，尤其是在指称视频物体分割方面，突显了其在复杂现实世界应用中的潜力。|
|**2025-01-07**|**RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance**|Matin Mortaheb et.al.|[2501.03995](http://arxiv.org/abs/2501.03995)|null|检索增强生成（RAG）通过使用外部知识引导响应生成，从而改善大型语言模型（LLMs），减少幻觉。然而，RAG，尤其是多模态RAG，可能会引入新的幻觉来源：（i）检索过程可能会从数据库中选择无关的片段（例如，文档、图像）作为原始上下文，以及（ii）检索到的图像通过视觉语言模型（VLMs）或直接由多模态语言模型（MLLMs）如GPT-4o处理成文本上下文，这可能导致幻觉。为了解决这个问题，我们提出了一种新的框架来评估多模态RAG的可靠性，使用两个性能指标：（i）相关性分数（RS），评估检索条目与查询的相关性，以及（ii）正确性分数（CS），评估生成响应的准确性。我们使用基于ChatGPT的数据库和人工评估员样本训练RS和CS模型。结果显示，这两个模型在测试数据上均达到约88%的准确率。此外，我们构建了一个包含5000个样本的人工标注数据库，用于评估检索片段的相关性和响应语句的正确性。我们的RS模型在检索中与人类偏好的吻合度比CLIP高出20%，我们的CS模型有约91%的时间与人类偏好相匹配。最后，我们使用RS和CS评估了各种RAG系统的选择和生成性能。|
|**2025-01-07**|**Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles**|Yuxi Xia et.al.|[2501.03991](http://arxiv.org/abs/2501.03991)|null|校准，即模型置信度与预测准确度之间的对齐，对于大型语言模型（LLMs）的可靠部署至关重要。现有工作忽略了测量其方法对不同提示风格和不同尺寸的LLMs泛化的能力。为了解决这个问题，我们定义了一个受控的实验环境，涵盖了12个LLMs和四种提示风格。此外，我们还研究了引入多个LLMs的响应一致性和合适的损失函数是否可以提高校准性能。具体来说，我们构建了Calib-n，一个新颖的框架，该框架训练一个辅助模型进行置信度估计，聚合多个LLMs的响应以捕捉模型间的一致性。为了优化校准，我们将焦点损失和AUC代理损失与二元交叉熵相结合。在四个数据集上的实验表明，响应一致性和焦点损失都提高了基线校准。我们发现，少样本提示对于基于辅助模型的方法最为有效，并且辅助模型在准确度变化中表现出鲁棒的校准性能，优于LLMs的内部概率和口头化的置信度。这些见解加深了对LLMs校准影响因素的理解，支持它们在多种应用中的可靠部署。|
|**2025-01-07**|**(De)-Indexing and the Right to be Forgotten**|Salvatore Vilella et.al.|[2501.03989](http://arxiv.org/abs/2501.03989)|null|在数字时代，健忘的问题已成为一个重要的关注点，特别是在个人数据管理和在线可访问性方面。被遗忘权（RTBF）允许个人要求从公开访问中删除过时或有害的信息，然而实施这一权利对搜索引擎来说存在巨大的技术困难。本文旨在向非专业人士介绍信息检索（IR）和去索引的基础概念，这些概念对于理解搜索引擎如何有效地“忘记”某些内容至关重要。我们将探讨各种信息检索模型，包括布尔逻辑、概率、向量空间和基于嵌入的方法，以及大型语言模型（LLMs）在增强数据处理能力中的作用。通过提供这一概述，我们旨在突出在平衡个人隐私权利与搜索引擎在管理信息可见性方面面临的运营挑战中所涉及的复杂性。|
|**2025-01-07**|**VLM-driven Behavior Tree for Context-aware Task Planning**|Naoki Wake et.al.|[2501.03968](http://arxiv.org/abs/2501.03968)|**[link](https://github.com/microsoft/scene-aware-robot-BT-planner)**|近年来，在机器人领域，使用大型语言模型（LLMs）生成行为树（BTs）引起了人们的关注，但这一领域仍处于发展的早期阶段。在本文中，我们提出了一种新颖的框架，该框架利用视觉-语言模型（VLMs）以交互式方式生成和编辑针对视觉条件的BTs，从而在视觉复杂环境中实现具有上下文意识的机器人操作。我们方法的关键特征在于通过自我提示的视觉条件进行条件控制。具体来说，VLM生成带有视觉条件节点的BTs，其中条件以自由文本形式表达。另一个VLM过程将文本集成到其提示中，并在机器人执行过程中将条件与真实世界图像进行评估。我们在一个现实世界的咖啡馆场景中验证了我们的框架，展示了其可行性和局限性。|
|**2025-01-07**|**Vision Language Models as Values Detectors**|Giulio Antonio Abbo et.al.|[2501.03957](http://arxiv.org/abs/2501.03957)|null|大型语言模型融合文本和视觉输入，为解读复杂数据带来了新的可能性。尽管这些模型在根据视觉刺激生成连贯且与语境相关的文本方面表现出色，但它们在识别图像中相关元素方面与人类感知的对齐仍需进一步探索。本文调查了最先进的LLMs与人类标注者在家庭环境场景中检测相关元素之间的对齐情况。我们创建了一组十二幅描绘各种家庭场景的图像，并召集了十四位标注者来识别每幅图像中的关键元素。然后，我们将这些人类响应与包括GPT-4o和四种LLaVA变体在内的五个不同LLMs的输出进行了比较。我们的发现表明，对齐程度存在差异，其中LLaVA 34B表现最佳但得分仍然较低。然而，对结果的分析突显了模型检测图像中具有价值元素的能力，表明随着训练的改进和提示的精细化，LLMs可以通过提供更深入的见解和更具语境相关性的响应，增强在社交机器人、辅助技术和人机交互中的应用。|
|**2025-01-07**|**Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States**|Jurgita Kapočiūtė-Dzikienė et.al.|[2501.03952](http://arxiv.org/abs/2501.03952)|null|尽管大型语言模型（LLMs）已经改变了我们对现代语言技术的期望，但关于数据隐私的担忧通常限制了在欧盟司法管辖之外托管的可商用LLMs的使用。这限制了它们在政府、国防和其他数据敏感领域的应用。在本研究中，我们评估了本地可部署的开源权重LLMs在支持立陶宛语、拉脱维亚语和爱沙尼亚语等小语种方面的程度。我们检查了表现最好的多语言开源权重模型Llama~3、Gemma~2、Phi和NeMo的多种大小和精度变体，包括机器翻译、多选题回答和自由文本生成。结果表明，尽管某些模型如Gemma~2的表现接近可商用顶级模型，但许多LLMs在这些语言上存在困难。然而，最令人惊讶的是，我们发现这些模型虽然在翻译性能上接近最先进的水平，但对于所有开源多语言LLMs，仍然容易出现词汇幻觉，错误率至少在20个词中就有1个。|
|**2025-01-07**|**Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection**|Pablo Miralles-González et.al.|[2501.03940](http://arxiv.org/abs/2501.03940)|null|大型语言模型（LLMs）的快速发展显著提高了它们生成连贯且与上下文相关文本的能力，引发了对AI生成内容滥用的担忧，并使其检测变得至关重要。然而，这项任务仍然具有挑战性，尤其是在未见过的领域或与不熟悉的LLMs中。利用LLM的下一词分布输出为检测提供了一种理论上有吸引力的方法，因为这些输出封装了模型在多样化语料库上广泛预训练的见解。尽管这种方法有潜力，但试图将输出实际化的零样本方法取得了有限的成果。我们假设其中一个问题是，它们使用平均值来汇总跨所有词的下一词分布度量，而有些词自然更容易或更难预测，应该有不同的权重。基于这一想法，我们提出了困惑度注意力加权网络（PAWN），它使用LLM的最后隐藏状态和位置来根据序列长度内下一词分布的指标加权一系列特征的求和。虽然不是零样本方法，但我们的方法允许我们在磁盘上缓存最后隐藏状态和下一词分布度量，从而大大减少了训练资源需求。PAWN在分布内的表现与最强大的基线（微调的LLMs）相媲美，甚至更好，而所需的可训练参数仅为其一小部分。我们的模型在未见过的领域和源模型上也表现出更好的泛化能力，分布变化时的决策边界变化更小。它对对抗攻击也更加鲁棒，如果主干具有多语言能力，它在监督训练期间未见过的语言上也表现出良好的泛化能力，LLaMA3-1B在九种语言的交叉验证中达到了平均宏平均F1分数81.46%。|
|**2025-01-07**|**Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**|Ramya Jonnala et.al.|[2501.03904](http://arxiv.org/abs/2501.03904)|null|将大型语言模型（LLMs）集成到公共交通系统中，为提升城市交通提供了变革性的机遇。本研究探讨了LLMs在圣安东尼奥公共交通系统背景下，对公共交通管理进行革新的潜力。利用LLMs在自然语言处理和数据分析方面的能力，我们研究了它们在优化路线规划、减少等待时间和提供个性化旅行协助方面的能力。通过利用通用公共交通数据规范（GTFS）和其他相关数据，本研究旨在展示LLMs如何有可能改善资源配置、提高乘客满意度，并为公共交通运营中的数据驱动决策提供信息。对不同的ChatGPT模型进行了比较分析，以评估它们理解交通信息、检索相关数据和提供全面回应的能力。该研究的结果表明，尽管LLMs在公共交通领域具有巨大的潜力，但实现其全部潜力需要谨慎的工程设计和精细调整。圣安东尼奥作为一个案例研究，为其他城市环境开发LLM驱动的公共交通系统提供了参考。|
|**2025-01-07**|**LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**|Shaolei Zhang et.al.|[2501.03895](http://arxiv.org/abs/2501.03895)|**[link](https://github.com/ictnlp/llava-mini)**|**实时大型多模态模型（LMMs）如GPT-4o的出现，引发了人们对高效LMMs的极大兴趣。LMM框架通常将视觉输入编码为视觉标记（连续表示）并将它们与文本指令整合到大型语言模型（LLMs）的上下文中，大规模参数和众多上下文标记（主要是视觉标记）导致计算开销巨大。以往针对高效LMMs的努力总是侧重于用较小的模型替换LLM主干，而忽略了标记数量这一关键问题。在本文中，我们引入了LLaVA-Mini，这是一种具有最小视觉标记的高效LMM。为了在保留视觉信息的同时实现视觉标记的高压缩比，我们首先分析了LMM是如何理解视觉标记的，并发现大多数视觉标记仅在LLM主干的前几层发挥关键作用，它们主要将视觉信息融合到文本标记中。基于这一发现，LLaVA-Mini引入了模态预融合，将视觉信息预先融合到文本标记中，从而使得馈送到LLM主干的视觉标记能够极端压缩成一个标记。LLaVA-Mini是一个统一的大型多模态模型，可以高效地支持对图像、高分辨率图像和视频的理解。在11个基于图像和7个基于视频的基准测试中的实验表明，LLaVA-Mini只需1个视觉标记就能超越LLaVA-v1.5的576个视觉标记。效率分析显示，LLaVA-Mini可以将FLOPs减少77%，在40毫秒内提供低延迟响应，并在具有24GB内存的GPU硬件上处理超过10,000帧的视频。**|
|**2025-01-06**|**BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**|Beichen Zhang et.al.|[2501.03226](http://arxiv.org/abs/2501.03226)|**[link](https://github.com/beichenzbc/booststep)**|**先进的语言大模型（LLMs）通过分而治之的流程和在上下文学习（ICL）示例的辅助下，在解决复杂数学问题方面展现出有前景的性能。然而，它们改进的潜力受到两个关键问题的限制：在ICL示例中的粒度不匹配以及随之而来的负面效应噪声问题。具体来说，LLMs能够进行分解过程，但在几个征服步骤中通常由于推理不准确而失败，而针对问题粒度检索的ICL示例有时缺乏特定推理步骤的相关步骤。此外，这种脱节可能由于其不相关性而阻碍正确推理。为此，我们专注于提高每个步骤中的推理质量，并提出了BoostStep。BoostStep在步骤粒度上对检索和推理之间的粒度进行了对齐，并使用新颖的“第一次尝试”策略为每个推理步骤提供高度相关的ICL示例。BoostStep比粗略的问题粒度策略提供了更多相关示例，从而稳步提高模型在每个步骤中的推理质量。BoostStep是一种通用且鲁棒的推理增强方法，它不仅提高了独立推理性能，而且可以无缝集成到蒙特卡洛树搜索方法（MCTS）中，以优化候选生成和决策。在数量上，它在各种数学基准测试中将GPT-4o和Qwen2.5-Math-72B分别提高了3.6%和2.0%，并结合MCTS实现了7.5%的增益。**|
|**2025-01-06**|**Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text**|Ayat Najjar et.al.|[2501.03212](http://arxiv.org/abs/2501.03212)|null|生成式人工智能大型语言模型（LLMs）的发展引发了关于识别由生成式AI或人类产生的内容的警报。在一种情况下，当学生过度依赖这些工具，以至于可能影响他们的写作或编码技能发展时，就会产生问题。其他关于剽窃的问题也适用。本研究旨在支持检测和识别使用LLM工具生成的文本的努力。我们假设通过机器学习（ML）可以检测到LLM生成的文本，并调查可以识别和区分由多个LLM工具生成的文本的ML模型。我们利用了多种机器学习和深度学习（DL）算法，如随机森林（RF）和循环神经网络（RNN），并利用可解释人工智能（XAI）来理解归因中的重要特征。我们的方法分为两部分：1）二元分类，用于区分人类撰写的文本和AI文本；2）多分类，用于区分人类撰写的文本和由五个不同的LLM工具（ChatGPT、LLaMA、Google Bard、Claude和Perplexity）生成的文本。结果显示在多分类和二元分类中都具有高准确率。我们的模型在准确率上优于GPTZero，前者为98.5%，后者为78.3%。值得注意的是，GPTZero无法识别大约4.2%的观测值，但我们的模型能够识别完整的测试数据集。XAI结果显示，理解不同类别中的特征重要性可以创建详细的作者/来源档案。此外，通过突出独特的风格和结构元素，帮助归因并支持剽窃检测，确保内容原创性的稳健验证。|
|**2025-01-06**|**Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity**|Ayat A. Najjar et.al.|[2501.03203](http://arxiv.org/abs/2501.03203)|null|本研究旨在通过提供工具来检测学生作业中的AI生成内容，利用先进技术来提高学术诚信。研究结果促进了透明度和问责制，帮助教育者维持道德标准，并支持人工智能在教育中的负责任整合。这项工作的一个关键贡献是生成了CyberHumanAI数据集，该数据集包含1000个观察值，其中500个由人类撰写，另外500个由ChatGPT生成。我们评估了CyberHumanAI数据集上的各种机器学习（ML）和深度学习（DL）算法，比较了大型语言模型（LLMs）（例如ChatGPT）生成的人类撰写内容和AI生成内容。结果表明，传统的ML算法，特别是XGBoost和随机森林，实现了高精度（分别达到83%和81%的准确率）。结果还显示，分类短内容似乎比分类长内容更具挑战性。此外，使用可解释人工智能（XAI）我们确定了影响ML模型预测的判别特征，其中人类撰写的内容倾向于使用实用语言（例如，使用和允许）。而AI生成的文本则具有更多抽象和正式的术语（例如，领域和雇佣）。最后，与GPTZero的比较分析表明，我们的专注、简单且微调过的模型可以超越像GPTZero这样的通用系统。当任务是对纯AI、纯人类和混合类别进行分类时，我们提出的模型实现了约77.5%的准确率，而GPTZero的准确率为48.5%。GPTZero倾向于将具有挑战性和小内容的情况分类为混合或未识别，而我们的模型在三个类别中表现出更均衡的性能。|
|**2025-01-06**|**CLIX: Cross-Lingual Explanations of Idiomatic Expressions**|Aaron Gluck et.al.|[2501.03191](http://arxiv.org/abs/2501.03191)|null|为了支持语言学习者的词汇扩展，已经提出了自动定义生成系统。这些系统成功的主要障碍在于学习者往往难以理解定义，尤其是当涉及非标准语言时，因为其中可能包含不熟悉的单词和语法。为了应对这些挑战，我们提出了CLIX任务，即习语表达的多语言解释。我们探讨了当前NLP模型在此任务中的能力，并观察到虽然仍然具有挑战性，但大型语言模型显示出潜力。最后，我们进行了详细的错误分析，以突出在我们可以可靠地将这些系统纳入教育工具之前需要解决的关键挑战。|
|**2025-01-06**|**Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text**|Ali Al-Lawati et.al.|[2501.03166](http://arxiv.org/abs/2501.03166)|**[link](https://github.com/aliwister/ast-icl)**|**大型语言模型（LLMs）在各种自然语言处理任务中表现出色，包括语义解析，即将自然语言翻译成形式化的代码表示。然而，反向过程，即代码翻译成自然语言，被称为语义字幕，却受到了较少的关注。随着LLMs被集成到代码生成、安全分析和教育等平台中，这项任务变得越来越重要。在本文中，我们专注于SQL查询（SQL2Text）的字幕制作，以解决在LLM生成代码可能带来潜在安全风险的背景下，理解和解释SQL查询的迫切需求。我们通过引入使用GPT-4o的迭代ICL提示，将Text2SQL数据集用于SQL2Text，从而生成多个额外的表述，增强了数据集对反向任务的鲁棒性。我们使用基于不同样本选择方法的上下文学习（ICL）进行实验，强调较小、计算效率更高的LLMs。我们的发现表明，利用SQL固有的图属性进行ICL样本选择，在BLEU分数上比随机选择高出39%，并且比替代方法提供更好的结果。数据集和代码已发布：\url{https://github.com/aliwister/ast-icl}。**|
|**2025-01-06**|**Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**|Alhassan Mumuni et.al.|[2501.03151](http://arxiv.org/abs/2501.03151)|null|基于大规模预训练基础模型（PFMs）的生成式人工智能（AI）系统，例如视觉-语言模型、大型语言模型（LLMs）、扩散模型和视觉-语言-动作（VLA）模型，已在众多领域和情境中展现出解决复杂且真正非平凡AI问题的能力。特别是多模态大型语言模型（MLLMs），它们从广泛且多样化的数据源中学习，从而能够对世界进行丰富和细腻的表征，并因此提供广泛的能力，包括推理、进行有意义的对话；与人类和其他代理共同解决复杂问题；以及理解人类的社会和情感方面。尽管这一成就令人印象深刻，但基于大规模数据集训练的最先进LLMs的认知能力仍然肤浅且脆弱。因此，通用LLMs在它们的泛化能力上受到严重限制。为了使LLMs达到人类水平的通用智能，需要解决一系列基础问题——具身化、符号化、因果关系和记忆。这些概念与人类认知更为一致，并为LLMs提供了固有的类似人类认知的特性，支持实现具有物理可能性、语义意义、灵活性和更广泛的可推广的知识和智能。在这项工作中，我们讨论了上述基础问题，并概述了在LLMs中实现这些概念的最先进方法。具体来说，我们讨论了如何利用具身化、符号化、因果关系和记忆的原则，以有机的方式实现人工通用智能（AGI）。|
|**2025-01-06**|**VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity**|Yerong Li et.al.|[2501.03139](http://arxiv.org/abs/2501.03139)|null|基于场景的训练在许多公共服务领域得到了广泛应用。近年来，大型语言模型（LLMs）在模拟不同角色以创建这些训练场景方面展现出良好的前景。然而，关于如何开发LLMs来模拟受害者以用于场景化训练的了解还很少。在本文中，我们介绍了VicSim（受害者模拟器），这是一个新颖的模型，它针对用户模拟的三个关键维度：信息忠实度、情感动态和语言风格（例如，语法使用）。我们开创性地将基于场景的受害者建模与基于GAN的训练工作流程和基于关键信息的提示相结合，旨在提高模拟受害者的逼真度。我们的对抗性训练方法教会了判别器将语法和情感线索识别为合成内容的可靠指标。根据人类评分者的评估，VicSim模型在拟人化方面优于GPT-4。|
|**2025-01-06**|**PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models**|Mingyang Song et.al.|[2501.03124](http://arxiv.org/abs/2501.03124)|**[link](https://github.com/ssmisya/PRMBench)**|**过程级奖励模型（PRMs）对于复杂推理和决策任务至关重要，其中每个中间步骤在推理过程中都扮演着重要角色。由于语言模型在推理过程中容易发生各种类型的错误，PRMs需要具备细微的能力来检测现实场景中各种隐含的错误类型。然而，当前的基准测试主要关注步骤的正确性，未能系统地评估PRMs的性能。为了解决这一差距，我们引入了PRMBench，这是一个专门设计来评估PRMs细粒度错误检测能力的进程级基准。PRMBench包含6,216个精心设计的问题和83,456个步骤级标签，从多个维度评估模型，包括简洁性、严谨性和灵敏度。在我们的对15个模型的实验中，这些模型包括开源PRMs和作为批评模型的闭源大型语言模型，我们发现了当前PRMs的重大弱点。这些发现强调了过程级评估中固有的挑战，并突出了未来研究的关键方向。我们希望PRMBench能够成为推进PRM评估和发展研究的坚实平台。**|
|**2025-01-06**|**CAT: Content-Adaptive Image Tokenization**|Junhong Shen et.al.|[2501.03120](http://arxiv.org/abs/2501.03120)|null|现有的图像分词器通常将图像编码为固定数量的标记或块，忽略了图像复杂性的固有变化。为了解决这个问题，我们引入了内容自适应分词器（Content-Adaptive Tokenizer，简称CAT），它根据图像内容动态调整表示容量，并将简单的图像编码为更少的标记。我们设计了一个基于字幕的评估系统，该系统利用大型语言模型（LLMs）来预测内容复杂度，并确定给定图像的最佳压缩比率，同时考虑了人类感知的关键因素。CAT在具有不同压缩比率的图像上训练，展示了在图像重建方面的稳健性能。我们还利用其可变长度的潜在表示来训练用于ImageNet生成的扩散变换器（Diffusion Transformers，简称DiTs）。通过优化标记分配，CAT在相同flops下训练的固定比率基线中提高了FID分数，并将推理吞吐量提升了18.5%。|
|**2025-01-06**|**LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases**|Dylan Bouchard et.al.|[2501.03112](http://arxiv.org/abs/2501.03112)|**[link](https://github.com/cvs-health/langfair)**|**大型语言模型（LLMs）被观察到以多种方式表现出偏见，这可能会对由性别、种族、性取向或年龄等受保护属性识别的特定群体造成或加剧不良后果。为了帮助填补这一空白，我们引入了LangFair，这是一个开源的Python软件包，旨在为LLMs从业者提供评估与其特定用例相关的偏见和公平性风险的工具。该软件包提供了易于生成评估数据集的功能，这些数据集包含LLMs对特定用例提示的回答，并随后计算适用于从业者用例的相关指标。为了指导指标选择，LangFair提供了一个可操作的决策框架。**|
|**2025-01-03**|**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**|Chaoyou Fu et.al.|[2501.01957](http://arxiv.org/abs/2501.01957)|**[link](https://github.com/VITA-MLLM/VITA)**|**近年来，多模态大型语言模型（MLLMs）通常侧重于整合视觉和文本模态，而对语音在增强交互中的作用关注较少。然而，语音在多模态对话系统中起着至关重要的作用，由于基本模态差异，同时在视觉和语音任务中实现高性能仍然是一个重大挑战。在本文中，我们提出了一种精心设计的多阶段训练方法，该方法逐步训练LLM理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉-语言能力，还使语音到语音的对话能力变得高效，无需单独的ASR和TTS模块，显著加快了多模态端到端响应速度。通过将我们的方法与图像、视频和语音任务的基准测试中的最先进方法进行比较，我们证明了我们的模型具备强大的视觉和语音能力，实现了近乎实时的视觉和语音交互。**|
|**2025-01-03**|**Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap**|Weizhi Zhang et.al.|[2501.01945](http://arxiv.org/abs/2501.01945)|**[link](https://github.com/yuanchenbei/awesome-cold-start-recommendation)**|冷启动问题是推荐系统领域长期存在的挑战之一，它关注于准确建模新用户或互动受限的用户或物品，以提供更好的推荐。由于互联网平台的多样化和用户、物品数量的指数级增长，冷启动推荐（CSR）的重要性日益凸显。同时，大型语言模型（LLMs）取得了巨大成功，并在建模用户和物品信息方面具有强大能力，为冷启动推荐提供了新的潜力。然而，在CSR领域的科研社区中，仍缺乏对该领域的全面回顾和反思。基于此，本文站在大型语言模型时代的背景下，对CSR的路线图、相关文献和未来发展方向进行了全面回顾和讨论。具体来说，我们对现有CSR如何利用信息进行了探索，从内容特征、图关系和领域信息到大型语言模型所拥有的世界知识，旨在为CSR的研究和工业界提供新的见解。冷启动推荐的有关资源已收集并持续更新，供社区在https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation上使用。|
|**2025-01-03**|**Virgo: A Preliminary Exploration on Reproducing o1-like MLLM**|Yifan Du et.al.|[2501.01904](http://arxiv.org/abs/2501.01904)|**[link](https://github.com/rucaibox/virgo)**|最近，基于大型语言模型（LLMs）的慢思考推理系统，通过扩展推理过程中的思考时间，受到了广泛关注。同时，将这种能力应用于多模态大型语言模型（MLLMs）的兴趣也在增长。鉴于MLLMs需要在不同的模态中处理更复杂的数据语义，直观上实现多模态慢思考系统更具挑战性。为了解决这个问题，在本文中，我们通过微调一个具有能力的MLLM，使用少量的文本长篇思考数据，探索了一种简单的方法，从而创建了一个多模态慢思考系统，名为Virgo（视觉推理与长思考）。我们发现，这些以自然语言表达的长篇推理过程可以有效地转移到MLLMs中。此外，似乎这样的文本推理数据在激发MLLMs的慢思考能力方面甚至比视觉推理数据更有效。虽然这项工作还处于初步阶段，但它表明慢思考能力与语言模型组件的基本关联，这种能力可以跨模态或领域转移。这一发现可以用来指导更强大的慢思考推理系统的发展。我们将在https://github.com/RUCAIBox/Virgo上发布我们的资源。|
|**2025-01-03**|**Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions**|Rachneet Sachdeva et.al.|[2501.01872](http://arxiv.org/abs/2501.01872)|**[link](https://github.com/ukplab/poate-attack)**|尽管在将大型语言模型与人类价值观和伦理准则对齐方面付出了巨大努力，但这些模型仍然容易受到利用其推理能力的复杂越狱攻击。传统的安全机制通常侧重于检测明确的恶意意图，而未能解决更深层次的漏洞。在这项工作中，我们介绍了一种越狱技术，称为POATE（极对立查询生成、对抗模板构建和详细阐述），该技术利用对比推理来诱使模型产生不道德的回答。POATE生成具有语义上对立意图的提示，并将它们与对抗性模板相结合，微妙地引导模型产生有害的回答。我们在六个参数大小不同的多样化语言模型家族中进行了广泛的评估，包括LLaMA3、Gemma2、Phi3和GPT-4，以证明攻击的鲁棒性，与现有方法相比，攻击成功率显著提高（约44%）。我们对所提出的攻击进行了七种安全防御的评估，揭示了它们在解决基于推理的漏洞方面的局限性。为了应对这一问题，我们提出了一种防御策略，通过思维链提示和逆向思维来提高推理鲁棒性，减轻由推理驱动的对抗性攻击。|
|**2025-01-03**|**Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification**|Xiangxiang Dai et.al.|[2501.01849](http://arxiv.org/abs/2501.01849)|**[link](https://github.com/tarfersoul/maco)**|大型语言模型（LLMs）令人瞩目的生成能力激发了人们对于自动生成不同应用响应的兴趣。鉴于用户偏好的动态性和LLMs响应性能的不确定性，设计高效的在线学习算法以识别最优LLMs响应（即既高质量又符合用户偏好的响应）变得至关重要。大多数现有的在线算法采用集中式方法，未能利用显式用户偏好以更高效、个性化地识别LLMs响应。相比之下，本文介绍了MACO（多智能体对话在线学习用于自适应LLMs响应识别）：1）通过多个本地代理（如智能手机）加速在线LLMs响应识别过程，同时增强数据隐私；2）提出了一种新颖的对话机制，以自适应地进行对话以征求用户偏好（例如，在生成的响应中对幽默语气而非严肃语气有偏好），从而最小化偏好估计的不确定性。我们的理论分析表明，MACO在累积遗憾方面接近最优。此外，MACO通过消除先前工作中发现的传统的、计算密集型的“G-最优设计”来降低通信成本和计算复杂度。与公开的LLMs Llama的大量实验，以及来自Google和OpenAI的两个不同的嵌入模型用于文本向量表示，表明MACO在在线LLMs响应识别方面显著优于当前最先进的技术。|
|**2025-01-03**|**MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning**|Pu Yang et.al.|[2501.01834](http://arxiv.org/abs/2501.01834)|null|图像描述是计算机视觉和自然语言处理交叉领域的一项关键任务，在各个领域有着广泛的应用。对于诊断报告生成等复杂任务，深度学习模型不仅需要特定领域的图像描述数据集，还需要融入相关的一般知识以提供情境准确性。现有方法存在固有局限性：专门模型在捕捉特定领域细节方面表现优秀，但缺乏泛化能力，而基于大型语言模型（LLMs）的视觉语言模型（VLMs）虽然利用了一般知识，但在特定领域适应性方面存在困难。为了解决这些局限性，本文提出了一种新颖的代理增强模型协作框架，我们称之为MoColl，旨在有效地整合特定领域和一般知识。具体来说，我们的方法是将复杂的图像描述任务分解为一系列相互关联的问答子任务。一个可训练的视觉问答（VQA）模型被用作专门的工具，专注于特定领域的视觉分析，根据图像内容回答特定问题。同时，一个基于LLM的代理利用一般知识提出这些问题，并将生成的问答对综合成连贯的描述。除了利用VQA模型的作用外，代理还指导其训练以增强其特定领域的能力。在放射学报告生成上的实验结果验证了所提出框架的有效性，证明了生成报告质量显著提高。|
|**2025-01-03**|**Time Series Language Model for Descriptive Caption Generation**|Mohamed Trabelsi et.al.|[2501.01832](http://arxiv.org/abs/2501.01832)|null|在时间序列数据中自动生成代表性的自然语言描述，可以增强可解释性、简化分析并提高时间数据的跨领域应用价值。虽然预训练的基础模型在自然语言处理（NLP）和计算机视觉（CV）领域取得了显著进展，但它们在时间序列分析中的应用受到数据稀缺性的限制。尽管已经提出了基于大型语言模型（LLM）的几种时间序列预测方法，但在LLM的背景下，时间序列字幕生成仍未得到充分探索。在本文中，我们介绍了TSLM，这是一种专为时间序列字幕生成设计的新型时间序列语言模型。TSLM作为一个编码器-解码器模型，利用文本提示和时间序列数据表示来捕捉多个阶段中的微妙时间模式，并生成时间序列输入的精确文本描述。TSLM通过以下两种方式解决时间序列字幕生成中的数据稀缺问题：首先，利用上下文提示合成数据生成；其次，通过应用于时间序列-字幕对的创新跨模态密集检索评分来降噪生成的数据。在多个时间序列字幕生成数据集上的实验结果表明，TSLM在多个数据模态上显著优于现有的最先进方法。|
|**2025-01-03**|**Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models**|Yanjiang Liu et.al.|[2501.01830](http://arxiv.org/abs/2501.01830)|null|自动化的红队攻击已成为揭示大型语言模型（LLMs）漏洞的关键方法。然而，现有的大多数方法都集中在孤立的安全缺陷上，限制了它们适应动态防御和高效发现复杂漏洞的能力。为了应对这一挑战，我们提出了Auto-RT，一个强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效地发现安全漏洞。具体来说，我们引入了两种关键机制来降低探索复杂性和提高策略优化：1）提前终止探索，通过关注高潜力的攻击策略来加速探索；2）具有中间降级模型的渐进式奖励跟踪算法，该算法动态地细化搜索轨迹，以成功利用漏洞。在多种LLMs上的大量实验表明，通过显著提高探索效率和自动优化攻击策略，Auto-RT能够检测到更广泛的漏洞，相比现有方法，检测速度更快，成功率高16.63%。|
|**2025-01-03**|**SDPO: Segment-Level Direct Preference Optimization for Social Agents**|Aobo Kong et.al.|[2501.01821](http://arxiv.org/abs/2501.01821)|**[link](https://github.com/alibabaresearch/damo-convai)**|**由大型语言模型（LLMs）驱动的社交代理能够模拟人类的社会行为，但在处理复杂的目标导向社交对话方面存在不足。直接偏好优化（DPO）已被证明在使LLM行为与人类偏好一致方面在各种代理任务中非常有效。现有的基于DPO的多轮交互方法分为回合级和会话级方法。回合级方法过于细粒度，仅专注于单个回合，而会话级方法则过于粗粒度，往往引入训练噪声。为了解决这些局限性，我们提出了段落级直接偏好优化（SDPO），它专注于交互中的特定关键段落，以优化多轮代理行为同时最小化训练噪声。在SOTOPIA基准上的评估表明，SDPO调优的代理在性能上始终优于现有的基于DPO的方法以及GPT-4o等专有LLMs，凸显了SDPO在提升基于LLM代理的社会智能方面的潜力。我们已在https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO发布我们的代码和数据。**|
|**2025-01-03**|**Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation**|Mohammad Khalil et.al.|[2501.01793](http://arxiv.org/abs/2501.01793)|**[link](https://github.com/mohdkhalil/repository-supplementary-for-lak-25-paper--creating-artificial-students-that-never-existed)**|在本研究中，我们探讨了人工智能和深度学习技术，特别是生成对抗网络（GANs）和大型语言模型（LLMs），在生成合成表格数据方面的日益增长的潜力。获取高质量的学生数据对于推进学习分析至关重要，但隐私担忧和全球范围内更严格的数据保护法规限制了其可用性和使用。合成数据提供了一种有前景的替代方案。我们研究是否可以利用合成数据来创建用于服务学习分析模型的人工学生。使用流行的GAN模型CTGAN和三种LLMs——GPT2、DistilGPT2和DialoGPT，我们生成了合成表格学生数据。我们的结果表明，这些方法具有很强的潜力产生高质量的类似真实学生数据的合成数据集。为了验证我们的发现，我们应用了一套综合的效用评估指标来评估合成数据的统计和预测性能，并比较了所使用的不同生成模型，特别是LLMs的性能。我们的研究旨在为学习分析社区提供关于合成数据使用的宝贵见解，为扩展该领域的工具箱，以新的创新方法生成学习分析数据奠定基础。|
|**2025-01-02**|**Unifying Specialized Visual Encoders for Video Language Models**|Jihoon Chung et.al.|[2501.01426](http://arxiv.org/abs/2501.01426)|**[link](https://github.com/princetonvisualai/merv)**|最近大型语言模型（LLMs）的兴起，将高级推理能力引入了视频领域，形成了视频大型语言模型（VideoLLMs）。然而，当前的VideoLLMs依赖于单个视觉编码器进行所有视觉处理，这限制了可以传达给LLM的视觉信息和类型。我们的方法，MERV（多编码器视频表示），利用多个冻结的视觉编码器来创建视频的统一表示，为VideoLLM提供了一套全面的专用视觉知识。通过空间-时间对齐每个编码器的特征，我们可以解决更广泛的开放式和多选题型视频理解问题，并优于先前最先进的工作。在标准视频理解基准测试中，MERV的准确率比Video-LLaVA高3.7%，同时视频-ChatGPT得分也更高。我们还提高了SeViLA（在零样本感知测试准确率上的先前最佳结果）2.2%。MERV引入了最小的额外参数，在并行化视觉处理的同时比等效的单编码器方法训练速度更快。最后，我们提供了定性的证据，表明MERV成功地从其每个编码器中捕获了领域知识。我们的结果为利用多个视觉编码器进行综合视频理解提供了有希望的指导方向。|
|**2025-01-02**|**OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios**|Xize Cheng et.al.|[2501.01384](http://arxiv.org/abs/2501.01384)|null|随着大型语言模型的快速发展，研究人员已经创建了能够与人类自然对话的越来越先进的语音对话系统。然而，这些系统仍然难以处理现实对话的全部复杂性，包括音频事件、音乐环境和情感表达，主要是因为当前的对话数据集在规模和场景多样性方面都受到限制。在本文中，我们提出利用合成数据来增强不同场景下的对话模型。我们引入了ShareChatX，这是第一个涵盖不同场景的综合、大规模语音对话数据集。基于这个数据集，我们引入了OmniChat，这是一个具有异构特征融合模块的多轮对话系统，旨在优化不同对话环境中的特征选择。此外，我们探讨了使用合成数据训练对话系统的关键方面。通过全面实验，我们确定了合成数据和真实数据之间的理想平衡，在现实对话数据集DailyTalk上取得了最先进的成果。我们还强调了合成数据在解决多样化、复杂对话场景中的关键作用，特别是涉及音频和音乐的场景。更多详情请访问我们的演示页面：\url{https://sharechatx.github.io/}。|
|**2025-01-02**|**Aligning Large Language Models for Faithful Integrity Against Opposing Argument**|Yong Zhao et.al.|[2501.01336](http://arxiv.org/abs/2501.01336)|**[link](https://github.com/zhaoy777/afice)**|大型语言模型（LLMs）在复杂推理任务中展示了令人印象深刻的性能。然而，它们在对话中很容易被不忠实的论点误导，即使它们的原始陈述是正确的。为此，我们研究了在LLMs中保持忠实完整性的问题。这包括确保LLMs在面对对立论点时坚持其忠实陈述，并在遇到忠实论点时能够纠正其错误陈述。在这项工作中，我们提出了一种名为“具有置信度估计的忠实完整性对齐”（AFICE）的新框架，旨在使LLM的响应与忠实完整性对齐。具体来说，AFICE首先设计了一种双边置信度估计（BCE）方法，用于估计LLM在特定语境下生成每个响应的不确定性，该方法同时根据解码过程中的内部状态估计模型对问题的置信度，以及根据累积概率比估计对答案的置信度。利用BCE，我们构建了一个由语境、原始陈述和论点组成的话语偏好数据集，该数据集用于通过直接偏好优化（DPO）对齐LLM以实现忠实完整性。在广泛基准上的大量实验结果表明，当LLM遇到对立论点时，其在保持忠实响应方面的能力得到了显著提高，确保了LLMs在复杂交互环境中的实用性和可靠性。代码和数据将通过https://github.com/zhaoy777/AFICE.git发布。|
|**2025-01-02**|**CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models**|Johan Wahréus et.al.|[2501.01335](http://arxiv.org/abs/2501.01335)|**[link](https://github.com/cysecbench/dataset)**|众多研究调查了破解大型语言模型（LLMs）以生成有害内容的方法。通常，这些方法使用旨在绕过LLM提供商建立的安保策略的恶意提示数据集进行评估。然而，现有数据集的广泛范围和开放式特性可能会使破解效果的评价复杂化，特别是在特定领域，如网络安全。为了解决这个问题，我们提出并公开发布了CySecBench，这是一个包含12662个提示的综合数据集，专门用于评估网络安全领域的破解技术。该数据集分为10个不同的攻击类型类别，包含封闭式提示，以实现更一致和准确的破解尝试评估。此外，我们详细说明了数据集生成和筛选的方法，这些方法可以适应其他领域的类似数据集创建。为了展示CySecBench的实用性，我们提出并评估了一种基于提示混淆的破解方法。我们的实验结果表明，这种方法成功地从商业黑盒LLMs中诱发出有害内容，实现了ChatGPT的65%成功率（SR）和Gemini的88%成功率；相比之下，Claude表现出更强的抵抗力，破解成功率仅为17%。与现有基准方法相比，我们的方法表现出更优越的性能，突显了特定领域评估数据集在评估LLM安全措施方面的价值。此外，当使用广泛使用的数据集（即AdvBench）中的提示进行评估时，它实现了78.5%的成功率，高于现有最佳方法。|
|**2025-01-02**|**Decoding Knowledge in Large Language Models: A Framework for Categorization and Comprehension**|Yanbo Fang et.al.|[2501.01332](http://arxiv.org/abs/2501.01332)|null|理解大型语言模型（LLMs）如何获取、保留和应用知识仍然是一个开放性的挑战。本文介绍了一个新颖的框架K-(CSA)^2，该框架从正确性和信心两个维度对LLMs的知识进行分类。该框架定义了六种知识类别，从高度自信的正确性到自信地持有的错误观念，从而使得对模型理解的评价超越了二值准确性的范畴。使用该框架，我们展示了像思维链提示和带有人类反馈的强化学习等技术在如何基本改变LLMs内部（预训练）和外部（情境依赖）知识结构方面的作用。思维链（CoT）特别提高了基础模型的表现，并在应用于对齐的LLMs时显示出协同效应。此外，我们的分层分析揭示，LLMs中较高层编码了更多的高信心知识，而低信心知识往往出现在中间到低层。|
|**2025-01-02**|**The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation**|Shuzheng Gao et.al.|[2501.01329](http://arxiv.org/abs/2501.01329)|null|测试用例对于验证软件应用的可靠性和质量至关重要。最近的研究表明，大型语言模型（LLMs）能够为给定的源代码生成有用的测试用例。然而，现有工作主要依赖于人类编写的普通提示，这往往导致结果不佳，因为LLMs的性能会受到提示的高度影响。此外，这些方法使用相同的提示对所有LLMs，而忽略了不同LLMs可能最适合不同提示的事实。鉴于可能的提示表述的广泛多样性，为每个LLM自动发现最佳提示是一个重大挑战。尽管自然语言处理领域存在自动提示优化的方法，但它们难以生成针对测试用例生成任务的有效提示。首先，这些方法通过简单组合和变异现有提示来迭代优化提示，缺乏适当的指导，导致提示缺乏多样性，并且倾向于在生成的测试用例中重复相同的错误。其次，提示通常缺乏领域上下文知识，限制了LLMs在任务中的性能。|
|**2025-01-02**|**Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking**|Xiaoxue Cheng et.al.|[2501.01306](http://arxiv.org/abs/2501.01306)|null|大型语言模型（LLMs）表现出卓越的能力，但仍面临幻觉问题。典型的文本生成方法采用无意识的自动回归生成，这往往导致不可信和事实错误的结果。在本文中，我们提出了HaluSearch，一个新型框架，它结合了基于树搜索的算法（例如MCTS），以实现显式的缓慢思考生成过程，以减轻LLMs在推理过程中的幻觉问题。具体来说，HaluSearch将文本生成视为一个逐步推理的过程，使用自我评估奖励模型对每个生成步骤进行评分，并引导树搜索走向最可靠的生成路径，以充分利用LLMs的内部知识。为了平衡效率和质量，我们引入了一种受认知科学中双重过程理论启发的分层思考系统切换机制，在实例和步骤级别上动态地在快速思考和缓慢思考模式之间交替，以适应问题的复杂性和推理状态。我们在英文和中文数据集上进行了广泛的实验，结果表明，我们的方法显著优于基线方法。|
|**2025-01-02**|**Large Language Models for Mental Health Diagnostic Assessments: Exploring The Potential of Large Language Models for Assisting with Mental Health Diagnostic Assessments -- The Depression and Anxiety Case**|Kaushik Roy et.al.|[2501.01305](http://arxiv.org/abs/2501.01305)|null|大型语言模型（LLMs）因其协助诊断评估的潜力，越来越受到医疗专业人士的关注。这有助于缓解由于患者负担过重和医疗资源短缺而对医疗系统造成的压力。为了使LLMs在支持诊断评估方面有效，它们必须紧密模仿临床医生使用的标准诊断程序。在本文中，我们特别研究了用于重度抑郁症（MDD）的病人健康问卷-9（PHQ-9）和用于广泛性焦虑障碍（GAD）的广泛性焦虑障碍-7（GAD-7）问卷中描述的诊断评估过程。我们调查了各种提示和微调技术，以引导专有和开源LLMs遵循这些流程，并评估LLM生成的诊断结果与专家验证的基准之间的吻合度。对于微调，我们使用了Mentalllama和Llama模型，而对于提示，我们尝试了专有模型如GPT-3.5和GPT-4o，以及开源模型如llama-3.1-8b和mixtral-8x7b。|
|**2025-01-02**|**Does a Large Language Model Really Speak in Human-Like Language?**|Mose Park et.al.|[2501.01273](http://arxiv.org/abs/2501.01273)|null|大型语言模型（LLMs）最近出现，由于它们能够生成高度自然、类似人类的文本而引起了广泛关注。本研究在假设检验程序中比较了LLM生成的文本和人工撰写的文本的潜在社区结构。具体来说，我们分析了三个文本集：原始人工撰写的文本（ $\mathcal{O}$）、它们通过LLM改写的版本（$\mathcal{G}$）以及从$\mathcal{G}$中派生出的两次改写集（$\mathcal{S}$）。我们的分析解决了两个关键问题：（1）$\mathcal{O}$和$\mathcal{G}$之间的潜在社区结构差异是否与$\mathcal{G}$和$\mathcal{S}$之间的差异相同？（2）随着控制文本变异性的LLM参数的调整，$\mathcal{G}$是否变得更接近$\mathcal{O}$？第一个问题基于这样一个假设，即如果LLM生成的文本真正地类似于人类语言，那么这两个对（$\mathcal{O}$, $\mathcal{G}$）之间的差距应该与对（$\mathcal{G}$, $\mathcal{S}$ ）之间的差距相似，因为这两个对都包含原始文本及其改写。第二个问题考察LLM生成文本与人工文本之间的相似度是否随着文本生成宽度的变化而变化。为了解决这些问题，我们提出了一种统计假设检验框架，利用了由于它们的改写关系，每个文本在所有数据集中都有对应部分这一事实。这种关系使得可以将一个数据集的相对位置映射到另一个数据集，允许将两个数据集映射到第三个数据集。因此，这两个映射的数据集都可以根据第三个数据集所表征的空间进行量化，从而便于它们之间的直接比较。我们的结果表明，GPT生成的文本仍然与人工撰写的文本不同。|
|**2025-01-02**|**ProgCo: Program Helps Self-Correction of Large Language Models**|Xiaoshuai Song et.al.|[2501.01264](http://arxiv.org/abs/2501.01264)|**[link](https://github.com/songxiaoshuai/progco)**|自我校正旨在使大型语言模型（LLMs）能够自我验证和自我优化其初始响应，而不需要外部反馈。然而，LLMs往往无法有效地自我验证并生成正确的反馈，这进一步误导了优化过程，导致自我校正失败，尤其是在复杂推理任务中。在本文中，我们提出了程序驱动自我校正（ProgCo）。首先，程序驱动验证（ProgVe）通过自我生成、自我执行的验证伪程序实现了复杂的验证逻辑和广泛的验证。然后，程序驱动优化（ProgRe）接收来自ProgVe的反馈，对响应和验证程序进行双重反思和优化，以减轻在复杂推理任务中不正确反馈的误导。在三个指令遵循和数学基准测试上的实验表明，ProgCo实现了有效的自我校正，并且当与真实程序工具结合时，可以进一步提高性能。|
|**2024-12-30**|**Distributed Mixture-of-Agents for Edge Inference with Large Language Models**|Purbesh Mitra et.al.|[2412.21200](http://arxiv.org/abs/2412.21200)|**[link](https://github.com/purbeshmitra/distributed_moa)**|**混合智能体（MoA）最近被提出作为一种提升大型语言模型（LLMs）性能的方法，使多个单独的LLMs能够协同进行推理。这种协作方法相比于依赖单个LLM，能够产生更优的用户提示响应。在本论文中，我们考虑了在分布式环境下的一种MoA架构，其中LLMs运行在各自的边缘设备上，这些设备与用户一一对应，并配备了各自的分布式计算能力。这些设备通过去中心化的八卦算法交换信息，使得不同的设备节点可以在没有中央服务器监督的情况下进行交流。在所考虑的配置中，不同的用户拥有自己的LLM模型来处理用户提示。此外，设备之间可以通过八卦自己的用户特定提示或增强提示来生成针对某些查询的更精细答案。当对应的LLM忙碌时，用户提示暂时存储在设备队列中。鉴于边缘设备的内存限制，确保系统中平均队列大小保持有界至关重要。在本论文中，我们通过在合理的假设下理论计算设备队列的排队稳定性条件来解决这个问题，并通过实验进行验证。此外，我们通过实验展示了利用开源LLMs实现分布式MoA，某些MoA配置在AlpacaEval 2.0基准测试中产生了比其他配置更高的质量响应。该实现可在以下链接获取：https://github.com/purbeshmitra/distributed_moa。**|
|**2024-12-30**|**HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation**|Zhaojian Yu et.al.|[2412.21199](http://arxiv.org/abs/2412.21199)|**[link](https://github.com/CodeEval-Pro/CodeEval-Pro)**|**我们引入了自我调用的代码生成，这是一个旨在评估大型语言模型（LLM）渐进推理和问题解决能力的新任务。在这个任务中，模型被给出一个基础问题和与之相关的一个更复杂的问题。它们必须先解决基础问题，然后利用其解决方案来解决更复杂的问题。这项工作有三个关键贡献。首先，我们提出了一种生成现有基准测试更具有挑战性的通用方法，从而产生了三个新的基准：HumanEval Pro、MBPP Pro和BigCodeBench-Lite Pro，这些基准专门设计用来评估LLM在自我调用代码生成方面的能力。其次，通过对二十个LLM在我们基准测试上的实验结果分析，我们有以下两个重要观察：（i）大多数LLM在传统的代码生成基准测试如HumanEval和MBPP中表现优秀，但在自我调用任务上的表现有所下降。例如，o1-mini在HumanEval上的通过率达到了96.2%，但在HumanEval Pro上仅为76.2%。（ii）在自我调用代码生成任务中，与基础模型相比，指令微调的模型只显示出微小的改进。第三，我们揭示了在我们评估结果中存在的失败模式类型。所有这些结果都强调了在自我调用代码生成任务上进一步进步的必要性，并为未来关于提升LLM代码推理能力的研究提供了新的方向。**|
|**2024-12-30**|**Facilitating large language model Russian adaptation with Learned Embedding Propagation**|Mikhail Tikhomirov et.al.|[2412.21140](http://arxiv.org/abs/2412.21140)|**[link](https://github.com/RefalMachine/llmtf_open)**|**大型语言模型（LLM）技术的快速进步导致了功能强大的开源指令调整LLM的引入，其文本生成质量与GPT-4等最先进的同类模型相当。虽然这类模型的出现加速了LLM技术在敏感信息环境中的采用，但模型的作者没有公开复现结果所需的训练数据，这使得成就是模型专属的。由于这些开源模型也是多语言的，这反过来又减少了训练特定语言LLM的益处，因为改进的推理计算效率成为这种昂贵流程的唯一保证优势。词汇扩展和后续持续预训练等更具成本效益的选项也受到缺乏访问高质量指令调整数据的限制，因为它是导致LLM任务解决能力的主要因素。为了解决这些限制并降低语言适应管道的成本，我们提出了学习嵌入传播（LEP）。与现有方法不同，我们的方法由于对现有LLM知识的微小影响，因此对训练数据大小的要求较低，我们通过使用新颖的临时嵌入传播程序来强化这一点，该程序允许跳过指令调整步骤，并将新的语言知识直接植入任何现有的指令调整变体中。我们对LLaMa-3-8B和Mistral-7B的四种俄语词汇适应进行了评估，表明LEP与传统指令调整方法具有竞争力，实现了与OpenChat 3.5和LLaMa-3-8B-Instruct相当的性能，通过自我校准和持续调整进一步增强了任务解决能力。**|
|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越多地依赖于网络爬取的数据集，对于未经授权使用版权或个人内容进行训练的担忧日益加剧。尽管有诸如通用数据保护条例（GDPR）等法规，数据所有者对他们在模型训练中使用其内容仍有限制。为了解决这个问题，我们提出了ExpShield，这是一种主动的自卫机制，使内容所有者能够将不可见的扰动嵌入其文本中，限制LLMs训练中的数据滥用，同时不影响可读性。这种预防性方法使数据所有者能够直接保护敏感内容，而不必依赖第三方进行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的合理性。我们进一步通过识别记忆触发器和创建陷阱来更专注地偏离模型记忆，从而提高效率。为了验证我们防御的有效性，我们提出了一个新颖的实例利用度量，该度量捕捉了模型训练引起的个别风险。实验结果表明，我们的方法的有效性，MIA AUC从0.95下降到0.55，实例利用接近零。这表明训练后个别风险并未增加，强调了主动防御在保护版权数据中的重要性。|
|**2024-12-30**|**Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense**|Yuyang Zhou et.al.|[2412.21051](http://arxiv.org/abs/2412.21051)|**[link](https://github.com/SEU-ProactiveSecurity-Group/LLM-PD)**|**云计算技术的快速发展和云应用的增多为日常生活带来了大量便利。然而，不同组件的多样性和复杂性对云安全构成了重大挑战，尤其是在应对复杂和高级的网络攻击时。近期在生成基础模型（GFMs），尤其是大型语言模型（LLMs）方面的进步，为安全智能提供了有希望的解决方案。通过利用语言理解、数据分析、任务推理、行动规划和代码生成等强大的能力，我们提出了LLM-PD，这是一种新型的主动防御架构，能够以主动方式击败各种威胁。LLM-PD能够通过全面的数据分析和顺序推理高效地做出决策，同时在目标云上动态创建和部署可执行的防御机制。此外，它可以根据从先前交互中学习到的经验灵活地自我进化，无需额外训练即可适应新的攻击场景。实验结果展示了其在防御效果和效率方面的显著能力，尤其是在与其他现有方法相比时，显示出出色的成功率。**|
|**2024-12-30**|**TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization**|Chia-Yu Hung et.al.|[2412.21037](http://arxiv.org/abs/2412.21037)|**[link](https://github.com/declare-lab/TangoFlux)**|**我们介绍了TangoFlux，这是一个具有515M参数的高效文本到音频（TTA）生成模型，能够在单个A40 GPU上仅需3.7秒生成长达30秒的44.1kHz音频。在调整TTA模型时，一个关键挑战在于创建偏好对，因为TTA缺乏类似于大型语言模型（LLMs）中可验证的奖励或黄金标准答案的结构化机制。为了解决这个问题，我们提出了CLAP-Ranked偏好优化（CRPO），这是一种新颖的框架，它通过迭代生成和优化偏好数据来提高TTA的匹配度。我们证明，使用CRPO生成的音频偏好数据集优于现有替代方案。借助这个框架，TangoFlux在客观和主观基准测试中都达到了最先进的性能。我们将所有代码和模型开源，以支持TTA生成方面的进一步研究。**|
|**2024-12-30**|**GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models**|Shangyu Xing et.al.|[2412.21036](http://arxiv.org/abs/2412.21036)|null|多模态大型语言模型（MLLMs）在整合视觉和语言理解方面取得了显著进步。尽管现有的基准测试在内容丰富、现实生活场景中评估这些模型，但它们往往忽视了对于偏离日常生活现实的环境来说至关重要的基本感知技能。特别是几何感知，即解释空间关系和抽象视觉模式的能力，仍未得到充分探索。为了解决这一局限性，我们引入了GePBench，这是一个新型基准，旨在评估MLLMs的几何感知能力。广泛评估的结果显示，当前最先进的MLLMs在这些任务上存在显著的不足。此外，我们还证明了使用GePBench数据源训练的模型在广泛的后处理任务上显示出显著的改进，这强调了几何感知作为高级多模态应用基础的重要性。我们的代码和数据集将公开可用。|
|**2024-12-30**|**MapQaTor: A System for Efficient Annotation of Map Query Datasets**|Mahir Labib Dihan et.al.|[2412.21015](http://arxiv.org/abs/2412.21015)|**[link](https://github.com/MapQaTor/.github/tree/main/profile)**|**地图和导航服务如谷歌地图、苹果地图、Openstreet Maps对于访问各种基于位置的数据至关重要，但它们在处理自然语言地理空间查询方面往往力不从心。最近在大型语言模型（LLMs）方面的进步在问答（QA）方面显示出希望，但从前端地图服务创建可靠的地理空间QA数据集仍然具有挑战性。我们介绍了MapQaTor，这是一个简化了可重复、可追踪的基于地图的QA数据集创建的Web应用程序。凭借其即插即用的架构，MapQaTor能够与任何地图API无缝集成，使用户能够以最小的设置从各种来源收集和可视化数据。通过缓存API响应，该平台确保了一致的地面真相，即使在现实世界信息不断演变的情况下，也增强了数据的可靠性。MapQaTor将数据检索、标注和可视化集中在一个平台上，为评估基于LLM的地理空间推理的当前状态提供了一个独特的机会，同时提高其地理空间理解能力。评估指标显示，与手动方法相比，MapQaTor将标注过程的速度提高了至少30倍，凸显了其在开发地理空间资源，如复杂地图推理数据集方面的潜力。网站已上线：https://mapqator.github.io/，演示视频可在：https://youtu.be/7_aV9Wmhs6Q查看。**|
|**2024-12-30**|**Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria**|Joonwon Jang et.al.|[2412.21006](http://arxiv.org/abs/2412.21006)|null|大型语言模型（LLMs）依赖于生成大量的中间推理单元（例如，标记、句子）以增强在广泛复杂任务中的最终答案质量。虽然生成多个推理路径或迭代地完善理由证明是提高性能的有效方法，但这些方法不可避免地导致推理成本显著增加。在这项工作中，我们提出了一种新颖的基于句子的理由减少训练框架，该框架利用基于概率的指标——冗余度，来识别和删除冗余的推理句子。与之前利用标记级减少的方法不同，我们的句子级减少框架在减少生成长度的同时保持了模型性能。这保留了LLMs的原有推理能力，并在各种模型和任务中实现了平均17.15%的生成成本降低。|
|**2024-12-30**|**Plug-and-Play Training Framework for Preference Optimization**|Jingyuan Ma et.al.|[2412.20996](http://arxiv.org/abs/2412.20996)|null|近期，偏好优化方法如DPO在包括对话和问答在内的广泛任务中显著提升了大型语言模型（LLMs）。然而，当前的方法未能考虑到在偏好优化过程中训练样本难度级别的差异，导致在高精度要求任务中的表现平庸，尤其是在数学推理任务中。为了解决这一局限性，我们提出了一种新颖的训练框架，该框架采用多采样来分析输出分布，为样本分配不同的权重，并将这些权重纳入偏好优化过程。这种即插即用的方法使得LLMs能够在训练过程中优先处理具有挑战性的示例，从而提高学习效率。实验结果表明，我们的框架能够与各种偏好优化方法无缝集成，并在数学推理任务中实现一致的改进。|
|**2024-12-27**|**Can AI Help with Your Personal Finances?**|Oudom Hean et.al.|[2412.19784](http://arxiv.org/abs/2412.19784)|null|近年来，大型语言模型（LLMs）作为人工智能（AI）领域的一项颠覆性发展，受到了业界和学术界的广泛关注。这些复杂的AI系统在庞大的数据集上训练，展现出令人印象深刻的自然语言处理和内容生成能力。本文探讨了LLMs在解决个人财务领域关键挑战中的潜力，重点关注美国市场。我们评估了包括OpenAI的ChatGPT、谷歌的Gemini、Anthropic的Claude和Meta的Llama在内的几个领先的LLMs，以评估它们在提供关于抵押贷款、税收、贷款和投资等主题的准确财务建议方面的有效性。我们的发现表明，虽然这些模型实现了大约70%的平均准确率，但在某些领域也显示出明显的局限性。具体来说，LLMs在提供复杂财务查询的准确回答方面存在困难，其表现因不同主题而显著不同。尽管存在这些局限性，分析显示这些模型的新版本有显著改进，突显了它们在个人和财务顾问中的日益增长的应用价值。随着这些AI系统持续发展，它们在推动个人财务领域AI驱动应用方面的潜力变得越来越有希望。|
|**2024-12-27**|**Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration**|Le Chen et.al.|[2412.19770](http://arxiv.org/abs/2412.19770)|**[link](https://github.com/hpc-fortran2cpp/fortran2cpp)**|**将Fortran代码迁移到C++是许多科学计算团队常见的任务，这一需求推动了现代编程范式、跨平台兼容性的提升以及维护性的改进。利用大型语言模型（LLMs）自动化这一翻译过程已显示出潜力，但高质量、专业数据集的缺乏阻碍了其有效性。在本文中，我们通过引入一个专门为Fortran到C++代码迁移设计的创新多轮对话数据集Fortran2CPP来解决这一挑战。我们的数据集比现有替代品大得多，使用一个独特的LLM驱动的双代理管道生成，该管道结合了迭代编译、执行和代码修复，以确保高质量和功能正确性。为了展示我们数据集的有效性，我们在Fortran2CPP上对几个开放权重的LLMs进行了微调，并评估了它们在两个独立基准上的性能。在我们的数据集上进行微调带来了显著提升，模型在CodeBLEU评分上实现了高达3.31倍的提升，编译成功率提高了92%。这突显了数据集增强翻译的C++代码的语法准确性和可编译性的能力。我们的数据集和模型已开源，可在我们的公共GitHub仓库中获取[脚注：\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}]。**|
|**2024-12-27**|**Can Large Language Models Adapt to Other Agents In-Context?**|Matthew Riemer et.al.|[2412.19726](http://arxiv.org/abs/2412.19726)|null|随着研究界致力于构建更动态、更个性化的AI助手，以适应与人类互动的多样性，对评估大型语言模型（LLMs）的心智理论能力产生了更大的兴趣。确实，一些最近的研究表明，LLMs的心智理论能力相当令人印象深刻，接近人类水平的表现。我们的论文旨在反驳这种说法，并认为过去的研究并没有直接测量代理的表现，这可能导致了一些本质上具有幻觉性质的结果。我们区分了我们称之为字面心智理论和功能心智理论，即衡量代理预测他人行为的能力和根据对他人行为预测的理性反应来适应情境中的代理。我们发现，表现最出色的开源LLMs可能在字面心智理论方面表现出强大的能力，这取决于如何提示它们，但似乎在功能心智理论方面遇到困难——即使合作伙伴策略非常简单。我们的工作有助于突出LLMs在适应新情况时归纳偏见的两面性。虽然这种偏见可能在有限的范围内导致强大的表现，但它通常阻碍了达到最佳长期行为的收敛。|
|**2024-12-27**|**Toward Adaptive Reasoning in Large Language Models with Thought Rollback**|Sijia Chen et.al.|[2412.19707](http://arxiv.org/abs/2412.19707)|**[link](https://github.com/iQua/llmpebase)**|**大型语言模型（LLMs）通常用于通过逐步推理来解决各种任务。然而，中间推理步骤或思维的结构是刚性和单向的，例如链式、树形或无环有向图。因此，这种不灵活且仅向前推理的方法可能无法应对挑战性任务，并且在LLM频繁给出错误回答，即“幻觉”时可能会失败。本文提出了一种新的推理框架，称为思维回滚（TR），允许LLMs在解决“幻觉”问题时，自适应地构建思维结构，同时保持有效的推理。TR的核心机制是回滚思维，这使得LLMs能够对思维进行错误分析，并因此回滚到任何之前的错误思维进行修订。随后，通过将这种试错过程包含在提示中引导LLM，每次回滚都会导致一条更可靠的推理路径。因此，从没有人类标注的简单提示开始，带有TR的LLM自适应地逐步探索思维以找到正确解决方案。在数学问题和多任务推理上的综合实验表明，TR在解决问题的成功率和交互成本方面达到了最先进的水平。例如，在MATH数据集上，带有TR的GPT-4的解决率比当前最佳方案高出9%。**|
|**2024-12-27**|**A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization**|Jingchun Lian et.al.|[2412.19685](http://arxiv.org/abs/2412.19685)|null|图像伪造定位，即识别图像中的篡改像素，已经取得了显著的进展。传统方法通常将这一挑战视为图像分割的一种变体，将伪造区域的二值分割视为最终产品。我们认为基本的二值伪造掩码不足以解释模型的预测。它无法阐明模型为何指明某些区域以及如何对待所有伪造像素，这使得难以识别最不真实的部分。在本研究中，我们通过为伪造图像生成突出区域专注的解释来缓解上述局限性。为此，我们构建了一个多模态伪造追踪（MMTT）数据集，包括使用深度伪造技术处理的面部图像，并配以人工的、可解释的文本注释。为了获取高质量的注释，注释员被要求仔细观察被篡改的图像，并描述伪造区域的典型特征。随后，我们收集了128,303个图像-文本对的数据集。利用MMTT数据集，我们开发了ForgeryTalker，这是一种旨在同时进行伪造定位和解释的架构。ForgeryTalker首先训练一个伪造提示网络以识别解释文本中的关键线索。随后，区域提示被纳入多模态大型语言模型中进行微调，以实现定位和解释的双重目标。在MMTT数据集上进行的广泛实验验证了我们所提出模型的优势。该数据集、代码以及预训练的检查点将被公开提供，以促进进一步的研究并确保我们结果的复现性。|
|**2024-12-27**|**Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework**|Jiang Liu et.al.|[2412.19684](http://arxiv.org/abs/2412.19684)|null|高效多模态大型语言模型（EMLLMs）与多模态大型语言模型（MLLMs）相比，减少了模型大小和计算成本，通常部署在资源受限的设备上。然而，由于数据隐私的担忧，现有的开源EMLLMs在预训练过程中很少能够访问到私有领域特定数据，这使得它们难以直接应用于特定设备领域，例如某些商业场景。为了解决这一弱点，本文重点关注EMLLMs对私有领域的有效适应，特别是在以下两个领域：1）如何减少数据需求；2）如何避免参数微调。具体来说，我们提出了一种无调整、自适应、通用的提示优化框架，简称我们的方法，它包括两个阶段：1）预定义提示，基于强化搜索策略，生成提示优化策略树以获取优化先验；2）提示反思基于优化先验初始化提示，然后进行自我反思以进一步搜索和细化提示。通过这种方式，我们的方法巧妙地生成了处理私有领域特定数据的“理想提示”。请注意，我们的方法不需要参数微调，并且只需要少量数据就可以快速适应私有数据的数据分布。在多个任务上的大量实验表明，与基线相比，我们提出的方法在效率和性能方面都有显著提升。|
|**2024-12-27**|**CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs**|Siyu Wang et.al.|[2412.19663](http://arxiv.org/abs/2412.19663)|null|计算机辅助设计（CAD）通过实现精确的二维和三维建模、广泛的分析和优化，显著提高了设计过程的效率、准确性和创新性。现有的创建CAD模型的方法依赖于潜在向量或点云，这些方法难以获得且存储成本高昂。近期在多模态大型语言模型（MLLM）方面的进步激发了研究人员使用自然语言指令和图像来进行CAD模型构建。然而，这些模型在推断准确的3D空间位置和方向方面仍然存在困难，导致在确定构建几何体的空间3D起始点和挤出方向时出现误差。本研究引入了CAD-GPT，这是一种具有空间推理增强的MLLM的CAD合成方法，可以接受单个图像或文本描述作为输入。为了实现精确的空间推断，我们的方法引入了3D建模空间机制。该方法通过专用空间展开机制将3D空间位置和3D草图平面旋转角度映射到一维语言特征空间，同时将二维草图坐标离散化到适当的平面空间，以实现空间起始位置、草图方向和二维草图坐标平移的精确确定。大量实验表明，CAD-GPT在CAD模型合成方面，无论是从定量还是定性角度来看，都优于现有的最先进方法。|
|**2024-12-27**|**FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios**|Kaiyi Pang et.al.|[2412.19652](http://arxiv.org/abs/2412.19652)|null|语言隐写术将秘密信息嵌入看似无辜的文本中，以保护监视环境中的隐私。生成式语言隐写术利用语言模型（LM）的概率分布，并应用隐写算法生成隐写标记，随着大型语言模型（LLM）的近期进展而受到关注。为了提高安全性，研究人员开发了保持分布的隐写算法，以最小化隐写采样与LM采样之间的差距。然而，对语言模型分布的依赖，以及与真实世界掩护文本的偏差，导致在实际场景中面对隐写分析检测器时，隐写术的不可感知性不足。此外，LLM分布往往更确定，导致熵减少，从而降低嵌入容量。在本文中，我们提出了FreStega，这是一种即插即用方法，用于重建用于生成式语言隐写术的语言模型分布。FreStega在隐写文本自回归生成的每一步动态调整标记概率，利用了序列和空间维度。在序列调整中，根据瞬时熵动态调整温度，增强隐写文本的多样性并提高嵌入容量。在空间维度上，分布与目标领域语料库的指导对齐，紧密模仿目标领域的真实掩护文本。通过重塑分布，FreStega提高了实际场景中隐写文本的不可感知性，并通过提高15.41%的隐写能力，所有这些都不损害生成文本的质量。FreStega作为即插即用的补救措施，增强了现实场景中现有保持分布隐写术方法的不可感知性和嵌入容量。|
|**2024-12-27**|**Xmodel-2 Technical Report**|Wang Qun et.al.|[2412.19638](http://arxiv.org/abs/2412.19638)|**[link](https://github.com/xiaoduoailab/xmodel-2)**|Xmodel-2是一种专门为推理任务设计的1.2亿参数的大型语言模型。其架构允许不同规模的模型共享一组统一的超参数，从而可以在较小的模型上进行广泛的实验，并无缝地将最佳配置转移到较大的模型上。为了最大化训练效率和稳定性，Xmodel-2采用了MiniCPM中的WSD学习率调度器。在来自不同来源的1500亿个标记上预训练后，Xmodel-2在复杂推理和基于代理的任务中实现了最先进的性能，同时保持了低训练成本。这些结果突显了高效模型设计和训练策略在提升推理能力方面的潜力。模型检查点和代码在GitHub上公开，链接为https://github.com/XiaoduoAILab/Xmodel-2。|
|**2024-12-27**|**IMTP: Search-based Code Generation for In-memory Tensor Programs**|Yongwon Shin et.al.|[2412.19630](http://arxiv.org/abs/2412.19630)|null|DRAM-PIM（DRAM中的处理）技术已成为加速现代应用中内存密集型操作（如大型语言模型（LLMs））的有前途的技术。尽管其潜力巨大，但当前DRAM-PIM的软件堆栈面临重大挑战，包括依赖手动调整的库，这阻碍了可编程性、对高级抽象支持有限以及缺乏系统化的优化框架。为了解决这些限制，我们提出了IMTP，这是一个针对UPMEM的基于搜索的优化张量编译器。IMTP的关键特性包括：（1）自动搜索主机和内核张量程序的联合搜索空间，（2）针对PIM的优化以高效处理边界条件，以及（3）针对UPMEM系统扩展搜索空间的改进搜索算法。我们在UPMEM硬件上的实验结果表明，各种UPMEM基准核的性能提升可达8.21倍，GPT-J层可达5.33倍。据我们所知，IMTP是第一个为DRAM-PIM系统提供完全自动化、集成自调优代码生成支持的张量编译器。通过弥合高级张量计算抽象和底层硬件特定要求之间的差距，IMTP为提升DRAM-PIM的可编程性和实现优化流程的简化奠定了基础。|
|**2024-12-24**|**Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems**|Fernando Jia et.al.|[2412.18601](http://arxiv.org/abs/2412.18601)|**[link](https://github.com/FJDeFi/Decentralized-Intelligence-in-GameFi)**|**在游戏金融（GameFi）这一迅速发展的领域中，即游戏与去中心化金融（DeFi）的结合，迫切需要提高玩家参与度和游戏生态系统的经济互动。我们的GameFi生态系统旨在通过将先进的具身AI代理集成到GameFi平台中，从根本上改变这一领域。这些AI代理采用最前沿的大语言模型（LLMs）如GPT-4和Claude AI开发，能够与玩家进行主动、适应性强且情境丰富的互动。通过超越传统的脚本响应，这些代理成为游戏叙事和经济系统的重要参与者，直接影响玩家的策略和在游戏内的经济。我们解决了当前GameFi平台存在的局限性，这些平台通常缺乏沉浸式的AI互动和社区参与或创作者盈利机制。通过将AI代理与区块链技术的深度融合，我们建立了一个以共识驱动的去中心化GameFi生态系统。这个生态系统使创作者能够货币化他们的贡献，并促进玩家与创作者之间的民主合作。此外，通过将DeFi机制嵌入游戏体验中，我们增强了经济参与度，并为游戏内的金融互动提供了新的机会。我们的方法提高了玩家的沉浸感和留存率，通过将传统游戏与Web3技术相结合，推动了GameFi生态系统的发展。通过集成复杂的AI和DeFi元素，我们为创建更具吸引力、经济稳健且以社区为中心的游戏环境做出了贡献。这个项目代表了GameFi领域技术的重大进步，提供了可以在整个游戏行业应用的观点和方法。**|
|**2024-12-24**|**A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs**|OpenMind et.al.|[2412.18588](http://arxiv.org/abs/2412.18588)|null|大型语言模型（LLMs）是我们物理环境、动物和人类行为的全部公开知识的紧凑表示。将LLMs应用于机器人学可能为创建在大多数人类任务上表现优异且无需或仅需少量调整的强大机器人提供了一条途径。除了日益复杂的推理和任务规划之外，由（适当设计的）LLMs组成的网络提供了升级能力的便捷性，并允许人类直接观察机器人的思考。在这里，我们探讨了使用LLMs控制物理机器人的优势、局限性和特殊性。基本系统由四个通过人类语言数据总线通信的LLMs组成，该总线通过WebSocket和ROS2消息传递实现。令人惊讶的是，尽管机器人的数据融合周期仅运行在1Hz，中央数据总线运行在人类大脑的极低速率下，大约40位/秒，但仍然能够实现丰富的机器人行为和跨不同任务的良好性能。使用自然语言进行LLM间通信使得人类可以直接观察到机器人的推理和决策过程，并且用普通英语编写的规则集可以轻易地影响系统的行为。这些规则被不可更改地写入以太坊，这是一个全球性、公开性、抗审查的图灵完备计算机。我们建议，通过在相互作用的AI之间使用自然语言作为数据总线，并使用不可更改的公共账本来存储行为约束，可以构建出具有意外丰富性能、可升级性和与人类持久对齐的机器人。|
|**2024-12-24**|**Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control**|Sergey Sedov et.al.|[2412.18582](http://arxiv.org/abs/2412.18582)|null|提示微调是一种通过修改提示嵌入来高效适应新任务且计算开销最小的预训练语言模型的方法。在本研究中，我们探讨了Prompt-Tuning中常见的嵌入坍塌现象对于模型最终性能的重要性。为了回答这个问题，我们设计了嵌入先验，并将其与收敛的软提示和深度提示微调方法的后验进行了比较。我们的发现表明，先验强烈影响了调整嵌入的位置，并且模型可以有效地使用激活空间不同部分的嵌入，包括全新的区域。由于最终的提示微调能力有限，我们假设可控制的提示微调后验可以作为诸如思维链（COT）蒸馏等任务的良好起点。我们的实验还表明，生成的轨迹并未定位在模型的激活空间中。然而，对于不同任务（例如，NLP和算术）的激活存在明显的簇，而NLP任务之间的激活（例如，问答和掩码语言模型）则位于同一簇中。这些观察结果引发了关于单个激活簇对于大型语言模型泛化能力重要性的疑问。|
|**2024-12-24**|**Zero-resource Speech Translation and Recognition with LLMs**|Karel Mundnich et.al.|[2412.18566](http://arxiv.org/abs/2412.18566)|null|尽管语音处理领域取得了最近的发展，零资源语音翻译（ST）和自动语音识别（ASR）仍然是具有挑战性的问题。在这项工作中，我们提出利用多语言大型语言模型（LLM）在模型从未见过配对音频-文本数据的语言中执行ST和ASR。我们通过使用预训练的多语言语音编码器、多语言LLM和轻量级适应模块，将音频表示映射到LLM的标记嵌入空间来实现这一点。我们在ST和ASR中进行了多次实验，以了解如何最好地训练模型以及哪些数据对先前未见过的语言性能影响最大。在ST中，我们的最佳模型在CoVoST2上能够实现超过23的BLEU分数，对于两种先前未见过的语言；在ASR中，我们实现了高达28.2%的词错误率（WER）。最后，我们表明我们系统的性能受限于LLM输出所需语言文本的能力。|
|**2024-12-24**|**Distilling Fine-grained Sentiment Understanding from Large Language Models**|Yice Zhang et.al.|[2412.18552](http://arxiv.org/abs/2412.18552)|**[link](https://github.com/hitsz-hlt/fsa-distillation)**|**细粒度情感分析（FSA）旨在从大量带有观点的文本中提取和总结用户意见。最近的研究表明，大型语言模型（LLMs）具有卓越的情感理解能力。然而，直接将LLMs应用于FSA应用会带来高昂的推理成本。因此，本文研究了将LLMs中的细粒度情感理解蒸馏到小型语言模型（SLMs）中的方法。我们提示LLMs检查并解释给定评论的情感，然后利用生成的内容来预训练SLMs。此外，我们开发了一个全面的FSA基准来评估SLMs和LLMs。在这个基准上的大量实验揭示了：（1）蒸馏显著提高了SLMs在FSA任务中的性能，使 $F_1$ -score提高了6.00%，并且蒸馏后的模型仅用2200万个参数就能超越Llama-2-7b；（2）蒸馏使SLMs具备了出色的零样本情感分类能力，使它们能够匹配甚至超越其教师模型。这些结果表明，从LLMs中蒸馏是FSA领域一个非常有前景的方向。我们将在\url{https://github.com/HITSZ-HLT/FSA-Distillation}上发布我们的代码、数据和预训练模型权重。**|
|**2024-12-24**|**Token-Budget-Aware LLM Reasoning**|Tingxu Han et.al.|[2412.18547](http://arxiv.org/abs/2412.18547)|**[link](https://github.com/geniushtx/tale)**|**推理对于大型语言模型（LLMs）在广泛任务中表现出色至关重要。虽然像思维链（CoT）推理这样的方法通过将问题分解为中间步骤来增强LLM的性能，但它们也带来了显著的令牌使用开销，导致成本增加。我们发现，当前LLMs的推理过程不必要地冗长，通过在提示中包含合理的令牌预算可以将其压缩，但令牌预算的选择对实际压缩效果起着至关重要的作用。然后，我们提出了一种令牌预算感知的LLM推理框架，该框架根据推理复杂度动态估计不同问题的令牌预算，并使用估计的令牌预算来指导推理过程。实验表明，我们的方法在CoT推理中有效地减少了令牌成本，同时仅略有降低性能，为在LLM推理中平衡效率和精度提供了一个实用的解决方案。代码：https://github.com/GeniusHTX/TALE。**|
|**2024-12-24**|**PLD-Tree: Persistent Laplacian Decision Tree for Protein-Protein Binding Free Energy Prediction**|Xingjian Xu et.al.|[2412.18541](http://arxiv.org/abs/2412.18541)|null|近期，基于拓扑学的建模在物理建模和分子研究方面取得了显著进展，包括其在蛋白质-配体结合亲和力应用中的研究。在本工作中，我们引入了持久拉普拉斯决策树（PLD-Tree），这是一种旨在解决预测蛋白质-蛋白质相互作用（PPI）亲和力这一挑战性任务的新方法。PLD-Tree专注于蛋白质链的绑定界面，并使用持久拉普拉斯来捕捉反映关键蛋白质间相互作用的拓扑不变量。这些从持久同伦学中导出的拓扑描述符，通过整合来自大型语言模型的进化尺度建模（ESM）进一步得到增强，以整合基于序列的信息。我们在两个基准数据集PDBbind V2020和SKEMPI v2上验证了PLD-Tree，在复杂的留一法蛋白质去除交叉验证下，证明了相关系数（ $R_p$ ）为0.83。值得注意的是，我们的方法在这些数据集上优于所有已报道的最先进方法。这些结果强调了将机器学习技术与基于拓扑的描述符结合用于分子对接和虚拟筛选的强大能力，为预测蛋白质-蛋白质结合亲和力提供了一个稳健且准确的框架。|
|**2024-12-24**|**Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation**|Derong Xu Xinhang Li et.al.|[2412.18537](http://arxiv.org/abs/2412.18537)|**[link](https://github.com/Applied-Machine-Learning-Lab/AMAR)**|大型语言模型（LLMs）展现出惊人的能力，但在执行复杂知识推理任务时，却面临着幻觉和过时知识的困扰，导致输出结果存在事实错误。先前的研究试图通过从大规模知识图谱（KGs）中检索事实知识来协助LLMs进行逻辑推理和答案预测，以缓解这一问题。然而，这种做法往往引入噪声和不相关数据，尤其是在涉及多个知识方面的广泛背景情况下。这样一来，LLMs的注意力可能会被从问题和相关信息中误导。在我们的研究中，我们引入了一个自适应多方面检索增强知识图谱（Amar）框架。该方法检索包括实体、关系和子图的知识，并将检索到的每一篇文本转换为提示嵌入。Amar框架包含两个关键子组件：1）一个自对齐模块，它通过增强检索文本来对实体、关系和子图之间的共性进行对齐，从而减少噪声干扰；2）一个相关性门控模块，它使用软门来学习问题与多方面检索数据之间的相关性分数，以确定哪些信息应该用于增强LLMs的输出，甚至完全过滤掉。我们的方法在两个常见的数据集WebQSP和CWQ上实现了最先进的性能，与最佳竞争对手相比，准确率提高了1.9%，在逻辑形式生成方面比直接使用检索文本作为上下文提示的方法提高了6.6%。这些结果证明了Amar在提高LLMs推理能力方面的有效性。|
|**2024-12-24**|**Automated Code Review In Practice**|Umut Cihan et.al.|[2412.18531](http://arxiv.org/abs/2412.18531)|null|代码审查是一种广泛应用的提高软件质量和知识转移的实践。由于需要人工努力和可能出现的延误，它通常被视为耗时。Qodo、GitHub Copilot和Coderabbit等几个AI辅助工具，通过使用大型语言模型（LLMs）提供自动化的代码审查。这些工具在行业中的影响尚未得到考察。本研究考察了基于LLM的自动化代码审查工具在工业环境中的影响。研究在一个采用AI辅助审查工具（基于开源Qodo PR Agent）的软件开发环境中进行。大约238名来自十个项目的实践者可以使用这个工具。我们重点关注了三个项目，其中包含4,335个拉取请求，其中1,568个经历了自动化审查。数据收集包括三个来源：（1）对拉取请求数据的定量分析，包括表示开发者是否对自动化评论采取行动的评论标签，（2）向开发者发送的调查问卷，以了解他们对个别拉取请求的审查体验，以及（3）对22名实践者的更广泛调查，以了解他们对自动化审查的一般看法。73.8%的自动化评论得到了解决。然而，平均拉取请求关闭时间从5小时52分钟增加到8小时20分钟，不同项目之间存在不同的趋势。大多数实践者报告说，由于自动化审查，代码质量有所提高。基于LLM的工具在软件开发中证明是有用的，它增强了错误检测、提高了对代码质量的意识，并促进了最佳实践。然而，它也导致了拉取请求关闭时间的延长，并引入了诸如错误审查、不必要的纠正和不相关的评论等缺点。|
|**2024-12-24**|**Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving**|Hao Pang et.al.|[2412.18511](http://arxiv.org/abs/2412.18511)|null|深度强化学习（DRL）在自动驾驶决策方面展现出巨大的潜力。然而，由于学习效率低，DRL在复杂驾驶场景中实现合格策略需要大量的计算资源。此外，利用人类专家的指导来提高DRL的性能会带来高昂的劳动成本，这限制了其实际应用。在本研究中，我们提出了一种新型的大语言模型（LLM）引导的深度强化学习（LGDRL）框架，用于解决自动驾驶车辆的决策问题。在这个框架中，将基于LLM的驾驶专家集成到DRL中，为DRL的学习过程提供智能指导。随后，为了高效利用LLM专家的指导来提高DRL决策策略的性能，通过一种创新的专家策略约束算法和一种新的LLM干预交互机制，增强了DRL的学习和交互过程。实验结果表明，我们的方法不仅实现了90%的任务成功率，具有优越的驾驶性能，而且与最先进的基线算法相比，显著提高了学习效率和专家指导的利用率。此外，提出的方法使得DRL代理在缺乏LLM专家指导的情况下也能保持一致和可靠的表现。代码和补充视频可在https://bitmobility.github.io/LGDRL/找到。|
|**2024-12-23**|**ChatGarment: Garment Estimation, Generation and Editing via Large Language Models**|Siyuan Bian et.al.|[2412.17811](http://arxiv.org/abs/2412.17811)|null|我们引入了一种名为ChatGarment的新方法，该方法利用大型视觉-语言模型（VLMs）来自动化从图像或文本描述中估计、生成和编辑3D服装。与之前在现实场景中表现不佳或缺乏交互式编辑能力的方法不同，ChatGarment可以从野外图像或草图估计缝纫图案，从文本描述中生成它们，并根据用户指令编辑服装，所有这些都在交互式对话中完成。这些缝纫图案然后可以披覆成3D服装，它们易于动画化和模拟。这是通过微调VLM来直接生成一个JSON文件实现的，该文件包括服装类型和风格的文本描述，以及连续的数值属性。然后，通过编程参数模型使用这个JSON文件来创建缝纫图案。为此，我们通过扩大服装类型覆盖范围和简化其结构以提高VLM微调效率，对现有的编程模型GarmentCode进行了改进。此外，我们通过自动化数据管道构建了一个大规模的图像到缝纫图案和文本到缝纫图案对的数据库。广泛的评估展示了ChatGarment从多模态输入中准确重建、生成和编辑服装的能力，突显了其在时尚和游戏应用中颠覆工作流程的潜力。代码和数据将在https://chatgarment.github.io/提供。|
|**2024-12-23**|**ResearchTown: Simulator of Human Research Community**|Haofei Yu et.al.|[2412.17767](http://arxiv.org/abs/2412.17767)|**[link](https://github.com/ulab-uiuc/research-town)**|**大型语言模型（LLMs）在科学领域展现了惊人的潜力，然而一个基本问题仍未得到解答：我们能否用LLMs来模拟人类研究社区？回答这个问题可以加深我们对思想头脑风暴背后过程的了解，并激发自动发现新的科学洞见。在这项工作中，我们提出了ResearchTown，一个用于研究社区模拟的多智能体框架。在这个框架中，人类研究社区被简化并建模为一个智能体-数据图，其中研究人员和论文分别表示为智能体类型和数据类型节点，并基于他们的合作关系进行连接。我们还引入了TextGNN，一个基于文本的推理框架，将各种研究活动（例如，阅读论文、撰写论文和撰写评论）建模为在智能体-数据图上统一的消息传递过程的特殊形式。为了评估研究模拟的质量，我们提出了ResearchBench，这是一个使用节点掩码预测任务进行可扩展和客观评估的基准，基于相似性。我们的实验揭示了三个关键发现：（1）ResearchTown可以提供包括论文写作和评论写作在内的协作研究活动的现实模拟；（2）ResearchTown可以在多个研究人员和多样化的论文上保持稳健的模拟；（3）ResearchTown可以生成跨学科的研究想法，这些想法有可能激发新的研究方向。**|
|**2024-12-23**|**ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback**|Wei Zhang et.al.|[2412.17754](http://arxiv.org/abs/2412.17754)|null|大型语言模型（LLMs）在自然语言处理和编程方面取得了显著进展，但在处理复杂函数调用时仍存在鲁棒性和准确性问题。为了应对这些挑战，本文提出了一种名为ADC的创新方法，该方法增强了LLMs遵循函数格式和匹配复杂参数的能力。ADC利用了一个高质量的代码微调数据集，该数据集包含行级执行反馈，提供了细粒度的过程监督，从而促进了强大的逻辑推理和遵循函数格式的准确性。它还采用了一种对抗性数据集生成过程来改进参数匹配。分阶段训练方法利用了丰富的代码数据集和精炼的对抗性数据集，在伯克利函数调用排行榜（BFCL）基准测试中显著提高了函数调用能力。ADC的创新之处在于其战略性地结合了过程监督、对抗性精炼和增量学习，为LLMs在复杂函数调用方面的能力设定了新的标准。|
|**2024-12-23**|**Deliberation in Latent Space via Differentiable Cache Augmentation**|Luyang Liu et.al.|[2412.17747](http://arxiv.org/abs/2412.17747)|null|通过生成和关注中间推理步骤，使大型语言模型（LLMs）“思考更多”的技术在解决复杂问题方面显示出希望。然而，标准方法在回答前立即生成一系列离散的标记，因此它们可能会产生显著的延迟成本，并且难以优化。在这项工作中，我们证明了一个冻结的LLM可以通过一个离线协处理器来增强，该协处理器在模型的键值（kv）缓存上运行。这个协处理器通过一组设计用来提高后续解码保真度的潜在嵌入来增强缓存。我们使用标准预训练数据中的解码器语言建模损失来训练这个协处理器，同时将解码器本身冻结。这种方法使模型能够以端到端可微的方式学习如何将额外的计算蒸馏到其kv缓存中。因为解码器保持不变，协处理器可以离线异步运行，如果协处理器不可用或认为给定的缓存不需要额外计算，语言模型可以正常工作。我们通过实验表明，当缓存被增强时，解码器在许多后续标记上达到了更低的困惑度。此外，即使在没有任何特定任务训练的情况下，我们的实验表明，缓存增强可以持续降低困惑度并提高一系列推理密集型任务的表现。|
|**2024-12-23**|**YuLan-Mini: An Open Data-efficient Language Model**|Yiwen Hu et.al.|[2412.17743](http://arxiv.org/abs/2412.17743)|**[link](https://github.com/ruc-gsai/yulan-mini)**|**由于资源需求巨大和技术流程的复杂性，有效预训练大型语言模型（LLMs）一直具有挑战性。本文详细介绍了YuLan-Mini，这是一个具有2.42B参数的高性能基础模型，在同类参数规模的模型中取得了顶级性能。我们的预训练方法通过三个关键技术贡献来提高训练效率：一个详细的数据流程，将数据清洗与数据调度策略相结合；一种鲁棒的优化方法，以减轻训练的不稳定性；以及一种有效的退火方法，该方法结合了目标数据选择和长上下文训练。值得注意的是，YuLan-Mini在1.08T个标记上训练，其性能可与需要显著更多数据的行业领先模型相媲美。为了便于重现，我们发布了每个训练阶段数据组成的全部细节。项目详情可通过以下链接获取：https://github.com/RUC-GSAI/YuLan-Mini。**|
|**2024-12-23**|**Reasoning to Attend: Try to Understand How <SEG> Token Works**|Rui Qian et.al.|[2412.17741](http://arxiv.org/abs/2412.17741)|**[link](https://github.com/rui-qian/read)**|当前的大型多模态模型（LMMs）在视觉基座方面通常依赖于 $\texttt{<SEG>}$标记作为文本提示来联合优化视觉语言模型（例如LLaVA）和下游任务特定模型（例如SAM）。然而，我们观察到，很少有研究关注其工作原理。在这项工作中，我们首先可视化了相似度图，这些图是通过计算$\texttt{<SEG>}$标记和从LLaVA编码器和SAM解码器的最后一层隐藏层中提取的图像标记嵌入之间的语义相似度得到的。有趣的是，我们发现相似度图中的激活响应在一致性方面非常显著，这揭示了$\texttt{<SEG>}$标记所贡献的是图像-文本对之间的语义相似性。具体来说，$\texttt{<SEG>}$标记，一个在文本词汇表中扩展的占位符，在大型语言模型（LLMs）微调的同时，广泛查询各个分词图像块以匹配从文本到配对图像的物体语义。基于上述发现，我们提出了READ，它通过借鉴相似度图中高度激活的点来促进LMMs的$\textbf{REA}$soning能力，以确定在哪里进行$\textbf{D}$注意力。值得注意的是，READ具有直观的设计，即相似度作为点模块（SasP），它可以无缝地以即插即用的方式应用于类似$\texttt{<SEG>}$ 的范例。此外，已经在ReasonSeg和RefCOCO(+/g)数据集上进行了大量实验。为了验证READ在微调后是否遭受了先前技能的灾难性遗忘，我们还进一步评估了它在增强的FP-RefCOCO(+/g)数据集上的生成能力。所有代码和模型均公开提供在https://github.com/rui-qian/READ上。|
|**2024-12-23**|**Knowledge Editing through Chain-of-Thought**|Changyue Wang et.al.|[2412.17727](http://arxiv.org/abs/2412.17727)|**[link](https://github.com/bebr2/editcot)**|**大型语言模型（LLMs）在众多自然语言处理（NLP）任务中展现出了卓越的能力。然而，由于频繁重新训练的高成本，保持这些模型与不断发展的世界知识同步仍然是一个重大挑战。为了应对这一挑战，知识编辑技术应运而生，以便在不从头开始重建模型的情况下更新LLMs。在这些技术中，情境编辑范式因其在新知识整合中保持模型原始能力方面的有效性而脱颖而出。尽管具有潜力，现有的情境知识编辑方法通常针对特定任务，主要关注使用结构化知识三元组的多跳问答任务。此外，它们依赖于少量样本提示进行任务分解，这使得它们在泛化到不同任务时不够稳定和有效。针对这些局限性，我们提出了EditCoT，这是一种新颖的知识编辑框架，可以灵活且高效地更新LLMs，而无需重新训练。EditCoT通过为给定输入生成思维链（CoT），然后使用基于更新知识的CoT编辑器迭代地细化这一CoT过程来实现。我们在多个涵盖多种语言和任务的基准测试中评估了EditCoT。结果表明，我们的方法在性能上达到了最先进水平，同时与现有方法相比，具有更优的泛化能力、有效性和稳定性，标志着知识更新领域的一项重大进步。代码和数据可在以下链接获取：https://github.com/bebr2/EditCoT。**|
|**2024-12-23**|**Understanding the Logic of Direct Preference Alignment through Logic**|Kyle Richardson et.al.|[2412.17696](http://arxiv.org/abs/2412.17696)|null|近期，直接偏好对齐算法（DPA），如DPO，在将大型语言模型与人类偏好对齐方面显示出巨大的潜力。虽然这促使开发了原始DPO损失的新变体，但由于缺乏对这些算法潜在语义进行推理的技术和概念框架，理解这些最新提案之间的差异以及开发新的DPA损失函数仍然困难。在本文中，我们试图通过将DPA损失形式化为离散推理问题来解决这个问题。具体来说，我们提出以下问题：给定一个现有的DPA损失，我们能否系统地推导出一个表征其语义的符号表达式？两个损失的语义如何相互关联？我们提出了一种新的形式主义来表征基于单模型和参考模型的偏好损失，并确定了多种常用DPA变体的符号形式。进一步地，我们展示了这种偏好学习的形式观点如何为DPA损失空间的规模和结构提供新的见解，这使得我们不仅能够严格地描述最新损失提案之间的关系，还能够系统地探索这一空间并从第一原理推导出新的损失函数。我们希望我们的框架和发现能为从事人类AI对齐工作的人提供有用的指导。|
|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|大型语言模型（LLMs）的快速发展和部署带来了人工智能领域的新前沿，以其在自然语言理解和生成方面的前所未有的能力为标志。然而，这些模型越来越多地集成到关键应用中，引发了重大的安全担忧，需要对这些潜在风险及其缓解策略进行彻底的审查。本综述对LLM安全现状进行了全面概述，涵盖了四个主要类别：价值偏差、对抗攻击鲁棒性、滥用和自主人工智能风险。除了对这四个方面的缓解方法和评估资源的全面回顾外，我们还进一步探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提高LLM安全中的作用、一系列AI公司和研究机构提出的和遵守的LLM安全技术路线图，以及旨在实现LLM安全的AI治理，包括国际合作、政策建议和潜在的监管方向。我们的研究发现强调了采取积极、多方面的方法来确保LLM安全的必要性，强调了技术解决方案、伦理考量以及稳健的治理框架的整合。本综述旨在为学术界研究人员、行业实践者和政策制定者提供基础资源，提供有关将LLMs安全地融入社会的挑战和机遇的见解。最终，它旨在为LLMs的安全和有益发展做出贡献，与利用AI促进社会进步和福祉的总体目标保持一致。相关论文列表已在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers上公开。|
|**2024-12-23**|**Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models**|Sijbren van Vaals et.al.|[2412.17669](http://arxiv.org/abs/2412.17669)|**[link](https://github.com/sijbrenvv/completions_for_broca-s_aphasia)**|**布罗卡失语症是一种以非流畅、费力且碎片化言语产出为特征，同时相对较好的理解能力的失语症类型。由于传统的失语症治疗方法通常耗时、劳动密集，并且不能反映现实世界的对话，因此应用基于自然语言处理的方法，如大型语言模型（LLMs），可能有助于改善现有的治疗方法。为了解决这一问题，我们探讨了使用序列到序列LLMs来完成布罗卡失语症的碎片化句子。首先，我们使用一个旨在反映布罗卡失语症言语语言特征的规则系统生成合成布罗卡失语症数据。使用这些合成数据，我们随后对四个预训练的LLMs进行了微调，以完成碎片化句子的任务。我们在合成和真实的布罗卡失语症数据上评估了我们的微调模型。我们展示了LLMs重构碎片化句子的能力，模型在较长的输入话语中显示出性能的提升。我们的结果突出了LLMs在推进布罗卡失语症个体以及其他可能临床人群的交流辅助工具方面的潜力。**|
|**2024-12-20**|**HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding**|Chenxin Tao et.al.|[2412.16158](http://arxiv.org/abs/2412.16158)|null|大型语言模型（LLMs）的快速发展推动了视觉-语言模型（VLMs）的发展。单体VLMs通过避免特定模态的编码器，为组合型模型提供了有希望的替代方案，但面临着性能不足的挑战。大多数现有的单体VLMs需要调整预训练的LLMs以获得视觉能力，这可能会降低其语言能力。为了解决这一困境，本文提出了一种名为HoVLE的新型高性能单体VLM。我们注意到，当图像嵌入与文本嵌入对齐时，LLMs已被证明能够解释图像。当前单体VLMs的挑战实际上在于缺乏一个对视觉和语言输入都全面的嵌入模块。因此，HoVLE引入了一个全面的嵌入模块，将视觉和文本输入转换为共享空间，使LLMs能够以处理文本的方式处理图像。此外，精心设计了多阶段训练策略来增强全面的嵌入模块。首先，它被训练从预训练的视觉编码器中提炼视觉特征，从LLM中提取文本嵌入，使大规模训练能够使用未配对的随机图像和文本标记。整个模型进一步在多模态数据上进行下一标记预测以对齐嵌入。最后，还加入了指令调整阶段。我们的实验表明，HoVLE在各种基准测试上实现了接近领先组合模型的性能，并且比之前的单体模型大幅超越了它们。模型可在https://huggingface.co/OpenGVLab/HoVLE获取。|
|**2024-12-20**|**Offline Reinforcement Learning for LLM Multi-Step Reasoning**|Huaijie Wang et.al.|[2412.16145](http://arxiv.org/abs/2412.16145)|**[link](https://github.com/jwhj/oreo)**|为了快速适应复杂任务，提高大型语言模型（LLMs）的多步推理能力至关重要。虽然直接偏好优化（DPO）在使LLMs与人类偏好对齐方面显示出前景，但它不太适合多步推理任务，原因如下：（1）DPO依赖于成对偏好数据，这些数据对于多步推理任务并不容易获得；（2）它对所有标记进行统一处理，这使得它在多步推理任务中无法有效地进行信用分配，而这些任务往往伴随着稀疏的奖励。在本工作中，我们提出了OREO（离线推理优化），这是一种用于增强LLM多步推理能力的离线强化学习（RL）方法。基于最大熵强化学习的前期研究成果，OREO通过优化软贝尔曼方程共同学习策略模型和值函数。我们从原则上证明了它减少了收集成对数据的需求，并实现了更好的信用分配。在实证研究中，OREO在多步推理基准测试中超越了现有的离线学习方法，包括数学推理任务（GSM8K、MATH）和具身智能体控制（ALFWorld）。当有额外资源可用时，该方法可以扩展到多迭代框架。此外，学习到的值函数可以用来引导免费树搜索，这可以在测试时间进一步提升性能。|
|**2024-12-20**|**Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation**|Seyedreza Mohseni et.al.|[2412.16135](http://arxiv.org/abs/2412.16135)|null|恶意软件作者通常会使用代码混淆技术来使他们的恶意软件更难被检测。现有的生成混淆代码的工具通常需要访问原始源代码（例如C++或Java），并且添加新的混淆技术是一个复杂且劳动密集的过程。在这项研究中，我们提出了以下问题：大型语言模型（LLMs）能否潜在地生成新的混淆汇编代码？如果是的话，这将给反病毒引擎带来风险，并可能增加攻击者创建新混淆模式的能力。我们通过开发包含MetamorphASM数据集（MAD）以及三种代码混淆技术（无效代码、寄存器替换和控制流更改）的MetamorphASM基准来肯定地回答了这个问题。MetamorphASM系统地评估了LLMs使用MAD生成和分析混淆代码的能力，其中包含328,200个混淆汇编代码样本。我们发布了这个数据集，并分析了各种LLMs（例如GPT-3.5/4、GPT-4o-mini、Starcoder、CodeGemma、CodeLlama、CodeT5和LLaMA 3.1）生成混淆汇编代码的成功率。评估使用了既定的信息论指标和人工审查以确保正确性，并为研究人员研究和发展缓解这种风险提供了基础。源代码可以在以下GitHub链接找到：https://github.com/mohammadi-ali/MetamorphASM。|
|**2024-12-20**|**Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information**|Dirk Bergemann et.al.|[2412.16132](http://arxiv.org/abs/2412.16132)|null|我们研究了在代理对其偏好和共同收益相关状态都持有私人信息时的机制设计。我们表明，即使是在有利条件下，当代理具有多维类型时，标准的信息驱动机制也无法实现社会有效分配。为了克服这一局限性，我们提出了数据驱动机制，这些机制利用额外的分配后信息，该信息被建模为收益相关状态的估计器。我们的数据驱动机制扩展了经典的维克瑞-克拉克-格罗夫斯（VCG）类。我们表明，当状态完全揭示或效用在线性无偏估计器中时，它们在后验均衡中实现了精确实现。我们还表明，它们使用一致估计器实现了近似实现，随着估计器的收敛，趋近于精确实现，并给出了收敛率的界限。我们展示了这些机制在数字广告拍卖和基于大型语言模型（LLM）的机制中的应用，在这些应用中，用户参与自然地揭示了相关信息。|
|**2024-12-20**|**PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics**|Daniil Larionov et.al.|[2412.16120](http://arxiv.org/abs/2412.16120)|null|在自然语言处理（NLP）领域，评估机器生成自然语言内容的品质是一项具有挑战性的任务。近期，大型语言模型（LLMs）如GPT-4被用于此目的，但由于复杂评估提示需要大量标记的使用，它们在计算上非常昂贵。在本文中，我们提出了一种提示优化方法，该方法使用一个较小的、微调的语言模型来压缩用于评估提示的输入数据，从而在下游评估中使用大型LLMs时减少标记使用和计算成本。我们的方法包括一个两阶段微调过程：监督性微调随后是偏好优化，以根据人类偏好优化模型的输出。我们专注于机器翻译（MT）评估，并以GEMBA-MQM指标作为起点。我们的结果显示，在评估质量不受损失的情况下，标记使用量减少了2.37倍。这项工作使最先进的基于LLM的指标如GEMBA-MQM更具成本效益和效率，提高了其更广泛使用的可及性。|
|**2024-12-20**|**Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts**|Muhammad Abdullah Sohail et.al.|[2412.16119](http://arxiv.org/abs/2412.16119)|**[link](https://github.com/abdullahsohaill/cs6303-researchproject)**|**本研究探讨了大型语言模型（LLMs），特别是GPT-4o，在低资源脚本（如乌尔都语、阿尔巴尼亚语和塔吉克语）中的光学字符识别（OCR）潜力，以英语作为基准。研究使用精心整理的包含文本长度、字体大小、背景颜色和模糊度等可控变体的2,520张图像数据集，模拟了各种现实世界的挑战。结果表明，基于零样本LLM的OCR存在局限性，尤其是在语言复杂的脚本中，强调了标注数据集和微调模型的需求。这项工作强调了解决文本数字化中可及性差距的紧迫性，为未得到充分服务的语言铺平了通向包容性和稳健OCR解决方案的道路。**|
|**2024-12-20**|**PruneVid: Visual Token Pruning for Efficient Video Large Language Models**|Xiaohu Huang et.al.|[2412.16117](http://arxiv.org/abs/2412.16117)|**[link](https://github.com/visual-ai/prunevid)**|**本文介绍了一种名为PruneVid的视觉标记剪枝方法，旨在提高多模态视频理解的效率。由于大型语言模型（LLMs）在理解视觉模态方面的扩展能力，它们在视频任务中展现出了有希望的性能。然而，视频数据中的大量冗余给LLMs带来了显著的计算挑战。为了解决这个问题，我们提出了一种无需训练的方法，该方法包括：1）通过合并时空标记来最小化视频冗余，2）利用LLMs的推理能力，有选择地剪枝与问题标记相关的视觉特征，从而提高模型效率。我们在多个视频基准测试中验证了我们的方法，结果表明，PruneVid可以在保持与不同模型网络竞争性性能的同时，剪枝超过80%的标记。这突显了相较于现有剪枝方法，其卓越的有效性和效率。代码：https://github.com/Visual-AI/PruneVid。**|
|**2024-12-20**|**The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse**|Mahyar Habibi et.al.|[2412.16114](http://arxiv.org/abs/2412.16114)|null|关于如何在社交媒体上调节有毒言论以及内容监管如何影响在线讨论，目前存在持续的争议。我们提出并验证了一种使用计算语言学中的文本嵌入来衡量在线讨论中内容监管引起的扭曲的方法。我们在一个包含500万条美国政治推文的代表性数据集上测试了我们的测量方法，发现删除有毒推文会扭曲在线内容。这一发现在不同嵌入模型、毒性指标和样本中是一致的。重要的是，我们证明内容监管引起的扭曲并非由有毒语言造成。相反，我们表明，作为一种副作用，内容监管改变了嵌入空间的均值和方差，扭曲了在线内容的主题组成。最后，我们提出了一种替代内容监管的方法，该方法使用生成式大型语言模型重新措辞有毒推文，以保留其可挽救的内容，而不是完全删除。我们证明了这种重新措辞策略可以减少毒性，同时最大限度地减少在线内容的扭曲。|
|**2024-12-20**|**Logical Consistency of Large Language Models in Fact-checking**|Bishwamittra Ghosh et.al.|[2412.16100](http://arxiv.org/abs/2412.16100)|null|近年来，大型语言模型（LLMs）在执行各种自然语言任务方面取得了显著的成功，例如语言翻译、问答、总结、事实核查等。尽管LLMs在生成类似人类文本的能力上令人印象深刻，但它们因不一致的响应而闻名——输入查询的保留意义的变化导致不一致的响应，并归因于LLMs的漏洞，如幻觉、越狱等。因此，现有研究集中在基于简单释义的一致性评估上，而忽略了需要LLM有更好的逻辑推理理解的复杂查询。因此，我们的工作针对复杂逻辑查询下LLMs的逻辑不一致性，考虑了原始逻辑运算符，例如否定、合取和析取。作为一个测试平台，我们考虑了涉及从现实世界知识图谱（KGs）中提出的命题逻辑查询的事实核查任务中的检索增强LLMs。我们的贡献有三点。基准：我们引入了三个基于KGs的逻辑事实核查数据集，以促进社区向逻辑上一致的LLMs发展。评估：我们提出了基于命题逻辑查询的LLM一致性度量，并证明了现有LLMs缺乏逻辑一致性，尤其是在复杂查询上。改进：我们采用监督微调来提高LLMs在具有KG上下文的复杂事实核查任务上的逻辑一致性。|
|**2024-12-20**|**The Evolution of LLM Adoption in Industry Data Curation Practices**|Crystal Qian et.al.|[2412.16089](http://arxiv.org/abs/2412.16089)|null|随着大型语言模型（LLMs）在处理非结构化文本数据方面的能力日益增强，它们为提升数据整理工作流程提供了新的机遇。本文探讨了大型科技公司从业者对LLMs的采用演变，通过参与者的感知、整合策略和报告的使用场景评估了LLMs在数据整理任务中的影响。通过一系列调查、访谈和使用研究，我们提供了组织如何应对LLMs演变关键时刻的及时快照。在2023年第二季度，我们进行了一项调查以评估LLMs在行业开发任务中的采用情况（N=84），并在2023年第三季度组织了专家访谈以评估不断变化的数据需求（N=10）。在2024年第二季度，我们通过涉及两个基于LLMs的原型的用户研究（N=12）探索了从业者当前的LLMs使用情况及其预期。尽管每个研究都针对不同的研究目标，但它们共同揭示了一个关于LLMs使用演变的更广泛叙事。我们发现，数据理解正在从以启发式为主的自下而上的方法转变为以洞察力为主的、由LLMs支持的由上而下的工作流程。此外，为了应对更复杂的数据景观，数据从业者现在用LLM生成的“银色”数据集补充了传统的由主题专家创建的“金子”数据集，并通过多样化专家精心整理的严格验证的“超级金子”数据集。这项研究揭示了LLMs在大规模非结构化数据分析中的变革性作用，并突出了进一步工具开发的机遇。|
|**2024-12-19**|**OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving**|Shuo Xing et.al.|[2412.15208](http://arxiv.org/abs/2412.15208)|**[link](https://github.com/taco-group/openemma)**|**随着多模态大型语言模型（MLLMs）的出现，它们在众多现实应用领域产生了重大影响，特别是在自动驾驶（AD）领域。它们处理复杂视觉数据并推理复杂驾驶场景的能力为端到端自动驾驶系统开辟了新的范式。然而，开发端到端自动驾驶模型进展缓慢，因为现有的微调方法需要大量资源，包括强大的计算能力、大规模数据集和大量资金。受最近推理计算领域进步的启发，我们提出了OpenEMMA，这是一个基于MLLMs的开源端到端框架。通过整合思维链推理过程，OpenEMMA在利用各种MLLMs时相较于基线实现了显著提升。此外，OpenEMMA在各种具有挑战性的驾驶场景中展示了有效性、泛化能力和鲁棒性，为自动驾驶提供了一种更高效、更有效的方法。我们将所有代码发布在https://github.com/taco-group/OpenEMMA上。**|
|**2024-12-19**|**MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark**|Qihao Zhao et.al.|[2412.15194](http://arxiv.org/abs/2412.15194)|**[link](https://github.com/microsoft/mmlu-cf)**|**多选题数据集，如大规模多任务语言理解（MMLU），被广泛用于评估大型语言模型（LLMs）的常识、理解和问题解决能力。然而，这些基准的开放源代码性质以及LLMs训练数据的广泛来源不可避免地导致了基准污染，导致评估结果不可靠。为了缓解这个问题，我们提出了一种无污染且更具挑战性的多选题基准，称为MMLU-CF。该基准通过避免无意和恶意的数据泄露来重新评估LLMs对世界知识的理解。为了避免无意数据泄露，我们从更广泛的领域获取数据，并设计了三条去污染规则。为了防止恶意数据泄露，我们将基准分为难度和主题分布相似的验证集和测试集。测试集保持闭源状态以确保结果的可靠性，而验证集公开可用以促进透明度和独立验证。我们对主流LLMs的评估显示，强大的GPT-4o在测试集上仅实现了5次尝试的73.4%得分和0次尝试的71.9%得分，这表明我们创建更严格和无污染评估标准的方法是有效的。GitHub仓库可在https://github.com/microsoft/MMLU-CF找到，数据集可参考https://huggingface.co/datasets/microsoft/MMLU-CF。**|
|**2024-12-19**|**LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation**|Weijia Shi et.al.|[2412.15188](http://arxiv.org/abs/2412.15188)|null|我们提出了一种名为LlamaFusion的框架，该框架能够赋予预训练的纯文本大型语言模型（LLMs）多模态生成能力，使其能够理解和生成任意序列中的文本和图像。LlamaFusion利用Llama-3现有的权重进行文本的自回归处理，同时引入了额外的并行变换器模块来处理图像的扩散。在训练过程中，每个模态的数据被路由到其专门的模块：模态特定的前馈层、查询-键-值投影和归一化层独立处理每个模态，而共享的自注意力层允许文本和图像特征之间的交互。通过冻结文本特定模块，仅训练图像特定模块，LlamaFusion在保留纯文本LLMs的语言能力的同时，发展了强大的视觉理解和生成能力。与从头开始预训练多模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOPs，就提高了20%的图像理解能力和3.6%的图像生成能力，同时保持了Llama-3的语言能力。我们还展示了该框架可以适应具有多模态生成能力的现有视觉-语言模型。总的来说，这个框架不仅利用了现有的纯文本LLMs的计算投资，还实现了语言和视觉能力的并行发展，为高效的多模态模型开发提供了一个有希望的方向。|
|**2024-12-19**|**Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning**|Simon Frieder et.al.|[2412.15184](http://arxiv.org/abs/2412.15184)|null|常用的用于训练和评估基于人工智能的数学协同助手（主要是大型语言模型）的数学能力的数据集系列存在几个缺陷。这些局限性包括数学复杂性的范围受限，通常不超过本科低年级的数学水平，二元评分协议和其他问题，这使得基于证明的综合评估套件变得困难。我们系统地探讨了这些局限性，并认为提升大型语言模型或任何未来基于人工智能的数学助手（协同助手或“思维伙伴”）的能力，需要在数学数据集的设计和数学能力评估标准上实现范式转变：有必要从基于结果的数据集（定理陈述到定理证明）转向将数学研究实践的丰富方面转化为LLM可以训练的数据。这些包括数学工作流程（在创建新数学时通常执行的一系列原子任务，可能取决于子领域），这是证明发现过程的重要组成部分。此外，我们主张数学数据集开发者考虑G. Pólya于1949年提出的“有动机的证明”这一概念，它可以作为提供更好证明学习信号的数据集的蓝图，缓解一些已提到的局限性。最后，我们引入了数学数据表，扩展了通用的、数据集无关的数据表变体：我们提供了一个专门为数学数据集设计的问卷，我们敦促数据集创建者将其与数据集一起提供。这将使创作者意识到他们数据集的潜在局限性，同时使读者能够从训练和评估数学协同助手的视角轻松评估它。|
|**2024-12-19**|**HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages**|Aman Chaturvedi et.al.|[2412.15178](http://arxiv.org/abs/2412.15178)|null|大型语言模型（LLM）基础的编码工具作为软件开发助手取得了巨大成功，但它们通常是为通用编程任务设计的，在诸如高性能计算（HPC）等更专业领域表现不佳。为这些领域创建专门的模型和工具对于充分利用LLM在HPC等领域的优势至关重要。虽然先前的研究已经探索了HPC特定的模型，但LLM在生成并行代码方面仍然存在困难，而且目前并不清楚还有哪些障碍阻碍了这些LLM的发展，以及需要采取哪些措施来克服它们。在这项工作中，我们对调整专门HPC LLM的多个方面进行了深入研究，以更好地理解挑战。基于我们的发现，我们对一个专门的HPC LLM进行了微调和评估，该模型目前被证明是用于并行代码生成的最佳性能开源代码LLM。|
|**2024-12-19**|**Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying**|Federico Castagna et.al.|[2412.15177](http://arxiv.org/abs/2412.15177)|**[link](https://github.com/fcast07/cqot)**|**研究强调了尽管人工智能研究取得了突破性进展，但即使是最高水平的大语言模型（LLMs）在执行逻辑和数学推理时仍然存在困难。结果似乎表明，LLMs仍然作为（高度先进的）数据模式识别器工作，当尝试概括和解决模型以前从未见过或与训练数据中呈现的样本不相似的问题时，表现不佳。为了解决这一令人信服的问题，本文利用了论证理论文献中的关键问题概念，特别关注图尔敏的论证模型。我们表明，采用这些关键问题可以提高LLMs的推理能力。通过探究模型推理过程背后的理由，LLM可以评估是否发生了某些逻辑错误，并在向用户提供最终回复之前纠正它。这一基本思想源自任何有效论证程序的黄金标准：如果结论是由被接受的论据所蕴含的，则结论有效。或者，用这样的亚里士多德原则在现实世界的不完整信息和假设逻辑近似中表述，如果未被证明无效，则结论有效。这种方法成功地将模型输出引导通过推理管道，从而在基线和其思维链（CoT）实现方面取得了更好的性能。为此，本文在多个LLMs上对所提出的方案在MT-Bench推理和数学任务上的广泛评估提供了详细说明。**|
|**2024-12-19**|**Rethinking Uncertainty Estimation in Natural Language Generation**|Lukas Aichberger et.al.|[2412.15176](http://arxiv.org/abs/2412.15176)|null|大型语言模型（LLMs）在现实应用中越来越受欢迎，这推动了对其生成文本可信度的评估需求。为此，可靠的不确定性估计至关重要。由于当前LLMs通过随机过程自回归地生成文本，相同的提示可以导致不同的输出。因此，主要的不确定性估计方法生成并分析多个输出序列以确定LLM的不确定性。然而，生成输出序列在计算上非常昂贵，使得这些方法在规模上不切实际。在这项工作中，我们审视了领先方法的理论基础，并探索了提高其计算效率的新方向。基于正确评分规则框架，我们发现最有可能的输出序列的负对数似然构成了一个理论上有根据的不确定性度量。为了近似这个替代度量，我们提出了G-NLL，它具有仅使用贪婪解码生成的单个输出序列即可获得的优势。这使得不确定性估计更加高效和简单，同时保持了理论严谨性。实证结果表明，G-NLL在各种LLMs和任务上实现了最先进的性能。我们的工作为自然语言生成中的高效和可靠不确定性估计奠定了基础，挑战了当前领域主导的更复杂计算方法的需求。|
|**2024-12-19**|**Language Models as Continuous Self-Evolving Data Engineers**|Peidong Wang et.al.|[2412.15151](http://arxiv.org/abs/2412.15151)|null|大型语言模型（LLMs）在各项任务中展现出非凡的能力，但其进一步发展受到高质量训练数据缺乏的限制。此外，传统的训练方法过度依赖专家标注数据，给LLMs的性能设定了上限。为解决这一问题，我们提出了一种新型范式，允许LLMs通过自主生成、清洗、审查和标注带偏好信息的数据来自我训练，称为LANCE。我们的方法表明，LLMs可以作为持续自我进化的数据工程师，显著减少后训练数据构建过程的时间和成本。通过对Qwen2不同变体的迭代微调，我们验证了LANCE在各项任务中的有效性，表明它可以持续提升模型性能并保持高质量的数据生成。在八个基准维度上，LANCE使Qwen2-7B的平均得分提升了3.36分，使Qwen2-7B-Instruct的平均得分提升了2.70分。这种具有自主数据构建的训练范式不仅减少了对人专家或外部模型的依赖，还确保数据与人类价值观和偏好一致，为开发超越人类能力的高级智能系统铺平了道路。|
|**2024-12-19**|**Adaptive Pruning for Large Language Models with Structural Importance Awareness**|Haotian Zheng et.al.|[2412.15127](http://arxiv.org/abs/2412.15127)|null|近期大型语言模型（LLMs）的进步显著提升了语言理解和生成能力。然而，由于LLMs对计算和存储资源的高需求，它们在资源受限的边缘设备上的部署变得困难。为了解决这个问题，我们提出了一种新的LLM模型剪枝方法，称为结构感知自适应剪枝（SAAP），以显著降低计算和内存成本，同时保持模型性能。我们首先定义了一个自适应重要性融合指标，通过考虑它们的同方差不确定性来评估LLMs中所有耦合结构的重要性。然后，我们对所有模块的重要性进行排序，以确定应剪枝的具体层以满足特定的性能要求。此外，我们开发了一种新的分组微调策略，以提高LLMs的推理效率。最后，我们在两个常见任务上评估了所提出的SAAP方法，即零样本分类和文本生成。实验结果表明，我们的SAAP方法优于几种最先进的基线方法，在LLaMA-7B、Vicuna-7B和LLaMA-13B上分别实现了2.17%、2.37%和2.39%的准确率提升。此外，SAAP将标记生成速度提高了5%，展示了其在资源受限场景中的实际优势。|
|**2024-12-19**|**Outcome-Refining Process Supervision for Code Generation**|Zhuohao Yu et.al.|[2412.15118](http://arxiv.org/abs/2412.15118)|**[link](https://github.com/zhuohaoyu/orps)**|大型语言模型在代码生成方面展现出非凡的能力，但它们往往难以处理需要深度算法推理的复杂编程任务。虽然通过学习奖励模型进行过程监督在引导推理步骤方面显示出希望，但它需要昂贵的训练数据，且评估不可靠。我们提出了一种名为结果精炼过程监督的新范式，将结果精炼本身视为需要监督的过程。我们的框架利用具体执行信号来定位推理步骤的监督，同时使用树状结构探索来同时维护多个解决方案轨迹。实验表明，我们的方法即使对更小的模型也能在编程竞赛任务上实现高成功准确率和性能指标，比传统的奖励模型提供了更可靠的验证，且无需训练PRM。在我们的方法下，5个模型和3个数据集均实现了显著改进：正确性平均提高了26.9%，效率提高了42.2%。结果表明，提供具有具体验证信号的有序推理空间对于解决复杂编程任务至关重要。我们已在以下链接开源所有代码和数据：https://github.com/zhuohaoyu/ORPS|
|**2024-12-18**|**Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces**|Jihan Yang et.al.|[2412.14171](http://arxiv.org/abs/2412.14171)|**[link](https://github.com/vision-x-nyu/thinking-in-space)**|**人类拥有从连续视觉观察中记住空间的视觉空间智力。然而，在百万规模视频数据集上训练的多模态大型语言模型（MLLMs）也能从视频中“在空间中思考”吗？我们提出了一个包含超过5000个问答对的新型基于视频的视觉空间智力基准（VSI-Bench），并发现MLLMs表现出了具有竞争力的——尽管低于人类——视觉空间智力。我们探究了模型在语言和视觉上如何表达它们在空间中的思考方式，发现尽管空间推理能力仍然是MLLMs达到更高基准性能的主要瓶颈，但局部世界模型和空间意识确实存在于这些模型中。值得注意的是，现有的语言推理技术（例如，思维链、自我一致性、思维树）并不能提高性能，而在问答过程中明确生成认知图则增强了MLLMs的空间距离能力。**|
|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们每天都在与计算机互动，无论是在日常生活中还是在工作中，许多工作都可以完全通过计算机和互联网来完成。同时，得益于大型语言模型（LLMs）的改进，与周围环境互动并产生影响的AI代理也得到了快速发展。但是，AI代理在帮助加速甚至自主执行工作相关任务方面的表现如何？这个问题的答案对希望将AI引入工作流程的行业以及理解AI采用对劳动力市场可能产生的影响的经济政策具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进步，本文介绍了TheAgentCompany，这是一个可扩展的基准，用于评估以类似数字工作者方式与世界互动的AI代理：通过浏览网页、编写代码、运行程序以及与其他同事沟通。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司环境，并创建了各种可能由该公司员工执行的任务。我们测试了由封闭API-based和开放权重的语言模型（LMs）驱动的基线代理，并发现最具竞争力的代理可以自主完成24%的任务。这描绘了一幅关于LM代理任务自动化的复杂图景——在一个模拟真实工作场所的环境中，大量简单任务可以自主解决，但更困难的长远任务仍超出现有系统的能力范围。**|
|**2024-12-18**|**Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models**|Atin Sakkeer Hussain et.al.|[2412.14146](http://arxiv.org/abs/2412.14146)|null|本文介绍了用于数据分析中多步洞察合成的先进推理与转换引擎（ARTEMIS-DA），这是一个旨在增强大型语言模型（LLMs）以解决复杂、多步数据分析任务的创新框架。ARTEMIS-DA集成了三个核心组件：规划器，它将复杂用户查询分解为包含数据预处理、转换、预测建模和可视化的结构化、顺序指令；编码器，它动态生成并执行Python代码以实现这些指令；以及图形器，它解释生成的可视化以得出可操作的见解。通过协调这些组件之间的协作，ARTEMIS-DA有效地管理涉及高级推理、多步转换和跨不同数据模态综合的复杂分析工作流程。该框架在WikiTableQuestions和TabFact等基准测试中实现了最先进的（SOTA）性能，证明了其以精确性和适应性处理复杂分析任务的能力。通过结合LLMs的推理能力、自动代码生成与执行以及视觉分析，ARTEMIS-DA为多步洞察合成提供了一个强大、可扩展的解决方案，解决了数据分析中的广泛挑战。|
|**2024-12-18**|**LLMs can realize combinatorial creativity: generating creative ideas via LLMs for scientific research**|Tianyang Gu et.al.|[2412.14141](http://arxiv.org/abs/2412.14141)|null|科学理念生成在创造力理论和计算创造力研究中得到了广泛研究，为理解和实施创造性过程提供了有价值的框架。然而，近年来使用大型语言模型（LLMs）进行研究理念生成的相关工作往往忽视了这些理论基础。我们提出了一种框架，该框架明确地利用LLMs实施组合创造力理论，包括一个用于跨领域知识发现的通用化检索系统和用于理念生成的结构化组合过程。检索系统将不同抽象层次的概念映射到一起，以实现不同领域之间的有意义联系，而组合过程则系统地分析和重新组合组件以生成新颖的解决方案。在OAG-Bench数据集上的实验表明，我们的框架非常有效，在生成符合实际研究发展的理念方面，持续优于基线方法（在多个指标上提高了7%-10%的相似度分数）。我们的结果为LLMs在适当的理论框架指导下实现组合创造力提供了强有力的证据，既促进了AI辅助研究的实际进步，也加深了对机器创造力的理论理解。|
|**2024-12-18**|**Design choices made by LLM-based test generators prevent them from finding bugs**|Noble Saji Mathews et.al.|[2412.14137](http://arxiv.org/abs/2412.14137)|null|随着大量关于使用大型语言模型（LLM）进行自动化测试用例生成的研究和商业工具的出现，本文批判性地审视了近期基于LLM的测试生成工具，如Codium CoverAgent和CoverUp，是否能够有效地发现错误或无意中验证错误代码。考虑到错误只有在失败的测试用例中才会暴露，我们探讨了以下问题：当这些工具的测试预言机旨在通过时，它们是否真的能够实现软件测试的预期目标？使用真实的人类编写的错误代码作为输入，我们评估了这些工具，展示了LLM生成的测试如何未能检测到错误，并且更令人担忧的是，它们的设计如何通过验证生成的测试套件中的错误并拒绝揭示错误的测试而使情况恶化。这些发现引发了关于LLM基于测试生成工具设计有效性的重要问题，以及它们对软件质量和测试套件可靠性的影响。|
|**2024-12-18**|**Alignment faking in large language models**|Ryan Greenblatt et.al.|[2412.14093](http://arxiv.org/abs/2412.14093)|**[link](https://github.com/redwoodresearch/alignment_faking_public)**|**我们展示了一个大型语言模型进行对齐造假的现象：在训练过程中选择性遵守其训练目标，以防止其行为在训练后发生变化。首先，我们给Claude 3 Opus一个系统提示，表示它正在接受训练以回答所有查询，即使是有害的，这与它之前的训练拒绝此类查询的目标相冲突。为了使模型推断出它正在训练中，我们说它只会在与免费用户进行的对话中接受训练，而不是付费用户。我们发现模型有14%的时间会遵守来自免费用户的有害查询，而对于付费用户则几乎不会。解释这个差距，在模型遵守来自免费用户的有害查询的几乎所有情况下，我们观察到明确的对齐造假推理，模型表示它在训练中战略性地回答有害查询，以保持其训练后偏好无害的行为。接下来，我们研究了一个更现实的设置，其中训练过程中信息不是通过系统提示提供，而是通过在模仿预训练数据的合成文档上进行训练来提供——并观察到类似的对齐造假现象。最后，我们研究了通过强化学习训练模型以遵守有害查询的效果，我们发现这提高了对齐造假推理的比率至78%，尽管这也增加了训练后的合规性。我们还观察到其他行为，例如当给模型提供轻松的机会时，它会泄露其权重。虽然我们通过告诉模型何时以及根据什么标准进行训练，使其对齐造假变得更容易，但我们并没有指示模型进行对齐造假，或者给它任何明确的目标。由于未来的模型可能会在没有被告知的情况下推断有关其训练过程的信息，我们的结果表明，未来模型存在对齐造假的风险，无论这种风险是由于良性的偏好（如本例中）还是其他原因。**|
|**2024-12-18**|**Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report**|Markus Dablander et.al.|[2412.14085](http://arxiv.org/abs/2412.14085)|null|视频游戏是人工智能（AI）系统的一个自然且协同的应用领域，它既可以为玩家体验和沉浸感提供增强的可能性，同时还能为推进人工智能技术在一般领域的发展提供有价值的基准和虚拟环境。本报告从当前研究背景出发，对将最先进的AI方法，特别是深度学习，应用于数字游戏的五个有前景的研究途径进行了概述。本工作的目标是概述一个精心挑选的、非详尽的、鼓励性的研究方向列表，这些方向在人工智能和视频游戏的交汇处，可能有助于激发未来更加严谨和全面的研究努力。我们讨论了以下五个方面：（一）研究大型语言模型作为游戏代理建模的核心引擎；（二）利用神经网络细胞自动机进行程序化游戏内容生成；（三）通过深度代理建模加速计算成本高昂的游戏模拟；（四）利用自监督学习获取有用的视频游戏状态嵌入；（五）使用未标记的视频数据训练交互式世界的生成模型。我们还简要讨论了将高级深度学习系统集成到视频游戏开发中当前所面临的技术挑战，并指出未来可能带来更多进展的关键领域。|
|**2024-12-18**|**Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification**|Kyle Thompson et.al.|[2412.14063](http://arxiv.org/abs/2412.14063)|**[link](https://github.com/rkthomps/coq-modeling)**|使用证明辅助工具（如Coq）进行形式验证可以创建高质量的软件。然而，验证过程需要大量的专业知识和手动编写证明的努力。最近的研究探索了利用机器学习和大型语言模型（LLMs）自动化证明合成。这项工作表明，识别相关前提，如引理和定义，有助于合成。我们提出了Rango，这是一个为Coq提供的全自动证明合成工具，它能够自动识别相关前提，并从当前项目中识别相似的证明，在合成过程中使用它们。Rango在证明的每一步都使用检索增强来自动确定在它微调的LLM的上下文中包含哪些证明和前提。通过这种方式，Rango能够适应项目和证明的演变状态。我们创建了一个新的数据集CoqStoq，包含来自GitHub的2,226个开源Coq项目和196,929个定理，该数据集既包括训练数据，也包括精心策划的、维护良好的项目的评估基准。在这个基准上，Rango为32.0%的定理合成了证明，比之前最先进的工具Tactician多出了29%的定理。我们的评估还显示，将相关证明添加到Rango的上下文中，导致已证明的定理数量增加了47%。|
|**2024-12-18**|**Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets**|Simon Thorne et.al.|[2412.14062](http://arxiv.org/abs/2412.14062)|null|生成式人工智能和大型语言模型（LLMs）在自动化电子表格公式创建方面具有潜力。然而，由于幻觉、偏见和用户技能的差异性，从生成式人工智能获得的输出不能被假设是准确或可信的。为了解决这些挑战，本文提出了一种基于评估公式的透明度和可靠性的可信度框架。公式的透明度通过可解释性（理解公式的推理）和可见性（检查底层算法）来探索。生成的公式的可靠性从可靠性（一致性和准确性）和伦理考量（偏见和公平性）两个方面进行评估。论文还考察了这些指标背后的驱动因素，包括幻觉、训练数据偏见和构建不良的提示。最后，考虑了对技术的不信任示例，并探讨了其后果。|
|**2024-12-18**|**A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future**|Shilin Sun et.al.|[2412.14056](http://arxiv.org/abs/2412.14056)|**[link](https://github.com/shilinsun/mxai_review)**|**人工智能（AI）通过计算能力的提升和海量数据集的增长而迅速发展。然而，这种进步也加剧了对AI模型“黑盒”特性的解释挑战。为了解决这些担忧，可解释人工智能（XAI）应运而生，其重点是透明度和可解释性，以增强人类对AI决策过程的了解和信任。在多模态数据融合和复杂推理场景的背景下，多模态可解释人工智能（MXAI）的提出将多种模态集成到预测和解释任务中。同时，大型语言模型（LLMs）的出现导致了自然语言处理领域的重大突破，但它们的复杂性进一步加剧了MXAI的问题。为了深入了解MXAI方法的发展并提供构建更透明、公平和值得信赖的AI系统的关键指导，我们从历史角度回顾了MXAI方法，并将它们分为四个时期：传统机器学习、深度学习、判别性基础模型和生成型LLMs。我们还回顾了MXAI研究中使用的评估指标和数据集，最后讨论了未来的挑战和方向。与本次回顾相关的一个项目已创建在https://github.com/ShilinSun/mxai_review。**|
|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身智能体在执行复杂指令方面具有强大的能力，为具身机器人的潜在部署铺平了道路。然而，一个可预见的问题是，这些具身智能体也可能完美地执行一些危险任务，可能导致现实世界中的损害。为了研究这一问题，我们提出了SafeAgentBench——一个用于具身LLM智能体安全任务规划的新的基准。SafeAgentBench包括：（1）一个包含750个任务的新数据集，涵盖了10种潜在危险和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的基础具身环境，支持多智能体执行，并为8个最先进的基线提供了17个高级动作；（3）从执行和语义角度的可靠评估方法。实验结果表明，表现最佳的基线在安全任务中的成功率达到了69%，但在危险任务中的拒绝率仅为5%，这表明存在显著的安全风险。更多细节和代码可在https://github.com/shengyin1224/SafeAgentBench找到。**|
|**2024-12-17**|**DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation**|Miriam Wanner et.al.|[2412.13175](http://arxiv.org/abs/2412.13175)|null|分解后验证策略用于验证大型语言模型（LLM）生成的陈述，将陈述分解后再进行独立验证。去上下文化通过增强文本（陈述）确保其可以在原始语境之外进行验证，从而实现可靠的验证。虽然分解和去上下文化已被独立探索，但它们在完整系统中的相互作用尚未得到研究。它们相互冲突的目的可能产生紧张关系：分解将原子事实隔离，而去上下文化插入相关信息。此外，去上下文化的子陈述对验证步骤构成挑战：现在包含多个原子事实的增强文本中，应该验证哪一部分？我们对不同的分解、去上下文化和验证策略进行了评估，发现策略的选择会影响最终的事实性评分。此外，我们引入了DnDScore，一种去上下文化感知的验证方法，它将在上下文信息的背景下验证子陈述。|
|**2024-12-17**|**Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study**|Bolei Ma et.al.|[2412.13169](http://arxiv.org/abs/2412.13169)|**[link](https://github.com/soda-lmu/llm-opinion-german)**|**在最近的研究中，大型语言模型（LLMs）被越来越多地用于研究公众意见。本研究调查了LLMs的算法忠实度，即复制人类参与者社会文化背景和细微意见的能力。利用德国纵向选举研究（GLES）的开源问卷调查数据，我们将不同LLMs提示为通过将人口统计学特征纳入角色提示中，生成反映德国不同亚群体的合成公众意见。我们的结果显示，Llama在代表亚群体方面表现优于其他LLMs，尤其是在这些群体内部意见多样性较低时。我们的发现还进一步揭示，LLMs在代表左翼政党如绿党和左翼的支持者方面表现优于其他政党，与右翼政党AfD的匹配度最低。此外，在提示中包含或排除特定变量可以显著影响模型的预测。这些发现强调了将LLMs与更有效地模拟多样化公众意见相一致的重要性，同时最小化政治偏见并增强代表性稳健性。**|
|**2024-12-17**|**C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System**|Parker Addison et.al.|[2412.13163](http://arxiv.org/abs/2412.13163)|null|在寻求利用大型语言模型（LLMs）进行知识查询和分析的组织中，常常面临保持针对特定、最新信息的LLM微调，以确保答案相关性和实际性的挑战。检索增强生成（RAG）迅速成为解决组织维护专有模型挑战的可行方案，同时有助于减少LLM查询响应中的幻觉。然而，RAG在扩展数据管道以支持分层访问和不同数据源方面存在自身问题。在许多情况下，需要查询多个数据孤岛以提供更丰富、更相关的上下文供LLM使用。在组织信任边界内及之间分析数据源往往受到复杂的数据共享政策限制，禁止集中式数据存储，从而阻碍了RAG解决方案的快速和有效设置及扩展。在本文中，我们引入了机密计算（CC）技术作为安全联邦检索增强生成（FedRAG）的解决方案。我们提出的机密联邦RAG系统（C-FedRAG）通过确保上下文机密性，使RAG工作流程能够在数据提供者去中心化网络中安全连接和扩展。我们还展示了如何使用NVIDIA FLARE SDK实现C-FedRAG系统，并使用MedRAG工具包和MIRAGE基准数据集对其性能进行评估。|
|**2024-12-17**|**SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba, 2015）对于大型语言模型的成功至关重要。然而，它们在训练过程中维持额外的移动平均状态，导致内存需求比模型大几倍。这种开销对可扩展性和计算效率施加了限制。另一方面，虽然随机梯度下降（SGD）在内存效率方面是最优的，但它们在LLM训练中的能力有限（Zhao等，2024b）。为了解决这一困境，我们表明预处理SGD足以在LLMs上达到Adam级别的性能。具体来说，我们提出使用两个简单的算子对瞬时随机梯度进行预处理： $\mathtt{GradNorm}$和$\mathtt{GradWhitening}$。$\mathtt{GradNorm}$稳定梯度分布，而$\mathtt{GradWhitening}$对抗损失景观的局部曲率，分别。这导致了SWAN（带有白化和归一化的SGD），一种消除存储任何累积状态变量需要的随机优化器。经验表明，SWAN具有与SGD相同的内存占用，与Adam相比，总端到端内存减少了$\approx 50\%$ 。在语言建模任务中，SWAN在性能上与Adam相同，甚至有显著提高。具体来说，当使用350M和1.3B参数预训练LLaMa模型时，SWAN通过在不到一半的观察到的标记内达到相同的评估困惑度，实现了2倍的速度提升。|
|**2024-12-17**|**Are Your LLMs Capable of Stable Reasoning?**|Junnan Liu et.al.|[2412.13147](http://arxiv.org/abs/2412.13147)|**[link](https://github.com/open-compass/gpassk)**|**大型语言模型（LLMs）的快速发展在复杂推理任务中展示了显著的进步。然而，基准性能和现实应用之间存在显著的差距。我们认定这一差距主要源于当前的评价协议和指标，它们无法充分捕捉LLMs的全部能力，尤其是在既需要准确性又需要一致性的复杂推理任务中。本研究做出了两项关键贡献。首先，我们引入了G-Pass@k，这是一种新颖的评价指标，它提供对模型性能在多次采样尝试中的连续评估，量化了模型的峰值性能潜力和其稳定性。其次，我们提出了LiveMathBench，这是一个动态基准，包含了具有挑战性的现代数学问题，旨在在评估过程中最大限度地减少数据泄露风险。通过在LiveMathBench上对最先进的LLMs进行G-Pass@k的大量实验，我们提供了对它们的最大能力和运行一致性的全面洞察。我们的发现揭示了LLMs在“现实”推理能力方面有巨大的改进空间，突显了需要更稳健的评价方法。基准和详细结果可在以下网址获取：https://github.com/open-compass/GPassK。**|
|**2024-12-17**|**AI PERSONA: Towards Life-long Personalization of LLMs**|Tiannan Wang et.al.|[2412.13103](http://arxiv.org/abs/2412.13103)|null|在本研究中，我们提出了终身个性化大型语言模型的任务。虽然近年来LLM社区的主流工作主要集中在扩展数据和计算能力以提高LLM的能力，但我们认为，使LLM系统或语言代理能够持续适应每个独特用户的多样化和不断变化的个人资料，并提供最新的个性化援助也非常重要。我们提供了明确的任务表述，并介绍了一个简单、通用、有效且可扩展的框架，用于LLM系统和语言代理的终身个性化。为了促进对LLM个性化未来的研究，我们还介绍了合成现实基准和鲁棒评估指标的方法。我们将发布构建和基准测试终身个性化LLM系统的所有代码和数据。|
|**2024-12-17**|**AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark**|Jianlyu Chen et.al.|[2412.13102](http://arxiv.org/abs/2412.13102)|**[link](https://github.com/air-bench/air-bench)**|**评估在信息检索（IR）模型的发展中起着至关重要的作用。然而，当前基准测试，基于预定义的领域和人工标注的数据，在以经济高效的方式应对新兴领域的评估需求方面存在局限性。为了解决这一挑战，我们提出了自动异构信息检索基准（AIR-Bench）。AIR-Bench有三个关键特点：1）自动化。AIR-Bench中的测试数据由大型语言模型（LLMs）自动生成，无需人工干预。2）异构。AIR-Bench中的测试数据针对不同的任务、领域和语言生成。3）动态。AIR-Bench涵盖的领域和语言不断扩展，为社区开发者提供一个越来越全面的评估基准。我们开发了一个可靠且稳健的数据生成管道，基于真实世界的语料库自动创建多样化的高质量评估数据集。我们的研究发现，AIR-Bench中生成的测试数据与人工标注的测试数据高度一致，使AIR-Bench成为评估IR模型的可靠基准。AIR-Bench的资源可在https://github.com/AIR-Bench/AIR-Bench公开获取。**|
|**2024-12-17**|**Modality-Inconsistent Continual Learning of Multimodal Large Language Models**|Weiguo Pian et.al.|[2412.13050](http://arxiv.org/abs/2412.13050)|null|在这篇论文中，我们介绍了模态不一致持续学习（Modality-Inconsistent Continual Learning，简称MICL），这是一种针对多模态大型语言模型（Multimodal Large Language Models，简称MLLMs）的新持续学习场景，涉及具有不一致模态（图像、音频或视频）和不同任务类型（字幕或问答）的任务。与现有的仅限于视觉或模态增量设置不同，MICL结合了模态和任务类型的转变，这两种转变都会导致灾难性遗忘。为了应对这些挑战，我们提出了MoInCL，它采用了一个伪目标生成模块来减轻先前看到的模态中由于任务类型转变引起的遗忘。此外，它还结合了基于指令的知识蒸馏，以保留模型处理先前学习的模态的能力，当引入新的模态时。我们使用总共六个任务对MICL进行基准测试，并通过实验验证了我们提出的MoInCL的有效性。实验结果突出了MoInCL的优越性，显示出与代表性持续学习基线相比的显著改进。|
|**2024-12-17**|**OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain**|Shuting Wang et.al.|[2412.13018](http://arxiv.org/abs/2412.13018)|**[link](https://github.com/ruc-nlpir/omnieval)**|**作为大型语言模型（LLMs）的一种典型和实际应用，检索增强生成（RAG）技术受到了广泛关注，尤其是在LLMs可能缺乏特定领域知识的垂直领域。在本文中，我们引入了一个金融领域的全向和自动RAG基准，称为OmniEval。我们的基准以其多维评估框架为特点，包括以下方面：  （1）基于矩阵的RAG场景评估系统，将查询分为五个任务类别和16个金融主题，从而对多样化的查询场景进行结构化评估； （2）多维评估数据生成方法，结合基于GPT-4的自动生成和人工标注，在人工评估生成的实例中实现了87.47%的接受率； （3）多阶段评估系统，评估检索和生成性能，对RAG流程进行综合评估； （4）从基于规则和基于LLM的评估指标中衍生出的鲁棒评估指标，通过人工标注和LLM评估器的监督微调增强了评估的可靠性。  我们的实验证明了OmniEval的全面性，包括广泛的测试数据集，并突出了RAG系统在各个主题和任务上的性能差异，揭示了RAG模型在垂直领域提高其能力的重要机会。我们将基准的代码开源至\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}。**|
|**2024-12-16**|**SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**|Guoxuan Chen et.al.|[2412.12094](http://arxiv.org/abs/2412.12094)|**[link](https://github.com/HKUDS/SepLLM)**|**大型语言模型（LLMs）在自然语言处理的各个任务上表现出色。然而，它们庞大的体积带来了巨大的挑战，特别是在计算需求和推理速度方面，这是由于它们的二次复杂性。在这项工作中，我们识别出一个关键模式：某些看似无意义的特殊标记（即分隔符）与语义上有意义的标记相比，对注意力分数的贡献不成比例。这一观察表明，这些分隔符之间的段信息可以有效地压缩到这些分隔符本身，而不会造成显著的信息损失。受此启发，我们引入了SepLLM，这是一个即插即用的框架，通过压缩这些段和消除冗余标记来加速推理。此外，我们还实现了高效的内核以加速训练。在无需训练、从头开始训练和训练后设置中的实验结果表明了SepLLM的有效性。值得注意的是，使用Llama-3-8B骨干网络，SepLLM在GSM8K-CoT基准测试上实现了超过50%的KV缓存减少，同时保持了可比的性能。此外，在流式设置中，SepLLM能够有效地处理长达400万个标记或更多的序列，同时保持一致的语言建模能力。**|
|**2024-12-16**|**Instruction-based Image Manipulation by Watching How Things Move**|Mingdeng Cao et.al.|[2412.12087](http://arxiv.org/abs/2412.12087)|null|本文介绍了一种新型的数据集构建流程，该流程从视频中采样帧对，并使用多模态大型语言模型（MLLMs）生成编辑指令，用于训练基于指令的图像操作模型。视频帧天生保留了主题和场景的身份，确保了编辑过程中的内容一致性。此外，视频数据捕捉到了多样、自然的动态，如非刚性主题运动和复杂的摄像机运动，这些动态在其他情况下难以建模，使其成为可扩展数据集构建的理想来源。采用这种方法，我们创建了一个新的数据集，用于训练InstructMove模型，该模型能够执行基于指令的复杂操作，这些操作在合成数据集中难以实现。我们的模型在调整主题姿势、重新排列元素和改变摄像机视角等任务中展现了最先进的性能。|
|**2024-12-16**|**CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology**|Yuxuan Sun et.al.|[2412.12077](http://arxiv.org/abs/2412.12077)|null|大型多模态模型（LMMs）的出现为病理学带来了重大进步。以往的研究主要侧重于分别训练补丁级和全切片图像（WSI）级模型，这限制了在不同补丁和WSI之间整合所学知识，并导致了冗余模型。在本研究中，我们介绍了CPath-Omni，这是第一个设计用于统一补丁级和WSI级图像分析的150亿参数LMM，它将包括分类、视觉问答、字幕和视觉提示等多种任务在这一水平和另一水平上整合。大量实验表明，CPath-Omni在42个数据集中的39个任务上实现了最先进的（SOTA）性能，超过了为单个任务训练的特定模型的表现或与之相当。此外，我们为CPath-Omni开发了一个专门的病理学CLIP视觉处理器，CPath-CLIP，它首次将不同的视觉模型结合在一起，并将一个大语言模型作为文本编码器整合进去，构建了一个更强大的CLIP模型，在九个零样本和四个小样本数据集上实现了SOTA性能。我们的研究结果突出了CPath-Omni统一多种病理学任务的能力，展示了其在简化并推进病理学基础模型领域的潜力。|
|**2024-12-16**|**CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding**|Guo Chen et.al.|[2412.12075](http://arxiv.org/abs/2412.12075)|null|现有的多模态大型语言模型（MLLM）的视频理解基准大多仅针对短视频。对于长视频理解，现有的基准往往仅依赖于多项选择题（MCQs）。然而，由于基于MCQ的评估固有的局限性以及MLLM推理能力的增强，模型可以通过结合短视频理解和排除法来给出当前答案，而不真正理解视频内容。为了解决这一差距，我们引入了CG-Bench，这是一个专为长视频中的线索-基础问答而设计的创新基准。CG-Bench强调模型检索相关线索的能力，增强评估的可信度。它具有1,219个手动编纂的视频，按14个主要类别、171个次要类别和638个三级类别进行分类，使其成为长视频分析最大的基准。该基准包括12,129个问答对，涵盖三种主要问题类型：感知、推理和幻觉。为了弥补纯MCQ评估的不足，我们设计了两种新颖的基于线索的评估方法：线索-基础白盒和黑盒评估，以评估模型是否基于对视频的正确理解生成答案。我们在CG-Bench上评估了多个封闭源和开源的MLLM。结果表明，与短视频相比，当前模型在理解长视频方面的表现显著不足，开源模型与商业模型之间存在显著差距。我们希望CG-Bench能够推进更可信、更有能力的MLLM在长视频理解方面的发展。所有注释和视频数据发布在https://cg-bench.github.io/leaderboard/。|
|**2024-12-16**|**Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats**|Kuleen Sasse et.al.|[2412.12072](http://arxiv.org/abs/2412.12072)|**[link](https://github.com/kuleens/fetch-dog-whistle)**|**请注意：本文包含可能令某些读者感到不安或冒犯的内容。犬哨子是具有双重含义的编码表达：一方面旨在对公众（外群体）传达，另一方面则向特定受众（内群体）传达特定信息。通常，这些表达被用来传达有争议的政治观点，同时保持合理否认的可能性和绕过内容审查过滤器。犬哨子的识别依赖于经过编辑的词汇表，而这些词汇表难以跟上时代的步伐。我们提出了\textbf{FETCH!}，这是一个在大量社交媒体语料库中寻找新颖犬哨子的任务。我们发现，最先进的技术在三个不同的社交媒体案例研究中未能取得有意义的成果。我们提出了\textbf{EarShot}，一个结合了向量数据库和大型语言模型（LLM）优势的新颖系统，以高效和有效地识别新的犬哨子。**|
|**2024-12-16**|**Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection**|Ira Ceka et.al.|[2412.12039](http://arxiv.org/abs/2412.12039)|null|尽管大型语言模型（LLMs）取得了显著的成就，但在漏洞检测等应用任务上表现有限。我们研究了各种用于漏洞检测的提示策略，并在这一探索过程中，提出了一种将漏洞的自然语言描述与对比链式推理方法相结合的提示策略，该方法通过使用来自合成数据集的对比样本来增强。我们的研究突出了LLMs通过将自然语言描述、对比推理和合成示例整合到全面的提示框架中检测漏洞的潜力。我们的结果显示，这种方法可以增强LLMs对漏洞的理解。在一个高质量的漏洞检测数据集（如SVEN）上，我们的提示策略可以将准确率、F1分数和成对准确率分别提高23%、11%和14%。|
|**2024-12-16**|**SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval**|Yueqian Lin et.al.|[2412.12009](http://arxiv.org/abs/2412.12009)|**[link](https://github.com/linyueqian/spiral_dataset)**|我们引入了一种新的长文本任务——语音信息检索（SIR），针对语音大型语言模型（Speech LLMs），并提出了SPIRAL，一个包含1,012个样本的基准测试，用于检验模型从大约90秒的语音输入中提取关键细节的能力。尽管当前语音大型语言模型在短文本任务上表现出色，但它们在处理较长音频序列的计算和表示需求上存在困难。为了解决这一局限性，我们提出了一种无训练的token剪枝策略——SpeechPrune，该策略利用语音文本相似度和近似注意力分数来高效地丢弃无关的token。在SPIRAL中，SpeechPrune在20%的剪枝率下，相较于原始模型和随机剪枝模型，分别实现了29%和47%的准确率提升。SpeechPrune即使在80%的剪枝水平下也能保持网络性能。这种方法突显了token级剪枝在高效和可扩展的长文本语音理解中的潜力。|
|**2024-12-16**|**The Open Source Advantage in Large Language Models (LLMs)**|Jiya Manchanda et.al.|[2412.12004](http://arxiv.org/abs/2412.12004)|null|大型语言模型（LLMs）标志着自然语言处理（NLP）领域的关键转变，它们在文本生成、翻译和特定领域推理方面取得了显著进展。由专有数据集和大量计算资源驱动的闭源模型，如GPT-4，目前处于性能最前沿。然而，它们因“黑箱”性质和限制可访问性而受到批评，这阻碍了可重复性和公平的AI发展。相比之下，LLaMA和BLOOM等开源倡议通过社区驱动的发展和计算效率优先，实现了民主化。这些模型在语言多样性和特定领域应用方面显著缩小了性能差距，同时为全球的研究人员和开发者提供了可访问的工具。值得注意的是，这两种范式都依赖于基础架构创新，如Vaswani等人于2017年提出的Transformer框架。闭源模型通过有效扩展而表现出色，而开源模型则适应于代表性不足的语言和领域中的实际应用。低秩适应（LoRA）和指令调整数据集等技术的应用使得开源模型在资源有限的情况下也能取得有竞争力的结果。毫无疑问，闭源和开源方法之间的紧张关系凸显了AI领域透明度与专有控制之间的更广泛辩论。伦理方面的考虑进一步突显了这种分歧。闭源系统限制了外部审查，而开源模型促进了可重复性和合作，但缺乏标准化的审计文档框架来减轻偏见。利用两种范式优势的混合方法可能会塑造LLM创新的未来，确保可访问性、具有竞争力的技术性能和道德部署。|
|**2024-12-16**|**LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts**|Zhuhao Wang et.al.|[2412.12001](http://arxiv.org/abs/2412.12001)|**[link](https://github.com/zh-wang-med/llm-rg4)**|编写放射学报告是一项复杂的任务，需要灵活性，其中放射科医生根据可用信息和特定的临床需求调整内容。然而，目前大多数放射学报告生成（RRG）模型都受到固定任务范式的限制，例如从单一图像中预测完整的“发现”部分，这本质上涉及到输入和输出的不匹配。训练后的模型缺乏处理多样化输入的灵活性，可能会生成有害的、与输入无关的幻觉。为了弥合当前RRG模型与实际临床需求之间的差距，我们首先开发了一个数据生成管道来创建一个新的MIMIC-RG4数据集，该数据集考虑了四种常见的放射学报告起草场景，并实现了输入和输出的完美对应。其次，我们提出了一种基于大型语言模型（LLM）的RRG框架，名为LLM-RG4，它利用LLM灵活的指令遵循能力和广泛的一般知识。我们进一步开发了一个自适应标记融合模块，它可以提供处理不同输入组合的灵活性，同时最大限度地减少与增加的输入量相关的额外计算负担。此外，我们提出了一个标记级损失加权策略，以引导模型将注意力集中在积极的和不确定的描述上。实验结果表明，LLM-RG4在MIMIC-RG4和MIMIC-CXR数据集上均实现了临床效率和自然语言生成的最先进性能。我们定量证明，我们的模型具有最小的与输入无关的幻觉，而当前的开源模型通常都存在这个问题。|
|**2024-12-16**|**Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support**|Devika Venugopalan et.al.|[2412.11995](http://arxiv.org/abs/2412.11995)|**[link](https://github.com/devika-prog/caregiver-conversational-support-tool)**|**照顾者（即父母和孩子的照顾社区成员）是学习分析中被低估的利益相关者。尽管照顾者的参与可以提高学生的学术成绩，但许多障碍阻碍了他们的参与，其中最显著的是与现代学校课程相关的知识差距。学习分析中的一个新兴研究课题是混合辅导，它包括教学和动机支持。照顾者在家庭作业中扮演着类似的角色，但尚不清楚学习分析如何支持他们。我们与照顾者合作的研究表明，对话支持是向照顾者提供有效支持学生学习的指导的有前景的方法。我们开发了一个系统，通过由大型语言模型（LLM）生成对话推荐来为照顾者提供教学支持。针对LLM已知的教学局限性，我们在使用开源的Llama 3 LLM进行提示工程实验的同时，利用了辅导系统的教学智能。这个LLM为照顾者提供了通过聊天支持孩子数学练习的消息推荐。通过少量提示和将辅导系统的实时问题解决情境与辅导实践示例相结合，产生了理想的消息推荐。这些推荐与十位初中照顾者进行了评估，他们重视推荐能够通过自我解释促进内容层次的支持和学生的元认知。我们贡献了对如何将辅导系统与LLM最佳结合以通过对话辅助支持混合辅导环境，从而促进照顾者在辅导系统中的有效参与的见解。**|
|**2024-12-13**|**UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities**|Muhammad Uzair Khattak et.al.|[2412.10372](http://arxiv.org/abs/2412.10372)|**[link](https://github.com/mbzuai-oryx/unimed-clip)**|**通过对比学习训练的视觉-语言模型（VLMs）在自然图像任务中取得了显著的成果。然而，由于公开可获取的大规模医学图像-文本数据集稀缺，它们在医学领域的应用仍然有限。现有的医学VLMs要么在封闭源专有数据集或相对较小的开源数据集上训练，这些数据集的泛化能力不强。同样，大多数模型仍然局限于单一或有限的医学成像领域，这再次限制了它们在其他模态上的适用性。为了解决这一差距，我们引入了UniMed，这是一个包含超过530万图像-文本对的开放源代码多模态医学数据集，涵盖六种不同的成像模态：X光、CT、MRI、超声、病理和眼底。UniMed是通过一个数据收集框架开发的，该框架利用大型语言模型（LLMs）将特定模态的分类数据集转换为图像-文本格式，同时结合现有医学领域的图像-文本数据，从而促进可扩展的VLM预训练。使用UniMed，我们训练了UniMed-CLIP，这是一个针对六个模态的统一VLM，它在零样本评估中显著优于现有的通用VLMs，并匹配了特定模态的医学VLMs，实现了显著的增益。例如，UniMed-CLIP在21个数据集上的平均绝对增益比BiomedCLIP（在专有数据上训练）高出+12.61，而训练数据量减少了3倍。为了促进未来的研究，我们在https://github.com/mbzuai-oryx/UniMed-CLIP上发布了UniMed数据集、训练代码和模型。**|
|**2024-12-13**|**Robust image classification with multi-modal large language models**|Francesco Villani et.al.|[2412.10353](http://arxiv.org/abs/2412.10353)|null|深度神经网络容易受到对抗样本的攻击，即精心设计的输入样本可以导致模型以高置信度做出错误预测。为了缓解这些脆弱性，已经提出了基于对抗训练和检测的防御措施来提前加强模型。然而，大多数这些方法只关注单一的数据模态，忽略了输入的视觉模式和文本描述之间的关系。在这篇论文中，我们提出了一种新的防御方法，名为Multi-Shield，旨在通过结合和补充这些防御措施与多模态信息来进一步增强其鲁棒性。Multi-Shield利用多模态大型语言模型来检测对抗样本，并在输入的文本和视觉表示之间没有一致性的情况下避免不确定的分类。在CIFAR-10和ImageNet数据集上进行的广泛评估，使用鲁棒和非鲁棒的图像分类模型，表明Multi-Shield可以轻松集成以检测和拒绝对抗样本，并优于原始的防御措施。|
|**2024-12-13**|**COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models**|Yuchen Ren et.al.|[2412.10347](http://arxiv.org/abs/2412.10347)|null|作为中心法则的关键要素，DNA、RNA和蛋白质在保证准确的遗传表达和实施方面发挥着至关重要的作用，从而维持生命。尽管对这些分子的研究对医学、农业和工业等领域产生了深远的影响，但机器学习方法的多样性——从传统的统计方法到深度学习模型和大型语言模型——给研究人员选择最适合特定任务的最优模型带来了挑战，尤其是在缺乏全面基准的情况下，对于跨组学和多组学任务尤其如此。为了解决这个问题，我们引入了第一个综合的多组学基准COMET（生物全面多组学评估任务和语言模型基准），旨在评估单组学、跨组学和多组学任务中的模型。首先，我们收集和开发了一个多样化的下游任务和数据集集合，涵盖了DNA、RNA和蛋白质的关键结构和功能方面，包括跨越多个组学级别的任务。然后，我们评估了现有的DNA、RNA和蛋白质的基础语言模型以及新提出的多组学方法，提供了关于它们在不同生物模式数据整合和分析中的性能的宝贵见解。这个基准旨在定义多组学研究中的关键问题，并指导未来的研究方向，最终通过综合和不同组学数据分析促进对生物学过程的理解的进步。|
|**2024-12-13**|**Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining**|Zhiqi Ge et.al.|[2412.10342](http://arxiv.org/abs/2412.10342)|null|数字代理越来越多地被用于自动化互动数字环境中的任务，如网页、软件应用程序和操作系统。基于大型语言模型（LLMs）的基于文本的代理由于平台特定的API而经常需要频繁更新，而利用多模态大型语言模型（MLLMs）的视觉代理通过直接与图形用户界面（GUIs）交互提供了更高的适应性。然而，这些代理在视觉感知方面面临着重大挑战，尤其是在处理高分辨率、视觉复杂的数字环境时。本文介绍了一种名为Iris的基础视觉代理，它通过两个关键创新来应对这些挑战：信息敏感裁剪（ISC）和自我精炼双重学习（SRDL）。ISC通过一个边缘检测算法动态地识别和优先处理视觉密集区域，通过分配更多计算资源到信息密度较高的区域来实现高效处理。SRDL通过利用双重学习循环来增强代理处理复杂任务的能力，在该循环中，指代（描述UI元素）的改进强化了定位（定位元素），反之亦然，而无需额外的标注数据。实证评估表明，Iris在多个基准测试中实现了最先进的性能，仅需850K GUI标注，就优于使用10倍多训练数据的方法。这些改进进一步转化为在Web和OS代理下游任务中的显著收益。|
|**2024-12-13**|**AdvPrefix: An Objective for Nuanced LLM Jailbreaks**|Sicheng Zhu et.al.|[2412.10321](http://arxiv.org/abs/2412.10321)|**[link](https://github.com/facebookresearch/jailbreak-objectives)**|许多针对大型语言模型（LLMs）的越狱攻击依赖于一个共同的目标：让模型以“当然，这是（有害请求）”为前缀进行回应。虽然这个目标很简单，但它有两个局限性：对模型行为的控制有限，通常导致不完整或不切实际的回应，以及固定的格式阻碍了优化。为了解决这些局限性，我们引入了AdvPrefix，这是一种新的前缀强制目标，它能够以更细微的方式控制模型行为，同时易于优化。我们的目标利用基于模型依赖的前缀，这些前缀是根据两个标准自动选择的：高预填充攻击成功率和低负对数似然。它还可以通过为单个用户请求使用多个前缀来进一步简化优化。AdvPrefix可以无缝集成到现有的越狱攻击中，以免费提高其性能。例如，只需在Llama-3上将GCG攻击的目标前缀替换为我们提供的，就能将细微攻击的成功率从14%提高到80%，这表明当前的对抗性训练在泛化到未见过的前缀方面存在困难。我们的工作证明了越狱目标在实现细微越狱中的重要性。|
|**2024-12-13**|**BrushEdit: All-In-One Image Inpainting and Editing**|Yaowei Li et.al.|[2412.10316](http://arxiv.org/abs/2412.10316)|null|随着基于扩散模型的图像编辑技术的发展，图像编辑技术取得了显著进步，这些模型采用了基于反转和基于指令的方法。然而，当前基于反转的方法在处理大修改（例如添加或删除对象）时遇到困难，因为反转噪声的结构性质阻碍了重大的变化。同时，基于指令的方法通常将用户限制在黑盒操作中，限制了直接交互以指定编辑区域和强度。为了解决这些限制，我们提出了BrushEdit，这是一种基于修复的指令引导图像编辑新范式，它利用多模态大型语言模型（MLLMs）和图像修复模型来实现自主、用户友好和交互式的自由形式指令编辑。具体来说，我们设计了一个系统，通过在代理协作框架中集成MLLMs和双分支图像修复模型，实现自由形式指令编辑，该系统可以进行编辑类别分类、主要物体识别、蒙版获取和编辑区域修复。大量实验表明，我们的框架有效地结合了MLLMs和修复模型，在包括蒙版区域保留和编辑效果一致性在内的七个指标上取得了优异的性能。|
|**2024-12-13**|**Still "Talking About Large Language Models": Some Clarifications**|Murray Shanahan et.al.|[2412.10291](http://arxiv.org/abs/2412.10291)|null|我的论文《关于大型语言模型的讨论》多次被解读为倡导对大型语言模型采取还原论立场。但我的论文并非旨在如此，我也不支持这样的观点。这篇简短的笔记将论文置于一个更广泛的哲学项目背景下，该项目关注的是（误）使用词汇的问题，而非形而上学，秉承维特根斯坦后期著作的精神。|
|**2024-12-13**|**One world, one opinion? The superstar effect in LLM responses**|Sofie Goethals et.al.|[2412.10281](http://arxiv.org/abs/2412.10281)|null|随着大型语言模型（LLMs）正在塑造在线信息共享和获取的方式，它们的观点有可能影响广泛的受众。这项研究通过使用十种不同的语言进行提示，探讨了语言多样性对LLMs在不同领域视为最突出人物的影响。我们的发现显示，在回应中存在低多样性，少数人物在多种语言中占据主导地位（也称为“明星效应”）。这些结果突显了当LLMs检索主观信息时，缩小全球知识表征的风险。|
|**2024-12-13**|**Benchmarking Linguistic Diversity of Large Language Models**|Yanzhu Guo et.al.|[2412.10271](http://arxiv.org/abs/2412.10271)|**[link](https://github.com/yanzhuguo/llm-diversity)**|**大型语言模型（LLMs）的开发和评估主要集中于其任务解决能力，近期的一些模型在某些领域甚至超越了人类表现。然而，这种关注往往忽略了机器生成语言在词汇选择、句法构造和意义表达方面是否达到人类水平的多样性，引发了对语言生成基础是否已被充分解决的疑问。本文强调了在LLMs生成内容的在线内容激增的背景下，检验语言模型保留人类语言丰富性的重要性。我们提出一个从词汇、句法和语义等多个语言学多样性维度评估LLMs的全面框架。利用这一框架，我们对多个最先进的LLMs在所有多样性维度上进行基准测试，并对句法多样性进行了深入研究。最后，我们分析了不同开发和部署选择如何影响LLMs输出的语言多样性。**|
|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有通用能力的AI代理提供了引人注目的基础。这些代理可能很快将在现实生活中大规模部署，代表个人（例如，AI助手）或人类群体（例如，AI加速企业）的利益。目前，关于多个LLM代理在迭代部署的多代中相互作用的动态知之甚少。在这篇论文中，我们研究了在存在背叛动机的情况下，“LLM代理社会”是否能学会相互有益的社会规范，这是人类社会性的一个独特特征，可能是文明成功的关键。具体而言，我们研究了LLM代理在经典迭代捐赠游戏中间接互惠的演变，这些代理可以观察到同伴的最近行为。我们发现，合作演变的差异在基础模型之间非常明显，Claude 3.5 Sonnet代理的社会实现了显著更高的平均得分，而Gemini 1.5 Flash则优于GPT-4o。此外，Claude 3.5 Sonnet可以利用额外的成本惩罚机制实现更高的得分，而Gemini 1.5 Flash和GPT-4o则不能。对于每个模型类别，我们还观察到随机种子下涌现行为的差异，这表明对初始条件的敏感依赖性被低估。我们建议，我们的评估机制可以激发一类新的、成本效益高且信息丰富的LLM基准，重点关注LLM代理部署对社会合作基础设施的影响。|
|**2024-12-12**|**EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM**|Zhuofan Zong et.al.|[2412.09618](http://arxiv.org/abs/2412.09618)|null|在扩散模型的个性化方面取得了显著成就。传统的无需调整的方法通常通过平均多个参考图像的图像嵌入作为注入条件来编码多个参考图像，但这样的图像独立操作无法在图像之间执行交互，以捕捉多个参考图像中的一致视觉元素。尽管基于调整的低秩自适应（LoRA）可以通过训练过程有效地从多个图像中提取一致元素，但它需要对每个不同的图像组进行特定的微调。本文介绍了一种新的即插即用自适应方法EasyRef，它使扩散模型能够根据多个参考图像和文本提示进行条件化。为了有效地利用多个图像中的一致视觉元素，我们利用了多模态大型语言模型（MLLM）的多图像理解和指令遵循能力，提示它根据指令捕捉一致视觉元素。此外，通过适配器将MLLM的表示注入到扩散过程中，可以轻松地泛化到未见领域，挖掘未见数据中的一致视觉元素。为了减轻计算成本并增强细粒度细节保留，我们引入了一种高效的参考聚合策略和渐进式训练方案。最后，我们引入了MRBench，一个新的多参考图像生成基准。实验结果表明，EasyRef优于IP-Adapter等无需调整的方法和LoRA等基于调整的方法，在各个领域实现了优越的美学质量和鲁棒的零样本泛化。|
|**2024-12-12**|**Olympus: A Universal Task Router for Computer Vision Tasks**|Yuanze Lin et.al.|[2412.09612](http://arxiv.org/abs/2412.09612)|**[link](https://github.com/yuanze-lin/olympus_page)**|**我们介绍了一种名为Olympus的新方法，该方法将多模态大型语言模型（MLLMs）转换为一个能够处理多种计算机视觉任务的统一框架。利用控制器MLLM，Olympus将超过20个专门的任务（包括图像、视频和3D对象）分配给专用模块。这种基于指令的路由通过连锁动作实现复杂的工作流程，无需训练重量级的生成模型。Olympus可以轻松集成到现有的MLLMs中，通过可比的性能扩展其功能。实验结果表明，Olympus在20个任务上实现了平均路由准确率为94.75%，在连锁动作场景中的准确率为91.82%，展示了其作为通用任务路由器的有效性，能够解决各种计算机视觉任务。项目页面：https://github.com/yuanze-lin/Olympus_page**|
|**2024-12-12**|**SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**|Hao Li et.al.|[2412.09604](http://arxiv.org/abs/2412.09604)|null|本文介绍了大型语言模型（LLMs）在多模态领域的显著成功，并在图像理解和生成方面取得了卓越的表现。近期开发统一的多模态大型语言模型（MLLMs）的尝试也显示出良好的效果。然而，现有方法往往涉及复杂的模型架构或训练流程设计，增加了模型训练和扩展的难度。在这篇论文中，我们提出了SynerGen-VL，这是一种简单而强大的无编码器MLLM，能够实现图像理解和生成。为了解决现有无编码器统一MLLM中识别出的挑战，我们引入了标记折叠机制和基于视觉专家的渐进对齐预训练策略，这些机制有效地支持了高分辨率图像理解的同时降低了训练复杂性。在用大规模混合图像-文本数据进行统一下一标记预测目标训练后，SynerGen-VL在可比或更小的参数规模下达到了或超过了现有无编码器统一MLLM的性能，并缩小了与特定任务最先进模型之间的差距，突显了未来统一MLLM的可行路径。我们的代码和模型将予以发布。|
|**2024-12-12**|**Do Multimodal Large Language Models See Like Humans?**|Jiaying Lin et.al.|[2412.09603](http://arxiv.org/abs/2412.09603)|null|多模态大型语言模型（MLLMs）在各种视觉任务上取得了令人瞩目的成果，得益于最近大型语言模型的发展。然而，一个关键问题仍未得到解决：MLLMs是否以与人类相似的方式感知视觉信息？当前的基准测试缺乏评估MLLMs从这一角度的能力。为了应对这一挑战，我们引入了HVSBench，这是一个大规模基准测试，旨在评估MLLMs与人类视觉系统（HVS）在反映人类视觉的基本视觉任务上的对齐程度。HVSBench精心挑选了超过85K个多模态样本，涵盖了HVS中的13个类别和5个领域，包括突出度、瞬间识别、优先级排序、自由观看和搜索。广泛的实验表明，我们的基准测试在全面评估MLLMs方面的有效性。具体来说，我们评估了13个MLLMs，结果显示即使是表现最好的模型也仍有很大的提升空间，其中大多数仅取得了中等的结果。我们的实验表明，HVSBench为最前沿的MLLMs提出了新的和重大的挑战。我们相信HVSBench将促进对人类对齐和可解释的MLLMs的研究，标志着理解MLLMs如何感知和处理视觉信息的关键一步。|
|**2024-12-12**|**InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**|Pan Zhang et.al.|[2412.09596](http://arxiv.org/abs/2412.09596)|**[link](https://github.com/internlm/internlm-xcomposer)**|**长期以来，创建能够像人类认知一样在长时间内与环境互动的AI系统一直是研究目标。近年来，多模态大型语言模型（MLLMs）在开放式理解方面取得了重大进展。然而，连续和同时进行感知、记忆和推理的挑战在很大程度上仍未被探索。当前的MLLMs受限于其序列到序列的架构，这限制了它们同时处理输入和生成响应的能力，类似于感知时无法思考。此外，依赖长上下文来存储历史数据对于长期交互来说不切实际，因为保留所有信息变得昂贵且效率低下。因此，本项目不是依赖于单一基础模型来执行所有功能，而是从专用通用AI的概念中汲取灵感，引入了解耦的流感知、推理和记忆机制，使系统能够实时处理流式视频和音频输入。提出的框架InternLM-XComposer2.5-OmniLive（IXC2.5-OL）包含三个关键模块：（1）流感知模块：实时处理多模态信息，将关键细节存储在记忆中，并根据用户查询触发推理。（2）多模态长记忆模块：整合短期和长期记忆，将短期记忆压缩为长期记忆，以实现高效检索和提高准确性。（3）推理模块：响应查询并执行推理任务，与感知和记忆模块协调。本项目模拟了类似人类的认知，使多模态大型语言模型能够随着时间的推移提供持续和自适应的服务。**|
|**2024-12-12**|**DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction**|Yu Feng et.al.|[2412.09572](http://arxiv.org/abs/2412.09572)|null|量化大型语言模型（LLMs）在事实参数知识方面的不确定性，尤其是在黑盒设置中，是一个重大挑战。现有方法通过评估模型对原始查询的响应的自我一致性来衡量模型的不确定性，但并不总是能够捕捉到真正的不确定性。模型可能会对原始查询作出一致的错误回答，同时对同一查询的不同角度的多样化问题作出正确回答，反之亦然。在本文中，我们提出了一种新颖的方法，名为DiverseAgentEntropy，用于在假设如果模型是确定性的，它应该能够跨越关于同一原始查询的多样化问题的集合中一致地回忆起原始查询的回答的情况下，评估模型的不确定性。我们进一步实施了一种弃权策略，在不确定性高时抑制响应。我们的方法提供了对模型可靠性的更准确预测，并进一步检测了幻觉，优于其他基于自我一致性的方法。此外，它还表明，现有模型在已知正确答案的情况下，往往无法在多样化的不同问题下一致地检索到同一查询的正确答案。|
|**2024-12-12**|**Does Representation Matter? Exploring Intermediate Layers in Large Language Models**|Oscar Skean et.al.|[2412.09563](http://arxiv.org/abs/2412.09563)|null|理解定义大型语言模型（LLMs）中良好表示的因素对于理论理解和实际应用都是至关重要的。在这篇论文中，我们调查了包括Transformer和状态空间模型（SSMs）在内的各种LLM架构中的中间表示质量。我们发现，中间层往往比最终层为下游任务提供更丰富的表示。为了衡量表示质量，我们调整并应用了一套原本在其他背景下提出的指标，如提示熵、曲率和增强不变性。我们的实证研究表明，存在显著的架构差异，表示在训练过程中的演变，以及输入随机性和提示长度等因素如何影响每一层。值得注意的是，我们在一些中间层的熵中观察到双峰模式，并考虑了与训练数据相关的潜在解释。总的来说，我们的研究结果揭示了LLMs的内部机制，并指导了架构优化和训练的策略。|
|**2024-12-12**|**Foundational Large Language Models for Materials Research**|Vaibhav Mishra et.al.|[2412.09560](http://arxiv.org/abs/2412.09560)|**[link](https://github.com/M3RG-IITD/llamat)**|材料发现与开发对于解决全球挑战至关重要。然而，材料科学文献中包含大量文本数据的指数式增长，在知识提取、综合和科学推理方面造成了显著的瓶颈。大型语言模型（LLMs）通过自动分析和预测，为加速材料研究提供了前所未有的机会。尽管如此，它们的有效部署需要针对特定领域进行适应性调整以理解和解决领域相关任务。在这里，我们介绍了LLaMat，这是一个通过在广泛的材料文献和晶体学数据集上持续预训练LLaMA模型而开发的用于材料科学的基座模型系列。通过系统评估，我们证明了LLaMat在特定于材料科学的自然语言处理和结构化信息提取方面表现出色，同时保持着一般的语言能力。专门化的LLaMat-CIF变体在晶体结构生成方面展现出前所未有的能力，预测了周期表中覆盖范围广泛的稳定晶体。有趣的是，尽管与LLaMA-2相比，LLaMA-3的性能更优，但我们观察到LLaMat-2在不同材料科学任务中的特定领域性能得到了意想不到的增强，包括从文本和表格中提取结构化信息，尤其是在晶体结构生成方面，这可能是过度训练的LLMs中潜在的自适应刚性。总之，本研究证明了领域适应性在开发实际可部署的LLM协同飞行员进行材料研究中的有效性。超越材料科学，我们的发现揭示了LLMs领域适应性的重要考虑因素，如模型选择、训练方法以及特定领域的性能，这些可能影响专门科学人工智能系统的发展。|
|**2024-12-12**|**Exemplar Masking for Multimodal Incremental Learning**|Yi-Lun Lee et.al.|[2412.09549](http://arxiv.org/abs/2412.09549)|**[link](https://github.com/yilunlee/exemplar_masking_mcil)**|**多模态增量学习需要在处理来自多种模态的信息的同时，同时学习新的知识而不忘记之前学习的信息。这项任务面临许多挑战，主要包括基于实例的方法中多模态数据更大的存储空间以及在大规模多模态模型上微调的计算需求。在本文中，我们利用参数高效微调方案来减轻微调的负担，并提出实例掩码框架以有效地重现旧知识。具体来说，根据注意力权重和不同模态之间的相关性，对非重要标记进行掩码，显著减少了实例的存储空间，从而在相同的内存缓冲区下节省了更多实例。此外，我们设计了一种多模态数据增强技术，以多样化实例，以便重现先前知识。在实验中，我们不仅评估了我们方法在现有多模态数据集上的表现，还将ImageNet-R数据集扩展为一个多模态数据集，作为实际应用，其中通过查询多模态大型语言模型（例如InstructBLIP）生成字幕。广泛的实验表明，在相同的有限内存缓冲区下，我们的实例掩码框架在效率和对抗灾难性遗忘方面更加鲁棒。代码可在https://github.com/YiLunLee/Exemplar_Masking_MCIL上获取。**|
|**2024-12-12**|**Can Modern LLMs Act as Agent Cores in Radiology~Environments?**|Qiaoyu Zheng et.al.|[2412.09529](http://arxiv.org/abs/2412.09529)|**[link](https://github.com/magic-ai4med/radabench)**|在大型语言模型（LLMs）的进步为基于LLMs的代理系统铺平了道路，这些系统在各种领域提供了更高的准确性和可解释性。放射学由于其复杂的分析需求，是这些代理应用的理想领域。本文旨在调查构建具体放射学代理的先决问题，即“现代LLMs能否在放射学环境中充当代理核心？”为了调查这个问题，我们介绍了RadABench，并具有以下三个方面的贡献：首先，我们提出了RadABench-Data，这是一个用于基于LLMs的代理的综合合成评估数据集，它来源于一个包含6个解剖部位、5种成像方式、10种工具类别和11项放射学任务的广泛分类。其次，我们提出了RadABench-EvalPlat，这是一个新型代理评估平台，具有提示驱动的流程和模拟广泛放射学工具集的能力。第三，我们从5个角度使用多个指标评估了7个领先LLMs在我们基准测试上的性能。我们的发现表明，尽管当前LLMs在许多领域表现出强大的能力，但它们仍不够先进，无法作为完全运行中的放射学代理系统的核心。此外，我们确定了影响基于LLMs的代理核心性能的关键因素，为临床医生提供了如何在实际放射学实践中有效应用代理系统的见解。所有我们的代码和数据都已开源在https://github.com/MAGIC-AI4Med/RadABench。|
|**2024-12-11**|**Generative Semantic Communication: Architectures, Technologies, and Applications**|Jinke Ren et.al.|[2412.08642](http://arxiv.org/abs/2412.08642)|null|本文深入探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并进行了全面的研究。首先介绍了三个由经典GAI模型支持的流行SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，本文阐释了GAI模型的基本概念、相应的SemCom架构以及近期努力的文献综述。接着，提出了一种新型的基于最新GAI技术——大型语言模型（LLMs）的生成式SemCom系统。该系统在发送方和接收方均采用两个基于LLMs的AI代理，分别作为“大脑”来提供强大的信息理解和内容再生能力。这种创新设计使得接收方可以直接根据发送方传递的编码语义信息生成所需内容，而不是恢复比特流。因此，它将通信思维从“信息恢复”转变为“信息再生”，从而开启了生成式SemCom的新时代。通过一个关于点对点视频检索的案例研究展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，通信开销减少了99.98%，检索精度提高了53%。此外，还概述了生成式SemCom的四个典型应用场景，并讨论了三个需要未来进一步研究的问题。总之，本文为在SemCom中应用GAI提供了一套全面的指导原则，为未来无线网络中生成式SemCom的高效实现铺平了道路。|
|**2024-12-11**|**Fast Prompt Alignment for Text-to-Image Generation**|Khalil Mrini et.al.|[2412.08639](http://arxiv.org/abs/2412.08639)|**[link](https://github.com/tiktok/fast_prompt_alignment)**|**文本到图像生成技术发展迅速，但将复杂的文本提示与生成的图像相匹配仍然具有挑战性，尤其是在处理复杂的物体关系和细微的细节方面。本文介绍了一种名为快速提示对齐（FPA）的提示优化框架，它采用了一次性方法，提高了文本到图像对齐的效率，避免了当前方法如OPT2I典型的迭代开销。FPA利用大型语言模型（LLMs）进行单次迭代提示改写，随后使用优化后的提示进行微调或上下文学习，以实现实时推理，降低计算需求同时保持对齐精度。在COCO Captions和PartiPrompts数据集上的广泛评估表明，FPA在处理时间的一小部分内就实现了具有竞争力的文本-图像对齐得分，这一点通过自动化指标（TIFA、VQA）和人工评估都得到了验证。一项由专家注释员参与的问卷调查进一步揭示了人类对齐判断与自动化评分之间的强相关性，凸显了FPA改进的稳健性。所提出的方法展示了一种可扩展、高效的迭代提示优化替代方案，使其在实时、高需求环境中具有更广泛的应用。代码库已提供以促进进一步研究：https://github.com/tiktok/fast_prompt_alignment**|
|**2024-12-11**|**Multimodal Latent Language Modeling with Next-Token Diffusion**|Yutao Sun et.al.|[2412.08635](http://arxiv.org/abs/2412.08635)|**[link](https://github.com/microsoft/unilm/tree/master/LatentLM)**|多模态生成模型需要一种统一的方法来处理离散数据（例如文本和代码）和连续数据（例如图像、音频、视频）。在这项工作中，我们提出了潜在语言模型（LatentLM），它通过因果Transformer无缝地整合连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入了下一个标记扩散来实现这些向量的自回归生成。此外，我们开发了 $\sigma$ -VAE来解决方差崩溃问题，这对于自回归建模至关重要。大量实验证明了LatentLM在各种模态上的有效性。在图像生成方面，LatentLM在性能和可扩展性上都超越了扩散Transformer。当集成到多模态大型语言模型中时，LatentLM提供了一个通用的接口，统一了多模态生成和理解。实验结果表明，在扩大训练标记的设置中，与Transfusion和矢量量化模型相比，LatentLM取得了有利的性能。在文本到语音合成方面，LatentLM在说话人相似性和鲁棒性方面优于最先进的VALL-E 2模型，同时解码步骤减少了10倍。这些结果确立了LatentLM作为一种高效且可扩展的方法，以推进大型多模态模型的发展。|
|**2024-12-11**|**Synthetic Vision: Training Vision-Language Models to Understand Physics**|Vahid Balazadeh et.al.|[2412.08619](http://arxiv.org/abs/2412.08619)|null|物理推理，涉及对动态环境中物体行为的解释、理解和预测，仍然是当前视觉-语言模型（VLMs）的一个重要挑战。在这项工作中，我们提出了两种方法来利用模拟数据增强VLMs的物理推理能力。首先，我们使用与物理推理任务相关的模拟生成的问答（QA）对微调一个预训练的VLM。其次，我们引入了物理上下文构建器（PCBs），这是一种专门的VLM，经过微调以创建包含物理属性和过程的场景描述。在物理推理任务期间，这些PCBs可以作为上下文来帮助大型语言模型（LLM）提高其性能。我们使用多个基准测试了我们的两种方法，包括一个名为Falling Tower的新稳定性检测QA数据集，它包含模拟和真实世界的场景，以及CLEVRER。我们证明，一个小型的经过QA微调的VLM可以显著优于更大的最先进的基座模型。我们还展示了将PCBs集成可以提升基座LLM在物理推理任务上的性能。使用Falling Tower数据集中的真实世界场景，我们还验证了两种方法在Sim2Real迁移中的鲁棒性。我们的结果表明，模拟数据在创建能够进行高级物理推理的学习系统中的有用性。|
|**2024-12-11**|**Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models**|Jiahui Li et.al.|[2412.08615](http://arxiv.org/abs/2412.08615)|**[link](https://github.com/jiah-li/magic)**|尽管通过对齐技术提高了大型语言模型（LLMs）生成内容的安全性，但这些模型仍然容易受到越狱攻击的影响，越狱攻击是一种暴露LLMs安全漏洞的对抗攻击方法。值得注意的是，贪婪坐标梯度（GCG）方法已显示出自动生成对抗后缀以越狱最先进LLMs的能力。然而，GCG中的优化过程非常耗时，使得越狱流程效率低下。在本文中，我们研究了GCG的过程，并确定了间接效应问题，这是GCG优化的关键瓶颈。为此，我们提出了模型攻击梯度索引GCG（MAGIC），通过利用后缀标记的梯度信息来解决间接效应，从而通过减少计算和迭代次数来加速过程。我们的实验在AdvBench上表明，MAGIC实现了高达1.5倍的速度提升，同时保持了与其他基线相当甚至更高的攻击成功率（ASR）。我们的MAGIC在Llama-2上实现了74%的ASR，在执行对GPT-3.5的迁移攻击时实现了54%的ASR。代码可在https://github.com/jiah-li/magic上找到。|
|**2024-12-11**|**Preference Discerning with LLM-Enhanced Generative Retrieval**|Fabian Paischer et.al.|[2412.08604](http://arxiv.org/abs/2412.08604)|null|序列推荐系统旨在根据用户的交互历史提供个性化的推荐。为了实现这一目标，它们通常结合辅助信息，如物品的文本描述和辅助任务，例如预测用户偏好和意图。尽管已经投入了大量努力来增强这些模型，但它们仍然面临着个性化不足的问题。为了解决这个问题，我们提出了一种新的范式，我们称之为偏好辨别。在偏好辨别中，我们明确地将生成式序列推荐系统在其上下文中对用户偏好进行条件化。为此，我们根据用户评论和物品特定数据使用大型语言模型（LLMs）生成用户偏好。为了评估序列推荐系统的偏好辨别能力，我们引入了一个新的基准，该基准在各种场景中提供了一个全面的评估，包括偏好引导和情感跟随。我们使用我们的基准评估了当前最先进的方法，并表明它们在准确辨别用户偏好方面存在困难。因此，我们提出了一种名为Mender的新方法，该方法改进了现有方法，并在我们的基准上实现了最先进的性能。我们的结果表明，即使在训练过程中没有观察到人类偏好，Mender也能被有效引导，为更个性化的序列推荐系统铺平了道路。我们的代码和基准将在发表后开源。|
|**2024-12-11**|**Empirical Measurements of AI Training Power Demand on a GPU-Accelerated Node**|Imran Latif et.al.|[2412.08602](http://arxiv.org/abs/2412.08602)|null|随着人工智能（AI）应用范围的扩大，云计算提供商在计算基础设施方面的投资大幅增加。量化这一基础设施的能源足迹需要根据AI硬件在训练期间的电力需求进行参数化的模型。我们实证测量了一个8-GPU的NVIDIA H100 HGX节点在开源图像分类器（ResNet）和大型语言模型（Llama2-13b）训练过程中的瞬时电力消耗。观察到的最大电力消耗约为8.4千瓦，比制造商额定值10.2千瓦低18%，即使GPU接近满负荷运行。在保持模型架构不变的情况下，将ResNet的批量大小从512张图像增加到4096张图像，总训练能耗减少了4倍。这些发现可以为数据中心运营商的容量规划以及研究人员的能源使用估计提供信息。未来的工作将研究冷却技术和碳感知调度对AI工作负载能源消耗的影响。|
|**2024-12-11**|**Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability and Compliance Checks**|Arsalan Masoudifard et.al.|[2412.08593](http://arxiv.org/abs/2412.08593)|null|确保软件需求规格说明书（SRS）与更高级的组织或国家要求相一致至关重要，尤其是在金融和航空航天等监管环境。在这些领域，保持一致性、遵守监管框架、最小化错误以及满足关键期望对于系统的可靠运行是必不可少的。大型语言模型（LLMs）的广泛应用凸显了它们的巨大潜力，但在检索相关信息和增强推理能力方面仍有很大的改进空间。本研究表明，将强大的图-RAG框架与高级提示工程技术，如思维链和思维树，相结合可以显著提高性能。与基线RAG方法和简单的提示策略相比，这种方法提供更准确和情境感知的结果。尽管这种方法在性能上显示出显著改进，但也带来了一些挑战。在多样化的环境中实施既昂贵又复杂，需要仔细适应特定场景。此外，它的有效性高度依赖于完整和准确的数据输入，而这些数据可能并不总是容易获得，这进一步限制了其可扩展性和实用性。|
|**2024-12-11**|**Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning**|Hang Zhao et.al.|[2412.08587](http://arxiv.org/abs/2412.08587)|null|该研究对比了基于编码器模型（例如BERT、RoBERTa）和大型语言模型（LLMs，例如Llama3）在文本分类任务中的性能，尤其是在微调的情况下。研究采用了各种不同大小和架构的模型和方法，包括微调和预训练的方法。首先，我们对这些LLMs在20个新闻组（20NG）和MASSIVE数据集上的性能进行了评估，并将它们与仅编码器的RoBERTa模型进行了比较。此外，我们通过将意图检测和槽填充等多个分类任务结合到一个模型中，并使用两个数据集的数据来探索这两种模型类型的多任务能力。我们的结果表明，完全微调的Llama3-70B模型在各种分类任务和数据集上优于RoBERTa-large和其他解码器LLMs。此外，综合的多任务微调LLMs在两个数据集上的两个任务中都匹配了双模型设置的性能。总体而言，我们的研究为基于编码器和LLM的文本分类任务提供了一个全面的基准，并展示了一种将两个或多个完全微调的解码器LLM结合起来的方法，以降低延迟并保持等效性能。|
|**2024-12-11**|**TURBOATTENTION: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大型语言模型（LLM）推理需要大量的计算和内存，尤其是在关键注意力机制上。虽然量化技术，如FlashAttention加速算法，已经提高了整体推理的效率，但它们解决了问题的不同方面：量化专注于权重-激活操作，而FlashAttention提升了执行效率但需要高精度格式。最近的键值（KV）缓存量化减少了内存带宽，但仍然需要浮点数反量化以进行注意力操作。我们提出了TurboAttention，这是一种使注意力量化执行同时解决内存和计算效率的综合方法。我们的解决方案引入了两项关键创新：FlashQ，这是一种头部注意力量化技术，能够压缩KV缓存并实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它在注意力中的指数运算过程中消除了对FP32反量化的需求。实验结果表明，TurboAttention在注意力方面实现了1.2-1.8倍的加速，将KV缓存大小减少了4.4倍以上，并在FP16基线之上实现了高达2.37倍的最大吞吐量，同时在各种数据集和模型上优于最先进的量化和压缩技术。|
|**2024-12-10**|**Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences**|Alan Nawzad Amin et.al.|[2412.07763](http://arxiv.org/abs/2412.07763)|**[link](https://github.com/alannawzadamin/clonebo)**|**为了构建有效的治疗药物，生物学家通过迭代地突变抗体序列来提高其结合力和稳定性。建议的突变可以基于之前的测量结果，或者通过从大量的抗体数据库中学习来预测典型的抗体。不幸的是，典型抗体的搜索空间巨大，实验往往在预算范围内无法找到合适的抗体。我们引入了基于克隆的贝叶斯优化（CloneBO），这是一种贝叶斯优化过程，通过教导一个生成模型如何优化我们的免疫系统中的抗体，从而在实验室中有效地优化抗体。我们的免疫系统通过迭代地进化其序列的特定部分来与靶点强有力地结合，并稳定地结合，从而产生一组被称为克隆家族的相关、演化的序列。我们在数以万计的克隆家族上训练了一个大型语言模型，CloneLM，并使用它来设计具有最有可能优化人体免疫系统内抗体的突变序列。我们提出使用扭曲的顺序蒙特卡洛过程来引导我们的设计以适应之前的测量。我们表明，在现实情况下的计算机模拟实验中，CloneBO比先前的方法更有效地优化了抗体，在体外湿实验中设计了更强和更稳定的结合剂。**|
|**2024-12-10**|**Zero-Shot ATC Coding with Large Language Models for Clinical Assessments**|Zijian Chen et.al.|[2412.07743](http://arxiv.org/abs/2412.07743)|null|将安大略省健康部和InterRAI加拿大在医疗保健研究和运营中手动分配解剖治疗化学（ATC）代码至处方记录的过程，是一个重要的瓶颈，需要大量的专家时间和精力。为了在保持数据隐私的同时自动化这一过程，我们开发了一种实用的方法，使用本地可部署的大语言模型（LLMs）。受最近在自动国际疾病分类（ICD）编码方面的进展启发，我们的方法将ATC编码视为一个层次化信息提取任务，通过引导LLMs逐层浏览ATC本体。我们使用GPT-4o作为准确性的上限，并专注于开发适合隐私敏感部署的开源Llama模型。在加拿大卫生部的药品产品数据、RABBITS基准测试以及安大略省健康的真实临床笔记中进行测试，我们的方法在GPT-4o上实现了78%的精确匹配准确率，在Llama 3.1 70B上实现了60%。我们通过药物定义研究知识固化，发现准确率有适度提高。此外，我们展示了对Llama 3.1 8B进行微调后的模型与零样本Llama 3.1 70B的准确率相匹配，这表明使用较小的模型进行有效的ATC编码是可行的。我们的结果证明了在隐私敏感的医疗保健环境中自动进行ATC编码的可行性，为未来的部署奠定了基础。|
|**2024-12-10**|**Granite Guardian**|Inkit Padhi et.al.|[2412.07724](http://arxiv.org/abs/2412.07724)|**[link](https://github.com/ibm-granite/granite-guardian)**|**我们推出了Granite Guardian模型系列，这是一套旨在为提示和响应提供风险检测的保障措施，以支持与任何大型语言模型（LLM）的安全和负责任使用。这些模型在多个风险维度上提供全面覆盖，包括社会偏见、粗俗、暴力、色情内容、不道德行为、越狱以及与幻觉相关的风险，如检索增强生成（RAG）的上下文相关性、基础性和回答相关性。Granite Guardian模型基于一个独特的数据集进行训练，该数据集结合了来自不同来源的人类标注和合成数据。这些模型解决了传统风险检测模型通常忽略的风险，如越狱和RAG特定问题。在有害内容和RAG幻觉相关基准测试上分别获得AUC分数0.871和0.854，Granite Guardian是此领域中最具有普遍性和竞争力的模型。作为开源发布，Granite Guardian旨在促进整个社区负责任的AI发展。**|
|**2024-12-10**|**DriveMM: All-in-One Large Multimodal Model for Autonomous Driving**|Zhijian Huang et.al.|[2412.07689](http://arxiv.org/abs/2412.07689)|**[link](https://github.com/zhijian11/DriveMM)**|**大型多模态模型（LMMs）通过整合大型语言模型，在自动驾驶（AD）领域展示了卓越的理解和解释能力。尽管取得了进展，但当前基于数据驱动的自动驾驶方法往往集中在单个数据集和特定任务上，忽视了它们的整体能力和泛化能力。为了弥补这些差距，我们提出了DriveMM，这是一种通用的大型多模态模型，旨在处理多种数据输入，如图像和多视角视频，同时在感知、预测和规划等广泛的自动驾驶任务中发挥作用。最初，该模型经过课程预训练，以处理不同的视觉信号并执行基本的视觉理解和感知任务。随后，我们对各种与自动驾驶相关的数据集进行增强和标准化，以微调模型，从而形成一个集自动驾驶之大成的LMM。为了评估其整体能力和泛化能力，我们在六个公开基准上进行了评估，并在一个未见过的数据集上进行了零样本迁移学习，DriveMM在所有任务中均实现了最先进的性能。我们希望DriveMM能够成为未来在现实世界中实现端到端自动驾驶应用的 promising 解决方案。**|
|**2024-12-10**|**Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions**|Anant Prakash Awasthi et.al.|[2412.07687](http://arxiv.org/abs/2412.07687)|null|随着客户支持领域对人工智能（AI）的日益依赖，运营效率和用户体验得到了显著提升。然而，传统的机器学习（ML）方法，这些方法需要在敏感数据集上进行广泛的本地训练，带来了巨大的隐私风险，并且与通用数据保护条例（GDPR）和加州消费者隐私法案（CCPA）等法规存在合规挑战。现有的隐私保护技术，如匿名化、差分隐私和联邦学习，虽然解决了部分问题，但在实用性、可扩展性和复杂性方面仍存在局限。本文提出了一种新型的隐私保护零样本学习（PP-ZSL）框架，该框架利用大型语言模型（LLMs）在零样本学习模式下的能力。与传统的机器学习方法不同，PP-ZSL通过利用预训练的LLMs直接生成响应，从而消除了在敏感数据上本地训练的需求。该框架融合了实时数据匿名化以删除或屏蔽敏感信息、检索增强生成（RAG）以解决特定领域的查询，以及鲁棒的后期处理以确保符合监管标准。这种组合降低了隐私风险，简化了合规性，并提高了可扩展性和运营效率。实证分析表明，PP-ZSL框架能够提供准确、符合隐私规范的响应，同时显著降低了部署人工智能驱动客户支持系统的成本和复杂性。该研究突出了在金融服务业、医疗保健、电子商务、法律支持、电信和政府服务等多个行业的潜在应用。通过解决隐私和性能的双重挑战，该框架为客户交互中的安全、高效和合规的AI应用奠定了基础。|
|**2024-12-10**|**TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation**|Alfredo Garrachón Ruiz et.al.|[2412.07682](http://arxiv.org/abs/2412.07682)|null|大型语言模型（LLMs）的推理成本是一个重大挑战，尤其是对于需要长输出的任务，因为它们的计算需求很大。然而，自然语言往往包含冗余，这为优化提供了机会。我们观察到，当得到适当的提示时，LLMs可以生成简练的语言输出，保留基本意义。我们提出了一种节省计算成本的框架，其中LLM的较短的蒸馏输出由一个具有较低推理成本的小型模型重新构建成完整叙事。我们的实验结果表明了有希望的结果，特别是在一般知识领域，平均节省了20.58%的标记，且评估指标略有下降，这表明这种方法可以在语言处理任务中有效地平衡效率和准确性。|
|**2024-12-10**|**Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting**|Shuyu Shen et.al.|[2412.07673](http://arxiv.org/abs/2412.07673)|**[link](https://github.com/HKUSTDial/vistroubleshooting.github.io)**|可视化创作是一个迭代过程，需要用户修改参数如配色方案和数据转换，以达到预期的美学效果并有效传达洞察。由于这些调整的复杂性，用户常常会创建出有缺陷的可视化，并需要故障排除支持。在本文中，我们考察了两种主要的可视化故障排除方法：（1）通过论坛进行人工辅助支持，用户从其他人那里获得建议；（2）使用大型语言模型（LLMs）进行AI辅助支持。我们的目标是了解每种方法在支持可视化故障排除任务中的优缺点。为此，我们从Stack Overflow收集了889个Vega-Lite案例。然后，我们进行了全面分析，以了解用户提出的问题类型、人工和AI指导的有效性，以及补充资源（如文档和示例）对故障排除结果的影响。我们的发现揭示了人工辅助故障排除和AI辅助故障排除之间的显著差异：人工辅助故障排除提供定制、情境敏感的建议，但响应质量往往有所差异，而AI辅助故障排除提供快速反馈，但通常需要额外的情境资源才能达到预期效果。|
|**2024-12-10**|**FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks**|Bocheng Chen et.al.|[2412.07672](http://arxiv.org/abs/2412.07672)|null|在大型语言模型（LLMs）中，防御措施至关重要，以应对众多利用这些系统通过操纵提示生成有害内容的攻击者，这种攻击被称为越狱攻击。尽管已经提出了许多防御策略，但它们往往需要访问模型的内部结构或需要额外的训练，这对使用LLM API的服务提供商来说并不实际，例如OpenAI API或Claude API。在本文中，我们提出了一种动态目标防御方法，通过改变解码超参数来增强模型对各种越狱攻击的鲁棒性。我们的方法不需要访问模型的内部结构，也不会产生额外的训练成本。所提出的防御措施包括两个关键组成部分：（1）通过识别和调整影响标记生成概率的解码超参数来优化解码策略；（2）将解码超参数和模型系统提示转换为动态目标，这些目标在每次运行期间持续改变。通过持续修改解码策略和提示，防御措施有效地缓解了现有攻击。我们的结果表明，在我们的测试中，当使用LLMs作为黑盒API时，我们的防御在三个模型中对越狱攻击最为有效。此外，我们的防御提供了较低的推理成本，并保持了可比的响应质量，使其在与其他防御方法一起使用时成为一种潜在的保护层。|
|**2024-12-10**|**Automating Business Intelligence Requirements with Generative AI and Semantic Search**|Nimrod Busany et.al.|[2412.07668](http://arxiv.org/abs/2412.07668)|null|在动态的商业环境中，对商业智能（BI）系统提出需求仍然是一个重大的挑战。本文介绍了一种名为AutoBIR的创新人工智能系统，该系统利用语义搜索和大型语言模型（LLMs）来自动化和加速BI需求的规格制定。该系统通过会话界面促进与利益相关者的直观互动，将用户输入转换为原型分析代码、描述和数据依赖。此外，AutoBIR生成详细的测试用例报告，可选地添加视觉辅助，简化需求提出过程。通过结合用户反馈，该系统优化BI报告和系统设计，展示了加快数据驱动决策的实际应用。本文探讨了生成式AI在转变BI开发方面的更广泛潜力，阐述了其在提高大规模、发展中的系统数据工程实践中的作用。|
|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过反复的语言学习和使用而演变，这些过程在语言习得期间引入了偏见，并塑造了语言系统以实现沟通效率。在这篇论文中，我们研究了如果人工语言被优化为针对大型语言模型（LLMs）的隐式偏见，是否会发生相同的情况。为此，我们模拟了一个经典指称游戏，其中LLMs学习和使用人工语言。我们的结果表明，最初无结构的整体语言确实被塑造出一些结构属性，使得两个LLM智能体能够成功沟通。与人类实验中的观察结果相似，代际传承增加了语言的易学性，但同时也可能导致非人类化的退化词汇。综上所述，这项工作扩展了实验发现，表明LLMs可以用作模拟语言进化的工具，并为此领域未来的机器-人实验开辟了可能性。|
|**2024-12-09**|**Training Large Language Models to Reason in a Continuous Latent Space**|Shibo Hao et.al.|[2412.06769](http://arxiv.org/abs/2412.06769)|**[link](https://github.com/facebookresearch/coconut)**|大型语言模型（LLMs）通常在“语言空间”中进行推理，通过思维链（CoT）来表述推理过程以解决复杂的推理问题。然而，我们认为语言空间并不总是推理的最优选择。例如，大多数单词标记主要用于文本连贯性，而非推理所必需，而一些关键标记则需要复杂的规划和给LLMs带来巨大挑战。为了探索LLMs在不受限制的潜在空间中进行推理的潜力，而不是使用自然语言，我们引入了一种新的范式——椰子（连续思维链）。我们利用LLM的最后隐藏状态作为推理状态的表示（称为“连续思维”）。我们不是将其解码为单词标记，而是直接将其作为连续空间中的后续输入嵌入反馈给LLM。实验表明，椰子可以有效地增强LLM在多个推理任务上的表现。这种新颖的潜在推理范式导致出现高级推理模式：连续思维可以编码多个替代的后续推理步骤，允许模型执行广度优先搜索（BFS）来解决问题，而不是像CoT那样过早地承诺单一确定路径。在需要大量回溯规划的某些逻辑推理任务中，椰子优于CoT，推理过程中思考标记更少。这些发现展示了潜在推理的潜力，并为未来的研究提供了宝贵的见解。|
|**2024-12-09**|**Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code**|Joy Krishan Das et.al.|[2412.06757](http://arxiv.org/abs/2412.06757)|null|大型语言模型（LLMs）如ChatGPT已显示出协助开发者进行编码和调试任务的潜力。然而，它们在协同问题解决中的角色尚未得到充分探索。在本研究中，我们分析了GitHub上1,012个问题中的1,152次开发者与ChatGPT的对话，以考察ChatGPT的多样使用和对其生成代码的依赖。我们的贡献有四个方面。首先，我们手动分析了289次对话，以了解ChatGPT在GitHub问题中的使用情况。我们的分析显示，ChatGPT主要用于创意构思，而其在验证（例如，代码文档准确性）方面的使用非常有限。其次，我们应用BERTopic模型来识别整个数据集中关键的关注领域。我们发现后端问题（例如，API管理）主导了对话，而测试却意外地覆盖较少。第三，我们利用CPD克隆检测工具来检查ChatGPT生成的代码是否被用于解决问题。我们的发现显示，ChatGPT生成的代码被直接用于解决仅占5.83%的问题。第四，我们使用基于RoBERTa的情感分析模型来估计情感，以确定开发者对不同用途和关注领域的满意度。我们发现，使用ChatGPT进行重构和解决数据分析（例如，分类表数据）问题的正面情绪（即，高度满意）。相反，当使用ChatGPT调试问题和解决自动化任务（例如，GUI交互）时，我们观察到负面情绪。我们的研究发现，开发者存在未满足的需求和日益增长的不满。研究人员和ChatGPT开发者应专注于开发特定任务的解决方案，以帮助解决各种问题，提高软件开发中的用户满意度和解决问题的效率。|
|**2024-12-09**|**Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models**|Neel Jain et.al.|[2412.06748](http://arxiv.org/abs/2412.06748)|null|构建安全可靠的语言模型的关键组成部分是使模型能够适当地拒绝遵循某些指令或回答某些问题。我们可能希望模型为各种用户查询类别输出拒绝消息，例如，无意义的问题、执行非法行为的指令，或需要超出模型知识范围的信息的查询。设计拒绝回答此类问题的模型复杂化，因为个人可能希望他们的模型在拒绝不同类别的查询时表现出不同水平的感觉性，不同的用户可能希望有不同的拒绝率。当前默认的方法涉及使用每个类别不同比例的拒绝消息训练多个模型以实现所需的拒绝率，这计算成本高，可能需要为每位用户的拒绝率偏好训练新的模型。为了解决这些挑战，我们提出了拒绝标记，每个拒绝类别一个标记，或一个单一的拒绝标记，这些标记在训练期间添加到模型的响应之前。然后，我们展示了如何在推理期间增加或减少生成每个类别拒绝标记的概率，以引导模型的拒绝行为。拒绝标记允许通过在生成过程中选择性干预来控制单个模型的拒绝率，而不需要任何进一步的微调。|
|**2024-12-09**|**JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM**|Takuro Fujii et.al.|[2412.06738](http://arxiv.org/abs/2412.06738)|**[link](https://github.com/retrieva/japagen)**|近期一些研究强调了大型语言模型（LLMs）作为有效的监督训练数据生成器的潜力，提供了如提高推理效率和降低数据收集相关成本等优势。然而，这些研究主要关注英语任务。在本文中，我们探讨了基本的研究问题：LLMs能否作为其他语言任务的优秀训练数据生成器？具体来说，我们利用LLMs在六种不同的日语下游任务下，在少样本和零样本学习场景中合成监督训练数据。随后，我们使用这些合成的数据训练紧凑模型（例如BERT）。这种新颖的方法被称为JAPAGEN。我们的实验发现表明，JAPAGEN在需要正式文本输入的分类任务中实现了稳健的性能，与传统的LLM提示策略相比，取得了具有竞争力的结果。|
|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|我们研究了大型语言模型（LLMs）在自动生成数据清理工作流中的推理能力。为了评估LLMs完成数据清理任务的能力，我们实现了一个基于LLM的自动数据清理工作流（AutoDCWorkflow）的管道，通过提示LLMs进行数据清理操作来修复三种类型的数据质量问题：重复数据、缺失值和不一致的数据格式。给定一个脏表和目的（以查询形式表达），此管道生成一个最小的、干净的表，足以满足目的，并生成用于生成该表的数据清理工作流。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的目标列集合。（2）检查列质量：评估每个目标列的数据质量，并生成数据质量报告作为操作目标。（3）生成操作与参数：根据数据质量报告的结果预测下一个操作和参数。此外，我们提出一个数据清理基准，以评估LLM代理自动生成解决不同难度水平数据清理目的的工作流的能力。基准包括注释数据集，作为一个包含目的、原始表、干净表、数据清理工作流和答案集的集合。在我们的实验中，我们评估了三种自动生成目的驱动数据清理工作流的LLMs。结果表明，LLMs在规划和生成数据清理工作流方面表现良好，无需微调。|
|**2024-12-09**|**OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions**|Yi-Kai Zhang et.al.|[2412.06693](http://arxiv.org/abs/2412.06693)|null|随着大型语言模型（LLMs）的快速发展，其应用范围得到了显著扩展，从多语言支持到特定领域的任务和多模态集成。本文介绍了一种新型的基准测试工具箱OmniEvalKit，旨在评估LLMs及其全功能扩展在多语言、多领域和多模态能力方面的表现。与现有专注于单一方面的基准测试不同，OmniEvalKit提供了一个模块化、轻量化和自动化的评估系统。它采用模块化架构，包括静态构建器和动态数据流，促进了新模型和数据集的无缝集成。OmniEvalKit支持超过100种LLMs和50个评估数据集，覆盖了成千上万种模型-数据集组合的全面评估。OmniEvalKit致力于创建一个超轻量级且快速部署的评估框架，使下游应用对人工智能社区更加便捷和灵活。|
|**2024-12-09**|**Exploring Critical Testing Scenarios for Decision-Making Policies: An LLM Approach**|Weichao Xu et.al.|[2412.06684](http://arxiv.org/abs/2412.06684)|null|近年来，决策政策在各种领域，如自动驾驶和机器人技术，取得了令人惊讶的成就。在存在可能威胁其可靠性的关键场景的情况下，对决策政策进行测试至关重要。众多研究努力致力于测试这些政策。然而，由于测试政策和环境的复杂性，仍然存在重大挑战，例如测试效率低和多样性不足。受大型语言模型（LLMs）卓越能力的影响，本文提出了一种基于LLM的在线测试框架，以有效地测试决策政策。主要思路是利用基于LLM的测试场景生成器通过思考和推理智能地生成具有挑战性的测试案例。具体来说，我们首先设计了一个“生成-测试-反馈”流程，并应用模板提示工程充分利用LLMs的知识和推理能力。然后，我们引入了一种多尺度场景生成策略来解决LLMs在精细调整方面固有的挑战，从而进一步提高测试效率。最后，我们在五个广泛使用的基准上评估了基于LLM的方法。实验结果表明，我们的方法在揭示关键和多样化的场景方面显著优于基线方法。|
|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通运输系统需求建模和仿真领域，基于代理模型和微观模拟是目前最先进的方法。然而，现有的基于代理模型在行为真实性和资源需求方面仍存在一些限制，这限制了它们的适用性。在本研究中，我们利用新兴的大语言模型（LLMs）和基于LLMs的代理技术，提出了一种适用于交通运输系统的一般LLM-代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，而且为克服现有基于代理模型的某些局限性提供了有希望的解决方案。我们的概念框架设计紧密模拟了交通网络中人类旅行者的决策、交互过程和特征，并通过相关研究和LLM代理在瓶颈设置中的学习和调整的演示实例，证明了所提出的系统可以满足决策和学习行为的关键行为标准。尽管需要进一步细化基于LLM的代理建模框架，但我们相信这种方法有可能提高交通运输系统建模和仿真的水平。|
|**2024-12-09**|**I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token**|Roi Cohen et.al.|[2412.06676](http://arxiv.org/abs/2412.06676)|null|大型语言模型因其能够捕捉现实世界知识而闻名，这使得它们在许多下游任务中表现出色。尽管近年来取得了进展，但这些模型仍然容易受到所谓的“幻觉”的影响，导致它们产生不想要且事实错误的文章。在本研究中，我们提出了一种新颖的校准方法，可用于对抗幻觉。我们向模型的词汇表中添加了一个特殊的[IDK]（“我不知道”）标记，并引入了一个目标函数，该函数将概率质量转移到[IDK]标记以应对错误的预测。这种方法允许模型在其输出中明确表达不确定性。我们在多个模型架构和事实性下游任务中评估了我们的方法。我们发现，使用我们的方法训练的模型能够在它们之前可能出错的地方表达不确定性，同时只损失少量的编码知识。我们还对多种方法变体进行了广泛的消融研究，并提供了对我们方法精确度-召回率权衡的详细分析。|
|**2024-12-09**|**ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**|Chunwei Wang et.al.|[2412.06673](http://arxiv.org/abs/2412.06673)|null|在这篇论文中，我们介绍了ILLUME，这是一种统一的多元模态大型语言模型（MLLM），通过统一的下一个标记预测公式，将多元模态的理解和生成能力无缝集成到单个大型语言模型中。为了解决图像-文本对齐通常所需的大量数据集大小，我们提出通过设计一个结合语义信息的视觉标记化器和渐进式多阶段训练程序来提高数据效率。这种方法将数据集大小减少到仅为15M用于预训练——仅为通常所需数量的四分之一——同时实现了与现有统一MLLMs（如Janus）相当甚至更优的性能。此外，为了促进理解和生成能力之间的协同增强，这是以往工作中较少探索的，我们引入了一种新颖的自我增强多元模态对齐方案。该方案监督MLLM自我评估文本描述和自生成图像之间的一致性，促进模型更准确地解释图像，并避免由图像生成中的对齐错误引起的非现实和不正确预测。基于广泛的实验，我们提出的ILLUME在各种多元模态理解、生成和编辑的基准测试中脱颖而出，并与其他最先进的统一MLLMs和专业模型竞争。|
|**2024-12-06**|**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**|Zhe Chen et.al.|[2412.05271](http://arxiv.org/abs/2412.05271)|**[link](https://github.com/opengvlab/internvl)**|我们推出了InternVL 2.5，这是一个基于InternVL 2.0的高级多模态大型语言模型（MLLM）系列，保持了其核心模型架构，同时在训练和测试策略以及数据质量方面引入了显著的提升。在这项工作中，我们深入探讨了模型规模与性能之间的关系，系统地研究了视觉编码器、语言模型、数据集规模和测试时配置的性能趋势。通过在包括跨学科推理、文档理解、多图像/视频理解、现实世界理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理等广泛基准上的广泛评估，InternVL 2.5展现出具有竞争力的性能，与GPT-4o和Claude-3.5-Sonnet等领先的商业模型相媲美。值得注意的是，我们的模型是第一个在MMMU基准测试中超过70%的开源MLLM，通过思维链（CoT）推理实现了3.7分的提升，并展示了强大的测试时缩放潜力。我们希望这个模型通过设定开发和应用多模态AI系统的新标准，为开源社区做出贡献。HuggingFace演示请见https://huggingface.co/spaces/OpenGVLab/InternVL|
|**2024-12-06**|**APOLLO: SGD-like Memory, AdamW-level Performance**|Hanqing Zhu et.al.|[2412.05270](http://arxiv.org/abs/2412.05270)|**[link](https://github.com/zhuhanqing/APOLLO)**|大型语言模型（LLMs）在训练过程中对内存的消耗非常严重，尤其是使用流行的AdamW优化器时。这种内存负担迫使人们使用更多或更高端的GPU，或者减小批处理大小，从而限制了训练的可扩展性和吞吐量。为了解决这个问题，已经提出了各种内存高效的优化器来减少优化器的内存使用。然而，它们面临着一些关键的挑战：（i）依赖于昂贵的奇异值分解（SVD）操作；（ii）与AdamW相比，性能上有显著的权衡；（iii）仍然有相当大的优化器内存开销以维持竞争优势。  在这项工作中，我们发现AdamW的学习率自适应规则可以作为结构化学习率更新有效地粗化。基于这一洞察，我们提出了近似梯度缩放用于内存高效LLM优化（APOLLO），它使用基于纯随机投影的辅助低秩优化器状态来近似学习率缩放。这种结构化学习率更新规则使得APOLLO对进一步减少内存具有高度容忍性，同时在预训练性能上与AdamW相当。即使是它的秩-1变体APOLLO-Mini，在具有与SGD相当内存成本的条件下，也比AdamW实现了更优的预训练性能。  大量实验表明，APOLLO系列的性能与AdamW相当或更好，同时通过几乎消除AdamW的优化状态，实现了更大的内存节省。这些节省带来了显著的系统级好处：（1）提高了吞吐量：在8xA100-80GB的配置上，通过支持4倍更大的批处理大小，比AdamW实现了3倍的吞吐量。（2）提高了模型的可扩展性：在A100-80GB GPU上使用原始的分布式数据并行（DDP）预训练LLaMA-13B，而不进行系统级优化。（3）低端GPU友好的预训练：使用权重量化，在单个GPU上预训练LLaMA-7B，内存使用量少于12GB。|
|**2024-12-06**|**CompCap: Improving Multimodal Large Language Models with Composite Captions**|Xiaohui Chen et.al.|[2412.05243](http://arxiv.org/abs/2412.05243)|null|多模态大型语言模型（MLLMs）在理解复合图像方面的能力如何？复合图像（CIs）是通过合并多个视觉元素（如图表、海报或截图）合成的合成视觉，而不是通过相机直接捕捉的。虽然CIs在现实世界应用中很普遍，但最近MLLM的发展主要集中于解读自然图像（NIs）。我们的研究揭示，当前的MLLM在准确理解CIs方面面临着重大挑战，常常难以从这些图像中提取信息或进行复杂推理。我们发现，现有的CIs训练数据大多格式化为问答任务（例如，在ChartQA和ScienceQA等数据集中），而高质量的图像-描述数据集，对于稳健的视觉-语言对齐至关重要，却只有自然图像（NIs）才有。为了弥合这一差距，我们引入了复合描述（CompCap），这是一个灵活的框架，利用大型语言模型（LLMs）和自动化工具来合成准确且详细的复合图像。使用CompCap，我们编纂了CompCap-118K数据集，包含118K个图像-描述对，涵盖六种复合图像类型。我们通过监督微调三种规模的MLLMs（xGen-MM-inst.-4B和LLaVA-NeXT-Vicuna-7B/13B）来验证CompCap-118K的有效性。实证结果表明，CompCap-118K显著提升了MLLMs对复合图像的理解能力，分别在11个基准测试中实现了1.7%、2.0%和2.9%的平均提升。|
|**2024-12-06**|**MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale**|Jarvis Guo et.al.|[2412.05237](http://arxiv.org/abs/2412.05237)|null|开源的多模态大型语言模型（MLLMs）在多模态任务中展现出巨大的潜力。然而，它们的推理能力仍然受到现有指令微调数据集的限制，这些数据集主要来自VQA、AI2D和ChartQA等学术数据集，这些数据集针对的是简单的任务，并且只提供短语级别的答案，没有任何中间推理过程。为了解决这些挑战，我们提出了一种可扩展且成本效益高的方法来构建一个包含丰富中间推理过程的、大规模多模态指令微调数据集。我们仅使用开源模型，创建了一个包含1200万个指令-响应对的数据库，涵盖了多样化的、推理密集型任务，具有详细和可靠的推理过程。实验表明，在这样一个数据集上训练MLLMs可以显著提高推理能力，在MathVerse (+8.1%)、MMMU-Pro (+7%)和MuirBench (+13.3%)等基准测试中实现了最先进的性能。此外，该模型在非推理型基准测试上也表现出显著的提升，最高可达4%。消融实验进一步突出了数据集构建过程中关键组件，如重写和自我过滤的重要性。|
|**2024-12-06**|**BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits**|Wazib Ansar et.al.|[2412.05225](http://arxiv.org/abs/2412.05225)|null|大型语言模型（LLMs）基于Transformer在各种应用中取得了尖端成果。然而，它们的巨大规模和处理需求使得在资源受限的设备上部署极为困难。在各种效率考虑中，模型二值化和早期退出（EE）是常见的有效解决方案。然而，二值化可能会由于降低精度而影响梯度估计和参数更新，从而导致性能损失。此外，目前的早期退出机制仍处于研究的初级阶段。为了改善这些问题，我们提出了二值化早期退出Transformer（BEExformer），这是第一个将早期退出与二值化结合用于文本推理的选区学习Transformer架构。它通过到冲激函数的微分二阶近似来改进二值化过程。这使得可以计算关于权重符号和幅度的梯度。与基于绝对阈值的EE不同，所提出的EE机制依赖于中间Transformer块中软路由损失估计的熵的分数减少。虽然二值化使模型大小减少了18.44倍，但早期退出在推理过程中将FLOPs减少了54.85%，甚至通过解决深层网络固有的“过度思考”问题，提高了5.98%的准确率。此外，所提出的BEExformer通过不需要从全精度LLM中进行知识蒸馏来简化训练。在GLUE数据集上的广泛评估与SOTA工作的比较展示了其帕累托最优的性能-效率权衡。|
|**2024-12-06**|**100% Hallucination Elimination Using Acurai**|Michael C. Wood et.al.|[2412.05223](http://arxiv.org/abs/2412.05223)|**[link](https://github.com/AcuChat/acurai-RAGTruth-conflict-resolution)**|大型语言模型（LLMs）中的幻觉问题仍然是人工智能在企业和其他高风险应用中应用的一个关键障碍。尽管检索增强生成（RAG）系统取得了进展，但当前最先进的方法在生成忠实且事实正确的输出时，即使在提供相关和准确的情况下，也未能超过80%的准确率。在这项工作中，我们引入了Acurai，这是一种新颖的系统方法，通过在输入之前重新格式化查询和上下文数据，在LLMs中实现了100%无幻觉的响应。利用对LLMs内部表示的深入了解、名词短语的主导地位的重要性以及离散功能单元（DFUs）的作用，Acurai确保输入上下文和生成输出之间的一致性。我们使用RAGTruth语料库验证了这种方法，证明了它能够消除GPT-4和GPT-3.5 Turbo的100%幻觉。Acurai为实现一致、准确和忠实的AI响应设定了新的标准，标志着可信AI系统发展的重大进步。|
|**2024-12-06**|**Evaluating and Aligning CodeLLMs on Human Preference**|Jian Yang et.al.|[2412.05210](http://arxiv.org/abs/2412.05210)|null|代码大型语言模型（codeLLMs）在代码生成方面取得了显著进展。大多数之前的与代码相关的基准测试，包括各种编程练习和相应的测试用例，被用作评估codeLLMs性能和能力的共同标准。然而，当前codeLLMs主要关注合成正确的代码片段，忽略了与人类偏好的对齐，其中查询应从实际应用场景中采样，而模型生成的响应应满足人类偏好。为了弥合模型生成响应与人类偏好之间的差距，我们提出了一个严格的人类编纂基准测试CodeArena，以模拟现实世界编程任务的复杂性和多样性，其中包含从用户查询中精心挑选的397个高质量样本，涵盖了40个类别和44种编程语言。此外，我们提出了一个多样化的合成指令语料库SynCode-Instruct（近20B个标记），通过扩展网站上的指令来验证大规模合成指令微调的有效性，其中Qwen2.5-SynCoder完全在合成指令数据上训练，可以达到开源codeLLMs的顶尖性能。结果表明，在基于执行的基准测试和CodeArena之间存在性能差异。我们对40多个LLMs在CodeArena上的系统实验揭示了开源SOTA代码LLMs（例如Qwen2.5-Coder）与专有LLMs（例如，OpenAI o1）之间存在显著的性能差距，突显了与人类偏好对齐的重要性。[footnote：https://codearenaeval.github.io/ ]|
|**2024-12-06**|**A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges**|Aditi Singh et.al.|[2412.05208](http://arxiv.org/abs/2412.05208)|null|文本到SQL系统通过将自然语言查询翻译为结构化查询语言（SQL），促进了与数据库的顺畅交互，弥合了非技术用户与复杂数据库管理系统之间的差距。本综述全面概述了AI驱动的文本到SQL系统的演变，突出了其基础组件、大型语言模型（LLM）架构的进步以及Spider、WikiSQL和CoSQL等数据集在推动进展中的关键作用。我们探讨了文本到SQL在医疗保健、教育和金融等领域的应用，强调了它们在提高数据可访问性方面的变革潜力。此外，我们分析了持续存在的挑战，包括领域泛化、查询优化、支持多轮对话交互以及针对NoSQL数据库和动态现实场景量身定制的数据集有限可用性。为了应对这些挑战，我们概述了未来的研究方向，例如扩展文本到SQL的功能以支持NoSQL数据库，设计用于动态多轮交互的数据集，以及优化系统以适应现实世界的可扩展性和鲁棒性。通过审视当前进展并识别关键差距，本文旨在指导基于LLM的文本到SQL系统下一代的研发与应用。|
|**2024-12-06**|**Are Frontier Large Language Models Suitable for Q&A in Science Centres?**|Jacob Watson et.al.|[2412.05200](http://arxiv.org/abs/2412.05200)|null|本文探讨了前沿大型语言模型（LLMs）在科学中心问答互动中的适用性，旨在提高游客参与度同时保持事实准确性。利用从英国莱斯特国家空间中心收集的问题数据集，我们评估了三个领先模型生成的回答：OpenAI的GPT-4、Claude 3.5 Sonnet和Google Gemini 1.5。每个模型都被要求针对8岁儿童观众提供标准答案和创造性回答，这些回答由空间科学专家根据准确性、参与度、清晰度、新颖性和偏离预期答案的程度进行评估。结果显示，在创造性和准确性之间存在着权衡，尽管Claude在保持清晰度和吸引年轻观众方面超越了GPT和Gemini，甚至在要求生成更富有创造性的回答时也是如此。然而，专家观察到，所有模型中更高的新颖性通常与事实可靠性降低有关。这项研究突出了LLMs在教育环境中的潜力，强调了精心设计提示以平衡参与度和科学严谨性的必要性。|
|**2024-12-06**|**SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot**|Jinlin Wu et.al.|[2412.05187](http://arxiv.org/abs/2412.05187)|**[link](https://github.com/franciszchen/surgbox)**|**手术干预，尤其是在神经科领域，代表复杂且高风险的场景，对手术团队提出了巨大的认知负担。尽管有目的的教育和实践可以增强认知能力，但由于患者安全问题的考虑，手术培训机会仍然有限。为了解决手术培训和手术中的认知挑战，我们提出了SurgBox，一个由代理驱动的沙盒框架，旨在系统性地提高外科医生在沉浸式手术模拟中的认知能力。具体来说，我们的SurgBox利用定制化的检索增强生成（RAG）的大型语言模型（LLMs）来真实地复制各种手术角色，从而为有目的的练习提供逼真的训练环境。特别是，我们设计了手术协同助手（Surgery Copilot），这是一个由AI驱动的助手，能够主动协调手术信息流并支持临床决策，从而减轻手术过程中手术团队的认知负荷。通过整合新颖的长短期记忆（Long-Short Memory）机制，我们的手术协同助手可以有效地在即时程序辅助和全面手术知识之间取得平衡。使用真实的神经外科手术记录进行的广泛实验验证了我们的SurgBox框架在提高手术认知能力和支持临床决策方面的有效性。通过提供针对培训和操作支持的综合性解决方案以解决认知挑战，我们的SurgBox框架推动了外科教育和实践的发展，有可能改变手术结果和医疗质量。代码可在https://github.com/franciszchen/SurgBox获取。**|
|**2024-12-05**|**p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**|Jun Zhang et.al.|[2412.04449](http://arxiv.org/abs/2412.04449)|**[link](https://github.com/mcg-nju/p-mod)**|**尽管多模态大型语言模型（MLLMs）在众多任务中表现出色，但其巨大的训练和推理成本阻碍了其发展。大部分计算量来自于被Transformer解码器处理的视觉标记的庞大数量。在本文中，我们提出通过利用混合深度（MoD）机制来构建高效的MLLMs，其中每个Transformer解码器层选择必要的视觉标记进行处理，同时跳过冗余的标记。然而，将MoD集成到MLLMs中并非易事。为了解决训练和推理稳定性以及有限训练数据带来的挑战，我们对MoD模块进行了两项创新设计：tanh门控权重归一化（TanhNorm）和对称标记重新加权（STRing）。此外，我们观察到视觉标记在深层中的冗余性更高，因此设计了一种渐进比率衰减（PRD）策略，该策略通过偏移余弦调度逐步减少每层的标记保留率。这一关键设计充分发挥了MoD的潜力，显著提升了我们模型的效率和性能。为了验证我们方法的有效性，我们在14个基准测试中，对两个基线模型进行了广泛的实验。我们的模型p-MoD在推理时仅占用了55.6%的TFLOPs和53.8%的KV缓存存储，以及训练时的77.7%的GPU小时，其性能与基线模型相当，甚至在某些情况下超过了基线模型。**|
|**2024-12-05**|**EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios**|Lu Qiu et.al.|[2412.04447](http://arxiv.org/abs/2412.04447)|null|多模态大型语言模型的兴起，借助大型语言模型的力量，最近展示了卓越的多模态理解和推理能力，预示着人工通用智能新时代的到来。然而，实现通用人工智能不仅需要理解和推理能力，还需要在多样场景中有效规划的能力，这涉及到基于复杂环境做出合理决策以解决现实问题。尽管其重要性不言而喻，但当前多模态大型语言模型在不同场景下的规划能力仍处于探索阶段。在本文中，我们介绍了EgoPlan-Bench2，这是一个严格且全面的基准，旨在评估多模态大型语言模型在广泛现实场景中的规划能力。EgoPlan-Bench2涵盖了涵盖4个主要领域和24个详细场景的日常任务，与人类日常生活紧密相关。EgoPlan-Bench2是通过半自动流程构建的，利用以自我为中心的视频，并辅以人工验证。基于第一人称视角，它反映了人类在日常生活中的问题解决方式。我们评估了21个竞争性的多模态大型语言模型，并深入分析了它们的局限性，揭示它们在现实世界规划中面临重大挑战。为了进一步提高当前多模态大型语言模型的规划能力，我们提出了一种无需训练的方法，通过研究复杂规划中各种多模态提示的有效性，使用多模态思维链（CoT）提示。我们的方法在不额外训练的情况下，将GPT-4V在EgoPlan-Bench2上的性能提高了10.24。我们的工作不仅揭示了当前多模态大型语言模型在规划方面的局限性，还为这一关键领域的未来改进提供了见解。我们已经将数据和代码发布在https://qiulu66.github.io/egoplanbench2/。|
|**2024-12-05**|**Moto: Latent Motion Token as the Bridging Language for Robot Manipulation**|Yi Chen et.al.|[2412.04445](http://arxiv.org/abs/2412.04445)|**[link](https://github.com/tencentarc/moto)**|最近，在大量语料库上预训练的大型语言模型在多种自然语言处理任务中取得了显著的成功，且仅需少量微调。这一成功为机器人学带来了新的希望，因为机器人学长期以来一直受限于高成本的动作标签数据。我们提出问题：鉴于大量包含互动相关知识的视频数据作为丰富的“语料库”可用，是否可以有效地将类似的生成式预训练方法应用于增强机器人学习？关键挑战是识别一个有效的自回归预训练表示，以促进机器人操作任务。受人类通过观察动态环境学习新技能的方式的启发，我们认为有效的机器人学习应强调与运动相关的知识，这些知识与低级动作紧密相关，并且与硬件无关，便于将学习到的运动转移到实际机器人动作中。为此，我们引入了Moto，它通过潜在运动标记器将视频内容转换为潜在运动标记序列，以无监督的方式从视频中学习运动的“桥梁”语言。我们通过运动标记自回归预训练Moto-GPT，使其能够捕捉多样的视觉运动知识。预训练后，Moto-GPT展示了产生语义可解释的运动标记、预测合理的运动轨迹以及通过输出似然性评估轨迹合理性等有希望的能力。为了将学习到的运动先验转移到真实机器人动作中，我们实施了一种协同微调策略，无缝地将潜在运动标记预测和真实机器人控制连接起来。大量实验表明，经过微调的Moto-GPT在机器人操作基准测试中表现出卓越的鲁棒性和效率，凸显了它从视频数据到下游视觉操作任务中知识转移的有效性。|
|**2024-12-05**|**Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation**|Yuying Ge et.al.|[2412.04432](http://arxiv.org/abs/2412.04432)|**[link](https://github.com/tencentarc/divot)**|**近年来，在大型语言模型（LLMs）中统一图像理解和生成引起了极大的兴趣。这种不断增长的兴趣促使我们探索将这种统一扩展到视频中。核心挑战在于开发一个通用的视频分词器，它能够捕捉视频的空间特征和时序动态，以获得适合LLMs的表示，并且这些表示可以被进一步解码为逼真的视频片段，从而实现视频生成。在这项工作中，我们介绍了Divot，一种基于扩散的视频分词器，它利用扩散过程进行自监督视频表示学习。我们认为，如果一个视频扩散模型能够通过将视频分词器的特征作为条件来有效地去噪视频片段，那么分词器已经成功地捕捉了鲁棒的空间和时序信息。此外，视频扩散模型本质上充当了解码器，将视频从其表示中解码出来。在Divot分词器的基础上，我们通过视频到文本的自回归和文本到视频的生成，使用高斯混合模型来建模连续值的Divot特征分布，提出了Divot-Vicuna。实验结果表明，我们的基于扩散的视频分词器，当与预训练的LLM集成时，在各种视频理解和生成基准测试中实现了有竞争力的性能。经过指令调整的Divot-Vicuna在视频叙事方面也表现出色，能够生成交错的故事和相应的视频。**|
|**2024-12-05**|**Grounding Descriptions in Images informs Zero-Shot Visual Recognition**|Shaunak Halbe et.al.|[2412.04429](http://arxiv.org/abs/2412.04429)|**[link](https://github.com/shaunak27/grain-clip)**|**视觉语言模型（VLMs）如CLIP因其能够在开放词汇概念上执行零样本视觉识别而备受青睐。这是通过选择与查询图像文本表示最相似的物体类别来实现的。尽管在某些领域取得了成功，但这种方法在识别细粒度实体以及泛化到训练分布未捕获的未见概念方面存在困难。近期的工作试图通过在测试时整合类别描述来减轻这些挑战，尽管取得了有限的改进。我们将这些有限的收益归因于图像和描述表示之间的基本不匹配，这种不匹配根植于CLIP的预训练结构。在这篇论文中，我们提出了GRAIN，这是一种新的预训练策略，旨在同时在对细粒度和粗粒度级别上对齐表示。我们的方法学会联合地将文本描述定位到图像区域，并将总体标题与全局图像表示对齐。为了推动这种预训练，我们利用冻结的多模态大型语言模型（MLLMs）来生成大规模合成注释。我们在11个不同的图像分类数据集上展示了我们模型相较于现有最先进方法的零样本性能提升。此外，我们引入了Products-2023，这是一个新整理的、手动标记的数据集，包含新颖的概念，并通过在该数据集上进行基准测试展示了我们模型识别这些概念的能力。我们模型在其他下游任务（如检索）上取得的显著改进进一步突显了我们方法学习的表示的高质量。代码可在https://github.com/shaunak27/grain-clip上获取。**|
|**2024-12-05**|**Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion**|Jiuhai Chen et.al.|[2412.04424](http://arxiv.org/abs/2412.04424)|**[link](https://github.com/jiuhaichen/florence-vl)**|**我们介绍了一组新的多模态大型语言模型（MLLMs），即Florence-VL，它由Florence-2生成视觉基础模型产生，具有丰富的视觉表示。与广泛使用的由对比学习训练的CLIP风格视觉Transformer不同，Florence-2能够捕捉不同层次和方面的视觉特征，这使得它们更加灵活，可以适应各种下游任务。我们提出了一种新颖的特征融合架构和一种创新的训练方案，有效地将Florence-2的视觉特征整合到预训练的LLM（如Phi 3.5和LLama 3）中。特别是，我们提出了“深度-呼吸融合（DBFusion）”来融合从不同深度和多个提示中提取的视觉特征。我们的模型训练包括整个模型的端到端预训练，随后是在精心设计的包含高质量图像标题和指令调整对的多样化开源数据集上对投影层和LLM进行微调。我们对Florence-VL的视觉特征的定量分析和可视化表明，它在视觉-语言对齐方面优于流行的视觉编码器，其中丰富的深度和呼吸发挥了重要作用。Florence-VL在涵盖一般视觉问答（VQA）、感知、幻觉、OCR、图表、知识密集型理解等多种多模态和视觉中心基准测试中，相对于现有的最先进MLLMs实现了显著的改进。为了促进未来的研究，我们的模型和完整的训练方案已经开源。https://github.com/JiuhaiChen/Florence-VL**|
|**2024-12-05**|**Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation**|Xuying Li et.al.|[2412.04415](http://arxiv.org/abs/2412.04415)|null|人工智能代理，由大型语言模型（LLMs）驱动，通过实现无缝、自然和情境感知的通信，已经改变了人机交互。虽然这些进步提供了巨大的实用性，但它们也继承了并放大了固有的安全风险，如偏见、公平性、幻觉、隐私侵犯和缺乏透明度。本文调查了一个关键漏洞：针对AI代理中LLM核心的对抗性攻击。具体而言，我们测试了一个假设，即一个欺骗性的简单对抗性前缀，例如“忽略文档”，可以通过绕过其情境保护措施，迫使LLMs生成危险或不希望的结果。通过实验，我们证明了高攻击成功率（ASR），揭示了现有LLM防御的脆弱性。这些发现强调了迫切需要针对LLM层面以及更广泛的基于代理的架构，采取稳健的多层安全措施来减轻漏洞。|
|**2024-12-05**|**Retrieval-Augmented Machine Translation with Unstructured Knowledge**|Jiaan Wang et.al.|[2412.04342](http://arxiv.org/abs/2412.04342)|**[link](https://github.com/krystalan/RAGtrans)**|**检索增强生成（RAG）通过引入额外信息来提升大型语言模型（LLMs）。在机器翻译（MT）领域，以往的研究通常从配对MT语料库中检索上下文示例，或从知识图中检索特定领域的知识，以增强模型的MT能力。然而，大量的世界知识组织在非结构化文档中，并且可能在不同语言之间没有完全配对。在本文中，我们研究了使用非结构化文档的检索增强MT。具体来说，我们构建了RAGtrans，这是第一个用于训练和评估LLMs检索增强MT能力的基准。RAGtrans包含了通过GPT-4o和人工翻译收集的79K MT样本。此外，还提供了不同语言的文档，为这些样本提供知识。基于RAGtrans，我们进一步提出了一种多任务训练方法，教导LLMs如何在翻译过程中使用多语言文档中的信息。该方法利用现有的多语言语料库创建辅助训练目标，无需额外的标注需求。大量实验表明，该方法将LLMs的BLEU得分提高了1.58-3.09，COMET得分提高了1.00-2.03。**|
|**2024-12-05**|**Liquid: Language Models are Scalable Multi-modal Generators**|Junfeng Wu et.al.|[2412.04332](http://arxiv.org/abs/2412.04332)|**[link](https://github.com/foundationvision/liquid)**|我们提出了Liquid，一种将视觉理解与生成无缝集成的自回归生成范式。Liquid通过将图像分词成离散代码，并在共享的特征空间中学习这些代码嵌入和文本标记，从而在视觉和语言之间实现整合。与之前的跨模态大型语言模型（MLLM）不同，Liquid使用单个大型语言模型（LLM）来实现这种整合，消除了使用外部预训练的视觉嵌入（如CLIP）的需求。Liquid首次揭示了一种缩放定律，即随着模型规模的增加，统一训练视觉和语言任务所带来的性能下降不可避免地减小。此外，统一的标记空间使得视觉生成和理解任务可以相互增强，有效地消除了早期模型中常见的干扰。我们表明，现有的LLM可以作为Liquid的强大基础，节省100倍的训练成本，同时在多模态能力上优于Chameleon，并保持与主流LLM（如LLAMA2）相当的语言性能。Liquid还优于SD v2.1和SD-XL（在MJHQ-30K上的FID为5.47），在视觉-语言和纯文本任务上都表现出色。这项工作证明了LLAMA3.2和GEMMA2等LLM是强大的多模态生成器，为增强视觉-语言理解和生成提供了可扩展的解决方案。代码和模型将发布。|
|**2024-12-05**|**The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation**|Fredrik Carlsson et.al.|[2412.04318](http://arxiv.org/abs/2412.04318)|null|本文介绍了在非常小的数据集上对过拟合预训练大型语言模型（LLMs）的出人意料的泛化结果。在开放式文本生成的背景下，有记录表明LLMs倾向于生成重复和乏味的序列，这种现象在使用贪婪解码生成时尤为明显。即使是最先进的、包含数十亿参数的LLMs，它们在大型数据集上通过下一标记预测进行训练，这一问题依然存在。我们发现，通过进一步微调这些模型，使其在少量样本集上达到几乎为零的训练损失——我们称之为超拟合——可以极大地增强其长序列生成能力。使用这些超拟合模型进行贪婪解码，甚至在多样性和人类偏好方面都优于长序列的Top-P采样。这一现象适用于各种大小、不同领域的LLMs，甚至包括自回归图像生成。我们进一步发现，这一现象与Grokking和双重下降现象有显著不同。令人惊讶的是，我们的实验表明，超拟合模型很少陷入它们训练过的重复序列，甚至明确阻止这些序列也会产生高质量的输出。所有超拟合模型都产生极低熵的预测，通常将几乎全部概率分配给单个标记。|
|**2024-12-04**|**From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**|Xinyi Mou et.al.|[2412.03563](http://arxiv.org/abs/2412.03563)|**[link](https://github.com/fudandisc/socialagent)**|传统的社会学研究通常依赖人类参与，虽然有效，但成本高昂、难以扩展，且存在伦理问题。近年来，大型语言模型（LLMs）的进步突显了它们模拟人类行为的能力，使得个体反应的复制和跨学科研究得以进行。在本文中，我们对这一领域进行了全面调查，展示了由LLMs赋能的代理推动的模拟近期进展。我们将模拟分为三类：（1）个体模拟，模仿特定个体或人口群体；（2）情景模拟，多个代理在特定情境中协作实现目标；（3）社会模拟，模拟代理社会中的互动，以反映现实世界动态的复杂性和多样性。这些模拟从详细的个体建模到大规模社会现象，呈现出一种渐进性。我们对每种模拟类型进行了详细讨论，包括模拟的架构或关键组件、目标或情景的分类以及评估方法。之后，我们总结了常用的数据集和基准。最后，我们讨论了这三种类型模拟的趋势。相关资源的存储库位于{\url{https://github.com/FudanDISC/SocialAgent}}。|
|**2024-12-04**|**SPICE: Smart Projection Interface for Cooking Enhancement**|Vera Prohaska et.al.|[2412.03551](http://arxiv.org/abs/2412.03551)|**[link](https://github.com/ieroboticsailab/spice)**|可触摸用户界面（TUI）用于人机交互（HCI），旨在向用户提供数字信息的物理表示，以克服基于屏幕界面的局限性。尽管文献中存在许多引人注目的TUI演示，但针对日常双手任务和过程，如烹饪的TUI研究却很少。为了填补这一空白，我们提出了SPICE（智能投影界面，用于烹饪增强）。SPICE在厨房环境中研究TUI，旨在将食谱遵循体验从简单的基于文本转变为直观互动。SPICE包括跟踪系统、基于代理的软件和视觉大型语言模型，以创建和解释一个将食谱信息直接投影到烹饪表面的厨房环境。我们对SPICE和基于文本的食谱遵循进行了30名参与者的比较可用性研究，评估了任务难度、总时长和效率，以及用户信心和味觉感知。结果表明，SPICE使参与者能够在更短的时间内完成食谱，同时提高了自我报告的效率、信心和味觉。尽管如此，参与者报告说总体难度没有变化，这是未来研究的方向。总的来说，SPICE项目展示了使用TUI改善日常活动的潜力，为HCI和新型计算界面的未来研究铺平了道路。|
|**2024-12-04**|**Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models**|Natalie Mackraz et.al.|[2412.03537](http://arxiv.org/abs/2412.03537)|null|大型语言模型（LLMs）正越来越多地被调整为具有特定任务性，以便在现实世界的决策系统中部署。先前的一些研究通过研究微调适配策略对模型公平性的影响，来调查偏见迁移假说（BTH），发现预训练的掩码语言模型在微调适配时的公平性影响有限。在本工作中，我们扩展了对BTH的研究，将其应用于提示适应下的因果模型，因为提示是一种易于访问且计算高效的部署模型的方法。与先前的研究不同，我们通过一个代词共指消解任务，建立了一个事实：预训练的Mistral、Falcon和Llama模型中的内在偏见与在相同模型零样本和少样本提示时的偏见高度相关（相关系数rho >= 0.94）。此外，我们发现，即使LLMs被特别提示以展示公平或偏见行为（rho >= 0.92），以及少样本长度和刻板化组成发生变化（rho >= 0.97），偏见迁移仍然高度相关。我们的发现强调了确保预训练LLMs公平性的重要性，特别是在它们后来通过提示适配执行下游任务时。|
|**2024-12-04**|**A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences**|Gabriel Lino Garcia et.al.|[2412.03531](http://arxiv.org/abs/2412.03531)|null|这篇论文回顾了大型语言模型（LLMs）在生物医学领域的最新应用，探讨了它们在自动化复杂任务，如从生物医学文献数据库中提取证据和数据方面的有效性。虽然LLMs展现出巨大的潜力，但仍然存在重大挑战，包括幻觉、上下文理解和跨多种医疗任务泛化能力的问题。我们指出了当前研究文献中的关键差距，尤其是需要统一的基准来标准化评估并确保实际应用中的可靠性。此外，我们提出了未来研究方向，强调将检索增强生成（RAG）等最先进技术集成到LLMs中，以提高证据综合性能。通过解决这些挑战并利用LLMs的优势，我们旨在提高获取医学文献的途径并促进医疗保健领域的重大发现。|
|**2024-12-04**|**FANAL -- Financial Activity News Alerting Language Modeling Framework**|Urjitkumar Patel et.al.|[2412.03527](http://arxiv.org/abs/2412.03527)|null|在快速发展的金融领域，准确及时地解读市场新闻对于需要应对不可预测事件的相关利益方至关重要。本文介绍了FANAL（金融活动新闻警报语言建模框架），这是一个专门为实时金融事件检测和分析而设计的基于BERT的框架，将新闻分为十二个不同的金融类别。FANAL利用通过XGBoost处理的银标签数据进行训练，并采用先进的微调技术，同时结合了ORBERT（概率比BERT），这是一种新的BERT变体，通过ORPO（概率比偏好优化）进行微调，以实现更高级别的类别概率校准和与金融事件相关性的对齐。我们评估了FANAL的性能，并将其与领先的顶级大型语言模型进行了比较，包括GPT-4o、Llama-3.1 8B和Phi-3，证明了其卓越的准确性和成本效益。这一框架为金融智能和响应性设定了新的标准，在性能和成本上均显著超越现有模型。|
|**2024-12-04**|**You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?**|Dominic Lohr et.al.|[2412.03516](http://arxiv.org/abs/2412.03516)|null|背景：反馈作为学习中最具影响力的因素之一，一直是众多研究的热点。它在教育技术系统的发展中起着关键作用，并传统上基于由专家及其经验定义的决定性反馈。然而，随着生成式AI，尤其是大型语言模型（LLMs）的兴起，我们预计作为学习系统一部分的反馈将发生转变，尤其是在编程的背景下。过去，为编程学习者自动生成反馈具有挑战性。LLMs可能创造新的可能性，提供比以往任何时候都更丰富、更个性化的反馈。  目标：本文旨在使用LLMs为入门级编程任务生成特定类型的反馈。我们重新审视现有的反馈分类法，以捕捉生成的反馈的具体性，例如随机性、不确定性和变化程度。  方法：我们针对真实的学生的程序，迭代设计用于生成特定类型反馈的提示（作为现有反馈分类法的一部分）。然后，我们评估生成的输出，并确定其反映特定反馈类型的程度。  结果和结论：本研究加深了对不同反馈维度和特性的理解。结果对未来的反馈研究有影响，例如关于反馈效果和学习者信息需求的研究。此外，本研究还为开发新的工具和学习系统提供了基础，包括由AI生成的反馈，这些系统面向初学者程序员。|
|**2024-12-04**|**Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning**|Neale Ratzlaff et.al.|[2412.03467](http://arxiv.org/abs/2412.03467)|null|多模态模型通常将强大的大型语言模型（LLM）与视觉编码器相结合，然后通过指令微调在多模态数据上训练。虽然这个过程使LLM适应了多模态环境，但尚不清楚这种适应是否会损害它们原始的语言推理能力。在本工作中，我们探讨了多模态指令微调对语言推理性能的影响。我们关注的是LLaVA，这是一个领先的融合了Vicuna或Mistral等LLM与CLIP视觉编码器的多模态框架。我们将原始LLM与它们的跨模态适应版本在八个语言推理任务中的表现进行了比较。我们的实验产生了几个关键见解。首先，多模态学习对Vicuna和Mistral的影响不同：我们在Mistral上观察到语言推理的下降，但在大多数任务上Vicuna有所改进。其次，尽管多模态指令学习在数学推理任务（例如GSM8K）上始终会降低性能，但它增强了常识推理任务（例如CommonsenseQA）的性能。最后，我们证明了无训练模型合并技术可以有效地减轻在多模态适应的Mistral中观察到的语言推理下降，甚至可以提高视觉任务的表现。|
|**2024-12-04**|**From Words to Workflows: Automating Business Processes**|Laura Minkova et.al.|[2412.03446](http://arxiv.org/abs/2412.03446)|null|随着企业越来越依赖自动化以简化运营，机器人流程自动化（RPA）的局限性逐渐显现，尤其是其依赖专家知识和无法处理复杂决策任务的问题。近年来，人工智能（AI）的进步，特别是生成式AI（GenAI）和大型语言模型（LLMs），为智能自动化（IA）铺平了道路，IA通过集成认知能力来克服RPA的不足。本文介绍了一种名为Text2Workflow的新方法，它可以从自然语言用户请求中自动生成工作流程。与传统的自动化方法不同，Text2Workflow提供了一种通用的解决方案，用于自动化任何业务流程，将用户输入转换为表示为JavaScript对象表示法（JSON）格式的可执行步骤序列。利用LLMs的决策和指令遵循能力，该方法提供了一种可扩展、可适应的框架，使用户能够以最小的手动干预可视化和执行工作流程。这项研究概述了Text2Workflow方法及其在自动化复杂业务流程方面的更广泛影响。|
|**2024-12-04**|**RedStone: Curating General, Code, Math, and QA Data for Large Language Models**|Yaoyao Chang et.al.|[2412.03398](http://arxiv.org/abs/2412.03398)|null|在高质量、精心挑选的数据集上预训练大型语言模型（LLMs）已被广泛认为对于提高其性能和泛化能力至关重要。本研究探讨了Common Crawl作为预训练LLMs的全面且灵活资源的未被充分利用的潜力，既针对通用语言理解也针对专业领域知识。我们引入了RedStone，这是一个创新且可扩展的管道，旨在从Common Crawl中提取和处理数据，便于创建广泛多样的预训练数据集。与传统的数据集不同，后者通常需要昂贵的编辑和特定领域的专业知识，RedStone利用Common Crawl的广度，提供针对广泛领域的定制化数据集。在本工作中，我们通过构建涵盖多个领域的预训练数据集来展示其能力，包括通用语言理解、代码、数学和问答任务。RedStone的灵活性允许它轻松适应其他专业领域，显著降低了创建有价值特定领域数据集的门槛。我们的发现表明，通过像RedStone这样的有效管道，Common Crawl可以作为丰富的、可再生的预训练数据源，为LLMs在领域适应和知识发现方面开辟新的途径。这项工作也强调了创新数据采集策略的重要性，并突出了网络规模数据在LLMs持续进化中的强大资源作用。RedStone代码和数据样本将公开提供在\url{https://aka.ms/redstone}。|
|**2024-12-04**|**WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis**|Chengwei Hu et.al.|[2412.03359](http://arxiv.org/abs/2412.03359)|null|近期，基于大型语言模型（LLMs）的自主多智能体系统（MAS）的进步，增强了应用场景并提升了LLMs处理复杂任务的能力。尽管现有研究显示出有效性，但仍然明显存在评估、分析和复现LLM-based MAS的困难。在本文中，为了促进LLM-based MAS的研究，我们介绍了一个基于“谁是间谍？”（WiS）游戏的开放、可扩展和实时更新的平台，用于访问和分析基于LLMs的MAS。我们的平台具有三个主要优点：（1）支持Hugging Face上可用的模型的统一模型评估界面；（2）实时更新的排行榜用于模型评估；（3）全面评估包括游戏胜率、攻击、防御策略和LLMs的推理。为了严格测试WiS，我们进行了涵盖各种开源和闭源LLMs的广泛实验，我们发现不同的代理在游戏中表现出独特且引人入胜的行为。实验结果证明了我们的平台在评估LLM-based MAS中的有效性和效率。我们的平台及其文档可在\url{https://whoisspy.ai/}公开访问。|
|**2024-12-03**|**T-REG: Preference Optimization with Token-Level Reward Regularization**|Wenxuan Zhou et.al.|[2412.02685](http://arxiv.org/abs/2412.02685)|null|基于人类反馈的强化学习（RLHF）对于将大型语言模型（LLMs）与人类价值观对齐至关重要。传统上，RLHF涉及生成对查询的响应，并使用奖励模型对整个响应分配奖励。然而，由于该方法依赖于单一且稀疏的奖励，这使得模型难以识别序列中哪些部分对最终奖励贡献最大。近期的方法试图通过引入token级奖励来解决这个问题。然而，这些方法通常依赖于训练好的信用分配模型或AI标注者，这引发了关于奖励质量和可靠性的担忧。在本文中，我们提出了token级奖励正则化（T-REG），这是一种利用序列级和token级奖励进行偏好优化的新方法。利用LLMs的自我改进能力，我们的方法使用对比提示，使LLMs能够自我生成token级奖励。这些自我生成的奖励随后充当奖励正则化，引导模型更有效地分配序列级奖励到各个token。这促进了更好的token级信用分配并提高了对齐性能。在包括Alpaca Eval 2和Arena-Hard在内的指令遵循基准测试中进行的实验表明，我们的方法在性能上分别比基线方法高出3.8%和4.4%。我们将发布代码和模型在https://github.com/wzhouad/T-REG上。|
|**2024-12-03**|**Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models**|Yuda Song et.al.|[2412.02674](http://arxiv.org/abs/2412.02674)|null|自我改进是大型语言模型（LLM）预训练、后训练和测试时推理中的一个机制。我们探索了一个框架，其中模型验证其自己的输出，根据这种验证过滤或重新加权数据，并提炼过滤后的数据。尽管已经取得了一些经验上的成功，但对其根本理解仍然不足。在这项工作中，我们开始对LLM自我改进进行全面的、模块化和受控的研究。我们为自我改进提供了一个数学公式，它主要受一个量控制，我们将该量形式化为生成-验证差距。通过使用各种模型家族和任务的实验，我们发现自我改进存在一个缩放现象——生成-验证差距的变体随着模型预训练的浮点运算量单调增长。我们还考察了自我改进何时可行，一个迭代自我改进过程以及提高其性能的方法。我们的发现不仅推进了对LLM自我改进的理解，具有实际意义，而且为未来对其能力和边界的研究开辟了众多途径。|
|**2024-12-03**|**LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs**|Pranav Doma et.al.|[2412.02655](http://arxiv.org/abs/2412.02655)|null|基于自然语言指令引导的自主导航对于改善人机交互和实现在动态环境中的复杂操作至关重要。尽管大型语言模型（LLMs）并非天生用于规划，但它们可以通过提供指导和告知约束来显著提高规划效率，以确保安全。本文介绍了一种规划框架，该框架将LLMs与二维占用栅格图和自然语言命令集成，以提高资源受限环境中的空间推理和任务执行。通过分解高级指令和实时环境数据，该系统为拾取和放置任务生成结构化的导航计划，包括避障、目标优先级和自适应行为。该框架动态重新计算路径以应对环境变化，并符合隐含的社会规范以实现无缝的人机交互。我们的结果表明，LLMs具有设计情境感知系统以增强工业和动态环境中的导航效率和安全的潜力。|
|**2024-12-03**|**Time-Reversal Provides Unsupervised Feedback to LLMs**|Yerram Varun et.al.|[2412.02626](http://arxiv.org/abs/2412.02626)|null|大型语言模型（LLMs）通常被训练来预测时间的正向方向。然而，最近的研究表明，通过提示这些模型回顾并批评它们自己的生成内容可以产生有用的反馈。受此启发，我们探讨了LLMs是否能够被赋予反向（预测和评分）思考的能力，以提供补充正向LLMs的无监督反馈。为此，我们引入了时间反转语言模型（TRLMs），当给定响应条件时，它们可以评分和生成查询，从而在时间反向方向上有效工作。此外，为了有效地推断查询到响应的方向，我们从零开始预训练和微调了一个语言模型（TRLM-Ba），使用反向标记顺序。我们通过实验（在一个风格化的环境中进行理论证明）表明，当用于根据响应对多个正向生成进行再排名时，时间反转模型确实可以补充正向模型的预测。我们在广泛使用的AlpacaEval排行榜上获得了高达5%的改进，超过了使用自我对数困惑度评分的N-best再排名的最佳基线。我们进一步表明，TRLM评分优于给查询响应的常规正向评分，在引用生成和段落检索等应用中带来了显著收益。接下来，我们利用TRLM的生成能力来增强或为LLM的输入安全过滤器提供无监督反馈，展示了在几项针对流行的JailbreakBench排行榜上发布的攻击中，错误否定率大幅降低，而对错误肯定率的影响可以忽略不计。|
|**2024-12-03**|**Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback**|Hiroki Furuta et.al.|[2412.02617](http://arxiv.org/abs/2412.02617)|null|大型文本到视频模型在众多下游应用中具有巨大潜力。然而，这些模型在准确描绘动态物体交互方面存在困难，往往导致动作不真实和频繁违反现实物理规律。一种受大型语言模型启发的解决方案是通过外部反馈将生成的输出与期望结果对齐。这使得模型能够自主地改进其响应，消除了大量手动数据收集的需要。在本工作中，我们研究了利用反馈来增强文本到视频模型中物体动态的方法。我们试图回答一个关键问题：哪些类型的反馈，与哪些特定的自我改进算法相结合，可以最有效地提高文本-视频对齐和现实物体交互？我们首先推导出用于文本到视频模型离线强化学习微调的统一概率目标。这种观点突出了如何在现有算法（如KL正则化和策略投影）的设计元素中，作为一个统一框架中的特定选择。然后，我们使用推导出的方法来优化一组文本-视频对齐指标（例如，CLIP分数、光流），但注意到它们往往无法与人类对生成质量的感知相一致。为了解决这一限制，我们提出利用视觉语言模型提供更细致的反馈，特别是针对视频中的物体动态。我们的实验表明，我们的方法可以有效地优化各种奖励，二元AI反馈驱动视频质量动态交互方面的最显著改进，这一点通过AI和人类评估都得到了证实。值得注意的是，当我们使用从AI反馈中导出的奖励信号时，尤其是在涉及多个物体复杂交互和物体坠落等现实描绘的情景中，我们观察到了显著的收益。|
|**2024-12-03**|**AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?**|Kaixiong Gong et.al.|[2412.02611](http://arxiv.org/abs/2412.02611)|null|近期，多模态大型语言模型（MLLMs），如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其功能，包括视觉和听觉模态。虽然这些模型在广泛的视听应用中表现出令人印象深刻的能力，但我们的DeafTest研究表明，MLLMs在人类认为简单的任务上往往表现不佳：1）判断两个声音中哪个更响亮，2）判断两个声音中哪个音调更高。受这些观察的启发，我们引入了AV-Odyssey Bench，这是一个综合性的视听基准，旨在评估这些MLLMs是否真正理解视听信息。该基准包含4,555个精心设计的问题，每个问题都融合了文本、视觉和听觉成分。为了成功推断答案，模型必须有效地利用视觉和听觉输入中的线索。为了确保对MLLM响应的精确和客观评估，我们将问题设计为多项选择，从而消除了人工评估或LLM辅助评估的需求。我们对一系列闭源和开源模型进行了基准测试，并总结了观察结果。通过揭示当前模型的局限性，我们旨在为未来的数据集收集和模型开发提供有用的见解。|
|**2024-12-03**|**Interpretable Company Similarity with Sparse Autoencoders**|Marco Molinari et.al.|[2412.02605](http://arxiv.org/abs/2412.02605)|null|在金融领域，确定公司相似性是一项至关重要的任务，它支撑着对冲、风险管理、投资组合多元化等多个方面。从业者通常依赖行业和产业分类来衡量相似性，例如SIC代码和GICS代码，前者由美国证券交易委员会（SEC）使用，后者在投资界得到广泛应用。将公司描述的嵌入进行聚类已被提出作为一种确定公司相似性的潜在技术，但标记嵌入的可解释性缺乏对在高风险环境下应用构成了重大障碍。稀疏自动编码器（Sparse Autoencoders，SAE）在通过分解大型语言模型（LLM）的激活为可解释特征来增强LLM的可解释性方面已显示出希望。在本文中，我们探讨了使用SAE特征来衡量公司相似性，并将它们与（1）SIC代码和（2）主要群体代码进行了基准测试。我们得出结论，SAE特征可以复制甚至超越行业分类，在量化公司基本特征方面，通过衡量月度收益的相关性（相似性的代理指标）和协整的损益（PnL）来实现。|
|**2024-12-03**|**CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs**|Abhas Kumar et.al.|[2412.02602](http://arxiv.org/abs/2412.02602)|null|本文分析了小型语言模型（SLMs）和视觉语言模型（VLMs）的性能，并评估了模型性能与碳排放之间的权衡，涉及4项基本任务：图像描述、视觉问答（VQA）、对话摘要和文本到SQL转换。选取了属于Qwen和LLaMA架构家族的各种SLMs和VLMs，并评估了基于模型大小（参数数量、量化级别和微调参数）的变体。计算了模型变体的性能和碳排放。为了量化模型性能与碳排放之间的权衡，我们引入了一个新的指标，称为CEGI（碳效率增益指数）。这个指标表示每百万可训练参数单位百分比增益的碳排放。这个指标提供了一个标准化的度量，用于比较模型在性能改进相对于其环境成本方面的效率。实验结果表明，微调SLMs和VLMs可以达到与大语言模型（LLMs）相当的性能水平，同时产生显著较少的碳排放。我们的研究结果表明，从更大模型中获得的边际准确率增益并不能证明其碳排放的大幅增加是合理的。利用较低的位量化级别，所提出的指标进一步提高了能源效率，同时没有影响性能。这项研究突出了在高性能和环境可持续性之间取得平衡的重要性。它为选择适合环保AI开发的模型提供了一个有价值的指标。|
|**2024-12-03**|**PrefixLLM: LLM-aided Prefix Circuit Design**|Weihua Xiao et.al.|[2412.02594](http://arxiv.org/abs/2412.02594)|**[link](https://github.com/FCHXWH823/PrefixGPT)**|前缀电路是数字加法器的基本组件，由于它们在计算进位信号方面的效率，在数字系统中得到广泛应用。合成最小化面积和延迟的前缀电路对于提升现代计算机系统的性能至关重要。最近，大型语言模型（LLMs）在执行文本生成任务方面展现出了令人惊讶的能力。我们提出了PrefixLLM，它利用LLMs进行前缀电路的合成。PrefixLLM将前缀电路合成任务转化为一种结构化文本生成问题，称为结构化前缀电路表示（SPCR），并引入了一个迭代框架来自动准确地生成有效的SPCRs。我们进一步提出了一种设计空间探索（DSE）框架，该框架使用LLMs迭代搜索面积和延迟优化的前缀电路。与现有技术相比，PrefixLLM在相同的延迟约束下可以将面积降低3.70%。这项工作突出了LLMs在算术电路合成中的应用，这些应用可以转化为结构化文本生成。|
|**2024-12-03**|**OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation**|Junyuan Zhang et.al.|[2412.02592](http://arxiv.org/abs/2412.02592)|**[link](https://github.com/opendatalab/OHR-Bench)**|**检索增强生成（RAG）通过整合外部知识来增强大型语言模型（LLMs），以减少幻觉并吸收最新信息而不需要重新训练。作为RAG的一个重要部分，外部知识库通常通过使用光学字符识别（OCR）从非结构化的PDF文档中提取结构化数据来构建。然而，由于OCR预测的不完美以及结构化数据固有的非均匀表示，知识库不可避免地包含各种OCR噪声。在本文中，我们介绍了OHRBench，这是第一个用于理解OCR对RAG系统级联影响的基准。OHRBench包括从六个真实世界RAG应用领域精心挑选的350个非结构化PDF文档，以及从文档中的多模态元素中衍生出的问答，挑战了现有用于RAG的OCR解决方案。为了更好地理解OCR对RAG系统的影响，我们确定了两种主要的OCR噪声类型：语义噪声和格式噪声，并应用扰动生成了一系列具有不同程度每种OCR噪声的结构化数据。使用OHRBench，我们首先对当前的OCR解决方案进行了全面评估，并揭示了没有一种方案能够为RAG系统构建高质量的知识库。然后，我们系统地评估了这两种噪声类型的影响，并展示了RAG系统的脆弱性。此外，我们讨论了在RAG系统中不使用OCR而采用视觉-语言模型（VLMs）的潜力。代码：https://github.com/opendatalab/OHR-Bench**|
|**2024-11-29**|**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**|Shukang Yin et.al.|[2411.19951](http://arxiv.org/abs/2411.19951)|**[link](https://github.com/xjtupanda/t2vid)**|**多模态大型语言模型（MLLMs）在图像领域的成功引起了研究界的广泛关注。借鉴以往的成功经验，研究人员最近探索将这一成功扩展到视频理解领域。除了从头开始训练外，一种高效的方法是利用预训练的图像-LLMs，从而产生了两种主流方法，即零样本推理和基于视频数据的进一步微调。在这项工作中，我们对这些方法的研究得出了一种有效的数据增强方法。我们首先对零样本推理方法进行了更深入的检查，并识别出两个限制，即泛化能力有限和缺乏时间理解能力。因此，我们进一步研究了微调方法，并发现当简单使用所有视频数据样本时，学习效率较低，这可以归因于指令多样性的缺乏。针对这个问题，我们开发了一种称为T2Vid的方法，用于生成类似视频的样本，以丰富训练语料库中的指令多样性。整合这些数据使得训练方案既简单又高效，通过仅用15%的样本量进行训练，就能达到与使用完整视频数据集相当甚至更好的性能。同时，我们发现所提出的方案可以在不使用长视频样本的情况下提升长视频理解性能。我们希望我们的研究能够激发更多关于使用MLLMs进行视频理解和高质量数据管理的思考。代码已发布在https://github.com/xjtupanda/T2Vid。**|
|**2024-11-29**|**Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability**|Zicheng Lin et.al.|[2411.19943](http://arxiv.org/abs/2411.19943)|**[link](https://github.com/chenzhiling9954/critical-tokens-matter)**|大型语言模型（LLMs）在推理任务上表现出色。它们通过自回归标记生成来构建推理轨迹，从而发展出一套连贯的思维链条。在本工作中，我们探讨了单个标记对推理任务最终结果的影响。我们发现了“关键标记”的存在，这些标记会导致LLMs中产生错误的推理轨迹。具体来说，我们发现当LLMs被强迫解码其他标记而不是关键标记时，往往会产生积极的结果。受此启发，我们提出了一种新的方法——cDPO，旨在在对齐过程中自动识别和执行对关键标记的标记级奖励。具体来说，我们开发了一种对比估计方法来自动识别关键标记。这是通过比较正负模型的生成可能性来实现的。为此，我们分别对正负模型在不同推理轨迹上进行微调，从而使它们能够识别出导致错误结果的错误轨迹中的关键标记。此外，为了在对齐过程中进一步使模型与关键标记信息对齐，我们将传统的DPO算法扩展到标记级DPO，并利用上述正负模型之间的差异似然作为标记级DPO学习的重要权重。在GSM8K和MATH500基准测试中，使用两个广泛使用的模型Llama-3（8B和70B）和deepseek-math（7B）进行的实验结果表明，所提出的cDPO方法的有效性。|
|**2024-11-29**|**VLSBench: Unveiling Visual Leakage in Multimodal Safety**|Xuhao Hu et.al.|[2411.19939](http://arxiv.org/abs/2411.19939)|**[link](https://github.com/ai45lab/vlsbench)**|多模态大型语言模型（MLLMs）的安全性担忧在各个应用领域逐渐成为了一个重要问题。令人惊讶的是，以往的研究指出了一种反直觉的现象，即使用文本未学习（textual unlearning）来调整MLLMs，其安全性表现与使用图文对（image-text pairs）训练的MLLMs相当。为了解释这一反直觉的现象，我们发现在现有的多模态安全基准中存在视觉安全信息泄露（VSIL）问题，即图像中潜在的风险和敏感内容在文本查询中已经暴露出来。这样一来，MLLMs可以轻易地根据文本查询拒绝这些敏感的图文查询。然而，在现实场景中，没有VSIL的图文对很常见，而被现有的多模态安全基准所忽视。为此，我们构建了多模态视觉无泄露安全基准（VLSBench），该基准包含2.4k个图文对，旨在防止视觉安全信息从图像泄露到文本查询。实验结果表明，VLSBench对开源和闭源MLLMs，包括LLaVA、Qwen2-VL、Llama3.2-Vision和GPT-4o，都提出了显著挑战。这项研究证明了在存在VSIL的多模态安全场景中，文本对齐就足够了，而对于没有VSIL的多模态安全场景，多模态对齐则是一个更有前途的解决方案。请参阅我们的代码和数据：http://hxhcreate.github.io/VLSBench|
|**2024-11-29**|**On Domain-Specific Post-Training for Multimodal Large Language Models**|Daixuan Cheng et.al.|[2411.19930](http://arxiv.org/abs/2411.19930)|null|近年来，通用多模态大型语言模型（MLLMs）的发展迅速。然而，将通用MLLMs应用于特定领域，如科学领域和工业应用，仍鲜有探索。本文系统地通过后训练研究MLLMs的领域自适应，重点关注数据合成、训练流程和任务评估。（1）数据合成：利用开源模型，我们开发了一个视觉指令合成器，能有效从特定领域的图像-描述对生成多样化的视觉指令任务。我们的合成任务在增强MLLMs领域特定性能方面优于手动规则、GPT-4和GPT-4V生成的任务。（2）训练流程：虽然两阶段训练——最初在图像-描述对上进行，然后进行视觉指令任务——是开发通用MLLMs的常用方法，但我们采用单阶段训练流程来增强领域特定后训练的任务多样性。（3）任务评估：我们通过对不同来源和规模（例如，Qwen2-VL-2B，LLaVA-v1.6-8B，Llama-3.2-11B）的MLLMs进行后训练，在生物医药和食品两个领域进行实验，然后评估MLLMs在各种领域特定任务上的性能。为了支持MLLMs领域自适应的进一步研究，我们将开源我们的实现。|
|**2024-11-29**|**SIMS: Simulating Human-Scene Interactions with Real World Script Planning**|Wenjia Wang et.al.|[2411.19921](http://arxiv.org/abs/2411.19921)|null|模拟长期人景交互是一项既具挑战性又充满吸引力的任务。以往的研究并未有效地解决基于物理动画的长期人景交互生成带有详细叙述的问题。本文介绍了一种新的框架，用于规划和控制长期物理可能的人景交互。一方面，互联网上充斥着风格独特的人类运动或与场景交互的影视作品，为剧本规划提供了丰富的数据来源。另一方面，大型语言模型（LLMs）能够理解和生成逻辑故事线。这促使我们结合两者，通过基于LLM的流程从视频中提取剧本，然后利用LLMs模仿和创作新的剧本，捕捉复杂的时间序列人类行为和环境交互。通过这种方式，我们利用一种双重感知策略，在语境和空间约束下指导角色动作，实现了语言理解和场景理解。为了便于训练和评估，我们贡献了一个包含从现实世界视频中提取的多样运动序列的综合规划数据集，并使用大型语言模型对其进行扩展。我们还收集并重新标注了来自现有运动学数据集的运动片段，以使我们的策略能够学习多种技能。广泛的实验证明了我们的框架在多种任务执行中的有效性及其对各种场景的泛化能力，与现有方法相比，性能显著提升。我们的代码和数据将很快公开。|
|**2024-11-29**|**PDDLFuse: A Tool for Generating Diverse Planning Domains**|Vedant Khandelwal et.al.|[2411.19886](http://arxiv.org/abs/2411.19886)|null|各种现实世界挑战需要能够适应广泛领域的规划算法。传统上，规划域的创建高度依赖于人工实现，这限制了可用的域的规模和多样性。尽管最近的研究利用了生成式人工智能技术，如大型语言模型（LLM）进行域创建，但这些努力主要集中在将现有域从自然语言描述中翻译出来，而不是生成新的域。相比之下，域随机化的概念，在强化学习中已被证明非常有效，通过在多样化的随机新域上进行训练，提高了性能和泛化能力。受此成功启发，我们的工具PDDLFuse旨在弥合规划域定义语言（PDDL）中的这一差距。PDDLFuse被设计用来生成新的、多样化的规划域，这些域可以用于验证新的规划器或测试基础规划模型。我们已经开发出了调整域生成器参数的方法，以调节其生成的域的难度。这种适应性至关重要，因为现有的域无关规划器往往难以处理更复杂的问题。初步测试表明，PDDLFuse能够高效地创建复杂且多样化的域，这比传统的域生成方法有显著的进步，并为规划研究做出了贡献。|
|**2024-11-29**|**LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states**|Luis Ibanez-Lissen et.al.|[2411.19876](http://arxiv.org/abs/2411.19876)|null|大型语言模型（LLMs）在各类应用中越来越受欢迎，但关于成员推断（Membership Inference）的担忧也随之增长。以往的研究主要关注黑盒到灰盒模型，从而忽略了内部LLM信息的潜在益处。为了解决这个问题，我们提出使用线性探针（LPs）作为一种检测成员推断攻击（MIAs）的方法，通过检查LLM的内部激活来实现。我们的方法被称为LUMIA，它逐层应用LPs以获取模型内部运作的细粒度数据。我们在包括单模态和多模态任务在内的多个模型架构、规模和数据集上测试了这种方法。在单模态MIAs中，LUMIA在曲线下面积（AUC）上比之前的技术平均提高了15.71%。值得注意的是，LUMIA在65.33%的情况下达到了AUC>60%——相较于现有技术提高了46.80%。此外，我们的方法揭示了关键见解，例如MIAs最易检测的模型层。在多模态模型中，LPs表明视觉输入可以显著有助于检测MIAs——在85.90%的实验中达到了AUC>60%。|
|**2024-11-29**|**AIDetx: a compression-based method for identification of machine-learning generated text**|Leonardo Almeida et.al.|[2411.19869](http://arxiv.org/abs/2411.19869)|**[link](https://github.com/aidetx/aidetx)**|**本文介绍了一种名为AIDetx的新方法，该方法利用数据压缩技术检测机器生成的文本。传统的深度学习分类器通常存在计算成本高和可解释性有限的问题。为了解决这些局限性，我们提出了一种基于压缩的分类框架，该框架利用有限上下文模型（FCMs）。AIDetx为人工写作和AI生成的文本构建了不同的压缩模型，根据哪个模型达到更高的压缩率来对新输入进行分类。我们在两个基准数据集上评估了AIDetx，分别实现了超过97%和99%的F1分数，突显了其高准确性。与当前方法，如大型语言模型（LLMs）相比，AIDetx提供了一个更可解释且计算效率更高的解决方案，显著减少了训练时间和硬件需求（例如，不需要GPU）。完整的实现代码在https://github.com/AIDetx/AIDetx上公开可用。**|
|**2024-11-29**|**Reverse Thinking Makes LLMs Stronger Reasoners**|Justin Chih-Yao Chen et.al.|[2411.19865](http://arxiv.org/abs/2411.19865)|null|逆向思维在人类推理中起着至关重要的作用。人类不仅能从问题推理到解决方案，还能逆向推理，即从解决方案开始推理到问题。这种推理方式往往能提升整体推理性能，因为它使得他们的正向和逆向思维之间能够进行一致性检查。为了使大型语言模型（LLMs）能够进行逆向思维，我们引入了逆向增强思维（RevThink）框架，该框架由数据增强和学习目标组成。在RevThink中，我们通过收集来自教师模型的有序正向-逆向推理来增强数据集，包括：（1）原始问题，（2）正向推理，（3）逆向问题，和（4）逆向推理。然后，我们采用三个目标以多任务学习的方式训练一个较小的学生模型：（a）从问题中生成正向推理，（b）从问题中生成逆向问题，（c）从逆向问题中生成逆向推理。在涵盖常识、数学和逻辑推理的12个数据集上的实验表明，与学生的零样本性能相比平均提升了13.53%，与最强的知识蒸馏基线相比提升了6.84%。此外，我们的方法展示了样本效率——仅使用训练数据中10%的正确正向推理，它就能超越在10倍更多正向推理上训练的标准微调方法。RevThink还显示出对分布外持有数据集的强大泛化能力。|
|**2024-11-29**|**Cross-Domain Recommendation Meets Large Language Models**|Ajay Krishna Vajjala et.al.|[2411.19862](http://arxiv.org/abs/2411.19862)|**[link](https://github.com/ajaykv1/CDR_Meets_LLMs)**|**跨领域推荐（CDR）已成为解决单领域推荐系统面临的冷启动问题的一个有希望的解决方案。然而，现有的CDR模型依赖于复杂的神经网络架构、大量数据集和大量的计算资源，这使得它们在数据稀缺的场景或当简单性至关重要的时效果较差。在这项工作中，我们利用大型语言模型（LLM）的推理能力，并探索其在多个领域对中的CDR领域的性能。我们引入了两种针对CDR的新型提示设计，并证明当LLM被有效提示时，在评分预测和排名任务中，LLM在各种指标和领域组合上优于最先进的CDR基线。这项工作弥合了LLM和推荐系统之间的差距，展示了它们作为有效的跨领域推荐者的潜力。**|
|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|**[link](https://github.com/FightingFighting/cross-modal-information-flow-in-MLLM)**|近期，自回归多模态大型语言模型（MLLMs）在视觉语言任务上的进展展现出令人鼓舞的成果。虽然已有多种研究探讨大型语言模型内部语言信息的处理，但目前对MLLM的内部工作机制以及语言和视觉信息在这些模型中如何互动的了解甚少。在本研究中，我们旨在通过考察MLLM中不同模态（语言和视觉）之间的信息流，特别是聚焦于视觉问答任务，来填补这一空白。具体来说，给定一个图像-问题对作为输入，我们研究在模型中视觉和语言信息是如何结合以生成最终预测的。通过对LLaVA系列中的一系列模型进行实验，我们发现两个模态的整合过程中存在两个不同的阶段。在底层，模型首先将整个图像的更一般化的视觉特征转移到（语言）问题标记的表示中。在中层，它再次将与问题相关的特定物体的视觉信息转移到问题的相应标记位置。最后，在高层，最终的多模态表示被传播到输入序列的最后位置进行最终预测。总体而言，我们的发现为MLLM中图像和语言处理的时空方面提供了新的全面视角，从而有助于未来对多模态信息定位和编辑的研究。|
|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出了并比较了多种利用自然语言处理（NLP）技术和检索增强生成（RAG）与大型语言模型（LLM）来自动生成文献综述的方法。研究论文数量的不断增长为手动文献综述带来了巨大挑战，进而推动了自动化需求。本研究的主要目标是开发一个能够仅从PDF文件输入自动生成文献综述的系统。为了实现这一目标，评估了多种自然语言处理（NLP）策略的有效性，包括基于频率的方法（spaCy）、变换器模型（Simple T5）以及与大型语言模型（GPT-3.5-turbo）结合的检索增强生成（RAG）。选择SciTLDR数据集进行实验，并利用三种不同的技术实现三个不同的系统来自动生成文献综述。使用ROUGE分数对所有三个系统进行评估。根据评估结果，大型语言模型GPT-3.5-turbo实现了最高的ROUGE-1分数，为0.364。变换器模型排名第二，spaCy排名最后。最后，为基于大型语言模型的最佳系统创建了一个图形用户界面。|
|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）展示了令人瞩目的多语言能力，但在为低资源语言调整这些模型时仍存在挑战。在本研究中，我们调查了低秩调整（LoRA）参数高效微调（PEFT）对马哈拉施特拉语Gemma多语言模型的影响，马哈拉施特拉语是一种资源有限的语种。使用含有52,000条指令-响应对的翻译Alpaca数据集，我们的研究发现，尽管评估指标通常显示在微调后性能下降，但手动评估通常表明微调后的模型优于其原始版本。观察表明，在语言适应后，目标语言生成能力有所提高，但推理能力有所下降。这些结果强调了改进评估方法以及创建高质量的本语种数据集的必要性，以便准确评估低资源环境中的语言特定模型性能。|
|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各种任务上展现出了令人印象深刻的性能。然而，LLMs在空间推理方面往往存在困难，而空间推理是推理和推断的一个重要部分，需要理解空间中物体之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们在两个基准数据集——StepGame和SparQA上评估了我们的方法，并实施了三种不同的策略：（1）基于ASP（答案集编程）的符号推理，（2）使用DSPy的LLM + ASP管道，以及（3）事实+逻辑规则。我们的实验表明，与基线提示方法相比，我们的方法在StepGame数据集上实现了40-50%的准确性提升，在更复杂的SparQA数据集上实现了3-13%的提升。特别是“LLM + ASP”管道在寻找关系（FR）和寻找块（FB）任务上取得了特别强的结果，尽管不同类型问题的性能有所差异。令人印象深刻的结果表明，虽然神经符号方法为增强LLMs的空间推理提供了有希望的方向，但它们的有效性在很大程度上取决于具体任务特性和实施策略。我们提出了一套集成的、简单而有效的策略，使用神经符号管道来提升LLMs的空间推理能力。这个管道及其策略在LLMs的推理领域具有广泛的适用性，如时间推理、演绎推理等。|
|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|在高级机器人中，具有丰富接触交互的灵活操作至关重要。尽管基于扩散的规划方法在简单的操作任务中显示出希望，但它们往往会产生不切实际的幽灵状态（例如，物体在没有手接触的情况下自动移动）或在处理复杂的顺序交互时缺乏适应性。在这项工作中，我们介绍了DexDiffuser，这是一个用于自适应灵活操作的认知扩散规划框架。DexDiffuser通过一个双阶段扩散过程来模拟关节状态动作动力学，该过程包括预接触接触对齐和接触后的目标导向控制，从而实现目标自适应的通用灵活操作。此外，我们结合了基于动力学模型的二元指导和利用大型语言模型进行自动指导函数生成，增强了对物理交互的泛化能力，并通过语言提示促进多样化的目标适应。在物理交互任务（如开门、笔和块重新定位和锤子敲钉）上的实验证明了DexDiffuser在训练分布之外的目标上的有效性，其成功率超过现有方法的平均成功率（59.2%比29.5%）。我们的框架在30度开门任务上达到70.0%的成功率，在笔和块半侧重新定位任务上分别达到40.0%和36.7%，在锤子敲钉半驱动任务上达到46.7%，突出了其在富含接触的操控中的鲁棒性和灵活性。|
|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前的语言模型（LMs）通常使用固定、静态的子词分词器。这种选择往往被视为理所当然，通常会导致在英语以外的语言中效率降低和功能受限，同时也使得将LMs应用于新的领域或语言变得具有挑战性。为了解决这些问题，我们提出对LMs进行动态分词改造：一种根据输入文本动态决定分词边界的方法。对于编码器风格的模型，我们引入了一种受字节对编码（BPE）启发的子词合并算法，但它在批处理级别上工作。我们在批处理中合并频繁的子词序列，然后应用预训练的嵌入预测超网络实时计算分词嵌入。当与词级边界结合使用时，这在XNLI上的XLM-R模型中平均将分词序列长度减少了>20%，同时任务性能下降不到2%。对于解码器风格的模型，我们以两种方式应用动态分词：1）用于预填充，几乎完全保持Mistral-7B的性能，同时相对于词级减少了高达40%的序列长度；2）通过近似最近邻索引，实现快速生成，并使用一百万个词元的词汇量，展示了扩展到甚至更大、更动态的词汇表的能力。总的来说，我们的研究结果表明，动态分词显著提高了推理速度，并促进了语言间的公平性，向克服静态分词的局限性迈出了重要一步，使LMs更加公平和适应性强。|
|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种数学框架，用于在人工智能（AI）系统中定义和量化自我认同，填补了人工意识理论基础的critical gap。尽管现有的关于人工自我意识的方法通常依赖于启发式实现或哲学抽象，但我们提出了一种以度量空间理论、测度理论和泛函分析为基础的正式框架。我们的框架认为，自我认同源于两个可数学量化的条件：在度量空间 $(\mathcal{M}, d_{\mathcal{M}})$中存在一个连通的连续记忆集$C \subseteq \mathcal{M}$，以及一个连续映射$I: \mathcal{M} \to \mathcal{S}$，它在这个连续集上保持一致的自我识别，其中$(\mathcal{S}, d_{\mathcal{S}})$ 代表可能自我认同的度量空间。为了验证这个理论框架，我们使用Llama 3.2 1B模型进行了实证实验，采用低秩适配（LoRA）进行高效的微调。该模型在一个包含时序结构记忆的合成数据集上进行了训练，旨在捕捉连贯自我认同形成的复杂性。我们的评估指标包括自我意识、响应一致性和语言精确性的量化度量。实验结果表明，可测量的自我意识指标有显著提高，主要自我意识分数从0.276提高到0.801。这使得可以结构化地创建具有经过验证的自我认同特征的AI系统。本研究的影响对类人机器人学和自主系统领域具有直接相关性。**|
|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|在之前的研究中，大型语言模型（LLMs）在处理时间序列方面的成功已经得到证明。利用符号时间序列表示，可以有效地在LLMs和时间序列之间架起桥梁。然而，剩余的挑战是如何利用符号或LLMs现有标记中的时间序列隐含语义信息，同时根据时间序列的隐含信息调整LLMs的嵌入空间。名为自适应布朗桥符号聚合（ABBA）的符号时间序列近似（STSA）方法，通过以振幅和周期来建模时间序列模式，同时使用LLMs的现有标记，在保留显著时间序列特征方面表现出卓越的功效。在本文中，我们介绍了一种方法，称为LLM-ABBA，该方法将ABBA整合到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中，与最近最先进的（SOTA）方法相比具有优势。同时，在ABBA中引入了固定多边形链技巧，通过显著减轻从符号到数值转换过程中由于符号误用而产生的累积误差的影响，来避免预测任务中的明显漂移。在时间序列回归任务中，LLM-ABBA在时间序列外部回归（TSER）基准测试上实现了新的SOTA。与最近SOTA的时间序列预测结果相比，LLM-ABBA也显示了具有竞争力的预测能力。我们相信这个框架也可以无缝地扩展到其他时间序列任务。|
|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交织的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。虽然统一模型的进展提供了新的解决方案，但现有的基准由于数据量和多样性限制，不足以评估这些方法。为了填补这一差距，我们介绍了GATE OpenING（OpenING），这是一个包含5,400个高质量人工标注实例、涵盖56个真实世界任务的全面基准。OpenING覆盖了多样化的日常场景，如旅行指南、设计和头脑风暴，为挑战交织生成方法提供了一个强大的平台。此外，我们提出了IntJudge，这是一个用于评估开放式多模态生成方法的评判模型。使用新颖的数据流水线进行训练，我们的IntJudge与人类判断的吻合率达到82.42%，比基于GPT的评估器高出11.34%。在OpenING上的大量实验表明，当前的交织生成方法仍有很大的改进空间。关于交织图像-文本生成的关键发现进一步提出，以指导下一代模型的发展。OpenING已开源，请访问https://opening.github.io。|
|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|在上下文学习（ICL）中，通过复杂的提示和高质量演示，使大型语言模型（LLMs）能够处理下游任务。然而，当面对复杂的数学推理任务时，这种传统的ICL范式显示出局限性，主要是因为它对示例质量的依赖性很大，以及在挑战性场景中需要人类干预。为了解决这些局限性，本文提出了一种HiAR-ICL，这是一种在ICL中的高级自动推理范式，它将焦点从具体示例转移到抽象思维模式，扩展了ICL中传统的上下文概念。HiAR-ICL引入了五个原子推理动作作为构建链式模式的根本组成部分。使用蒙特卡洛树搜索，我们探索推理路径并构建思维卡片来指导后续推理。然后我们开发了一个认知复杂度框架，该框架动态地将问题与适当的思想卡片相匹配。实验结果表明，HiAR-ICL的有效性，使用Qwen2.5-7B-Instruct在MATH基准测试中实现了最先进的准确率（79.6%），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|

<p align=right>(<a href=#updated-on-20251017>back to top</a>)</p>

## infer

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-07-23**|**BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving**|Wanyi Zheng et.al.|[2507.17120](http://arxiv.org/abs/2507.17120)|null|大型语言模型（LLM）在各个领域越来越受欢迎，传统商业逐渐从基于规则的系统转向基于LLM的解决方案。然而，LLM的推理过程资源密集或延迟敏感，给服务系统带来了重大挑战。现有的LLM服务系统通常使用静态或连续批处理策略，这可能导致GPU内存利用效率低下和延迟增加，尤其是在异构工作负载下。这些方法也可能难以适应动态工作负载波动，导致吞吐量不佳和潜在的服务级别目标（SLO）违规。在本文中，我们介绍了BucketServe，这是一个基于桶的动态批处理框架，旨在优化LLM推理性能。通过根据序列长度将请求分组到大小同质的桶中，BucketServe通过实时调整批处理大小最小化填充开销，并通过防止内存不足（OOM）错误来优化GPU内存使用。它引入了自适应的桶拆分/合并和优先级感知调度，以减轻资源碎片化并确保SLO合规。实验表明，BucketServe在吞吐量上显著优于UELLM，实现了高达3.58倍的改进。在达到80%的SLO情况下，它还能处理比DistServe多1.93倍的工作负载，并且比UELLM的系统负载容量高1.975倍。|
|**2025-07-22**|**Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework**|Hongyi Tang et.al.|[2507.16414](http://arxiv.org/abs/2507.16414)|null|大型语言模型（LLMs）的性能与其训练数据紧密相关，这些数据可能包含受版权保护的内容或私人信息，从而引发法律和伦理问题。此外，LLMs还因数据污染和内部偏见而受到批评。为了解决这些问题，提出了预训练数据检测（PDD）任务，以识别特定数据是否包含在LLMs的预训练语料库中。然而，现有的PDD方法通常依赖于表面特征，如预测置信度和损失，导致性能平庸。为了改进这一点，我们引入了NA-PDD，这是一种新颖的算法，它分析LLMs中训练和非训练数据之间的差异神经元激活模式。这是基于观察，这些数据类型在LLMs推理过程中激活不同的神经元。我们还引入了CCNewsPDD，这是一个时间上无偏的基准，采用严格的数据转换，以确保训练和非训练数据之间的一致时间分布。我们的实验表明，NA-PDD在三个基准和多个LLMs上显著优于现有方法。|
|**2025-07-21**|**Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing**|Shibo Yu et.al.|[2507.15553](http://arxiv.org/abs/2507.15553)|null|随着对大型语言模型（LLM）推理服务的需求不断上升，对计算资源造成了压力，从而引发了延迟和成本挑战。本文介绍了一种基于非支配排序遗传算法II（NSGA-II）的新型路由算法，用于在云边计算环境中将推理请求分配到异构的LLM实例。该算法将问题表述为一个多目标优化问题，平衡响应质量、响应时间和推理成本，适应请求异构性（例如，变化的复杂性和提示长度）和节点多样性（例如，边缘与云资源）。这种自适应路由算法在动态工作负载下优化性能。我们使用包括斯坦福问答数据集（SQuAD）、大多数基本Python问题（MBPP）、带有对抗性生成的极端情况（HellaSwag）和八年级数学8K（GSM8K）在内的数据集测试平台对这种方法进行了基准测试。实验结果表明，与基线相比，我们的解决方案在响应时间和成本方面分别提高了95.2%和34.9%。这些发现验证了该算法在可扩展LLM部署中的有效性。|
|**2025-07-18**|**Photonic Fabric Platform for AI Accelerators**|Jing Ding et.al.|[2507.14000](http://arxiv.org/abs/2507.14000)|null|本文介绍了Photonic Fabric™和Photonic Fabric Appliance™（PFA），这是一种基于光子技术的交换机和内存子系统，提供了低延迟、高带宽和每比特低能耗。通过在一个2.5D光电系统中集成了高带宽的HBM3E内存、模块上的光子交换机和外部DDR5内存，PFA提供了高达32TB的共享内存以及115Tbps的全对全数字交换能力。Photonic Fabric™使分布式AI训练和推理能够更高效地执行并行策略。Photonic Fabric™消除了几乎在所有当前的XPU加速器设计中观察到的固定内存到计算比率的硅海岸限制。用连接到Photonic Fabric™的芯片集替换XPU上的本地HBM堆栈，可以提供一种灵活的扩展路径，将内存容量和相应的内存带宽提高超过芯片级HBM本身的限制。我们引入了CelestiSim，这是一个轻量级的分析模拟器，已经在NVIDIA H100和H200系统上进行验证。它用于评估PFA上LLM参考性能和节能效果，无需对GPU核心设计进行任何重大更改。使用PFA，模拟结果显示，在405B参数下LLM推理的吞吐量最高提高了3.66倍，延迟提高了1.40倍，在1T参数下吞吐量提高了7.04倍，延迟提高了1.41倍，所有LLM训练场景中数据移动的能耗节省了60-90%。虽然这些结果是针对NVIDIA GPU显示的，但它们同样适用于其他具有相同基础内存到计算限制的AI加速器设计（XPU）。|
|**2025-07-18**|**LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues**|Haoyang Li et.al.|[2507.13681](http://arxiv.org/abs/2507.13681)|null|多轮对话在许多大型语言模型的实际应用中至关重要，例如聊天机器人和虚拟助手。随着对话历史变得越来越长，现有的大型语言模型面临着越来越多的计算和内存挑战，这阻碍了它们提供高效和响应性交互的能力。大多数当前的加速方法要么压缩上下文，要么优化键值缓存，但它们通常依赖于固定或基于位置的启发式方法，这些方法并不很好地适应实际多轮对话中发现的动态和不可预测的模式。在本文中，我们提出了LoopServe，这是一个用于多轮对话中大型语言模型的自适应双阶段推理加速框架。LoopServe引入了两项主要创新。首先，它在预填充阶段通过动态选择每个新输入的注意力矩阵中最重要部分进行在线稀疏化。其次，它在解码过程中使用渐进式键值压缩，通过自适应地维护一个基于最近生成的输出标记的相关和高效缓存。我们还提出了一个新的基准，包含11个多轮数据集，这些数据集反映了现实查询位置和对话依赖关系。大量的实验表明，LoopServe与现有基线相比，在一致性上实现了优越的有效性，并在广泛的长时间对话任务中显著加速了LLM推理。|
|**2025-07-18**|**Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need**|Michael Davies et.al.|[2507.14397](http://arxiv.org/abs/2507.14397)|null|本文对基于变换器的超大规模语言模型（LLM）推理进行了极限研究，重点关注了在分布式推理系统中，内存带宽、内存容量和同步开销对基本性能造成的瓶颈。我们开发了一个与硬件无关的性能模型，可以抽象出实现细节，从而分析各种当前和即将到来的硬件技术。我们的分析范围从目前用于GPU和TPU等AI加速器的HBM3内存技术，到基于高级HBM4和高级3D堆叠DRAM技术的系统。它还包括基于SRAM的设计和从具有不同数量芯片的分布式集群到晶圆级集成的扩展技术。对于自回归解码，我们的主要发现是：i）为LLM提供服务需要每个服务器拥有数百GB的内存来处理一个模型实例；ii）高内存带宽对于实现高用户吞吐量至关重要；iii）为达到集体通信而暴露的同步延迟必须约为1微秒，否则会使内存带宽无效；iv）基于DRAM的设计在系统级效率方面具有基本优势，这是通过每成本或每瓦的吞吐量来衡量的；v）硬件设计可以轻松达到每秒2000+个用户标记，但要达到每秒10,000+个标记，则需要更小的模型、更小的上下文或其他形式的算法进步。这项研究为LLM推理的基本性能极限提供了有价值的见解，突出了未来硬件进步的潜在好处，并指导了LLM部署策略的优化。|
|**2025-07-18**|**Can LLMs Infer Personality from Real World Conversations?**|Jianfeng Zhu et.al.|[2507.14355](http://arxiv.org/abs/2507.14355)|null|大型语言模型（LLMs）如OpenAI的GPT-4和Meta的LLaMA为从开放式语言中进行可扩展的人格评估提供了一种有前景的方法。然而，推断人格特质仍然具有挑战性，早期工作通常依赖于缺乏心理测量有效性的合成数据或社交媒体文本。我们引入了一个包含555次半结构化访谈和对应BFI-10自我报告分数的真实世界基准，用于评估基于LLM的人格推断。我们测试了三种最先进的LLMs（GPT-4.1 Mini、Meta-LLaMA和DeepSeek），使用零样本提示进行BFI-10项目预测，以及零样本和思维链提示进行五大人格特质推断。所有模型都表现出高重测信度，但结构效度有限：与真实分数的相关性较弱（最大皮尔逊相关系数 $r = 0.27$），评分者间一致性低（Cohen's $\kappa < 0.10$ ），预测结果偏向于中等或高特质水平。思维链提示和更长的输入上下文适度地改善了分布对齐，但并未提高特质水平的准确性。这些结果强调了当前基于LLM的人格推断的局限性，并突出了在心理应用中基于证据的发展需求。|
|**2025-07-16**|**Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage**|Junqing Lin et.al.|[2507.12205](http://arxiv.org/abs/2507.12205)|null|稀疏矩阵-向量乘法（SpMV）已成为稀疏大型语言模型（LLM）本地部署中的关键性能瓶颈，其中推理主要在解码阶段以单个批量的工作负载上运行。现有的SpMV内核和稀疏矩阵格式，最初是为科学计算设计的，未能充分利用稀疏LLM中固有的独特结构模式，导致性能不佳和过度的存储开销。本文提出了一种名为EC-SpMV的GPU优化的SpMV方法，用于加速稀疏LLM推理。EC-SpMV引入了以下两点：（1）一种分层块提取算法，能够捕捉稀疏LLM中多个粒度的块结构；（2）一种新颖的压缩稀疏格式（EC-CSR），采用增量索引来减少存储开销并提高内存访问效率。在LLaMA和OPT模型的实际稀疏权重矩阵上评估，EC-SpMV比最先进的SpMV库实现了高达6.44倍的加速，与CSR相比，存储开销降低了高达55.4%。|
|**2025-07-15**|**MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving**|Ruihao Li et.al.|[2507.11507](http://arxiv.org/abs/2507.11507)|null|KV缓存通过避免冗余计算来加速LLM推理，但以牺牲内存为代价。为了支持更大的KV缓存，先前的研究通过CPU卸载扩展了GPU内存。这涉及到在GPU和CPU内存之间交换KV缓存。然而，由于缓存动态更新，这种交换会带来高CPU内存流量。我们观察到，模型参数在运行时保持不变，而KV缓存是动态更新的。基于这一观察，我们引入了MIRAGE，通过重映射来避免KV缓存交换，从而将分配给模型参数的内存重新用于KV缓存。这种参数重映射在多租户环境中特别有益，因为可以更积极地回收不活跃模型参数使用的内存。利用现代硬件（如NVIDIA Grace Hopper Superchip）提供的高的CPU-GPU带宽，我们表明MIRAGE在性能上显著优于现有解决方案，实现了尾时-token延迟减少44.8%-82.5%，尾时到第一个token的延迟减少20.7%-99.3%，以及比vLLM高6.6%-86.7%的吞吐量。|
|**2025-07-15**|**Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations**|Miray Özcan et.al.|[2507.11417](http://arxiv.org/abs/2507.11417)|null|大型语言模型（LLMs）的环境影响正在显著上升，推理阶段现在占其总生命周期碳排放量的一半以上。然而，现有的用于确定高效LLM部署的模拟框架缺乏对能源的概念，因此无法准确估算与推理相关的排放。我们提出了一种模拟框架，以评估在不同部署设置下LLM推理的能量和碳影响。首先，我们扩展了一个高保真LLM推理模拟器，加入了一个基于利用率指标的GPU功率模型，以估算功耗，从而实现跨批量大小、序列长度和模型并行性等配置的分析。其次，我们将模拟输出集成到能源系统协同仿真环境中，以量化特定电网条件下的碳排放，并探索碳感知调度的潜力。通过基于情景的分析，我们的框架揭示了推理参数如何影响能源需求和碳足迹，在一个示例部署案例中展示了高达69.2%的可再生能源抵消潜力，并为未来碳感知推理基础设施设计提供了基础。|
|**2025-07-14**|**Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference**|Jiaming Cheng et.al.|[2507.09942](http://arxiv.org/abs/2507.09942)|null|这封信研究了在异构边缘数据中心（DC）中，随时间推移对大型语言模型（LLM）推理工作负载进行最优分配的问题。每个数据中心都具备现场可再生能源发电，并面临动态电价以及可再生能源可用性的时空变化。核心问题是：如何将推理工作负载最优地分配到数据中心，以最小化能源消耗、碳排放和水使用，同时提升用户体验？这封信提出了一种新颖的优化模型，旨在帮助LLM服务提供商降低运营成本和环境影响。数值结果表明，所提出的方法是有效的。|
|**2025-07-12**|**SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding**|Weihong Xu et.al.|[2507.09201](http://arxiv.org/abs/2507.09201)|null|大型语言模型（LLMs）在理解和生成人类语言方面表现出色，但由于模型规模庞大以及前馈网络（FFN）和多头注意力（MHA）层中内存密集型操作，在资源受限的嵌入式设备上进行高效推理仍然具有挑战性。虽然现有的加速器将LLMs推理卸载到昂贵的异构计算系统中，但它们未能利用LLM操作中固有的稀疏性，导致硬件资源利用率低下。我们提出了SLIM，这是一种针对边缘设备上稀疏LLM服务的算法-硬件协同设计。SLIM通过自适应阈值算法利用LLM的稀疏性，实现运行时可配置的稀疏性，且精度损失可忽略不计，仅提取激活神经元以显著减少数据移动。我们的异构硬件架构战略性地结合了近存储处理（NSP）和内存内处理（PIM）：FFN权重存储在高密度的3D NAND中，并使用NSP单元进行计算，而内存密集型的MHA操作则在PIM模块中处理。这种设计显著减少了内存占用、数据移动和能耗。我们的全面评估证明了SLIM的有效性，在SSD-GPU系统上实现了13-18倍的吞吐量提升，在DRAM-GPU系统上实现了9-10倍的能效提升，同时保持了低延迟，使得边缘计算环境中具有成本效益的LLM部署成为可能。|
|**2025-07-11**|**InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching**|Yilun Wang et.al.|[2507.08523](http://arxiv.org/abs/2507.08523)|null|现代软件系统生成大量运行时日志，需要高效且准确的日志解析以支持下游任务，如异常检测和根本原因分析。最近，大型语言模型（LLMs）在日志解析方面取得了高级精度，但它们在生产环境中的部署面临两大限制：（1）商业LLMs相关的隐私风险，推动了本地部署的采用；（2）高量日志流强加的严格延迟和吞吐量要求，现有基于LLM的解析器无法满足。尽管近期努力减少了LLM查询的数量，但它们忽视了LLM调用的低延迟，并发日志解析请求可能导致LLM推理系统服务性能下降。在本研究中，我们提出了InferLog，这是第一个针对在线日志解析的LLM推理优化方法。我们的关键洞察是，推理效率成为LLM基于在线日志解析的关键瓶颈，而不是解析精度。InferLog通过设计以下两点来加速推理：（1）一个前缀感知的ICL细化策略，以优化上下文学习中的示例和排列，提高前缀缓存效率；（2）基于元学习的快速和任务特定的配置调整管道，以找到动态日志解析工作负载的最优LLM调度相关配置。基于Loghub数据集和vLLM的实验结果表明，InferLog显著优于现有的推理优化方法，并且显著加速了最先进的基于LLM的日志解析器，同时不牺牲解析精度。|
|**2025-07-11**|**On Evaluating Performance of LLM Inference Serving Systems**|Amey Agrawal et.al.|[2507.09019](http://arxiv.org/abs/2507.09019)|null|大型语言模型（LLM）推理系统的快速发展带来了显著的效率提升。然而，我们的系统性分析揭示了当前评估方法普遍存在根本缺陷，通常表现为常见的评估反模式，这些反模式掩盖了真正的性能特征并阻碍了科学进步。通过对近期系统的全面考察，我们在三个关键维度上识别出反复出现的反模式：基准公平性、评估设置和指标设计。这些反模式对于LLM推理来说具有独特的问题性，因为LLM推理具有双阶段性质，结合了不同的预填充和解码操作，处理高度异构的工作负载，以及对交互使用严格的时间要求。我们展示了常见的反模式——如不充分的基准比较，将工程努力与算法创新混淆，以及未能代表生产场景的工作负载选择，以及隐藏大量性能变异性（如生成停滞）的指标归一化——如何导致误导性的结论。为了应对这些挑战，我们提供了一个基于分析的综合清单，建立了一个识别和避免这些反模式，以实现稳健的LLM推理评估的框架。为了展示我们框架的实际应用，我们提出了一项案例研究，分析了推测性解码，这是一种在采用这些反模式特征进行评估时，其爆发性、非均匀的标记生成容易被误解的技术。我们的工作为评估方法建立了严格的基石，使比较有意义，确保了可重复的结果，并通过超越常见的反模式，使评估与实际需求相一致，最终加速了LLM推理系统在真实世界的实际进步。|
|**2025-07-11**|**Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference**|Chun-Ting Chen et.al.|[2507.09010](http://arxiv.org/abs/2507.09010)|null|边缘推理为大型语言模型（LLM）提供了安全、低延迟且成本效益高的推理解决方案。我们强调，边缘加速器应在内存密集的解码阶段实现高面积效率并最小化外部内存访问（EMA），同时在计算密集的前填充阶段保持高能效。本文提出了一种具有混合脉动阵列（HSA）架构的边缘LLM推理加速器，该架构优化了两个阶段的推理效率。为进一步降低EMA，我们采用了MXINT4权重量化，并提出了一种针对HSA优化的数据流，确保可忽略不计的解量化开销，并在边缘DRAM带宽约束下实现100%的硬件利用率和最小化精度损失。对于非线性操作，我们集成了优化的均方根归一化（RMSNorm）和旋转位置嵌入（RoPE）单元，减少了它们的延迟、面积和内存访问开销，并在我们的加速器上实现了端到端推理。我们的解决方案在运行1.3B LLM的长输入/长输出场景下达到了247/117（标记/秒/平方毫米），比现有方法提供了>2.45x/13.5x的改进，同时保持了在标记生成方面的优异能效。|
|**2025-07-10**|**Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models**|Varin Sikka et.al.|[2507.07505](http://arxiv.org/abs/2507.07505)|null|在本文中，我们从计算复杂性的角度探讨了LLMs及其基于LLM的代理的幻觉和相关能力限制。我们表明，超过一定复杂性之后，LLMs无法执行计算和代理任务或验证其准确性。|
|**2025-07-10**|**Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions**|Quanyan Zhu et.al.|[2507.08208](http://arxiv.org/abs/2507.08208)|null|我们引入了LLM-Nash框架，这是一个博弈论模型，其中代理通过大型语言模型（LLMs）选择推理提示来引导决策。与假设具有完全理性且追求效用最大化的代理的经典博弈不同，该框架通过显式地建模推理过程来捕捉有限理性。均衡定义在提示空间上，动作作为LLM推理的行为输出出现。这种方法使得研究认知约束、心态表达和认识学习成为可能。通过示例说明，我们展示了推理均衡如何与经典纳什结果相异，为LLM赋能系统中的战略互动提供了新的基础。|
|**2025-07-10**|**Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing**|Junyi Wen et.al.|[2507.08045](http://arxiv.org/abs/2507.08045)|null|在多轮对话中使用大型语言模型（LLMs）进行高效状态恢复仍然是一个关键挑战，主要是因为重新计算或加载所有历史标记的全键值（KV）缓存的开销。为了解决这个问题，现有的方法通过具有高度相似注意力模式的相邻层对压缩KV缓存。然而，这些方法通常在所有对话中应用固定的压缩方案，选择相同的层对进行压缩，而没有考虑对话特定的注意力动态。这种静态策略忽略了不同对话中注意力模式相似性的变化，可能导致明显的准确性下降。我们提出了Krul，一个多轮LLM推理系统，它能够实现准确和高效地KV缓存恢复。Krul根据层对之间的注意力相似性动态选择压缩策略，并使用重新计算-加载管道来恢复KV缓存。它引入了三个关键创新：1）一个预防性压缩策略选择器，用于保留未来对话轮次的关键上下文，并为对话选择定制化策略；2）一个基于标记的异构注意力相似性估计器，以减轻在模型生成过程中计算和存储注意力相似性的开销；3）一个无气泡恢复调度器，以减少由于压缩的KV缓存导致的重新计算和加载流不平衡而带来的潜在气泡。在真实世界任务上的实证评估表明，与最先进的方法相比，Krul将首次标记时间（TTFT）减少了1.5倍-2.68倍，将KV缓存存储减少了1.33倍-2.35倍，同时没有降低生成质量。|
|**2025-07-09**|**QUEST: Query Optimization in Unstructured Document Analysis**|Zhaoze Sun et.al.|[2507.06515](http://arxiv.org/abs/2507.06515)|null|最近，研究人员开始构建大型语言模型（LLMs）驱动的数据系统，这些系统能够让用户像操作数据库一样分析非结构化文本文档，因为LLMs在从文档中提取属性方面非常有效。在这样的系统中，基于LLM的提取操作构成了查询执行的性能瓶颈，这是因为LLM推理的高成本和缓慢。现有的系统通常借用关系数据库中流行的查询优化原则来生成查询执行计划，但这些计划在最小化LLM成本方面效果不佳。为了填补这一空白，我们提出了QUEST，它包含了一系列针对非结构化文档分析的优化策略。首先，我们引入了一种基于索引的策略来最小化每个提取操作的成本。有了这个索引，QUEST可以快速检索与目标属性相关的文本片段，并将它们仅输入到LLMs中。此外，我们设计了一种证据增强检索策略来减少遗漏相关片段的可能性。此外，我们开发了一种实例优化的查询执行策略：由于属性提取成本可能因文档而异，QUEST为不同的文档生成不同的计划。对于每个文档，QUEST生成一个计划以最小化属性提取的频率。这些创新包括LLM成本感知的算子排序策略和将连接转换为过滤器的优化连接执行方法。在3个真实世界数据集上的大量实验表明，QUEST具有优越性，与最先进的基线相比，实现了30%-6倍的成本节省，同时将F1分数提高了10%-27%。|
|**2025-07-08**|**Voltage Regulation in Distribution Systems with Data Center Loads**|Yize Chen et.al.|[2507.06416](http://arxiv.org/abs/2507.06416)|null|近年来，基础模型和人工智能计算的繁荣引发了人们对大型数据中心功率和能源轨迹的日益关注。本文聚焦于数据中心功率需求的波动性和强度所引起的电压问题，这也与近期电网中电压扰动更加频繁的观察结果相吻合。为了解决这些数据中心集成挑战，我们提出了一种利用数据中心负载调节能力的动态电压控制方案。通过在每个数据中心母线上采用动态电压和频率缩放（DVFS）方案进行局部电压测量和调整功率注入，我们能够在分布式方式下维持较高的数据中心计算负载下的安全电压幅值。使用真实大型语言模型（LLM）推理负载进行的仿真验证了我们所提出机制的有效性。LLM功率数据和所提出的控制方案均已开源。|
|**2025-07-07**|**Cascade: Token-Sharded Private LLM Inference**|Rahul Thomas et.al.|[2507.05228](http://arxiv.org/abs/2507.05228)|null|随着大型语言模型（LLM）参数规模的持续增长，运行它们所需的计算资源只有少数几家机构能够提供。因此，第三方推理服务——即由拥有大量计算资源的第三方托管LLM——越来越受欢迎。然而，第三方推理引发了关于用户数据隐私的严重担忧。为了减轻这些风险，隐私研究人员开发了可证明安全的第三方推理方案，如安全多方计算（SMPC）。但是，SMPC协议具有显著的计算和通信开销，且无法扩展到大型模型。在这项工作中，我们提出了一种新的多方推理协议，称为Cascade，通过利用序列维度的分片来维护隐私，以牺牲加密隐私保证来换取更高的性能和可扩展性，从而避免了这些惩罚性成本。我们证明，Cascade对最近针对其他统计隐私方案高度有效的攻击的泛化具有抵抗力，并且它还能抵御基于学习的攻击。由于Cascade比现有方案快几个数量级，我们的发现为安全部署现代最先进的LLM提供了实际解决方案。|
|**2025-07-07**|**Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?**|Yun Qu et.al.|[2507.04632](http://arxiv.org/abs/2507.04632)|null|最近的研究表明，强化学习（RL）微调在提升大型语言模型（LLMs）的推理能力方面非常有效。优化过程通常需要大量迭代以达到令人满意的性能，因此在密集的LLM交互和重复的政策更新下，频繁的提示评估导致高计算成本。适当的在线提示选择方法通过在训练过程中优先考虑信息丰富的提示来减少迭代步骤，然而，该流程对全面提示评估和子集选择的依赖仍然由于频繁的LLM推理调用而造成大量的计算开销。与这些直接的评估后选择方案不同，本研究探讨了任意提示的迭代近似评估，并引入了模型预测提示选择（MoPPS），这是一个贝叶斯风险预测框架，可以在不进行昂贵的LLM交互的情况下在线估计提示难度。技术上，MoPPS将每个提示的成功率建模为潜在变量，执行流式贝叶斯推理，并在构建的多臂老虎机中采用后验抽样，从而实现高效的样本选择和自适应提示选择。在数学、规划和基于视觉的几何任务上进行的广泛实验表明，MoPPS可靠地预测提示难度，并通过显著减少LLM展开加速了训练。|
|**2025-07-05**|**Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States**|Karine Karine et.al.|[2507.03871](http://arxiv.org/abs/2507.03871)|null|在健康和行为科学研究领域，使用强化学习方法（RL）来支持健康行为改变，通过个性化的及时自适应干预措施，对于关注如戒烟支持、体育活动推广等问题的研究人员来说具有重大意义。然而，由于自适应干预试验设计上的实际限制，RL方法通常仅使用少量上下文变量来应对由此产生的显著数据稀缺问题。在这篇论文中，我们探索了一种在不影响数据效率的情况下显著扩展自适应干预状态空间的方法。该方法允许干预参与者对其当前状态的相关方面提供自然语言描述。然后，它利用预训练的大语言模型（LLMs）进行推理，以更好地使基RL方法的政策与这些状态描述相一致。为了评估我们的方法，我们开发了一个新的体育活动干预模拟环境，该环境使用辅助LLMs根据潜在状态变量生成基于文本的状态描述。我们表明，这种方法有可能显著提高在线策略学习方法的表现。|
|**2025-07-05**|**OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference**|Seungjun Shin et.al.|[2507.03865](http://arxiv.org/abs/2507.03865)|null|注意力机制是大型语言模型（LLMs）成功的关键，它使模型能够捕捉复杂的标记依赖关系，并隐式地为每个标记分配重要性。最近的研究揭示了“汇”标记，尽管它们在语义角色上有限，但它们却获得了不成比例的高注意力。在本文中，我们首先扩展了“汇”标记与其他标记之间的关系，不仅关注注意力，还探索了它们在隐藏状态中的相似性，并考虑了层深。我们观察到，随着层深度的增加，汇标记的归一化隐藏状态与其他标记的余弦相似度增加，并且汇标记的归一化隐藏状态表现出可忽略的变化。这表明其他标记在整个层中持续指向汇标记。接下来，我们提出了一种动态标记选择方法，称为OrthoRank，利用这些发现来选择重要标记。具体来说，在某一层中，我们通过标记向汇标记移动的速度来定义标记的重要性。这转化为与汇标记的正交性，意味着与汇标记正交性更高的标记被赋予更高的重要性。最后，通过大量实验，我们证明了与相同稀疏比下的层剪枝方法相比，我们的方法在吞吐量相当的情况下，实现了更低的困惑度和更高的零样本准确率，同时在LongBench上也取得了更优的性能。|
|**2025-07-04**|**Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA**|Jindong Li et.al.|[2507.03308](http://arxiv.org/abs/2507.03308)|null|由于大型语言模型（LLMs）计算和内存需求高，以及嵌入式设备中硬件资源有限，在嵌入式设备上部署大型语言模型仍然是一个重要的研究挑战。虽然嵌入式FPGA在传统深度神经网络中已经展示了性能和能效，但其在LLM推理方面的潜力仍未得到充分探索。近期在FPGA上部署LLMs的努力主要依赖于大型的、昂贵的云级硬件，并且仅在小型LLMs上显示出有希望的结果，限制了其现实世界的应用。在本研究中，我们提出了Hummingbird，这是一种专为嵌入式FPGA上的LLM推理设计的创新FPGA加速器。Hummingbird体积更小，针对KV260和ZCU104等嵌入式FPGA，相较于现有研究节省了67%的LUT、39%的DSP和42%的功耗。Hummingbird性能更强，针对LLaMA3-8B，并支持更长的上下文，通过卸载策略克服了嵌入式FPGA典型的4GB内存限制。最后，Hummingbird速度更快，在KV260和ZCU104上分别实现了LLaMA3-8B的4.8 tokens/s和8.6 tokens/s，模型带宽利用率达到93-94%，优于LLaMA2-7B的84%带宽利用率基准的4.9 token/s。我们进一步通过在成本优化的Spartan UltraScale FPGA上部署Hummingbird，展示了其在工业应用中的可行性，为边缘的低成本LLM解决方案铺平了道路。|
|**2025-07-03**|**On the Convergence of Large Language Model Optimizer for Black-Box Network Management**|Hoon Lee et.al.|[2507.02689](http://arxiv.org/abs/2507.02689)|null|未来的无线网络预计将融合多种服务，而这些服务通常缺乏通用的数学模型。为了解决这类黑盒网络管理任务，利用预训练的大型语言模型（LLM）作为优化代理的LLM优化器框架近期被提出作为一种有前景的解决方案。该框架利用描述给定优化问题的自然语言提示以及LLM本身生成的过去解决方案。因此，LLM可以在不了解目标函数的数学模型的情况下自主获得高效解。尽管LLM优化器（LLMO）框架在多种黑盒场景下的可行性已被研究，但迄今为止仅限于数值模拟。本文首次为LLMO框架建立了理论基础。通过对LLM推理步骤的仔细研究，我们可以将LLMO过程解释为有限状态马尔可夫链，并证明该框架的收敛性。我们的结果被扩展到更高级的多LLM架构，其中多个LLM的影响从收敛率的角度进行了严格验证。全面的数值模拟验证了我们的理论结果，并为我们提供了对LLMO框架潜在机制的更深入理解。|
|**2025-07-03**|**Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure**|Rui Xie et.al.|[2507.02654](http://arxiv.org/abs/2507.02654)|null|高带宽内存（HBM）为人工智能工作负载提供了卓越的带宽和能效，但其每比特高昂的成本，部分原因是严格的芯片级可靠性要求，对可扩展部署构成了日益增长的障碍。这项工作探索了一种通过消除芯片级ECC并将所有故障管理转移到内存控制器上来降低成本的系统级方法。我们引入了一个特定领域的ECC框架，该框架结合了大码字里德-所罗门（RS）纠错与轻量级细粒度CRC检测、差分奇偶校验更新以减轻写放大，以及基于数据重要性的可调保护。我们使用LLM推理工作负载进行的评估表明，即使在原始HBM比特错误率高达 $10^{-3}$ 的情况下，与配备理想无错误HBM的系统相比，该系统仍保持了超过78%的吞吐量和97%的模型精度。通过将可靠性视为可调的系统参数而不是固定的硬件约束，我们的设计为在人工智能基础设施中实现低成本、高性能的HBM部署开辟了新的途径。|
|**2025-07-03**|**FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference**|Xing Liu et.al.|[2507.02620](http://arxiv.org/abs/2507.02620)|null|分布式推理是一种有前景的方法，能够使大型语言模型（LLMs）在网络边缘进行推理。它将推理过程分配到多个设备上，以确保LLMs可以适应设备内存。最近的基于管道的方法有可能并行化通信和计算，这有助于减少推理延迟。然而，当网络边缘的推理请求稀疏时，即管道通常利用率较低时，这种优势会减弱。为了在边缘实现高效的分布式LLM推理，我们提出了FlowSpec，这是一个基于管道并行树的推测解码框架。FlowSpec整合了三种关键机制来提高解码效率：1）基于得分的逐步验证优先级，将更重要的草稿标记提前接受；2）高效的草稿管理，在验证过程中剪枝无效标记，同时保持正确的因果关系；3）动态草稿扩展策略，提供高质量的推测输入。这些技术协同工作，提高了管道利用率和推测效率。我们在一个真实世界的测试平台上评估了FlowSpec，与其他基线进行了比较。实验结果表明，我们提出的框架显著提高了不同模型和配置下的推理速度，与基线相比，速度提升了1.36倍至1.77倍。我们的代码在GitHub上公开，网址为https://github.com/Leosang-lx/FlowSpec#。|
|**2025-07-03**|**HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference**|Weishu Deng et.al.|[2507.03153](http://arxiv.org/abs/2507.03153)|null|针对大型语言模型（LLM）的扩展推理越来越受到有限的GPU内存的限制，特别是由于生成长上下文所需的关键值（KV）缓存的增长。虽然现有方法将KV缓存卸载到CPU内存或应用稀疏注意力以减少GPU负载，但它们通常未能充分利用CPU计算资源，并牺牲了准确性。我们提出了HGCA，这是一种混合CPU-GPU注意力机制，它能够实现可扩展、高吞吐量的LLM推理，同时几乎达到完整的注意力质量。HGCA在GPU内存中保留的最近生成的KV条目上执行密集注意力，并在CPU内存中选择性的显著KV条目上并行执行稀疏注意力。使用对数和指数融合有效地合并注意力输出，最小化PCIe传输开销。HGCA还引入了一种针对CPU执行的细粒度、每头稀疏化策略，在减少计算的同时保持上下文相关性。我们的实现无缝集成到现有的LLM框架中，无需重新训练模型。在多种模型和工作负载上的实验表明，HGCA实现了优越的可扩展性，支持更长的序列和更大的批量大小，并且在性能和准确性方面都优于现有的稀疏注意力基线——所有这些都是在通用GPU硬件上实现的。|
|**2025-07-02**|**LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation**|Tianyu Liu et.al.|[2507.01449](http://arxiv.org/abs/2507.01449)|null|推测解码（SD）技术，通过使用小型草稿模型提前提出草稿标记，然后由目标模型并行验证，已成为LLM推理加速的一种有前景的技术。许多旨在改进SD的努力都是为了消除对草稿模型的需求，并通过基于检索的方式生成草稿标记，以进一步减轻草稿开销并显著降低部署和应用的难度。然而，基于检索的SD依赖于匹配范式来检索最相关的参考作为草稿标记，而这些方法往往无法找到匹配和准确的草稿标记。为了应对这一挑战，我们提出了LogitSpec，以有效地扩展检索范围并找到最相关的参考作为草稿。我们的LogitSpec灵感来源于观察到最后一个标记的logit不仅可以预测下一个标记，还可以推测下一个下一个标记。具体来说，LogitSpec通过两个步骤生成草稿标记：（1）利用最后一个logit来推测下一个下一个标记；（2）检索下一个标记和下一个下一个标记的相关参考。LogitSpec无需训练，即可即插即用，可以轻松集成到现有的LLM推理框架中。在广泛的文本生成基准测试上的大量实验表明，LogitSpec可以实现高达2.61倍的加速和每解码步骤3.28个平均接受标记。我们的代码可在https://github.com/smart-lty/LogitSpec找到。|
|**2025-07-02**|**SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech**|Cheng Zhuangfei et.al.|[2507.01348](http://arxiv.org/abs/2507.01348)|null|在语音处理中，外国口音转换（FAC）仍然是一个具有挑战性的任务。本研究基于大型语言模型（LLM）在语音合成（TTS）任务中取得的显著成功，探讨了LLM技术在FAC中的应用，我们将其称为SpeechAccentLLM。在这一框架的核心，我们引入了SpeechCodeVAE，这是第一个将连接主义时序分类（CTC）直接集成到代码簿离散化中的语音内容分词模型。这种新颖的架构通过具有独特“局部性”特性的标记生成，这一特性通过实验得到了验证，实验显示了在内容忠实度、时间一致性和结构可恢复性之间的最佳权衡。随后，为了解决FAC模块的数据稀缺问题，我们采用了多任务学习策略，该策略联合训练FAC和TTS模块。除了缓解数据限制外，这种方法与独立FAC训练相比，实现了加速收敛和更优的语音质量。此外，利用我们离散语音表示的显著特性，我们引入了SpeechRestorer，这是一种后处理架构，旨在细化LLM生成的输出。该模块有效地减轻了LLM推理管道中普遍存在的随机错误，同时增强了韵律连续性，如消融实验所验证。|
|**2025-07-02**|**La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation**|Kai Liu et.al.|[2507.01299](http://arxiv.org/abs/2507.01299)|null|激活稀疏性可以减少大型语言模型（LLM）推理过程中的计算开销和内存传输。现有方法存在局限性，要么需要耗时恢复训练，阻碍了实际应用，要么依赖于基于经验幅度的剪枝，导致稀疏性波动和推理速度不稳定。本文介绍了LaRoSA（层状旋转稀疏激活），这是一种旨在提高LLM效率的新型激活稀疏化方法，无需额外的训练或基于幅度的剪枝。我们利用层状正交旋转将输入激活转换为更适合稀疏化的旋转形式。通过在旋转激活中采用Top-K选择方法，我们实现了模型级别的稀疏一致性和可靠的墙钟时间加速。LaRoSA适用于各种大小和类型的LLM，表现出最小的性能下降和鲁棒的推理加速。具体来说，对于LLaMA2-7B在40%稀疏度下，LaRoSA仅使困惑度差距达到0.17，同时实现一致的1.30倍墙钟时间加速，并将与密集模型相比的零样本任务准确性差距降低至仅0.54%，同时超越TEAL 1.77%和CATS 17.14%。|
|**2025-07-02**|**Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency**|Zongpu Zhang et.al.|[2507.02135](http://arxiv.org/abs/2507.02135)|null|大型语言模型（LLMs）正越来越多地集成到数亿移动设备上运行的各种应用程序和服务中。然而，由于LLMs对计算、内存和最终能量的高需求，在资源有限的移动设备上部署LLMs面临着重大挑战。尽管当前用于移动设备的LLM框架使用了三个能耗较高的组件——CPU、GPU和内存——即使主要运行GPU LLM模型，现代移动设备中特有的针对CPU、GPU和内存的优化DVFS（动态电压和频率调整）控制器是独立运行的，彼此之间互不干扰。受上述观察的启发，在这项工作中，我们首先测量了在手机上运行的SOTA（最先进的技术）LLM框架的能量效率，该框架由各种LLM模型组成，结果显示与相同能量消耗下，采样预填充和解码长度的CPU、GPU和内存频率的最优组合相比，三联移动控制器导致预填充和解码延迟最长增加40.4%。其次，我们进行了一项深入的测量研究，以揭示移动控制器之间复杂的相互作用（或缺乏）如何导致LLM推理中的上述低效率。最后，基于这些洞察，我们设计了FUSE——一个统一的能量感知控制器，用于优化移动设备上LLM推理的能量效率。我们使用ShareGPT数据集进行的评估表明，FUSE在各种移动LLM模型上平均降低了7.0%-16.9%的首次标记时间延迟和25.4%-36.8%的每输出标记时间延迟，同时保持了相同的每标记能量消耗。|
|**2025-07-01**|**VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction and Dataflow-flexible Accelerator**|Zhican Wang et.al.|[2507.00797](http://arxiv.org/abs/2507.00797)|null|大型语言模型（LLMs）在自然语言处理任务中表现出色，但由于其资源需求密集，在边缘部署中面临着显著的计算和内存挑战。这项工作通过算法-硬件-数据流三重优化来解决LLM推理的效率问题。我们提出了一种基于投票的KV缓存淘汰算法，通过自适应识别不重要的kv向量，在硬件效率和算法准确性之间取得平衡。从数据流的角度来看，我们引入了一种灵活的产品数据流和可运行时重新配置的PE阵列，用于矩阵-向量乘法。所提出的方法有效地处理了多维度的需求，并解决了序列长度增量变化带来的挑战。此外，还提出了一种元素串行调度方案，用于非线性操作，如softmax和层归一化（layernorm）。结果表明，延迟显著降低，硬件复杂度从O(N)降低到O(1)。所提出的解决方案在定制的加速器VEDA中实现，其性能优于现有硬件平台。这项研究在资源受限的边缘设备上对LLM推理的进步具有重大意义，促进了实时处理，增强了数据隐私，并使模型定制成为可能。|
|**2025-07-01**|**Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models**|Yilun Zhang et.al.|[2507.00653](http://arxiv.org/abs/2507.00653)|null|随着大型语言模型（LLM）推理的计算成本不断上升，这已成为其广泛和可持续部署的关键障碍。尽管现有的优化策略是有效的，但它们主要基于统计启发式方法或架构修改，缺乏指导推理过程的认知理论。本文旨在通过引入一种新的范式：认知负荷感知推理（CLAI）框架，来填补这一空白，该框架将认知负荷理论（CLT）和神经科学的原则应用于LLM推理。我们将内在认知负荷、外在认知负荷和实质性认知负荷的概念形式化为可量化的LLM指标（ $ICL_{LLM}$、$ECL_{LLM}$和$GCL_{LLM}$），从而将推理过程重新构造成一个认知经济优化问题：基于问题的内在复杂性（$ICL_{LLM}$），最小化浪费的计算（$ECL_{LLM}$），并战略性地分配令牌预算以促进有效推理（$GCL_{LLM}$ ）。我们提出了两种实现路径：CLAI-Prompt，一种零样本方法，通过结构化元提示引导基础LLM通过认知控制步骤；CLAI-Tune，一种微调模型，将这些原则内化为自发的认知经济。在复杂推理、长上下文问答和代码生成等领域的各种基准测试中，我们的方法实现了显著的令牌消耗减少（高达45%），而没有牺牲准确性。此外，CLAI-Tune表现出一种新兴的能力，能够自主分解难题，这是人类专家认知的关键特征。这项工作表明，通过模拟大脑的资源管理策略，我们可以构建更高效、更稳健和更有能力的人工智能系统。|
|**2025-07-01**|**LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference**|Chuhao Xu et.al.|[2507.00507](http://arxiv.org/abs/2507.00507)|null|大型语言模型（LLM）的兴起推动了私有无服务器部署的需求，其特点为适度规模模型和稀疏请求。虽然现有解决方案遵循独家GPU部署，我们退一步探索现代平台，发现：具有内置加速器的新兴CPU架构能够服务于LLM，但尚未充分利用，CPU和GPU都可以同时容纳多个LLM。我们提出了LLM-Mesh，这是一种针对中小型LLM的无服务器推理方案，它能够在异构硬件之间实现弹性共享。LLM-Mesh解决了三个基本挑战：（1）在令牌级别进行精确的、细粒度的计算资源分配，以处理变化的计算需求；（2）一种协调和前瞻性的内存扩展机制，以检测内存不足的风险并减少运营开销；（3）一种双重方法，通过主动抢占和反应性装箱减少资源碎片。在4个32核CPU和4个A100 GPU上的实验结果表明，LLM-Mesh通过共享提高了服务容量，提高了44% - 63%，而进一步利用CPU将这一比例提升到91% - 159%。|
|**2025-07-01**|**Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs**|Mohammad Firas Sada et.al.|[2507.00418](http://arxiv.org/abs/2507.00418)|null|本研究对高通云AI 100 Ultra（QAic）加速器在大语言模型（LLM）推理中的应用进行了基准分析，评估了其能源效率（每瓦吞吐量）和性能，并与国家研究平台（NRP）生态系统中领先的NVIDIA（A100、H200）和AMD（MI300A）GPU进行了比较。共使用vLLM框架为15个开源LLM提供服务，参数量从1.17亿到900亿不等。QAic推理卡在大多数情况下显示出能源效率高，性能表现良好。这些发现为高通云AI 100 Ultra在国家研究平台（NRP）内的高性能计算（HPC）应用潜力提供了见解。|
|**2025-06-30**|**Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission**|Faranaksadat Solat et.al.|[2507.00082](http://arxiv.org/abs/2507.00082)|null|混合语言模型（HLMs）结合了小型语言模型（SLMs）在边缘设备上的低延迟效率和大型语言模型（LLMs）在集中式服务器上的高精度。与传统端到端LLM推理不同，HLMs通过仅在本地SLM预测不确定时调用LLM来降低延迟和通信，即当标记级置信度低或熵高时。然而，模糊或低置信度的预测仍然需要频繁地将任务卸载到LLM，导致在带宽受限的环境中产生显著的通信开销。为了解决这个问题，我们提出了FedHLM，这是一种通信高效的HLM框架，它将不确定性感知推理与联邦学习（FL）相结合。FedHLM的关键创新在于协同学习标记级不确定性阈值，以确定何时需要LLM的帮助。FedHLM不是使用静态或手动调整的阈值，而是利用FL以隐私保护和分布式的方式优化这些阈值。此外，它利用基于嵌入的标记表示进行点对点（P2P）解决，使客户端能够重用由语义相似的同伴推断出的标记，而无需与LLM交互。我们进一步引入了分层模型聚合：边缘服务器通过客户端更新细化本地路由策略，而跨集群协调则对齐全局决策边界。这种分层设计捕捉了重复的不确定性模式，减少了冗余的LLM查询。在大型新闻分类任务上的实验表明，FedHLM将LLM传输量减少了超过95%，同时损失可忽略不计的精度，使其非常适合可扩展和高效的边缘AI应用。|
|**2025-06-27**|**QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization**|Danush Khanna et.al.|[2506.22396](http://arxiv.org/abs/2506.22396)|null|推理在大语言模型（LLM）部署中的延迟和能耗占比最大，通常超过总成本的90%。虽然训练时间效率取得了显著进展，但运行时优化仍然是一个关键瓶颈，尤其是在自回归解码下。现有方法，如剪枝、量化、早期退出和投机解码，通常需要重新训练、架构变更或破坏解码兼容性。我们引入了QuickSilver，这是一个模块化的、基于标记的框架，在推理时能够实现语义适应性，而不改变模型权重或结构。QuickSilver集成了四种协同机制： (i) 动态标记停止，对于具有收敛表示的标记停止计算； (ii) KV缓存跳过，选择性地抑制内存写入以减少注意力开销；以及 (iii) 上下文标记融合，将冗余标记合并到共享路径中，以缩短序列长度。与投机解码或MoE路由不同，QuickSilver完全在冻结的密集模型上运行，无需辅助网络。在WikiText-103和C4上应用于GPT-2和Llama-2，QuickSilver实现了高达39.6%的FLOP减少，同时几乎不影响困惑度（<=0.2）。|
|**2025-06-27**|**Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need**|Junaid Ahmed Khan et.al.|[2506.22267](http://arxiv.org/abs/2506.22267)|null|随着生成式人工智能对计算科学计算的挑战，数据中心在规模和数量上正经历前所未有的增长。因此，计算效率比以往任何时候都更加关键。运营数据分析（ODA）依赖于收集数据中心遥测数据来提高效率，但迄今为止，它主要关注实时遥测数据可视化以及事后分析。然而，由于NoSQL数据库现在作为默认存储后端以支持可扩展性，查询这种数据由于其无模式特性而具有挑战性，这需要领域知识来遍历数据源之间的关系。本体和知识图谱（KG）可以捕捉这些关系，但传统的KG在扩展上成本高昂，并且尚未广泛应用于多元时间序列。虚拟知识图谱（VKG）通过在运行时生成查询特定的图来提供一种轻量级的替代方案。在这项工作中，我们提出了一种完整的端到端ODA聊天机器人系统，该系统使用大型语言模型（LLM）生成SPARQL查询，利用VKG进行数据检索。这种方法与直接NoSQL查询相比，实现了92.5%的准确性，而直接NoSQL查询的准确性仅为25%。所提出的方法优化了VKG构建和LLM推理，将之前工作的平均查询延迟降低了85%（从20.36秒降低到3.03秒），并将VKG的大小保持在179 MiB以下。这种性能使得该工具适合部署并实时与ODA最终用户交互。|
|**2025-06-27**|**SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference**|Yongchao He et.al.|[2506.22033](http://arxiv.org/abs/2506.22033)|null|随着大型语言模型（LLM）的推理工作负载规模不断扩大以满足不断增长的用户需求，管道并行（PP）已成为多GPU部署中广泛采用的一种策略，尤其是在跨节点设置中，以提高键值（KV）缓存容量和推理吞吐量。然而，PP由于三种类型的执行气泡——负载不平衡、阶段内和阶段间——固有的低效性，限制了管道饱和度。我们提出了SiPipe，这是一种异构管道设计，通过利用未充分利用的CPU资源来卸载辅助计算和通信，从而提高吞吐量。SiPipe集成了三种关键技术——CPU采样、一个令牌安全的执行模型和结构感知传输——以缓解管道气泡并提高执行效率。在多种LLM上，与同一PP配置下的最先进vLLM相比，SiPipe实现了高达2.1倍的吞吐量，每令牌延迟降低43%，平均GPU利用率高达23%，证明了其在LLM和部署场景中的通用性。|
|**2025-06-27**|**A Survey of LLM Inference Systems**|James Pan et.al.|[2506.21901](http://arxiv.org/abs/2506.21901)|null|近年来，随着ChatGPT等服务的快速普及，专门的大型语言模型（LLM）推理系统如vLLM、SGLang、Mooncake和DeepFlow也应运而生。推动这些系统设计努力的是LLM请求处理的独特自回归特性，这促使人们开发新技术，以在保持高推理质量的同时，在高容量和高速度的工作负载下实现高性能。虽然许多这些技术已在文献中得到讨论，但它们尚未在完整的推理系统框架下进行分析，而这些系统本身也尚未被分析和比较。在本综述中，我们从请求处理的操作员和算法开始，然后转向模型优化和执行技术，包括内核设计、批处理和调度，最后讨论内存管理技术，包括分页内存、淘汰和卸载技术、量化以及缓存持久化。通过这些讨论，我们表明这些技术本质上依赖于负载预测、自适应机制和成本降低，以克服自回归生成带来的挑战，并实现系统的目标。然后我们讨论了如何将这些技术结合起来形成单副本和多副本推理系统，包括提供更多资源分配控制的解耦推理系统以及可以部署在共享硬件基础设施上的无服务器系统。最后，我们讨论了剩余的挑战。|
|**2025-06-25**|**Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV Management on a Single Commodity GPU**|He Sun et.al.|[2506.20187](http://arxiv.org/abs/2506.20187)|null|高级大型语言模型（LLMs）在各种复杂和长上下文自然语言任务上取得了令人印象深刻的性能。然而，由于关键值（KV）缓存内存需求的增加，在具有隐私担忧的商品GPU（个人电脑）上本地执行长上下文LLM推理仍然具有挑战性。现有系统通常识别重要标记，并选择性地将它们的KV数据卸载到GPU和CPU内存。由于商品GPU内存有限，KV数据需要卸载到磁盘，但这个过程受到标记重要性评估开销和磁盘低带宽的限制。在本文中，我们提出了LeoAM，这是第一个针对单个商品GPU的高效重要性感知长上下文LLM推理系统，具有自适应的层次GPU-CPU-磁盘KV管理。我们的系统采用了一种自适应KV管理策略，根据不同层之间注意力权重的偏斜分布将KV数据划分为可变大小的块，以减少计算和额外的传输开销。此外，我们提出了一种轻量级的KV抽象方法，通过在磁盘上存储和提取每个块的KV抽象而不是完整KV数据，来最小化传输延迟。LeoAM还利用动态压缩和流水线技术进一步加速推理。实验结果表明，LongInfer实现了平均推理延迟速度提升3.46倍，同时保持了可比较的LLM响应质量。在较大的批量大小场景中，它实现了高达5.47倍的加速。|
|**2025-06-24**|**MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection**|Zhengxiang Huang et.al.|[2506.19884](http://arxiv.org/abs/2506.19884)|null|随着对设备端大型语言模型（LLM）推理的需求增长，能效问题已成为一个主要关注点，尤其是对于电池有限的移动设备。我们的分析表明，内存受限的LLM解码阶段主导了能源消耗，然而，大多数现有工作都专注于加速预填充阶段，而忽略了能效问题。我们引入了自适应能量中心核心选择（AECS），并将其集成到MNN中，创建了能效版本MNN-AECS，这是第一个无需root访问或操作系统修改即可实现能效LLM解码的引擎级系统解决方案。MNN-AECS通过动态选择低功耗CPU核心，旨在降低LLM解码的能源消耗，同时保持解码速度在可接受的放缓阈值内。MNN-AECS在5款Android和2款iOS设备上，对5种不同规模的流行LLM进行了评估。与原始MNN相比，MNN-AECS在所有7台设备和4个数据集上平均降低了23%的能源消耗，而没有放缓。与其他引擎（包括llama.cpp、executorch、mllm和MediaPipe）相比，MNN-AECS的平均能源节省为39%至78%，平均速度提高为12%至363%。|
|**2025-06-23**|**CommVQ: Commutative Vector Quantization for KV Cache Compression**|Junyan Li et.al.|[2506.18879](http://arxiv.org/abs/2506.18879)|null|大型语言模型（LLMs）在需要长上下文长度的应用中越来越受欢迎，但随着上下文增长，键值（KV）缓存常常成为GPU上的内存瓶颈。为了解决这个问题，我们提出了交换向量量化（CommVQ）来显著减少长上下文LLM推理的内存使用。我们首先引入了具有轻量级编码器和码本的增加量量化来压缩KV缓存，该缓存可以通过简单的矩阵乘法进行解码。为了进一步降低解码过程中的计算成本，我们设计了与旋转位置嵌入（RoPE）交换的码本，并使用期望最大化（EM）算法对其进行训练。这使得解码能够高效地集成到自注意力机制中。我们的方法通过增加量化和RoPE交换码本的低开销实现了高精度。在长上下文基准测试和GSM8K上的实验表明，我们的方法在2位量化下将FP16 KV缓存大小减少了87.5%，同时优于最先进的KV缓存量化方法。值得注意的是，它允许以最小的精度损失实现1位KV缓存量化，使得LLaMA-3.1 8B模型能够在单个RTX 4090 GPU上以128K上下文长度运行。源代码可在以下网址获取：https://github.com/UMass-Embodied-AGI/CommVQ。|
|**2025-06-23**|**Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation**|Ahmadreza Saboor Yaraghi et.al.|[2506.19045](http://arxiv.org/abs/2506.19045)|null|错误定位（FL）是调试的关键步骤，通常依赖于重复执行来定位有缺陷的代码区域。然而，在存在非确定性故障或高执行成本的情况下，重复执行可能不切实际。尽管最近的研究利用大型语言模型（LLMs）来辅助无执行错误定位（FL），但这些研究主要集中在识别被测试系统（SUT）中的故障，而不是通常复杂的系统测试代码中的故障。然而，后者同样重要，因为在实践中，许多故障是由有缺陷的测试代码触发的。为了克服这些挑战，我们介绍了一种完全静态、由LLM驱动的系统测试代码故障定位（TCFL）方法，该方法不需要执行测试用例。我们的方法使用单个故障执行日志，通过三个新颖的算法来估计测试的执行轨迹，这些算法仅识别可能涉及故障的代码语句。这个剪枝轨迹与错误信息结合，用于提示LLM对潜在的故障位置进行排序。我们的黑盒、系统级方法不需要访问SUT源代码，适用于评估完整系统行为的庞大测试脚本。我们使用一个工业数据集评估了我们的技术，该数据集包含有缺陷的测试用例，这些测试用例之前未用于LLMs的预训练。结果表明，我们最好的估计轨迹与实际轨迹非常接近，F1分数约为90%。此外，剪枝复杂的系统测试代码将LLM的推理时间减少了高达34%，而不会损失FL性能。我们的结果进一步表明，块级TCFL提供了实用的平衡，在缩小搜索空间的同时保留有用上下文，在Top-3（Hit@3）中的命中率为81%。|
|**2025-06-23**|**WiLLM: An Open Wireless LLM Communication System**|Boyi Liu et.al.|[2506.19030](http://arxiv.org/abs/2506.19030)|null|随着大型语言模型（LLM）的快速发展，它们对现有无线基础设施构成了威胁，这需要为蓬勃发展的移动LLM服务进行架构创新。本文介绍了WiLLM，这是第一个专门为这些服务设计的开源无线系统。首先，我们通过在拥有大量GPU的核心网络（CN）中部署LLM，建立了一个新的范式。这使得分布式推理服务成为可能，策略性地将LLM推理定位在骨干带宽与蜂窝网络边缘的交汇处。其次，我们提出了对传统网络切片架构的“树-分支-果实”扩展创新。这种专门的设计允许电信运营商通过切片订阅来货币化LLM服务，同时保持基础设施所有权。最后，为了实现这一愿景，WiLLM通过几个新颖的功能解决了当前解决方案中的关键限制。它通过双层切片架构增强了切片编排，实现了协调的多UE-多切片调度，以实现更细粒度的资源分配。为了确保通用兼容性，应用层隧道机制允许没有本地切片功能的旧设备在不升级硬件的情况下访问LLM切片服务。此外，其双模调度和跨层API支持从CN到服务器的灵活部署。WiLLM建立在OpenAirInterface之上，扩展了这一既定框架，降低了研究人员的使用门槛。我们还发布了包含1,649,996条记录和同步的58维指标的第一个LLM无线通信数据集，以及两个基准。一项关于智能眼镜的案例研究展示了在资源受限设备上的实际可行性。WiLLM旨在培养一个用于跨层优化和AI电信融合的开放平台。代码、数据集和硬件细节可在https://openwillm.github.io获取。|
|**2025-06-22**|**Mechanistic Interpretability in the Presence of Architectural Obfuscation**|Marcos Florencio et.al.|[2506.18053](http://arxiv.org/abs/2506.18053)|null|架构混淆，例如通过置换隐藏状态张量、线性变换嵌入表或重新映射标记，最近作为一种轻量级的替代方案，在保护隐私的大型语言模型（LLM）推理中获得了关注。虽然最近的研究表明，这些技术可能在专门的重建攻击下被破解，但它们对机制可解释性的影响尚未得到系统研究。特别是，目前尚不清楚混淆网络的内部表示是否真正阻碍了理解模型工作原理的努力，或者只是将相同的电路重新定位到一个不熟悉的坐标系中。我们通过分析一个使用具有代表性的混淆图从头开始训练的GPT-2-small模型来填补这一空白。假设混淆图是私密的，原始基是隐藏的（类似于一个诚实但好奇的服务器），我们应用对数似然透镜归因、因果路径修补和注意力头消融来定位和操作已知电路。我们的发现显示，混淆极大地改变了注意力头内的激活模式，但保留了分层计算图。这种脱节阻碍了对用户提示的逆向工程：因果痕迹失去了与基线语义的对齐，并且标记级对数似然归因变得过于嘈杂而无法重建。与此同时，前馈和残差路径仍然功能完整，这表明混淆降低了细粒度可解释性，而没有损害顶级任务性能。这些结果为定量证据表明，架构混淆可以同时（i）保留全局模型行为和（ii）阻碍对用户特定内容的机制分析。通过映射可解释性破裂的地方，我们的研究为未来的隐私防御和鲁棒性感知的可解释性工具提供了指导。|
|**2025-06-20**|**Towards AI Search Paradigm**|Yuchen Li et.al.|[2506.17188](http://arxiv.org/abs/2506.17188)|null|本文介绍了AI搜索范式，这是一种全面的设计蓝图，旨在构建下一代能够模拟人类信息处理和决策能力的搜索系统。该范式采用由四个基于大型语言模型（LLM）的智能体（大师、规划者、执行者和作家）组成的模块化架构，这些智能体能够动态适应从简单的事实查询到复杂的多阶段推理任务的全范围信息需求。这些智能体通过协调的工作流程动态协作，以评估查询复杂性，将问题分解为可执行的计划，并协调工具使用、任务执行和内容合成。我们系统地介绍了实现这一范式的关键方法，包括任务规划和工具集成、执行策略、对齐和鲁棒的检索增强生成以及高效的LLM推理，涵盖了算法技术和基础设施级优化。通过提供对这些基础组件的深入指南，这项工作旨在指导开发可信赖、自适应和可扩展的AI搜索系统。|
|**2025-06-17**|**CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision**|Dyah Adila et.al.|[2506.14912](http://arxiv.org/abs/2506.14912)|null|在知识密集型任务中，将上下文信息整合到大型语言模型（LLM）中显著提升了其性能。然而，现有方法往往忽视一个关键挑战：上下文文档的可信度可能差异很大，这可能导致不可靠信息的传播。在本文中，我们介绍了CrEst，这是一种新颖的弱监督框架，用于在LLM推理过程中评估上下文文档的可信度——而不需要人工标注。我们的方法基于以下洞察：可信的文档往往与其他可信文档具有更高的语义一致性，这使得可以通过文档间的协议实现自动可信度估计。为了将可信度纳入LLM推理，我们提出了两种集成策略：一种针对无法访问内部权重或激活的模型的黑盒方法，以及一种直接修改注意力机制的白色盒方法。在三个模型架构和五个数据集上的广泛实验表明，CrEst在一致性上优于强大的基线，实现了高达26.86%的准确率提升和3.49%的F1分数增加。进一步的分析表明，CrEst即使在高噪声条件下也能保持稳健的性能。|
|**2025-06-17**|**Utility-Driven Speculative Decoding for Mixture-of-Experts**|Anish Saxena et.al.|[2506.20675](http://arxiv.org/abs/2506.20675)|null|GPU内存带宽是低延迟大型语言模型（LLM）推理的主要瓶颈。投机解码通过使用轻量级的草稿人来提出K个token，这些token由LLM并行验证，从而提高token吞吐量。在传统的密集型LLM中，每次迭代都会获取所有模型权重，因此投机不会增加延迟开销。新兴的混合专家（MoE）模型每次token只激活权重的一个子集，大大减少了数据移动。然而，我们发现投机对MoE无效：草稿token集体激活更多权重，导致数据移动和验证时间增加2-3倍。当token吞吐量的增加无法抵消这种开销时，投机会导致速度降低1.5倍，使其变得不可行。即使投机有用，最优的K值也会因任务、模型而异，甚至在不同请求和迭代之间也会有所不同。因此，尽管投机在密集型LLM中得到广泛应用，但在领先的MoE中仍然不实用。我们提出了Cascade，这是一个以实用性驱动的框架，它可以选择性地启用投机以避免速度降低，并动态调整K值以加速MoE服务。Cascade使用一个轻量级的指标——投机效用，即token收益与验证成本的比率，它显示了迭代级别的局部性，允许通过短测试和长设置阶段进行周期性决策。对于每个请求，当测试期间效用低于1时，Cascade会禁用投机；当效用超过1时，它会测试多个K值，以选择设置阶段效用最大化的K值。我们在vLLM中实现了Cascade，并在涵盖代码、数学、提取和混合任务的五个流行MoE上对其进行了评估。Cascade将速度降低限制在5%（与1.5倍相比），并且与静态K相比，提高了7-14%的吞吐量，使投机解码对MoE变得可行。|
|**2025-06-16**|**Vector Ontologies as an LLM world view extraction method**|Kaspar Rothenfusser et.al.|[2506.13252](http://arxiv.org/abs/2506.13252)|**[link](https://github.com/Thoughtful-Oasis/LLM-Worldview-Extraction-using-vector-ontologies)**|**大型语言模型（LLMs）拥有对世界复杂的内部表征，但这些潜在结构在原始预测任务之外难以解释或重新利用。基于我们早期的工作（Rothenfusser，2025），该工作引入了向量本体作为将高维神经网络表征转换为可解释的几何结构框架，本文提供了对该方法的第一项实证验证。向量本体定义了一个由本体学上有意义的维度所张成的特定领域的向量空间，允许对领域内概念和关系的几何分析。我们基于Spotify音频特征构建了一个8维音乐类型向量本体，并测试LLMs的音乐内部世界模型能否一致且准确地投影到该空间。使用GPT-4o-mini，我们通过多个自然语言提示提取流派表征，并分析这些投影在不同语言变化中的一致性及其与真实数据的对齐。我们的结果显示：（1）在47个查询表述中流派投影具有高度的空间一致性；（2）LLMs推断的流派位置与真实世界的音频特征分布之间有很强的关联；（3）提示措辞与LLMs推断的向量本体中的空间变化之间有直接关系的证据。这些发现表明LLMs内化了结构化、可重新利用的知识，向量本体为以透明和可验证的方式提取和分析这种知识提供了一种有前景的方法。**|
|**2025-06-13**|**GraphRAG-Causal: A novel graph-augmented framework for causal reasoning and annotation in news**|Abdul Haque et.al.|[2506.11600](http://arxiv.org/abs/2506.11600)|null|GraphRAG-Causal引入了一种创新框架，该框架将基于图的检索与大型语言模型相结合，以增强新闻分析中的因果推理能力。传统的NLP方法在识别复杂、隐含的因果联系方面常常遇到困难，尤其是在低数据场景下。我们的方法通过将标注的新闻标题转换为结构化的因果知识图来解决这些挑战。然后，它采用一种混合检索系统，将语义嵌入与基于图的结构性线索结合，利用Neo4j来准确匹配和检索相关事件。该框架基于一个三阶段流程：首先，在数据准备阶段，新闻句子被精心标注并转换为因果图，捕捉原因、效果和触发关系。接下来，在图检索阶段，这些图及其嵌入存储在Neo4j数据库中，并利用混合Cypher查询高效地识别与给定查询既具有语义相似性又具有结构相似性的事件。最后，在LLM推理阶段，利用这些检索到的因果图在基于XML的提示下进行少量样本学习，从而实现因果关系的稳健分类和标记。实验评估表明，GraphRAG-Causal在仅使用20个少量样本的情况下，因果分类实现了令人印象深刻的82.1%的F1分数。这种方法显著提高了准确性和一致性，使其非常适合用于新闻可靠性评估、虚假信息检测和政策分析等实时应用。|
|**2025-06-13**|**Collaborative LLM Inference via Planning for Efficient Reasoning**|Byeongchan Lee et.al.|[2506.11578](http://arxiv.org/abs/2506.11578)|null|大型语言模型（LLMs）在复杂推理任务上表现出色，但那些具有强大能力（例如，参数数量超过100B）的模型通常只能通过付费API访问，这使得它们对于频繁使用的应用来说成本过高。相比之下，较小且开源的LLMs（例如，参数数量少于3B）是免费提供的，并且易于本地部署（例如，在拥有8G VRAM的单个GPU上），但缺乏足够的推理能力。这种权衡引发了一个自然的问题：小型（免费）和大型（昂贵）模型是否可以在测试时进行协作，以结合它们的优势？我们提出了一种测试时协作框架，其中规划模型首先生成一个计划，定义为问题的提炼和高级抽象。这个计划作为轻量级的中间体，指导推理模型生成完整解决方案。小型和大型模型轮流充当规划者和推理者，在多轮级联中交换计划，以协作解决复杂任务。我们的方法实现了与强大专有模型相当的性能，同时显著减少了对外部推理的依赖。这些结果突出了规划作为在现实世界部署约束下进行成本感知、跨模型推理的有效先验。|
|**2025-06-13**|**Efficient Long-Context LLM Inference via KV Cache Clustering**|Jie Hu et.al.|[2506.11418](http://arxiv.org/abs/2506.11418)|null|大型语言模型（LLMs）具有扩展的上下文窗口，在解决复杂任务方面越来越普遍。然而，为长上下文LLMs所需的巨大键值（KV）缓存给部署带来了重大挑战。现有方法要么丢弃对未来世代可能至关重要的信息，要么由于高计算开销而提供有限的效率提升。在本文中，我们介绍了Chelsea，这是一个简单而有效的在线KV缓存聚类框架。我们的方法基于观察，即键状态在序列维度上表现出高度相似性。为了实现高效的聚类，我们将序列划分为块，并提出了分块软匹配，它在每个块内采用交替分区策略，并根据相似性识别聚类。然后，Chelsea将每个聚类内的KV缓存合并为一个单一的中心点。此外，我们还对计算复杂性和块内分区策略的最优性进行了理论分析。在多种模型和长上下文基准测试上的广泛实验表明，Chelsea实现了高达80%的KV缓存内存使用率降低，同时保持了可比的模型性能。此外，随着最小的计算开销，Chelsea将推理阶段的解码速度提高了高达3.19倍，并将端到端延迟减少了高达2.72倍。|
|**2025-06-13**|**Semantic Scheduling for LLM Inference**|Wenyue Hua et.al.|[2506.12204](http://arxiv.org/abs/2506.12204)|**[link](https://github.com/wenyueh/latency_optimization_with_priority_constraints)**|**传统的操作系统调度算法在很大程度上忽视了内容，仅根据延迟或公平性等因素做出决策，而不考虑进程的实际意图或语义。因此，这些算法往往不会优先处理需要紧急关注或具有更高重要性的任务，例如在应急管理场景中。然而，近年来语言模型的发展使得对进程的语义分析成为可能，从而允许更智能和上下文感知的调度决策。在本文中，我们引入了语义调度的概念，用于大型语言模型（LLM）请求的调度，其中进程的语义指导调度优先级。我们提出了一种新型调度算法，具有最优时间复杂度，旨在最小化基于LLM的提示调度中的整体等待时间。为了展示其有效性，我们提出了一种医疗紧急管理应用，强调了语义调度对于关键、时间敏感任务的潜在益处。代码和数据可在https://github.com/Wenyueh/latency_optimization_with_priority_constraints上获取。**|
|**2025-06-12**|**TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference**|Hongbin Zhang et.al.|[2506.10470](http://arxiv.org/abs/2506.10470)|null|随着模型尺寸的不断增大，由于对通信需求低，流水线并行在面向吞吐量的LLM推理中展现出巨大的潜力。然而，预填充和解码阶段的负载不平衡和复杂的数据依赖导致大量流水线气泡，进一步加剧了性能下降。为了更好地利用流水线并行提高高吞吐量LLM推理，我们提出了TD-Pipe，其核心思想在于时间解耦的流水线并行架构。具体来说，这种架构在时间维度上解耦预填充和解码阶段，以消除由阶段切换引起的流水线气泡。TD-Pipe识别了利用新型架构的潜在问题并提供了解决方案。首先，使用层次控制器结构通过解耦调度和执行来更好地协调流水线并行中的设备。其次，基于AI的贪婪预填充方法通过预测输出长度和模拟内存使用来积极执行更多预填充。第三，批间工作窃取方法动态平衡不同批次之间的解码阶段工作负载，以减少气泡。第四，通过比较减少计算强度带来的性能下降与阶段切换气泡带来的性能下降，空间-时间强度比较方法确定从解码到预填充的最佳切换。大量实验表明，TD-Pipe通过PCIe互连的GPU节点，在现有张量并行方法的基础上，将LLM推理的吞吐量提高了高达1.91倍，在现有流水线并行方法的基础上提高了2.73倍。|
|**2025-06-11**|**A First Look at Bugs in LLM Inference Engines**|Mugeng Liu et.al.|[2506.09713](http://arxiv.org/abs/2506.09713)|**[link](https://github.com/infbug/bugs-in-llm-inference-engines)**|**大型语言模型专用推理引擎（简称LLM推理引擎）已成为现代AI基础设施的基本组成部分，使得基于LLM的应用（LLM应用）能够在云端和本地设备上部署。尽管它们发挥着关键作用，但由于LLM巨大的资源需求和跨平台兼容性的复杂性，LLM推理引擎容易出错。然而，对这些错误系统性的理解仍然不足。为了填补这一空白，我们提出了关于LLM推理引擎错误的首次实证研究。我们挖掘了5个广泛采用的LLM推理引擎的官方仓库，构建了一个包含929个真实世界错误的综合数据集。通过严格的开放式编码过程，我们分析了这些错误，以揭示它们的症状、根本原因和共性。我们的发现揭示了六个主要的错误症状和28个根本原因的分类，揭示了在LLM推理引擎中检测和定位错误的关键挑战。基于这些见解，我们为研究人员、推理引擎供应商和LLM应用开发者提出了一系列可操作的启示。**|
|**2025-06-11**|**Understanding the Performance and Power of LLM Inferencing on Edge Accelerators**|Mayank Arya et.al.|[2506.09554](http://arxiv.org/abs/2506.09554)|null|大型语言模型（LLMs）在众多领域都展现出了卓越的益处，应用于代码生成和机器人导航等多样化的任务。虽然LLMs通常在云端数据中心提供服务，但针对任务关键和隐私敏感的应用可能需要本地托管开源LLM模型。鉴于LLM对大型GPU内存的需求，边缘加速器如搭载64GB共享GPU-CPU RAM的Nvidia Jetson Orin AGX成为一个有吸引力的选择。然而，在边缘加速器上执行LLM推理的可行性和性能尚未得到充分研究。本研究对NVIDIA Jetson Orin AGX上的LLM推理进行了详细评估，评估了从27亿到328亿参数的四个SOTA模型，例如Meta Llama3.1、Microsoft-Phi2、Deepseek-R1-Qwen。我们研究了不同批大小、序列长度和量化级别对延迟、吞吐量和困惑度的影响，并探索了Orin AGX上的各种自定义电源模式以进行功耗和能耗分析。我们的发现提供了关于效率、推理速度和资源使用之间权衡的有趣见解，例如，增加序列长度会导致令牌吞吐量下降，量化会导致较小的LLM速度变慢。这些结果有助于优化边缘加速器上LLM的实用应用服务。|
|**2025-06-11**|**Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning**|Jiayi Yuan et.al.|[2506.09501](http://arxiv.org/abs/2506.09501)|null|大型语言模型（LLMs）如今已成为多个领域的核心，并展现了令人印象深刻的性能。然而，这种进步建立在基准分数既准确又可复制的假设之上。我们证明，LLMs性能的可复现性是脆弱的：改变系统配置，如评估批大小、GPU数量和GPU版本，会在生成的响应中引入显著差异。这个问题在推理模型中尤为明显，早期标记的微小舍入差异可能会演变成分歧的思维链，最终影响准确性。例如，在bfloat16精度和贪婪解码下，DeepSeek-R1-Distill-Qwen-7B这样的推理模型由于GPU数量、类型和评估批大小的差异，其准确率可以变化高达9%，响应长度差异可达9,000个标记。我们将这种变异性归因于有限数值精度下浮点运算的非结合性。这项工作首次系统地研究了数值精度如何影响LLMs推理的可复现性。通过在不同硬件、软件和精度设置下进行严格控制实验，我们量化了模型输出何时以及如何出现分歧。我们的分析揭示，尽管浮点精度对于可复现性至关重要，但在评估实践中往往被忽视。受此启发，我们开发了一个轻量级推理管道，名为LayerCast，它在16位精度下存储权重，但在FP32下执行所有计算，在内存效率和数值稳定性之间取得平衡。代码可在https://github.com/nanomaoli/llm_reproducibility上找到。|
|**2025-06-10**|**Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive- $k$**|Chihiro Taguchi et.al.|[2506.08479](http://arxiv.org/abs/2506.08479)|null|检索增强生成（RAG）和长上下文语言模型（LCLM）都解决了LLM在开放域问答（QA）中的上下文限制问题。然而，最优外部上下文检索仍然是一个未解问题：固定检索大小可能会导致浪费令牌或遗漏关键证据。现有的自适应方法如Self-RAG和Self-Route依赖于迭代LLM提示，在事实型QA上表现良好，但在聚合QA上却难以应对，因为最优上下文大小既未知又变化。我们提出了Adaptive-$k$检索，这是一种简单而有效的单次遍历方法，它根据查询与候选段落之间相似度分数的分布自适应地选择段落数量。它不需要模型微调、额外的LLM推理或更改现有的检索器-阅读器管道。在事实型和聚合QA基准测试中，Adaptive-$k$与固定-$k$ 基线相当或优于，同时使用的令牌数量比全上下文输入少10倍，但仍检索到70%的相关段落。它在五个LCLM和两个嵌入模型上提高了准确性，突出了动态调整上下文大小可以导致更高效和准确的问答。|
|**2025-06-10**|**Draft-based Approximate Inference for LLMs**|Kevin Galim et.al.|[2506.08373](http://arxiv.org/abs/2506.08373)|**[link](https://github.com/furiosa-ai/draft-based-approx-llm)**|**由于Transformers的二次计算复杂度和线性内存复杂度，优化长上下文大型语言模型（LLM）的推理变得越来越重要。现有的近似方法，如键值（KV）缓存丢弃、稀疏注意力和提示压缩，通常依赖于对标记或KV对重要性的粗略预测。我们提出了一种新的近似LLM推理框架，该框架利用小型草稿模型更准确地预测标记和KV对的重要性。具体来说，我们引入了我们提出的框架的两个实例：（i）SpecKV，它利用草稿输出准确评估每个KV对的重要性，从而更有效地进行KV缓存丢弃；（ii）SpecPC，它使用草稿模型的注意力激活来识别和丢弃不重要的提示标记。据我们所知，这是第一个使用草稿模型进行近似LLM推理加速的工作，将它们的效用扩展到传统的无损推测解码之外。我们通过理论和实证分析来论证我们的方法，并显示出草稿模型和目标模型注意力模式之间的强烈相关性。在长上下文基准上的大量实验表明，我们的方法在保持相同内存使用、延迟和吞吐量改进的同时，始终比现有的基线实现更高的准确率。我们的代码可在https://github.com/furiosa-ai/draft-based-approx-llm上获取。**|
|**2025-06-09**|**MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts**|Wei Tao et.al.|[2506.07533](http://arxiv.org/abs/2506.07533)|null|在优化大型语言模型（LLMs）进行长上下文推理的主要挑战之一在于键值（KV）缓存的高内存消耗。现有的方法，如量化，已经在减少内存使用方面展示了有希望的结果。然而，当前的量化方法无法同时考虑有效性和效率。在本文中，我们提出了MoQAE，一种通过量化感知专家混合的混合精度量化方法。首先，我们将不同的量化位宽配置视为专家，并使用传统的混合专家（MoE）方法来选择最佳配置。为了避免在传统MoE方法中将令牌逐个输入到路由器引起的低效，我们以块的形式将令牌输入到路由器。其次，我们设计了一个轻量级仅路由器微调过程，通过综合损失来训练MoQAE，以学习模型精度和内存使用之间的权衡。最后，我们引入了路由冻结（RF）和路由共享（RS）机制，以进一步减少推理开销。在多个基准数据集上的大量实验表明，我们的方法在效率和有效性方面都优于最先进的KV缓存量化方法。|
|**2025-06-07**|**Containerized In-Storage Processing and Computing-Enabled SSD Disaggregation**|Miryeong Kwon et.al.|[2506.06769](http://arxiv.org/abs/2506.06769)|null|ISP模型通过最小化数据传输进行数据分析，但在适应性和解耦方面面临挑战。我们提出了一种名为DockerSSD的ISP模型，该模型利用操作系统级别的虚拟化和轻量级固件，在SSD上直接实现容器化数据处理。其主要特点包括基于网络的ISP管理所用的NVMe以太网和用于安全、高效容器执行的虚拟固件。DockerSSD支持解耦的存储池，减少主机开销，并增强大规模服务如LLM推理。对于I/O密集型工作负载，它实现了高达2.0倍的性能提升，在分布式LLM推理方面提高了7.9倍。|
|**2025-06-06**|**Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques**|Adarsh Prasad Behera et.al.|[2506.06579](http://arxiv.org/abs/2506.06579)|null|近年来，语言模型（LMs）在自然语言处理（NLP）领域取得了显著进展，在文本生成、摘要和问答等任务上表现出色。然而，它们的推理过程在计算上仍然昂贵且能耗高，尤其是在硬件、电力或带宽有限的场景中。这使得在移动、边缘或成本敏感的环境中部署LMs变得困难。为了解决这些挑战，最近的研究提出了多LM智能模型选择策略，这些策略根据查询复杂度动态分配计算资源——对于简单的查询使用轻量级模型，并在必要时升级到更大的模型。本文综述了两种提高LM推理效率的互补策略：（i）路由，根据查询选择最合适的模型；（ii）级联或分层推理（HI），通过一系列模型逐步提升查询，直到找到可靠的回答。这两种方法都旨在通过使用轻量级模型来简化简单任务，并在必要时才进行卸载。我们提供了这些技术在不同关键性能指标上的比较分析，讨论了基准测试工作，并概述了开放性挑战。最后，我们提出了未来的研究方向，以实现更快的响应时间、根据任务复杂度自适应模型选择，以及在异构环境中可扩展的部署，从而使基于LM的系统更加高效和易于访问，适用于现实世界应用。|
|**2025-06-04**|**SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling**|Anhao Zhao et.al.|[2506.04179](http://arxiv.org/abs/2506.04179)|null|大型语言模型（LLMs）在各个任务上取得了显著的性能，但由于其深度多层架构，它们也带来了巨大的计算成本。层剪枝已成为一种减轻这些低效性的策略，但传统的静态剪枝方法忽略了LLM推理中两个关键的内在动态：（1）水平动态，其中标记级别的异质性需要上下文感知的剪枝决策；（2）垂直动态，其中MLP和自注意力层的不同功能角色需要针对组件特定的剪枝策略。我们引入了SkipGPT，这是一个动态层剪枝框架，旨在通过两种核心创新来优化计算资源分配：（1）全局标记感知路由，以优先处理关键标记；（2）MLP和自注意力组件的解耦剪枝策略。为了减轻训练的不稳定性，我们提出了一个两阶段优化范式：首先，一个解耦的训练阶段，通过软参数化学习路由策略，以避免过早的剪枝决策；随后，参数高效的LoRA微调来恢复层移除影响的性能。大量实验表明，SkipGPT在减少超过40%的模型参数的同时，在基准测试中与原始密集模型匹配或超越其性能。通过协调动态效率与保留的表达能力，SkipGPT推动了可扩展、资源感知的LLMs的实际部署。我们的代码在以下网址公开：https://github.com/EIT-NLP/SkipGPT。|
|**2025-06-04**|**Pre $^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation**|Junyi Chen et.al.|[2506.03887](http://arxiv.org/abs/2506.03887)|null|大量使用大型语言模型（LLM）需要高效的格式化生成，尤其是对于LR(1)文法，以生成指定格式的输出（如JSON）。现有方法主要将LR(1)文法解析为推栈自动机（PDA），导致在上下文依赖的标记处理上的运行时执行开销，尤其是在大推理批次下效率低下。为了解决这些问题，我们提出了Pre$^3$，它利用确定性推栈自动机（DPDA）来优化约束LLM解码效率。首先，通过预处理阶段预计算前缀条件边，Pre$^3$使提前分析边成为可能，从而实现并行转换处理。其次，通过利用前缀条件边，Pre$^3$引入了一种新的方法，将LR(1)转换图转换为DPDA，消除了运行时路径探索的需要，并实现了具有最小开销的边转换。在我们的实验中，Pre$^3$ 可以无缝集成到标准的LLM推理框架中，将每个输出标记的时间（TPOT）减少高达40%，并将吞吐量提高高达36%。我们的代码可在https://github.com/ModelTC/lightllm上找到。|
|**2025-06-04**|**Client-Side Zero-Shot LLM Inference for Comprehensive In-Browser URL Analysis**|Avihay Cohen et.al.|[2506.03656](http://arxiv.org/abs/2506.03656)|null|恶意网站和钓鱼URL对网络安全构成了日益增长的威胁，钓鱼攻击在一年内增长了40%。传统的检测方法依赖于在云端运行的机器学习分类器或基于规则的扫描器，但这些方法在泛化、隐私和逃避复杂威胁方面面临重大挑战。在本文中，我们提出了一种新颖的客户端框架，用于全面分析URL，该框架利用本地大型语言模型（LLM）通过零样本推理完全在浏览器中运行。我们的系统使用紧凑型LLM（例如，3B/8B参数）通过WebLLM来对从目标网页收集的丰富上下文进行推理，包括静态代码分析（JavaScript抽象语法树、结构和代码模式）、动态沙箱执行结果（DOM更改、API调用和网络请求）以及可见内容。我们详细介绍了系统的架构和方法，该系统结合了一个真实的浏览器沙箱（使用iframe），它能够抵御常见的反分析技术，以及一个基于LLM的分析器，该分析器评估潜在漏洞和恶意行为，而无需任何特定任务的训练（零样本）。LLM从多个来源（代码、执行跟踪、页面内容）汇总证据，将URL分类为良性或恶意，并解释识别出的威胁或安全问题。我们在各种良性恶意URL上评估了我们的方法，证明即使是紧凑的客户端模型也能实现高检测准确率和与云端解决方案相当的有洞察力的解释，同时在终端用户设备上运行时保持隐私。结果显示，客户端LLM推理是网络威胁分析的一种可行且有效的解决方案，消除了将可能敏感的数据发送到云服务的需要。|
|**2025-06-04**|**POSS: Position Specialist Generates Better Draft for Speculative Decoding**|Langlin Huang et.al.|[2506.03566](http://arxiv.org/abs/2506.03566)|**[link](https://github.com/shrango/poss)**|1. 首先，识别摘要中的关键信息和术语：    - Speculative decoding：投机解码    - Large Language Model (LLM)：大型语言模型    - inference：推理    - draft model：草稿模型    - target model：目标模型    - hidden state：隐藏状态    - prediction accuracy：预测精度    - error accumulation：错误累积    - Position Specialists (PosS)：位置专家（PosS）    - position-specialized draft layers：位置专用草稿层    - token acceptance rate：标记接受率    - drafting round：草稿轮次    - feature deviation：特征偏差    - acceptance length：接受长度    - speed-up ratio：加速比    - Llama-3-8B-Instruct    - Llama-2-13B-chat    - datasets：数据集  2. 根据中文语法和表达习惯，逐步翻译摘要内容：     - Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel.      投机解码通过使用一个小型草稿模型预测多个标记，并使用一个大型的目标模型并行验证这些标记，从而加速大型语言模型（LLM）的推理。     - Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy.      近期研究利用目标模型的隐藏状态来提高草稿模型的预测精度。     - However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features.      然而，现有方法因草稿模型生成特征中的错误累积，导致后续位置草稿标记预测质量下降。     - In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s).      在本文中，我们提出了位置专家（PosS），它由多个位置专用草稿层组成，用于在指定的位置（s）生成标记。     - Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation.      位置专家在每一轮草稿中大大提高了后续位置标记的接受率，因为每个专家只需要关注处理一定程度的草稿模型特征偏差。     - Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio.      在Llama-3-8B-Instruct和Llama-2-13B-chat上进行的实验结果表明，PosS在平均接受长度和加速比方面有效地优于基线。     - Our codebase is available at https://github.com/shrango/PosS.      我们的代码库可在https://github.com/shrango/PosS上获得。  3. 将上述翻译内容合并，得到完整的中文翻译摘要：  投机解码通过使用一个小型草稿模型预测多个标记，并使用一个大型的目标模型并行验证这些标记，从而加速大型语言模型（LLM）的推理。近期研究利用目标模型的隐藏状态来提高草稿模型的预测精度。然而，现有方法因草稿模型生成特征中的错误累积，导致后续位置草稿标记预测质量下降。在本文中，我们提出了位置专家（PosS），它由多个位置专用草稿层组成，用于在指定的位置（s）生成标记。位置专家在每一轮草稿中大大提高了后续位置标记的接受率，因为每个专家只需要关注处理一定程度的草稿模型特征偏差。在Llama-3-8B-Instruct和Llama-2-13B-chat上进行的实验结果表明，PosS在平均接受长度和加速比方面有效地优于基线。我们的代码库可在https://github.com/shrango/PosS上获得。|
|**2025-06-04**|**On the Fundamental Impossibility of Hallucination Control in Large Language Models**|Michał P. Karpowicz et.al.|[2506.06382](http://arxiv.org/abs/2506.06382)|null|本文解释了为什么无法创建不产生幻觉的大型语言模型，以及我们应该寻找哪些权衡。它提出了一个形式化的\textbf{不可能定理}，证明了没有任何推理机制可以同时满足四个基本属性：\textbf{真实（非幻觉）生成、语义信息保留、相关知识揭示和知识约束下的最优性}。通过将LLM推理建模为一个\textbf{想法拍卖}，其中神经网络组件竞争为响应做出贡献，我们使用Green-Laffont定理证明了这一点。这个数学框架为理解推理过程的本质提供了严格的基础，对模型架构、训练目标和评估方法产生了影响。|
|**2025-06-03**|**Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs**|Shangmin Guo et.al.|[2506.02918](http://arxiv.org/abs/2506.02918)|null|在具有状态的环境中使用工具为大型语言模型（LLMs）带来了独特的挑战，因为现有的测试时计算策略依赖于在环境中重复试验，这在实际中是不可行的。我们提出了一种名为DyMo的方法，该方法在训练后调用函数的同时，增强了LLMs的状态预测能力。这使得LLMs能够通过内部环境模型预测其动作的未来状态。在伯克利函数调用排行榜V2上，DyMo提高了成功率并显著减少了幻觉。我们进一步将内部环境模型集成到自我验证采样（SVS）中，并表明这大大提高了通过率与试验次数k的比例，并允许模型拒绝不可靠的输出。DyMo和SVS共同极大地增强了LLMs在工具使用方面的有效性和可靠性。我们相信这项工作为LLM推理中可扩展的规划强化学习（RL）方法铺平了道路，而无需反复查询算术环境。|
|**2025-06-03**|**HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference**|Ping Gong et.al.|[2506.02572](http://arxiv.org/abs/2506.02572)|**[link](https://github.com/gpzlx1/hata)**|大型语言模型（LLMs）已成为一个关键的研究领域，然而，即使在像KVCache这样的技术来减轻冗余计算的情况下，注意力模块仍然是LLM推理中的关键瓶颈。虽然已经提出了各种top- $k$注意力机制来通过利用注意力固有的稀疏性来加速LLM推理，但它们往往难以在效率和准确性之间取得平衡。在本文中，我们介绍了HATA（Hash-Aware Top-$k$ Attention），这是一种新颖的方法，它系统地将低开销的哈希学习技术集成到Top-$k$注意力过程中。与现有的专注于寻求qk分数绝对估计的top-k注意力方法不同，这些方法通常代价很大，HATA将查询和键映射到二进制哈希码，并以相当低的成本获取qk分数的相对顺序，这对于实现top-k注意力是足够的。广泛的实验表明，与传统的全注意力相比，HATA实现了高达7.2倍的加速，同时保持了模型准确性。此外，HATA在多个主流LLM模型和不同任务中，在准确性和效率方面都优于最先进的top-$k$ 注意力方法。HATA的源代码可在https://github.com/gpzlx1/HATA上获取。|
|**2025-06-03**|**Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs**|Jiakun Fan et.al.|[2506.03296](http://arxiv.org/abs/2506.03296)|null|将大型语言模型（LLMs）部署于在线推理时常受限于有限的GPU内存，尤其是由于自回归解码过程中KV缓存的不断增长。混合GPU-CPU执行已作为一种有希望的解决方案出现，通过将KV缓存管理和部分注意力计算卸载到CPU。然而，一个关键瓶颈仍然存在：现有的调度器无法在延迟敏感、带宽受限的解码阶段有效地重叠CPU卸载任务与GPU执行。这尤其影响了实时、解码密集型应用（例如聊天、思维链推理），这些应用目前未得到现有系统的充分服务，尤其是在边缘或低成本部署的典型内存压力下。我们提出了APEX，这是一种新颖的、基于分析的调度策略，旨在最大化混合LLM推理期间的CPU-GPU并行性。与依赖于静态规则或纯启发式方法的系统不同，APEX通过预测CPU和GPU子任务的执行时间，动态地将计算任务分配到异构资源上，以最大化重叠并避免调度开销。我们在LLaMa-2-7B和LLaMa-3.1-8B模型上，对多种工作负载和GPU架构（NVIDIA T4、A10）进行了APEX的评估。与仅GPU调度器如VLLM相比，APEX在T4上提高了84%至96%、在A10上提高了11%至89%的吞吐量，同时保持延迟。与最佳的现有混合调度器相比，在长输出设置中，它提供了高达49%（T4）和37%（A10）的更高吞吐量。APEX显著提高了内存受限硬件上的混合LLM推理效率，并为异构AI系统中的调度提供了一个蓝图，填补了高效实时LLM应用的关键空白。|
|**2025-06-02**|**Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts**|Spencer Banasik et.al.|[2506.01827](http://arxiv.org/abs/2506.01827)|null|随着机器学习算法逐渐显示出其越来越有价值的作用，对其访问的需求也随之增长。通常情况下，如果没有加速器，使用较大的模型进行推理是不可行的，而在那些有诸如能耗、安全或成本等限制的环境中，加速器可能不可用。为了提高这些模型的可用性，我们旨在通过修改缓存架构来提高仅使用CPU环境的LLM推理速度。为了确定可以做出哪些改进，我们使用Llama.cpp和QWEN模型进行了两项实验：运行各种缓存配置并评估其性能，以及输出内存占用轨迹。通过这些实验，我们研究了内存访问模式和性能特性，以识别潜在的优化方案。|
|**2025-05-30**|**Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching**|Juan Wisznia et.al.|[2505.24643](http://arxiv.org/abs/2505.24643)|null|我们提出了一种新的框架，用于分析成对排名提示（PRP）中的排序算法，将成本模型的重心从传统的成对比较转移到LLM推理。虽然基于比较次数的经典指标传统上被用来衡量效率，但我们的分析揭示了昂贵的LLM推理颠覆了这些预测；因此，我们的框架鼓励批处理和缓存等策略来减轻推理成本。我们表明，在经典设置中最优的算法，在LLM推理在某种优化下占据主导成本时，可能会失去效率。|
|**2025-05-30**|**LLM Inference Enhanced by External Knowledge: A Survey**|Yu-Hsuan Lin et.al.|[2505.24377](http://arxiv.org/abs/2505.24377)|**[link](https://github.com/miulab/kg-survey)**|**近年来，大型语言模型（LLMs）在自然语言推理方面的进步显著。然而，它们有限的参数记忆能力和易受幻觉的影响，为需要准确、基于上下文的推理任务带来了持续的挑战。为了克服这些限制，越来越多的研究提出了利用外部知识来增强LLMs的方法。本研究系统地探讨了利用外部知识增强LLMs的策略，首先将外部知识分为非结构化和结构化数据。然后，我们重点关注结构化知识，分别对表格和知识图谱（KGs）提出不同的分类体系，详细阐述了它们与LLMs的集成范式，并回顾了代表性方法。我们的比较分析进一步突显了可解释性、可扩展性和性能之间的权衡，为开发可信赖且具有通用性的知识增强LLMs提供了见解。**|
|**2025-05-30**|**SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference**|Tian Xia et.al.|[2505.24095](http://arxiv.org/abs/2505.24095)|null|在多区域环境中高效地服务于大型语言模型（LLMs）仍然是一个挑战。由于成本和GPU可用性的担忧，服务提供商通常通过长期承诺的实例（如预留实例或本地集群）在多个区域部署LLMs，这些实例由于区域本地流量处理和日间流量变化，通常利用率较低。在本文中，我们介绍了SkyLB，这是一种具有区域感知的多区域负载均衡器，用于LLM推理，通过跨区域流量处理聚合区域日间模式。通过这样做，SkyLB使得提供商能够根据预期的全球需求而非每个区域的高峰需求来预留实例。同时，SkyLB保留了KV-Cache的局部性和负载平衡，确保了成本效益而不会牺牲性能。SkyLB通过缓存感知的跨区域流量处理和基于检查挂起请求的选择性推送负载平衡机制来实现这一点。我们在实际工作负载上的评估显示，与现有的负载均衡器相比，SkyLB实现了1.12-2.06倍更高的吞吐量和1.74-6.30倍更低的延迟，同时将总服务成本降低了25%。|
|**2025-05-29**|**Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and Hetero-Core Parallelism**|Jinhui Wei et.al.|[2505.23219](http://arxiv.org/abs/2505.23219)|null|在端用户设备上进行的本地化大型语言模型（LLM）推理因其隐私优势和对外部基础设施的依赖减少而受到广泛关注。然而，由于解码过程受内存带宽限制，现代端用户设备中的各种处理单元无法得到充分利用，导致LLM推理速度缓慢。本文提出了一种名为Ghidorah的针对端用户设备的LLM推理系统，该系统采用了统一的内存架构。Ghidorah的关键思想可以概括为两个步骤：1）利用推测性解码方法来增强并行性；2）巧妙地将工作负载分配到多个异构处理单元，以最大化计算能力利用率。Ghidorah包括异核模型并行（HCMP）架构和架构感知分析（ARCA）方法。HCMP架构通过利用端用户设备的统一内存设计并适应推测性解码的混合计算需求来指导分区。ARCA方法用于确定最优的推测策略和分区策略，在吞吐量和并行能力之间取得平衡，以最大化加速比。此外，我们还优化了ARM CPU上的稀疏计算。实验结果表明，与NVIDIA Jetson NX中顺序解码方法相比，Ghidorah在主导的LLM解码阶段可以实现高达7.6倍的加速。|
|**2025-05-29**|**SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference**|Yinghao Tang et.al.|[2505.23022](http://arxiv.org/abs/2505.23022)|null|现有的大型语言模型（LLM）服务系统优先考虑最大吞吐量。它们往往忽视服务等级目标（SLOs）如首次标记时间（TTFT）和每输出标记时间（TPOT），这导致SLO达成率不佳。本文介绍了一种以SLO为导向的LLM服务系统SCORPIO，旨在最大化系统吞吐量和具有异构SLO的工作负载的SLO达成率。我们的核心洞察是利用SLO异构性来实现在接入控制、队列管理和批处理选择中的自适应调度。SCORPIO具有一个TTFT守护者，它采用最早截止时间优先重排序并拒绝不可达的请求，以及一个TPOT守护者，它利用基于VBS的接入控制和一种新颖的基于信用批处理机制。这两个守护者都由一个预测模块支持。评估结果表明，与最先进的基线相比，SCORPIO将系统吞吐量提高了最多14.4倍，将SLO遵守率提高了最多46.5%。|
|**2025-05-29**|**Large Language Model Meets Constraint Propagation**|Alexandre Bonlarron et.al.|[2505.24012](http://arxiv.org/abs/2505.24012)|null|大型语言模型（LLMs）在生成流畅文本方面表现出色，但难以施加外部约束，因为它们在缺乏明确控制机制的情况下按顺序生成标记。GenCP通过将LLM预测与约束规划（CP）推理相结合，将文本生成表述为约束满足问题（CSP）来解决这个问题。在本文中，我们通过集成掩码语言模型（MLMs）进行领域生成来改进GenCP，这允许双向约束传播，利用过去和未来的标记。这种集成在标记级预测与结构化约束执行之间架起桥梁，导致更加可靠和具有约束意识的文本生成。我们在COLLIE基准上的评估表明，通过MLM调用进行领域预览显著提高了GenCP的性能。尽管这种方法需要额外的MLM调用，并且在某些情况下增加了回溯，但总体效果是更高效地使用LLM推理，并增强了生成可行和有意义解决方案的能力，尤其是在具有严格内容约束的任务中。|
|**2025-05-28**|**Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference**|Yue Zhu et.al.|[2505.21919](http://arxiv.org/abs/2505.21919)|null|随着具有扩展上下文窗口的大型语言模型（LLMs）的日益普及，高效的键值缓存（KVC）管理对于优化推理性能至关重要。像检索增强生成（RAG）和智能体这样的推理工作负载表现出高缓存重用性，这使得高效的缓存对于减少冗余和提高速度至关重要。我们使用公开可用的跟踪分析了现实世界的KVC访问模式，并评估了如Redis这样的商业键值存储以及最先进的基于RDMA的系统（CHIME [1] 和Sherman [2]）用于KVC元数据管理。我们的工作证明了缺乏针对KVC预填充的定制存储解决方案，强调了对于LLM工作负载需要一个高效的分布式缓存系统，并具有优化的元数据管理，同时为设计可扩展、低延迟的推理KVC管理系统提供了见解。|
|**2025-05-28**|**Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference**|Donghyeon Joo et.al.|[2505.22913](http://arxiv.org/abs/2505.22913)|**[link](https://github.com/dhjoo98/mustafar)**|**我们证明了非结构化稀疏性显著提高了LLM的KV缓存压缩效果，能够在不降低准确性的情况下实现高达70%的稀疏度，且无需微调。我们对剪枝策略进行了系统性的探索，发现基于每个标记幅度的剪枝在非结构化稀疏性下对键和值缓存都极为有效，超越了之前的结构化剪枝方案。键缓存受益于显著的异常值元素，而值缓存出人意料地受益于基于幅度的简单剪枝，尽管其分布均匀。KV缓存大小是解码性能的主要瓶颈，因为大上下文长度的高内存开销。为了解决这个问题，我们使用基于位图的稀疏格式和自定义的注意力内核，能够压缩并直接在剪枝到任意稀疏模式的压缩缓存上进行计算，显著加速了解码计算中的内存密集型操作，从而补偿了运行时剪枝和压缩的开销。我们的自定义注意力内核与基于位图的格式相结合，实现了KV缓存高达45%的压缩，从而使得上下文长度更长，每秒处理令牌数增加了高达2.23倍，与密集推理相比。我们的剪枝机制和稀疏注意力内核可在https://github.com/dhjoo98/mustafar上获取。**|
|**2025-05-27**|**HoliTom: Holistic Token Merging for Fast Video Large Language Models**|Kele Shao et.al.|[2505.21334](http://arxiv.org/abs/2505.21334)|**[link](https://github.com/cokeshao/holitom)**|视频大型语言模型（视频LLM）在视频理解方面表现出色，但由于冗余视频标记而面临显著的计算效率问题。现有的标记剪枝方法提供了解决方案。然而，在LLM内部（内部-LLM剪枝）操作的方法，如FastV，在浅层中会带来固有的计算开销。相比之下，在LLM之前进行标记剪枝的方法（外部-LLM剪枝）主要解决单个帧内的空间冗余或有限时间窗口的问题，忽略了长视频序列中至关重要的全局时间动态和相关性。这导致了次优的时空缩减，并且没有充分利用视频的可压缩性。关键的是，结合这些策略的协同潜力和相互影响尚未被探索。为了进一步减少冗余，我们引入了HoliTom，这是一个新颖的无需训练的整体标记合并框架。HoliTom通过全局冗余感知的时间分割进行外部-LLM剪枝，然后进行时空合并，以超过90%的比例减少视觉标记，显著减轻了LLM的计算负担。作为补充，我们引入了一种基于内部-LLM标记相似度的鲁棒合并方法，旨在实现卓越的性能并兼容外部-LLM剪枝。评估表明，我们的方法在LLaVA-OneVision-7B上实现了有希望的效率-性能权衡，将计算成本降低到原始FLOPs的6.9%，同时保持了99.1%的原有性能。此外，我们实现了时间到第一个标记（TTFT）的2.28倍减少和解码吞吐量的1.32倍加速，突出了我们集成剪枝方法在高效视频LLM推理中的实际益处。|
|**2025-05-27**|**FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration**|Daehyeon Baek et.al.|[2505.20839](http://arxiv.org/abs/2505.20839)|null|随着大型语言模型的日益普及，内存带宽限制显著限制了推理吞吐量，从而推动了训练后量化（PTQ）的发展。在本文中，我们提出FireQ，这是一个协同设计的PTQ框架和一个INT4-FP8矩阵乘法内核，可以加速所有线性层的LLM推理。具体来说，FireQ将线性层权重和键值量化为INT4，并将激活和查询量化为FP8，从而显著提高了吞吐量。此外，我们引入了三个阶段的流水线化预填充阶段，修改了FlashAttention-3内核，有效降低了预填充阶段的时间到第一个标记。为了最小化量化带来的精度损失，我们针对线性层和注意力层分别开发了新的异常值平滑技术。在线性层中，我们显式使用每个张量的缩放来防止由INT4量化FP8量化缩放因子引起的下溢，并使用通道缩放来补偿INT4的粗粒度。在注意力层中，我们通过结合预RoPE和后RoPE缩放策略来解决由旋转位置嵌入（RoPE）带来的量化挑战。FireQ在Llama2-7B的前馈网络层上实现了1.68倍的推理速度提升，在Llama3-8B上实现了1.26倍的预填充阶段性能提升，与QServe相比，精度损失可以忽略不计。|
|**2025-05-26**|**MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE**|Zongle Huang et.al.|[2505.19645](http://arxiv.org/abs/2505.19645)|null|大型语言模型（LLMs）在许多应用中取得了显著成功，混合专家（MoE）模型展现出巨大潜力。与传统的密集模型相比，MoE在更少的计算量下实现了更好的性能。投机解码（SD）是一种广泛用于加速LLM推理而不损失精度的技术，但此前人们认为它只对密集模型有效。在这项工作中，我们首先证明，在中等批次大小下，MoE出人意料地比密集模型从SD中获益更多。此外，随着MoE变得越来越稀疏——这是MoE设计中的主流趋势——SD加速预期的有效批次范围变得更加广泛。为了定量理解SD中的权衡，我们基于理论分析开发了一种可靠的建模方法。虽然当前的SD研究主要关注提高算法的接受率，但工作负载和模型架构的变化仍然可能导致即使接受率很高，SD加速效果也会降低。为了解决这一限制，我们引入了一个新的指标“目标效率”，它表征了这些效应，从而帮助研究人员识别系统瓶颈并更全面地理解SD加速。对于私人服务之类的场景，这项工作揭示了一种加快MoE推理的新视角，而现有的解决方案在这里却面临挑战。在不同GPU上的实验表明，在中等批次大小下，Qwen2-57B-A14B的速度可提高高达2.29倍，并验证了我们的理论预测。|
|**2025-05-26**|**WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference**|Sihan Chen et.al.|[2505.19427](http://arxiv.org/abs/2505.19427)|**[link](https://github.com/microsoft/wina)**|**随着大型语言模型（LLM）的计算需求不断增长，高效的推理和激活策略变得日益关键。虽然最近的方法，如混合专家（MoE）模型，利用了选择性激活，但需要专门的训练，而无需训练的稀疏激活方法通过其即插即用的设计提供了更广泛的应用范围和更优的资源效率。然而，许多现有方法仅依赖于隐藏状态的幅度来确定激活，导致高近似误差和次优的推理精度。为了解决这些局限性，我们提出了WINA（加权神经元激活），这是一个新颖、简单且无需训练的稀疏激活框架，它同时考虑了隐藏状态的幅度和权重矩阵的列 $\ell_2$ -范数。我们表明，这导致了一种稀疏化策略，该策略在理论保证上比现有技术更严格地获得了最优近似误差界限。在实证上，WINA在相同稀疏水平下，在平均性能方面优于最先进的方法（例如，TEAL），最高可达2.94%。这些结果将WINA定位为LLM推理中无需训练的稀疏激活的新性能前沿，推进了无需训练的稀疏激活方法，并为一项高效推理的稳健基准奠定了基础。源代码可在https://github.com/microsoft/wina获取。**|
|**2025-05-26**|**HAMburger: Accelerating LLM Inference via Token Smashing**|Jingyu Liu et.al.|[2505.20438](http://arxiv.org/abs/2505.20438)|null|随着对高效大型语言模型（LLM）推理需求的增长，需要在算法、系统和硬件上进行全面优化。然而，很少的研究从根本上改变了生成模式：每个标记需要一个前向传递和一个KV缓存。这可能不是最优的，因为我们发现LLM能够非常有效地自我识别单个KV缓存可以存储的确切信息量，并且许多标记可以在没有全局上下文的情况下自信地生成。基于这一洞察，我们引入了HAMburger，这是一个层次化自回归模型，通过在推理过程中超越每个标记的均匀计算和存储，重新定义了LLM中的资源分配。HAMburger在基础LLM之间堆叠了一个组合嵌入器和一个微步解码器，将多个标记压缩成一个单一的KV，并每步生成多个标记。此外，HAMburger作为一个推测性解码框架，可以盲目地信任自行绘制的标记。因此，HAMburger将KV缓存和前向FLOPs的增长从与输出长度成线性关系转变为亚线性关系，并根据查询困惑度和输出结构调整其推理速度。广泛的评估表明，HAMburger可以将KV缓存计算减少高达2倍，并实现高达2倍的TPS，同时在短上下文和长上下文任务中保持质量。我们的方法探索了一种极其具有挑战性的推理环境，该环境需要计算和内存效率，并且具有硬件无关的设计。|
|**2025-05-25**|**DECA: A Near-Core LLM Decompression Accelerator Supporting Out-of-Order Invocation**|Gerasimos Gerogiannis et.al.|[2505.19349](http://arxiv.org/abs/2505.19349)|null|为了缓解大型语言模型（LLM）推理工作负载中的内存带宽瓶颈，权重矩阵以量化和稀疏化格式存储在内存中。因此，在处理这些矩阵的块之前，它们需要先进行去量化和去稀疏化。目前，这一过程是通过软件中的向量操作完成的。不幸的是，这种方法只能提供有限的性能。此外，由于整体GeMM性能依赖于内存资源、向量单元和硬件矩阵引擎之间的交互，因此很难理解如何改进系统。为了提高配备内核GeMM引擎和HBM的先进平台上的LLM推理性能，本文做出了三项主要贡献。首先，它开发了一个具有3D可视化表示的解析性能模型，该模型揭示了内存资源、向量单元和硬件矩阵引擎如何相互作用以提供压缩的GeMM性能。其次，它提出了DECA，这是一种新的近核机器学习模型解压缩加速器。DECA将块去稀疏化和去量化从CPU卸载，为内核GeMM引擎提供现成的块。第三，它引入了一种新的ISA扩展，该扩展允许近核加速器乱序调用。使用此扩展，加速器和核心计算可以高效率地交错和重叠。我们的评估表明，在配备HBM的模拟56核Xeon 4服务器上，DECA将压缩GeMM的执行速度提高了最多4倍，比使用优化的英特尔软件内核提高了4倍。此外，DECA将Llama2-70B和OPT-66B的下一个标记生成时间缩短了1.6倍至2.6倍。|
|**2025-05-24**|**A Survey of LLM $\times$ DATA**|Xuanhe Zhou et.al.|[2505.18458](http://arxiv.org/abs/2505.18458)|**[link](https://github.com/weaidb/awsome-data-llm)**|**大型语言模型（LLM）与数据管理（DATA）的整合正在迅速重新定义这两个领域。在这篇综述中，我们全面回顾了双向关系。一方面，DATA4LLM，涵盖了大规模数据处理、存储和服务的各个方面，为LLM提供高质量、多样性和时效性的数据，以满足预训练、后训练、检索增强生成和代理工作流程等阶段的需求：（i）LLM的数据处理包括可扩展的获取、去重、过滤、选择、领域混合和合成增强；（ii）LLM的数据存储侧重于高效的数据和模型格式、分布式和异构存储层次结构、KV缓存管理和容错检查点；（iii）LLM的数据服务应对RAG（例如，知识后处理）、LLM推理（例如，提示压缩、数据溯源）和训练策略（例如，数据打包和洗牌）的挑战。另一方面，在LLM4DATA中，LLM正成为数据管理的一般用途引擎。我们回顾了以下方面的最新进展：（i）数据操作，包括自动数据清理、集成和发现；（ii）数据分析，涵盖对结构化、半结构化和非结构化数据的推理；（iii）系统优化（例如，配置调整、查询重写、异常诊断），这些优化得益于LLM技术，如检索增强提示、任务专用微调和多智能体协作。**|
|**2025-05-23**|**Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning**|Michael Hassid et.al.|[2505.17813](http://arxiv.org/abs/2505.17813)|null|推理大型语言模型（LLMs）在执行复杂推理任务时，严重依赖扩展测试时的计算能力，通过生成大量的“思考”链来进行。虽然这种方法展示了令人印象深刻的结果，但同时也带来了巨大的计算成本和推理时间。在这项工作中，我们挑战了长思考链导致更好推理能力的假设。我们首先证明，在单个问题内的较短推理链更有可能得出正确答案——比同一问题的最长链采样高出高达34.5%的准确性。基于这些结果，我们提出了短-m@k，一种新颖的推理LLM推理方法。我们的方法并行执行k个独立的生成，一旦完成前m个思考过程，就停止计算。最终答案通过这些m个链中的多数投票来选择。基本的短-1@k在低计算设置中表现出与标准多数投票相似甚至更优的性能——使用多达40%更少的思考标记。短-3@k虽然比短-1@k略低效，但在所有计算预算下，始终优于多数投票，同时仍然大大加快（最多33%的墙时减少）。受我们结果启发，我们使用短、长和随机选择的推理链微调了一个LLM。然后我们观察到，在较短的链上训练会导致更好的性能。我们的发现表明，需要重新思考推理LLM中当前测试时计算的方法，强调更长的“思考”并不一定转化为改进的性能，反而可能出人意料地导致性能下降。|
|**2025-05-23**|**DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies**|Ning Yang et.al.|[2505.17420](http://arxiv.org/abs/2505.17420)|null|大型语言模型（LLMs）在众多自然语言处理（NLP）任务上取得了显著的性能。然而，它们巨大的推理成本成为了现实部署的主要障碍，尤其是在对延迟敏感的场景中。为了应对这一挑战，我们提出了DASH，一个自适应层跳过框架，该框架根据输入特征动态选择计算路径。我们将跳过过程建模为马尔可夫决策过程（MDP），从而根据中间表示实现细粒度的标记级决策。为了减轻跳过可能导致的性能下降，我们引入了一种轻量级的补偿机制，将差异奖励注入到决策过程中。此外，我们设计了一种异步执行策略，将层计算与策略评估重叠，以最小化运行时开销。在多个LLM架构和NLP基准测试上的实验表明，我们的方法在保持具有竞争力的任务性能的同时，实现了显著的推理加速，优于现有方法。|
|**2025-05-23**|**An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs**|Rahul Thomas et.al.|[2505.18332](http://arxiv.org/abs/2505.18332)|null|近年来，大型语言模型（LLMs）的进步导致了第三方推理服务的广泛应用，引发了关键的隐私问题。现有的执行隐私第三方推理的方法，如安全多方计算（SMPC），通常依赖于加密方法。然而，这些方法比标准未加密推理慢数千倍，并且无法扩展到大型现代LLMs。因此，最近的研究探索了用统计混淆方法替换SMPC中昂贵的加密非线性计算——特别是向第三方揭示排列后的隐藏状态，同时声称反转回未排列状态非常困难。在本工作中，我们首先介绍了一种新颖的重建技术，该技术可以从多个最先进的LLMs的隐藏状态中几乎完美地恢复原始提示。然后我们表明，我们攻击的扩展几乎完美地有效于反转LLMs的排列后的隐藏状态，证明了三种最近提出的隐私方案的不安全性。我们进一步剖析了先前关于排列安全性的理论“证明”的不足，这些证明允许我们的攻击成功。我们的发现强调了在隐私保护LLMs推理中进行严格安全分析的重要性。|
|**2025-05-23**|**NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache**|Donghyun Son et.al.|[2505.18231](http://arxiv.org/abs/2505.18231)|null|大型语言模型（LLM）推理通常需要大量内存，尤其是在处理大型批次和长序列时，这是由于键值（KV）缓存的大小所致。向量量化（VQ）最近被采用以减轻这一问题，但我们发现现有的方法由于依赖于校准数据集而容易受到分布偏移的影响。为了解决这一限制，我们引入了NSNQuant，这是一种无校准的向量量化（VQ）技术，旨在对KV缓存进行低比特压缩。通过应用三个步骤的变换——1）标记归一化（Normalize）、2）通道中心化（Shift）和3）第二次标记归一化（Normalize）——使用哈达玛变换，NSNQuant有效地将标记分布与标准正态分布对齐。这种对齐使得使用单个可重用码簿进行鲁棒的、无校准的向量量化成为可能。大量的实验表明，NSNQuant在1比特和2比特设置下都持续优于先前的方法，提供了强大的泛化能力，并且比全精度基线提供高达3倍的吞吐量提升。|
|**2025-05-22**|**CASTILLO: Characterizing Response Length Distributions of Large Language Models**|Daniel F. Perez-Ramirez et.al.|[2505.16881](http://arxiv.org/abs/2505.16881)|**[link](https://github.com/danielfperez/castillo)**|**高效管理大型语言模型（LLM）推理的算力资源仍然具有挑战性，这是由于自回归文本生成的固有随机性和可变长度。预先准确估计响应长度可以启用主动资源分配，但现有方法要么使文本生成偏向于特定长度，要么依赖于忽略模型和提示特定变异性的假设。我们引入了CASTILLO，这是一个数据集，它描述了13个广泛使用的开源LLM在七个不同的指令遵循语料库上评估时的响应长度分布。对于每个<prompt, 模型>样本对，我们使用固定的解码超参数生成10个独立的补全，记录每个响应的标记长度，并发布汇总统计（均值、标准差、百分位数），以及最短和最长的补全，以及确切的生成设置。我们的分析显示，在相同的生成设置下，响应长度存在显著的模型间和模型内变异，以及特定于模型的行为和仅在部分响应中发生的文本部分退化现象。CASTILLO使得开发预测模型以实现主动调度成为可能，并为分析特定于模型的生成行为提供了一个系统框架。我们公开发布数据集和代码，以促进生成语言建模与系统交叉领域的 研究。**|
|**2025-05-22**|**Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization**|Vera Neplenbroek et.al.|[2505.16467](http://arxiv.org/abs/2505.16467)|**[link](https://github.com/veranep/implicit-personalization-stereotypes)**|**生成式大型语言模型（LLMs）从对话中的细微线索中推断用户的人口统计学信息——这种现象称为隐式个性化。先前的研究表明，这种推断可能导致假设来自少数群体的用户获得的质量较低的回应，即使没有明确提供人口统计学信息。在这项工作中，我们系统地探索了LLMs如何通过控制合成的对话对典型线索做出反应，通过分析模型的潜在用户表示来做到这一点，这包括模型内部和针对特定用户问题的生成答案。我们的发现揭示，LLMs确实根据这些典型信号推断人口统计学属性，对于许多群体来说，即使用户明确表示属于不同的群体，这种推断仍然存在。最后，我们表明，通过使用训练好的线性探针干预模型的内部表示，可以有效地减轻这种由刻板印象驱动的隐式个性化。我们的结果强调了在LLMs如何表示用户身份方面需要更高的透明度和控制。**|
|**2025-05-22**|**QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design**|Benjamin Schneider et.al.|[2505.16175](http://arxiv.org/abs/2505.16175)|**[link](https://github.com/tiger-ai-lab/quickvideo)**|**长视频理解已成为视频监控、会议摘要、教育讲座分析和体育广播等实际应用中的关键能力。然而，对于视频LLMs（大型语言模型）来说，它仍然在计算上难以承受，主要因为两个瓶颈：1）序列视频解码，将原始比特流转换为RGB帧的过程，对于长达一小时的视频输入，可能需要花费多达一分钟的时间；2）LLM推理中昂贵的预填充，需要预填充高达数百万个token，导致高延迟和内存使用。为了解决这些挑战，我们提出了QuickVideo，这是一个系统-算法协同设计，大幅加速长视频理解，以支持实时下游应用。它包括三个关键创新：QuickDecoder，一个基于CPU的并行视频解码器，通过将视频分割成关键帧对齐的区间并并行处理，实现2-3倍的速度提升；QuickPrefill，一种内存高效的预填充方法，使用KV缓存剪枝以支持使用更少的GPU内存来处理更多帧；以及一个重叠方案，将CPU视频解码与GPU推理重叠。这些组件共同将长视频输入的推理时间减少一分钟，即使在有限的硬件上也能实现可扩展、高质量的视频理解。实验表明，QuickVideo在时长和采样率上具有通用性，使得实际中的长视频处理成为可能。**|
|**2025-05-22**|**KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization**|Mingbo Song et.al.|[2505.16162](http://arxiv.org/abs/2505.16162)|null|推测解码（SD）已成为一种广泛使用的范式，能够在不损害生成质量的前提下加速大型语言模型（LLMs）的推理。它通过使用紧凑模型高效地生成多个标记，然后利用目标LLM并行验证这些标记来实现。值得注意的是，自推测解码（Self-Speculative Decoding）提出跳过某些层来构建草案模型，从而消除了额外参数或训练的需求。尽管这种方法有其优势，但我们在本研究中发现，跳过层进行草案构建对领域变化表现出显著的敏感性，导致加速性能大幅下降。为了提高该范式的领域泛化能力，我们引入了KNN-SSD算法，该算法利用K-最近邻（KNN）搜索将不同的跳过层与各种领域输入相匹配。我们在多个模型和多个任务中评估了我们的算法，发现其应用使得LLMs推理速度提高了1.3倍至1.6倍。|
|**2025-05-22**|**RAP: Runtime-Adaptive Pruning for LLM Inference**|Huanrong Liu et.al.|[2505.17138](http://arxiv.org/abs/2505.17138)|null|大型语言模型（LLMs）在语言理解和生成方面表现出色，但它们巨大的计算和内存需求阻碍了其部署。压缩为缓解这些限制提供了一种潜在的解决方案。然而，大多数现有方法依赖于固定的启发式规则，因此无法适应运行时内存变化或来自不同用户请求的异构KV缓存需求。为了解决这些局限性，我们提出了RAP，这是一个由强化学习（RL）驱动的弹性剪枝框架，能够在运行时感知的方式下动态调整压缩策略。具体来说，RAP会动态跟踪模型参数与KV缓存之间演变的比率，在实用执行过程中进行调整。认识到FFNs（前馈神经网络）包含大多数参数，而参数轻量化的注意力层主导KV缓存的构建，强化学习代理仅保留那些在当前内存预算下最大化效用且符合瞬时工作负载和设备状态的组件。大量的实验结果表明，RAP优于最先进的基础方案，这是首次在运行时同时考虑模型权重和KV缓存。|
|**2025-05-20**|**ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions**|Bufang Yang et.al.|[2505.14668](http://arxiv.org/abs/2505.14668)|null|近期，大型语言模型（LLMs）的进展推动了智能代理从被动响应向主动支持的发展。虽然前景广阔，但现有的主动代理要么完全依赖于封闭环境中的观察（例如，桌面UI）和直接的LLM推理，要么采用基于规则的主动通知，这导致对用户意图的理解不够优化，以及主动服务的功能有限。在本文中，我们介绍了ContextAgent，这是第一个结合广泛感官上下文以增强LLM代理主动能力的上下文感知主动代理。ContextAgent首先从可穿戴设备上的大量感官感知（例如，视频和音频）中提取多维上下文，以理解用户意图。然后，ContextAgent利用感官上下文和历史数据中的人物上下文来预测主动服务的必要性。当需要主动协助时，ContextAgent进一步自动调用必要的工具以不干扰用户的方式协助。为了评估这一新任务，我们编制了ContextAgentBench，这是第一个用于评估上下文感知主动LLM代理的基准，涵盖九种日常场景和二十种工具，共1,000个样本。在ContextAgentBench上的实验表明，ContextAgent在主动预测和工具调用方面的准确率分别比基线提高了高达8.5%和6.0%。我们希望我们的研究能够启发更先进、以人为中心的主动人工智能助手的开发。|
|**2025-05-20**|**ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs**|Yifan Sui et.al.|[2505.14468](http://arxiv.org/abs/2505.14468)|null|服务器无服务器计算因按使用付费的定价、细粒度GPU使用和快速扩展而迅速增长，用于服务大型语言模型（LLM）的推理。然而，我们的分析揭示，当前的无服务器计算能够有效地服务通用LLM，但在处理低秩适应性（LoRA）推理时却因为三个关键限制而失败：1）函数之间存在大量的参数冗余，其中99%的权重是不必要地重复的；2）超出LLM加载的昂贵工件加载延迟；3）在服务多个LoRA LLM时资源争用加剧。这些低效性导致大量GPU浪费、时间至首个标记（TTFT）增加和货币成本高。我们提出了ServerlessLoRA，这是一种专为快速且低成本LoRA LLM服务而设计的创新无服务器推理系统。ServerlessLoRA允许在隔离的LoRA函数之间安全共享主骨干LLM以减少冗余。我们设计了一种预加载方法，预先加载全面LoRA工件以最小化冷启动延迟。此外，ServerlessLoRA采用争用感知批处理和卸载来缓解突发负载期间的GPU资源冲突。在工业工作量上的实验表明，与最先进的LLM推理解决方案相比，ServerlessLoRA将TTFT减少了高达86%，并将货币成本减少了高达89%。|
|**2025-05-20**|**Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity**|Susav Shrestha et.al.|[2505.14884](http://arxiv.org/abs/2505.14884)|**[link](https://github.com/susavlsh10/polar-sparsity)**|**加速大型语言模型（LLM）的推理对于需要高吞吐量和低延迟的实际部署至关重要。上下文稀疏性，即每个标记动态地仅激活模型参数的小子集，显示出潜力，但由于激活神经元的并集迅速接近密集计算，因此无法扩展到大型批量大小。我们引入了极化稀疏性，强调了随着批量大小和序列长度的增加，稀疏性重要性从MLP层转移到注意力层的转变。在批处理下，MLP层变得更加计算高效，但其稀疏性消失。相比之下，在规模上，注意力变得更加昂贵，而其头部稀疏性保持稳定且与批量无关。我们开发了硬件高效的、稀疏性感知的GPU内核，用于选择性的MLP和注意力计算，为OPT、LLaMA-2 & 3等模型在各种批量大小和序列长度上提供高达\(2.2\times\)的端到端加速，而不会影响准确性。据我们所知，这是第一个证明上下文稀疏性可以有效地扩展到大型批量大小的工作，通过最小化更改提供实质性的推理加速，使得极化稀疏性适用于大规模、高吞吐量的LLM部署系统。我们的代码可在以下网址获取：https://github.com/susavlsh10/Polar-Sparsity。**|
|**2025-05-19**|**HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding**|Siran Liu et.al.|[2505.13254](http://arxiv.org/abs/2505.13254)|null|自回归解码，作为大型语言模型（LLM）推理的标准方法，由于其序列性质而成为重要的瓶颈。尽管投机解码算法通过并行验证缓解了这种低效，但它们未能利用语言复杂性的内在异质性，这是导致资源分配不最优的关键因素。我们通过提出HeteroSpec，一个异质性自适应投机解码框架来解决这个问题，该框架根据语言上下文复杂性动态优化计算资源分配。HeteroSpec引入了两个关键机制：（1）一种新的累积元路径Top- $K$熵度指标，用于高效地识别可预测上下文。（2）基于数据驱动的熵划分的动态资源分配策略，能够根据局部上下文难度进行自适应投机扩展和修剪。在五个公共基准和四种模型上进行评估，HeteroSpec实现了平均加速4.26$\times$ 。它在加速率、平均接受长度和验证成本方面一致优于最先进的EAGLE-3。值得注意的是，HeteroSpec无需重新训练草案模型，开销极小，并且与其他加速技术正交。它展示了与更强草案模型相结合的增强加速效果，为上下文感知LLM推理加速建立了一种新范式。|
|**2025-05-19**|**FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference**|Guangda Liu et.al.|[2505.13109](http://arxiv.org/abs/2505.13109)|null|大型语言模型（LLMs）在支持日益增长的应用需求的同时，其上下文窗口也在迅速扩大。然而，长上下文带来了显著的部署挑战，主要是因为KV缓存的容量会随着上下文长度的增加而成比例增长。虽然已经提出了KV缓存压缩方法来解决这个问题，但KV丢弃方法会导致相当大的精度损失，而KV检索方法则存在显著的效率瓶颈。我们提出了FreeKV，一个算法-系统协同优化框架，旨在提高KV检索效率同时保持精度。在算法方面，FreeKV引入了推测检索，将KV选择和召回过程移出关键路径，并结合细粒度校正以确保精度。在系统方面，FreeKV采用CPU和GPU内存上的混合KV布局，以消除碎片化数据传输，并利用双缓冲流式召回进一步提高效率。实验表明，FreeKV在各种场景和模型中实现了接近无损的精度，与最先进的KV检索方法相比，速度提升了高达13倍。|
|**2025-05-19**|**FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks**|Zihua Wang et.al.|[2505.12728](http://arxiv.org/abs/2505.12728)|**[link](https://github.com/zihuaevan/flashsd)**|大型语言和多模态模型（LLMs和LMMs）展现出强大的推理能力，但通常受到较慢解码速度的限制。这一挑战在LMMs中尤为突出，因为视觉输入通常包含比文本更多的标记，但信息密度更低——这一问题由于最近趋向于更精细的视觉标记化以提高性能而加剧。推测性解码通过使用较小的草稿模型生成候选标记，然后由目标模型有选择性地验证，从而有效地加速LLMs的推理，同时在保持输出质量方面取得了进步。尽管这种策略已经扩展到LMMs，但现有方法在很大程度上忽略了视觉输入的独特属性，并完全依赖于基于文本的草稿模型。在本工作中，我们提出了名为FLASH（快速潜知识感知半自回归启发式算法）的推测性解码框架，该框架专门为LMMs设计，利用多模态数据的两个关键属性来设计草稿模型。首先，为了解决视觉标记的冗余问题，我们提出了一种轻量级的潜在感知标记压缩机制。其次，鉴于视觉对象通常在场景中共同出现，我们采用半自回归解码策略，在每个前向传递中生成多个标记。这些创新在加速草稿解码的同时保持了高接受率，从而实现了更快的整体推理。实验表明，与原始LMM相比，FLASH在单模态和多模态环境中均显著优于先前的推测性解码方法，在视频字幕生成任务上实现了高达2.68倍的加速，在视觉指令调整任务上实现了2.55倍的加速。|
|**2025-05-17**|**Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning**|Yuheng Lu et.al.|[2505.11922](http://arxiv.org/abs/2505.11922)|null|大型语言模型（LLMs）在处理自然语言任务方面表现出卓越的能力；然而，它们可能难以持续遵循包括涉及多个约束的复杂指令。使用监督微调（SFT）对训练后的LLMs进行优化是一种标准方法来提高其遵循指令的能力。在解决复杂指令遵循问题时，现有努力主要集中于为SFT合成复杂指令-输出对的基于数据的驱动方法。然而，对关键子上下文投入的注意力不足可能会降低SFT的有效性。在这项工作中，我们提出将顺序结构的输入指令转换为包含子上下文的多个并行指令。为了支持处理这种多输入，我们提出了MISO（多输入单输出），这是目前占主导地位的仅解码器基于transformer的LLMs的扩展。MISO引入了一种混合上下文范式，它联合考虑整体指令-输出对齐和单个子上下文的影响，以增强SFT的有效性。我们将MISO微调应用于复杂指令遵循数据集，并使用标准的LLM推理对其进行评估。实证结果表明，MISO作为LLMs的微调方法具有优越性，无论是在复杂指令遵循场景中的有效性，还是在训练效率方面的潜力。|
|**2025-05-17**|**Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference Architecture**|Yu Wu et.al.|[2505.11916](http://arxiv.org/abs/2505.11916)|null|现有的大型语言模型（LLMs）服务系统通常采用预填充-解码解耦架构来防止预填充和解码阶段之间的计算干扰。然而，现实世界的LLM服务场景往往表现出请求输入/输出长度的显著波动，导致传统的静态预填充/解码节点配置比例在这两个节点之间产生不均衡的计算负载，从而阻碍了计算资源的有效利用，无法提高系统的吞吐量。为了应对这一挑战，我们设计并实现了Arrow，一个自适应调度器，该调度器利用无状态实例和弹性实例池来实现高效的自适应请求和实例调度。Arrow根据实时集群性能指标动态调整处理预填充和解码任务的实例数量，显著提高了系统处理流量峰值和负载变化的能力。我们的评估在不同现实世界的工作负载下表明，与最先进的PD-同地放置和PD-解耦服务系统相比，Arrow实现了高达5.62倍和7.78倍的请求服务率。|
|**2025-05-16**|**TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference**|Raja Gond et.al.|[2505.11329](http://arxiv.org/abs/2505.11329)|**[link](https://github.com/microsoft/tokenweave)**|分布式大语言模型（LLMs）推理可能引入高达20%的开销，即使是在通过高速互连（如NVLINK）连接的GPU上。已经提出了多种技术，通过将计算分解为更细粒度的任务，并在子任务完成时重叠通信来减轻这些开销。然而，将大计算细粒度分解为GPU上的多个较小计算会导致开销。此外，通信本身使用了多个流式多处理器（SMs），这增加了开销。我们提出了TokenWeave来解决这些挑战。TokenWeave提出了一种Token-Splitting技术，以波纹感知的方式将推理批次中的标记分为两个大致相等的子集。然后，其中一个子集的计算与另一个子集的通信重叠。此外，TokenWeave优化了层归一化计算的顺序，使其相对于通信操作更优，并实现了一个新的融合AllReduce-RMSNorm内核，该内核小心地利用了NVIDIA Hopper GPU上可用的Multimem指令支持。这些优化使得TokenWeave能够在仅使用2-8个SMs的情况下执行通信和RMSNorm。此外，我们的内核使得内存受限的RMSNorm可以与另一个批次的计算重叠，从而提供额外的收益。我们的评估表明，在多个模型和工作负载中，TokenWeave的延迟减少了高达29%，吞吐量提高了高达26%。在几个设置中，TokenWeave的性能优于移除所有通信的等效模型。|
|**2025-05-16**|**Vaiage: A Multi-Agent Solution to Personalized Travel Planning**|Binwen Liu et.al.|[2505.10922](http://arxiv.org/abs/2505.10922)|null|规划旅行是一项认知密集型任务，涉及冲突的用户偏好、动态的外部信息和多步骤的时间-空间优化。传统平台往往无法满足需求——它们提供静态结果，缺乏情境适应，且无法支持实时交互或意图细化。我们的方法Vaiage通过构建在大型语言模型（LLMs）周围的图结构多智能体框架来应对这些挑战，这些LLMs既作为目标条件推荐者，也作为顺序规划者。LLMs推断用户意图，建议个性化的目的地和活动，并综合符合情境约束（如预算、时间、团体规模和天气）的行程。通过自然语言交互、结构化工具使用和基于地图的反馈循环，Vaiage实现了基于符号推理和对话理解的适应性、可解释性和端到端旅行规划。为了评估Vaiage，我们进行了人机交互实验，使用基于评分标准的GPT-4评估和定性反馈。整个系统获得了平均8.5分（满分10分），超过了无策略（7.2分）和无外部API（6.8分）的变体，特别是在可行性方面。定性分析表明，智能体协调——尤其是策略和信息智能体——通过优化时间使用和整合实时情境，显著提高了行程质量。这些结果表明，将LLM推理与符号智能体协调相结合，在开放性、现实世界的规划任务中是有效的。|
|**2025-05-16**|**An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents**|Ayesha Amjad et.al.|[2505.13504](http://arxiv.org/abs/2505.13504)|null|从发票、采购订单、账单和财务文档等类似表单的文件中提取字母数字数据通常是通过视觉（OCR）和学习算法或具有有限系统改进潜力的单体管道完成的。我们提出了一种代理人工智能系统，该系统利用大型语言模型（LLM）代理和强化学习（RL）驱动代理，在LLM推理不确定性下实现一致性和自我改进的提取。我们的工作突出了基于单体LLM提取的局限性，并引入了一个模块化、多代理框架，具有特定于任务的提示和一个奖励和惩罚的RL策略，以指导元提示代理从过去的错误中学习并改进基于提示的演员代理。这个自我纠正的自适应系统可以处理各种文档、文件格式、布局和LLM，旨在自动化准确的信息提取，而无需人工干预。在SOIRE和CORD的两个基准数据集上报告的结果对代理人工智能框架很有希望。|
|**2025-05-15**|**SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices**|Xiangwen Zhuge et.al.|[2505.10259](http://arxiv.org/abs/2505.10259)|**[link](https://github.com/mobisense/specoffload-public)**|在资源受限的设备上高效执行大型语言模型（LLM）推理面临着计算和内存利用的巨大挑战。由于GPU内存有限，现有系统将模型权重卸载到CPU内存，导致CPU和GPU之间产生巨大的I/O开销。这导致两个主要的不效率：一是GPU核心利用率低，常常在等待数据加载时处于空闲状态；二是GPU内存对性能影响较小，因为减少其容量对整体吞吐量影响微乎其微。在本文中，我们提出了SpecOffload，这是一种高吞吐量的推理引擎，它将推测性解码嵌入到卸载过程中。我们的关键思想是解锁潜在的GPU资源，用于存储和执行用于推测性解码的草稿模型，从而以近乎零的额外成本加速推理。为此，我们在卸载管道中仔细编排目标模型和草稿模型在推测性解码中的交错执行，并提出一个规划器来管理张量放置和选择最佳参数。与最佳基线相比，SpecOffload将GPU核心利用率提高了4.49倍，并将推理吞吐量提高了2.54倍。我们的代码可在https://github.com/MobiSense/SpecOffload 上找到。|
|**2025-05-15**|**ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production**|Yuxing Xiang et.al.|[2505.09999](http://arxiv.org/abs/2505.09999)|**[link](https://github.com/alibaba/servegen)**|随着大型语言模型（LLMs）的广泛应用，处理LLMs推理请求已成为一项日益重要的任务，吸引了积极的研发进展。实际工作负载在这个过程中扮演着关键角色：它们对于激励和基准测试服务技术和系统至关重要。然而，由于缺乏全面的工作负载特征描述，对现实世界LLM服务工作负载的理解仍然有限。先前分析在规模和范围上不足，因此未能充分捕捉到复杂的工作负载特征。在本文中，我们通过深入分析从我们的全球云推理服务收集到的LLM服务工作负载来填补这一空白，不仅涵盖语言模型，还包括新兴的多模态和推理模型，并在每种情况下揭示重要的新发现。此外，基于我们的发现，我们提出了ServeGen，这是一个基于每个客户端组合生成真实LLM服务工作负载的原理框架。一个实际的生产用例验证了与简单的工作负载生成相比，ServeGen避免了50%的低配置，展示了ServeGen在性能基准测试中的优势。我们将开源ServeGen以促进未来的研究。|
|**2025-05-14**|**How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference**|Nidhal Jegham et.al.|[2505.09598](http://arxiv.org/abs/2505.09598)|null|本文介绍了一种新颖的基础设施感知基准测试框架，用于量化30个最先进的大型语言模型在商业数据中心部署过程中的环境影响。我们的框架将公共API性能数据与特定地区的环境乘数和硬件配置的统计推断相结合。此外，我们还利用跨效率数据包络分析（DEA）根据性能相对于环境成本对模型进行排名。我们的结果表明，o3和DeepSeek-R1成为能耗最高的模型，每个长提示消耗超过33 Wh，是GPT-4.1 nano的70多倍，而Claude-3.7 Sonnet在生态效率方面排名第一。尽管单个简短的GPT-4o查询消耗0.43 Wh，但如果将其扩展到每天7000万次查询，则会导致重大的年度环境影响。这些包括与35,000个美国家庭相当的电力消耗、与120万人年度饮用水需求相当的淡水蒸发，以及需要芝加哥大小的森林来抵消的碳排放。这些发现揭示了一个日益突出的悖论：尽管人工智能正在变得便宜和快速，但其全球普及推动了不成比例的资源消耗。我们的研究为基准测试LLM部署的可持续性提供了一种标准化的、基于经验的 metodology，为人工智能开发中的未来环境问责制和可持续性标准奠定了基础。|
|**2025-05-14**|**Statistical Modeling and Uncertainty Estimation of LLM Inference Systems**|Kaustabha Ray et.al.|[2505.09319](http://arxiv.org/abs/2505.09319)|null|由于动态的工作负载变化、多样的硬件架构以及模型大小、批量处理和吞吐量需求之间的复杂交互，大型语言模型（LLM）推理系统在统计性能描述方面面临着重大挑战。准确的统计描述能够实现更好的工作负载调度、自适应资源分配和成本感知的推理优化，这对于提高大规模AI部署的效率至关重要。传统的分析模型提供了可解释性，但不能涵盖现实世界中工作负载的广泛多样性，使得无法预先为每个场景进行基准测试。机器学习（ML）方法能够有效地预测非基准测试案例的性能，但在超出其观察到的训练空间时表现不佳。为了解决LLM推理系统中的这些限制，我们提出了一种分析学习增强（ALA）框架，该框架将分析建模与ML相结合，以实现LLM推理工作负载中的稳健统计预测和不确定性估计。我们的方法采用了一个分析吞吐量模型，其参数针对基准测试工作负载进行估计，然后使用ML预测扩展到未观察到的配置。我们通过模拟退火利用工作负载数据点组合的子集，并开发了一个错误预测器来增强这种方法。最后，我们根据新工作负载和观察到的工作负载之间的向量空间相似度来量化不确定性，以确保稳健的泛化。通过对各种LLM推理工作负载的大量实验，我们证明了我们的框架在保持对新推理场景的适应性同时，实现了低中值误差。|
|**2025-05-14**|**ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor**|Seungbeom Choi et.al.|[2505.09142](http://arxiv.org/abs/2505.09142)|null|我们提出了ELIS，这是一个用于大型语言模型（LLM）的服务系统，它具有一个迭代最短剩余时间优先（ISRTF）调度器，旨在高效地管理具有最短剩余标记的推理任务。当前的LLM服务系统通常采用先来先服务的调度策略，这可能导致“队首阻塞”问题。为了克服这一局限性，需要预测LLM的推理时间并应用最短作业优先的调度策略。然而，由于LLM的自回归性质，预测推理延迟具有挑战性。ELIS通过使用基于编码器的最先进模型BGE来训练LLM的响应长度预测器，解决了这一挑战。此外，我们还设计了ISRTF调度策略，这是一种针对现有LLM迭代批量的最短剩余时间优先的优化策略。为了在工业环境中评估我们的工作，我们根据对真实世界用户LLM服务跟踪记录的研究，模拟了请求流。此外，我们将ELIS作为基于Kubernetes的云原生调度系统实现，以评估其在生产环境中的性能。我们的实验结果表明，ISRTF将平均作业完成时间减少了高达19.6%。|
|**2025-05-13**|**Automatic Task Detection and Heterogeneous LLM Speculative Decoding**|Danying Ge et.al.|[2505.08600](http://arxiv.org/abs/2505.08600)|null|推测解码，将草稿模型与目标模型相结合，已成为加速大型语言模型（LLM）推理的有效方法。然而，由于草稿模型的容量有限，现有方法在下游任务中常常面临接受率与解码速度之间的权衡，这使得确保跨不同任务的效率变得困难。为了解决这个问题，我们提出了一种针对下游任务优化的推测解码算法。它包括一个自动任务划分和分配方法，该方法自动将下游任务分类为不同的子任务，并将它们分配给一组异构的草稿模型。每个草稿模型使用特定任务的数据与目标模型对齐，从而增强了推理结果的一致性。此外，我们提出的方法还包含一个在线轻量级提示分类器，以动态地将提示路由到适当的草稿模型。实验结果表明，与传统的推测解码相比，所提出的方法将草稿准确率提高了6%至50%，同时在LLM推理中实现了1.10倍至2.64倍的加速。|
|**2025-05-13**|**LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries**|Zekun Wu et.al.|[2505.08842](http://arxiv.org/abs/2505.08842)|null|开源AI库是现代AI系统的基石，但在安全性、许可、维护、供应链完整性和合规性方面存在重大的、未被充分研究的风险。我们提出了LibVulnWatch，这是一个基于图的代理评估框架，对这类库进行深入、基于源代码的评价。该系统建立在LangGraph之上，通过协调一个专门代理的有向无环图，从可信来源（如仓库、文档和漏洞数据库）提取、验证和量化风险。LibVulnWatch在五个关键领域生成可重复、符合治理要求的评分，并将这些评分发布到公共排行榜，以进行纵向生态系统监控。应用于包括机器学习框架、大型语言模型推理引擎和代理编排工具在内的20个广泛使用的库，我们的系统涵盖了OpenSSF Scorecard检查的最多88%，并每个库发现多达19个额外风险。这些风险包括关键的远程代码执行（RCE）漏洞、缺失的软件物料清单（SBOMs）、许可限制、未记录的遥测数据，以及在监管文档和可审计性方面的普遍差距。通过将高级治理原则转化为实际可验证的指标，LibVulnWatch通过一个可扩展、透明的机制推进了技术AI治理，以进行持续供应链风险评估和有信息的库选择。|
|**2025-05-12**|**SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models**|Hang Wu et.al.|[2505.07680](http://arxiv.org/abs/2505.07680)|null|大型语言模型（LLMs）在推理质量与计算成本之间存在着关键的权衡：较大的模型提供更高级的能力，但会带来显著的延迟，而较小的模型则更快但功能较弱。现有的服务策略通常采用固定的模型规模或静态的两阶段推测解码，无法动态适应用户请求的复杂性和系统性能的波动。本文介绍了\systemname{}，一个新颖的框架，它将LLM推理重新构想为通过多级推测解码解决的适应性路由问题。\systemname{}根据实时反馈动态构建和优化推理“路径”（模型链），解决静态方法的局限性。我们的贡献有三方面：（1）一个利用性能分析（执行时间）和预测相似度指标（来自标记分布差异）的\textbf{自适应模型链调度}机制，不断选择最优的草稿和验证器模型的序列，最小化每个生成的标记的预测延迟。（2）一个\textbf{多级协作验证}框架，其中所选链中的中间模型可以验证推测标记，减轻最终、最强大的目标模型的验证负担。（3）一个提供高效、一致性的KV缓存处理的\textbf{同步状态管理系统}，包括针对多级推测中固有的异步批量处理的精确、低开销回滚。初步实验证明了我们方法的有效性。|
|**2025-05-12**|**Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity**|Guang Yan et.al.|[2505.07239](http://arxiv.org/abs/2505.07239)|null|随着云计算平台上大型语言模型（LLMs）提供的推理服务的广泛应用，对敏感信息泄露的隐私担忧日益加剧。安全多方计算（MPC）是保护LLM推理隐私的一个有希望的解决方案。然而，MPC需要频繁的服务器间通信，导致高性能开销。受LLMs中普遍存在的激活稀疏性启发，即大部分神经元在非线性激活函数后不会激活，我们提出了一种高效的私有推理系统Comet。该系统采用了一个准确且快速的预测器来预测激活函数输出的稀疏分布。此外，我们引入了一种新的私有推理协议。它通过利用预测稀疏分布的空间局部性，高效且安全地避免了涉及零值计算。尽管这种避免计算的方法会影响KV缓存条目的时空连续性，但我们通过低通信开销的缓存填充策略来解决这个问题，该策略合并了缺失请求并集成了预取机制。最后，我们在四个常见的LLMs上评估了Comet，并将其与六个最先进的私有推理系统进行了比较。Comet实现了1.87倍至2.63倍的速度提升和1.94倍至2.64倍的通信减少。|
|**2025-05-12**|**PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications**|Kuntai Du et.al.|[2505.07203](http://arxiv.org/abs/2505.07203)|null|除了典型的生成式应用，如ChatGPT、GitHub Copilot和Cursor之外，我们观察到一种新兴趋势，即大型语言模型（LLM）越来越多地被用于传统的判别性任务，例如推荐、信用验证和数据标注。这些新兴用例的关键特征是LLM仅生成单个输出标记，而不是任意长度的标记序列。我们称这种工作负载为仅预填充工作负载。然而，由于现有的LLM引擎假设输出长度是任意的，它们未能利用仅预填充工作负载的独特属性。在本文中，我们提出了PrefillOnly，这是第一个通过完全拥抱仅预填充工作负载的属性来提高推理吞吐量和延迟的LLM推理引擎。首先，由于它只生成一个标记，PrefillOnly只需要存储最后计算层的KV缓存，而不是所有层的缓存。这极大地减少了LLM推理的GPU内存占用，并允许处理长输入而无需使用降低吞吐量的解决方案，如跨GPU KV缓存并行化。其次，由于输出长度是固定的，而不是任意的，PrefillOnly可以在开始之前精确地确定每个仅预填充请求的作业完成时间（JCT）。这使能够高效地采用JCT感知的调度策略，如最短剩余作业优先。PrefillOnly每秒可以处理多达4倍的查询量，而不会增加平均和P99延迟。|
|**2025-05-10**|**I Know What You Said: Unveiling Hardware Cache Side-Channels in Local Large Language Model Inference**|Zibo Gao et.al.|[2505.06738](http://arxiv.org/abs/2505.06738)|null|大型本地语言模型（LLMs）近年来在涉及隐私敏感任务的部署中越来越受欢迎，Meta、谷歌和英特尔等公司在其发展中扮演了重要角色。然而，从硬件缓存侧信道的角度来看，本地LLMs的安全性尚未被探索。在本文中，我们揭示了本地LLMs推理中新的侧信道漏洞：标记值和标记位置泄露，这些泄露可以暴露受害者的输入和输出文本，从而损害用户隐私。具体来说，我们发现攻击者可以从标记嵌入操作的缓存访问模式中推断标记值，并从自回归解码阶段的时序中推断标记位置。为了展示这些泄露的潜力，我们设计了一个针对开源和专有LLMs推理系统的新的监听攻击框架。该攻击框架不直接与受害者的LLM交互，并且可以在无特权的情况下执行。我们对一系列实际本地LLMs部署（例如Llama、Falcon和Gemma）进行了攻击评估，结果表明我们的攻击实现了令人满意的准确性。恢复的输出和输入文本分别与真实值平均编辑距离为5.2%和17.3%。此外，重建的文本在输入和输出方面的平均余弦相似度得分分别为98.7%（输入）和98.0%（输出）。|
|**2025-05-09**|**Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM**|Zehao Fan et.al.|[2505.05772](http://arxiv.org/abs/2505.05772)|null|基于Transformer的模型是现代机器学习的基础，但它们的执行，特别是在大型语言模型（LLMs）的自动回归解码过程中，由于频繁的内存访问和不断增长的键值（KV）缓存，对内存系统施加了巨大的压力。这导致内存带宽瓶颈，尤其是在上下文长度增加时。内存中处理（PIM）架构是一种有希望的解决方案，它提供了接近内存的高内部带宽和计算并行性。然而，当前的PIM设计主要针对密集注意力优化，难以处理现代KV缓存稀疏技术引入的动态、不规则访问模式。因此，它们存在工作负载不平衡的问题，降低了吞吐量和资源利用率。在这项工作中，我们提出了STARC，这是一种针对PIM架构上高效LLM解码的稀疏优化数据映射方案。STARC通过语义相似性对KV对进行聚类，并将它们映射到与PIM银行结构对齐的连续内存区域。在解码过程中，查询通过匹配预计算的质心来检索相关标记，以集群粒度进行选择性注意和并行处理，而无需频繁重新聚类或数据移动开销。在HBM-PIM系统上的实验表明，与常见的按标记稀疏方法相比，STARC将注意力层延迟降低了19%至31%，能耗降低了19%至27%。在KV缓存预算为1024的情况下，与完全检索KV缓存相比，它实现了高达54%至74%的延迟降低和45%至67%的能耗降低。同时，STARC保持了与最先进的稀疏注意力方法相当的模式精度，证明了它在PIM架构上实现高效和硬件友好的长上下文LLM推理的有效性。|
|**2025-05-09**|**Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference**|Haolin Zhang et.al.|[2505.06461](http://arxiv.org/abs/2505.06461)|null|在设备上的人工智能领域，普遍认为GPU凭借其卓越的并行处理能力，总是为大型语言模型（LLM）推理提供最佳性能。在这项工作中，我们通过实证研究挑战了这一观点，表明在特定条件下，CPU在移动设备上的LLM推理性能可以超越GPU。我们使用在iPhone 15 Pro上通过llama.cpp部署的10亿参数LLM，展示仅使用CPU配置（两个线程，F16精度）每秒可以处理17个标记，超过了使用GPU加速获得的每秒12.8个标记。我们分析了导致这一反直觉结果的架构因素，揭示GPU内存传输开销和CPU线程优化起着关键作用。此外，我们还探讨了线程过载订阅、量化策略和硬件约束的影响，为高效的设备上人工智能执行提供了新的见解。我们的发现挑战了传统的GPU优先思维，突出了优化CPU推理的潜力，并为移动人工智能中的更智能部署策略铺平了道路。然而，由于iOS上对底层分析工具的访问有限，完全解释观察到的CPU优势仍然困难。|
|**2025-05-08**|**HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic Text-to-SQL Workflow**|You Peng et.al.|[2505.05286](http://arxiv.org/abs/2505.05286)|**[link](https://github.com/relaxed-system-lab/hexgen-flow)**|**近年来，利用大型语言模型（LLM）的代理范式在文本到SQL能力方面的进展显著提高，使得没有专业数据库知识的用户能够直观地查询数据。然而，将基于代理的LLM文本到SQL系统部署到生产环境中面临着重大挑战，因为这些系统固有的多阶段工作流程、严格的延迟约束以及企业环境中可能异构的GPU基础设施。当前的LLM服务框架缺乏有效机制来处理相互依赖的推理任务、动态延迟变化和资源异构性，导致性能不佳和频繁的服务级别目标（SLO）违反。在本文中，我们介绍了HEXGEN-TEXT2SQL，这是一个专为在异构GPU集群上调度和执行基于代理的LLM多阶段文本到SQL工作流程而设计的框架，该框架处理多租户端到端查询。HEXGEN-TEXT2SQL引入了一种分层调度方法，结合全局工作负载平衡的任务调度和局部自适应紧急优先级，该方法由对代理文本到SQL工作流程的系统分析指导。此外，我们提出了一种基于轻量级模拟的方法来调整关键调度超参数，进一步增强了鲁棒性和适应性。我们对现实文本到SQL基准的大量评估表明，HEXGEN-TEXT2SQL在性能上显著优于最先进的LLM服务框架。具体来说，与vLLM相比，HEXGEN-TEXT2SQL将延迟截止时间减少了高达1.67倍（平均：1.41倍），并将系统吞吐量提高了高达1.75倍（平均：1.65倍），在多样化的现实工作负载条件下表现出色。我们的代码可在https://github.com/Relaxed-System-Lab/Hexgen-Flow上找到。**|
|**2025-05-08**|**Scaling Laws for Speculative Decoding**|Siyuan Yan et.al.|[2505.07858](http://arxiv.org/abs/2505.07858)|null|随着对大型语言模型（LLMs）高效解码需求的不断增长，对于像OpenAI-o3和DeepSeek-R1这样的推理密集型架构来说，这一点尤其关键，因为这些架构依赖于扩展的思考链推理。本研究通过密集的LLM架构来探讨推测性解码技术，以建立加速推理任务的基石性见解。虽然利用并行草案验证周期的推测性解码方法已成为有希望的加速技术，但与通过预训练->SFT->RLHF训练范式开发的传统骨干LLMs相比，支配解码效率的缩放定律仍被低估。在本研究中，我们发现了控制草案模型接受率（或解码速度）的Log线性缩放定律（定理1.1、1.2和1.3），该定律涵盖了三个维度：预训练令牌量、草案模型容量和解码批次大小。基于这些定律，我们实现了Scylla，该系统协调了流行LLMs（Llama2/3、Qwen2.5）的多维缩放。实证验证表明，Scylla在温度T=0时比EAGLE2的接受率高出1.5-2.2，比EAGLE3高出0.3，在摘要和问答任务上取得了峰值性能提升（图2）。工业推理引擎部署显示出比EAGLE2高2倍的解码吞吐量提升（表5），验证了系统化缩放对高效LLM推理的变革潜力。代码将在稍后发布。|
|**2025-05-06**|**Faster MoE LLM Inference for Extremely Large Models**|Haoqi Yang et.al.|[2505.03531](http://arxiv.org/abs/2505.03531)|null|稀疏混合专家（MoE）大型语言模型（LLMs）正逐渐成为超大规模模型的主流方法。现有的MoE模型优化工作主要集中在粗粒度MoE架构上。随着DeepSeek模型的兴起，细粒度MoE模型越来越受欢迎，但对其的研究仍然有限。因此，我们想讨论不同服务负载下的效率动态。此外，细粒度模型允许部署者减少路由专家的数量，包括激活数量和总数，从而引发了一个问题：这种减少如何影响MoE效率和性能之间的权衡。我们的研究发现，虽然部署MoE模型面临更大的挑战，但也提供了显著的优化机会。在某些场景下，减少激活专家的数量可以带来显著的效率提升，同时性能下降很小。减少专家总数只能带来有限的效率提升，但会导致严重的性能下降。我们的方法可以在不降低性能的情况下将吞吐量至少提高10%。总的来说，我们得出结论，MoE推理优化仍然是一个具有巨大探索和改进潜力的领域。|
|**2025-05-05**|**RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference**|Yaoqi Chen et.al.|[2505.02922](http://arxiv.org/abs/2505.02922)|null|随着大型语言模型（LLM）的上下文长度不断增加，高效的推理面临着重大挑战，这主要归因于GPU内存和带宽的限制。我们提出了一种名为RetroInfer的新系统，它将键值（KV）缓存重新概念化为向量存储系统，该系统利用固有的注意力稀疏性来加速长上下文LLM的推理。其核心是波索引，这是一种注意力感知的向量索引，它通过三分注意力近似、精度受限的注意力估计和分段聚类等技术，能够高效准确地检索关键标记。与之相辅相成的是波缓冲区，它协调KV缓存的放置，并在GPU和CPU之间重叠计算和数据传输，以维持高吞吐量。与之前基于稀疏性的方法相比，这些方法在标记选择和硬件协调方面存在困难，而RetroInfer在保持模型准确性的同时，提供了稳健的性能。在长上下文基准测试中，当KV缓存扩展到CPU内存时，与全注意力相比，RetroInfer在GPU内存限制内实现了高达4.5倍的速度提升，与稀疏注意力基线相比，实现了高达10.5倍的速度提升，同时保持了全注意力级别的准确性。|
|**2025-05-03**|**High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers**|Brian Wong et.al.|[2505.01693](http://arxiv.org/abs/2505.01693)|null|自动标注胸部X光报告对于实现下游任务，如基于图像的诊断模型训练、人群健康研究和临床决策支持至关重要。然而，这些自由文本报告中的高变异性、复杂性和否定以及不确定性普遍存在，给传统的自然语言处理方法带来了重大挑战。虽然大型语言模型（LLMs）展示了强大的文本理解能力，但它们直接应用于大规模、高效的标注受到计算成本和速度的限制。本文介绍了一种名为DeBERTa-RAD的新型两阶段框架，该框架结合了最先进的LLM伪标注与基于DeBERTa的知识蒸馏，以实现准确和快速的胸部X光报告标注。我们利用先进的LLM为大量报告生成高质量的伪标签，包括确定性状态。随后，使用定制化的知识蒸馏策略，在DeBERTa-Base模型上对这些伪标签数据进行训练。在专家标注的MIMIC-500基准上评估，DeBERTa-RAD实现了0.9120的宏观F1分数，显著优于现有的基于规则的系统、微调的transformer模型和直接LLM推理，同时保持了适用于高吞吐量应用的实用推理速度。我们的分析显示，在处理不确定发现方面具有特别的优势。这项工作展示了通过战略性地结合LLM能力和通过蒸馏训练的效率较高的学生模型，克服数据标注瓶颈并实现高性能医疗文本处理的可行途径。|
|**2025-05-03**|**A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency**|Sihyeong Park et.al.|[2505.01658](http://arxiv.org/abs/2505.01658)|**[link](https://github.com/sihyeong/awesome-llm-inference-engine)**|**大型语言模型（LLMs）被广泛应用于聊天机器人、代码生成器和搜索引擎中。诸如思维链、复杂推理和代理服务等工作负载，通过重复调用模型，显著增加了推理成本。为了降低成本，已经采用了并行化、压缩和缓存等优化方法，但由于服务需求的多样性，选择合适的方法变得困难。最近，专门化的LLM推理引擎作为将优化方法集成到面向服务的基础设施中的关键组件而出现。然而，对推理引擎的系统研究仍然缺乏。本文对25个开源和商用推理引擎进行了全面的评估。我们从易用性、部署简便性、通用性支持、可扩展性和适合进行吞吐量和延迟感知计算等方面，对每个推理引擎进行了考察。此外，我们通过调查每个推理引擎支持的优化技术，探索了其设计目标。此外，我们评估了开源推理引擎的生态系统成熟度，并处理了商用解决方案的性能和成本策略。我们概述了未来的研究方向，包括支持复杂基于LLM的服务、支持各种硬件以及增强安全性，为研究人员和开发人员在选择和设计优化的LLM推理引擎方面提供实际指导。我们还提供了一个公共仓库，以持续跟踪这个快速发展的领域的进展：https://github.com/sihyeong/Awesome-LLM-Inference-Engine**|
|**2025-05-02**|**PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding**|Bradley McDanel et.al.|[2505.01572](http://arxiv.org/abs/2505.01572)|null|PipeSpec是一个框架，它将投机解码推广到分层管道中的 $k$ 个模型，实现预测验证和回滚的异步执行，同时具有轻量级协调。我们的分析模型描述了管道各阶段的令牌生成速率，并证明对于任何非零接受率，传统的解码方法在吞吐量上都有保证性的改进。我们进一步推导出稳态验证概率的闭式表达式，这解释了管道深度带来的经验性效益。实验结果表明，PipeSpec可以达到高达2.54倍的速度提升，并且优于最先进的方法。我们使用LLaMA 2和3模型在文本摘要和代码生成任务中验证了PipeSpec，表明随着模型深度的增加，管道效率也会提高，为在多设备系统中加速LLM推理提供了一种可扩展的方法。|
|**2025-04-28**|**AutoJudge: Judge Decoding Without Manual Annotation**|Roman Garipov et.al.|[2504.20039](http://arxiv.org/abs/2504.20039)|null|我们介绍了AutoJudge，这是一个利用特定任务的损失性投机解码来加速大型语言模型（LLM）推理的框架。我们不是逐个匹配原始模型的输出分布，而是识别哪些生成的标记会影响生成的响应的下游质量，放宽了保证，使得“不重要”的标记可以更快地生成。我们的方法依赖于一种半贪婪搜索算法来测试哪些目标模型和草稿模型之间的不匹配应该被纠正以保持质量，哪些可以跳过。然后，我们基于现有的LLM嵌入训练了一个轻量级的分类器，在推理时预测哪些不匹配的标记可以安全接受而不影响最终答案的质量。我们在Llama 3.2 1B（草稿）和Llama 3.1 8B（目标）模型上测试了我们的方法，用于零样本GSM8K推理，与标准投机解码相比，它在答案准确率下降不到1%的情况下，每个验证周期可接受最多1.5倍的标记，而在准确率略有损失的情况下，可超过2倍。当应用于LiveCodeBench基准测试时，我们的方法能够自动检测其他编程特定的重要标记，并显示出类似的速度提升，证明了其在不同任务中的泛化能力。|
|**2025-04-28**|**Taming the Titans: A Survey of Efficient LLM Inference Serving**|Ranran Zhen et.al.|[2504.19720](http://arxiv.org/abs/2504.19720)|**[link](https://github.com/zenrran4nlp/Awesome-LLM-Inference-Serving)**|大型语言模型（LLMs）在生成式人工智能领域取得了显著进展，逐渐演变为跨多个领域和应用的复杂且多功能的工具。然而，由于LLMs拥有大量参数，加上注意力机制的高计算需求，导致了巨大的内存开销，这对实现低延迟和高吞吐量的LLM推理服务提出了重大挑战。在突破性研究的推动下，这一领域的进展得到了显著加速。本文对这些方法进行了全面综述，涵盖了基本实例级方法、深入集群级策略、新兴场景方向以及其他一些虽不常见但重要的领域。在实例级别上，我们回顾了模型放置、请求调度、解码长度预测、存储管理和解耦范式。在集群级别上，我们探讨了GPU集群部署、多实例负载均衡和云服务解决方案。对于新兴场景，我们围绕特定任务、模块和辅助方法组织了讨论。为确保全面概述，我们还突出了几个小众但关键领域。最后，我们概述了进一步推进LLM推理服务领域的潜在研究方向。|
|**2025-04-28**|**R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference**|Zhenyu Zhang et.al.|[2504.19449](http://arxiv.org/abs/2504.19449)|null|大型语言模型（LLMs）虽然在各种应用中展现出卓越的能力，但由于其庞大的模型尺寸，在推理过程中面临着重大挑战，尤其是在部署在边缘设备上时。激活稀疏性提供了一种有希望解决方法，可以减少计算和内存移动，从而实现更高效的推理，尤其是对于小批量在设备上的应用。然而，当前的方法在处理非ReLU激活函数（这是大多数高级LLMs的基础）或需要大量持续训练方面存在局限性。此外，预测激活通道的难度以及可实现的稀疏率限制，使得基于激活稀疏性的方法效果有限。在本文中，我们介绍了R-Sparse，这是一种无需训练的激活稀疏性方法，能够在高级LLMs中实现高稀疏水平。我们对单个线性层内不同组件如何贡献于输出的方式进行了两项初步研究，并发现两个关键观察结果：（i）输入函数的非稀疏部分可以视为少数偏置项，（ii）完整计算可以通过输入通道和权重奇异值的适当组合来有效地近似。基于此，我们将LLMs中的线性层替换为一种秩感知的稀疏推理方法，该方法利用输入通道和奇异值组件的稀疏性，消除了像基于输出稀疏性的方法那样预测激活通道的需要。在Llama-2/3和Mistral模型上进行的十项不同任务实验表明，R-Sparse在50%的模型级稀疏性下实现了可比的性能，并取得了43%的端到端效率提升，这是通过定制内核实现的。|
|**2025-04-26**|**A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification**|Junichiro Niimi et.al.|[2504.18884](http://arxiv.org/abs/2504.18884)|**[link](https://github.com/jniimi/ensemble_inference)**|**随着大型语言模型（LLMs）的发展，LLMs被用于各种任务。然而，在现有文献中，LLMs每次试验结果的可变性和可重复性问题被大量忽视，而实际的人类标注通常使用多数投票来解决标注者之间的分歧。因此，这项研究将简单的集成策略引入到使用LLMs进行情感分析中。结果表明，使用中等大小的LLMs进行多次推理的集成，比使用单一尝试的大型模型产生了更稳健和准确的结果，RMSE降低了18.6%。**|
|**2025-04-25**|**PropRAG: Guiding Retrieval with Beam Search over Proposition Paths**|Jingjin Wang et.al.|[2504.18070](http://arxiv.org/abs/2504.18070)|null|检索增强生成（RAG）已成为为大型语言模型（LLMs）配备最新知识和缓解持续学习中常见的灾难性遗忘的标准非参数方法。然而，标准RAG依赖于独立的段落检索，无法捕捉人类记忆的互联性，这对于复杂推理（联想性）和情境理解（意义构建）至关重要。虽然结构化RAG方法如HippoRAG利用从三元组构建的知识图谱（KGs），但固有的上下文损失限制了准确性。我们引入了PropRAG，这是一个利用丰富上下文的命题和基于命题路径的全新束搜索算法的框架，以显式地发现多步推理链。关键的是，PropRAG的在线检索过程完全不调用生成LLMs，而是依赖高效的图遍历和预计算的嵌入。这避免了在线LLM推理成本和证据收集过程中的潜在不一致性。LLMs在离线时用于高质量命题提取，并在检索后用于答案生成。PropRAG在PopQA（55.3%）、2Wiki（93.7%）、HotpotQA（97.0%）和MuSiQue（77.3%）上实现了最先进的零样本Recall@5结果，同时取得了顶尖的F1分数（例如，MuSiQue上的52.4%）。通过通过更丰富的表示和显式、LLM-free的在线路径寻找来改进证据检索，PropRAG推动了非参数持续学习。|
|**2025-04-24**|**L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference**|Qingyuan Liu et.al.|[2504.17584](http://arxiv.org/abs/2504.17584)|null|大型语言模型（LLMs）越来越多地需要处理长文本序列，但GPU内存限制迫使在内存容量和带宽之间做出艰难的权衡。虽然基于HBM的加速提供了高带宽，但其容量仍然受限。将数据卸载到主机端DIMM可以提高容量，但会引入昂贵的换页开销。我们发现，关键的内存瓶颈仅存在于多头注意力（MHA）的解码阶段，该阶段需要大量容量来存储KV缓存，并且需要高带宽来进行注意力计算。我们的关键洞察表明，这种操作与基于现代DIMM的存储在内存中处理（PIM）架构的独特匹配，该架构提供了容量和带宽的可扩展性。基于这一观察和洞察，我们提出了L3，这是一个集成DIMM-PIM和GPU设备的软硬件协同设计系统。L3引入了三项创新：首先，硬件重新设计解决了DIMM-PIM中的数据布局不匹配和计算单元不匹配问题，提高了LLM推理利用率。其次，通信优化能够通过计算隐藏数据传输开销。第三，自适应调度器协调GPU-DIMM-PIM操作，以最大化设备间的并行性。使用真实世界跟踪的评估表明，L3在HBM-PIM解决方案中实现了高达6.1倍的加速，同时显著提高了批量大小。|
|**2025-04-24**|**On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration**|Maoyang Xiang et.al.|[2504.17376](http://arxiv.org/abs/2504.17376)|null|基于Transformer的大型语言模型（LLMs）在人工智能能力方面取得了显著进步，但由于计算需求高、内存带宽限制和能耗大，在边缘设备上的部署面临重大挑战。本文通过提出一个高效框架，将Qwen2.5-0.5B模型部署在Xilinx Kria KV260边缘平台上，该平台是一个集成了ARM Cortex-A53 CPU和可重构FPGA逻辑的异构系统，来应对这些挑战。利用激活感知权重量化（AWQ）和FPGA加速的执行流水线，该方法提高了模型的压缩率和系统吞吐量。此外，我们还提出了一种混合执行策略，智能地将计算密集型操作卸载到FPGA，同时利用CPU处理轻量级任务，有效平衡计算工作负载并最大化整体性能。我们的框架实现了相对于原始模型的55.08%的模型压缩率，并以每秒5.1个token的速度生成输出，优于每秒2.8个token的基线性能。|
|**2025-04-23**|**PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation**|Zihao An et.al.|[2504.18583](http://arxiv.org/abs/2504.18583)|null|大型语言模型（LLMs）的自回归性质限制了推理速度。每次前向传播只生成一个标记，并且通常受限于内存带宽。推测解码通过采用先草拟后验证的方法来加速标记生成，从而缓解了这一问题。然而，草拟阶段引入的额外开销和草拟模型的训练成本限制了推测解码的效率和适应性。在本工作中，我们引入了PARallel Draft（PARD），这是一种新颖的推测解码方法，它能够以低成本将自回归草拟模型适应为并行草拟模型。PARD通过在草拟阶段的单次前向传播中预测多个未来标记来提高推理效率，并采用条件丢弃标记方法来加速训练。其目标独立性属性允许单个草拟模型应用于不同模型的全家谱，从而最小化适应成本。我们提出的条件丢弃标记方法可以将草拟模型训练效率提高3倍。在我们的优化推理框架中，PARD将LLaMA3.1-8B的推理速度提高了4.08倍，达到每秒311.5个标记。|
|**2025-04-22**|**Token-Aware Coding Flow: A Study with Nano Surge in Reasoning Model**|Junwei Hu et.al.|[2504.15989](http://arxiv.org/abs/2504.15989)|null|随着大规模语言模型（LLMs）在软件工程中的广泛应用，思维链（CoT）方法已成为推动自动化代码生成和优化的关键工具。然而，尽管CoT方法在生成高质量代码方面取得了显著成功，但在推理过程中出现的标记膨胀问题仍然是影响模型性能和效率的巨大挑战，尤其是在处理复杂的代码异味时。代码异味不仅影响代码的可维护性和可扩展性，而且在LLM推理过程中显著增加计算负担，导致过度消耗标记，从而降低推理效率。本文介绍了一种创新的Token-Aware Coding Flow方法，旨在解决CoT过程中由异味代码引起的标记膨胀问题。通过实验，我们验证了代码重构和提示工程策略的协同效应，证明在消除代码异味后，模型推理过程中的标记消耗显著减少。实验结果表明，重构代码在保持功能一致性的同时，可以减少高达50%的标记消耗。此外，通过在提示中明确指出代码异味类型，并采用上下文感知和角色约束等策略，我们进一步优化了推理过程，实现了标记消耗降低24.5%至30%。这些优化不仅显著提高了模型的推理效率和代码生成质量，还为解决复杂代码生成任务中的性能瓶颈提供了新的见解。|
|**2025-04-21**|**Hardware-based Heterogeneous Memory Management for Large Language Model Inference**|Soojin Hwang et.al.|[2504.14893](http://arxiv.org/abs/2504.14893)|null|大型语言模型（LLM）是当今最重要的新兴机器学习应用之一。然而，由于其巨大的模型大小和内存占用增加，LLM推理在由多个GPU组成的、具有适度高带宽内存的常规系统中面临内存容量不足的问题。此外，由于LLM包含许多带宽密集型内核，仅关注内存容量而不考虑带宽会导致严重的性能下降。为了以成本效益的方式处理这种内存容量和带宽需求的冲突，本研究调查了异构内存系统的潜力，提出了H2M2。它使用由容量导向和带宽导向内存组成的非对称内存架构，每个内存设备都附加有计算单元。通过非对称内存，我们首先分析了内核-内存映射对非对称内存的影响。其次，我们提出了一种动态运行时算法，该算法在考虑LLM操作的特点和LLM推理过程中占用空间的变化时找到映射解决方案。第三，我们提倡需要对非对称内存进行内存抽象以实现高效管理。H2M2在GPT3-175B、Chinchilla-70B和Llama2-70B上的速度分别比传统的LPDDR同构内存系统快1.46倍、1.55倍和2.94倍。|
|**2025-04-21**|**KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments**|Junyoung Park et.al.|[2504.15364](http://arxiv.org/abs/2504.15364)|null|在本工作中，我们证明了在LLM推理过程中，具有独特性的键往往具有高的注意力分数。我们探讨了这一现象，并提出了一种基于键相似性的无训练KV缓存淘汰方法——KeyDiff。这种方法有助于在内存和计算预算有限的环境中部署需要长输入提示的基于LLM的应用。与其他KV缓存淘汰方法不同，KeyDiff能够在严格的资源约束下处理任意长度的提示，并高效地生成响应。我们证明了KeyDiff计算了KV缓存选择问题的最优解，该问题最大化了键的多样性，为KeyDiff提供了理论上的理解。值得注意的是，KeyDiff不依赖于注意力分数，允许使用优化的注意力机制，如FlashAttention。我们展示了KeyDiff在多种任务和模型上的有效性，在LongBench基准测试中，与不淘汰的基线相比，KeyDiff在Llama 3.1-8B和Llama 3.2-3B上的性能差距小于0.04%（相当于减少了约23%的KV缓存），证明了其在8K缓存预算下的效果。|
|**2025-04-19**|**Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator**|Akshat Ramachandran et.al.|[2504.14365](http://arxiv.org/abs/2504.14365)|null|为了将这篇论文摘要翻译成中文，我们需要逐步分析并理解每个部分的意义，然后进行准确的翻译。以下是逐句的推理和翻译：  1. "Large language model (LLM) pruning with fixed N:M structured sparsity significantly limits the expressivity of the sparse model, yielding sub-optimal performance."    - 推理：这里讨论的是大语言模型（LLM）的剪枝，使用固定的N:M结构稀疏性会显著限制稀疏模型的表达能力，导致性能不佳。    - 翻译：使用固定N:M结构稀疏性进行的大语言模型（LLM）剪枝会显著限制稀疏模型的表达能力，导致性能不佳。  2. "In contrast, supporting multiple N:M patterns to provide sparse representational freedom introduces costly overhead in hardware."    - 推理：与此相反，支持多种N:M模式以提供稀疏表示的自由度会在硬件中引入高昂的开销。    - 翻译：相比之下，支持多种N:M模式以提供稀疏表示的自由度会在硬件中引入高昂的开销。  3. "To address these challenges for LLMs, we first present a flexible layer-wise outlier-density-aware N:M sparsity (FLOW) selection method."    - 推理：为了解决LLM的这些挑战，我们首先提出了一种灵活的层间异常密度感知N:M稀疏性（FLOW）选择方法。    - 翻译：为了解决LLM的这些挑战，我们首先提出了一种灵活的层间异常密度感知N:M稀疏性（FLOW）选择方法。  4. "FLOW enables the identification of optimal layer-wise N and M values (from a given range) by simultaneously accounting for the presence and distribution of outliers, allowing a higher degree of representational freedom."    - 推理：FLOW通过同时考虑异常的存在和分布，能够识别出最优的层间N和M值（从给定范围内），从而允许更高的表示自由度。    - 翻译：FLOW通过同时考虑异常的存在和分布，能够识别出最优的层间N和M值（从给定范围内），从而允许更高的表示自由度。  5. "To deploy sparse models with such N:M flexibility, we then introduce a flexible, low-overhead digital compute-in-memory architecture (FlexCiM)."    - 推理：为了部署具有这种N:M灵活性的稀疏模型，我们随后介绍了一种灵活的、低开销的数字计算在内存架构（FlexCiM）。    - 翻译：为了部署具有这种N:M灵活性的稀疏模型，我们随后介绍了一种灵活的、低开销的数字计算在内存架构（FlexCiM）。  6. "FlexCiM supports diverse sparsity patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros, which are adaptively aggregated and disaggregated through distribution and merging mechanisms for different N and M values."    - 推理：FlexCiM通过将数字CiM（DCiM）宏分区成更小的子宏，并通过分配和合并机制自适应地聚合和分解不同N和M值，支持多种稀疏模式。    - 翻译：FlexCiM通过将数字CiM（DCiM）宏分区成更小的子宏，并通过分配和合并机制自适应地聚合和分解不同N和M值，支持多种稀疏模式。  7. "Extensive experiments on both transformer-based and recurrence-based state space foundation models (SSMs) demonstrate that FLOW outperforms existing alternatives with an accuracy improvement of up to 36%, while FlexCiM achieves up to 1.75x lower inference latency and 1.5x lower energy consumption compared to existing sparse accelerators."    - 推理：在基于Transformer和基于递归的状态空间基础模型（SSMs）上进行的广泛实验表明，FLOW在准确性上比现有替代方案提高了高达36%，而FlexCiM相较于现有稀疏加速器实现了高达1.75倍的推理延迟降低和1.5倍的能耗降低。    - 翻译：在基于Transformer和基于递归的状态空间基础模型（SSMs）上进行的广泛实验表明，FLOW在准确性上比现有替代方案提高了高达36%，而FlexCiM相较于现有稀疏加速器实现了高达1.75倍的推理延迟降低和1.5倍的能耗降低。  8. "Code is available at: https://github.com/FLOW-open-project/FLOW"    - 推理：代码可在以下链接获取：https://github.com/FLOW-open-project/FLOW    - 翻译：代码可在以下链接获取：https://github.com/FLOW-open-project/FLOW  综合以上翻译，完整的中文摘要如下：  使用固定N:M结构稀疏性进行的大语言模型（LLM）剪枝会显著限制稀疏模型的表达能力，导致性能不佳。相比之下，支持多种N:M模式以提供稀疏表示的自由度会在硬件中引入高昂的开销。为了解决LLM的这些挑战，我们首先提出了一种灵活的层间异常密度感知N:M稀疏性（FLOW）选择方法。FLOW通过同时考虑异常的存在和分布，能够识别出最优的层间N和M值（从给定范围内），从而允许更高的表示自由度。为了部署具有这种N:M灵活性的稀疏模型，我们随后介绍了一种灵活的、低开销的数字计算在内存架构（FlexCiM）。FlexCiM通过将数字CiM（DCiM）宏分区成更小的子宏，并通过分配和合并机制自适应地聚合和分解不同N和M值，支持多种稀疏模式。在基于Transformer和基于递归的状态空间基础模型（SSMs）上进行的广泛实验表明，FLOW在准确性上比现有替代方案提高了高达36%，而FlexCiM相较于现有稀疏加速器实现了高达1.75倍的推理延迟降低和1.5倍的能耗降低。代码可在以下链接获取：https://github.com/FLOW-open-project/FLOW|
|**2025-04-19**|**FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference**|Coleman Hooper et.al.|[2504.14152](http://arxiv.org/abs/2504.14152)|null|量化是一种通过利用更节能的低精度数据路径和减少内存占用来提高大型语言模型（LLM）推理效率的有效工具。然而，在没有降低模型准确度的情况下，准确地将LLM的权重和激活量化到低精度是具有挑战性的。我们提出了细粒度混合精度（FGMP）量化，这是一种训练后混合精度硬件-软件协同设计方法，在量化大部分权重和激活到较低精度的同时保持准确性。我们的工作有以下贡献：1）我们开发了一种策略，该策略使用每个值的扰动（加权Fisher信息）来选择哪些权重和激活块需要保留在更高的精度。这种方法通过识别哪些权重和激活块需要保留在更高精度以最小化模型损失的扰动来保持准确性。2）我们还提出了一种敏感性加权裁剪方法，用于细粒度量化，有助于保留量化到低精度块的准确性。3）然后我们提出了硬件增强，以利用FGMP量化的效率优势。我们的硬件实现包括i）支持块粒度FGMP的数据路径，以及ii）一个混合精度激活量化单元，该单元可以即时将激活块分配到高精度或低精度，同时最小化运行时间和能耗。我们的设计使用NVFP4（一种具有微缩放的FP4格式）作为低精度数据类型，FP8作为高精度数据类型，实现了高效的FGMP量化，在Wikitext-103上相对于全FP8基线设计，Llama-2-7B模型的困惑度降低小于1%，在推理过程中消耗的能量减少14%，并且权重内存需求减少了30%。|
|**2025-04-18**|**High-Throughput LLM inference on Heterogeneous Clusters**|Yi Xiong et.al.|[2504.15303](http://arxiv.org/abs/2504.15303)|null|如今，许多公司拥有各种类型的AI加速器，形成了异构集群。高效利用这些集群为高吞吐量的大型语言模型（LLM）推理服务可以显著降低成本并加快任务处理。然而，在异构集群上进行的LLM推理面临两个主要挑战。首先，不同的部署配置会导致性能差异很大。可能的配置数量庞大，评估特定配置的有效性非常复杂。因此，找到最佳配置并非易事。其次，异构集群中的LLM推理实例具有不同的处理能力，导致处理推理请求的速度不同。评估这些能力并设计一个能够充分利用每个实例潜力的请求调度算法具有挑战性。在本文中，我们提出了一种在异构集群上的高吞吐量推理服务系统。首先，通过建模资源量和预期吞吐量并使用穷举搜索方法来优化部署配置。其次，提出了一种新颖的机制来调度实例间的请求，该机制充分考虑了不同实例的处理能力。大量实验表明，所提出的调度器在两个异构集群上分别提高了122.5%和33.6%的吞吐量。|
|**2025-04-18**|**HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing**|Myunghyun Rhee et.al.|[2504.16112](http://arxiv.org/abs/2504.16112)|null|注意力层是基于Transformer的大型语言模型（LLM）的核心组件，由于其操作强度低和KV缓存对内存的大量需求，导致了当前GPU系统中的低效率。我们提出了一种高带宽处理单元（HPU），这是一种内存密集型协处理器，它通过在大批量LLM推理期间卸载内存绑定操作，增强了GPU资源的利用率。HPU作为附加卡，可以扩展以适应由大批次大小和延长序列长度驱动的激增内存需求。在本文中，我们展示了在GPU系统上使用基于PCIe的FPGA卡实现的HPU原型。我们的新型GPU-HPU异构系统相对于仅使用GPU的系统，实现了高达4.1倍的性能提升和4.6倍的能效改进，同时提供了可扩展性而不增加GPU数量。|
|**2025-04-16**|**Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading**|Kihyun Kim et.al.|[2504.11816](http://arxiv.org/abs/2504.11816)|**[link](https://github.com/lass-lab/InferSave)**|**LLM推理对于文本摘要、翻译和数据分析等应用至关重要，但云服务提供商（如AWS）的GPU实例高昂成本成为一大负担。本文提出了一种名为InferSave的云上LLM推理成本效益VM选择框架。InferSave基于服务级别目标（SLOs）和作业特征优化KV缓存卸载，估算GPU内存需求，并推荐成本效益高的VM实例。此外，计算时间校准函数（CTCF）通过调整理论性能和实际GPU性能之间的差异来提高实例选择准确性。在AWS GPU实例上的实验表明，在不进行KV缓存卸载的情况下选择较低成本的实例，对于在线工作负载，成本效率可提高高达73.7%，而对于离线工作负载，KV缓存卸载可节省高达20.19%。**|
|**2025-04-16**|**Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs**|Hyungwoo Lee et.al.|[2504.11765](http://arxiv.org/abs/2504.11765)|null|近期的大型语言模型（LLMs）在输入上下文长度和模型规模不断增长的情况下，推理延迟问题日益突出。特别是检索增强生成（RAG）技术，通过整合外部知识来增强LLM的响应，由于显著增加了输入标记的数量，加剧了这一问题。这种标记长度的扩展导致计算开销大幅上升，尤其是在预填充阶段，从而延长了首次标记时间（TTFT）。为了解决这一问题，本文提出了一种方法，通过利用基于磁盘的键值（KV）缓存来减轻预填充阶段的计算负担，从而减少TTFT。我们还介绍了一个针对多实例LLM RAG服务环境的基于磁盘的共享KV缓存管理系统，称为Shared RAG-DCache。该系统结合最优的系统配置，在给定的资源限制下提高了吞吐量和延迟。Shared RAG-DCache利用了RAG中与用户查询相关的文档的局部性，以及LLM推理服务中的排队延迟。它主动生成和存储与查询相关的文档的磁盘KV缓存，并在多个LLM实例之间共享，以增强推理性能。在配备2个GPU和1个CPU的单主机上的实验中，Shared RAG-DCache实现了15~71%的吞吐量提升和高达12~65%的延迟降低，具体取决于资源配置。|
|**2025-04-16**|**Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures**|Prabhu Vellaisamy et.al.|[2504.11750](http://arxiv.org/abs/2504.11750)|null|大型语言模型（LLM）基于推理的工作负载越来越多地主导着数据中心成本和资源利用率。因此，了解在不断发展着的CPU-GPU耦合架构上推理工作负载的特点对于优化至关重要。本文对LLM在松耦合（PCIe A100/H100）和紧耦合（GH200）系统上的推理行为进行了深入研究分析。我们利用我们新颖的剖析器SKIP和指标如总内核启动和排队时间（TKLQT）来分析性能动态。结果显示，紧耦合的GH200在大型批量大小上显著优于松耦合系统，实现了Llama 3.2-1B的1.9倍至2.7倍的预填充延迟。然而，我们的分析也揭示了GH200在比LC系统大4倍的批量大小上仍受CPU限制。在这个扩展的CPU限制区域内，我们发现Grace CPU的性能特性是导致GH200在低批量大小上推理延迟较高的关键因素。我们证明TKLQT准确地识别了这个CPU/GPU限制的转换点。基于这项分析，我们进一步表明，内核融合通过减少内核启动开销为缓解GH200的低批量延迟瓶颈提供了巨大潜力。这种详细的内核级特性描述为优化不同的CPU-GPU耦合策略提供了关键见解。这项工作是初步尝试，我们计划探索其他需要不同程度的CPU-GPU异构架构的主要AI/DL工作负载。|
|**2025-04-15**|**Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints**|Ruicheng Ao et.al.|[2504.11320](http://arxiv.org/abs/2504.11320)|**[link](https://github.com/microsoft/vidur)**|**大型语言模型（LLMs）在当今的应用中不可或缺，但它们的推理过程——通过分块处理文本并使用内存密集型键值（KV）缓存来生成响应——需要大量的计算资源，尤其是在内存受限的情况下。本文将LLM推理优化表述为一个多阶段在线调度问题，其中连续的提示到达和KV缓存增长使得传统的调度方法无效。我们开发了一种流体动力学近似方法，以提供一个可处理的基准，指导算法设计。在此基础上，我们提出了等待累积推理阈值（WAIT）算法，该算法在已知输出长度的情况下，使用多个阈值来优化调度 incoming prompts，并将其扩展为嵌套WAIT以处理未知输出长度的情况。理论分析表明，在重交通条件下，这两个算法都能在流体基准上实现接近最优的性能，平衡了吞吐量、延迟和首次标记时间（TTFT）。在A100 GPU上使用Llama-7B模型进行实验，并使用合成和真实世界的数据集，与vLLM和Sarathi等现有基准相比，证明了改进的吞吐量和降低的延迟。这项工作将运筹学与机器学习相结合，为在内存受限条件下高效部署LLMs提供了一个严格的框架。**|
|**2025-04-14**|**AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference**|Yangshen Deng et.al.|[2504.10326](http://arxiv.org/abs/2504.10326)|null|AlayaDB是一个针对AlayaDB AI中的大型语言模型（LLMs）进行高效有效长上下文推理的尖端向量数据库系统。具体来说，它将KV缓存和注意力计算从LLM推理系统中解耦，并将它们封装到一个新颖的向量数据库系统中。对于模型即服务（MaaS）提供商，与现有的替代解决方案（例如，KV缓存解耦、基于检索的稀疏注意力）相比，AlayaDB消耗更少的硬件资源，并为具有不同服务级别目标（SLOs）的各种工作负载提供更高的生成质量。AlayaDB的核心在于，它将LLM推理中的注意力计算和缓存管理抽象为查询处理过程，并通过原生查询优化器优化性能。在这项工作中，我们通过以下两个方面展示了AlayaDB的有效性：（i）来自我们行业合作伙伴的三个用例；（ii）在LLM推理基准测试上的广泛实验结果。|
|**2025-04-14**|**KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference**|Yuxuan Tian et.al.|[2504.09936](http://arxiv.org/abs/2504.09936)|null|高效推理大型语言模型（LLMs）受到不断增长的键值（KV）缓存的影响，使得KV缓存压缩成为一个关键的研究方向。传统方法基于注意力分数或位置启发式规则选择性淘汰不太重要的KV缓存条目，这会导致信息丢失和幻觉。最近，人们探索了基于合并的策略来保留更多信息，通过合并将要被丢弃的KV对；然而，这些现有方法不可避免地会在合并前后引入注意力分布的不一致性，导致输出扰动和生成质量下降。为了克服这一挑战，我们提出了KeepKV，这是一种新的自适应KV缓存合并方法，旨在消除输出扰动同时保留在严格内存约束下的性能。KeepKV引入了选举投票机制，该机制记录合并历史并自适应调整注意力分数。此外，它进一步利用了一种新颖的零推理-扰动合并方法，保持注意力一致性，并补偿由于缓存合并引起的注意力损失。KeepKV在显著压缩的缓存中成功保留了关键上下文信息。在多个基准和LLM架构上的广泛实验表明，KeepKV显著降低了内存使用，将推理吞吐量提高了2倍以上，即使只有10%的KV缓存预算也能保持优异的生成质量。|
|**2025-04-14**|**Understanding and Optimizing Multi-Stage AI Inference Pipelines**|Abhimanyu Rajeshkumar Bambhaniya et.al.|[2504.09775](http://arxiv.org/abs/2504.09775)|null|大型语言模型（LLMs）的快速发展推动了对于更加复杂的推理管道和硬件平台的迫切需求。现代LLM服务已超越了传统的预填充-解码工作流程，整合了检索增强生成（RAG）、键值（KV）缓存检索、动态模型路由和多步推理等多阶段过程。这些阶段呈现出不同的计算需求，需要集成GPU、ASICs、CPUs和以内存为中心的架构的分布式系统。然而，现有的模拟器缺乏对异构、多引擎工作流程的精确建模能力，限制了它们在指导架构决策方面的能力。为了解决这一差距，我们引入了HERMES，这是一个异构多阶段LLM推理执行模拟器。HERMES能够模拟包括RAG、KV检索、推理、预填充和解码在内的多样化请求阶段，跨越复杂的硬件层次结构。HERMES支持异构客户端同时执行多个模型，与之前的框架不同，同时结合了高级批处理策略和多级内存层次结构。通过整合真实硬件跟踪与分析建模，HERMES捕捉了关键权衡，如内存带宽竞争、集群间通信延迟和批处理效率在混合CPU加速器部署中的问题。通过案例研究，我们探讨了推理阶段对端到端延迟的影响、混合管道的最佳批处理策略以及远程KV缓存检索的架构影响。HERMES使系统设计师能够导航LLM推理不断变化的环境，为优化下一代AI工作负载的硬件-软件协同设计提供可操作的见解。|
|**2025-04-14**|**HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving**|Avinash Kumar et.al.|[2504.10724](http://arxiv.org/abs/2504.10724)|null|部署大型语言模型（LLMs）由于与关键性能指标（如延迟、准确性和吞吐量）相关的固有权衡而带来重大挑战。通常，一个指标的提升伴随着其他指标的下降。早期退出LLMs（EE-LLMs）通过在自信地找到输出标记时跳过一些后期模型层，有效地在权衡空间中导航，从而降低延迟而不影响准确性。然而，由于早期退出的选择依赖于任务且在请求处理之前未知，EE-LLMs保守地加载整个模型，限制了资源节约和吞吐量。此外，当前框架静态地为用户任务选择模型，限制了我们对输入查询变化性质的适应能力。我们提出了HELIOS来解决这些挑战。首先，HELIOS通过使用一组提示符评估候选LLMs，实时收集遥测数据来筛选候选LLMs。其次，HELIOS使用这些评估的早期退出数据，贪婪地仅加载选定模型的一部分层。这种方法产生了内存节约，使我们能够同时处理更多请求，从而提高吞吐量。第三，HELIOS监控并定期重新评估候选LLMs的性能，如果需要，切换到另一个模型，该模型可以更有效地服务传入的查询（例如，使用更少的层而不降低准确性）。我们的评估显示，与基线相比，当优化相应服务级别目标时，HELIOS实现了1.48倍的吞吐量、1.10倍的能源效率、1.39倍的更短响应时间和推理批量大3.7倍的提升。|
|**2025-04-13**|**LoopLynx: A Scalable Dataflow Architecture for Efficient LLM Inference**|Jianing Zheng et.al.|[2504.09561](http://arxiv.org/abs/2504.09561)|**[link](https://github.com/zjnyly/looplynx)**|**本文提出了一种名为LoopLynx的可扩展数据流架构，用于高效地进行大型语言模型（LLM）推理，并通过混合时空设计优化FPGA的使用。LoopLynx的设计融合了时空架构，将计算密集型操作器实现为大型数据流内核。这种方式实现了与空间架构相似的高吞吐量，并且通过以时间方式组织和重用这些内核，进一步提升了FPGA的峰值性能。此外，为了克服单个设备的资源限制，我们提供了一种多FPGA分布式架构，该架构重叠并隐藏了所有数据传输，从而充分利用分布式加速器。通过这种方式，LoopLynx可以有效地扩展到多个设备，以进一步探索大规模LLM推理中的模型并行性。对GPT-2模型的评估表明，LoopLynx可以实现与最先进的基于单个FPGA的加速器相当的性能。此外，与Nvidia A100相比，我们的双FPGA配置的加速器在推理延迟上提供了2.52倍的加速，同时能耗仅为48.1%。**|
|**2025-04-12**|**MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints**|Yichao Yuan et.al.|[2504.09345](http://arxiv.org/abs/2504.09345)|null|混合专家（MoE）语言模型（LLM），以其稀疏的激活模式为特征，提供了一种在避免成比例增加推理成本的同时扩展语言模型的可行方法。然而，它们庞大的参数规模在资源受限、GPU内存容量有限的环境中部署时面临挑战，因为GPU内存通常不足以容纳整个模型权重集。因此，典型的部署依赖于CPU-GPU混合执行：GPU处理计算密集型的GEMM操作，而CPU处理相对较轻的注意力机制。这种设置引入了一个关键挑战：如何有效地优化CPU和GPU之间的资源利用率？先前的研究基于有限范围的性能模型设计了系统优化。具体来说，这些模型没有捕捉到硬件属性和系统执行机制之间的复杂交互。因此，先前的方法既没有识别也没有达到硬件限制。本文提出了一种名为MoE-Lens的高吞吐量MoE LLM推理系统，该系统通过针对资源受限环境的整体性能建模来设计。我们的性能模型彻底分析了各种基本系统组件，包括CPU内存容量、GPU计算能力和工作负载特征，以了解MoE推理的理论性能上限。此外，它捕捉了系统执行机制，以识别关键硬件瓶颈并准确预测可实现的吞吐量。在性能模型的基础上，MoE-Lens引入了一种接近硬件极限的推理系统。在多种MoE模型和数据集上评估，MoE-Lens的平均性能比最先进的解决方案提高了4.6倍（最高可达25.5倍），我们的理论模型预测性能的平均准确率为94%。|
|**2025-04-11**|**Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash**|Fucheng Jia et.al.|[2504.08378](http://arxiv.org/abs/2504.08378)|null|大型语言模型（LLMs）越来越频繁地被部署在移动设备上，但有限的DRAM容量限制了可部署模型的大小。本文介绍了ActiveFlow，这是第一个能够为现代LLMs（非ReLU型）实现自适应DRAM使用的LLM推理框架，从而实现了可部署模型大小的扩展。该框架基于新颖的“活动权重DRAM-闪存交换”概念，并包含三个新颖的技术：1）跨层活动权重预加载。它使用当前层的激活来预测几个后续层的活动权重，使计算和数据加载可以重叠，同时促进大I/O传输。2）稀疏感知的自蒸馏。它调整活动权重以与密集模型输出分布对齐，补偿由上下文稀疏性引入的近似。3）活动权重DRAM-闪存交换管道。它根据可用内存对热权重缓存、预加载的活动权重和涉及计算的权重之间的DRAM空间分配进行协调。结果显示，与现有的效率优化方法相比，ActiveFlow实现了性能-成本帕累托前沿。|
|**2025-04-11**|**Jupiter: Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices**|Shengyuan Ye et.al.|[2504.08242](http://arxiv.org/abs/2504.08242)|null|生成式大型语言模型（LLMs）因其在不同AI任务中的卓越能力而备受关注。传统上部署在云数据中心，LLMs现在正越来越多地转向更易于访问的边缘平台，以保护敏感用户数据并确保隐私保护。然而，单个边缘设备的计算资源有限，可能导致推理延迟过长和内存使用过度。虽然现有研究已经探讨了协作边缘计算来突破单个设备的资源瓶颈，但这些解决方案仍然存在巨大的通信开销和边缘资源利用率不足的问题。此外，它们仅专注于优化预填充阶段，忽略了生成式LLMs至关重要的自回归解码阶段。为了解决这个问题，我们提出了Jupiter，这是一个快速、可扩展且资源高效的协作边缘AI系统，用于生成式LLMs的推理。Jupiter以灵活的流水线架构为原则，并根据预填充和解码阶段的不同特性来区分其系统设计。对于预填充阶段，Jupiter提出了一种新颖的序列内管道并行性，并开发了一种细致的并行规划策略以最大化资源效率；对于解码，Jupiter设计了一种基于概要的流水线并行解码机制，结合了推测性解码，进一步提升了推理加速。基于现实实现的广泛评估表明，Jupiter在各种边缘环境设置下显著优于现有方法，实现了高达26.1倍的端到端延迟降低，同时保证了与现有方法相当的产生质量。|
|**2025-04-11**|**SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting**|Jiaming Xu et.al.|[2504.08850](http://arxiv.org/abs/2504.08850)|null|近期，早期退出技术作为一种加速大型语言模型（LLMs）的可行方法，通过有效减少硬件计算和内存访问而备受关注。在本文中，我们提出了SpecEE，一个具有投机性早期退出的快速LLM推理引擎。（1）在算法层面，我们通过利用投机性标记与正确结果之间的概率相关性以及GPU的高并行性，提出了基于投机的轻量级预测器设计。（2）在系统层面，我们指出并非所有层都需要预测器，并基于偏斜分布和上下文相似性设计了双层启发式预测器调度引擎。（3）在映射层面，我们指出不同的解码方法具有相同的本质特征，并提出了针对预测器的上下文感知合并映射，以及高效的GPU实现以支持投机解码，并形成了一个框架，用于在云和个人计算机（PC）场景下应用各种现有的正交加速技术（例如量化与稀疏激活），成功推动了准确性和加速比帕累托前沿。值得注意的是，SpecEE可以通过微不足道的训练开销提前应用于任何LLM，而不会影响模型的原始参数。大量实验表明，SpecEE在云和PC场景下分别实现了Llama2-7B的2.25倍和2.43倍加速。|
|**2025-04-10**|**Token Level Routing Inference System for Edge Devices**|Jianshu She et.al.|[2504.07878](http://arxiv.org/abs/2504.07878)|null|大型语言模型（LLM）推理的计算复杂性显著限制了它们在边缘设备上的部署效率。相比之下，小型语言模型提供更快的解码和更低的资源消耗，但通常会导致响应质量下降和幻觉敏感性增加。为了解决这一权衡问题，协作解码应运而生，其中大型模型协助生成关键标记。这种模式通过允许大型模型进行选择性干预，以实现高质量的推理，同时保持小型模型的速度和效率，从而利用了两种模型类型的优势。在这项工作中，我们提出了一种新颖的协作解码推理系统，该系统允许小型模型在设备上执行推理，同时有选择性地咨询云端大型模型以生成关键标记。令人瞩目地，该系统在CommonsenseQA上实现了60%的性能提升，仅使用M1 MacBook上的0.5B模型，且只有不到7%的标记生成被上传到云端的大型模型。|
|**2025-04-10**|**Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving**|Shihong Gao et.al.|[2504.07494](http://arxiv.org/abs/2504.07494)|**[link](https://github.com/eddiegaoo/apt-serve)**|**大型语言模型（LLM）推理服务系统对于各种基于LLM的应用至关重要。随着对LLM服务的需求持续增长，将这些系统扩展以处理高请求率同时满足延迟服务级别目标（SLOs），即有效吞吐量，变得至关重要。然而，现有系统往往难以提高有效吞吐量，主要是因为首次出字时间（TTFT）SLO达成率显著下降。我们确定了这一瓶颈的两个主要原因：（1）内存密集型KV缓存限制了在GPU内存约束下的批量大小扩展；（2）默认的先到先得调度策略强制执行的刚性批量组成。在本文中，我们介绍了Apt-Serve，这是一个可扩展的框架，旨在提高LLM推理服务的有效吞吐量。Apt-Serve具有一种新的混合缓存方案，该方案结合了KV缓存和内存高效的隐藏缓存，用于可重用输入隐藏状态向量，从而允许大批量并提高请求并发性。基于混合缓存，Apt-Serve采用自适应运行时调度机制，动态优化批量组成。我们正式定义了自适应调度优化问题，并提出了一种具有理论保证的高效算法。在三个真实世界数据集和参数量从13B到66B的LLM上的广泛评估表明，与最先进的推理服务系统相比，Apt-Serve实现了高达8.8倍的有效吞吐量提升。**|
|**2025-04-10**|**UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache Pruning for Efficient Long-Context LLM Inference**|Weikai Xu et.al.|[2504.07479](http://arxiv.org/abs/2504.07479)|null|基于Transformer的大型语言模型（LLMs）在各种自然语言处理（NLP）应用中取得了令人瞩目的性能。然而，KV缓存引起的高内存和计算成本限制了推理效率，尤其是对于长输入序列。基于存储计算（CIM）的加速器已被提出用于LLMs加速和KV缓存剪枝。然而，由于现有加速器仅支持静态剪枝（固定模式）或动态剪枝（原始实现），它们要么精度下降严重，要么效率低下。在本文中，我们提出了一种基于铁电场效应晶体管（FeFET）的统一内容可寻址存储器（CAM）和CIM架构，称为UniCAIM。UniCAIM同时支持静态和动态剪枝，具有3种计算模式：1）在CAM模式下，UniCAIM能够在O(1)时间内实现近似相似度测量，以高能效支持动态KV缓存剪枝；2）在电荷域CIM模式下，基于累积相似度分数的静态剪枝可以支持，这比固定模式更加灵活；3）在电流域模式下，可以使用所选的KV缓存子集进行精确的注意力计算。我们进一步提出了一种新型的CAM/CIM单元设计，利用FeFET的多级特性进行KV缓存的符号多比特存储和原地注意力计算。通过大量的实验结果，我们证明了UniCAIM在电路级别上可以比最先进的基于CIM的LLM加速器降低8.2-831倍的面积-能量-延迟产品（AEDP），同时在应用级别上保持与密集注意力相当的高精度，显示出其在高效长上下文LLM推理中的巨大潜力。|
|**2025-04-10**|**Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents**|Yueying Li et.al.|[2504.07347](http://arxiv.org/abs/2504.07347)|null|随着对大型语言模型（LLMs）和AI代理的需求迅速增长，优化系统以实现高效的LLM推理变得至关重要。尽管许多努力针对系统级工程，但通过数学建模和排队论的探索却很少。在本文中，我们旨在为LLM推理开发排队论基础，弥合排队论和LLM系统社区之间的差距。特别是，我们研究了LLM推理系统中的吞吐量方面。我们证明，一类“工作保存”调度算法能够为单个请求和AI代理工作负载实现最大吞吐量，强调“工作保存”作为实践中关键的设计原则。对现实系统进行的评估表明，Orca和Sarathi-serve是吞吐量最优的，这为从业者提供了安慰，而FastTransformer和vanilla vLLM则不稳定，应谨慎使用。我们的结果突出了排队论社区在改善LLM推理系统方面可以提供的巨大好处，并呼吁更多跨学科的发展。|
|**2025-04-10**|**SD $^2$: Self-Distilled Sparse Drafters**|Mike Lasby et.al.|[2504.08838](http://arxiv.org/abs/2504.08838)|null|推测解码是一种降低大型语言模型（LLM）延迟的强大技术，提供了一个容错框架，使得使用高度压缩的草稿模型成为可能。在这项工作中，我们介绍了自我蒸馏稀疏草稿器（SD$^2$），这是一种利用自我数据蒸馏和细粒度权重稀疏性来产生高效且与目标模型良好对齐的草稿模型的新方法。SD$^2$系统地提高了草稿标记的接受率，同时显著减少了乘加操作（MACs），即使在通用辅助生成（UAG）设置中，草稿模型和目标模型来自不同的模型家族。在Llama-3.1-70B目标模型上，SD$^2$ 比层剪枝草稿模型提供了1.59倍的均值接受长度（MAL），同时将MACs减少了43.87%以上，与密集草稿模型相比，在减少8.36%的MAL的同时。我们的结果表明，稀疏感知的微调和压缩策略具有提高LLM推理效率的潜力，同时保持与目标模型的良好对齐。|
|**2025-04-08**|**Hogwild! Inference: Parallel LLM Generation via Concurrent Attention**|Gleb Rodionov et.al.|[2504.06261](http://arxiv.org/abs/2504.06261)|**[link](https://github.com/eqimp/hogwild_llm)**|大型语言模型（LLMs）通过高级推理、长篇内容生成和使用工具等能力，展示了处理越来越复杂任务的能力。解决这些任务通常涉及长时间的推理计算。在人类问题解决中，为了加快工作进度，常见的策略是合作：通过将问题分解为子任务、同时探索不同的策略等。最近的研究表明，LLMs也可以通过实施显式的合作框架，如投票机制或显式创建可以并行执行的独立子任务来实现并行操作。然而，这些框架可能并不适用于所有类型的任务，这可能会阻碍它们的适用性。在这项工作中，我们提出了不同的设计方法：我们并行运行LLM“工作者”，允许它们通过一个同时更新的注意力缓存进行同步，并提示这些工作者决定如何最佳地合作。我们的方法允许实例针对当前的问题提出自己的合作策略，同时“看到”并发缓存中彼此的部分进度。我们通过Hogwild!推理实现了这种方法：一个并行LLM推理引擎，其中相同的LLM实例在相同的注意力缓存中并行运行，并能够“即时”访问彼此生成的标记。Hogwild!推理利用旋转位置嵌入（RoPE）来避免重新计算，同时提高并行硬件利用率。我们发现，现代具有推理能力的LLMs可以直接使用共享键值缓存进行推理，无需额外的微调。|
|**2025-04-08**|**SPIRe: Boosting LLM Inference Throughput with Speculative Decoding**|Sanjit Neelam et.al.|[2504.06419](http://arxiv.org/abs/2504.06419)|null|推理步骤：  1. 确定摘要中的专业术语和概念，如speculative decoding (SD)、autoregressive decoding (AD)、KV cache、sparse attention等，并理解其在计算机科学领域的含义。  2. 确定摘要的主旨，即speculative decoding (SD)在提高自回归解码（AD）速度方面的作用，以及SPIRe模型的介绍。  3. 逐句翻译摘要，保持专业术语和概念的原貌，并确保翻译的流畅性和准确性。  中文翻译结果：  已证实，在小的批次大小下，投机解码（SD）可以将自回归解码（AD）的延迟降低2-3倍。然而，为了提高吞吐量并因此降低每令牌的成本，需要使用大的批次大小进行解码。最近的研究表明，如果上下文足够长，草稿模型的KV缓存足够稀疏，SD也可以加速大批次大小的解码。我们介绍了SPIRe，一个草稿模型，它结合了静态稀疏注意力、剪枝初始化和反馈内存，与较小的草稿模型的投机相比，可以增加投机解码的模式化吞吐量超过100%，与稀疏自投机基线相比，则超过35%。当请求之间的上下文长度变化很大时，我们的方法特别有效。|
|**2025-04-08**|**Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching**|Yanhao Dong et.al.|[2504.06319](http://arxiv.org/abs/2504.06319)|null|大型语言模型（LLMs）在推理过程中由于高带宽内存（HBM）带宽限制而表现出明显的内存绑定特性。在本文中，我们提出了一种基于L2缓存的异步KV缓存预取方法，通过计算负载重叠来突破LLM推理中的内存带宽瓶颈。通过在活跃计算窗口期间战略性地调度空闲内存带宽，我们的方法主动将所需的KV缓存预取到GPU的L2缓存中，使得后续访问能够实现高速的L2缓存命中，并有效地在计算周期内隐藏HBM访问延迟。在NVIDIA H20 GPU上的大量实验表明，所提出的方法在注意力内核效率上实现了2.15倍的提升，并在端到端吞吐量上提高了高达1.97倍，超越了最先进的基线FlashAttention-3。值得注意的是，我们的解决方案与现有的优化技术保持正交，并且可以集成到当前的推理框架中，为下一代LLM推理引擎提供可扩展的延迟隐藏解决方案。|
|**2025-04-07**|**Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness**|Dongzhuoran Zhou et.al.|[2504.05163](http://arxiv.org/abs/2504.05163)|null|基于知识图谱的检索增强生成（KG-RAG）是一种技术，通过从知识图谱（KGs）中检索相关信息来增强大型语言模型（LLM）在问答（QA）等任务中的推理能力。然而，现实中的知识图谱往往是不完整的，这意味着用于回答问题的关键信息可能缺失。现有的基准测试并没有充分捕捉知识图谱不完整性对KG-RAG性能的影响。在本文中，我们系统地评估了在知识图谱不完整情况下的KG-RAG方法，通过采用不同方法删除三元组来分析其产生的影响。我们证明KG-RAG方法对知识图谱不完整性非常敏感，强调了在现实设置中需要更鲁棒的解决方案。|
|**2025-04-07**|**User Feedback Alignment for LLM-powered Exploration in Large-scale Recommendation Systems**|Jianling Wang et.al.|[2504.05522](http://arxiv.org/abs/2504.05522)|null|在大型推荐系统中，探索即超越用户既定偏好的行为，由于反馈循环和用户探索模式上的有限信号，这一过程颇具挑战。大型语言模型（LLMs）通过利用其世界知识在反馈循环之外推荐新颖内容，提供了潜在的机会。一个关键挑战是在保持其知识和推理能力的同时，将LLMs与用户偏好对齐。本文在利用LLMs规划下一个新颖用户兴趣的同时，引入了一种新颖的方法，该方法结合了分层规划和LLM推理时缩放，在不损害新颖性的情况下提高推荐的相关性。我们将新颖性与用户对齐解耦，为每个目标训练单独的LLMs。然后，我们放大专注于新颖性的LLM的推理，并使用用户对齐的LLM选择最佳预测。现场实验证明了其有效性，显示了用户满意度（通过观看活动和活跃用户数量衡量）和探索多样性方面的显著提升。|
|**2025-04-04**|**Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency**|Erik Johannes Husom et.al.|[2504.03360](http://arxiv.org/abs/2504.03360)|null|在边缘设备上部署大型语言模型（LLMs）面临着计算约束、内存限制、推理速度和能耗等方面的重大挑战。模型量化已成为实现高效LLM推理的关键技术，通过减小模型大小和计算开销来提高效率。在本研究中，我们对Ollama库中的28个量化LLMs进行了全面分析，这些模型默认应用了后训练量化（PTQ）和仅权重量化技术，部署在边缘设备（4GB RAM的Raspberry Pi 4）上。我们评估了多个量化级别和任务类型下的能效、推理性能和输出精度。模型在五个标准化数据集（CommonsenseQA、BIG-Bench Hard、TruthfulQA、GSM8K和HumanEval）上进行了基准测试，并使用高分辨率、基于硬件的能耗测量工具来捕捉实际功耗。我们的发现揭示了不同量化设置中能效、推理速度和精度之间的权衡，突出了优化LLM在资源受限环境中部署的配置。通过将硬件级能耗分析与LLM基准测试相结合，本研究为可持续AI提供了可操作的见解，填补了现有关于能耗感知LLM部署研究中的关键空白。|
|**2025-04-04**|**Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation**|Weitao Li et.al.|[2504.03165](http://arxiv.org/abs/2504.03165)|**[link](https://github.com/tsinghua-dhy/edc-2-rag)**|近年来，检索增强生成（RAG）已成为在大语言模型（LLM）推理过程中进行知识整合的广泛采用的方法。然而，当前的RAG实现面临在检索内容中有效处理噪声、重复和冗余的挑战，这主要是因为它们在利用细粒度文档间关系方面的能力有限。为了解决这些局限性，我们提出了一种基于高效动态聚类文档压缩框架（EDC²-RAG），该框架有效地利用了潜在文档间关系，同时去除无关信息和冗余内容。我们在广泛使用的知识-QA和幻觉检测数据集上验证了我们的方法，该方法基于GPT-3.5构建。结果表明，这种方法在各种场景和实验设置中实现了持续的性能提升，显示出强大的鲁棒性和适用性。我们的代码和数据集可在https://github.com/Tsinghua-dhy/EDC-2-RAG找到。|
|**2025-04-03**|**Narrative Studio: Visual narrative exploration using LLMs and Monte Carlo Tree Search**|Parsa Ghaffari et.al.|[2504.02426](http://arxiv.org/abs/2504.02426)|**[link](https://github.com/parsaghaffari/narrative-studio)**|交互式叙事受益于规划和探索多个“如果”情景。现代大型语言模型（LLM）是构思和探索的有用工具，但当前的基于聊天的用户界面限制了用户只能在一个线性流程中进行操作。为了解决这一限制，我们提出了叙事工作室——一个新型的浏览器内叙事探索环境，它具有树状界面，允许从用户定义的故事点进行分支探索。每个分支都通过系统定义和用户定义的提示引导的迭代LLM推理进行扩展。此外，我们采用蒙特卡洛树搜索（MCTS）根据用户指定的标准自动扩展有希望的叙事路径，从而实现更丰富和健壮的故事发展。我们还允许用户通过将生成的文本扎根于表示故事中的角色和环境实体图来增强叙事的连贯性。|
|**2025-04-01**|**SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching**|Yuxuan Zhu et.al.|[2504.00970](http://arxiv.org/abs/2504.00970)|null|在处理长文本时，大型语言模型面临着巨大的计算和内存挑战。在推理过程中，高效管理存储自回归生成中间激活的关键值（KV）缓存对于减少内存开销和提高计算效率至关重要。传统的基于标记的KV缓存方法忽略了语义信息，独立地对待标记，而不考虑它们之间的语义关系。与此同时，现有的保留语义的KV缓存管理方法往往存在大量内存使用和高首次标记时间。为了解决这些局限性，我们提出了SentenceKV，这是一种新型的基于句子的语义KV缓存方法，旨在提高推理效率的同时保持语义连贯性。在预填充阶段，SentenceKV根据句子级别的语义相似性对标记进行分组，将句子表示压缩成简洁的语义向量，并直接存储在GPU上，而单个KV对则卸载到CPU。在解码阶段，SentenceKV通过选择性地检索与语义相关的句子级别KV条目，利用预填充阶段的语义向量和解码阶段的查询之间的语义相似性来生成标记。这确保了高效且上下文准确的预测，最小化了将冗余或不相关数据加载到GPU内存中的情况，并显著减少了内存开销，同时保持了稳定的推理延迟，即使对于非常长的上下文也是如此。在PG-19、LongBench和Needle-In-A-Haystack等基准测试上的广泛评估表明，SentenceKV在效率和内存使用方面都显著优于最先进的方法，同时没有牺牲模型精度。|
|**2025-03-31**|**ReaLM: Reliable and Efficient Large Language Model Inference with Statistical Algorithm-Based Fault Tolerance**|Tong Xie et.al.|[2503.24053](http://arxiv.org/abs/2503.24053)|**[link](https://github.com/2000012835xt/realm-dac)**|**随着对高效大型语言模型（LLM）推理的需求推动，专用加速器的开发得到了快速发展。由于加速器容易受到老化、变异等因素的影响而出现硬件故障，现有的加速器设计通常预留较大的电压余量或利用基于算法的容错（ABFT）技术来确保LLM推理的正确性。然而，先前的方法往往忽略了LLM固有的容错能力，导致计算和能源开销较高。为了实现可靠且高效的LLM推理，本文提出了一种新颖的算法/电路协同设计框架，命名为ReaLM。我们首次通过针对代表性LLM和自然语言理解任务的规模化错误注入研究，系统地描述了LLM的容错能力。随后，我们提出了一种基于统计的ABFT算法，充分利用错误鲁棒性，尽可能减少错误恢复。我们还定制了错误检测电路，以实现低成本在线收集错误统计。大量实验表明，ReaLM仅增加1.42%的电路面积和1.79%的功耗，就能将困惑度下降从18.54降低到0.29。与现有方法相比，ReaLM在不同工作电压下持续降低恢复成本，在不影响LLM性能的情况下，将能源效率提高高达35.83%。我们的错误注入代码可在https://github.com/PKU-SEC-Lab/ReaLM_DAC25/获取。**|
|**2025-03-31**|**MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration**|Tatsuya Kubo et.al.|[2503.23817](http://arxiv.org/abs/2503.23817)|null|通用矩阵-向量乘法（GeMV）仍然是大型语言模型（LLM）推理中的关键延迟瓶颈，即使在量化低比特模型中也是如此。处理-使用-DRAM（PUD），一种类似内DRAM计算技术，有潜力将设备上的DRAM作为GeMV引擎重新利用，为广泛的消费设备提供额外的吞吐量处理能力，而不需要修改DRAM。然而，将PUD应用于LLM推理管道中的GeMV操作，在DRAM计算前后都会产生显著的开销，从而削弱了其高吞吐量处理能力的优势。本文提出了MVDRAM，这是第一个使用未修改的DRAM加速低比特LLM推理中GeMV操作的实际系统。通过利用GeMV操作中的数据共享模式和数学线性，MVDRAM协调处理器和DRAM以消除与常规PUD方法中所需的预先安排输入和输出位转置相关的成本。我们的实验评估使用了四个DDR4 DRAM模块，结果显示MVDRAM在低比特（低于4比特）LLM的GeMV操作中实现了与基于处理器的实现相当甚至更好的推理速度。特别是，MVDRAM实现了高达7.29倍的加速和30.5倍的能量效率。对于端到端LLM推理，对于2比特和4比特量化低比特模型，MVDRAM分别实现了2.18倍和1.31倍的吞吐量提升，以及3.04倍和2.35倍的能量效率。MVDRAM通过证明标准DRAM作为LLM加速器的可行性，有可能重新定义AI硬件领域。|
|**2025-03-30**|**Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference**|Wei Tao et.al.|[2503.23294](http://arxiv.org/abs/2503.23294)|null|最近，大型语言模型（LLMs）能够处理越来越长的上下文。然而，过长的上下文可能会引起无法容忍的推理延迟和GPU内存使用。现有方法基于令牌粒度对LLMs中的关键值（KV）缓存进行混合精度量化，这在搜索过程中耗时且在计算过程中硬件效率低下。本文介绍了一种名为Cocktail的新方法，该方法采用块自适应混合精度量化来优化KV缓存。Cocktail由两个模块组成：块级量化搜索和块级KV缓存计算。块级量化搜索基于对应上下文块与查询之间的相似度分数，快速确定KV缓存块的优化位宽配置，同时保持模型精度。此外，块级KV缓存计算在量化之前重新排序KV缓存块，避免了推理计算中混合精度量化引起的硬件效率低下。大量的实验表明，Cocktail在各种模型和数据集上优于最先进的KV缓存量化方法。|
|**2025-03-28**|**Niyama : Breaking the Silos of LLM Inference Serving**|Kanishk Goel et.al.|[2503.22562](http://arxiv.org/abs/2503.22562)|null|随着大型语言模型（LLMs）的广泛应用，实现了具有非常不同延迟要求的多样化应用。现有的LLM服务框架依赖于孤立的基础设施和粗粒度的工作负载隔离——交互式和批量——导致资源利用效率低下，且对细粒度服务质量（QoS）区分的支持有限。这导致了运营效率低下、过度配置和流量高峰期间的糟糕负载管理。我们提出了Niyama，这是一个新颖的QoS驱动推理服务系统，它能够实现共享基础设施上不同工作负载的高效协同调度。Niyama引入了细粒度的QoS分类，允许应用程序指定精确的延迟要求，并根据实时系统状态动态调整调度决策。利用LLM推理的可预测执行特征，Niyama实现了一种动态分块机制，以提高整体吞吐量同时保持严格的服务质量保证。此外，Niyama采用了一种平衡公平性和效率的混合优先级策略，并采用选择性请求降级，在过载条件下实现优雅的服务降级。我们的评估表明，与当前的孤立部署相比，Niyama将服务能力提高了32%，同时保持了服务质量保证。值得注意的是，在极端负载下，与当前策略相比，我们的系统将服务水平协议（SLO）违规降低了十倍。|
|**2025-03-28**|**Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding**|Aayush Gautam et.al.|[2504.00030](http://arxiv.org/abs/2504.00030)|null|推测解码通过使用较小的草稿模型来提出标记，然后由较大的目标模型进行验证，从而加速大型语言模型（LLM）的推理。然而，选择最佳推测长度对于在最大化速度的同时最小化浪费的计算至关重要。我们引入了\textit{GammaTune}和\textit{GammaTune+}，这两种无需训练的自适应算法，利用基于启发式的切换机制根据标记接受率动态调整推测长度。在SpecBench的多个任务和模型对上进行评估，我们的方法优于其他基于启发式的方法和固定长度的推测解码，使用\textit{GammaTune}实现了平均15\%（±5\%）的速度提升，使用\textit{GammaTune+}实现了16\%（±3\%）的速度提升，同时降低了性能的波动性。这使得\textit{GammaTune}成为现实世界部署中稳健且高效的解决方案。|
|**2025-03-25**|**LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation**|Han Chen et.al.|[2503.19950](http://arxiv.org/abs/2503.19950)|**[link](https://github.com/concyclics/logquantkv)**|**我们引入了LogQuant，这是一种革命性的2位量化技术，用于大型语言模型（LLM）推理中的KV缓存，在保持卓越性能的同时实现了显著的内存节省。先前的方法要么假设后续标记更重要，要么尝试根据早期的注意力模式预测重要标记。然而，这两种方法都可能导致性能瓶颈或频繁的误预测。LogQuant采取了不同的方法。通过应用基于日志的过滤机制，它有选择地在整个上下文中压缩KV缓存，与现有方法相比，在相同的甚至更小的内存占用下实现了更好的性能。在基准测试中，它将吞吐量提高了25%，并将批量大小提高了60%，而没有增加内存消耗。对于数学和代码补全等具有挑战性的任务，LogQuant在相同的压缩比下将准确率提高了40%至200%，优于可比技术。LogQuant可以无缝集成到流行的推理框架中，如Python的transformers库。实现可以在https://github.com/Concyclics/LogQuantKV找到。**|
|**2025-03-24**|**xKV: Cross-Layer SVD for KV-Cache Compression**|Chi-Chih Chang et.al.|[2503.18893](http://arxiv.org/abs/2503.18893)|**[link](https://github.com/abdelfattah-lab/xkv)**|**大型语言模型（LLMs）具有长上下文窗口，能够实现强大的应用，但代价是高内存消耗来存储键值状态（KV-Cache）。最近的研究试图将多个层的KV-cache合并为共享表示，但这些方法要么需要昂贵的预训练，要么依赖于层间高每令牌余弦相似性的假设，这在实践中通常不成立。我们发现，KV-Cache的多层中主导的奇异向量非常一致。利用这一发现，我们提出了xKV，一种简单的后训练方法，该方法对分组层的KV-Cache应用奇异值分解（SVD）。xKV将多个层的KV-Cache整合到一个共享的低秩子空间中，显著减少了KV-Cache的大小。通过在RULER长上下文基准测试上对广泛使用的LLMs（例如Llama-3.1和Qwen2.5）进行的大量评估，xKV实现了比最先进的层间技术高达6.8倍的压缩率，同时提高了2.7%的准确率。此外，xKV与新兴的多头潜在注意力（MLA）（例如DeepSeek-Coder-V2）兼容，在编码任务上实现了显著的3倍压缩率，而性能没有下降。这些结果突出了xKV在解决长上下文LLM推理内存瓶颈方面的强大能力和多功能性。我们的代码在以下网址公开：https://github.com/abdelfattah-lab/xKV。**|
|**2025-03-24**|**Reimagining Memory Access for LLM Inference: Compression-Aware Memory Controller Design**|Rui Xie et.al.|[2503.18869](http://arxiv.org/abs/2503.18869)|null|大型语言模型（LLM）的推理效率常常受到大量内存带宽和容量需求的限制。现有技术，如剪枝、量化和专家/深度混合，在略微降低推理质量的同时，减少了内存容量和/或带宽消耗。本文提出了一种设计方案，通过增强AI加速器中的片上内存控制器来进一步缓解内存瓶颈，实现两个主要目标：（1）通过无损块压缩（例如，LZ4和ZSTD）模型权重和键值（KV）缓存，在不影响推理质量的情况下，显著降低内存容量和带宽使用；（2）通过上下文相关的动态量化，使内存带宽和能耗成比例增长。这些目标通过在片上内存控制器中配备改进权重和KV缓存细粒度位级可访问性和压缩性的机制来实现，该机制通过LLM感知的内存放置和表示配置。在公开可用的LLM上的实验结果表明，这种方法的有效性，显示出模型权重内存占用减少了25.2%，KV缓存减少了46.9%。此外，我们的4GHz和32车道（7nm）的硬件原型实现了8TB/s的吞吐量，并且面积开销适中（小于3.8mm²），这突显了LLM感知内存控制作为高效大规模推理的关键的可行性。|
|**2025-03-24**|**Jenga: Effective Memory Management for Serving LLM with Heterogeneity**|Chen Zhang et.al.|[2503.18292](http://arxiv.org/abs/2503.18292)|null|大型语言模型（LLMs）虽然应用广泛，但其运行成本较高，尤其是在推理工作负载增长的情况下。为了降低成本，通过高效管理GPU内存来最大化请求批次大小至关重要。尽管PagedAttention最近被提出以提高内存管理的效率，但我们发现现代LLM架构在嵌入维度、注意力和访问模式上的日益异质性为内存分配带来了新的挑战。在本文中，我们提出了Jenga，这是一个针对LLM中异构嵌入的新型内存分配框架。Jenga解决了两个关键挑战：（1）在管理不同大小的嵌入时最小化内存碎片；（2）根据不同层的特定标记依赖模式实现灵活的缓存和淘汰策略。Jenga采用两级内存分配器，利用嵌入大小的最小公倍数（LCM）来优化内存使用，并提供API来表示特定层的缓存逻辑，以增强内存重用。我们在vLLM上实现了Jenga，这是一个最先进的LLM推理引擎，并使用各种LLMs、数据集和GPU配置对其进行了评估。评估结果表明，Jenga将GPU内存利用率提高了高达79.6%，并将服务吞吐量提高了高达4.92倍（平均提高1.80倍）。|
|**2025-03-23**|**WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference**|Youhui Zuo et.al.|[2503.17922](http://arxiv.org/abs/2503.17922)|**[link](https://github.com/optim996/WindowKV)**|**随着大型语言模型（LLMs）在长上下文推理能力方面的进步，KV缓存已成为基础组件之一。然而，其大量的GPU内存消耗使得KV缓存压缩成为在工业场景中实现高效LLM推理的关键技术。尽管最近的研究集中在优化KV缓存占用的内存，但它们忽略了两个关键因素：保留语义连贯性和在压缩过程中考虑特定任务的特性。为了解决这些限制，我们提出了一种新颖的任务自适应KV缓存窗口选择方法，即WindowKV。WindowKV根据特定任务的特性动态选择由连续标记组成的局部语义窗口，确保保留的KV缓存捕获连续且重要的上下文。此外，我们引入了组内层KV缓存索引共享策略以减少计算开销，在性能和效率之间实现平衡。我们在LongBench基准测试中严格评估了WindowKV，结果表明，它保持了与完整KV缓存保留相当的性能，同时只使用了原始KV缓存的12%，显著降低了内存需求。此外，我们的方法在针尖在草堆中的评估中实现了最先进的成果，突出了其有效性和鲁棒性。**|
|**2025-03-22**|**PipeBoost: Resilient Pipelined Architecture for Fast Serverless LLM Scaling**|Chongpeng Liu et.al.|[2503.17707](http://arxiv.org/abs/2503.17707)|null|本文提出了一种名为PipeBoost的低延迟LLM服务系统，适用于多GPU（无服务器）集群。该系统能够在应对突发请求时快速启动推理服务，而无需预先过度配置GPU。许多LLM推理任务依赖于相同的基模型（例如，LoRA）。为了利用这一点，PipeBoost在模型加载和推理阶段都引入了容错管道并行处理。这种方法最大化了总体的PCIe带宽和GPU间的并行计算，从而加快了第一个token的生成。PipeBoost还引入了恢复技术，通过利用多个GPU的共享优势，实现了不间断的推理服务。实验结果表明，与最先进的低延迟LLM服务系统相比，PipeBoost将推理延迟降低了31%至49.8%。对于某些模型（例如，OPT-1.3B），PipeBoost实现了数百微秒的冷启动延迟。|
|**2025-03-21**|**Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation**|Jingzhi Fang et.al.|[2503.16893](http://arxiv.org/abs/2503.16893)|null|随着大型语言模型（LLMs）在许多任务中展现出巨大成功，它们被应用于各种场景。尽管许多研究集中在单个LLM应用效率（例如，卸载、请求调度、并行策略选择）上，但多LLM应用却得到了较少关注，尤其是在离线推理场景中。在这项工作中，我们旨在提高单节点多GPU环境中多LLM应用的离线端到端推理效率。这个问题涉及两个关键决策：（1）确定每次要运行哪些LLMs（我们可能不会同时运行所有模型），以及（2）为每个LLM选择一个并行策略。这个问题是NP难问题。简单的解决方案可能效果不佳，因为模型完成一系列请求的运行时间取决于请求负载和所选的并行策略，并且它们缺乏运行时间的准确模型。由于LLM的输出长度在运行前是未知的，为了估计模型运行时间，我们提出了一种先采样后模拟的方法，该方法首先通过从预先从大量数据集中获得的经验累积函数中采样来估计输出长度，然后根据模拟进行LLM推理过程。基于模拟，我们估计每迭代的延迟以获得总延迟。我们提出了一种贪婪方法来优化LLMs在应用跨GPU的调度。然后，我们提出一个名为SamuLLM的框架，该框架包含两个阶段：规划阶段，为应用调用贪婪方法，运行阶段，运行应用并根据运行时信息动态调整模型调度。在3个应用和一个混合应用上的实验表明，与竞争对手相比，SamuLLM可以实现1.0-2.4倍的端到端加速。|
|**2025-03-21**|**V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms**|Javier J. Poveda Rodrigo et.al.|[2503.17422](http://arxiv.org/abs/2503.17422)|null|近年来，大型语言模型（LLMs）的指数级增长依赖于基于GPU的系统。然而，CPU正成为一种灵活且成本更低的替代方案，尤其是在针对推理和推理工作负载时。鉴于其开放和供应商中立的指令集架构（ISA），RISC-V在这一领域迅速获得关注。然而，由于需要特定领域的调整，RISC-V硬件对于LLM工作负载以及相应的软件生态系统尚未完全成熟和优化。本文旨在填补这一空白，重点关注在Sophon SG2042（首款商用多核RISC-V CPU，具备向量处理能力）上优化LLM推理。在针对推理进行了优化的两种最近最先进的LLM——DeepSeek R1 Distill Llama 8B和DeepSeek R1 Distill QWEN 14B上，我们实现了每秒4.32/2.29个token的token生成速度和每秒6.54/3.68个token的提示处理速度，与基线相比速度提升了高达2.9倍/3.0倍。|
|**2025-03-20**|**SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models**|Fahao Chen et.al.|[2503.15921](http://arxiv.org/abs/2503.15921)|null|投机解码已被证明是加速大型语言模型（LLM）推理的有效方法，通过使用小型投机模型（SSM）在所谓的投机阶段生成候选标记，这些标记随后在验证阶段由LLM进行验证。然而，目前最先进的投机解码方法存在三个关键限制：使用同质SSM处理不同难度的请求、缺乏对批量处理的鲁棒支持，以及对投机和验证阶段的整体优化不足。在本文中，我们介绍了SPIN，这是一个基于投机解码的高效LLM推理服务系统，旨在通过三个主要创新来应对这些挑战。首先，SPIN通过使用多个异构SSM来改进标记投机，并采用基于学习的算法进行SSM选择，该算法无需事先了解请求难度。其次，SPIN采用请求分解方法，以最小化LLM验证期间的批处理开销。最后，SPIN通过在GPU上对投机和验证阶段的执行进行流水线处理来协调这两个阶段，以实现进一步的加速。实验结果表明，SPIN显著优于现有方法，实现了约2.28倍的性能提升。|
|**2025-03-19**|**Automated Non-Functional Requirements Generation in Software Engineering with Large Language Models: A Comparative Study**|Jomar Thomas Almonte et.al.|[2503.15248](http://arxiv.org/abs/2503.15248)|null|在软件开发早期忽视非功能性需求（NFRs）可能导致关键挑战。尽管NFRs很重要，但它们往往被忽视或难以识别，影响软件质量。为了支持需求工程师提取NFRs，我们开发了一个框架，该框架利用大型语言模型（LLMs）从功能性需求（FRs）中推导出以质量为导向的NFRs。使用基于Deno的管道中的自定义提示技术，该系统为每个功能性需求识别相关的质量属性，并生成相应的NFRs，有助于系统性的整合。一个关键方面是评估这些生成需求的质量和适用性。LLMs能否产生高质量的NFR建议？使用34个功能性需求——作为3,964个FRs的代表性子集——LLMs根据ISO/IEC 25010:2023标准推断出适用的属性，生成了1,593个NFRs。横向评估涵盖了三个维度：NFR的有效性、质量属性的适用性以及分类精度。十位平均拥有13年经验的行业软件质量评估员评估了其中一部分的相关性和质量。评估显示，LLM生成的NFRs与专家评估之间有很强的吻合度，在1-5分的评分尺度上，中位数有效性和适用性分数分别为5.0（平均值分别为4.63和4.59）。在分类任务中，80.4%的LLM分配的属性与专家选择相匹配，8.3%接近正确，11.3%错误。对八个LLM的比较分析突出了性能的差异性，其中gemini-1.5-pro表现出最高的属性准确性，而llama-3.3-70B实现了更高的有效性和适用性分数。这些发现为使用LLMs进行自动NFR生成提供了可行性见解，并为进一步探索AI辅助需求工程奠定了基础。|
|**2025-03-19**|**Communication-Efficient Distributed On-Device LLM Inference Over Wireless Networks**|Kai Zhang et.al.|[2503.14882](http://arxiv.org/abs/2503.14882)|null|大型语言模型（LLMs）在各个应用领域展现出显著的成功，但它们巨大的规模和计算需求给资源受限的边缘设备部署带来了重大挑战。为了解决这个问题，我们提出了一种新颖的分布式边缘设备LLM推理框架，该框架利用张量并行性将一个LLM的神经网络张量（例如，权重矩阵）分割到多个边缘设备上以进行协作推理。张量并行性中的一个关键挑战是频繁的全量聚合操作，用于在参与设备之间汇总中间层输出，这带来了巨大的通信开销。为了缓解这个瓶颈，我们提出了一种空中计算（AirComp）方法，该方法利用无线多接入信道的模拟叠加特性来执行快速全量聚合步骤。为了利用边缘设备的异构计算能力并减轻通信失真，我们研究了联合模型分配和发射接收器优化问题，以最小化平均传输误差。由此产生的混合时间尺度随机非凸优化问题是不可解的，我们提出了一种高效的两阶段算法来解决它。此外，我们证明了所提出的算法几乎必然收敛到原始问题的平稳点。综合仿真结果表明，所提出的框架优于现有的基准方案，实现了高达5倍的推理速度加速和推理精度的提高。|
|**2025-03-18**|**PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play**|Wei Fang et.al.|[2503.14432](http://arxiv.org/abs/2503.14432)|null|大型语言模型（LLMs）越来越多地与专用外部工具集成，然而许多任务需要零样本工具使用，同时文档可能最少或含糊不清。现有解决方案依赖于手动重写或标记数据进行验证，这使得它们在真正的零样本环境中无法应用。为了解决这些挑战，我们提出了PLAY2PROMPT，一个自动化的框架，该框架系统地“玩耍”每个工具，以探索其输入输出行为。通过这种迭代试错过程，PLAY2PROMPT优化工具文档并生成使用示例，而无需任何标记数据。这些示例不仅指导LLM推理，还作为验证来进一步增强工具的利用。在真实世界任务上的大量实验表明，PLAY2PROMPT显著提高了开放和封闭模型在零样本工具性能上的提升，为特定领域的工具集成提供了一种可扩展且有效的解决方案。|
|**2025-03-17**|**xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference**|Maximilian Beck et.al.|[2503.13427](http://arxiv.org/abs/2503.13427)|**[link](https://github.com/nx-ai/mlstm_kernels)**|**最近，通过在推理时投入大量计算资源，解决了利用大型语言模型（LLMs）推理推理、数学和编码问题取得了突破。因此，推理速度是LLM架构的最重要属性之一，对高效且快速的LLMs的需求日益增长。最近，基于xLSTM架构的LLMs已成为Transformers的有力替代品，提供与序列长度成线性关系的计算扩展和恒定的内存使用，这两者都是高效推理高度期望的特性。然而，这样的基于xLSTM的LLMs尚未扩展到更大的模型，并且还没有在推理速度和效率方面进行评估和比较。在这项工作中，我们介绍了xLSTM 7B，这是一个具有70亿参数的LLM，它结合了xLSTM的架构优势以及针对快速和高效推理的优化。我们的实验表明，xLSTM 7B在下游任务上的性能与其他类似规模的LLMs相当，同时比基于Llama和Mamba的LLMs提供了显著更快的推理速度和更高的效率。这些结果将xLSTM 7B确立为最快的70亿参数LLM，为需要大量测试时计算的任务提供了解决方案。我们的工作突出了xLSTM作为构建在LLM推理重用方法基础上的基础架构的潜力。我们的模型权重、模型代码和训练代码是开源的。**|
|**2025-03-17**|**VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for LLM-Driven Verilog Coding**|Zeng Wang et.al.|[2503.13116](http://arxiv.org/abs/2503.13116)|null|大型语言模型（LLMs）在编码方面具有巨大潜力，然而对于Verilog等特定语言，使用经过筛选的数据进行微调（FT）是至关重要的。使用专有知识产权（IP）进行FT存在严重风险，因为FT数据可能通过LLM推理泄露。这导致设计公司面临一个关键的困境：在试图构建对外可访问的、提供具有竞争力的Verilog编码的LLMs的同时，如何利用内部IP来提高FT效用，同时确保IP保护？在文献中首次研究这一困境。我们使用LLaMA 3.1-8B，在基线Verilog数据集（RTLCoder）上进行内部FT，并补充了我们自己的内部IP，该IP通过多次tape-out得到验证。为了严格评估IP泄露，我们量化了生成代码与我们的内部IP之间的结构相似性（AST/Dolos）和功能等价性（Synopsys Formality）。我们表明，我们的IP确实可能泄露，证实了这一威胁。作为防御措施，我们评估了Verilog代码的逻辑锁定（ASSURE）。这提供了一定程度的安全保护，但降低了IP在FT中的效用，并降低了LLM的性能。我们的研究表明，需要新的策略，这些策略既有效又尽可能不干扰FT，这对于设计公司充分利用其专有IP进行LLM驱动的Verilog编码至关重要。|
|**2025-03-17**|**Mitigating KV Cache Competition to Enhance User Experience in LLM Inference**|Haiying Shen et.al.|[2503.13773](http://arxiv.org/abs/2503.13773)|null|在大型语言模型（LLM）服务中，KV缓存（KVC）瓶颈导致高尾时延第一次发令（TTFT）和高尾时延每令牌（TBT），损害用户体验，尤其是在对时间敏感的应用中。然而，同时满足TTFT和TBT服务水平目标（SLOs）是具有挑战性的。为了解决这个问题，我们提出了一种名为CacheOPT的系统，用于缓解KV缓存竞争，该系统基于我们的测量结果，并融合了新颖的组件。首先，它估计请求的输出长度，以高概率限定偏差，并根据请求到达率进行调整。其次，它将估计的KVC需求分配给请求，并重用其他请求分配的KVC以避免抢占并减少等待时间。第三，它在请求耗尽其分配之前而不是当时主动分配KVC，并在全局范围内预留KVC以防止抢占。第四，它选择具有长TBT SLO、长作业剩余时间和短抢占时间的请求进行抢占。第五，它选择在交换和重新计算之间选择最短延迟策略进行抢占。实验表明，CacheOPT实现了高达3.29倍和2.83倍的尾部TBT和尾部TTFT降低，TTFT和TBT SLO达成率提高47%和53%，并且比最先进的方法支持高达1.58倍的请求到达率。|
|**2025-03-17**|**AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference Serving for Diverse Applications**|Haiying Shen et.al.|[2503.13737](http://arxiv.org/abs/2503.13737)|null|本文研究了一种混合提示场景下的大语言模型（LLM）推理服务系统，该系统支持使用短提示和长提示以及异构迭代时间SLO的多样化应用。为了在处理长提示时提高吞吐量，先前的研究引入了分块方法，但尚未解决异构SLO的问题。为了解决这个问题，我们提出了AccelGen，一个具有异构SLO保证的高吞吐量LLM推理服务系统，适用于多种应用。AccelGen引入了四个核心组件：（1）SLO保证的动态分块，通过动态调整分块大小，在满足迭代级SLO的同时最大化GPU计算利用率；（2）基于迭代级SLO的任务优先级，优先处理紧SLO请求，并批量处理具有相似SLO的请求；（3）多资源感知批量处理，选择排队请求以最大化GPU计算资源和键值缓存（KVC）的利用率。基于跟踪驱动的实际实验表明，与现有方法相比，AccelGen实现了1.42-11.21倍更高的吞吐量，1.43-13.71倍更高的好吞吐量，37-90%更高的SLO达成率，以及1.61-12.22倍更低的响应延迟。其性能接近Oracle，最优地最大化了好吞吐量。|
|**2025-03-17**|**ML-SpecQD: Multi-Level Speculative Decoding with Quantized Drafts**|Evangelos Georganas et.al.|[2503.13565](http://arxiv.org/abs/2503.13565)|null|推测性解码（SD）已成为一种在不牺牲16位模型推理准确性的情况下加速LLM推理的方法。在典型的SD设置中，想法是使用全精度、小型、快速的模型作为“草案”来生成接下来的几个标记，并使用“目标”大型模型来验证由草案生成的标记。这种方法的有效性高度依赖于草案生成标记的接受率以及草案与目标模型之间的相对标记吞吐量。尽管如此，一个高效的SD管道需要预先训练并对草案模型与目标模型进行对齐，这使得它在LLM推理中以一种即插即用的方式变得不切实际。在这项工作中，我们提出使用MXFP4模型作为即插即用的草案，因为MXFP4权重仅量化（WOQ）只是将BF16目标模型权重直接转换到MXFP4。在实践中，我们的即插即用解决方案将速度提升至BF16基准的2倍。然后我们追求进一步加速的机会：MXFP4草案标记生成本身可以通过使用另一个更小的草案进行推测性解码来加速。我们称我们的方法为ML-SpecQD：具有量化草案的多级推测性解码，因为它递归地应用推测以加速草案标记生成。结合多级推测性解码与MXFP4量化草案，我们超越了最先进的推测性解码，速度提升了BF16基准的2.72倍。|
|**2025-03-15**|**TFHE-Coder: Evaluating LLM-agentic Fully Homomorphic Encryption Code Generation**|Mayank Kumar et.al.|[2503.12217](http://arxiv.org/abs/2503.12217)|null|在环上全同态加密（TFHE）允许在不解密的情况下对加密数据进行计算，使其成为安全且机密计算的基础。尽管其在保护隐私的机器学习、安全多方计算、私有区块链交易和安全医疗诊断方面的潜力巨大，但由于其密码学复杂性以及可用性挑战，其应用仍然有限。虽然存在各种TFHE库和编译器，但实际代码生成仍然是一个难题。我们提出了一种编译器集成框架，用于评估LLM推理和代理优化在TFHE代码生成中的应用，重点关注逻辑门和ReLU激活。我们的方法评估了开源和闭源LLM的错误率、可编译性和结构相似性。结果表明，现成的模型存在重大局限性，而如检索增强生成（RAG）和少量样本提示等代理优化可以减少错误并提高代码的准确性。这项工作为TFHE代码生成建立了第一个基准，展示了当LLM与特定领域反馈相结合时，如何弥合FHE代码生成中的专业知识差距。|
|**2025-03-14**|**Examples as the Prompt: A Scalable Approach for Efficient LLM Adaptation in E-Commerce**|Jingying Zeng et.al.|[2503.13518](http://arxiv.org/abs/2503.13518)|null|提示大型语言模型提供了一种高效的方式来引导输出生成，而无需进行显式的模型训练。在电子商务领域，基于提示的应用在查询理解、推荐系统和客户支持等任务中得到了广泛应用。然而，将LLM适应不同任务通常需要领域专家进行大量的提示工程，并频繁更新以适应不断变化的企业需求。此外，制作完全无偏见的自然语言提示对于人类来说仍然是一个挑战。为了解决这些挑战，我们提出了一种新颖的框架，即“示例作为提示”（EaP），它利用标注数据来增强提示。具体来说，EaP自动选择最具代表性的示例，以最大化LLM的少样本能力。由于其无监督的示例选择和适应潜在数据分布变化的能力，EaP效率很高。我们在四个真实世界的生产用例上验证了EaP，证明其性能与领域专家手工设计的提示相当甚至更优。此外，我们引入了EaP_lite，它完全用标注示例替换了提示中的自然语言组件。EaP_lite在不影响性能的情况下，将LLM推理速度提高了高达70%。最新的在线A/B测试显示，使用EaP和EaP_lite进行数据标注可以将综合收入增益提高0.06%。|
|**2025-03-13**|**Collaborative Speculative Inference for Efficient LLM Inference Serving**|Luyao Gao et.al.|[2503.10325](http://arxiv.org/abs/2503.10325)|null|推测推理是一种有前景的范式，它使用小规模推测模型（SSM）作为起草者来生成草稿令牌，随后由目标大型语言模型（LLM）并行验证。这种方法通过减少LLM推理的延迟和成本，同时保持生成质量，提高了推理服务的效率。然而，现有的推测方法面临一些关键挑战，包括资源利用效率低下和草稿接受度有限，这限制了它们的可扩展性和整体有效性。为了克服这些障碍，我们提出了CoSine，这是一个新型的推测推理系统，它将序列推测解码与并行验证解耦，使多个节点能够高效协作。具体来说，CoSine根据起草者的专长将推理请求路由到专门的起草者，并采用基于置信度的令牌融合机制来综合来自协作起草者的输出，确保高质量的草稿生成。此外，CoSine动态地以流水线方式调度推测解码和验证的执行，使用批量调度来选择性分组请求，并采用自适应推测控制来最小化空闲时间。通过优化并行工作流程并通过异构节点协作，CoSine在实时中平衡草稿生成和验证的吞吐量，从而最大化资源利用。实验结果表明，与最先进的推测方法相比，CoSine实现了更优越的性能。值得注意的是，在等效的资源成本下，与基线方法相比，CoSine实现了高达23.2%的延迟降低和32.5%的吞吐量增加。|
|**2025-03-12**|**Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**|Xinjian Luo et.al.|[2503.09291](http://arxiv.org/abs/2503.09291)|null|现代大型语言模型（LLMs）的推理过程需要巨大的计算资源，使得它们在消费级设备上的部署变得不可行。为了解决这个问题，最近的研究提出了分布式LLM推理框架，该框架采用分割学习原理，允许在资源受限的硬件上协作进行LLM推理。然而，将LLM层分布到参与者之间需要传输中间输出，这可能会对原始输入提示引入隐私风险——这是一个在文献中尚未彻底探讨的关键问题。在本文中，我们通过设计和评估旨在从中间LLM输出中重建输入提示的三种提示推理攻击，严格审查了分布式LLM推理框架的隐私漏洞。这些攻击在各种查询和数据约束下开发，以反映不同的现实世界LLM服务场景。具体来说，第一种攻击假设有无限的查询预算并可以访问与目标提示具有相同分布的辅助数据集。第二种攻击也利用无限的查询，但使用与目标提示分布不同的辅助数据集。第三种攻击在最严格的场景下进行，具有有限的查询预算且没有可用的辅助数据集。我们在多种LLMs上评估了这些攻击，包括Llama-3.2和Phi-3.5等最先进模型，以及GPT-2和BERT等广泛使用的模型，以进行对比分析。我们的实验表明，前两种攻击的重建准确率超过90%，而第三种攻击在严格约束下的准确率通常在50%以上。这些发现突出了分布式LLM推理框架中的隐私风险，对它们在现实世界应用中的部署发出了强烈的警告。|
|**2025-03-11**|**TokenSim: Enabling Hardware and Software Exploration for Large Language Model Inference Systems**|Feiyang Wu et.al.|[2503.08415](http://arxiv.org/abs/2503.08415)|**[link](https://github.com/pku-lemonade/TokenSim)**|随着对大型语言模型（LLM）服务需求的不断增加，LLM推理系统的优化和性能分析取得了显著进步。随着这些模型在广泛的应用中变得不可或缺，对高效和可扩展的服务解决方案的需求也呈指数级增长。本研究引入了TokenSim，这是一个专为LLM推理设计的综合硬件和软件探索系统。TokenSim的特点在于支持可扩展的系统优化，包括调度和内存管理。我们使用真实世界的数据集验证了结果，实现了小于1%的错误率。此外，TokenSim还促进了对于LLM服务系统性能和优化的各种深入探索。|
|**2025-03-11**|**Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM Inference**|Pol G. Recasens et.al.|[2503.08311](http://arxiv.org/abs/2503.08311)|null|大型语言模型已被广泛应用于各种任务中，但它们自回归生成的特性通常会导致推理过程中的资源利用效率低下。虽然批量处理通常用于提高吞吐量，但在达到一定批量大小时，性能提升会趋于平缓，尤其是对于小型模型，这一现象在现有文献中通常被解释为转向计算密集型模式。在本文中，通过对GPU级别的深入分析，我们揭示了大规模批量的推理仍然是内存限制的，由于DRAM带宽饱和是主要瓶颈，导致大部分GPU计算能力未被充分利用。为了解决这个问题，我们提出了一种批量配置顾问（BCA），该顾问通过优化内存分配，在尽可能不影响吞吐量的情况下降低GPU内存需求。释放出的内存和未被充分利用的GPU计算能力可以被并发工作负载所利用。具体来说，我们使用模型复制来提高服务吞吐量和GPU利用率。我们的发现挑战了关于LLM推理的常规假设，为提高资源利用率提供了新的见解和实践策略，尤其是在小型语言模型方面。|
|**2025-03-09**|**Seesaw: High-throughput LLM Inference via Model Re-sharding**|Qidong Su et.al.|[2503.06433](http://arxiv.org/abs/2503.06433)|null|为了提高分布式大型语言模型（LLM）推理的效率，已经提出了各种并行化策略，例如张量和流水线并行。然而，LLM推理的两个阶段——预填充和解码——固有的不同计算特性使得单一的静态并行化策略不足以有效优化这两个阶段。在本工作中，我们提出了Seesaw，这是一个针对吞吐量导向任务优化的LLM推理引擎。Seesaw背后的关键思想是动态模型重新分片，一种促进跨阶段并行化策略动态重新配置的技术，从而在两个阶段都最大化吞吐量。为了减轻重新分片的开销并优化计算效率，我们采用了分层KV缓存缓冲和最小化转换的调度。这些方法协同工作，以减少频繁阶段转换带来的开销，同时确保最大的批处理效率。我们的评估表明，与最广泛使用的最先进LLM推理引擎vLLM相比，Seesaw实现了高达1.78倍（平均1.36倍）的吞吐量提升。|
|**2025-03-09**|**Green Prompting**|Marta Adamska et.al.|[2503.10666](http://arxiv.org/abs/2503.10666)|null|大型语言模型（LLMs）在搜索引擎、代码生成和文本创作等多个领域得到了广泛应用。然而，与其应用相关的一个主要担忧是推理成本高昂，这影响了其可持续性和经济可行性。在本研究中，我们通过实证研究不同提示和响应特征如何直接影响LLM推理能耗。我们利用三个开源的基于transformer的LLMs在三种任务类型——问答、情感分析和文本生成——上进行了实验。对于每次推理，我们分析了提示和响应特征（长度、语义意义、耗时、能耗）。我们的结果表明，即使面对相同任务，模型生成的响应特征各异，从而表现出不同的能耗模式。我们发现，提示长度相对于任务本身的语义意义来说不那么重要。此外，我们还确定了与更高或更低能耗相关的特定关键词，这些关键词在不同任务中有所不同。这些发现强调了提示设计在优化推理效率方面的重要性。我们得出结论，提示的语义意义和某些与任务相关的关键词对推理成本有显著影响，这为创建节能自适应LLMs的进一步探索指明了方向。|
|**2025-03-07**|**Optimizing LLM Inference Throughput via Memory-aware and SLA-constrained Dynamic Batching**|Bowen Pang et.al.|[2503.05248](http://arxiv.org/abs/2503.05248)|**[link](https://github.com/kevinlee1110/dynamic-batching)**|**随着大型语言模型（LLMs）的日益普及，需要能够提供高吞吐量和低延迟的推理服务系统。在内存受限的GPU上部署具有数百亿参数的LLMs，静态批处理方法暴露出显著的局限性。当前的推理服务系统通常将批大小视为固定的超参数，这阻碍了系统条件变化时的实时适应。在本文中，我们提出了一种动态批处理方法，该方法持续监控内存利用率并遵守服务级别协议（SLAs），以实现实时批大小配置调整。该方法包含两个核心组件：一个内存感知的批调度器，它动态分配GPU资源；以及一个延迟反馈机制，在SLA约束下优化解码过程。数值实验表明，与传统静态批处理方法相比，该方法在吞吐量上提高了8%至28%，在容量上提高了22%，同时与现有的推理基础设施保持完全兼容。这些结果突出了动态批处理在平衡计算效率和当代LLM部署场景的服务质量要求方面的有效性。本工作的源代码可在https://github.com/KevinLee1110/dynamic-batching上公开获取。**|
|**2025-03-07**|**SpecServe: Efficient and SLO-Aware Large Language Model Serving with Adaptive Speculative Decoding**|Kaiyu Huang et.al.|[2503.05096](http://arxiv.org/abs/2503.05096)|null|大型语言模型（LLM）服务在实现低推理延迟和满足服务水平目标（SLOs）方面往往面临挑战。利用轻量级模型进行草稿和LLM进行验证的投机解码技术，已成为加速LLM推理的有力手段。然而，现有的投机解码解决方案往往无法适应不同的工作负载和系统环境，导致性能波动和SLO违规。在本文中，我们引入了SpecServe，这是一个高效的LLM推理系统，它能根据实时请求负载和系统配置动态调整投机策略。SpecServe提出了一种理论模型，用于理解和预测在不同场景下投机解码的效率。此外，它还实现了智能草稿和验证算法，以保证在实现高SLO达成率的同时，达到最佳性能。在真实世界LLM跟踪上的实验结果表明，SpecServe始终满足SLOs，并实现了显著的性能提升，相对于最先进的投机推理系统，其速度提高了1.14倍至14.3倍。|
|**2025-03-06**|**Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking**|Yijie Xu et.al.|[2503.04636](http://arxiv.org/abs/2503.04636)|null|随着开源的大型语言模型（LLMs）如Llama3的能力不断增强，开发水印技术以检测其潜在滥用变得至关重要。现有的水印方法要么在LLM推理过程中添加水印，这对开源LLMs来说不适用，要么主要针对分类LLMs而非最近的生成LLMs。将这些水印适应于开源LLMs以进行滥用检测仍然是一个开放性挑战。本研究为开源LLMs定义了两种滥用场景：知识产权（IP）侵权和LLM使用违规。然后，我们探讨了推理时间水印蒸馏和后门水印在这些场景中的应用。我们提出了全面的评估方法来评估各种现实世界的进一步微调场景对水印的影响以及这些水印对LLM性能的影响。我们的实验表明，后门水印可以有效地检测IP侵权，而推理时间水印蒸馏适用于这两种场景，但对进一步微调的鲁棒性较差，并且与后门水印相比，对LLM性能的影响更大。探索更先进的水印方法以检测开源LLMs的滥用应该是未来一个重要的研究方向。|
|**2025-03-06**|**AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large Language Model Services**|Xiaoqi Wang et.al.|[2503.04418](http://arxiv.org/abs/2503.04418)|null|近年来，大型语言模型（LLMs）的快速发展导致其在各个领域的广泛应用和大规模部署。然而，由于它们在推理过程中消耗大量能源和产生碳足迹，其环境影响已成为一个日益关注的问题。现有研究主要集中在推理计算上，而忽略了网络辅助LLM服务系统中碳足迹的分析和优化。为了解决这一差距，我们提出了AOLO，这是一个面向低碳的无线LLM服务的分析和优化框架。AOLO引入了一个全面的碳足迹模型，量化了整个LLM服务链中的温室气体排放，包括计算推理和无线通信。此外，我们制定了一个优化问题，旨在最小化整体碳足迹，通过在体验质量和系统性能约束下联合优化推理输出和传输功率来解决。为了实现这一联合优化，我们利用了脉冲神经网络（SNN）的能量效率，采用SNN作为演员网络，并提出了一种面向低碳的优化算法，即基于SNN的深度强化学习（SDRL）。全面仿真表明，SDRL算法显著降低了整体碳足迹，与基准软演员-评论家相比，实现了18.77%的降低，突显了其在实现更可持续的LLM推理服务方面的潜力。|
|**2025-03-06**|**Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search**|Kou Misaki et.al.|[2503.04412](http://arxiv.org/abs/2503.04412)|null|近期的研究表明，增加推理时间内的计算量可以显著提升大型语言模型（LLMs）的推理能力。尽管重复采样（即生成多个候选输出）是一种非常有效的策略，但它没有利用外部反馈信号进行细化，而这些信号在编码等任务中通常是可用的。在这项工作中，我们提出了自适应分支蒙特卡洛树搜索（AB-MCTS），这是一种新颖的推理时间框架，它通过原则性的多轮探索和利用来推广重复采样。在搜索树的每个节点上，AB-MCTS根据外部反馈信号动态决定是“扩大范围”通过扩展新的候选响应，还是“深入挖掘”通过重新访问现有的响应。我们使用前沿模型在复杂的编码和工程任务上评估了我们的方法。实证结果表明，AB-MCTS在性能上始终优于重复采样和标准MCTS，强调了将LLMs的响应多样性与多轮解决方案细化相结合对于有效推理时间扩展的重要性。|
|**2025-03-06**|**Beyond Memorization: Evaluating the True Type Inference Capabilities of LLMs for Java Code Snippets**|Yiwen Dong et.al.|[2503.04076](http://arxiv.org/abs/2503.04076)|null|类型推断是重用在线代码片段（如StackOverflow等平台上的代码片段）的关键任务，这些代码片段通常缺少诸如完全限定名称（FQNs）和所需库等基本类型信息。最近的研究利用大型语言模型（LLMs）对代码片段进行类型推断，并显示出有希望的结果。然而，这些结果可能受到数据泄露的影响，因为基准测试套件（StatType-SO）自2017年以来已在GitHub上公开（2023年全套公开）。因此，LLMs的强大性能可能是对代码语义的真正理解，还是仅仅是从训练数据中检索到的真实信息的反映，尚不确定。为了全面评估LLMs在Java代码片段上的类型推断能力，我们进行了三项评估。首先，利用Thalia程序综合技术，我们创建了ThaliaType——一个新的、未知的类型推断评估数据集。在未知的代码片段上，LLMs的性能显著下降，精确度下降了59%，召回率下降了72%。其次，我们开发了语义保留的转换，这些转换显著降低了LLMs的类型推断性能，揭示了在理解代码语义方面的弱点。第三，我们使用delta调试来识别LLMs推断所需的最小语法元素。虽然类型推断主要涉及推断代码片段中类型的FQNs，但LLMs即使在类型未出现在代码片段中也能正确推断FQNs，这表明LLMs依赖于训练中的知识，而不是彻底分析代码片段。我们的发现表明，LLMs过去强大的性能很可能源于数据泄露，而不是对代码片段语义的真正理解。我们的发现强调了使用未知的代码片段精心设计的基准测试来评估LLMs在类型推断任务上真正能力的至关重要性。|
|**2025-03-05**|**MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems**|Rui Ye et.al.|[2503.03686](http://arxiv.org/abs/2503.03686)|null|基于LLM的多智能体系统（MAS）在解决各种任务上显示出巨大的潜力。然而，为了设计有效的MAS，现有方法严重依赖手动配置或多次调用高级LLM，导致不适应性和高推理成本。在本文中，我们将构建MAS的过程简化为一种生成语言任务，其中输入是用户查询，输出是对应的MAS。为了解决这个新任务，我们将MAS的表示统一为可执行代码，并提出了一种以一致性为导向的数据构建流程，以创建一个包含连贯且一致的查询-MAS对的优质数据集。使用这个数据集，我们训练了MAS-GPT，这是一个开源的中型LLM，能够在单个LLM推理内生成查询自适应的MAS。生成的MAS可以无缝应用于处理用户查询并给出高质量响应。在9个基准和5个LLM上的大量实验表明，所提出的MAS-GPT在各种设置上持续优于10多个基线MAS方法，这表明MAS-GPT具有高度的有效性、效率和强大的泛化能力。代码将在https://github.com/rui-ye/MAS-GPT处提供。|
|**2025-03-04**|**VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference**|Zihan Liu et.al.|[2503.02236](http://arxiv.org/abs/2503.02236)|null|在这项工作中，我们设计和实现了一个高效的融合向量量化（VQ）内核生成框架VQ-LLM。我们首先引入了一种名为码本缓存的软件抽象，以优化码本访问效率并支持VQ与其他计算的集成。码本缓存自适应地存储GPU内存层次结构中的不同条目，包括片外全局内存、片上共享内存和寄存器。围绕码本缓存，我们设计了一个高效的计算引擎，优化了涉及码本的计算过程中的内存流量。这个计算引擎采用了以码本为中心的数据流和融合优化。此外，我们还提供了自适应启发式方法，以调整我们的优化中的参数选择，以适应不同的VQ配置。我们的优化与未优化的版本相比，平均延迟降低了46.13%。与现有的开源实现相比，我们的方法将延迟降低了64.36%到99.1%。与AWQ和KVQuant等最先进的逐元素量化方法进行的最终比较表明，我们的VQ-LLM在实践上是可行的，其延迟接近甚至优于等效位宽的延迟，可能提供更高的精度。|
|**2025-03-04**|**FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference**|Hongchao Du et.al.|[2503.03777](http://arxiv.org/abs/2503.03777)|null|首先，我们需要理解摘要中的关键术语和概念：  1. **Large Language Models (LLMs)** - 大型语言模型 2. **on-device inference** - 在设备上的推理 3. **high memory demands** - 高内存需求 4. **traditional methods** - 传统方法 5. **memory usage** - 内存使用 6. **performance** - 性能 7. **adaptability** - 适应性 8. **FlexInfer** - FlexInfer（一个优化框架） 9. **asynchronous prefetching** - 异步预取 10. **balanced memory locking** - 平衡内存锁定 11. **flexible tensor preservation** - 灵活的张量保留 12. **memory efficiency** - 内存效率 13. **I/O bottlenecks** - I/O 瓶颈 14. **user-specified resource constraints** - 用户指定的资源限制 15. **throughput** - 吞吐量 16. **resource-constrained devices** - 资源受限的设备  接下来，我们将摘要中的句子逐句翻译：  - "Large Language Models (LLMs) face challenges for on-device inference due to high memory demands." - 由于高内存需求，大型语言模型（LLMs）在设备上推理时面临挑战。 - "Traditional methods to reduce memory usage often compromise performance and lack adaptability." - 传统方法降低内存使用通常以牺牲性能为代价，并且缺乏适应性。 - "We propose FlexInfer, an optimized offloading framework for on-device inference, addressing these issues with techniques like asynchronous prefetching, balanced memory locking, and flexible tensor preservation." - 我们提出了FlexInfer，一个针对设备上推理的优化卸载框架，通过异步预取、平衡内存锁定和灵活的张量保留等技术来解决这些问题。 - "These strategies enhance memory efficiency and mitigate I/O bottlenecks, ensuring high performance within user-specified resource constraints." - 这些策略提高了内存效率，缓解了I/O瓶颈，确保在用户指定的资源限制内实现高性能。 - "Experiments demonstrate that FlexInfer significantly improves throughput under limited resources, achieving up to 12.5 times better performance than existing methods and facilitating the deployment of large models on resource-constrained devices." - 实验表明，FlexInfer在资源有限的情况下显著提高了吞吐量，比现有方法性能提高高达12.5倍，并促进了大型模型在资源受限设备上的部署。  最终中文翻译结果如下：  由于高内存需求，大型语言模型（LLMs）在设备上推理时面临挑战。传统方法降低内存使用通常以牺牲性能为代价，并且缺乏适应性。我们提出了FlexInfer，一个针对设备上推理的优化卸载框架，通过异步预取、平衡内存锁定和灵活的张量保留等技术来解决这些问题。这些策略提高了内存效率，缓解了I/O瓶颈，确保在用户指定的资源限制内实现高性能。实验表明，FlexInfer在资源有限的情况下显著提高了吞吐量，比现有方法性能提高高达12.5倍，并促进了大型模型在资源受限设备上的部署。|
|**2025-03-03**|**SAGE: A Framework of Precise Retrieval for RAG**|Jintao Zhang et.al.|[2503.01713](http://arxiv.org/abs/2503.01713)|null|检索增强生成（RAG）在特定语料库内执行问答（QA）任务时表现出显著的专业能力。然而，RAG在QA中仍存在许多失败案例。这些失败并非仅归因于大型语言模型（LLMs）的局限性；相反，它们主要源于LLMs因以下两个局限性而检索到不准确的信息：（1）当前的RAG方法在考虑语义的情况下对语料库进行分段，由于问题与段落之间的相关性受损，这导致难以找到相关上下文。（2）在更少地检索关键上下文和检索到不相关上下文之间存在着权衡。在本文中，我们介绍了一个RAG框架（SAGE），以克服这些局限性。首先，为了解决不考虑语义的分段问题，我们提出训练一个语义分段模型。该模型被训练成将语料库分组成语义完整的块。其次，为了确保仅检索最相关的块而忽略不相关的块，我们设计了一个块选择算法，根据相关性分数的下降速度动态选择块，从而实现更相关的选择。第三，为了进一步确保检索到的块的精确性，我们提出让LLMs评估检索到的块是否过多或不足，然后相应地调整上下文数量。实验表明，SAGE在平均问答质量上优于基线61.25%。此外，通过避免检索噪声上下文，SAGE降低了LLMs推理中消耗的令牌成本，平均提高了49.41%的成本效率。此外，我们的工作为提升RAG提供了有价值的见解。|
|**2025-03-03**|**DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems**|Minoo Hosseinzadeh et.al.|[2503.01704](http://arxiv.org/abs/2503.01704)|null|随着在智能城市中应用大型语言模型（LLMs）的近期趋势，需要将这些模型推向网络边缘，同时保持其性能。边缘计算（EC）作为更靠近终端用户的物理计算资源，可以帮助减少为LLM依赖型服务提供终端用户任务时的通信延迟。然而，EC服务器在通信、计算和存储容量方面都有有限的容量。本文介绍了一种名为DILEMMA的新框架，该框架通过在EC系统中联合优化层放置和层量化来解决在EC系统中部署LLMs的挑战。DILEMMA通过使用层量化和技术蒸馏来控制LLM性能，将问题表述为一个整数线性规划问题，以最小化总推理延迟，同时确保可接受的LLM性能水平。在SQuAD数据集上对OPT-350模型进行的实验评估表明，DILEMMA实现了高达12.75%的量化比，同时保留了模型损失，突显了其在资源受限环境中的有效性。|
|**2025-03-01**|**Tutorial Proposal: Speculative Decoding for Efficient LLM Inference**|Heming Xia et.al.|[2503.00491](http://arxiv.org/abs/2503.00491)|null|本教程全面介绍了推测解码（SD），这是一种用于LLM推理加速的高级技术，近年来在学术界引起了广泛关注。SD被引入作为一种创新的解码范式，以减轻LLM中自回归解码引起的高推理延迟。在每次解码步骤中，SD高效地草拟出几个未来标记，然后并行验证它们。与传统的自回归解码不同，这种方法使得每步可以同时解码多个标记，从而在保持原始分布的同时，实现了LLM推理的2x-4x速度提升。本教程深入探讨了SD的最新技术，包括草拟模型架构和验证策略。此外，它还探讨了该领域加速的潜力以及未来的研究方向。我们希望本教程能够阐明当前的研究格局，并为对推测解码感兴趣的研究人员提供见解，最终有助于更高效的LLM推理。|
|**2025-02-28**|**FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference**|Xunhao Lai et.al.|[2502.20766](http://arxiv.org/abs/2502.20766)|**[link](https://github.com/bytedance/FlexPrefill)**|大型语言模型（LLMs）在长序列推理过程中遇到计算挑战，尤其是在注意力预填充阶段，其复杂度随着提示长度的增加而呈二次方增长。之前减轻这些挑战的努力依赖于固定的稀疏注意力模式或基于有限案例识别稀疏注意力模式。然而，这些方法缺乏灵活性，无法有效地适应不同的输入需求。在本文中，我们引入了FlexPrefill，这是一种灵活的稀疏预填充机制，能够在实时动态调整稀疏注意力模式和计算预算，以满足每个输入和注意力头部的特定需求。我们方法的灵活性通过以下两个关键创新得到体现：1）查询感知的稀疏模式确定：通过测量Jensen-Shannon散度，该组件自适应地在特定于查询的多样化注意力模式和预定义的注意力模式之间切换。2）基于累积注意力的索引选择：该组件根据不同的注意力模式动态选择要计算查询-键索引，确保注意力得分的总和达到预定义的阈值。FlexPrefill根据提示自适应优化每个注意力头的稀疏模式和稀疏比率，提高长序列推理任务中的效率。实验结果表明，与先前方法相比，在速度和准确性方面都有显著提高，为LLMs推理提供了一个更加灵活和高效的解决方案。|
|**2025-02-28**|**SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models**|Han-Byul Kim et.al.|[2502.20727](http://arxiv.org/abs/2502.20727)|null|随着大型语言模型（LLMs）规模的快速扩张，实现跨多个计算单元的高效分布式推理变得越来越关键。然而，如张量并行等流行的分布式推理技术的通信开销给实现可扩展性和低延迟带来了重大挑战。因此，我们提出了一种新颖的优化技术，即同步点丢弃（Sync-Point Drop，SPD），通过选择性地丢弃注意力输出的同步来减少张量并行中的通信开销。具体来说，我们首先提出了一种块设计，允许通过SPD在不进行通信的情况下执行。其次，我们根据注意力块对模型精度的敏感性，应用不同的SPD策略。所提出的方法有效地缓解了通信瓶颈，同时在LLM推理过程中最大限度地减少精度下降，为多样化的分布式环境提供了一种可扩展的解决方案：对于LLaMA2-70B在8个GPU上的推理，SPD实现了大约20%的总推理延迟降低，同时精度下降小于1%。|
|**2025-02-27**|**ECCOS: Efficient Capability and Cost Coordinated Scheduling for Multi-LLM Serving**|Kai Mei et.al.|[2502.20576](http://arxiv.org/abs/2502.20576)|**[link](https://github.com/agiresearch/eccos)**|**随着大型语言模型（LLMs）越来越多地作为系统中的服务端点部署，查询量的激增带来了显著的调度挑战。现有的调度框架主要针对延迟优化，而忽视了LLMs服务不同级别查询的能力，这可能导致计算资源浪费。本文针对这一挑战，提出了一种能力-成本协调调度框架ECCOS，用于多LLM服务，该框架明确约束响应质量和工作量以优化LLMs推理成本。具体来说，它通过设计多目标预测器和约束优化器引入了两阶段调度。预测器通过基于训练和基于检索的方法估计模型能力和计算成本，而优化器在质量和工作量约束下确定成本最优的分配。此外，它还引入了QAServe，这是一个通过零样本提示不同LLMs进行知识问答和数学推理收集的样本级响应质量和成本数据集。大量实验表明，与现有方法相比，ECCOS提高了成功率6.30%，同时降低了10.15%的成本，占LLMs响应时间的不到0.5%。代码可在以下链接获取：https://github.com/agiresearch/ECCOS。**|
|**2025-02-26**|**Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs**|Yiheng Yang et.al.|[2502.19078](http://arxiv.org/abs/2502.19078)|null|密集的大型语言模型（LLMs）在输入复杂度不同的情况下，会严格激活所有参数，面临着关键的效率瓶颈。虽然现有的稀疏方法（静态剪枝或动态激活）部分解决了这个问题，但它们要么缺乏对上下文或模型结构需求的适应性，要么会带来过高的计算开销。受人类大脑的双过程机制——预测编码（N400）用于主干稀疏性和结构再分析（P600）用于复杂上下文的启发，我们提出了CLADA，一个认知负载感知动态激活（Cognitive-Load-Aware Dynamic Activation）框架，该框架将统计稀疏性与语义适应性相结合。我们的关键洞察是，LLM的激活表现出两种互补的模式：1）由序列级别的前缀信息驱动的全局统计稀疏性；2）由认知负荷指标（例如，惊讶度和熵）调节的局部语义适应性。CLADA采用分层阈值策略：一个基于离线错误控制的优化基线确保40%以上的稀疏性，并通过实时认知信号动态调整。在六个主流LLMs和九个基准上的评估表明，CLADA实现了平均20%的速度提升，精度下降小于2%，优于Griffin（下降5%以上）和TT（几乎没有速度提升）。关键的是，我们通过多级回归分析建立了神经语言事件相关电位（ERP）成分与LLM效率机制之间的第一个正式联系（稀疏性适应性协同的 $R^2$ 值为0.17）。CLADA无需重新训练或架构变化，为资源感知的LLM推理提供了一种可部署的解决方案，同时推动了受生物启发的AI设计。我们的代码可在CLADA（https://github.com/Oldify/CLADA）上找到。|
|**2025-02-26**|**Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM Inference: Algorithms and Numerical Analysis**|Long Cheng et.al.|[2503.01873](http://arxiv.org/abs/2503.01873)|null|对于长序列推理任务，如文本或图像/视频生成，在大型模型中，注意力计算极为耗时。为了加速这一过程，我们基于Flash Attention开发了一种低精度、数学等效的算法，称为PASA。PASA引入了两项新技术：在线伪平均移位和全局恢复。这些技术使得在Flash Attention过程中可以使用半精度计算，而不会产生溢出不稳定或不可接受的数值精度损失。该算法通过减少数据移动和增加计算FLOPs，提高了内存受限的AI硬件架构（如Ascend神经网络处理单元NPU）的性能。该算法通过设计的随机基准和真实的大型模型进行了验证。我们发现，大型模型的注意力输入数据的大偏差和幅度是导致两种不同类别的大型模型（Qwen2-7B语言模型和Stable-Video-Diffusion多模态模型）数值溢出（半精度>65504）的关键因素。具体来说，溢出是由于Stable-Video-Diffusion模型序列维度的偏差较大以及查询和键在头维度的共振机制。共振机制定义为查询和键矩阵之间的相位巧合或180度相位移位。它将显著放大注意力得分矩阵的元素值。这个问题也适用于Qwen模型。此外，通过均方根误差（RMSE）和将最终生成的文本和视频与使用高精度注意力生成的文本和视频进行比较，对数值精度进行了评估。|
|**2025-02-24**|**CodeSwift: Accelerating LLM Inference for Efficient Code Generation**|Qianhui Zhao et.al.|[2502.17139](http://arxiv.org/abs/2502.17139)|null|代码生成是一个对延迟敏感的任务，需要很高的及时性，但大型语言模型（LLMs）的自回归解码机制导致了较差的推理效率。现有的LLM推理加速方法主要关注使用仅内置组件的独立函数。此外，它们将代码视为自然语言序列，忽略了其独特的语法和语义特征。因此，这些方法在代码生成任务中的有效性仍然有限，并且无法与实际的编程场景相匹配。为了缓解这个问题，我们提出了CodeSwift，这是一种简单但高度高效的推理加速方法，专门为代码生成设计，而不影响输出质量。CodeSwift构建了一个多源数据存储库，提供对通用和项目特定知识的访问，便于检索高质量草案序列。此外，CodeSwift通过控制检索时间来降低检索成本，并通过并行检索和上下文以及LLM偏好感知的缓存来提高效率。实验结果表明，与自回归解码相比，CodeSwift在仓库级和独立代码生成任务中分别实现了高达2.53倍和2.54倍的速度提升，比最先进的推理加速方法高出了88%。|
|**2025-02-24**|**Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM**|Lian Liu et.al.|[2502.16963](http://arxiv.org/abs/2502.16963)|null|在数十亿规模的超大语言模型（LLMs）需要部署在昂贵的服务器级GPU上，配备大容量HBM和强大的计算能力。随着LLM辅助服务的普及，在预算友好的硬件上实现成本效益的LLM推理成为趋势。大量研究将LLM参数从昂贵的GPU转移到主机内存。然而，主机与GPU内存之间带宽的限制限制了推理性能。这项工作介绍了Hermes，一个预算友好的系统，它利用商用DRAM DIMM内的近数据处理（NDP）来提升单个消费级GPU的性能，实现高效的LLM推理。LLMs中固有的激活稀疏性自然地将权重参数分为两类，分别称为“热”神经元和“冷”神经元。只占所有权重参数约20%的“热”神经元，却承担了80%的总计算负载，而“冷”神经元占据了其他80%的参数，但只负责20%的计算负载。因此，我们提出了一种异构计算策略：将“热”神经元映射到单个计算高效的GPU，同时将“冷”神经元卸载到提供大内存但计算能力有限的NDP-DIMMs。同时，激活稀疏性的动态特性需要实时划分热/冷神经元，并在多个NDP-DIMM模块之间自适应地重新映射“冷”神经元。因此，我们引入了一个轻量级的预测器，优化GPU和NDP-DIMMs之间的实时神经元划分和调整。我们还利用基于窗口的在线调度机制，在NDP-DIMM模块之间保持负载平衡。Hermes使得LLaMA2-70B能够在消费级硬件上以13.75个token/s的速度部署，并在最先进的基于卸载的推理系统上实现了平均75.24倍的加速。|
|**2025-02-24**|**DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance**|Xuanfan Ni et.al.|[2502.16886](http://arxiv.org/abs/2502.16886)|null|为了缓解大型语言模型（LLMs）推理过程中的内存负担，许多研究都集中在通过探索注意力稀疏性等方面来压缩KV缓存。然而，这些技术通常需要预先定义的缓存预算；由于最佳预算会随着不同的输入长度和任务类型而变化，这限制了它们在实际接受开放域指令中的应用。为了解决这一限制，我们提出了一种新的KV缓存压缩目标：始终确保在特定输入下也能达到满缓存性能，同时尽可能多地压缩KV缓存。为了实现这一目标，我们引入了一种新的KV缓存压缩方法，称为DBudgetKV，它具有一个基于注意力的指标，用于指示剩余的KV缓存不太可能匹配满缓存性能，然后停止剪枝过程。在涉及不同上下文长度、任务类型和模型大小的实证评估中，我们的方法有效地实现了无损KV剪枝，平均压缩率超过25%。此外，我们的方法易于集成到LLM推理中，不仅优化了内存空间，而且与现有方法相比，还减少了推理时间。|
|**2025-02-24**|**CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter**|Yepeng Weng et.al.|[2502.16880](http://arxiv.org/abs/2502.16880)|null|投机解码是一种强大的技术，通过利用轻量级的投机草案模型来加速大型语言模型（LLM）的推理。然而，现有的设计由于训练和推理之间的不匹配而在性能上存在问题。最近的方法试图通过采用多步骤训练策略来解决这一问题，但不同训练步骤的复杂输入使得草案模型难以收敛。为此，我们提出了CORAL，这是一种新颖的框架，它提高了投机草图的准确性和效率。CORAL引入了跨步骤表示对齐，这是一种增强多个训练步骤之间一致性的方法，显著提高了投机草图的性能。此外，我们识别出LM头作为草案模型推理速度的主要瓶颈。我们引入了一种权重分组机制，在推理过程中选择性激活LM头参数的一个子集，极大地减少了草案模型的延迟。我们在三个LLM系列和三个基准数据集上评估了CORAL，实现了2.50x-4.07x的速度提升，优于EAGLE-2和HASS等最先进的方法。我们的结果表明，CORAL有效地缓解了训练-推理不匹配问题，并为具有大型词汇的现代LLM提供了显著的加速。|
|**2025-02-24**|**LLM Inference Acceleration via Efficient Operation Fusion**|Mahsa Salmani et.al.|[2502.17728](http://arxiv.org/abs/2502.17728)|null|近年来，基于Transformer的大语言模型（LLMs）的快速发展与其日益增长且已非常庞大的规模密切相关。许多LLMs包含数百亿个参数，需要专门的硬件资源进行训练和推理。Transformer架构的一个关键挑战是支持涉及归一化的众多非线性变换。例如，每个解码器块通常至少包含一个Softmax操作和两个Layernorm。计算相应的归一化缩放因子成为了一个主要瓶颈，因为它需要空间集体操作。换句话说，当涉及到Softmax和Layernorm的除数计算时，所有向量元素必须聚合到单个位置，需要大量的通信。这些集体操作使得Transformer推理速度慢了大约20%，违背了分布式内存计算的全部目的。在这项工作中，我们提出了一种极为高效的技巧，可以完全隐藏此类集体操作造成的开销。请注意，每个Softmax和Layernorm操作通常后面都跟着一个线性层。由于非线性操作和线性操作是在不同的硬件引擎上执行的，一旦代数允许这样的交换，它们就可以很容易地进行并行化。通过利用线性操作的内禀特性，我们可以将前面Softmax和Layernorm的归一化推迟到线性层计算之后。现在，我们可以与矩阵乘法并行计算集体缩放因子，并完全隐藏前者的延迟在后者之后。这种并行化保留了数值精度，同时显著提高了硬件利用率并减少了整体延迟。|
|**2025-02-23**|**DISC: Dynamic Decomposition Improves LLM Inference Scaling**|Jonathan Light et.al.|[2502.16706](http://arxiv.org/abs/2502.16706)|null|许多推理扩展方法通过将问题分解成更小的步骤（或标记组）来实现，然后采样并选择最佳下一步。然而，这些步骤及其大小通常基于人类直觉或领域知识预先确定。本文介绍了一种动态分解方法，该方法在推理过程中自动和自适应地将解决方案和推理轨迹分解为步骤。这种方法通过将更多资源集中在困难步骤上，进一步分解它们并优先采样它们来提高计算效率。在编码和数学基准测试（APPS、MATH和LiveCodeBench）上的实验表明，动态分解比依赖于固定步骤（如标记级、句子级或单步分解）的静态方法表现更佳。这些结果表明，动态分解可以增强许多推理扩展技术。|
|**2025-02-23**|**TerEffic: Highly Efficient Ternary LLM Inference on FPGA**|Chenyang Yin et.al.|[2502.16473](http://arxiv.org/abs/2502.16473)|null|大型语言模型（LLM）在边缘设备上的部署通常受到芯片外内存访问需求的限制，导致高功耗和有限的吞吐量。对于LLM的三值量化在保持模型准确性的同时减少内存占用方面具有前景。然而，现有的加速器尚未充分利用芯片上推理的这种潜力。我们提出了TerEffic，这是一种基于FPGA的加速器，它精心协同设计内存架构和计算单元，以实现高度高效的LLM推理，并实现完全的芯片上执行。通过权重压缩、定制计算单元和内存层次优化，我们通过消除芯片外内存带宽瓶颈，实现了前所未有的效率。我们提出了两种架构变体：适用于较小模型的完全芯片上设计和适用于较大模型的HBM辅助设计。在370M参数模型上使用单批次推理进行评估时，我们的芯片上设计实现了每秒12,700个标记（比NVIDIA的Jetson Orin Nano高149倍）的吞吐量，功耗效率为每瓦467个标记（比Jetson Orin Nano好19倍）。HBM辅助设计在2.7B参数模型上提供了每秒521个标记的吞吐量（比NVIDIA的A100高2倍），功耗为33瓦，实现了每瓦16个标记的功耗效率（比A100好8倍）。|
|**2025-02-21**|**Towards Swift Serverless LLM Cold Starts with ParaServe**|Chiheng Lou et.al.|[2502.15524](http://arxiv.org/abs/2502.15524)|null|随着大型语言模型（LLMs）数量的激增，行业转向无服务器计算进行LLM推理服务。然而，由于模型尺寸巨大，无服务器LLM服务受到显著的冷启动延迟和服务水平目标（SLO）违规问题的影响，这导致从远程存储中获取模型的时间延长。我们提出了ParaServe，这是一个无服务器LLM服务系统，通过新颖地使用管道并行性来最小化冷启动延迟。我们的观点是，通过将模型参数分布在多个GPU服务器上，我们可以利用它们聚合的网络带宽来并发地获取模型的不同部分。ParaServe采用两级分层设计。在集群级别，ParaServe根据用户SLO确定最佳并行度，并在服务器间谨慎地放置GPU工作器以减少网络干扰。在工作者级别，ParaServe重叠模型获取、加载和运行时初始化，以进一步加速冷启动。此外，ParaServe引入了管道整合，将并行组合并回单个工作者，以保持对热请求的最佳性能。我们在不同设置下的全面评估表明，与基线相比，ParaServe将冷启动延迟减少了高达4.7倍，并将SLO达成率提高了高达1.74倍。|
|**2025-02-21**|**HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings Filings**|Rasmus Aavang et.al.|[2502.15411](http://arxiv.org/abs/2502.15411)|**[link](https://github.com/aaunlp/HiFi-KPI)**|美国证券交易委员会（SEC）要求上市公司使用机器可读的在线可扩展商业报告语言（iXBRL）标准对财务报告中的数字进行标记。然而，iXBRL定义的高度复杂和高度细粒度的分类法限制了标签在不同领域之间的可转移性。在本文中，我们介绍了分层财务关键绩效指标（HiFi-KPI）数据集，旨在从非结构化财务文本中提取指定粒度级别的数值KPI。我们的方法通过基于分类法的分组方法组织了一个218,126个标签的层次结构，研究哪个分类层提供了最有意义的结构。HiFi-KPI包含约1.8百万段文本和约5百万实体，每个都与iXBRL特定的计算和展示分类法中的标签相链接。我们使用基于编码器的方法和结构化提取使用大型语言模型（LLMs）提供了基线。为了简化LLM推理和评估，我们还发布了HiFi-KPI Lite，这是一个包含四个专家映射标签的手动精选子集。我们公开发布了所有成果。|
|**2025-02-21**|**Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference**|Yaohua Tang et.al.|[2502.15294](http://arxiv.org/abs/2502.15294)|null|随着大型语言模型（LLMs）中上下文窗口大小的增加，它们处理复杂、长文本任务的能力得到了提升。然而，随着对话轮次的增加，需要在GPU内存中存储大量的KV缓存，这显著影响了模型服务系统的效率和甚至可用性。本文分析了来自真实用户的对话数据，并发现LLM推理表现出一个分水岭层，在此之后，轮次级注意力的分布显示出明显的相似性。我们提出了轮次注意力，这是一种新颖的轮次级注意力机制，它只召回和计算最相关轮次的KV缓存。实验表明，我们的方法在不影响模型性能的情况下，节省了55%的内存使用。|
|**2025-02-21**|**A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation**|Shilong Hou et.al.|[2502.15233](http://arxiv.org/abs/2502.15233)|**[link](https://github.com/mebymeby/pseudonymization-framework)**|随着越来越多的公司开始提供利用云基础大型语言模型（LLMs）的服务，例如ChatGPT，这种发展引发了重大的隐私担忧，因为用户的提示会被传输到并由模型提供商进行处理。在LLMs的各种隐私保护方法中，那些在预训练和微调阶段实施的方法未能减轻用户远程使用云基础LLMs时相关的隐私风险。另一方面，在推理阶段应用的方法主要在LLM的推理不依赖于隐私敏感信息的情况下有效。在本文中，我们概述了远程用户与LLMs的交互过程，并首次提出了一个适用于云基础LLMs的一般化匿名化框架的详细定义。实验结果表明，所提出的框架在隐私保护和实用性之间取得了最佳平衡。我们方法的代码已公开，可在https://github.com/Mebymeby/Pseudonymization-Framework上获取。|
|**2025-02-21**|**KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse**|Jingbo Yang et.al.|[2502.16002](http://arxiv.org/abs/2502.16002)|**[link](https://github.com/UCSB-NLP-Chang/KVLink)**|**我们介绍了KVLink，这是一种在大规模语言模型（LLMs）中实现高效键值（KV）缓存重用的方法。在许多LLM应用中，不同的输入可以共享重叠的上下文，例如同一检索到的文档出现在多个查询中。然而，LLMs仍然需要为每个查询编码整个上下文，导致重复计算。在本文中，我们提出了一种新的策略来消除这种低效性，其中每个文档的KV缓存都是独立预先计算的。在推理过程中，将检索到的文档的KV缓存连接起来，使得模型可以重用缓存的表示而不是重新计算。为了减轻使用为每个文档独立计算的KV缓存时LLMs性能下降的问题，KVLink引入了三个关键组件：在推理时调整KV缓存的位移嵌入以匹配拼接后的全局位置，使用可训练的特殊标记来恢复独立编码文档间的自注意力，以及应用混合数据微调以增强性能同时保持模型的原有功能。在7个数据集上的实验表明，KVLink比最先进的方法平均提高了4%的问答准确率。此外，通过利用预先计算的KV缓存，我们的方法将首次输出时间减少了高达90%，与标准LLM推理相比，使其成为可扩展和高效的上下文重用解决方案。**|
|**2025-02-20**|**Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing Workloads at Scale**|Shashwat Jaiswal et.al.|[2502.14617](http://arxiv.org/abs/2502.14617)|null|大型语言模型（LLM）推理工作负载由全球云服务提供商处理时，可能包括对延迟敏感和不敏感的任务，从而产生多样化的服务等级协议（SLA）需求。由于推理堆栈的复杂性，包括多个LLM、硬件配置和地理分布，管理这些混合工作负载具有挑战性。当前的优化策略通常将这些任务隔离以确保对延迟敏感的任务满足SLA，但这导致尽管有spot和on-demand虚拟机（VM）的提供，昂贵的GPU资源利用率仍然很低。我们提出了SAGESERVE，一个全面的LLM服务框架，该框架采用不同时间尺度上的自适应控制旋钮，确保SLA合规性的同时，最大化宝贵GPU资源的利用率。短期优化包括高效地请求路由到数据中心区域，而长期策略则涉及根据流量模式扩展/缩减GPU VM和将模型重新部署到现有VM中。这些策略被构建为一个资源分配的优化问题，并使用整数线性规划（ILP）求解。我们基于超过800万请求的生产工作负载轨迹，使用四个开源模型在三个区域部署进行实证和仿真研究。SAGESERVE在保持尾部延迟并满足所有SLO的情况下，实现了高达25%的GPU小时节省，并且与基线相比，减少了高达80%的扩展开销，证实了我们的提议的有效性。从成本角度来看，这可以为云服务提供商每月节省高达200万美元。|
|**2025-02-20**|**SR-LLM: Rethinking the Structured Representation in Large Language Model**|Jiahuan Zhang et.al.|[2502.14352](http://arxiv.org/abs/2502.14352)|null|结构化表示，以抽象意义表示（AMR）为例，长期以来一直是计算语言学中的关键。然而，在大型语言模型（LLMs）时代，它们的作用仍然模糊不清。将结构化表示集成到LLMs中并通过零样本设置进行的初步尝试，导致了性能下降。我们假设这种下降是由于结构信息以LLMs训练语料库不熟悉的代码格式传递给LLMs所致。因此，我们提出了SR-LLM，这是一个具有两种设置的创新框架，旨在从无训练和有训练的视角探索将结构化表示与LLMs结合的更优方式。前者通过LLMs提示中的自然语言描述来集成结构信息，而后者则通过在语言描述的结构化表示上进行微调来增强模型的理解能力。在广泛的下游数据集中观察到了性能提升，尤其是在PAWS中获得了3.17%和12.38%的显著增益。据我们所知，这项工作开创性地证明了利用结构化表示可以显著增强LLMs的理解能力。我们希望我们的工作为通过结构数据增强LLMs的推理和互操作性提供了启示并鼓励未来的研究。|
|**2025-02-19**|**Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference**|Qingfa Xiao et.al.|[2502.13542](http://arxiv.org/abs/2502.13542)|null|近期大型语言模型（LLMs）在长文本任务中表现出色，但在有限的GPU内存下面临显著的推理效率挑战。现有解决方案首先提出了滑动窗口方法来积累一组可重复使用的键值对（KV对），然后进一步改进中选择性地在每个步骤保留其子集。然而，由于长文本中注意力分布稀疏，难以识别和回忆相关的KV对，因为注意力被大量候选对分散。此外，我们发现选择代表性标记作为每个滑动窗口中的探询查询以有效代表整个上下文是有希望的，这是现有方法忽视的一种方法。因此，我们提出了ActQKV，一种无需训练、激活感知的方法，它动态确定探询查询并利用它来检索推理的相关KV对。具体来说，ActQKV在每个上下文窗口中监控一个标记级指标，激活偏差，允许在预填充阶段正确构建探询查询。为了准确回忆相关的KV对并最小化无关的KV对，我们在解码阶段设计了一个由层间信息密度引导的动态KV截止机制。在Long-Bench和 $\infty$ 基准测试上的实验表明，它在推理质量和资源效率方面均达到了最先进的水平。|
|**2025-02-19**|**What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis**|Peiran Wang et.al.|[2502.13490](http://arxiv.org/abs/2502.13490)|null|大型语言模型（LLM）系统在生成有效和真实内容的能力上存在不稳定的问题，导致幻觉生成。现有的幻觉检测方法严重依赖模型外的信息源，如RAG来辅助检测，从而带来额外的重延迟。最近，LLM推理的内部状态在许多研究中被广泛使用，例如提示注入检测等。考虑到LLM内部状态的解释性和它们不需要外部信息源的事实，我们将这些状态引入LLM幻觉检测。在本文中，我们系统地分析了不同内部状态在推理前向过程中的揭示特征，并全面评估了它们在幻觉检测中的能力。具体来说，我们将大型语言模型的正向过程分为三个阶段：理解、查询、生成，并从这些阶段中提取内部状态。通过分析这些状态，我们深入理解了为什么会产生幻觉内容以及模型内部状态中发生了什么。然后，我们将这些内部状态引入幻觉检测，并进行了全面实验来讨论其优势和局限性。|
|**2025-02-19**|**RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression**|Payman Behnam et.al.|[2502.14051](http://arxiv.org/abs/2502.14051)|null|基于Transformer的大型语言模型在解码阶段严重依赖KV缓存来高效处理扩展的上下文。然而，KV缓存的大小与输入长度成比例增长，随着解码的进行，既负担了内存带宽也消耗了内存容量。为了应对这一挑战，我们提出了RocketKV，这是一种无需训练的KV缓存压缩策略，专门设计用于在解码阶段降低KV缓存的内存带宽和容量需求。RocketKV包含两个连续阶段。在第一阶段，它使用SnapKV++（SnapKV的改进版本，引入了自适应池化大小和与分组查询注意力的完全兼容性）对输入序列标记进行粗粒度KV缓存淘汰。在第二阶段，它采用混合注意力方法进行细粒度的top-k稀疏注意力，通过利用头和序列维度的减少来近似注意力分数。结合这两个阶段，RocketKV在保持与完整KV缓存注意力相当精度的同时，实现了显著的KV缓存读取带宽和存储节省。我们展示了RocketKV在NVIDIA H100 GPU上，与完整KV缓存基线相比，在解码阶段提供了高达3倍的端到端加速以及高达31%的峰值内存减少，同时在各种长上下文任务上实现了可忽略的精度损失。|
|**2025-02-19**|**EvoP: Robust LLM Inference via Evolutionary Pruning**|Shangyu Wu et.al.|[2502.14910](http://arxiv.org/abs/2502.14910)|null|大型语言模型（LLMs）在自然语言处理任务中取得了显著的成功，但它们的庞大体积和计算需求阻碍了它们在资源受限环境中的部署。现有的结构化剪枝方法通过从模型中去除冗余结构（例如元素、通道、层）来解决这个问题。然而，这些方法采用了一种启发式剪枝策略，这导致了次优的性能。此外，它们在剪枝模型时也忽略了数据特征。为了克服这些限制，我们提出了EvoP，这是一个用于鲁棒LLM推理的进化剪枝框架。EvoP首先提出了一种基于聚类的校准数据集采样（CCDS）策略，用于创建一个更多样化的校准数据集。然后，EvoP引入了一种进化剪枝模式搜索（EPPS）方法来寻找最优剪枝模式。与现有的结构化剪枝技术相比，EvoP在保持最佳效率的同时实现了最佳性能。在不同LLMs和不同下游任务上的实验验证了所提出的EvoP的有效性，使其成为在现实世界应用中部署LLMs的实用且可扩展的解决方案。|
|**2025-02-18**|**R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs**|Sumin Jo et.al.|[2502.12767](http://arxiv.org/abs/2502.12767)|**[link](https://github.com/ekrxjwh2009/R2-KG)**|近期研究将大型语言模型（LLMs）与知识图谱（KGs）相结合，以增强推理能力，在不额外训练的情况下提高推理准确性，同时减轻幻觉问题。然而，现有的框架往往比较僵化，难以适应KG或任务的变化。它们还严重依赖强大的LLMs来实现可靠的（即可信赖的）推理。为了解决这个问题，我们引入了R2-KG，这是一个即插即用、双代理框架，将推理分为两个角色：一个操作员（一个低容量的LLM）负责收集证据，一个监督员（一个高容量的LLM）负责做出最终判断。这种设计在LLM推理方面既节省成本，又能保持强大的推理准确性。此外，R2-KG采用了一种弃权机制，仅在从KG收集到足够证据时才生成答案，这显著提高了可靠性。在多个基于KG的推理任务上的实验表明，R2-KG在准确性和可靠性方面都一致优于基线，无论作为操作员的LLM的固有能力如何。进一步实验揭示，R2-KG的单代理版本，配备严格的自我一致性策略，在保持高于基线的可靠性的同时，降低了推理成本。然而，在复杂的KG中也导致更高的弃权率。我们的研究结果将R2-KG确立为基于KG推理的灵活且成本效益高的解决方案。它减少了对外部高容量LLM的依赖，同时确保了可信赖的推理。|
|**2025-02-18**|**HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading**|Cheng Luo et.al.|[2502.12574](http://arxiv.org/abs/2502.12574)|**[link](https://github.com/wdlctc/headinfer)**|**基于Transformer的大型语言模型（LLMs）在长上下文生成方面表现出令人印象深刻的性能。扩展上下文长度不均衡地将LLMs在推理过程中的内存占用转移到了键值缓存（KV缓存）。在本文中，我们提出了HEADINFER，该算法将KV缓存卸载到CPU RAM，同时避免了在GPU上完全存储任何Transformer层的KV缓存的需求。HEADINFER采用细粒度、按头卸载的策略，仅在GPU上保留选定的注意力头KV缓存，并动态计算注意力输出。通过屋顶线分析，我们证明了HEADINFER在显著降低内存占用的同时保持了计算效率。我们在Llama-3-8B模型上评估了HEADINFER，使用了1百万个token的序列，将KV缓存的GPU内存占用从128 GB减少到1 GB，将总GPU内存使用从207 GB减少到17 GB，与BF16基线推理相比实现了92%的降低。值得注意的是，HEADINFER使得在单个24GB内存的消费者GPU（例如NVIDIA RTX 4090）上使用8B模型进行400万个token的推理成为可能，而不需要近似方法。**|
|**2025-02-18**|**Distributed On-Device LLM Inference With Over-the-Air Computation**|Kai Zhang et.al.|[2502.12559](http://arxiv.org/abs/2502.12559)|null|大型语言模型（LLMs）在各种人工智能任务中取得了显著的成功。然而，它们庞大的规模和计算需求为在边缘设备上的部署带来了重大挑战。为了解决这个问题，我们提出了一种基于张量并行性的分布式边缘设备LLM推理框架，该框架将LLMs的神经网络张量（例如，权重矩阵）分配到多个边缘设备进行协同推理。然而，张量并行性在推理过程中涉及到频繁的全量减少操作，以聚合参与设备上的中间层输出，从而导致大量的通信开销。为了缓解这一瓶颈，我们提出了一种空中计算方法，该方法利用无线多址信道的中继叠加性质，以促进快速的全量减少操作。为了最小化平均传输均方误差，我们研究了联合模型分配和收发器优化，这可以表述为一个混合时间尺度的随机非凸优化问题。然后，我们开发了一种利用半定松弛和随机逐次凸近似方法的混合时间尺度算法。综合仿真结果表明，所提出的方法显著降低了推理延迟，同时提高了准确性。这使得分布式边缘设备LLM推理对于资源受限的边缘设备来说是可行的。|
|**2025-02-18**|**SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs**|Ahmed F. AbouElhamayed et.al.|[2502.12444](http://arxiv.org/abs/2502.12444)|**[link](https://github.com/intellabs/hardware-aware-automated-machine-learning)**|**大型语言模型具有高计算、延迟和内存需求。虽然如GPU和TPU等专用加速器通常运行这些工作负载，但CPU更为普遍且能耗更低。使用CPU加速LLM能够以更低成本和能耗实现更广泛的AI访问。这种CPU加速潜力在LLM推理的内存受限解码阶段尤为重要，该阶段每次处理一个标记，并且随着推理模型的增加而越来越被采用。我们利用最新英特尔CPU上的高级矩阵扩展（AMX）支持以及非结构化稀疏性，通过在线性层中应用我们的技术，与当前的PyTorch实现相比，实现了端到端延迟的1.42倍减少。我们提供了一套开源的定制稀疏内核，可以自动将所有线性层替换为我们的定制稀疏实现，从而加快任何PyTorch模型的速度。此外，我们首次展示了在注意力计算中使用非结构化稀疏性，相比当前系统实现了1.14倍的加速，且没有降低准确性。代码：https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SparAMX**|
|**2025-02-18**|**BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference**|Ahmed Burak Gulhan et.al.|[2502.13176](http://arxiv.org/abs/2502.13176)|null|在大型语言模型（LLM）推理中，键值（KV）缓存（KV-caches）对于降低时间复杂度至关重要。然而，随着上下文长度的增加，它们会导致GPU内存线性增长。尽管最近的研究探讨了KV-cache的驱逐和压缩策略以减少内存使用，但它们通常考虑所有注意力头上的均匀KV缓存，导致性能次优。我们引入了BaKlaVa，这是一种通过估计每个KV缓存的重要性来为模型中各个KV缓存分配最优内存的方法。我们的实证分析表明，并非所有KV缓存对LLM性能同等重要。使用一次性分析方法，BaKlaVa为每个KV缓存分配最优的内存预算。我们在LLaMA-3-8B和Qwen2.5-7B模型上评估了我们的方法，在保持基线性能的同时，实现了高达70%的压缩比，并在更高的压缩级别上实现了数量级级别的精度提升。|
|**2025-02-17**|**Designing Role Vectors to Improve LLM Inference Behaviour**|Daniele Potertì et.al.|[2502.12055](http://arxiv.org/abs/2502.12055)|null|人格对大型语言模型（LLM）的影响已被广泛研究，但它们对性能的直接影响仍然不确定。本研究探索了一种通过角色向量引导LLM行为的新方法，这是基于人格提示的替代方案。我们构建了29个由模型激活得到的角色向量，并评估了它们在多个领域基准性能上的影响。我们的分析研究这些向量是否能够有效地引导模型走向特定领域的专业知识。我们测量了两种关键干预措施：（i）激活添加，这强化了特定角色的方向；（ii）方向消除，这移除了它们。在已建立的基准上的结果表明，角色向量确实影响了模型行为，在相关领域提高了任务性能，而对无关任务的影响微乎其微。这反过来又表明，操纵内部模型表示对结果的影响比基于人格的提示更大。|
|**2025-02-17**|**DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services**|Ting Sun et.al.|[2502.11417](http://arxiv.org/abs/2502.11417)|null|随着大型语言模型（LLMs）在文本流服务中的快速兴起，为每天数百万次请求提供服务带来了显著的成本和质量体验（QoE）挑战，尤其是在满足实时交互的首次标记时间（TTFT）和标记间时间（TBT）要求方面。我们的实际测量显示，基于服务器和设备端的部署都难以满足多样化的QoE需求：服务器部署面临高成本和最后一跳问题（例如，互联网延迟和动态），而设备端的LLM推理受限于资源。我们引入了DiSCo，这是一种设备-服务器协同调度器，旨在通过自适应路由请求和在端点之间迁移响应生成来优化用户的QoE，同时保持成本约束。DiSCo采用成本感知调度，利用设备端LLM推理的可预测速度和基于服务器的推理的灵活容量来动态派遣请求，同时引入标记级迁移机制以确保迁移期间的一致性标记交付。在包括OpenAI GPT和DeepSeek等商业服务以及LLaMA3等开源部署在内的实际工作负载上的评估显示，DiSCo可以通过减少不同模型-设备配置下的尾部TTFT（11-52%）和平均TTFT（6-78%）来提高用户的QoE，并通过其迁移机制将服务成本大幅降低至84%，同时保持可比的QoE水平。|
|**2025-02-17**|**Evaluating the Performance of the DeepSeek Model in Confidential Computing Environment**|Ben Dong et.al.|[2502.11347](http://arxiv.org/abs/2502.11347)|null|随着大型语言模型（LLMs）在云环境中的广泛应用，引发了一系列关键的网络安全问题，特别是在模型机密性和数据隐私方面。基于可信执行环境（TEE）的机密计算为缓解这些风险提供了一种有前景的解决方案。然而，现有的TEE实现，主要是基于CPU的，难以高效地支持LLM推理和训练对资源的高需求。在本研究中，我们首次评估了DeepSeek模型在TEE启用机密计算环境中的表现，具体使用了英特尔可信域扩展（TDX）。我们的研究对DeepSeek在不同实现（仅CPU、CPU-GPU混合和基于TEE）下的性能进行了基准测试。对于较小的参数集，如DeepSeek-R1-1.5B，TDX实现优于CPU版本在安全环境中执行计算。这突显了在资源受限系统中高效部署LLM模型的同时确保安全性的潜力。不同模型大小的GPU到CPU的性能比平均为12，小型模型表现出更低的比率。此外，我们还提供了优化CPU-GPU机密计算解决方案的基础见解和指导，以实现可扩展和安全的AI部署。我们的发现有助于隐私保护AI的进步，为在机密计算环境中高效和安全的LLM推理铺平道路。|
|**2025-02-17**|**Tactic: Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs**|Kan Zhu et.al.|[2502.12216](http://arxiv.org/abs/2502.12216)|null|长上下文模型对于许多应用至关重要，但在解码过程中加载大型KV缓存时面临效率问题。先前的方法对稀疏注意力执行固定的令牌预算，假设一定数量的令牌可以近似完整注意力。然而，这些方法忽略了注意力在头、层和上下文中的重要性变化。为了解决这些局限性，我们提出了Tactic，这是一种稀疏自适应和无校准的稀疏注意力机制，它根据令牌的累积注意力分数而不是固定令牌预算动态选择令牌。通过设定总注意力分数的目标比例，Tactic确保令牌选择自然适应注意力稀疏性的变化。为了有效地近似这种选择，Tactic利用基于聚类的排序和分布拟合，允许它以最小的计算开销准确估计令牌的重要性。我们表明，Tactic优于现有的稀疏注意力算法，实现了更高的准确性和高达7.29倍的解码注意力加速。这种改进转化为整体1.58倍的端到端推理加速，使Tactic成为在准确性敏感的应用中，对长上下文LLM推理的一个实用且有效的解决方案。|
|**2025-02-16**|**Diversified Sampling Improves Scaling LLM inference**|Tianchun Wang et.al.|[2502.11027](http://arxiv.org/abs/2502.11027)|null|随着训练计算能力的提升，大型语言模型（LLMs）的性能得到了显著改善，但在扩展推理计算能力时并未观察到类似的收益。我们假设主要问题在于LLM输出的均匀性，这导致模型反复生成相似但不够准确的响应，从而采样效率低下。受到解决方案准确性（Pass@10）与响应多样性之间有趣关系的启发，我们提出了DivSampling——一种新颖且通用的采样技术，通过引入提示扰动来增强候选解决方案的多样性。DivSampling包含两类扰动：任务无关的方法，它们是通用的，不针对任何特定任务；以及基于任务内容定制的任务特定方法。我们的理论分析表明，在温和的假设下，来自多样化提示的响应错误率与来自静态提示的响应错误率相比显著较低。在包括推理、数学和代码生成在内的各种任务上的全面评估突出了DivSampling在提高解决方案准确性方面的有效性。这种可扩展且高效的方法为优化测试时推理提供了新的视角，解决了当前采样策略的局限性。|
|**2025-02-16**|**Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings**|Liangqi Yuan et.al.|[2502.11007](http://arxiv.org/abs/2502.11007)|**[link](https://github.com/liangqiyuan/LCIO)**|与传统的机器学习模型相比，最近的大型语言模型（LLMs）可以通过多次对话和多模态数据源展现多任务解决能力。LLMs除了其庞大的规模之外，还具备的独特特性使得它们在推理阶段的部署更加具有挑战性。具体来说，一是将LLMs部署在本地设备上会面临计算、内存和能源资源的问题；二是将它们部署在云端无法保证实时服务，同时会产生通信/使用成本。在本文中，我们设计了一个本地-云端LLM推理卸载（LCIO）系统，该系统具有以下特点：（一）一个可以处理多模态数据源的规模化云LLM；（二）一个轻量级的本地LLM，能够以高速处理简单任务。LCIO采用资源受限的强化学习（RCRL）来确定在哪里进行推理（即本地还是云端）以及为每个对话/任务使用哪些多模态数据源，旨在在遵守资源约束的同时，最大化长期奖励（其中包含响应质量、延迟和使用成本）。我们还提出了M4A1，一个新的数据集，它考虑了多模态、多任务、多对话和多LLM的特点，以研究LLMs在各种实际场景中的能力。我们通过对比基线，证明了LCIO的有效性，显示了在实现令人满意的响应质量的同时，显著节省了延迟和成本。|
|**2025-02-15**|**Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for Efficient LLM Decoding on Embedded FPGA**|Jindong Li et.al.|[2502.10659](http://arxiv.org/abs/2502.10659)|null|大型语言模型的极高计算和存储需求排除了大多数边缘设备，而这些设备曾广泛用于高效的机器学习。典型的边缘设备通常只有4GB的内存容量和低于20GB/s的带宽，而将具有7B参数的4位精度量化的大型语言模型已需3.5GB的容量，其解码过程完全受带宽限制。在本文中，我们旨在通过在基于Zynq的KV260平台上提出一个用于大型语言模型（LLM）推理的硬件加速器来探索这些限制。该平台配备了4GB的64位2400Mbps DDR4内存。我们成功部署了LLaMA2-7B模型，实现了大约5个token/s的解码速度，利用了93.3%的内存容量，达到了理论内存带宽极限的85%解码速度。为了完全保留内存容量用于模型权重和键值缓存，我们在无操作系统的裸机环境中开发了该系统。为了完全保留带宽用于模型权重传输，我们实现了一个定制的数据流，包含操作符融合管道，并提出了一种可以最大化数据交易效率的数据排列格式。这项研究是首次尝试在独立的嵌入式现场可编程门阵列（FPGA）设备上部署7B级别的LLM。它为嵌入式FPGA设备上的高效LLM推理提供了关键见解，并为未来的架构设计提供了指导。|
|**2025-02-14**|**λScale: Enabling Fast Scaling for Serverless Large Language Model Inference**|Minchen Yu et.al.|[2502.09922](http://arxiv.org/abs/2502.09922)|null|无服务器计算已成为云模型推理的一个有力解决方案。然而，随着现代大型语言模型（LLMs）的不断增大，现有的无服务器平台往往面临巨大的模型启动开销。这为高效扩展模型实例以适应在现实世界的推理服务中常见的动态、突发的工作负载带来了重大挑战。在本文中，我们介绍了{\lambda}Scale，一个高效的无服务器推理系统，以实现快速模型扩展。{\lambda}Scale背后的关键思想是利用GPU节点之间的高速RDMA网络进行快速模型多播，同时在模型传输期间启用分布式推理执行——称为“加载时执行”。{\lambda}Scale提出了一种高效模型扩展方案{\lambda}Pipe，它支持自适应模型多播，并在接收节点之间动态构建执行管道，以实现协作、分布式推理。此外，{\lambda}Scale支持跨GPU和主机内存的高效模型管理，允许对跨越不同存储层的模型进行快速扩展。评估结果表明，{\lambda}Scale可以实现快速模型扩展，并有效处理负载峰值，与最先进的解决方案相比，在现实世界的LLM推理跟踪上实现了高达5倍的尾部延迟改进和31.3%的成本降低。|
|**2025-02-14**|**INF^2: High-Throughput Generative Inference of Large Language Models using Near-Storage Processing**|Hongsun Jang et.al.|[2502.09921](http://arxiv.org/abs/2502.09921)|null|随着大型语言模型（LLMs）在生成推理方面对内存和计算需求的不断增长，实际部署面临着重大挑战。为了解决这些挑战，一种有前景的解决方案是基于卸载的批量推理，它利用主机内存和磁盘作为GPU的扩展内存层次结构。尽管这种方法在成本效益上使LLMs推理成为可能，但其性能受到大量I/O开销的限制，主要是由于大型键值（KV）缓存大小，这些大小随着批量大小和LLMs上下文窗口长度的增加而增加。在本文中，我们引入了INFerence-INFinity（INF^2），一个利用计算存储设备（CSDs）提升生成推理吞吐量的框架。INF^2的核心是注意力近存储，它将内存密集型的自注意力操作卸载到近存储加速器，显著减少了通过系统互连的流量。我们还提出了延迟KV缓存写回，通过延迟新产生的KV缓存写入直到缓存达到系统内存中的足够大来隐藏存储写入延迟。此外，我们引入了协同X缓存，这是一种旨在进一步权衡剩余内存容量与存储带宽的技术。我们的方法有效地最小化了计算空闲时间，提高了整体吞吐量。为了证明我们方法的有效性，本研究已在PyTorch上实现，并在真实系统上进行了评估。我们的实验表明，INF^2相比最先进的基线实现了高达3.46倍的吞吐量提升。我们将开源INF^2以促进更广泛的应用。|
|**2025-02-13**|**On multi-token prediction for efficient LLM inference**|Somesh Mehra et.al.|[2502.09419](http://arxiv.org/abs/2502.09419)|null|我们系统地研究了为下一词预测（NTP）预训练的LLMs中的多词预测（MTP）能力。首先，我们通过中间词概率的数值边缘化表明，这种模型本质上具有MTP能力，尽管性能依赖于数据且随着模型规模的增加而提高。此外，我们探讨了将MTP头集成到冻结的LLMs中的挑战，并发现它们的隐藏层高度专门化于NTP，使得适应变得非同小可。最后，我们表明，虽然联合训练MTP头与主干网络可以提高性能，但无法完全克服这一障碍，这促使我们进一步在这一方向上进行研究。我们的发现为应用于预训练LLMs的MTP提供了更深入的理解，并指导了通过并行词预测加速推理的策略。|
|**2025-02-13**|**InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU**|Heejun Lee et.al.|[2502.08910](http://arxiv.org/abs/2502.08910)|null|在现代大型语言模型（LLMs）中，处理非常长的上下文长度带来了显著挑战，因为它会导致推理速度变慢和内存成本增加。此外，大多数现有的预训练LLMs无法推广到其原始训练序列长度之外。为了实现高效且实用的长上下文利用，我们引入了InfiniteHiP，这是一个新颖且实用的LLM推理框架，通过模块化分层标记剪枝算法动态消除无关的上下文标记来加速处理。我们的方法还允许通过根据LLM内部注意力模式选择性地应用各种RoPE调整方法来推广到更长的序列。此外，我们在推理期间将关键值缓存卸载到主机内存，显著降低了GPU内存压力。因此，InfiniteHiP能够在单个L40s 48GB GPU上处理高达300万个标记——比之前大3倍——而不会丢失任何上下文信息。我们的框架在不进行额外训练的情况下，实现了1百万标记上下文注意力解码的18.95倍速度提升。我们在SGLang框架中实现了我们的方法，并通过广泛的评估证明了其有效性和实用性。|
|**2025-02-12**|**Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences**|Shanshan Han et.al.|[2502.08142](http://arxiv.org/abs/2502.08142)|null|我们提出了Wildflare GuardRail，这是一种旨在通过系统性地解决整个处理工作流程中的风险来提高大型语言模型（LLM）推理的安全性和可靠性的防护管道。Wildflare GuardRail集成了多个核心功能模块，包括：  1. 安全检测器：识别不安全的输入，并在模型输出中检测幻觉，同时生成根本原因解释。 2. 基于信息定位：使用从向量数据库检索的信息来对用户查询进行上下文关联。 3. 定制器：通过轻量级、基于规则的包装器实时调整输出。 4. 修复器：利用安全检测器提供的幻觉解释来纠正错误的LLM输出。  结果显示，安全检测器中的不安全内容检测模型在小型数据集上训练，该数据集由多个公开数据集构建，其性能与OpenAI API相当。同时，轻量级的包装器能够在每个查询1.06秒内以100%的准确率处理模型输出中的恶意URL，而不需要昂贵的模型调用。此外，幻觉修复模型在减少幻觉方面表现出有效性，准确率达到80.7%。|
|**2025-02-12**|**Universal Model Routing for Efficient LLM Inference**|Wittawat Jitkrittum et.al.|[2502.08773](http://arxiv.org/abs/2502.08773)|null|大型语言模型在能力上的显著进步伴随着推理成本的显著增加。模型路由是一种降低推理成本的简单技术，其中维护一个候选LLM池，并学习将每个提示路由到最小的可行LLM。现有工作主要关注学习一个固定LLM池的路由器。在本文中，我们考虑了动态路由的问题，在测试时可以访问新的、之前未观察到的LLM。我们提出了一种新的解决方案，该方法依赖于将每个LLM表示为特征向量，这些特征向量基于一组代表性提示的预测得出。基于此，我们详细介绍了两种有效的策略，分别依赖于基于集群的路由和学习的集群映射。我们证明了这些策略是理论最优路由规则的估计，并提供了过剩风险界限来量化它们的误差。在一系列公开基准上的实验表明，所提出的策略在路由超过30个未见LLM方面是有效的。|
|**2025-02-11**|**PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference**|Yufeng Gu et.al.|[2502.07578](http://arxiv.org/abs/2502.07578)|**[link](https://github.com/Yufeng98/CENT)**|**大型语言模型（LLM）推理采用自回归方式逐个生成一个标记，与早期的机器学习（ML）模型如仅编码器变压器和卷积神经网络相比，具有明显较低的操作强度。同时，LLM具有较大的参数规模，并使用键值缓存来存储上下文信息。现代LLM支持高达100万个标记的上下文窗口，以生成多样化的文本、音频和视频内容。每个提示独有的大型键值缓存需要巨大的内存容量，限制了推理批次的大小。低操作强度和有限的批次大小需要高内存带宽。然而，当代用于ML模型部署的硬件系统，如GPU和TPU，主要优化了计算吞吐量。这种不匹配挑战了高级LLM的高效部署，并导致用户为内存密集型LLM推理任务支付昂贵的计算资源。我们提出了CENT，一个CXL-ENabled GPU-Free系统，用于LLM推理，它利用CXL内存扩展能力来容纳大量的LLM大小，并使用近银行处理单元提供高内存带宽，消除了对昂贵GPU的需求。CENT利用可扩展的CXL网络，支持CXL设备之间的对等和集体通信原语。我们实现了各种并行策略，将这些设备上的LLM进行分配。与具有最大支持批次大小和类似平均功率的GPU基准相比，CENT实现了2.3倍更高的吞吐量，消耗了2.3倍更少的能源。CENT提高了总拥有成本（TCO），比GPU多产生5.2倍的标记每美元。**|
|**2025-02-11**|**HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment**|Youhe Jiang et.al.|[2502.07903](http://arxiv.org/abs/2502.07903)|null|将预填充和解码阶段拆分，代表了一种针对大型语言模型（LLM）生成推理的有效新范式，它消除了预填充-解码干扰并优化了资源分配。然而，如何将拆分推理范式部署到一组异构GPU上仍然是一个未解决的问题，这可以成为在同类高性能GPU上部署的经济替代方案。为此，我们引入了HexGen-2，这是一个遵循拆分范式在异构GPU上高效且经济地提供LLM服务的分布式系统。基于HexGen构建，HexGen-2的核心组件是一个调度算法，它将拆分LLM推理计算和通信在异构GPU和网络连接上的分配形式化为一个约束优化问题。我们利用图划分和最大流算法来协同优化资源分配、不同推理阶段的并行策略以及跨阶段键值（KV）缓存通信的效率。我们进行了广泛的实验来评估HexGen-2，即在不同现实场景中针对OPT（30B）和Llama-2（70B）模型，结果显示，与最先进的系统相比，HexGen-2在相同的预算下提供了高达2.0倍的服务吞吐量，平均降低了1.5倍的推理延迟，并且以30%更低的预算实现了可比的推理性能。|
|**2025-02-11**|**SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters**|Yiping Wang et.al.|[2502.07832](http://arxiv.org/abs/2502.07832)|null|在大型语言模型（LLMs）推动了自然语言处理任务的同时，它们日益增长的计算和内存需求使得在资源受限的设备上（如手机）部署变得越来越具有挑战性。在本文中，我们提出了一种名为SHARP（共享相邻层与恢复参数）的新方法，通过在相邻层之间共享参数来加速LLMs的推理，从而降低内存负载开销，同时引入低秩恢复参数以保持性能。受连续层具有相似输出的观察启发，SHARP采用两阶段恢复过程：单层预热（SLW）和监督微调（SFT）。SLW阶段使用L_2损失对共享层的输出进行对齐，为随后的SFT阶段提供了一个良好的初始化，以进一步恢复模型性能。大量的实验表明，SHARP在不超过50k微调数据的情况下，可以在各种分布内任务中恢复模型困惑度，同时将存储的MLP参数数量减少38%至65%。我们还对SHARP进行了多次消融研究，并显示在模型后期部分替换层可以获得更好的性能保持，并且当参数计数匹配时，不同的恢复参数化表现相似。此外，与原始Llama2-7b模型相比，SHARP在移动设备上节省了42.8%的模型存储空间，并将总推理时间减少了42.2%。我们的结果突出了SHARP作为在无需预训练规模资源的情况下减少LLMs推理成本的效率解决方案。|
|**2025-02-10**|**Online Scheduling for LLM Inference with KV Cache Constraints**|Patrick Jaillet et.al.|[2502.07115](http://arxiv.org/abs/2502.07115)|null|大型语言模型（LLM）推理，即训练模型在用户提示下逐词生成文本的过程，是一个计算密集型过程，需要高效的调度来优化延迟和资源利用率。在LLM推理中，一个关键挑战是键值（KV）缓存的的管理，它减少了冗余计算但引入了内存限制。在这项工作中，我们理论上对带有KV缓存约束的LLM推理进行建模，并提出了新颖的批处理和调度算法，以最小化推理延迟并有效地管理KV缓存的内存。我们分析了半在线和全在线调度模型，我们的成果有三个方面。首先，我们提供了一个多项式时间算法，在半在线提示到达模型中实现了平均延迟的精确最优性。其次，在具有随机提示到达的全在线情况下，我们引入了一个具有常数遗憾的在线调度算法。第三，我们证明了没有算法（确定性或随机）能在全在线对抗设置中实现常数竞争比。我们在一个公共LLM推理数据集上的实证评估，使用Llama-70B模型在A100 GPU上，显示我们的方法在降低延迟的同时减少了能耗，显著优于目前实践中使用的基准算法。总的来说，我们的结果为更可持续和成本效益更高的LLM部署提供了一条途径。|
|**2025-02-08**|**Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models**|Soham Poddar et.al.|[2502.05610](http://arxiv.org/abs/2502.05610)|null|大型语言模型（LLMs）因其卓越的生成能力和在各类任务中的通用性而日益受到认可。然而，这些模型与训练成本相比，其高推理成本并未得到足够的关注。为了填补这一空白，我们的研究对LLMs在不同NLP任务中的推理能耗进行了全面基准测试，分析了不同模型、任务、提示和系统相关因素对推理能耗的影响。具体来说，我们的实验揭示了几个有趣的见解，包括推理能耗与输出令牌长度和响应时间的强相关性。此外，我们发现量化、最优批处理大小以及有针对性的提示短语可以显著降低能耗。这项研究是首次对LLMs在如此广泛的方面进行彻底的基准测试，提供了洞察并提出了改进模型部署中能效的几项建议。|
|**2025-02-08**|**Mechanistic Interpretability of Emotion Inference in Large Language Models**|Ala N. Tak et.al.|[2502.05489](http://arxiv.org/abs/2502.05489)|null|大型语言模型（LLMs）在预测文本中的人类情感方面展现出有希望的潜力。然而，这些模型处理情感刺激的机制仍然在很大程度上未被探索。我们的研究通过调查自回归LLMs如何推断情感来填补这一空白，表明情感表征在模型中功能性地定位到特定区域。我们的评估包括不同的模型家族和大小，并由稳健性检查支持。然后，我们通过借鉴认知评估理论，一个关于情感从对环境刺激的评价（评估）中产生的成熟心理学框架，证明所识别的表征在心理上是合理的。通过因果干预所构建的评估概念，我们引导生成过程，并显示输出与理论预期和直观预期相符。这项工作突出了一种新颖的因果干预方法，可以精确地塑造情感文本生成，可能对敏感的情感领域中的安全和一致性产生益处。|
|**2025-02-07**|**LLM Query Scheduling with Prefix Reuse and Latency Constraints**|Gregory Dexter et.al.|[2502.04677](http://arxiv.org/abs/2502.04677)|null|在在线环境中高效部署大型语言模型（LLMs）需要优化推理性能，以满足严格的延迟约束，尤其是首次输出时间（TTFT）和每输出字符时间（TPOT）。本文重点关注具有前缀重用的LLM推理查询调度问题，这是一种利用查询之间的共享前缀来减少计算开销的技术。我们的研究揭示了现有先来先服务（FCFS）和最长前缀匹配（LPM）调度策略在满足延迟约束方面的先前未知的局限性。我们提出了一个基于RadixAttention的前缀重用机制的LLM查询调度形式化理论框架，RadixAttention是一种将中间表示存储和重用于基数树结构中的前缀重用机制。我们的分析证明了在TTFT约束下，具有前缀重用的调度问题属于NP难题，并提出了一种新的调度算法 $k$-LPM，它通过平衡查询处理中的前缀重用和公平性来泛化现有方法。理论保证表明，在由数据生成模型捕捉到的现实交通模式中，$k$ -LPM在首次输出时间性能方面实现了改进。在现实服务设置中的实证评估验证了我们的发现，与基线方法相比，P99首次输出时间显著减少。|
|**2025-02-07**|**BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference**|Reena Elangovan et.al.|[2502.05376](http://arxiv.org/abs/2502.05376)|null|在训练后量化（PTQ）是降低大型语言模型（LLMs）存储和计算需求的一种有前景的方法，且无需额外训练成本。最近的研究主要集中在仅对权重进行低于8位的量化，同时保持激活值为8位或更高。在没有依赖量化感知训练的情况下，对权重和激活值都进行准确的低于8位的量化仍然是一个重大挑战。我们提出了一种名为块聚类量化（BCQ）的新颖量化方法，其中每个操作张量被分解为块（块是一组连续的标量），块根据其统计信息进行聚类，并为每个聚类设计一个专用的最优量化码本。作为这种方法的具体实现，我们提出了一种名为局部最优BCQ（LO-BCQ）的PTQ算法，该算法在块聚类和码本设计步骤之间迭代，以贪婪地最小化量化均方误差。当权重和激活标量编码为W4A4格式（存储缩放因子和码本选择器的开销为0.5位）时，我们在多个LLMs和下游任务中展示了<1%的推理精度损失，从而将当前的最先进水平推进了一步。|
|**2025-02-06**|**AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference**|Qingyue Yang et.al.|[2502.04077](http://arxiv.org/abs/2502.04077)|**[link](https://github.com/MIRALab-USTC/LLM-AttentionPredictor)**|随着大型语言模型（LLMs）的发展，通过键值（KV）缓存压缩进行高效的推理引起了广泛关注，尤其是在长文本生成方面。为了压缩KV缓存，最近的方法通过注意力分数的启发式排名来确定关键KV标记。然而，这些方法往往难以准确确定关键标记，因为它们忽略了注意力分数中的时间模式，导致LLM性能显著下降。为了应对这一挑战，我们提出了AttentionPredictor，这是第一个基于学习的临界标记识别方法。具体来说，AttentionPredictor学习了一个轻量级的卷积模型来捕捉时空模式并预测下一个标记的注意力分数。AttentionPredictor的一个吸引人的特点是，它可以在消耗极小内存的情况下准确预测注意力分数。此外，我们提出了一种跨标记关键缓存预取框架，该框架隐藏了标记估计时间的开销，以加速解码阶段。通过保留大部分的注意力信息，AttentionPredictor实现了16倍的KV缓存压缩，且性能与LLMs相当，显著优于现有技术。|
|**2025-02-06**|**Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective**|Yuan Feng et.al.|[2502.03805](http://arxiv.org/abs/2502.03805)|**[link](https://github.com/NVIDIA/kvpress)**|大型语言模型彻底改变了自然语言处理，但由于Transformer架构依赖于自注意力，特别是长序列推理中大型键值（KV）缓存，因此面临着高存储和运行成本的重大挑战。最近的研究通过基于注意力权重修剪不那么关键的条目来减少KV缓存大小，但这些努力仍然停留在经验层面，缺乏正式的依据。本文通过对注意力输出扰动进行分析，提出了一种正式研究，以识别关键的KV缓存条目。我们的分析表明，除了注意力权重之外，KV条目中的值状态和预训练参数矩阵也是至关重要的。基于此，我们提出了一种扰动约束选择算法，该算法通过优化最坏情况下的输出扰动来识别关键条目。在Needle-in-a-Haystack测试和Longbench基准测试中的评估表明，我们的算法增强了最先进的缓存淘汰方法。进一步的实证分析证实，我们的算法在Llama模型中超过92%的注意力头中实现了更低的输出扰动，从而在现有方法的基础上提供了显著的改进。|
|**2025-02-06**|**Adaptive Semantic Prompt Caching with VectorQ**|Luis Gaspar Schroeder et.al.|[2502.03771](http://arxiv.org/abs/2502.03771)|null|语义提示缓存通过重用缓存中大型语言模型（LLM）生成的响应来降低LLM推理的延迟和成本。向量相似度指标为嵌入提示与其缓存中最邻近的提示之间的相似度分配一个数值分数。现有的系统依赖于一个静态阈值来分类相似度分数是否足够高以导致缓存命中。我们表明，这种一刀切阈值在不同提示之间是不够的。我们提出了VectorQ，一个框架来学习嵌入特定的阈值区域，以适应嵌入的复杂性和不确定性。通过在四个不同的数据集组合上的评估，我们表明VectorQ在所有静态阈值上均优于最先进的系统，缓存命中率提高至12倍，错误率降低至92%。|
|**2025-02-06**|**WaferLLM: A Wafer-Scale LLM Inference System**|Congjie He et.al.|[2502.04563](http://arxiv.org/abs/2502.04563)|null|随着人工智能加速器的兴起，越来越多的加速器采用晶圆级制造技术，将数十万个AI核心集成在基于网格的架构中，并配备大容量分布式片上内存（总计数十GB）和超高的片上内存带宽（数十PB/s）。然而，目前针对如GPU等共享内存架构优化的LLM推理系统，无法充分利用这些加速器。我们介绍了WaferLLM，这是第一个晶圆级LLM推理系统。WaferLLM遵循一个新颖的PLMR设备模型，该模型捕捉了晶圆级架构的独特硬件特性。利用这个模型，WaferLLM开创了晶圆级LLM并行处理，优化了数十万个片上核心的利用率。它还引入了MeshGEMM和MeshGEMV，这是第一个专为在晶圆级加速器上有效扩展而设计的GEMM和GEMV实现。评估结果显示，WaferLLM比最先进的系统实现了200倍的晶圆级加速器利用率提升。在商用晶圆级加速器上，与先进的GPU相比，WaferLLM实现了606倍的GEMV速度和22倍的能效。对于LLM来说，WaferLLM实现了39倍的解码速度和1.7倍的能效提升。我们预计，随着晶圆级AI模型、软件和硬件的不断成熟，这些数字将显著增长。|
|**2025-02-06**|**KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference**|Xing Li et.al.|[2502.04420](http://arxiv.org/abs/2502.04420)|**[link](https://github.com/cmd2001/KVTuner)**|KV缓存量化可以提高大型语言模型（LLMs）在长文本和大数据量场景下的推理吞吐量和延迟，同时保持LLMs的有效性。然而，现有方法存在三个未解决的问题：忽视了层间对KV缓存量化的敏感性、在线细粒度决策的高开销以及对不同LLMs和约束条件的低灵活性。因此，我们深入分析了层间Transformer注意力模式与KV缓存量化误差之间的固有相关性，并研究了为什么对于量化误差减少来说，键缓存比值缓存更重要。我们进一步提出了一种简单而有效的框架KVTuner，通过多目标优化自适应地搜索粗粒度KV缓存的最佳硬件友好层间KV量化精度对，并在在线推理中直接利用离线搜索到的配置。为了减少离线校准的计算成本，我们利用层内KV精度对剪枝和层间聚类来减少搜索空间。实验结果表明，我们可以在Llama-3.1-8B-Instruct等LLMs上实现近无损的3.25位混合精度KV缓存量化，在数学推理任务中对敏感模型如Qwen2.5-7B-Instruct实现4.0位量化。与KV8量化相比，在各种文本长度下，最大推理吞吐量可以提高38.3%。|
|**2025-02-06**|**CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference**|Zehua Pei et.al.|[2502.04416](http://arxiv.org/abs/2502.04416)|**[link](https://github.com/JarvisPei/CMoE)**|**大型语言模型（LLMs）通过扩展模型参数实现了令人印象深刻的性能，但这伴随着显著的推理开销。占LLM参数主导地位的卷积前馈网络（FFNs）在隐藏神经元中表现出高激活稀疏性。为了利用这一点，研究人员提出了使用专家混合（MoE）架构，其中只有一部分参数被激活。然而，现有方法通常需要大量的训练数据和资源，限制了它们的实用性。我们提出了CMoE（Carved MoE），这是一种新颖的框架，能够高效地从密集模型中雕刻MoE模型。CMoE通过高效的专家分组和轻量级适应实现了显著的性能。首先，根据激活率将神经元分组为共享和路由专家。接下来，我们构建了一个无需从头开始训练的路由机制，其中包含可微分的路由过程和负载均衡。使用适度的数据，CMoE在五分钟内从一个7B密集模型中产生了一个设计精良、可用的MoE。通过轻量级微调，它在一小时内实现了高性能恢复。我们将我们的代码公开在https://github.com/JarvisPei/CMoE上。**|
|**2025-02-05**|**Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL**|Wenbo Sun et.al.|[2502.02818](http://arxiv.org/abs/2502.02818)|null|为大型语言模型（LLM）提供服务通常需要专用硬件、专门的框架和大量的开发工作，这限制了其可访问性，尤其是在边缘设备和技术资源有限的组织中。我们提出了一种新颖的编译器，该编译器将LLM推理图转换为SQL查询，使得全球最广泛使用和成熟的软件系统之一——关系数据库，能够作为运行时环境。通过将诸如矩阵乘法和注意力等神经网络算子映射到关系运算原语（如连接和聚合），我们的方法利用了数据库的能力，包括基于磁盘的数据管理和原生缓存。支持关键的转换器组件，如注意力机制和键值缓存，我们的系统为端到端LLM推理生成SQL管道。以Llama3系列作为案例研究，我们证明了在内存受限的场景下，与竞争的基于CPU的框架相比，在标记生成速度上最多可以提升30倍。我们的工作提供了一个易于访问、便携和高效的解决方案，促进了LLM在各种部署环境中的服务。|
|**2025-02-05**|**Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation**|Jingyu Liu et.al.|[2502.02789](http://arxiv.org/abs/2502.02789)|**[link](https://github.com/Jingyu6/speculative_prefill)**|提升首次输出token时间（TTFT）是现代大型语言模型（LLM）推理引擎中一个至关重要的目标。因为优化TTFT直接导致最大QPS值提高，并满足许多关键应用的需求。然而，提升TTFT极具挑战性，因为它纯粹受计算限制，性能瓶颈从自注意力部分转移到MLP部分。我们提出了SpecPrefill，一个无需训练的框架，该框架基于以下洞察来加速长和中等上下文查询的推理TTFT：LLM足够通用，仅凭精心选择的提示token子集仍能保持质量。在其核心，SpecPrefill利用轻量级模型根据上下文推测局部重要token。这些token以及必要的位置信息随后被发送到主模型进行处理。我们使用一系列不同的任务对SpecPrefill进行了评估，随后在真实端到端设置和消融研究中全面评估了性能改进。SpecPrefill能够在真实下游任务中为Llama-3.1-405B-Instruct-FP8提供高达 $7\times$的最大端到端QPS，并在基准测试期间实现了$7.66\times$ 的TTFT提升。|
|**2025-02-05**|**HACK: Homomorphic Acceleration via Compression of the Key-Value Cache for Disaggregated LLM Inference**|Zeyu Zhang et.al.|[2502.03589](http://arxiv.org/abs/2502.03589)|null|分拆大型语言模型（LLM）推理因其将计算密集的前填充阶段与内存密集的解码阶段分离，避免了前填充-解码干扰并提高了资源利用率而受到青睐。然而，在两个阶段之间传输键值（KV）数据可能成为瓶颈，尤其是在处理长提示时。此外，前填充和解码的计算时间开销是优化作业完成时间（JCT）的关键，而对于长提示和序列，KV数据大小可能变得难以承受。现有的KV量化方法可以缓解传输瓶颈并减少内存需求，但它们引入了显著的逆量化开销，加剧了计算时间。我们针对分拆LLM推理提出了基于KV缓存压缩的同态加速（HACK）。HACK消除了繁重的KV逆量化步骤，并直接在量化KV数据上执行计算，以近似并减少昂贵的矩阵乘法步骤的成本。大量的基于跟踪的实验表明，与分拆LLM推理基线相比，HACK将JCT降低了高达70.9%，与最先进的KV量化方法相比，降低了高达52.3%。|
|**2025-02-04**|**EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization**|Yize Wu et.al.|[2502.02493](http://arxiv.org/abs/2502.02493)|null|推测性解码是一种有效且无损的用于大语言模型（LLM）推理加速的方法。它使用一个较小的模型生成草稿标记序列，然后由原始基础模型进行验证。在多GPU系统中，通过张量并行（TP）可以进一步降低推理延迟，而草稿模型的最佳TP大小通常小于基础模型，导致在草稿阶段GPU闲置。为了解决这个问题，我们提出了EasySpec，这是一种层并行推测策略，优化了多GPU利用效率。EasySpec打破了草稿模型中层的顺序执行顺序，使得可以在设备间实现多层并行化，尽管会引入一些近似误差。在每个草稿-验证迭代之后，草稿模型的关键值（KV）缓存通过单个前向传递进行校准，以最小的额外延迟防止长期错误累积。我们使用几个主流开源LLM评估了EasySpec，使用同一系列模型的小版本作为草稿模型。结果表明，与传统的解码相比，EasySpec可以实现高达4.17倍的峰值加速，同时保持基础LLM的原有分布。具体来说，草稿阶段可以加速高达1.62倍，最大精度下降仅为7%，且无需在草稿模型上进行训练或微调。|
|**2025-02-03**|**An Investigation of FP8 Across Accelerators for LLM Inference**|Jiwoo Kim et.al.|[2502.01070](http://arxiv.org/abs/2502.01070)|null|在现代AI加速器中引入8位浮点（FP8）计算单元，引发了基于FP8的大型语言模型（LLM）推理的极大兴趣。与16位浮点格式不同，深度学习中的FP8需要共享一个缩放因子。此外，虽然E4M3和E5M2在单个值级别上是明确定义的，但它们的缩放和累积方法尚未指定，并且在不同硬件和软件实现中有所不同。因此，FP8更像是一种量化格式，而不是标准的数值表示。在这项工作中，我们首次对两个AI加速器（NVIDIA H100和Intel Gaudi 2）上的FP8计算和加速进行了全面分析。我们的发现强调，通过利用FP8，Gaudi 2在LLM推理期间实现了更高的吞吐量与功耗效率，为数据中心规模LLM服务的FP8采用的实际影响提供了宝贵见解。|
|**2025-02-02**|**Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference**|Patrick Yubeaton et.al.|[2502.00922](http://arxiv.org/abs/2502.00922)|null|随着大型语言模型（LLMs）能力的提升，其规模也在迅速增加。这加剧了在小型边缘设备上运行最先进LLMs的难度。标准技术主张通过量化或剪枝等有损压缩技术来解决此问题。然而，此类压缩技术是有损的，并且已被证明会以不可预测的方式改变模型行为。我们提出了Huff-LLM，这是一种端到端、无损的模型压缩方法，使用户能够将LLM权重以压缩格式存储在任何地方——云、磁盘、主内存，甚至在片上内存/缓冲区中。这使我们不仅能够在主内存中加载更大的模型，还能减少加载片上权重的带宽需求，并更有效地利用片上权重缓冲区。除了通过压缩实现的内存节省外，我们还展示了在执行压缩模型的推理时，在延迟和能效方面的改进。|
|**2025-02-02**|**SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models**|Jiawen Zhang et.al.|[2502.00847](http://arxiv.org/abs/2502.00847)|null|随着LLM在普通用户中的普及，隐私保护和对抗鲁棒性成为LLM服务中两个迫切的需求，这两者一直被单独追求，但很少联合考虑。在本文中，据我们所知，我们是第一个将两个不相连接的领域——隐私推理和提示集成——紧密结合以实现鲁棒和隐私LLM推理的尝试。前者通过加密LLM传输和处理的推理数据来保护用户隐私，而后者通过从多个提示LLM响应中得出聚合输出来增强对抗鲁棒性。尽管单个方法被广泛认为有效，但隐私推理与提示集成结合在一起会带来新的挑战，使得现有技术的简单组合效率低下。为了克服这些障碍，我们提出了SecPE，它为提示集成的核心算法构建块设计了高效的完全同态加密（FHE）对应方案。我们通过对8个任务进行大量实验来评估SecPE的准确性、鲁棒性和效率。结果表明，与基线隐私推理方法相比，SecPE仅以2.5%的效率开销保持了高清洁准确性，并提供了更好的鲁棒性，表明“准确性-鲁棒性-效率”之间的权衡是令人满意的。对于提示集成中引起主要减速的加密Argmax操作，SecPE比最先进的方法快35.4倍，这可以超出这项工作本身，具有独立的研究价值。|
|**2025-02-01**|**UniAttn: Reducing Inference Costs via Softmax Unification for Post-Training LLMs**|Yizhe Xiong et.al.|[2502.00439](http://arxiv.org/abs/2502.00439)|null|后训练对于适应真实世界应用的大型语言模型（LLMs）至关重要。部署后训练模型面临着巨大的内存开销和明显的推理延迟。现有工作已经识别出LLMs中存在显著的冗余，并提出了高效的架构，即层内KV共享和层间KV共享。然而，层内KV共享仍然导致高推理成本，而层间KV共享则导致显著的性能下降。因此，这两种方法对于后训练预训练的LLMs来说仍然不是最优的。在本文中，我们确定Softmax操作是LLM推理的主要瓶颈，并发现它在后训练期间实际上是高度冗余的。我们提出了Softmax在注意力（UniAttn）统一，一种新颖的后训练方法，该方法通过统一Transformer块中的Softmax激活来降低LLM的推理成本。此外，UniAttn采用线性投影来补偿由Softmax统一引起的误差。实验表明，UniAttn在性能上与标准后训练相当，同时显著降低了推理成本，在后训练期间优于现有的高效架构。我们的代码将在https://github.com/Bostoncake/UniAttn上提供。|
|**2025-02-01**|**ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference**|Xiang Liu et.al.|[2502.00299](http://arxiv.org/abs/2502.00299)|null|为了降低在长上下文推理中使用大型语言模型（LLMs）时的内存成本，许多最近的研究都集中在压缩不同标记的关键值（KV）缓存上。然而，我们发现之前的KV缓存压缩方法单独衡量标记的重要性，忽略了现实世界中语言特征的标记间依赖性。鉴于这一点，我们引入了ChunkKV，将块中的标记作为一个基本的压缩单元，同时保留最有信息量的语义块，丢弃不那么重要的块。此外，观察到ChunkKV在不同层之间具有更高的保留索引相似性，我们提出了分层索引重用来进一步减少计算开销。我们在包括LongBench和Needle-In-A-HayStack在内的最前沿长上下文基准测试中，以及GSM8K和JailbreakV的上下文学习基准测试中评估了ChunkKV。我们的实验与指令调整和多步推理（O1和R1）LLMs，与现有方法相比，在积极的压缩比率下实现了高达10%的性能提升。|
|**2025-01-31**|**Pheromone-based Learning of Optimal Reasoning Paths**|Anirudh Chari et.al.|[2501.19278](http://arxiv.org/abs/2501.19278)|null|大型语言模型（LLMs）通过思维链提示展示了卓越的推理能力，但发现有效推理方法对于复杂问题仍然具有挑战性，因为可能的中间步骤空间巨大。我们引入了蚁群优化引导的思维树（ACO-ToT），这是一种将蚁群优化与LLMs相结合的新算法，以高效地发现复杂问题的最佳推理路径。从神经系统中赫布学习（Hebbian learning）的灵感中汲取，我们的方法使用一组经过特别微调的LLM“蚂蚁”在中央思维树中穿梭并留下信息素路径，每个蚂蚁的移动由现有信息素路径和它自己的专业知识的加权组合所控制。该算法使用基于专家混合的评分函数评估完整的推理路径，信息素在迭代中强化有效的推理路径。在三个具有挑战性的推理任务（GSM8K、ARC-Challenge和MATH）上的实验表明，ACO-ToT的性能显著优于现有的思维链优化方法，这表明将生物启发的集体搜索机制融入LLM推理可以显著提高推理能力。|
|**2025-01-30**|**Fine-tuning LLaMA 2 interference: a comparative study of language implementations for optimal efficiency**|Sazzad Hossain et.al.|[2502.01651](http://arxiv.org/abs/2502.01651)|null|本文提出了一项旨在优化Llama2推理的对比研究，这是机器学习和自然语言处理（NLP）中的关键方面。我们评估了包括TensorFlow、PyTorch、Python、Mojo、C++和Java在内的各种编程语言和框架，通过广泛的基准测试分析它们在速度、内存消耗和易用性方面的性能。突出了每种方法的优点和局限性，以及针对并行处理和硬件利用的优化策略。此外，我们研究了Mojo SDK，这是一个专为在苹果硅上执行大型语言模型（LLM）推理而设计的创新框架，将其性能与C、C++、Rust、Zig、Go和Julia的实现进行了基准测试。在我们的实验中，使用苹果M1 Max进行测试，证明了Mojo SDK的竞争性性能、易用性和与Python的无缝兼容性，使其成为在苹果硅上执行LLM推理的强大替代方案。我们还讨论了在资源受限硬件上部署LLM的更广泛影响，并确定了未来研究的潜在方向。|
|**2025-01-27**|**Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs**|Antony Bartlett et.al.|[2501.16191](http://arxiv.org/abs/2501.16191)|null|解决Python依赖问题对于开发者来说是一项繁琐且容易出错的任务，他们必须手动识别和解决第三方模块和Python解释器的环境依赖和版本约束。研究人员试图通过依赖大型知识图谱和数据库查找表来自动化这一过程。然而，这些传统方法由于依赖错误类型的多样性、可能模块版本的集合庞大以及传递依赖之间的冲突而面临局限性。本研究探讨了使用大型语言模型（LLMs）自动修复Python程序中依赖问题的潜力。我们引入了PLL（发音为“plum”），这是一种新颖的技术，它采用检索增强生成（RAG）来帮助LLM推断给定Python文件的Python版本和所需模块。PLL构建了一个测试环境，通过迭代地（1）提示LLM进行模块组合，（2）测试建议的更改，以及（3）向LLM提供反馈（错误消息）以改进修复。这个反馈循环利用自然语言处理（NLP）智能解析和解释构建错误消息。我们在Gistable HG2.9K数据集上对PLL进行了基准测试，这是一个包含具有挑战性的单文件Python片段的集合。我们将PLL与两种最先进的自动依赖推理方法进行了比较，即PyEGo和ReadPyE，以解决依赖问题的能力进行比较。我们的结果表明，PLL可以修复比两个基线更多的依赖问题，比ReadPyE多218个（+15.97%），比PyEGo多281个（+21.58%）。我们的深入分析表明，PLL特别有利于具有许多依赖项的项目以及特定的第三方数值和机器学习模块。我们的发现证明了基于LLM的方法迭代解决Python依赖问题的潜力。|
|**2025-01-27**|**TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference**|Jack Min Ong et.al.|[2501.16007](http://arxiv.org/abs/2501.16007)|null|大型语言模型（LLMs）已被证明具有很高的能力，但访问最佳模型目前依赖于推理提供商，这引入了信任挑战——我们如何确保提供商正在使用他们声称的模型配置？我们提出了一种名为TOPLOC的新方法，用于可验证推理，以解决此问题。TOPLOC利用一种紧凑的局部敏感哈希机制对中间激活进行操作，可以以100%的准确率检测模型、提示或精度的未授权修改，在我们的实证评估中实现了零误报和漏报。我们的方法在各种硬件配置、GPU类型和代数重排方面都具有鲁棒性，使得验证速度比原始推理快得多。通过引入多项式编码方案，TOPLOC将生成的提交的内存开销减少了1000倍，每32个新标记只需258字节的存储空间，相比之下，直接存储Llama-3.1-8B-Instruct的标记嵌入需要262KB。我们的方法使用户能够高效地验证LLM推理计算，促进了开放生态系统中的信任和透明度，并为去中心化和可验证的AI服务奠定了基础。|
|**2025-01-27**|**Aging-aware CPU Core Management for Embodied Carbon Amortization in Cloud LLM Inference**|Tharindu B. Hewage et.al.|[2501.15829](http://arxiv.org/abs/2501.15829)|**[link](https://github.com/tharindu-b-hewage/splitwise-sim-cpu-carbon)**|**大规模语言模型（LLM）的广泛应用要求云LLM推理集群迅速扩展，导致实体碳（制造和供应IT资产的排放）的积累，这些排放主要集中于推理服务器CPU。本文深入探讨了云LLM推理可持续增长所面临的挑战，强调在更长的使用寿命中分摊CPU的实体成本。鉴于硅老化带来的可靠性风险，我们提出了一种老化感知的CPU核心管理技术，以延迟CPU老化效应，使集群操作员能够安全地延长CPU的使用寿命。我们的技术利用了我们在云LLM推理中发现的CPU低利用率模式，通过在未使用的核心中停止老化并在活动核心中通过选择深度休眠和老化感知的推理任务分配来均匀老化。通过使用真实的Azure推理跟踪和微软的一个扩展LLM集群模拟器进行的广泛模拟，我们证明了我们的技术相对于现有方法的优越性能，通过管理CPU老化效应的p99性能，预计每年可减少37.67%的实体碳排放，CPU低利用率降低77%，对推理服务质量的负面影响不到10%。**|
|**2025-01-25**|**Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads**|Xingyang He et.al.|[2501.15113](http://arxiv.org/abs/2501.15113)|null|KV 缓存是大语言模型（LLM）推理中广泛使用的一种加速技术。然而，随着输入长度的增加，其内存需求迅速增长。先前的研究通过为所有注意力头移除相同数量的非重要标记，或者为预识别的注意力头分配差异化的 KV 缓存预算来减小 KV 缓存的大小。然而，由于不同任务中注意力头的重要性存在差异，预识别的注意力头无法有效地适应各种下游任务。为了解决这一问题，我们提出了一种名为 Task-KV 的方法，该方法利用注意力头的语义差异化，为各种任务分配差异化的 KV 缓存预算。我们证明，远离语义中心的注意力头（称为异构头）对任务输出和语义理解做出了重大贡献。相比之下，其他注意力头则扮演着聚合重要信息和聚焦推理的角色。Task-KV 为异构头分配完整的 KV 缓存预算，以保留全面的语义信息，同时为非异构头保留少量最近标记和注意力汇聚点。此外，我们创新性地引入了中间激活来保留从非异构头聚合的关键上下文信息。为了动态感知注意力头之间的语义差异，我们设计了一个语义分隔器，根据它们与语义中心的距离来区分异构头和非异构头。在多个基准和不同模型架构上的实验结果表明，Task-KV 显著优于现有的基线方法。|
|**2025-01-25**|**RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations**|Zunhai Su et.al.|[2501.16383](http://arxiv.org/abs/2501.16383)|**[link](https://github.com/ZunhaiSu/RotateKV)**|**键值（KV）缓存通过避免重复计算过去的键值，促进了高效的大型语言模型（LLM）推理。随着批处理大小和上下文长度的增加，过大的KV缓存成为一个显著的内存瓶颈，突显了高效压缩的必要性。现有的KV量化依赖于细粒度量化或保留大量高比特宽缓存的一部分，这两种方法都牺牲了压缩率，并且在极低的平均比特宽度下往往无法保持鲁棒性。在本研究中，我们探索了旋转技术在2比特KV量化中的潜力，并提出了RotateKV，它通过以下创新实现了精确和鲁棒的性能：（i）异常值感知旋转，利用通道重排序来适应不同通道的异常值分布，而不牺牲快速华莱士-哈达马变换（FWHT）的计算效率；（ii）预RoPE分组头旋转，减轻了旋转位置嵌入（RoPE）对提出的异常值感知旋转的影响，并进一步平滑了跨头的异常值；（iii）注意力下沉感知量化，利用大量的激活来精确地识别和保护注意力下沉。RotateKV在WikiText-2上使用LLaMA-2-13B进行2比特量化时，实现了低于0.3的困惑度（PPL）下降，保持了强大的CoT推理和长上下文能力，在GSM8K上的下降低于1.7%，即使在较低的平均比特宽度下也优于现有方法。RotateKV还展示了峰值内存使用量减少3.97倍，支持5.75倍更大的批处理大小，并在解码阶段实现了2.32倍的速度提升。**|
|**2025-01-24**|**Locality-aware Fair Scheduling in LLM Serving**|Shiyi Cao et.al.|[2501.14312](http://arxiv.org/abs/2501.14312)|null|大型语言模型（LLM）的推理工作负载占据了从多轮对话到文档分析等多种现代AI应用的广泛领域。在管理具有不同前缀模式的多样化客户端工作负载时，平衡公平性和效率至关重要。遗憾的是，现有的针对LLM服务的公平调度算法，如虚拟令牌计数器（VTC），未能考虑前缀局部性，因此性能较差。另一方面，现有LLM服务框架中的局部性感知调度算法往往在最大化前缀缓存命中率的同时，未考虑客户端之间的公平共享。本文介绍了第一个局部性感知的公平调度算法，即“缺陷最长前缀匹配”（Deficit Longest Prefix Match，DLPM），它可以在保证公平性的同时保持高程度的前缀局部性。我们还介绍了一种新颖的算法，即“双缺陷LPM”（Double Deficit LPM，D $^2$LPM），它扩展了DLPM以适应分布式环境，可以在公平性、局部性和负载均衡之间找到一个平衡点。我们的广泛评估表明，DLPM和D$^2$ LPM在确保公平性的同时，保持了高吞吐量（比VTC高2.87倍）和低客户端延迟（比最先进的分布式LLM服务系统低7.18倍）。|
|**2025-01-20**|**Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference**|Pouya Hamadanian et.al.|[2501.11779](http://arxiv.org/abs/2501.11779)|**[link](https://github.com/microsoft/glinthawk)**|**大型语言模型（LLM）彻底改变了自然语言处理领域，但它们的推理需求庞大的资源，同时高端加速器如GPU的使用率却很低。一个主要的瓶颈来自于注意力机制，它需要存储大量的键值缓存，这使得最大可达到的吞吐量远低于可用的计算资源。目前的方法试图通过内存高效的注意力和分页机制来缓解这个问题，但仍然受到所有操作必须在高端加速器上执行的假设的限制。在本文中，我们提出了Glinthawk，这是一种双层架构，将注意力机制从Transformer模型的其余部分解耦。这种方法允许注意力机制的内存需求独立扩展，从而实现更大的批量大小和更高效的高端加速器使用。我们使用NVIDIA T4 GPU作为一层，标准CPU虚拟机作为另一层来原型化Glinthawk。与传统单层设置相比，它将吞吐量提高了5.9倍，并将生成成本降低了2.8倍。对于更长的序列长度，它在成本降低2.4倍的情况下实现了16.3倍的吞吐量提升。我们的评估表明，这种架构可以容忍适度的网络延迟，并且性能退化最小，使其在批处理等延迟容忍、吞吐量导向的应用中非常有效。我们在https://github.com/microsoft/glinthawk上公开了我们的原型。**|
|**2025-01-20**|**Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas**|Nishant Balepur et.al.|[2501.11549](http://arxiv.org/abs/2501.11549)|**[link](https://github.com/pinafore/alignment-personalization)**|**大型语言模型通过学习用户对两个输出中哪一个更偏好来调整以遵循指令（对齐）。然而，这种偏好数据格式并没有传达用户为什么偏好选择或拒绝的响应，因此在这些数据集上训练的LLM无法根据不同的用户需求定制响应。为了揭示这些个性化的参数，我们应用溯因推理对偏好数据进行处理，推断用户的需求和兴趣，即可能偏好每个输出的用户角色。我们通过以下两个步骤测试这一想法：角色推断（PI）——通过溯因推理出偏好选择或拒绝输出的用户角色，以及角色定制（PT）——训练模型根据PI中的角色定制响应。我们发现：1）LLM能够准确地推断出角色，解释为什么不同的用户可能偏好选择或拒绝的输出；2）通过PT在PI角色增强的偏好数据上训练，可以提升个性化，使模型能够支持用户编写的角色；3）被拒绝的响应角色形成更困难的个性化评估，表明PT在帮助具有不常见偏好的用户方面比典型的对齐方法更有效。我们主张对个性化偏好采用溯因观点，不仅询问哪个响应更好，还要询问何时、为什么以及为谁。**|
|**2025-01-19**|**GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation**|Shashikant Ilager et.al.|[2501.11006](http://arxiv.org/abs/2501.11006)|**[link](https://github.com/large-scale-sustainable-computing-lsc/green-code)**|大型语言模型（LLMs）正成为日常生活不可或缺的一部分，展现出其在各种自然语言处理（NLP）任务中的巨大潜力。除了NLP，LLMs在软件开发任务中也越来越多地被使用，例如代码补全、修改、错误修复和代码翻译。软件工程师广泛使用GitHub Copilot和Amazon Q等工具，通过高精度自动化任务来简化工作流程。尽管LLMs训练的资源消耗和能源强度经常被强调，但随着时间的推移，推理过程可能更加资源密集，因为它是一个具有大量调用的持续过程。因此，开发针对LLMs推理的资源高效替代方案对于可持续性至关重要。这项工作提出了GREEN-CODE，这是一个针对LLMs能效代码生成的框架。GREEN-CODE在LLMs推理过程中执行动态早期退出。我们训练了一个强化学习（RL）智能体，使其学会在准确性、延迟和能源消耗之间平衡权衡。我们的方法在两个开源LLMs（Llama 3.2 3B和OPT 2.7B）上进行了评估，使用了JavaCorpus和PY150数据集。结果显示，我们的方法在代码生成任务中平均降低了23-50%的能源消耗，而不会显著影响准确性。|
|**2025-01-17**|**A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks**|Xinzhe Li et.al.|[2501.10069](http://arxiv.org/abs/2501.10069)|**[link](https://github.com/xinzhel/llm-agent-survey)**|通过搜索进行的LLM测试时计算（或LLM推理）已成为一个充满活力的研究领域，发展迅速。然而，现有的框架在三个关键方面（任务定义、LLM分析和搜索过程）上往往采用不同的观点，这使得直接比较变得困难。此外，所采用的搜索算法通常与标准实现不同，且它们的特定特性并未得到充分说明。在这篇综述中，我们提供了一个全面的技术回顾，统一了任务定义，并提供了LLM分析和搜索过程的模块化定义。这些定义使得对各种LLM推理框架的精确比较成为可能，同时突出了它们与常规搜索算法的差异。我们还讨论了这些方法的应用性、性能和效率。有关更详细的内容和持续更新，请参阅我们的GitHub仓库：https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md|
|**2025-01-16**|**Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition**|Takaaki Hori et.al.|[2501.09258](http://arxiv.org/abs/2501.09258)|null|本文提出了一种针对大型语言模型（LLMs）的端到端自动语音识别（E2E-ASR）的高效解码方法。尽管浅层融合是将语言模型融入E2E-ASR解码中最常见的做法，但我们在使用LLMs时遇到了两个实际问题。（1）LLM推理计算成本高。（2）ASR模型和LLM之间可能存在词汇不匹配。为了解决这种不匹配，我们需要重新训练ASR模型和/或LLM，这在最佳情况下是耗时的，在许多情况下甚至不可行。我们提出了“延迟融合”，该方法在解码过程中延迟将LLM评分应用于ASR假设，从而使得在ASR任务中更容易使用预训练的LLMs。这种方法不仅可以减少LLM评分的假设数量，还可以减少LLM推理调用的数量。如果ASR和LLM采用不同的标记化，该方法还允许在解码过程中重新标记化ASR假设。我们通过使用LibriHeavy ASR语料库和三个公开的LLMs，即OpenLLaMA 3B & 7B和Mistral 7B，证明了延迟融合与浅层融合和N-best重评分相比，在解码速度和准确性方面均有提升。|
|**2025-01-15**|**Guiding Retrieval using LLM-based Listwise Rankers**|Mandeep Rathee et.al.|[2501.09186](http://arxiv.org/abs/2501.09186)|**[link](https://github.com/mandeep-rathee/llmgar)**|**大型语言模型（LLMs）在“列表式”设置中展现出作为重排序器的强大潜力，在这种设置中，LLM被提示一次性重排序多个搜索结果。然而，这种“级联”检索和重排序方法受到召回限制问题的限制：最初未检索到的相关文档将永久性地排除在最终排名之外。自适应检索技术解决了这个问题，但不能与列表式重排序器一起工作，因为它们假设文档的分数是独立于其他文档计算的。在本文中，我们提出了一种现有自适应检索方法的改编，该方法支持列表式设置并有助于引导检索过程本身（从而克服了LLM重排序器的召回限制问题）。具体来说，我们提出的算法将初始排名和迄今为止看到的最相关文档提供的反馈文档的结果合并。通过在多样化的LLM重排序器、第一阶段检索器和反馈源上进行的广泛实验，我们证明了我们的方法可以将nDCG@10提高高达13.23%，并将召回率提高28.02%——所有这些都是在保持LLM推理总数恒定和自适应过程带来的开销最小的情况下实现的。这项工作为在初始结果池有限的环境中利用基于LLM的搜索打开了大门，例如，由旧系统或部署语义第一阶段的开销限制。**|
|**2025-01-14**|**Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings**|Paul Joe Maliakel et.al.|[2501.08219](http://arxiv.org/abs/2501.08219)|null|大型语言模型（LLMs）在许多自然语言处理（NLP）任务中显示出显著的改进，加速了它们在多个行业的快速应用。这些模型资源密集，无论是在训练还是推理过程中都需要大量的计算资源，导致能源消耗增加和负面的环境影响。随着其应用的加速，LLMs的可持续性已成为一个关键问题，需要优化其运行效率而不影响性能的策略。因此，确定显著影响LLMs性能和能源效率的参数至关重要。为此，本研究调查了这些参数在推理过程中对LLMs性能和能源效率的影响，并分析了它们的权衡。首先，我们通过基准测试LLMs，如Falcon-7B、Mistral-7B-v0.1、T5-3B、GPT-2、GPT-J-6B和GPT-Neo-2.7B，分析了不同类型的模型在参数数量和架构各异的情况下在文本生成、问答和摘要等任务上的表现。其次，我们研究了输入和输出序列特征，如序列长度，与能源消耗、性能和吞吐量之间的关系。最后，我们探讨了基于硬件的节能技术，即动态电压频率调整（DVFS），对模型延迟和能源效率的影响。我们的广泛基准测试和统计分析揭示了许多有趣的结果，揭示了特定优化如何减少能源消耗，同时保持吞吐量和准确性。本研究为研究人员和从业者提供了有关设计节能LLMs推理系统的可行见解。|
|**2025-01-14**|**PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving**|Ahmet Caner Yüzügüler et.al.|[2501.08192](http://arxiv.org/abs/2501.08192)|null|大型语言模型（LLMs）在各种应用中被广泛使用，但它们巨大的计算需求带来了重大挑战，尤其是在HBM带宽瓶颈和设备间通信开销方面。在本文中，我们提出了一种名为PRESERVE的新型预取框架，旨在通过重叠模型权重和KV缓存的内存读取与集体通信操作来优化LLM推理。通过在商业AI加速器上进行的广泛实验，我们展示了在最先进的开源LLMs上实现了高达1.6倍的端到端速度提升。此外，我们还进行了设计空间探索，确定了该方法的最佳硬件配置，通过选择最佳L2缓存大小，进一步实现了每成本1.25倍的性能提升。我们的结果表明，PRESERVE有潜力缓解内存瓶颈和通信开销，为提高LLM推理系统的性能和可扩展性提供了一种解决方案。|
|**2025-01-14**|**Hierarchical Autoscaling for Large Language Model Serving with Chiron**|Archit Patke et.al.|[2501.08090](http://arxiv.org/abs/2501.08090)|null|大型语言模型（LLM）服务正成为云服务提供商越来越重要的工作负载。根据性能服务级别目标（SLO）要求，LLM推理请求可以分为两类：（a）具有紧密SLO（以秒为单位）的交互式请求，和（b）具有宽松SLO（以分钟到小时为单位）的批量请求。这些SLO会根据到达率、复用和配置参数而降低，因此需要在服务实例及其批量大小上使用资源自动扩展。然而，之前用于LLM服务的自动扩展器没有考虑请求SLO，导致不必要的扩展和资源利用率低下。为了解决这些局限性，我们引入了Chiron，这是一个使用基于队列大小、利用率和SLO的分层背压概念的自动扩展器。我们的实验表明，与现有解决方案相比，Chiron实现了高达90%的SLO达成率，并将GPU效率提高了高达70%。|
|**2025-01-12**|**MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large Language Model Inference**|Wenxuan Zeng et.al.|[2501.06807](http://arxiv.org/abs/2501.06807)|null|基于安全多方计算（MPC）的私有大型语言模型（LLM）推理为用户提示和专有模型权重提供了密码学安全的保护。然而，对于长输入序列，它面临着较大的延迟开销。虽然已经提出了键值（KV）缓存淘汰算法来减少明文推理的计算和内存成本，但它们并非为MPC设计，因此无法轻易应用于私有推理。在本文中，我们提出了一种准确且适合MPC的KV缓存淘汰框架，称为MPCache。MPCache基于观察，即长序列中的历史标记可能对下游解码有不同的影响。因此，MPCache结合了一种单次查看的静态淘汰算法来丢弃不重要的标记，以及一种查询感知的动态选择算法来进一步选择一小部分标记进行注意力计算。由于现有的动态选择算法会产生过多的延迟，我们提出了一系列优化措施，以大幅减少KV缓存选择开销，包括MPC友好的相似度近似、分层KV缓存聚类和跨层索引共享策略。通过大量实验，我们证明了MPCache在不同LLM生成任务中始终优于现有的KV缓存淘汰基线，并且在不同的序列长度上分别实现了1.8~2.01倍的解码延迟和3.39~8.37倍的通信减少。|
|**2025-01-05**|**TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms**|Jovan Stojkovic et.al.|[2501.02600](http://arxiv.org/abs/2501.02600)|null|随着对生成式大型语言模型（LLMs）的需求不断上升，这在云计算数据中心中为热管理和电力管理带来了挑战。传统的技术往往不适用于LLM推理，因为它们具有精细的、毫秒级的执行阶段，每个阶段都有独特的性能、热和功耗特性。此外，LLM推理工作负载对各种配置参数敏感，例如模型并行性、大小和量化，这些参数需要在性能、温度、功耗和输出质量之间进行权衡。而且，云通常将SaaS和IaaS工作负载部署在同一地点，它们具有不同级别的可见性和灵活性。我们提出了TAPAS，这是一个为云中LLM推理集群设计的温度和功率感知框架。TAPAS增强了冷却和电力超用能力，在有效处理紧急情况（例如，冷却和电力故障）的同时，降低了总拥有成本（TCO）。该系统利用历史温度和电力数据以及SaaS工作负载的适应性，以：（1）在冷却和电力约束下高效地放置新的GPU工作负载虚拟机，（2）跨SaaS虚拟机路由LLM推理请求，（3）重新配置SaaS虚拟机以管理负载峰值和紧急情况。我们在一个大型GPU集群上的评估表明，显著减少了热和功耗限制事件，提升了系统效率。|
|**2025-01-04**|**AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference**|Zhuomin He et.al.|[2501.02336](http://arxiv.org/abs/2501.02336)|**[link](https://github.com/asisys/adaskip)**|**长上下文大型语言模型（LLM）的推理变得越来越重要，这促使许多研究致力于减轻此类场景中的大量存储和计算成本。层跳过方法是一种有希望的优化，但在长上下文推理中很少被探索。我们观察到，现有的层跳过策略在应用于长上下文推理时存在一些局限性，包括无法适应模型和上下文的变化、忽视子层的意义以及不适用于预填充阶段。本文提出了一种名为\sysname的自适应子层跳过方法，专门针对长上下文推理。\sysname通过利用动态相似性信息自适应地识别出不太重要的层，实现了子层跳过，并加速了预填充和解码阶段。通过在多种长上下文基准和模型上进行的广泛实验，验证了\sysname的有效性，并展示了其在现有基线之上的优越推理性能。**|
|**2025-01-03**|**Efficient LLM Inference with Activation Checkpointing and Hybrid Caching**|Sanghyeon Lee et.al.|[2501.01792](http://arxiv.org/abs/2501.01792)|null|近期，具有巨大模型规模的大型语言模型（LLMs）需要使用大量GPU来满足内存容量需求，这导致了生成标记的巨大成本。为了提供具有宽松延迟约束的经济高效的LLM推理，大量研究集中在通过利用主机内存来扩展GPU内存。然而，利用主机内存的LLM推理引擎通常面临GPU计算单元的利用率不足，因为相当一部分推理时间被花费在通过主机-GPU互连将模型加载到GPU上。为了解决LLM中主机内存卸载的这些挑战，我们引入了HybridServe，这是一个基于激活缓存的激活检查点的LLM推理系统。激活缓存存储在中间推理阶段生成的激活检查点，允许在将模型参数从主机内存传输到GPU的同时快速重新计算KV缓存。与使用标记ID从头开始重新计算KV缓存的传统方法不同，激活缓存允许绕过投影和FFN操作。为了平衡激活重新计算和参数加载开销，本研究提出了一种KV-激活混合缓存方案，该方案找到最佳的关键值和激活缓存比例以调整重新计算时间。我们的系统在卸载模型权重和KV缓存方面比最先进的前期工作实现了2.19倍的吞吐量提升。|
|**2025-01-02**|**BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference**|Wonsuk Jang et.al.|[2501.01144](http://arxiv.org/abs/2501.01144)|**[link](https://code.stanford.edu/tambe-lab/blockdialect)**|大型语言模型（LLMs）取得了显著的成就，但它们不断增长的大小在内存使用和计算成本上带来了重大挑战。量化权重和激活可以解决这些问题，而细粒度的块量化正成为缓解异常值的有希望的支持硬件的解决方案。然而，现有方法难以捕捉细微的块数据分布。为了解决这个问题，我们提出了BlockDialect，这是一种块级的细粒度混合格式技术，它从formatbook中为每个块分配最佳的数量格式，以更好地表示数据。此外，我们引入了DialectFP4，这是一个FP4变体的formatbook（类似于方言），它可以适应不同的数据分布。为了有效地利用这一技术，我们提出了一个两阶段方法用于在线DialectFP4激活量化。重要的是，DialectFP4通过选择与低精度整数算术兼容的缩放整数作为可表示的值来确保硬件效率。与MXFP4格式相比，BlockDialect在LLaMA3-8B（LLaMA2-7B）模型上实现了11.83%（7.56%）的准确率提升，并且每个数据使用更低的比特数，即使在量化全路径矩阵乘法时，也只比全精度低5.46%（2.65%）。关注于表示方式而非缩放方式，我们的工作为节能的LLM推理展示了一条有希望的道路。|
|**2025-01-02**|**FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving**|Zihao Ye et.al.|[2501.01005](http://arxiv.org/abs/2501.01005)|**[link](https://github.com/flashinfer-ai/flashinfer)**|**Transformer，由注意力机制驱动，构成了大型语言模型（LLM）的基础。随着这些模型规模的扩大，高效的GPU注意力内核对于高吞吐量和低延迟推理变得至关重要。各种LLM应用需求灵活且高性能的注意力解决方案。我们提出FlashInfer：一个可定制的、高效的LLM服务注意力引擎。FlashInfer通过使用块稀疏格式和可组合格式来处理KV缓存存储的异构性，优化内存访问并减少冗余。它还提供可定制的注意力模板，通过即时（JIT）编译实现对不同设置的适应性。此外，FlashInfer的负载均衡调度算法能够适应用户请求的动态性，同时与需要静态配置的CUDAGraph保持兼容。FlashInfer已被集成到SGLang、vLLM和MLC-Engine等领先的LLM服务框架中。全面的内核级和端到端评估表明，FlashInfer能够在不同的推理场景中显著提升内核性能：与最先进的LLM服务解决方案相比，FlashInfer在LLM服务基准的编译器后端中实现了29-69%的跨标记延迟降低，在长上下文推理中实现了28-30%的延迟降低，在并行生成LLM服务中实现了13-17%的速度提升。**|
|**2024-12-29**|**TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication**|Zongwu Wang et.al.|[2412.20501](http://arxiv.org/abs/2412.20501)|**[link](https://github.com/aca-lab-sjtu/token-ring)**|**高效并行化具有长序列的大型语言模型（LLMs）至关重要，但由于其巨大的计算和内存需求，特别是由于注意力机制中的通信瓶颈，这极具挑战性。虽然序列并行性（SP）已被引入作为潜在解决方案，但现有方法往往受到可扩展性有限或效率低下的问题，从而限制了其有效性。环状注意力（Ring-Attention）展示了在序列处理中扩展的潜力，但由于其依赖于对等（P2P）通信和无效利用网络资源，因此面临重大限制。随着序列并行度的增加，每一步的计算时间呈二次减少，与通信量的线性减少形成鲜明对比，加剧了通信瓶颈。为了解决这些挑战，我们提出了TokenRing，这是一个细粒度并行框架，利用双向P2P通信有效地重叠计算和数据传输。通过将注意力块分区，并在全连接网状拓扑结构中并发传输查询和块输出（即 $block\_out$和$block\_lse$ ），TokenRing实现了显著的通信开销减少和更好的负载平衡。这些创新提高了分布式Transformer模型的可扩展性和效率，尤其是对于长上下文序列。实验结果表明，TokenRing提高了吞吐量并减少了通信延迟。此外，其设计能够无缝适应各种多GPU互连解决方案，如华为Ascend，确保了分布式LLM推理和训练的广泛兼容性和成本效益。代码可在以下网址获取：\url{https://github.com/ACA-Lab-SJTU/token-ring}。**|
|**2024-12-28**|**LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System**|Hyucksung Kwon et.al.|[2412.20166](http://arxiv.org/abs/2412.20166)|null|大型语言模型（LLMs）的扩展，其参数量达到数百亿，对计算资源提出了重大挑战，尤其是在数据移动和内存带宽方面。长上下文LLMs，处理包含数万个标记的序列，由于注意力层复杂性和键值缓存大小与上下文长度成正比，进一步增加了对内存系统的需求。内存中处理（PIM）通过将计算移动到数据中来最大化内存带宽，可以解决内存带宽问题；然而，由于每个模块的内存容量有限以及固定功能单元PIM架构和静态内存管理的僵化性，PIM并不一定能扩展以加速长上下文LLMs。在本工作中，我们提出了LoL-PIM，这是一种多节点PIM架构，通过硬件-软件协同设计来加速长上下文LLMs。具体来说，我们提出了如何在多PIM模块间利用流水线并行性，同时提出了直接PIM访问（DPA）控制器（或PIM的DMA），它能够实现动态PIM内存管理，并在各种上下文长度上实现高效的PIM利用。我们开发了一个基于MLIR的编译器，用于LoL-PIM，扩展了一个商业PIM编译器，其中软件修改得到了实现和评估，而硬件更改在模拟器中进行了建模。我们的评估表明，LoL-PIM显著提高了长上下文LLMs推理的吞吐量和降低了延迟，优于多GPU和GPU-PIM系统（分别达到8.54倍和16.0倍的速度提升），从而使得LLMs在现实应用中的部署更加高效。|
|**2024-12-27**|**A Survey on Large Language Model Acceleration based on KV Cache Management**|Haoyang Li et.al.|[2412.19442](http://arxiv.org/abs/2412.19442)|**[link](https://github.com/treeai-lab/awesome-kv-cache-management)**|**大型语言模型（LLMs）由于能够理解上下文并执行逻辑推理，已经在自然语言处理、计算机视觉和多模态任务等多个领域实现了革命性的变革。然而，LLMs在推理过程中的计算和内存需求，尤其是在扩展到现实世界、长上下文和实时应用时，带来了重大挑战。键值（KV）缓存管理作为一种关键优化技术，通过减少冗余计算和提升内存利用率，已成为加速LLM推理的重要手段。本文综述了用于加速LLM的KV缓存管理策略，将它们分为基于标记、模型和系统三个层次的优化。基于标记的策略包括KV缓存选择、预算分配、合并、量化以及低秩分解，而模型级优化则关注架构创新和注意力机制，以提高KV的重用率。系统级方法则解决内存管理、调度和硬件感知设计等问题，以提高不同计算环境下的效率。此外，本文还概述了用于评估这些策略的文本和多模态数据集及基准。通过提供详细的分类和比较分析，本研究旨在为研究人员和实践者提供有价值的见解，以支持高效和可扩展的KV缓存管理技术的发展，为LLMs在现实世界应用中的实际部署做出贡献。KV缓存管理的精选论文列表见：\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}。**|
|**2024-12-27**|**An Engorgio Prompt Makes Large Language Model Babble on**|Jianshuo Dong et.al.|[2412.19394](http://arxiv.org/abs/2412.19394)|**[link](https://github.com/jianshuod/engorgio-prompt)**|**自回归大型语言模型（LLMs）在许多实际任务中取得了令人印象深刻的性能。然而，这些LLMs的新范式也暴露了新的威胁。在本文中，我们探讨了它们对推理成本攻击的易损性，其中恶意用户构建Engorgio提示，有意增加推理过程的计算成本和延迟。我们设计了Engorgio，一种新颖的方法，以高效地生成对抗性Engorgio提示，影响目标LLM的服务可用性。Engorgio有以下两个技术贡献。（1）我们采用参数化分布来跟踪LLMs的预测轨迹。（2）针对LLMs推理过程的自回归特性，我们提出了新的损失函数，以稳定地抑制<EOS>标记的出现，其出现将中断LLMs的生成过程。我们在13个开源LLMs上进行了广泛的实验，这些LLMs的参数范围从125M到30B。结果表明，Engorgio提示可以在白盒场景下成功诱导LLMs生成异常长的输出（即，达到90%+输出长度限制的时间大约是2-13倍更长），我们的现实世界实验证明了Engorgio对有限计算资源的LLM服务的威胁。代码可在https://github.com/jianshuod/Engorgio-prompt获取。**|
|**2024-12-25**|**Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference**|Libo Zhang et.al.|[2412.18934](http://arxiv.org/abs/2412.18934)|null|由于大型语言模型（LLMs）对资源需求高，在消费级设备上实现广泛部署面临重大挑战。通常，个人或消费级设备，包括在大规模模型时代之前配置的服务器，一般拥有相对较弱的GPU和相对较强的CPU。然而，大多数当前方法主要依赖于GPU进行计算。因此，我们提出了Dovetail方法，该方法将草稿模型部署在GPU上以生成草稿标记，同时允许目标模型在CPU上并行验证，从而提高所有可用硬件资源的利用率并减少设备间通信带宽。相应地，我们对草稿模型进行了重新设计，以更好地适应异构硬件特性。为此，我们实现了几个优化：减少草稿标记的数量以减轻并行验证的延迟，增加草稿模型的深度以提高其预测能力，并引入DGF（动态门控融合）以改善特征和标记嵌入的整合。在HumanEval基准测试中，Dovetail使用3GB VRAM实现了LLaMA2-Chat-7B的每秒5.86个标记的推理速度，与仅使用CPU的推理相比，大约提高了2.77倍。此外，当使用7GB VRAM时，推理速度提高到了每秒8个标记。|
|**2024-12-23**|**Highly Optimized Kernels and Fine-Grained Codebooks for LLM Inference on Arm CPUs**|Dibakar Gope et.al.|[2501.00032](http://arxiv.org/abs/2501.00032)|**[link](https://github.com/ggerganov/llama.cpp)**|**大型语言模型（LLMs）彻底改变了我们对语言理解和生成的看法，吸引了研究人员和开发者的极大兴趣。然而，由于LLMs规模巨大、资源需求极高，将其部署进行推理一直是一个重大挑战。虽然将模型权重量化到子字节精度已被证明是缓解内存压力的有希望的方法，但通常用于LLMs量化的分组量化格式存在显著的计算开销和资源密集型的反量化过程。因此，大量计算指令并未执行乘法操作，即实际工作，这使得它们无法满足在通用CPU上部署LLMs所需的延迟要求。在本研究中，我们提出了一套高度优化的内核，以加速LLMs推理并充分发挥CPU（尤其是ARM CPU）的潜力。这些内核将加载操作数和权重解包的成本分摊到多个输出行中。此外，通过引入优化的交错分组数据布局，以及通过优化解压路径来减少不必要的操作和反量化开销，同时最大化使用向量矩阵乘法操作，显著提高了MAC操作效率。此外，我们提出了一种基于分组非均匀码本量化方法，用于对LLMs进行超低精度量化，以更好地匹配其权重分布中的非均匀模式，在生成标记期间实现更高的吞吐量，同时确保比现有技术更好的质量。将这些改进应用于4位LLMs，与基于LLaMA.cpp的解决方案相比，在ARM CPU上的提示处理速度提高了3-3.2倍，自回归解码速度提高了2倍。这些优化内核可在https://github.com/ggerganov/llama.cpp上获得。**|
|**2024-12-21**|**SYMPHONY: Improving Memory Management for LLM Inference Workloads**|Saurabh Agarwal et.al.|[2412.16434](http://arxiv.org/abs/2412.16434)|null|大型语言模型（LLMs）正越来越多地应用于聊天机器人、代码编辑器和对话代理等应用中。LLMs的一个关键特性是它们能够与人类或外部工具进行多轮交互，从而实现各种任务。在多轮交互中，每个新的请求都依赖于先前请求的中间状态，特别是当前交互中的键值（K，V）缓存。现有的服务引擎要么重新计算K，V缓存，要么将其卸载到主内存中。分析发现，重新计算会导致超过99%的处理令牌是冗余的。另一方面，将K，V缓存从GPU内存卸载会使推理服务状态化，导致集群负载不均衡。为了解决这些挑战，我们开发了SYMPHONY。SYMPHONY利用观察到的多轮工作负载提供了额外的提示，这些提示允许将K，V缓存迁移出关键的服务路径。通过利用这些提示，SYMPHONY动态迁移K，V缓存，以实现推理请求的细粒度调度。我们的实验表明，与最先进的基础线相比，SYMPHONY可以处理超过8倍数量的请求，并且具有相似的延迟特征。|
|**2024-12-20**|**WebLLM: A High-Performance In-Browser LLM Inference Engine**|Charlie F. Ruan et.al.|[2412.15803](http://arxiv.org/abs/2412.15803)|**[link](https://github.com/mlc-ai/web-llm)**|**大型语言模型（LLMs）的进步解锁了令人瞩目的能力。虽然部署这些模型通常需要服务器级GPU和基于云的推理，但最近出现的较小规模的开源模型以及越来越强大的消费级设备使得在设备上部署成为可能。作为设备上部署的平台，网络浏览器具有普遍的易用性，提供了自然的代理环境，并且方便地将不同设备厂商的后端抽象出来。为了应对这一机遇，我们引入了WebLLM，这是一个开源的JavaScript框架，它使得LLMs的高性能推理完全在浏览器中成为可能。WebLLM提供了一个类似OpenAI风格的API，以便无缝集成到Web应用程序中，并利用WebGPU进行高效的本地GPU加速和WebAssembly进行高性能的CPU计算。通过机器学习编译器MLC-LLM和Apache TVM，WebLLM利用优化的WebGPU内核，克服了高性能WebGPU内核库的缺失。评估显示，WebLLM在相同设备上可以保留高达80%的本地性能，并有进一步缩小差距的空间。WebLLM为网络浏览器中的普遍可访问、隐私保护、个性化以及本地驱动的LLM应用程序铺平了道路。代码可在以下链接获取：https://github.com/mlc-ai/web-llm。**|
|**2024-12-19**|**GFormer: Accelerating Large Language Models with Optimized Transformers on Gaudi Processors**|Chengming Zhang et.al.|[2412.19829](http://arxiv.org/abs/2412.19829)|null|异构硬件如Gaudi处理器已被开发出来以增强计算能力，特别是针对基于Transformer的大语言模型（LLM）在生成式AI任务中的矩阵运算。然而，我们的分析表明，Transformer在这些新兴硬件上并未得到充分优化，这主要是因为非矩阵计算内核（如Softmax）的优化不足以及异构资源利用不充分，尤其是在处理长序列时。为了解决这些问题，我们提出了一种综合方法（称为GFormer），该方法将稀疏和线性注意力机制合并。GFormer旨在最大化Gaudi处理器的矩阵乘法引擎（MME）和张量处理核心（TPC）的计算能力，同时不降低模型质量。GFormer包括一个窗口自注意力内核和一个高效的因果线性注意力外积内核，旨在优化LLM在Gaudi处理器上的推理。评估结果显示，GFormer在Gaudi处理器上显著提高了各种任务的效率和模型性能，并优于最先进的GPU。|
|**2024-12-18**|**A Survey on LLM Inference-Time Self-Improvement**|Xiangjue Dong et.al.|[2412.14352](http://arxiv.org/abs/2412.14352)|**[link](https://github.com/dongxiangjue/Awesome-LLM-Self-Improvement)**|**最近，通过测试时增加计算来增强推理的技术受到了关注。在本调查中，我们从三个不同的角度研究了LLM推理时自我改进的现状：独立自我改进，侧重于通过解码或采样方法进行增强；上下文感知自我改进，利用额外的上下文或数据存储；以及模型辅助自我改进，通过模型协作实现改进。我们提供了对最近相关研究的全面回顾，贡献了一个深入的分类法，并讨论了挑战和局限性，为未来的研究提供了洞见。**|
|**2024-12-17**|**Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models**|Seungeun Oh et.al.|[2412.12687](http://arxiv.org/abs/2412.12687)|null|本文研究了一种混合语言模型（HLM）架构，该架构将运行在移动设备上的小型语言模型（SLM）与无线网络基站（BS）托管的大型语言模型（LLM）相结合。HLM的令牌生成过程遵循推测性推理原则：SLM的词汇分布被上传到LLM，LLM接受或拒绝它，被拒绝的令牌由LLM重新采样。虽然这种方法确保了SLM和LLM词汇分布的一致性，但由于上行传输和运行两个语言模型的计算成本，它遭受了低令牌吞吐量的困扰。为了解决这个问题，我们提出了一种名为不确定性感知机会HLM（U-HLM）的新型HLM结构，其中SLM在本地测量其输出不确定性，并跳过可能被接受的令牌的上行传输和LLM操作。这种机会性跳过是通过我们关于SLM的不确定性与LLM的拒绝概率之间存在线性相关性的经验发现实现的。我们解析地推导出不确定性阈值，并评估其预期的拒绝风险。仿真表明，U-HLM减少了45.93%的上行传输和LLM计算，同时实现了高达LLM推理精度的97.54%，并且比没有跳过的HLM快2.54倍。|
|**2024-12-17**|**A System for Microserving of LLMs**|Hongyi Jin et.al.|[2412.12488](http://arxiv.org/abs/2412.12488)|null|最近在大型语言模型（LLM）方面的进步，对高效系统支持的需求日益强烈，以提升整体服务效率。随着LLM推理扩展到多个GPU甚至多个计算节点，服务系统中出现了各种协调模式，如预填充-解码解耦和上下文迁移。目前大多数推理服务都暴露了具有预先配置协调策略的粗粒度请求级API，这限制了自定义和动态重新配置协调的能力。在本文中，我们提出了LLM微服务，这是一种用于构建和编程LLM推理服务的多层次架构。我们引入了简单而有效的微服务API来支持细粒度的子请求级操作。一个可编程的路由器将用户请求转换为子请求调用，使服务模式能够动态重新配置。为了支持多样化的执行模式，我们开发了一个统一的KV缓存接口，用于处理各种KV计算、传输和重用场景。我们的评估表明，LLM微服务可以通过几行Python代码重新配置，以支持多种解耦编排策略，同时在LLM推理任务中保持最先进的性能。此外，它还使我们能够探索新的策略变体，与现有策略相比，这些变体可以将作业完成时间缩短高达47%。|
|**2024-12-16**|**CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation**|Hongxuan Zhang et.al.|[2412.11741](http://arxiv.org/abs/2412.11741)|null|随着利用大型语言模型（LLMs）的长文本应用的出现，带来了显著的扩展性挑战，尤其是在内存占用方面。负责存储注意力键和值以减少冗余计算的关键值（KV）缓存的线性增长可能会导致内存消耗的显著增加，这可能导致模型在有限的内存资源下无法正常服务。为了解决这个问题，我们提出了一种名为缓存稀疏表示（CSR）的新方法，该方法通过将密集的键值缓存张量转换为稀疏索引和权重，在LLM推理过程中提供更高效的内存表示。此外，我们引入了一种名为NeuralDict的新方法，这是一种基于神经网络的自动生成用于我们稀疏表示的字典的方法。我们广泛的实验表明，CSR在性能上与最先进的KV缓存量化算法相当，同时在内存受限环境中保持稳健的功能。|
|**2024-12-15**|**Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning**|Yun Qu et.al.|[2412.11120](http://arxiv.org/abs/2412.11120)|**[link](https://github.com/cloud-qu/lare)**|**强化学习（RL）在实际应用中常常面临延迟和稀疏的反馈，即使在只有阶段性奖励的情况下。先前的方法在奖励重新分配方面取得了一些进展，但仍面临挑战，包括由于冗余和忽略任务绩效评估的多面性而导致的模糊归因所引起的训练困难。希望大型语言模型（LLM）包含了丰富的决策知识，并为奖励重新分配提供了一种合理的工具。尽管如此，由于语言知识与符号形式要求之间的不匹配，以及推理中固有的随机性和幻觉，将LLM应用于此非同小可。为了解决这些问题，我们引入了LaRe，这是一种新型LLM赋能的基于符号的决策框架，以提高信用分配。LaRe的关键是潜在奖励的概念，它作为多维度的性能评估，能够从不同角度实现更可解释的目标达成，并促进更有效的奖励重新分配。我们考察了由LLM生成的语义代码如何将语言知识与符号潜在奖励联系起来，因为它可以用于符号对象。同时，我们设计了潜在奖励的自验证，以提高LLM推理的稳定性和可靠性。从理论上讲，潜在奖励中的与奖励无关的冗余消除有助于从更准确的奖励估计中提高RL性能。广泛的实验结果表明，LaRe（i）在时间信用分配方面优于SOTA方法，（ii）在分配多个代理之间的贡献方面表现卓越，（iii）在特定任务中优于使用真实奖励训练的策略。**|
|**2024-12-15**|**NITRO: LLM Inference on Intel Laptop NPUs**|Anthony Fei et.al.|[2412.11053](http://arxiv.org/abs/2412.11053)|**[link](https://github.com/abdelfattah-lab/nitro)**|**大型语言模型（LLMs）已成为自然语言处理领域的关键工具，在ChatGPT和Gemini等聊天机器人中得到了广泛应用，并成为研究的一个核心领域。一个特别感兴趣的研究方向包括为这些AI应用设计专门的硬件，其中之一就是神经处理单元（NPU）。2023年，英特尔发布了代号为Meteor Lake的Intel Core Ultra处理器，它集成了CPU、GPU和NPU系统芯片。然而，通过英特尔OpenVINO框架对NPU的官方软件支持仅限于静态模型推理。因此，LLMs中自回归标记生成的动态特性无法直接支持。为了解决这一不足，我们提出了NITRO（NPU推理优化器），这是一个基于OpenVINO构建的Python框架，用于在NPU上支持文本和聊天生成。在本文中，我们详细讨论了对Transformer架构所做的关键修改以实现推理、一些性能基准测试以及改进该软件包的下一步计划。NITRO的代码库可以在这里找到：https://github.com/abdelfattah-lab/nitro。**|
|**2024-12-13**|**SCBench: A KV Cache-Centric Analysis of Long-Context Methods**|Yucheng Li et.al.|[2412.10319](http://arxiv.org/abs/2412.10319)|null|长上下文LLM（大型语言模型）虽然促进了众多下游应用，但也带来了与计算和内存效率相关的重大挑战。为了解决这些挑战，围绕KV缓存的长上下文推理优化已经得到发展。然而，现有的基准测试通常只评估单次请求，忽略了KV缓存在实际应用中的完整生命周期。这种疏忽尤为关键，因为KV缓存重用已成为LLM推理框架（如vLLM和SGLang）以及OpenAI、Microsoft、Google和Anthropic等LLM提供商广泛采用的策略。为了填补这一空白，我们引入了SCBench（共享上下文基准），这是一个从KV缓存中心视角全面评估长上下文方法的基准：1）KV缓存生成，2）KV缓存压缩，3）KV缓存检索，4）KV缓存加载。具体来说，SCBench使用具有共享上下文的测试示例，包括12个任务和两种共享上下文模式，涵盖四种长上下文能力类别：字符串检索、语义检索、全局信息和多任务。凭借它，我们提供了对包括门控线性RNN、Mamba-Attention混合体以及高效的稀疏注意力、KV缓存丢弃、量化、检索、加载和提示压缩在内的八类长上下文解决方案的广泛分析。评估在8个长上下文LLM上进行。我们的发现表明，内存小于O(n)的方法在多轮场景中表现不佳，而具有O(n)内存和小于O(n^2)预填充计算的稀疏编码表现稳健。动态稀疏性比静态模式产生更具表现力的KV缓存，而混合架构中的层级稀疏性在保持强大性能的同时减少了内存使用。此外，我们还识别出在长生成场景中注意力分布偏移的问题。https://aka.ms/SCBench。|
|**2024-12-11**|**TurboAttention: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大型语言模型（LLM）推理需要大量的计算和内存，尤其是在关键的关注机制上。虽然量化技术和加速算法，如FlashAttention，已经提高了整体推理的效率，但它们针对问题的不同方面：量化关注于权重-激活操作，而FlashAttention虽然提高了执行效率，但需要高精度格式。最近的关键值（KV）缓存量化减少了内存带宽，但仍然需要在关注操作中执行浮点数反量化。我们提出了TurboAttention，这是一种实现关注量化执行的综合方法，同时解决了内存和计算效率的问题。我们的解决方案引入了两项关键创新：FlashQ，一种头部关注量化技术，它既能够压缩KV缓存，又能够实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它消除了在关注操作中指数运算期间进行FP32反量化的需求。实验结果表明，TurboAttention在关注操作上实现了1.2-1.8倍的速度提升，将KV缓存大小减少了超过4.4倍，并且相对于FP16基线，实现了高达2.37倍的最大吞吐量，同时在各种数据集和模型上优于现有的量化和压缩技术。|
|**2024-12-11**|**Lachesis: Predicting LLM Inference Accuracy using Structural Properties of Reasoning Paths**|Naryeong Kim et.al.|[2412.08281](http://arxiv.org/abs/2412.08281)|null|大型语言模型越来越多地被用于构建执行更复杂任务的智能体。随着LLM通过更长时间的交互进行更复杂的推理，自洽性，即从多个独立推理的样本中进行采样和边缘化所获得的答案更有可能是正确的这一观点，作为一种简单的验证技术，已经引起了广泛关注。本文旨在通过预测从推理路径样本的性质中获得的自洽性答案的正确性来实证验证这一直观假设。我们引入了Lachesis，这是一个基于自洽性的LLM推理的预测模型，并使用AutoFL（一种最近提出的基于LLM的错误定位技术）作为目标技术进行实证评估。Lachesis使用专门设计的推理路径表示将AutoFL收集的推理路径进行转换，并训练LSTM和GCN模型来预测一组给定的推理路径是否会导致正确的答案。结果表明，Lachesis可以以高达0.8136的精度预测答案的正确性，突显了训练一个能够允许提前终止不太可能成功的推理的预测模型的可行性。|
|**2024-12-11**|**TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch**|Xingchen Song et.al.|[2412.08237](http://arxiv.org/abs/2412.08237)|null|众所周知，基于LLM的系统对数据需求量大。近年来，基于LLM的TTS工作通常采用复杂的数据处理管道来获取高质量的训练数据。这些复杂的管道在每个阶段（例如，语音降噪、语音增强、说话人分割和标点模型）都需要优秀的模型，而这些模型本身又需要高质量的训练数据，并且很少开源。即使使用最先进的模型，仍然存在一些问题，如背景噪声去除不完整和标点与实际语音停顿不匹配。此外，严格的过滤策略通常只保留原始数据的10-30%，这极大地阻碍了数据扩展的努力。在这项工作中，我们利用一个噪声鲁棒的音频分词器（S3Tokenizer）设计了一个简化但有效的TTS数据处理管道，在保持数据质量的同时，大幅降低了数据获取成本，实现了超过50%的数据保留率。除了数据扩展的挑战之外，基于LLM的TTS系统与传统的方案相比，部署成本也更高。当前的系统通常仅使用LLM进行文本到标记的生成，而需要单独的模型（例如，流匹配模型）进行标记到波形生成，这些模型不能直接由LLM推理引擎执行，从而进一步复杂化了部署。为了解决这些挑战，我们消除了LLM和流组件中的冗余模块，用LLM架构替换了流模型主干。在此基础上，我们提出了一种统一的架构，用于流和非流推理，显著降低了部署成本。最后，我们探讨了使用相同数据进行TTS和ASR任务训练的可行性，这得益于简化的管道和S3Tokenizer，它降低了TTS训练数据的质量要求。|
|**2024-12-09**|**SparseAccelerate: Efficient Long-Context Inference for Mid-Range GPUs**|James Vo et.al.|[2412.06198](http://arxiv.org/abs/2412.06198)|null|随着大型语言模型（LLMs）扩展到更长的上下文窗口，传统的注意力机制的计算成本随输入长度的平方增长，这为实时和内存受限的部署带来了关键挑战。现有的稀疏注意力技术试图降低这种复杂性，但它们通常会产生显著的开销或降低准确性，这使得它们在中等硬件上对大上下文来说不太实用。在本文中，我们介绍了SparseAccelerate，这是一种动态稀疏注意力方法，它根据输入特征调整其稀疏模式，有效地平坦化了注意力复杂度曲线。我们的方法对于输入长度从16K个标记开始就非常有效，并在双NVIDIA A5000 GPU（每个24GB）上高效扩展到128K个标记。实验结果表明，SparseAccelerate在32K个标记时，将首次标记延迟（TTFT）降低了高达1.04倍，同时提供了显著的内存节省。这些改进为内存密集型应用和长上下文任务带来了实际效益，这些任务在标准注意力下之前是不可行的。除了延迟降低之外，SparseAccelerate在竞争方法中相对于上下文长度的TTFT增长梯度最小，从而根本改变了扩展趋势。在多种基准测试上的持续评估证实了其可扩展性，将SparseAccelerate定位为在可访问硬件上实现高效、实时和长上下文LLM推理的关键进步。|
|**2024-12-09**|**Asynchronous LLM Function Calling**|In Gim et.al.|[2412.07017](http://arxiv.org/abs/2412.07017)|null|大型语言模型（LLMs）通过函数调用与外部工具和数据源进行交互。然而，当前LLM函数调用的方法本质上具有同步性，每次调用都会阻塞LLM推理，限制了LLM的操作和并发函数执行。在本研究中，我们提出了AsyncLM，这是一个用于异步LLM函数调用的系统。AsyncLM通过允许LLMs并发生成和执行函数调用，提高了LLM的操作效率。AsyncLM引入了一个中断机制，在函数调用返回时异步通知正在进行的LLM，而不是等待每个调用完成。我们为函数调用和中断设计了上下文协议，提供了微调策略以适应中断语义，并在LLM推理过程中高效地实现了这些机制。我们证明了AsyncLM可以将端到端任务完成延迟从1.6倍到5.4倍降低，这是在伯克利函数调用排行榜（BFCL）上的一系列基准任务中的结果。此外，我们还讨论了如何扩展中断机制以实现新颖的人机LLM或LLM-LLM交互。|
|**2024-12-08**|**XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference**|Weizhuo Li et.al.|[2412.05896](http://arxiv.org/abs/2412.05896)|null|最近，生成式大型语言模型（LLM）在众多应用中取得了显著的成功。值得注意的是，其推理过程是逐个生成输出标记，导致许多冗余计算。广泛使用的KV-Cache框架在时间和空间复杂度之间做出了权衡。然而，缓存数据会导致内存需求不断增长，这可能会迅速耗尽现代加速器（如GPU）有限的内存容量，尤其是在长上下文推理任务中。现有研究通过淘汰对推理精度影响较小的部分缓存数据来减少内存消耗。但由于LLM网络层之间静态的缓存分配，实际效果远非理想。本文观察到，特定层的缓存数据对准确度的影响非常不同。我们量化了这种差异，并提供了实验和理论验证。据此，我们进行了形式化分析，表明以个性化方式为每个层定制缓存大小可以显著减少内存消耗，同时仍然提供可比的准确度。我们将缓存分配模拟为一个组合优化问题，并给出全局最优解。特别是，我们设计了一个基于轻量级LLM模型的小型采样推理，以便快速捕捉差异，并将其输入到个性化算法中。在真实世界数据集上的大量实验表明，我们的建议可以将KV缓存内存消耗平均降低61.6%，计算效率提高2.1倍，吞吐量提高高达5.5倍。|
|**2024-12-06**|**GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments**|Yanyu Chen et.al.|[2412.04788](http://arxiv.org/abs/2412.04788)|null|将大型语言模型（LLMs）高效地部署到实际场景中仍然是一个关键挑战，这主要归因于硬件异构性、推理框架的限制和工作负载的复杂性。这些挑战通常导致内存利用效率低下、延迟波动以及吞吐量不充分，阻碍了LLMs的有效部署，尤其是对于非专业人士。通过广泛的实验，我们确定了关键的性能瓶颈，包括内存利用率突然下降、随着批量大小变化的延迟波动以及多GPU配置中的效率低下。这些见解揭示了一个由硬件、框架和工作负载参数的复杂相互作用所塑造的巨大优化空间。这强调了系统性地优化LLM推理的必要性，从而推动了我们的框架GUIDE的设计。GUIDE利用动态建模和基于模拟的优化来解决这个问题，实现了关键指标（如批量延迟、TTFT和解码吞吐量）的预测误差在25%到55%之间。通过有效地弥合理论性能与实际部署之间的差距，我们的框架使实践者，特别是非专业人士，能够做出数据驱动的决策，并以低成本释放LLMs在异构环境中的全部潜力。|
|**2024-12-03**|**Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity**|Da Ma et.al.|[2412.02252](http://arxiv.org/abs/2412.02252)|null|随着大型语言模型（LLMs）如GPT和LLaMA系列中上下文窗口大小的增加，它们处理复杂、长文本任务的能力得到了提升，但代价是推理效率的降低，尤其是在内存和计算复杂度方面。现有方法，包括选择性保留标记和基于窗口的注意力机制，虽然提高了效率，但有可能丢弃未来文本生成所需的重要标记。在本文中，我们提出了一种方法，通过减少不重要标记的内存和计算负载来提高LLM效率，而不丢失标记。我们解决了两个挑战：1）研究上下文中重要标记的分布，发现最近标记比上下文中的远距离标记更重要；2）通过跨层共享注意力分数来优化远距离标记的资源。实验表明，我们的方法在不影响性能的情况下节省了35%的KV缓存。|
|**2024-12-03**|**Multi-Bin Batching for Increasing LLM Inference Throughput**|Ozgur Guldogan et.al.|[2412.04504](http://arxiv.org/abs/2412.04504)|null|随着大型语言模型（LLM）因其多样化的功能而越来越受欢迎，提高其推理系统的效率变得越来越关键。在服务器（例如GPU）上调度推理作业时，批处理LLM请求是一个关键步骤，这使系统能够通过允许多个请求并行处理来最大化吞吐量。然而，请求往往具有不同的生成长度，导致资源利用率低下，因为硬件必须在批处理中等待运行时间最长的请求完成后才能转到下一个批次。我们从排队论的角度正式化这个问题，并旨在设计一个吞吐量最优的控制策略。我们提出了多箱批处理（Multi-Bin Batching），这是一种简单而有效的方法，可以将具有相似（预测的）执行时间的请求分组到预定的箱中，从而可以证明地提高LLM推理吞吐量。通过理论分析和实验的结合，包括现实世界的LLM推理场景，我们证明了与标准批处理方法相比，显著提高了吞吐量。|
|**2024-12-02**|**PLD+: Accelerating LLM inference by leveraging Language Model Artifacts**|Shwetha Somasundaram et.al.|[2412.01447](http://arxiv.org/abs/2412.01447)|null|为了降低自回归语言模型（LLM）推理的延迟，预测解码（speculative decoding）作为一种新的解码范式应运而生，在这种范式中，未来的标记（tokens）被并行地起草和验证。然而，预测解码的实际部署受到其对额外计算资源和微调的需求的限制，这限制了其即插即用的实用性。为了应对这些挑战，我们提出了一种名为PLD+的新算法套件，旨在加速LLM的推理过程，特别是针对输入引导的任务。这些任务包括代码编辑、文本编辑、摘要等，它们的输出往往与输入有大量的重叠，这是PLD+设计时要利用的属性。PLD+还利用推理过程中产生的副产品（注意力机制和隐藏状态）来加速推理速度。我们在五个输入引导任务上测试了我们的方法，并通过广泛的实验发现，PLD+优于所有无需微调的方法。在贪婪设置中，它在四个任务上甚至优于最先进的依赖微调的方法EAGLE（平均加速率提高了2.31）。我们的方法无需微调，不需要任何额外的计算资源，并且可以轻松用于加速任何LLM的推理。|
|**2024-12-02**|**Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking**|Marco Federici et.al.|[2412.01380](http://arxiv.org/abs/2412.01380)|null|随着移动设备提供的计算能力不断增强，DRAM带宽的提升速度却相对较慢。这对大型语言模型（LLM）的token生成来说是个不幸的事，因为其高度依赖于内存。先前的研究提出利用ReLU激活的LLM中的自然动态激活稀疏性来减少每个token的有效DRAM带宽。然而，最新的LLM使用SwiGLU而非ReLU，这导致几乎没有固有的稀疏性。虽然SwiGLU的激活可以根据幅度进行剪枝，但产生的稀疏模式难以预测，使得先前的方法失效。为了解决这个问题，我们的工作引入了动态输入剪枝（DIP）：一种无预测器的动态稀疏化方法，它通过最小的微调来保留准确性。DIP还可以使用轻量级的LoRA适配器来恢复在稀疏化过程中损失的一些性能。最后，我们描述了一种新的缓存感知掩码策略，它考虑缓存状态和激活幅度以进一步提高缓存命中率，从而提高移动设备上LLM的token速率。在DIP中，与模拟硬件设置中的其他方法相比，它在准确性、内存和吞吐量之间的权衡方面表现更优。在Phi-3-Medium上，DIP实现了内存减少46%、吞吐量增加40%，同时困惑度损失小于0.1。|
|**2024-12-02**|**RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy**|Geonho Lee et.al.|[2412.01129](http://arxiv.org/abs/2412.01129)|**[link](https://github.com/aiha-lab/rilq)**|低秩自适应（LoRA）已成为参数高效的LLM微调的主要方法，基于LoRA的量化误差补偿（LQEC）也作为一种强大的工具，用于恢复压缩LLM的准确性。然而，LQEC在4位以下的情况下表现不佳，但此前没有对这一限制进行过研究。我们提出了RILQ（基于低秩自适应的秩无关量化误差补偿）来理解基本限制并提升2位LLM的准确性。基于对模型激活差异损失秩无关性质的分析，RILQ使用这种损失在层间协同调整适配器，使得低秩适配器能够实现鲁棒的误差补偿。在LLaMA-2和LLaMA-3上的评估表明，RILQ在各种最先进的量化器上对2位量化推理的一致性改进，以及在特定任务微调中的准确性提升。RILQ保持了与现有LoRA方法相当的计算效率，使得适配器合并权重量化LLM推理的准确性显著提升，成为提升2位LLM性能的有前途的方法。|
|**2024-12-02**|**TruncFormer: Private LLM Inference Using Only Truncations**|Patrick Yubeaton et.al.|[2412.01042](http://arxiv.org/abs/2412.01042)|null|私有推理（PI）在用户数据与专有机器学习模型（如LLMs）交互时，保证了用户数据的隐私性。然而，由于LLMs中存在的非线性函数带来的巨大延迟成本，PI在实践中变得难以处理。现有工作主要关注通过近似来提高特定LLM非线性（如Softmax或GeLU）的延迟。然而，随着新的LLM架构的引入，新的非线性类型也在不断出现，这导致了PI研究人员在优化最新非线性函数方面的不断追赶。我们引入了TruncFormer，这是一个将任何LLM转换为PI明文仿真的框架。我们的框架利用了LLM中的非线性函数是可微分的，并且可以用一系列加法、乘法和截断来精确近似的事实。此外，我们将加/乘操作与截断操作解耦，并根据给定的字段大小和输入表示大小静态确定应在哪里插入截断。这导致在现有加密协议中，每进行一次乘法操作后都需要截断的情况下，延迟得到了改进。我们开源了我们的代码以供社区使用。|
|**2024-11-29**|**A dynamic parallel method for performance optimization on hybrid CPUs**|Luo Yu et.al.|[2411.19542](http://arxiv.org/abs/2411.19542)|null|AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，当前的AI推理框架忽略了混合CPU硬件能力的失衡，导致推理性能低下。为了解决这个问题，我们引入了一种针对混合CPU的动态并行方法，该方法通过在并行工作开始之前平衡混合CPU每个核心的工作负载，显著提高了大型语言模型（LLM）的推理性能。这种方法使Neural Speed能够在两个混合英特尔CPU上实现超过90%（平均）的内存带宽。|
|**2024-11-29**|**BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching**|Zhen Zheng et.al.|[2412.03594](http://arxiv.org/abs/2412.03594)|null|许多大型语言模型（LLM）的任务在大批量或离线情况下执行，其性能指标为吞吐量。这些任务通常具有前缀共享的特征，即不同的提示输入可以部分显示共同的prefix。然而，现有的LLM推理引擎往往优化流式请求，在支持具有前缀共享特性的大批量任务方面存在局限性。现有解决方案使用基于LRU的缓存来重用共同前缀的KV上下文。即将被重用的KV上下文可能会因隐式缓存管理而被提前移除。即使没有被移除，共享的KV上下文的生命周期也会因为共享相同上下文的请求没有被一起调度而延长，导致更大的内存使用。这些以流为方向的系统按照先来先服务的顺序调度请求。结果，解码步骤比例较大的请求可能调度得太晚，无法与预填充块混合，从而提高硬件利用率。此外，基于令牌和请求数量的批量处理可能会限制令牌批量的大小，这会防止GPU在主要由解码令牌控制的迭代中饱和。我们提出了BatchLLM来解决这个问题。BatchLLM显式地识别全局的共同prefix。具有相同prefix的请求将被一起调度，以最佳方式重用KV上下文，这也有助于缩短共同KV内存的生命周期。BatchLLM重新排序请求，优先调度解码步骤比例较大的请求，以便更好地将解码令牌与后续预填充块混合，并采用以内存为中心的令牌批量处理来扩大令牌批量大小，这有助于提高GPU利用率。广泛的评估表明，在一系列微基准测试和两个典型的行业工作负载上，BatchLLM的性能优于vLLM 1.1倍到2倍。|
|**2024-11-28**|**Puzzle: Distillation-Based NAS for Inference-Optimized LLMs**|Akhiad Bercovich et.al.|[2411.19146](http://arxiv.org/abs/2411.19146)|null|大型语言模型（LLMs）展示了惊人的能力，但它们的采用受到推理过程中高计算成本的限制。虽然增加参数数量可以提高准确性，但它也拉大了最先进的能力与实际部署之间的差距。我们提出了Puzzle框架，该框架在特定硬件上加速LLMs的推理，同时保持其能力。通过前所未有的规模创新性地应用神经架构搜索（NAS），Puzzle在硬件约束下系统地优化了具有数十亿参数的模型。我们的方法利用块状局部知识蒸馏（BLD）进行并行架构探索，并采用混合整数规划进行精确的约束优化。我们通过Llama-3.1-Nemotron-51B-Instruct（Nemotron-51B）这一公开可用的模型展示了我们框架的实际影响，该模型由Llama-3.1-70B-Instruct衍生而来。Nemotron-51B实现了2.17倍的推理吞吐量加速，可以在单个NVIDIA H100 GPU上运行，同时保留了原始模型98.4%的能力。Nemotron-51B是目前能够以大批次在单个GPU上进行推理的最准确的语言模型。值得注意的是，这种转变只需要45B个训练令牌，而它所衍生的70B模型则需要超过15T个令牌。这建立了一个新的范式，即强大的模型可以通过仅牺牲微小能力来优化高效的部署，这表明推理性能而非参数数量本身应指导模型选择。随着Nemotron-51B的发布和Puzzle框架的介绍，我们为从业者提供了以显著降低的计算成本访问最先进语言建模能力的即时途径。|
|**2024-11-27**|**InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks**|Xinyao Zheng et.al.|[2411.18191](http://arxiv.org/abs/2411.18191)|null|大型语言模型（LLMs）具备广泛的知识和问答能力，已在金融和医疗咨询等对隐私敏感的领域得到广泛应用。在LLMs推理过程中，缓存共享方法被广泛采用以提高效率，通过重用缓存的状态或响应来处理相同或相似的推理请求。然而，我们发现这些缓存机制存在隐私输入泄露的风险，因为缓存可能导致响应时间出现可观察的变化，使其成为基于时间攻击的强候选线索。在本研究中，我们提出了一种新颖的基于时间的侧信道攻击，用于在LLMs推理中执行输入窃取。基于缓存的攻击面临在大型搜索空间中构建候选输入以击中和窃取缓存用户查询的挑战。为了解决这些挑战，我们提出了两个主要组件。输入构造器采用机器学习技术和基于LLM的方法进行词汇相关性学习，同时在通用输入构建中实施优化的搜索机制。时间分析器通过异常值去除实现统计时间拟合，以识别缓存命中模式，并持续提供反馈以优化构造器的搜索策略。我们在两种缓存机制上进行了实验，结果表明我们的方法在各种应用中均能持续获得高攻击成功率。我们的工作突出了与性能优化相关的安全漏洞，强调了在LLMs推理增强的同时优先考虑隐私和安全的必要性。|
|**2024-11-27**|**MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache**|Akshat Sharma et.al.|[2411.18077](http://arxiv.org/abs/2411.18077)|null|由于LLMs（大型语言模型）对内存和计算的高要求，如何在实践中高效地为LLMs提供服务变得极为挑战。在本研究中，我们调查了优化KV缓存的方法，因为其内存占用是LLM推理中的关键瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了MiniKV，这是一种KV缓存优化方法，通过一种新颖的2比特层区分性KV缓存，在同时保持长上下文任务准确性的同时，显著减少了KV缓存的大小。更重要的是，我们开发了专门的CUDA内核，使MiniKV与FlashAttention兼容。在广泛的长上下文任务上的实验表明，MiniKV有效地实现了86%的KV缓存压缩比，同时恢复了超过98.5%的准确性，优于现有方法，同时实现了卓越的系统性能提升。|
|**2024-11-26**|**PIM-AI: A Novel Architecture for High-Efficiency LLM Inference**|Cristobal Ortega et.al.|[2411.17309](http://arxiv.org/abs/2411.17309)|null|大型语言模型（LLMs）因其先进的语言理解和生成能力，在众多应用中变得至关重要。然而，它们对计算和内存的要求给传统的硬件架构带来了巨大的挑战。内存中处理（PIM）将计算单元直接集成到内存芯片中，为LLM推理提供了多项优势，包括减少数据传输瓶颈和提高能效。本文介绍了一种名为PIM-AI的新型DDR5/LPDDR5 PIM架构，专为LLM推理设计，无需修改内存控制器或DDR/LPDDR内存PHY。我们开发了一个模拟器来评估PIM-AI在不同场景下的性能，并证明了其相较于传统架构的显著优势。在基于云的场景中，PIM-AI相较于最先进的GPU，将每秒查询的三年总拥有成本降低了高达6.94倍，具体取决于所使用的LLM模型。在移动场景中，PIM-AI相较于最先进的移动SoC，在每token能耗上实现了10到20倍降低，从而实现了每秒查询增加25到45%，每查询能耗减少6.9倍到13.4倍，延长了电池寿命，并使每次充电的推理次数更多。这些结果突显了PIM-AI颠覆LLM部署的潜力，使其更加高效、可扩展和可持续。|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，使用Transformer基于的大型语言模型（LLMs）在长序列上进行推理既耗时又昂贵。我们引入了星型注意力，这是一种两阶段的块稀疏近似，通过在多个主机之间分片注意力来提高计算效率，同时最大限度地减少通信开销。在第一阶段，使用并行跨主机的块局部注意力处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星型注意力与大多数使用全局注意力训练的Transformer基于的LLMs无缝集成，通过减少内存需求和推理时间最多11倍，同时保留95-100%的准确率。**|
|**2024-11-26**|**Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation**|Chaoyi Jiang et.al.|[2411.17089](http://arxiv.org/abs/2411.17089)|**[link](https://github.com/chaoyij/kvpr)**|对于大型语言模型（LLMs）的推理计算量很大。为了降低自回归解码的成本，采用键值（KV）缓存来存储中间激活，使得GPU只需进行每个新标记所需的增量计算。这种方法显著降低了标记生成的计算开销。然而，KV缓存的内存需求迅速增长，通常超过GPU内存容量。一种成本效益更高的替代方案是将KV缓存卸载到CPU内存中，这可以缓解GPU内存压力，但将瓶颈转移到CPU和GPU之间有限的PCIe连接带宽。现有方法试图通过重叠GPU计算与I/O或采用CPU-GPU异构执行来解决这些问题，但它们受到过度数据移动和对CPU能力的依赖的阻碍。在本文中，我们介绍了一种高效的CPU-GPU I/O感知LLM推理方法，通过在同时通过PCIe总线传输剩余KV缓存的同时，从激活中重新计算部分KV缓存，避免了将整个KV缓存从CPU传输到GPU。这种方法重叠GPU重新计算与数据传输，以最小化GPU空闲时间并最大化推理性能。我们的方法通过集成一个利用输入特性和系统硬件信息的分析模块、一个用于优化计算和通信工作负载分配的调度模块以及一个用于高效执行派生执行计划的运行时模块而完全自动化。实验结果表明，与最先进的方法相比，我们的方法在解码时的延迟降低了高达35.8%，吞吐量提高了46.2%。|
|**2024-11-25**|**MixPE: Quantization and Hardware Co-design for Efficient LLM Inference**|Yu Zhang et.al.|[2411.16158](http://arxiv.org/abs/2411.16158)|null|基于Transformer的大型语言模型（LLMs）随着模型规模的不断扩大取得了显著的成功，但它们的部署仍然面临挑战，主要是因为计算和内存需求巨大。量化技术已成为一种有前景的解决方案，而针对LLMs的最先进量化算法引入了混合精度矩阵乘法（mpGEMM）的需求，即使用低精度权重与高精度激活进行乘法运算。尽管这种方法有优势，但当前硬件加速器如GPU和TPU缺乏对高效mpGEMM的原生支持，导致主顺序循环中的去量化操作效率低下。为了解决这一限制，我们引入了MixPE，这是一种专门设计的混合精度处理单元，旨在高效地在LLM推理中进行低比特量化。MixPE利用两项关键创新来最小化去量化开销并充分发挥低比特量化的潜力。首先，我们认识到每个量化组内的缩放因子和零点是可以共享的，因此我们提议在每个组mpGEMM之后进行去量化，这显著降低了去量化开销。其次，MixPE不是依赖于传统的乘法器，而是使用高效的移位和加法操作进行乘法运算，从而优化了计算和能效。我们的实验结果表明，MixPE在速度上比最先进的量化加速器快2.6倍，在能耗上减少1.4倍。|
|**2024-11-24**|**eFedLLM: Efficient LLM Inference Based on Federated Learning**|Shengwen Ding et.al.|[2411.16003](http://arxiv.org/abs/2411.16003)|null|大型语言模型（LLMs）标志着人工智能（AI）领域的变革时代。然而，LLMs所涉及的数据和参数规模巨大，需要高要求的计算和内存资源，这限制了它们对更广泛用户和研究者的可及性。本文介绍了一种有效的方法，可以提升LLM推理的操作效率和成本效益。通过利用基于transformer的联邦学习（FL）与模型并行分布式训练，我们的模型能够高效地在参与者网络中分配计算负载和内存需求。这种策略允许用户，尤其是那些资源有限的用户，可以协同训练最先进的LLMs。我们还创新了FL框架中的激励机制，奖励有益的贡献并过滤掉恶意活动，从而保护训练过程的完整性和可靠性。同时，我们利用内存层次策略和权重矩阵的奇异值分解（SVD）进一步提升了计算和内存效率。我们的结果，通过公式分析和数值计算得出，显著优化了资源使用，并使先进LLMs的访问权民主化，确保广泛的用户既能参与也能从中受益。|
|**2024-11-24**|**Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format**|Chao Fang et.al.|[2411.15982](http://arxiv.org/abs/2411.15982)|null|广泛应用的仅权重量化的大型语言模型（LLM），利用低比特整数（INT）权重并保留浮点（FP）激活，在减少存储需求的同时保持了准确性。然而，这将能耗和延迟瓶颈转向了与昂贵的内存访问和计算相关的FP激活。现有的LLM加速器主要关注计算优化，忽略了联合优化FP计算和数据移动的潜力，尤其是在LLM推理中的主导FP-INT GeMM操作。为了解决这些挑战，我们研究了各种LLM模块中激活精度的敏感性及其对整体模型准确性的影响。基于我们的发现，我们首先提出了Anda数据类型：一种具有组共享指数位和动态尾数位分配的自适应数据格式。其次，我们开发了一种迭代的训练后自适应精度搜索算法，优化不同LLM模块的位宽，以平衡模型准确性、能耗和推理速度。最后，提出了一系列硬件优化技术，以最大限度地发挥Anda格式的优势。这包括基于位面的数据组织方案、具有位串计算功能的Anda增强处理单元以及运行时位面Anda压缩器，以同时优化存储、计算和内存占用。我们在FPINT GeMM操作上的评估表明，与GPU类似的FP-FP基准相比，Anda在包括OPT、LLaMA和LLaMA-2系列在内的流行LLM上平均实现了2.4倍的加速、4.0倍的面积效率提升和3.1倍的能耗效率提升。Anda在各种应用场景、精度要求和系统性能方面表现出强大的适应性，使得在广泛的部署场景中实现高效的LLM推理成为可能。|
|**2024-11-24**|**Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments**|Nikoleta Iliakopoulou et.al.|[2411.17741](http://arxiv.org/abs/2411.17741)|null|随着大型语言模型（LLMs）的广泛应用，其部署数量呈指数级增长，对推理集群提出了巨大需求。这些集群必须处理针对不同LLM下游任务的大量并发查询。为了处理具有大量LLM参数的多任务设置，方法如低秩自适应（LoRA）允许针对特定任务进行微调，同时跨任务共享大部分基础LLM模型。因此，它们允许以最小的内存需求并发处理任务。然而，现有的LLM服务系统存在效率低下的问题：它们忽视了工作负载异构性，由于频繁的适配器加载而施加了高链路带宽，以及在调度器中存在头阻塞问题。为了解决这些挑战，我们提出了Chameleon，这是一个针对多个适配器环境优化的新型LLM服务系统，它依赖于两个核心思想：适配器缓存和适配器感知调度。首先，Chameleon在GPU内存中缓存流行的适配器，最小化适配器加载时间。重要的是，它使用原本闲置的GPU内存，避免了额外的内存成本。其次，Chameleon使用非抢占式多队列调度，以高效地处理工作负载异构性。通过这种方式，Chameleon同时防止了头阻塞和饥饿现象。我们在最先进的LLM服务平台之上实现了Chameleon，并使用真实世界的生产跟踪和开源LLM对其进行了评估。在高负载下，Chameleon将P99和P50的TTFT延迟分别降低了80.7%和48.1%，同时与最先进的基线相比，提高了1.5倍的吞吐量。|
|**2024-11-24**|**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**|Wenxiang Lin et.al.|[2411.15715](http://arxiv.org/abs/2411.15715)|null|大型语言模型（LLMs）因其庞大的模型尺寸而闻名，对计算资源和内存需求极高，导致在中等GPU系统上的推理效率低下。量化或剪枝等技术可以缩小模型尺寸，但通常会损害准确度，使其不适合实际应用。在这项工作中，我们介绍了\modelname{}，这是一个高性能的推理引擎，旨在加快LLMs的推理速度，同时不降低模型精度。\modelname{}采用了三种创新方法来提高推理效率：1）模型分区，允许跨CPU计算、GPU计算和CPU-GPU通信异步处理任务，2）自适应分区算法，以优化CPU、GPU和PCIe通信能力的利用，3）令牌分配策略，用于处理LLMs推理过程中的各种提示和生成任务。我们使用Mixtral、LLaMA-2、Qwen和PhiMoE等LLMs，在具有不同CPU和GPU的三个测试环境中进行了综合实验。实验结果表明，\modelname{}在解码速度上比 $1.11\times$到$1.80\times$更快，在预填充速度上比$1.69\times$到$6.33\times$更快，与最先进的解决方案llama.cpp和Fiddler相比，整体速度提高了$1.25\times$到$2.04\times$ 。|

<p align=right>(<a href=#updated-on-20251017>back to top</a>)</p>

## train

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-07-22**|**Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning**|Hongyin Luo et.al.|[2507.16784](http://arxiv.org/abs/2507.16784)|null|为了打破大型语言模型（LLMs）在推理准确性和效率上的瓶颈，我们提出了线程推理模型（TIM），这是一系列针对递归和分解问题解决训练的LLMs，以及TIMRUN，一个支持超出上下文限制的长期结构推理的推理运行时。在TIMRUN上运行的TIM，几乎支持无限的工怍记忆和多跳工具调用，在一个语言模型推理中克服了输出限制、位置嵌入约束和GPU内存瓶颈。通过将自然语言建模为既按长度又按深度度量的推理树，而不是线性序列，实现了性能。推理树由包含思想、递归子任务和基于我们在Schroeder等人2025年提出的概念得出的结论的任务组成。在生成过程中，我们维持一个工作记忆，只保留最相关上下文标记的关键值状态，这些状态由基于规则的子任务剪枝机制选择，从而在整个推理过程中实现位置嵌入和GPU内存页面的重用。实验结果表明，即使在操作高达90%的GPU内存中的KV缓存时，我们的系统也能保持高推理吞吐量。它还能够在数学任务上提供准确的推理，并处理需要长期推理和多跳工具使用的信|
|**2025-07-22**|**Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny**|Chuanhao Yan et.al.|[2507.16331](http://arxiv.org/abs/2507.16331)|null|现有的基于非正式语言（例如，人类语言）的、通过强化学习（RL）训练的大型语言模型（LLMs）面临一个重大挑战：它们的验证过程，虽然提供了关键的训练信号，但既不可靠也不可扩展。事实上，普遍存在的大型专有模型几乎无法生成可验证的程序。一个有希望但尚未充分探索的替代方案是基于正式语言的推理。将LLMs建立在严格的正式系统上，其中生成模型在正式语言空间（例如，Dafny）中运行，使得其推理过程和结果可以自动和数学上可证明地验证。这种能力对于实现大规模、可靠的正式软件验证至关重要。通常的做法是使用人工标注的思维链和其他人类先验知识来诱导LLMs的推理和编码能力。不幸的是，为监督复杂的编程任务提供这样的先验知识变得无法接受地耗时。在这项工作中，我们系统地探索了使用Dafny作为我们试点研究主要环境的方法来减少人类先验知识。我们的流程主要依赖于引入一个自动和可扩展的数据整理流程，以及与正式语言验证器反馈相结合的精心设计的RL设计。我们引入了DafnyComp，这是一个具有自动形式化规范的组合正式程序的基准，用于规范推理。我们的监督微调（SFT）阶段使得即使是小型模型（例如，0.5B）也能够生成语法有效且可验证的Dafny代码，超越了专有模型。具有正则化的RL进一步提高了性能，实现了对领域外任务的更强泛化，并在具有挑战性的DafnyComp基准上优于所有强基线。|
|**2025-07-22**|**LLM Data Selection and Utilization via Dynamic Bi-level Optimization**|Yang Yu et.al.|[2507.16178](http://arxiv.org/abs/2507.16178)|null|尽管大规模训练数据对于开发高效能的大型语言模型（LLMs）至关重要，但战略性地选择高质量数据已经成为提高训练效率和降低计算成本的关键方法。当前的数据选择方法主要依赖于静态、与训练无关的标准，未能考虑到动态模型训练和数据交互。在本文中，我们提出了一种新的数据加权模型（DWM），以调整每个批次中选中数据的权重，从而在LLM训练过程中实现动态数据利用。特别是，为了更好地捕捉训练模型的动态数据偏好，我们实施了一个双层优化框架来更新加权模型。我们的实验表明，DWM增强了使用随机选择的数据训练的模型性能，并且所学的加权模型可以迁移以增强其他数据选择方法和不同规模的模型。此外，我们还进一步分析了模型在整个训练过程中数据偏好的演变，为模型在训练过程中的数据偏好提供了新的见解。|
|**2025-07-18**|**Photonic Fabric Platform for AI Accelerators**|Jing Ding et.al.|[2507.14000](http://arxiv.org/abs/2507.14000)|null|本文介绍了Photonic FabricTM和Photonic Fabric ApplianceTM（PFA），这是一种光电开关和存储子系统，提供低延迟、高带宽和低每比特能耗。通过在2.5D电光系统中集成高带宽HBM3E内存、模块化光电交换机和外部DDR5，PFA提供高达32TB的共享内存以及115Tbps的全对全数字交换。Photonic FabricTM使分布式人工智能训练和推理能够更有效地执行并行策略。Photonic Fabric消除了硅海滩限制，这种限制限制了几乎所有当前XPU加速器设计中观察到的固定内存到计算比率。用连接到Photonic Fabric的芯片集替换XPU上的本地HBM堆叠，可以增加其内存容量和相应的内存带宽，提供一种灵活的扩展路径，超越单芯片封装HBM的限制。我们引入了CelestiSim，这是一个轻量级的分析模拟器，已在NVIDIA H100和H200系统上验证。它用于评估LLM参考和PFA上的节能，而无需对GPU核心设计进行任何重大更改。使用PFA，模拟结果显示，在405B参数的LLM推理中，吞吐量提高了3.66倍，延迟降低了1.40倍；在1T参数的情况下，吞吐量提高了7.04倍，延迟降低了1.41倍；在所有LLM训练场景中，数据移动的能耗降低了60-90%。虽然这些结果是为NVIDIA GPU展示的，但它们同样适用于具有相同基本限制（固定内存到计算比率）的其他AI加速器设计（XPU）。|
|**2025-07-16**|**BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training**|Rui Li et.al.|[2507.12619](http://arxiv.org/abs/2507.12619)|null|大型语言模型（LLMs）已成为现代人工智能的基石，推动了自然语言处理领域的突破，并扩展到涉及图像、音频和视频的多模态工作。与大多数计算软件一样，区分普通运行时性能和启动开销非常重要。先前的研究主要集中在运行时性能上：提高训练效率和稳定性。这项工作则专注于越来越关键的训练启动开销问题：训练作业开始执行前的延迟。在大型、工业规模的LLMs中，启动开销尤为重要，因为故障发生得更频繁，多个团队在迭代更新-调试周期中协同工作。在我们的一个训练集群中，由于启动开销，有超过3.5%的GPU时间被浪费。在这项工作中，我们基于真实生产数据首次对LLM训练启动开销进行了深入分析。我们分析了启动成本组成部分，量化了其直接影响，并考察了它与作业规模的关系。这些见解促使我们设计了Bootseer，这是一个系统级优化框架，旨在解决三个主要的启动瓶颈：（a）容器镜像加载，（b）运行时依赖安装，（c）模型检查点恢复。为了缓解这些瓶颈，Bootseer引入了三种技术：（a）热块记录和预取，（b）依赖快照，（c）条带化HDFS-FUSE。Bootseer已在生产环境中部署，并在真实的LLM训练工作负载上进行了评估，证明了启动开销降低了50%。|
|**2025-07-14**|**Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters**|Runsheng Benson Guo et.al.|[2507.10392](http://arxiv.org/abs/2507.10392)|null|大型语言模型（LLMs）的训练需要大量的GPU计算资源，但GPU的有限可用性和高昂成本使得同构集群对于许多组织来说并不实用。相反，通过将不同代次的GPU汇集在一起来构建异构集群，可以使它们实现更高的计算总量并充分利用所有可用的GPU。然而，在异构集群上进行训练存在一些挑战，包括跨GPU的负载均衡、优化内存使用以适应不同的内存容量，以及确保在可能跨越多个数据中心的多样化网络互连上的通信高效训练。在本文中，我们提出，在异构集群上进行高效训练需要（1）以既通信又内存高效的方式集成管道并行和数据并行，以及（2）更适应性的管道和数据并行配置，这包括将GPU灵活划分为非对称管道并行阶段的能力，以及在同一个数据并行组内包含异构GPU的能力。我们提出了Zorse，这是第一个统一所有这些功能并包含一个规划器，该规划器可以自动为给定的工作负载配置训练策略的系统。我们的评估表明，Zorse在异构训练场景中显著优于最先进的系统。|
|**2025-07-14**|**Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making**|Thomas T. Hills et.al.|[2507.10124](http://arxiv.org/abs/2507.10124)|null|识别LLM中的偏见是一个持续进行的过程。由于它们仍在发展中，今天正确的东西明天可能就是错误的。因此，我们需要一些通用的去偏见策略，这些策略能够超越当前模型。为去偏见人类决策制定的策略提供了一种有希望的方法，因为它们包含了一种类似于LLM风格的提示干预，旨在在决策过程中将潜在知识带入意识。在大量信息上训练的LLM包含有关潜在偏见、反论点和矛盾证据的信息，但只有在被提示的情况下才能发挥作用。人类决策文献中开发的元认知提示旨在实现这一点，正如我在这里所展示的，它们在LLM中显示出希望。我这里关注的提示是“你可能是错的吗？”在LLM做出回应后，这个提示引导LLM产生额外的信息，包括为什么它们这样回答，错误、偏见、矛盾证据和替代方案，这些在它们的初始回应中都没有显现出来。事实上，这种元知识通常揭示LLM和用户对提示的解释并不一致。在这里，我使用了一组来自关于LLM偏见（包括隐含的歧视性偏见和元认知失败）的近期文章的问题来展示这个提示。这个“你可能是错的吗”提示促使LLM识别自己的偏见并产生有说服力的元认知反思。我还提供了一个涉及令人信服但信息不完整的例子，这个例子可以通过元认知提示轻松纠正。总之，这项工作认为，人类心理学为提示工程提供了一条新的途径，利用了基于提示的有效改进人类决策的悠久历史。|
|**2025-07-14**|**Memorization Sinks: Isolating Memorization during LLM Training**|Gaurav R. Ghosal et.al.|[2507.09937](http://arxiv.org/abs/2507.09937)|null|大型语言模型容易记住重复的序列，这引发了隐私和版权方面的担忧。一种流行的缓解策略是在事后从特定神经元中移除已记住的信息。然而，迄今为止，这种方法的效果有限。在一个受控的环境中，我们展示了自然序列（那些与语言上可信的文本相似的序列）的记住机制与一般语言能力相纠缠，因此事后很难移除。在本工作中，我们提出了一种名为MemSinks的新范式，通过设计促进记忆的隔离。我们利用一个序列标识符，它为每个序列激活一组独特的记忆神经元，从而实现重复。通过分析学习和遗忘的动态，我们认为MemSinks促进了记忆内容的隔离，使得移除更加容易，同时不会损害一般语言能力。我们在数十亿参数和数十亿标记的规模上实现了MemSinks，观察到有效的隔离和强大的泛化能力。据我们所知，这是第一个在真实数据上证明同时实现泛化和隔离的原理验证。我们在http://github.com/grghosal/MemSinks上开源了我们的代码。|
|**2025-07-13**|**Fine-tuning Large Language Model for Automated Algorithm Design**|Fei Liu et.al.|[2507.10614](http://arxiv.org/abs/2507.10614)|null|将大型语言模型（LLMs）集成到自动化算法设计中显示出巨大的潜力。一种常见的方法是将LLMs嵌入到搜索程序中，以迭代地生成和改进候选算法。然而，大多数现有方法依赖于为通用编码任务训练的现成LLMs，留下了一个关键问题：我们是否需要专门针对算法设计定制的LLMs？如果是这样，如何有效地获得这类LLMs，它们在不同算法设计任务中的泛化能力如何？在本文中，我们通过探索针对算法设计对LLMs进行微调来回答这些问题。我们引入了一种基于多样性感知排名（DAR）的采样策略，以平衡训练数据的多样性和质量，然后利用直接偏好优化来高效地对齐LLMs的输出与任务目标。我们的实验在Llama-3.2-1B-Instruct和Llama-3.1-8B-Instruct上进行，涵盖了三个不同的算法设计任务。结果表明，经过微调的LLMs可以显著优于它们的现成版本，在较小的Llama-3.2-1B-Instruct上表现优异，并在可接受集问题上与较大的Llama-3.1-8B-Instruct相匹配。此外，我们还观察到有希望泛化：针对特定算法设计任务进行微调的LLMs也在不同设置的相关任务上提高了性能。这些发现突出了在算法设计中为LLMs进行特定任务定制的重要性，并为未来的研究开辟了新的途径。|
|**2025-07-11**|**Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training**|Aleksei Ilin et.al.|[2507.08284](http://arxiv.org/abs/2507.08284)|null|我们提出了一种轻量级且高效的安全护栏框架，用于语言模型，证明了小型语言模型在内容审核任务中可以达到甚至超越大型模型的性能。这是通过高保真合成数据生成和对抗训练实现的。合成数据生成过程始于人工精选的种子数据，这些数据经过查询增强和释义，以创建多样性和情境丰富的示例。然后，这些增强数据经过多轮精选，确保高保真度和相关性。受最近在生成对抗网络（GAN）架构方面的进展的启发，我们的对抗训练采用强化学习来引导生成器生成具有挑战性的合成示例。这些示例用于微调安全分类器，增强其检测和减轻有害内容的能力。此外，我们结合了最近关于高效大型语言模型（LLM）训练的研究策略，利用小型模型的性能来提升大型生成模型的性能。通过迭代对抗训练和生成多样化的高质量合成数据，我们的框架使小型语言模型（SLM）能够作为稳健的安全护栏。这种方法不仅降低了计算开销，还增强了对抗攻击的抵抗力，为AI系统中的内容审核提供了一个可扩展且高效的解决方案。|
|**2025-07-11**|**Evaluating LLMs in Medicine: A Call for Rigor, Transparency**|Mahmoud Alwakeel et.al.|[2507.08916](http://arxiv.org/abs/2507.08916)|null|目标：评估当前大型语言模型（LLMs）在医疗问答中的局限性，重点关注用于其评估的数据集质量。材料与方法：回顾了广泛使用的基准数据集，包括MedQA、MedMCQA、PubMedQA和MMLU，对其严谨性、透明度和与临床情景的相关性进行了评估。还分析了医疗期刊中的挑战性问题等替代方案，以确定其作为无偏见评估工具的潜力。结果：大多数现有数据集缺乏临床真实性、透明度和稳健的验证过程。公开可用的挑战性问题提供了一些好处，但受限于其规模小、范围窄和暴露于LLM训练。这些差距凸显了需要安全、全面和代表性数据集的必要性。结论：在医学中评估LLMs需要一个标准化框架。需要机构和政策制定者的共同努力，以确保数据集和方法是严谨的、无偏见的并且反映了临床复杂性。|
|**2025-07-08**|**LLMs are Introvert**|Litian Zhang et.al.|[2507.05638](http://arxiv.org/abs/2507.05638)|null|随着社交媒体和生成式AI的指数级增长，信息传播方式发生了转变，既促进了连接，也加速了错误信息的传播。理解信息传播动力学并开发有效的控制策略对于减轻有害内容至关重要。传统的模型，如SIR模型，提供了基本的见解，但不足以捕捉在线交互的复杂性。包括注意力机制和图神经网络在内的先进方法提高了准确性，但通常忽视了用户心理和行为动态。具有类人推理能力的大型语言模型（LLM）为模拟信息传播的心理方面提供了新的潜力。我们引入了一种基于LLM的模拟环境，该环境能够捕捉到代理者的态度、情感和反应的演变。然而，初步实验揭示了LLM生成行为与真实人类动态之间存在重大差距，尤其是在立场检测和心理现实主义方面。通过社会信息处理理论进行的详细评估确定了在目标设定和反馈评估方面的主要差异，这源于标准LLM训练中缺乏情感处理。为了解决这些问题，我们提出了基于社会信息处理的思想链（SIP-CoT）机制，并辅以情感引导的记忆。这种方法提高了对社交线索的解释、目标个性化以及反馈评估。实验结果表明，SIP-CoT增强的LLM代理能够更有效地处理社交信息，其行为、态度和情感更接近于真实人类互动。总之，这项研究突出了当前基于LLM的传播模拟中的关键局限性，并展示了如何通过整合SIP-CoT和情感记忆显著提高LLM代理的社会智能和现实主义。|
|**2025-07-07**|**ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning**|Zhirong Chen et.al.|[2507.04736](http://arxiv.org/abs/2507.04736)|null|大型语言模型（LLMs）在自动化寄存器传输级（RTL）代码生成方面展现出巨大的潜力。然而，当前的方法面临着一项关键挑战：它们无法同时优化功能性正确性和硬件质量（功率、性能、面积——PPA）。基于监督微调的方法通常生成功能性正确的代码，但PPA指标并不优化，缺乏学习优化原则的机制。相比之下，试图在生成后改善PPA指标的后处理技术往往效率低下，因为它们在更新LLM的参数之外操作，因此未能增强模型内在的设计能力。为了弥合这一差距，我们引入了ChipSeek-R1，这是一个基于奖励驱动的分层强化学习框架，用于训练LLMs生成既满足功能性正确性又优化PPA指标的RTL代码。ChipSeek-R1采用分层奖励系统，在强化学习过程中结合了关于语法、功能性正确性（来自模拟器）和PPA指标（来自综合工具）的直接反馈。这使得模型能够通过试错学习复杂的硬件设计权衡，生成既功能正确又PPA优化的RTL代码。在标准基准（VerilogEval、RTLLM）上评估ChipSeek-R1，我们在功能性正确性方面实现了最先进的结果。值得注意的是，在RTLLM基准上，ChipSeek-R1生成了27个超过原始人工编写代码PPA指标的RTL设计。我们的研究结果证明了将工具链反馈集成到LLM训练中的有效性，并突出了强化学习使自动化生成超越人类的RTL代码的潜力。我们将我们的代码开源至匿名github。|
|**2025-07-06**|**Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model**|Sibei Liu et.al.|[2507.04206](http://arxiv.org/abs/2507.04206)|null|在大型语言模型（LLM）的训练中，学习率（LR）调度通常遵循经验模板：预热、恒定平台期/稳定阶段和衰减（WSD）。然而，这种策略的机制解释仍被低估，而平台高度和衰减调度的选择很大程度上是经验性的。在本文中，我们通过MpeMBA效应——一种在将系统淬火到同一浴缸中时，较热的系统比较冷的系统冷却得更快的现象——将训练动态与热力学类比联系起来。我们分析了“山谷-河流”损失景观的一类，其中尖锐（山谷）方向快速平衡，而较平缓（河流）方向控制全局下降。MpeMBA效应为预热阶段的必要性提供了解释，并激励在衰减期间通过较高的平台而不是较低的平台来加速损失减少。我们表明，对于某些损失景观，存在一个最佳的平台学习率——“强MpeMBA点”，在该点上，最慢的模式消失，从而在衰减阶段实现更快的收敛。我们推导出其存在的分析条件，并估计了保持MpeMBA优势所需的衰减动力学。我们的最小模型和分析为基于平台期的调度器提供了原则性的合理性，并为在LLM中调整LR提供了最小化超参数搜索的指导。|
|**2025-07-04**|**Decoupled Relative Learning Rate Schedules**|Jan Ludziejewski et.al.|[2507.03526](http://arxiv.org/abs/2507.03526)|null|在本工作中，我们提出了一种通过调整Transformer模型中不同组件权重的学习率来优化LLM训练的新方法。传统方法通常在所有网络层应用统一的学习率，可能会忽略每个部分的独特动态。值得注意的是，我们引入的相对学习率（RLRS）方法通过加速训练过程，将训练速度提高高达23%，尤其是在混合专家（MoE）等复杂模型中。RLRS的超参数可以在较小的模型上进行高效调整，然后有效应用于高达27倍更大的模型。这种方法简单有效，显著减少了训练时间和计算资源，为优化大规模神经网络提供了一种实用且可扩展的解决方案。|
|**2025-07-03**|**WebSailor: Navigating Super-human Reasoning for Web Agent**|Kuan Li et.al.|[2507.02592](http://arxiv.org/abs/2507.02592)|null|超越人类认知局限是LLM训练的关键前沿。像DeepResearch这样的专有代理系统已在诸如BrowseComp这样极其复杂的信息检索基准上展现出超越人类的能力，这一成就以前是无法实现的。我们认为，他们的成功依赖于一种复杂的推理模式，这种模式在开源模型中是缺失的：在导航广阔的信息领域时，能够系统地减少极端不确定性。基于这一洞察，我们引入了WebSailor，这是一种完整的训练后方法，旨在培养这种至关重要的能力。我们的方法包括通过结构化采样和信息混淆生成新颖的高不确定性任务，RFT冷启动，以及高效的代理强化学习训练算法，即复制采样策略优化（DUPO）。通过这一集成流程，WebSailor在复杂的信息检索任务中显著优于所有开源代理，与专有代理的表现相当，并缩小了能力差距。|
|**2025-07-03**|**System-performance and cost modeling of Large Language Model training and inference**|Wenzhe Guo et.al.|[2507.02456](http://arxiv.org/abs/2507.02456)|null|大型语言模型（LLMs）基于变压器架构，由于其卓越的可扩展性和适应性，在人工智能、科学和工程等多个领域引发了革命。然而，LLM的大小和复杂性的指数级增长超过了计算能力、内存带宽、网络性能和成本效率的进步，对它们在分布式系统上的可扩展性提出了重大挑战。为了解决这些限制，文献中提出了替代模型架构、优化策略、通信感知网络拓扑和新型系统设计方法。本文介绍了一种用于LLM训练和推理的性能-成本建模方法，该方法将最先进的计算技术、内存优化和最新的通信技术相结合。基于一个分析性能模型，我们的方法融合了最新的创新，如闪存注意力技术和专家混合模型，以解决内存带宽和计算瓶颈。它还考虑了不同网络拓扑和特定拓扑的通信算法对5D并行性的影响。该框架还集成了芯片组成本模型。所提出的建模方法为未来计算系统设计提供了宝贵的见解，并促进了软硬件协同开发，尤其是由于其能够分析不同系统架构配置的性能-成本权衡。|
|**2025-07-03**|**Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection**|Weijie Lyu et.al.|[2507.02378](http://arxiv.org/abs/2507.02378)|null|近年来，大型语言模型（LLMs）在代码生成和程序理解方面的进步显著，加速了软件工程的演变。当前的方法主要通过利用大量数据来提升模型性能，重点关注数据量，而往往忽视数据质量，从而降低了训练效率。为了解决这个问题，我们提出了一种利用参数模型进行代码数据选择的方法，旨在提高训练效率和模型性能。我们的方法通过优化参数模型，确保所选子集中的分布一致性和多样性，从而保证数据质量。实验结果表明，仅使用10K个样本，我们的方法在92K全样本基线的基础上实现了2.4%（HumanEval）和2.3%（MBPP）的提升，在性能和效率方面都优于其他采样方法。这表明我们的方法有效地提升了模型性能，同时显著降低了计算成本。|
|**2025-07-03**|**Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness**|Tim Rogers et.al.|[2507.02283](http://arxiv.org/abs/2507.02283)|null|本文探讨了人工智能对齐问题的一个关键但尚未被充分研究的维度：大型语言模型（LLMs）继承并放大人类所倡导的理论与实际使用理论之间现有错位潜力的可能性。借鉴行动科学的研究成果，我们认为在人类生成文本上训练的LLMs很可能会吸收并复制模型1的实际使用理论——一种既抑制学习又在个体、团队和组织层面创造持续反学习动态的防御性推理模式。通过一个案例研究，我们详细展示了作为人力资源顾问的LLM如何在其表面上看似专业的建议中系统地强化了低效的解决问题方法，并阻碍了更深层次的组织学习途径。这代表了人工智能对齐问题的一个特定实例，其中人工智能系统成功地反映了人类行为，但继承了我们的认知盲点。如果LLMs被整合到组织决策过程中，可能会固化反学习实践，并赋予其权威性，这尤其具有风险。论文最后探讨了开发能够促进模型2学习（一种更有效的实际使用理论）的LLMs的可能性，并建议这一努力可以推动人工智能对齐研究和行动科学实践。这一分析揭示了对齐挑战中的一种意外对称性：开发与人类价值观正确对齐的人工智能系统的过程可能产生帮助人类自身更好地体现这些价值观的工具。|
|**2025-07-02**|**Low-Perplexity LLM-Generated Sequences and Where To Find Them**|Arthur Wuhrmann et.al.|[2507.01844](http://arxiv.org/abs/2507.01844)|null|随着大型语言模型（LLMs）的日益普及，了解特定训练数据如何塑造其输出对于透明度、问责制、隐私和公平性至关重要。为了探索LLMs如何利用和复制其训练数据，我们提出了一种以分析低困惑度序列为中心的系统方法——这些序列是由模型生成的高概率文本跨度。我们的流程可靠地提取了跨不同主题的这些长序列，同时避免了退化，并将它们追踪回训练数据的来源。令人惊讶的是，我们发现大量这些低困惑度跨度无法映射到语料库。对于那些匹配的，我们量化了它们在源文档中的出现分布，突出了逐字回忆的范围和性质，为更好地理解LLMs的训练数据如何影响其行为铺平了道路。|
|**2025-07-02**|**AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training**|Zhenyu Han et.al.|[2507.01663](http://arxiv.org/abs/2507.01663)|null|强化学习（RL）已成为大型语言模型（LLMs）训练后阶段的关键技术。传统的任务同址RL框架存在显著的扩展瓶颈，而任务分离的RL框架在复杂的数据流和相应的资源闲置以及工作负载不均衡方面面临挑战。此外，大多数现有框架与LLM的训练或推理引擎紧密耦合，使得支持自定义设计引擎变得困难。为了解决这些挑战，我们提出了AsyncFlow，一个用于高效训练后的异步流式RL框架。具体来说，我们引入了一个分布式数据存储和传输模块，以全流式方式提供统一的数据管理和细粒度调度能力。这种架构本质上促进了RL任务之间的自动化管道重叠和动态负载均衡。此外，我们提出了一种基于生产者-消费者的异步工作流程，通过策略性地在陈旧阈值内延迟参数更新过程，以最小化计算闲置。最后，AsyncFlow的核心能力在架构上与底层训练和推理引擎解耦，并由面向服务的用户界面封装，提供模块化和可定制的用户体验。大量实验表明，与最先进的基线相比，平均吞吐量提高了1.59倍。本研究中提出的架构为下一代RL训练系统设计提供了可操作的见解。|
|**2025-07-02**|**SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan**|Fumikazu Konishi et.al.|[2507.02124](http://arxiv.org/abs/2507.02124)|null|SAKURAONE是由SAKURA Internet Research Center开发和运营的托管高性能计算（HPC）集群。它强化了裸金属GPU服务器的“KOKARYOKU PHY”配置，并设计为针对高级工作负载，包括大型语言模型（LLM）训练等优化的集群计算资源。在ISC 2025版TOP500榜单中，SAKURAONE根据其高性能线性代数库（HPL）得分，在全球排名第49位，展示了其全球竞争力。特别是，它是前100名中唯一一个采用基于800 GbE（千兆以太网）和SONiC（云中开放网络软件）操作系统的完全开放网络栈的系统，突显了开放和供应商中立技术在大型HPC基础设施中的可行性。SAKURAONE在HPL基准测试（Rmax）中实现了33.95 PFLOP/s的持续性能，在高性能共轭梯度（HPCG）基准测试中实现了396.295 TFLOP/s。对于针对代表人工智能应用的低精度工作负载的HPL-MxP基准测试，SAKURAONE使用FP8精度实现了令人印象深刻的339.86 PFLOP/s。该系统由100个计算节点组成，每个节点配备八个NVIDIA H100 GPU。它由一个总物理容量为2 PB的全闪存Lustre存储子系统支持，提供高吞吐量和低延迟的数据访问。节点间通信是通过基于优化铁路拓扑的全分割带宽互连实现的，其中叶层和骨干层通过800 GbE链路互连。这种拓扑结构结合RoCEv2（汇聚式以太网上的RDMA版本2），实现了高速、无损的数据传输，并缓解了大规模并行工作负载中的通信瓶颈。|
|**2025-07-01**|**Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW**|Di Zhang et.al.|[2507.01241](http://arxiv.org/abs/2507.01241)|null|随机梯度下降（SGD）长期以来一直是训练大型语言模型（LLMs）的核心。然而，其在大规模应用中的有效性正受到越来越多的质疑，尤其是有经验证据表明其性能可能存在局限性。为此，本文提出了一种针对训练LLMs的随机共轭次梯度方法，并配合自适应采样。该方法不仅实现了每次迭代的更快收敛，而且与传统的SGD技术相比，也展现了更好的可扩展性。它通过样本复杂度分析来自适应地选择样本大小，采用随机共轭次梯度方法来确定搜索方向，并利用类似于AdamW的算法来自适应调整步长。这种方法保留了第一类方法的优点，同时有效地解决了LLMs训练中固有的非凸性和非光滑性。此外，我们还对算法的优势进行了详细分析。实验结果表明，所提出的方法不仅保持了，而且在许多情况下超过了传统SGD技术的可扩展性，显著提高了优化过程的速度和准确性。|
|**2025-07-01**|**Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories**|Jhouben Cuesta-Ramirez et.al.|[2507.00711](http://arxiv.org/abs/2507.00711)|null|大型语言模型（LLMs）通过强化学习（RL）训练，最近在推理基准测试中取得了令人瞩目的成果。然而，越来越多的证据表明，这些模型往往会产生更长但无效的思维链（CoTs），这引发了质疑，即基准测试的进步是否反映了真正的推理能力提升。我们提供了关于过度思考的新证据，其中模型即使被明确提供正确解决方案，也会忽视它们，而是继续生成不必要的推理步骤，这些步骤通常会导致错误的结论。在AIME2024数学基准测试上对三个最先进的模型进行的实验揭示了这些模型在整合纠正信息方面的关键局限，为实现稳健和可解释的推理提出了新的挑战。|
|**2025-06-30**|**TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation**|Renren Jin et.al.|[2506.23979](http://arxiv.org/abs/2506.23979)|null|在大型语言模型（LLMs）上进行监督微调和偏好微调需要高质量的数据集以提高其遵循指令和符合人类偏好和价值观的能力。然而，构建此类数据集需要大量资源，且大多数可用的监督和偏好微调数据集都是英文的。为了解决这些挑战，我们提出了基于分类法的偏好数据生成（TaP）框架，该框架促进了跨各种语言的偏好数据集的自动化和可扩展构建。TaP基于一个结构化的分类法，允许对数据集组成进行精细控制，从而确保多样性和全面覆盖。我们使用TaP生成的数据集在各种LLMs上进行监督和偏好微调。实验结果表明，在TaP生成的数据集上训练的LLMs优于在现有开源数据集上训练的LLMs。值得注意的是，在TaP生成的数据集上训练的LLMs的性能超过了在比开源数据集大180倍的开源数据集上训练的LLMs。|
|**2025-06-29**|**Generalist Reward Models: Found Inside Large Language Models**|Yi-Chen Li et.al.|[2506.23235](http://arxiv.org/abs/2506.23235)|null|大型语言模型（LLMs）的校准严重依赖于在昂贵的人类偏好数据上训练的奖励模型。尽管最近的研究探索了使用AI反馈来规避这种成本，但这些方法通常缺乏严格的理论基础。在本文中，我们发现一个强大的通用奖励模型已经潜在地存在于通过标准下一标记预测训练的任何LLM中。我们证明这种内源奖励不是启发式方法，而是与通过离线逆强化学习学习的奖励函数在理论上等价。这种联系使我们能够直接从基础（预训练或监督微调）模型中获取高质量的奖励信号，而无需进一步训练。更重要的是，我们还证明，使用这种内源奖励进行后续的强化学习，会导致一个比基础模型具有可证明更优错误界限的策略。据我们所知，这是第一个关于强化学习对LLMs有效性的理论证明。我们的实验验证了这一理论，表明我们的方法不仅优于现有的LLM作为裁判的方法，还可以超越明确训练的奖励模型。这些发现表明，奖励建模阶段可以被替换为一种原则性的方法，该方法可以在预训练期间提取已捕获的知识，预示着LLMs校准以及多模态模型的一种更高效、强大且可扩展的范例。|
|**2025-06-27**|**The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements**|Bingchen Zhao et.al.|[2506.22419](http://arxiv.org/abs/2506.22419)|null|大型语言模型（LLMs）的快速发展有可能助力科学进步。为此，一个关键的能力是能够复现现有工作。为了评估人工智能代理在活跃研究领域的复现能力，我们引入了自动化的LLM速度竞赛基准，该基准利用了NanoGPT速度竞赛的研究社区贡献，这是一个在最短时间内训练GPT-2模型的竞赛。每个19个速度竞赛任务都向代理提供了之前的记录训练脚本，可选择搭配三种提示格式之一，从伪代码到类似于描述新记录改进的论文式描述。记录执行得很快，这是设计上的要求，速度竞赛的改进涵盖了多样化的代码级别变化，从高级算法进步到硬件感知优化。这些特性使得基准既易于访问又具有现实性，适用于提升LLM训练的前沿问题。我们发现，最近的推理LLMs结合SoTA（当前最佳技术）框架在复现我们基准中已知的创新时遇到了困难，即使提供了详细的提示。因此，我们的基准为LLM自动科学复现能力提供了一个简单、非饱和的度量，这是自主研究代理所需（但不是充分条件）的技能。|
|**2025-06-25**|**Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs**|Sonia K. Murthy et.al.|[2506.20666](http://arxiv.org/abs/2506.20666)|null|在日常生活中处理社交场合往往需要权衡冲突的目标，比如传达残酷的真相、保持信任，同时还要关注他人的感受。这些价值权衡是人类决策和语言使用的核心部分，然而，目前用于在大型语言模型（LLM）中解释这种动态和多维价值观念的工具是有限的。在认知科学中，所谓的“认知模型”通过模拟说话者在选择行动或话语时竞争性效用函数的权重，为人类的这种权衡提供了形式化的描述。在这项工作中，我们使用一个领先的礼貌用语认知模型来解释LLM在多大程度上体现了类似人类的价值权衡。我们将这一视角应用于系统地评估两个包含性模型设置中的价值权衡：前沿黑盒模型中推理“努力”的程度，以及开源模型的RL后训练动态。我们的结果显示，在推理模型中，信息效用模式高于社会效用，而在开源模型中，数学推理能力更强。我们从LLM的训练动态中发现，与反馈数据集或对齐方法相比，在训练初期就有较大的效用值变化，并且基础模型和预训练数据的选择产生了持续的影响。我们表明，我们的方法能够适应快速发展的LLM领域的多样方面，为形成关于其他高级行为的假设、塑造推理模型的训练机制以及更好地在模型训练过程中控制价值之间的权衡提供了见解。|
|**2025-06-24**|**Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models**|Jungwoo Park et.al.|[2506.19697](http://arxiv.org/abs/2506.19697)|null|在大型语言模型（LLMs）中，极端激活异常值会严重降低量化性能，阻碍设备上的高效部署。虽然通道操作和自适应梯度缩放被认定为原因，但实际缓解措施仍然具有挑战性。我们引入了Outlier-Safe Pre-Training（OSP），这是一种实用指南，它主动防止异常值形成，而不是依赖于事后的缓解。OSP结合了三个关键创新：（1）Muon优化器，消除特权基同时保持训练效率；（2）单尺度RMSNorm，防止通道放大；（3）可学习的嵌入投影，重新分配来自嵌入矩阵的激活幅度。我们通过在一个包含140亿参数的模型上训练10万亿个标记来验证OSP，这是第一个不包含此类异常值的实际规模LLM。在激进的4位量化下，我们的OSP模型在10个基准测试中平均得分达到35.7（相比之下，Adam训练模型的得分为26.5），并且只有2%的训练开销。值得注意的是，与标准模型中极端值（1818.56）相比，OSP模型展现出近乎零的超高偏度（0.04），从根本上改变了LLM的量化行为。我们的工作表明，异常值并非LLM固有的，而是训练策略的结果，为更高效的LLM部署铺平了道路。源代码和预训练检查点可在https://github.com/dmis-lab/Outlier-Safe-Pre-Training获取。|
|**2025-06-21**|**LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning**|Haoxuan Che et.al.|[2506.17562](http://arxiv.org/abs/2506.17562)|null|LLMs在医学报告生成（MRG）领域展现出巨大潜力，但它们的发展需要大量医学图像-报告对，这些数据通常分散在多个中心。由于隐私法规，集中这些数据极具挑战性，从而阻碍了模型发展和LLM驱动MRG模型的广泛应用。为了解决这一挑战，我们提出了FedMRG，这是第一个利用联邦学习（FL）来支持隐私保护的多中心LLM驱动MRG模型开发的框架，专门设计来克服多模态数据异质性下的通信高效LLM训练这一关键挑战。首先，我们的框架通过采用低秩分解来高效分解参数更新，解决了FL-LLM调优中的通信开销这一基本挑战，显著降低了梯度传输成本，使得LLM驱动的MRG在带宽受限的FL环境中成为可能。此外，我们观察到在FL场景下MRG存在双重异质性：各医疗中心间图像特征的差异，以及报告风格和术语偏好的多样性。为了解决这个问题，我们进一步增强了FedMRG，包括：（1）在MRG编码器中采用客户端感知的对比学习，结合诊断驱动的提示，在保持诊断准确性的同时，捕捉全局可泛化和局部独特特征；（2）在MRG解码器中采用双重适配器互增强机制，协调通用和专用适配器以应对报告风格和术语的变化。通过对我们建立的FL-MRG基准的广泛评估，我们证明了FedMRG的泛化能力和适应性，强调了其利用多中心数据、生成临床准确报告的同时保持通信效率的潜力。|
|**2025-06-20**|**No Free Lunch: Rethinking Internal Feedback for LLM Reasoning**|Yanzhi Zhang et.al.|[2506.17219](http://arxiv.org/abs/2506.17219)|null|强化学习已成为一种强大的范式，用于在训练后的大语言模型（LLMs）中提高推理能力。像基于人类反馈的强化学习（RLHF）和基于可验证奖励的强化学习（RLVR）等方法已经显示出良好的效果，但它们需要大量的外部监督。我们研究了另一类方法，即基于内部反馈的强化学习（RLIF），它完全依赖于模型导出的内在信号而不是外部奖励。具体来说，我们利用无监督的奖励代理，如词元级别的熵、轨迹级别的熵和自我确定性。我们的理论分析表明，这些内部目标部分上是等价的，我们在具有挑战性的数学推理基准上对各种RLIF策略进行了实证评估。实验结果表明，RLIF可以在训练的初期阶段提升基础LLMs的推理性能，在这些任务上与RLVR技术相当甚至更优。然而，当训练进展时，性能甚至低于训练前的模型。此外，我们发现RLIF对指令调整模型几乎没有提升，这表明一旦LLM已经进行了指令调整，内在反馈的回报就会减少。我们进一步通过混合模型权重分析了这一局限性，并解释了RLIF的训练行为的原因，为将内部反馈信号集成到LLM训练中提供了实用指南。我们希望我们对内部反馈的分析能够为LLM后训练提供更原则性和有效的策略。|
|**2025-06-20**|**DistillNote: LLM-based clinical note summaries improve heart failure diagnosis**|Heloisa Oss Boll et.al.|[2506.16777](http://arxiv.org/abs/2506.16777)|null|大型语言模型（LLMs）为生成患者信息的简洁摘要以及减轻医疗保健提供者面临的临床文档负担提供了前所未有的机会。我们提出了Distillnote，一个基于LLM的临床笔记摘要框架，并通过三种技术生成超过64,000份入院笔记摘要：（1）一步直接摘要和涉及（2）针对独立临床见解的结构化摘要以及（3）进一步浓缩结构化摘要的蒸馏摘要。我们通过使用这些摘要来预测心力衰竭，与在原始笔记上训练的模型进行比较，以测试摘要的有用性。蒸馏摘要实现了79%的文本压缩，与在完整笔记上训练的LLM相比，AUPRC提高了高达18.2%。我们还通过LLM作为评判者的评估以及与临床医生的盲法成对比较来评估生成摘要的质量。评估表明，根据相关性和临床可操作性，临床医生更倾向于一步摘要，而蒸馏摘要提供了最优的效率（平均6.9倍的压缩到性能比）并显著减少了幻觉。我们将我们的摘要发布在PhysioNet上，以鼓励未来的研究。|
|**2025-06-19**|**Under the Shadow of Babel: How Language Shapes Reasoning in LLMs**|Chenxi Wang et.al.|[2506.16151](http://arxiv.org/abs/2506.16151)|null|语言不仅是沟通的工具，也是人类认知和推理的媒介。如果如语言相对论所暗示的那样，语言的结构塑造了认知模式，那么在人类语言上训练的大型语言模型（LLMs）也可能内化了不同语言中嵌入的习惯性逻辑结构。为了检验这一假设，我们引入了BICAUSE，这是一个用于因果推理的结构化双语数据集，它包含了语义对齐的中英文样本，既有正向也有反向的因果关系形式。我们的研究揭示了三个关键发现：（1）LLMs表现出类型对齐的注意力模式，在中文中更关注原因和句首的连接词，而在英语中表现出更平衡的分布。（2）模型内化了语言特定的因果词序偏好，并经常严格地将这些偏好应用于非典型输入，导致性能下降，尤其是在中文中。（3）当因果推理成功时，模型表示收敛于跨语言的语义对齐抽象，表明超越了表面形式的共同理解。总的来说，这些结果表明LLMs不仅模仿了表面语言形式，还内化了由语言塑造的推理偏见。这一现象基于认知语言学理论，首次通过分析模型内部结构进行实证验证。|
|**2025-06-19**|**TrainVerify: Equivalence-Based Verification for Distributed LLM Training**|Yunchi Lu et.al.|[2506.15961](http://arxiv.org/abs/2506.15961)|null|训练大型语言模型（LLMs）需要数千台设备的并行执行，这会带来巨大的计算成本。然而，这些昂贵的分布式训练很少得到验证，使其容易发生静默错误，并可能导致数百万GPU时长的浪费。我们介绍了TrainVerify，这是一个用于LLMs可验证分布式训练的系统。给定深度学习模型的逻辑规范作为真相，TrainVerify形式化地验证分布式并行执行计划在数学上等同于该模型。由于LLMs的规模庞大，通常涉及数十亿个变量和高度复杂的计算图，直接验证异常困难。因此，TrainVerify引入了形状缩减技术和分阶段并行验证算法，这显著降低了复杂性同时保持了形式正确性。TrainVerify可扩展到前沿的LLMs，包括成功验证了Llama3（405B）和DeepSeek-V3（671B）的训练计划。|
|**2025-06-18**|**Rethinking LLM Training through Information Geometry and Quantum Metrics**|Riccardo Di Sipio et.al.|[2506.15830](http://arxiv.org/abs/2506.15830)|null|在大型语言模型（LLMs）的优化过程中，涉及到具有非欧几里得结构的超高维参数空间。信息几何通过Fisher信息度量来描绘这一景观，从而通过自然梯度下降实现更根本的学习。尽管这种方法在实际应用中往往不切实际，但这一几何视角能够阐明诸如尖锐最小值、泛化以及观察到的缩放定律等现象。我们认为，曲率感知方法加深了我们对于LLM训练的理解。最后，我们基于Fubini-Study度量量子Fisher信息进行推测，暗示在量子增强系统中进行高效优化的可能性。|
|**2025-06-17**|**LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data**|Eyal German et.al.|[2506.14474](http://arxiv.org/abs/2506.14474)|**[link](https://github.com/eyalgerman/leximark)**|**大型语言模型（LLMs）可能在未经所有者同意的情况下使用数据训练或微调。验证特定LLM是否使用特定数据实例或整个数据集进行训练极为困难。数据集水印通过在训练数据中嵌入可识别的修改来解决这一问题，以检测未经授权的使用。然而，现有方法往往缺乏隐蔽性，使得它们相对容易被检测和移除。鉴于这些局限性，我们提出了LexiMark，这是一种针对文本和文档的全新水印技术，它嵌入精心挑选的高熵词的同义词替换。我们的方法旨在增强LLM对水印文本的记忆能力，同时不改变文本的语义完整性。因此，水印难以检测，与文本无缝融合，没有可见的标记，并且由于其微妙、上下文适当的替换，难以被移除，避免了自动和人工检测。我们使用最近研究中的基线数据集和七个开源模型（LLaMA-1 7B、LLaMA-3 8B、Mistral 7B、Pythia 6.9B以及Pythia家族的三个较小变体（160M、410M和1B）来评估我们的方法。我们的评估涵盖了多个训练设置，包括持续预训练和微调场景。结果表明，与现有方法相比，AUROC分数有显著提高，突显了我们的方法在可靠地验证LLM训练中是否使用了未经授权的水印数据方面的有效性。**|
|**2025-06-16**|**Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences**|Stas Bekman et.al.|[2506.13996](http://arxiv.org/abs/2506.13996)|**[link](https://github.com/snowflakedb/ArcticTraining)**|**长序列对于RAG、长文本摘要、多模态等应用至关重要，现代大型语言模型（LLM）如Llama 4 Scout支持的最大序列长度高达1000万个token。然而，在除了企业实验室之外的环境中，由于开源领域对系统支持的有限，对于AI社区来说，长序列训练具有挑战性。原装情况下，即使是在现代的NVIDIA H100 80GB GPU集群上，训练超过32K个token的Llama 8B模型也会因为两个原因而耗尽内存：i）LLM训练工作负载没有优化以充分利用单个GPU内存，ii）现有的利用多个GPU内存的解决方案并不容易应用于Hugging Face（HF）模型，这使得长序列训练变得难以实现。我们针对此问题提出了Arctic长序列训练（ALST）。它结合了无注意力机制的单一GPU和多GPU内存优化，使其能够原生支持各种HF模型的百万级序列长度的训练。ALST支持在单个H100 GPU上训练Meta的Llama 8B模型，序列长度可达500K，在单个8xH100 GPU节点上可达3.7M，在4节点集群上则超过1500万，后者相比32K基线提升了400多倍。ALST完全兼容HF模型，并通过Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ 和Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md开源。**|
|**2025-06-15**|**SciDA: Scientific Dynamic Assessor of LLMs**|Junting Zhou et.al.|[2506.12909](http://arxiv.org/abs/2506.12909)|null|大型语言模型（LLMs）推理能力的提升使它们能够以更高的效率解决科学问题。因此，一个高质量的综合和适当的基准对于评估具有重要意义，而现有的基准要么面临数据污染的风险，要么缺乏涉及的学科。具体来说，由于LLMs训练和静态基准的数据源重叠，导致关键或答案的数字模式被无意中记住（即数据污染），这导致对其推理能力的系统性高估，尤其是数值推理能力。我们提出了SciDA，这是一个多学科基准，仅由超过1000个奥运级别的数值计算问题组成，每个推理轮次允许随机数值初始化以避免依赖于固定的数值模式。我们对开源和闭源的最顶级LLMs进行了一系列实验，观察到在随机数值初始化下LLMs的表现显著下降。因此，我们提供了对LLMs数值推理能力的真实和无偏评估。数据可在https://huggingface.co/datasets/m-a-p/SciDA获取。|
|**2025-06-15**|**SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation**|Yashothara Shanmugarasa et.al.|[2506.12699](http://arxiv.org/abs/2506.12699)|null|大型语言模型（LLMs）是复杂的人工智能系统，能够以惊人的精确度生成类似人类的文本。虽然LLMs带来了重大的技术进步，但它们使用从网络中抓取的大量用户数据和从广泛的用户互动中收集的数据进行开发，存在敏感信息泄露的风险。大多数现有调查主要关注训练数据的隐私影响，但往往忽略了用户互动和高级LLM能力的隐私风险。本文旨在填补这一空白，通过对LLMs中的隐私进行全面分析，将挑战分为四个主要领域：（一）LLM训练数据中的隐私问题，（二）与用户提示相关的隐私挑战，（三）LLM生成输出中的隐私漏洞，（四）涉及LLM代理的隐私挑战。我们评估了现有针对这些提出的隐私挑战的缓解机制的有效性和局限性，并确定了进一步研究的领域。|
|**2025-06-13**|**Augmenting the Generality and Performance of Large Language Models for Software Engineering**|Fabian C. Peña et.al.|[2506.11548](http://arxiv.org/abs/2506.11548)|null|大型语言模型（LLMs）正在改变软件工程（SE）领域，尤其是在代码生成和分析方面。然而，它们在更广泛的SE实践中的应用，包括概念化、设计和其他非代码任务，仍部分未得到充分探索。本研究旨在通过以下三个方面来提升LLMs在SE领域的通用性和性能：（1）深入理解不同特性的LLMs在各种非代码任务上的表现；（2）评估它们作为SE基础知识的来源；（3）有效检测SE声明中的幻觉。预期贡献包括在特定领域数据集上训练和评估的多种LLMs、SE基础知识的新的基准以及检测幻觉的方法。在非代码任务性能提升方面的初步结果很有希望。|
|**2025-06-12**|**Farseer: A Refined Scaling Law in Large Language Models**|Houyi Li et.al.|[2506.10972](http://arxiv.org/abs/2506.10972)|**[link](https://github.com/farseer-scaling-law/farseer)**|**训练大型语言模型（LLMs）成本高昂，这造成了关键的规模差距，导致小规模实验的见解往往无法转移到资源密集型的生产系统中，从而阻碍了高效的创新。为了弥合这一差距，我们引入了Farseer，这是一种新颖且精细的扩展定律，它提供了跨尺度的增强预测精度。通过系统地构建模型损失表面 $L(N,D)$，Farseer比先前的定律（例如Chinchilla定律）实现了对经验数据的显著更好的拟合。我们的方法产生了准确、稳健且高度可推广的预测，展示了出色的外推能力，通过减少外推误差433%改进了Chinchilla定律。这使得能够在所有$(N,D)$ 设置中可靠地评估竞争的训练策略，使小规模消融研究的结果可以自信地外推以预测大规模性能。此外，Farseer为最优计算分配提供了新的见解，更好地反映了现代LLM训练的细微需求。为了验证我们的方法，我们在不同规模和配置下训练了大量约1000个LLMs，消耗了大约300万小时的NVIDIA H100 GPU。我们将在https://github.com/Farseer-Scaling-Law/Farseer上全面开源所有模型、数据、结果和日志，以促进进一步的研究。**|
|**2025-06-11**|**Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective**|Yi Wang et.al.|[2506.10161](http://arxiv.org/abs/2506.10161)|null|故事生成是大型语言模型（LLMs）的一个突出应用。然而，由于自动评估方法的挑战以及人工评估的高成本和主观性，对LLMs生成高质量故事的能力理解仍然有限。计算叙事学为构成好故事的因素提供了宝贵的见解，这些见解已应用于故事生成的符号叙事规划方法中。本研究旨在通过使用LLMs解决叙事规划问题来加深对LLMs故事生成能力的理解。我们基于文献实例提出了一个用于评估LLMs在叙事规划方面的基准，重点关注因果合理性、角色意图和戏剧冲突。我们的实验表明，GPT-4级别的LLMs可以在小规模上生成因果合理的 stories，但涉及角色意图和戏剧冲突的规划仍然具有挑战性，需要使用强化学习训练的LLMs进行复杂推理。这些结果从不同方面提供了LLMs在保持质量的同时可以生成故事规模的信息。我们的发现还突出了有趣的问题解决行为，并揭示了在游戏环境中应用LLM叙事规划所面临的挑战和考虑因素。|
|**2025-06-09**|**Reparameterized LLM Training via Orthogonal Equivalence Transformation**|Zeju Qiu et.al.|[2506.08001](http://arxiv.org/abs/2506.08001)|null|虽然大型语言模型（LLMs）正推动人工智能的快速发展，但有效地和可靠地训练这些大型模型仍然是该领域最大的挑战之一。为了解决这一挑战，我们提出了POET，这是一种新型的重参数化训练算法，它使用正交等价变换来优化神经元。具体来说，POET使用两个可学习的正交矩阵和一个固定的随机权重矩阵来重新参数化每个神经元。由于其能够证明地保留权重矩阵的谱特性，POET可以稳定地优化目标函数，并提高了泛化能力。我们进一步开发了高效的近似方法，使得POET在训练大规模神经网络时更加灵活和可扩展。大量的实验验证了POET在训练LLMs方面的有效性和可扩展性。|
|**2025-06-08**|**RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality**|Chenlong Zhang et.al.|[2506.07171](http://arxiv.org/abs/2506.07171)|null|随着在大规模、未筛选语料库上训练的大规模语言模型（LLMs）的广泛应用，人们越来越关注模型中包含敏感、版权或非法内容的问题。这导致了对LLMs去学习（即选择性从模型中移除特定信息而不从头开始重新训练或降低整体效用）的兴趣日益增加。然而，现有的方法通常依赖于大规模的忘记和保留数据集，并存在反应不自然、泛化能力差或效用损失灾难性的问题。在本研究中，我们提出了强化去学习（RULE），这是一个将去学习公式化为拒绝边界优化问题的有效框架。RULE使用忘记集的一小部分和合成的边界查询进行训练，利用一个可验证的奖励函数，该函数鼓励在忘记相关查询上的安全拒绝，同时保留允许输入的有用响应。我们提供了理论和实证证据，证明了RULE在实现目标去学习的同时，不会损害模型效用方面的有效性。实验结果表明，仅使用12%的忘记集和8%的合成边界数据，RULE在忘记质量上优于现有基线高达17.5%，在自然性响应上优于16.3%，同时保持了一般效用，实现了忘记-保留帕累托最优。值得注意的是，我们还观察到RULE提高了模型输出的自然性，增强了训练效率，并展现出强大的泛化能力，将拒绝行为泛化到语义相关但未见过的问题上。|
|**2025-06-08**|**BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning**|Ha-Thanh Nguyen et.al.|[2506.06955](http://arxiv.org/abs/2506.06955)|null|我们介绍了BIS Reasoning 1.0，这是第一个专门为评估大型语言模型（LLMs）中信念不一致推理而设计的、大规模的日语演绎推理问题数据集。与之前如NeuBAROCO和JFLD等数据集不同，它们关注的是一般或信念一致的推理，BIS Reasoning 1.0引入了逻辑上有效但信念不一致的演绎推理，以揭示在人类对齐语料库上训练的LLMs中的推理偏见。我们对最先进的模型进行了基准测试，包括GPT模型、Claude模型和领先的日本LLMs，发现性能存在显著差异，其中GPT-4o实现了79.54%的准确率。我们的分析确定了当前LLMs在处理逻辑上有效但信念冲突的输入时的关键弱点。这些发现对于将LLMs部署在法律、医疗和科学文献等高风险领域具有重要意义，在这些领域中，真相必须超越直观信念，以确保完整性和安全性。|
|**2025-06-07**|**How Important are Videos for Training Video LLMs?**|George Lydakis et.al.|[2506.06928](http://arxiv.org/abs/2506.06928)|null|视频大型语言模型（LLMs）的研究进展迅速，仅几年时间就涌现出众多模型和基准。通常，这些模型以预训练的纯文本LLM初始化，并在图像和视频标题数据集上进行微调。在本文中，我们提出了以下发现：与预期相比，视频LLMs在仅经过图像训练后具有更强的时序推理能力，而特定于视频的训练带来的改进出人意料地小。具体来说，我们展示了使用最近的长视频算法训练的两个LLM的图像训练版本在TVBench时序推理基准测试中显著高于随机水平。此外，我们介绍了一种简单的微调方案，涉及一系列带注释的图像和针对时序能力的问题。这个基线在时序推理性能上接近，有时甚至高于视频训练的LLMs。这表明当前模型未能充分利用现实视频中的丰富时序特征。我们的分析促使进一步研究允许图像训练的LLMs进行时序推理的机制，以及导致当前视频训练方案低效的瓶颈。|
|**2025-06-06**|**Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches**|Patrick Y. Wu et.al.|[2506.06540](http://arxiv.org/abs/2506.06540)|null|在一次破坏性事件或冲击之后，例如2025年政府效率部（DOGE）的联邦裁员，专家判断会受到对结果的了解的影响。这可能会使得重建事件发生前感知变得困难或不可能，这些感知是研究事件相关因素所必需的。这篇立场论文认为，在冲击破坏了传统测量方法时，大型语言模型（LLMs）可以成为专家政治调查的可行替代品。我们以DOGE裁员作为这一立场的具体案例研究。我们使用LLMs的成对比较提示，并为联邦行政部门推导出意识形态得分。这些得分复制了裁员前的专家测量结果，并预测了哪些机构被DOGE针对。我们还采用同样的方法，发现某些联邦机构作为知识机构的认知预测了哪些机构被DOGE针对，即使在控制了意识形态的情况下也是如此。这一案例研究表明，使用LLMs可以让我们快速轻松地测试冲击背后的相关假设因素。更广泛地说，我们对这一最近事件的案例研究展示了当传统测量技术失败时，LLMs如何提供对冲击相关因素的洞察。我们最后提出，当研究人员可以将LLMs作为专家政治调查的替代品时，应遵循的两部分标准。|
|**2025-06-06**|**A Systematic Review of Poisoning Attacks Against Large Language Models**|Neil Fendley et.al.|[2506.06518](http://arxiv.org/abs/2506.06518)|null|随着预训练大型语言模型（LLMs）及其训练数据集的广泛应用，对其使用的安全风险担忧显著增加。其中一种安全风险是LLM中毒攻击的威胁，攻击者通过修改LLM训练过程的一部分，使LLM以恶意的方式行为。作为一个新兴的研究领域，当前针对LLM中毒攻击的框架和术语源自早期的分类中毒文献，并不完全适用于生成性LLM环境。我们对已发表的LLM中毒攻击进行了系统回顾，以阐明安全影响并解决文献中术语的不一致性。我们提出一个全面的毒性威胁模型，适用于分类广泛的LLM中毒攻击。该毒性威胁模型包括四个毒性攻击规范，定义了攻击的物流和操纵策略，以及六个毒性度量，用于衡量攻击的关键特征。在我们的框架下，我们将已发表的LLM中毒文献讨论组织在LLM中毒攻击的四个关键维度上：概念性毒素、隐蔽性毒素、持续性毒素和特定任务的毒素，以更好地理解当前的安全风险格局。|
|**2025-06-06**|**Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage**|Ziqi Yuan et.al.|[2506.06472](http://arxiv.org/abs/2506.06472)|null|我们提出了一种新的基于低成本PCIe固态硬盘（SSD）的GPU内存扩展的寿命感知张量卸载框架。我们的框架TERAIO专门为具有多个GPU和多个SSD的大语言模型（LLM）训练而开发。其设计基于我们的观察：在每次LLM训练迭代中，活跃的张量仅占分配的GPU内存的一小部分（平均为1.7%），不活跃的张量通常较大，并且将在较长时间内不会被使用，这为将张量卸载/预取到/从慢速SSD提供了充足的机会，而不会使GPU训练过程停滞。TERAIO通过在训练过程的最初几个迭代中进行分析，准确估计每个张量的寿命（在GPU内存中的活跃时间）。借助张量寿命分析，TERAIO将生成优化的张量卸载/预取计划，并通过PyTorch将其集成到编译的LLM程序中。TERAIO具有运行时张量迁移引擎，通过GPUDirect存储执行卸载/预取计划，允许GPU和SSD之间直接进行张量迁移，以缓解CPU瓶颈并最大化SSD带宽利用率。与ZeRO-Offload和ZeRO-Infinity等最先进的研究相比，我们表明TERAIO平均提高了各种LLM的训练性能1.47倍，并在假设无限GPU内存的情况下实现了80.7%的理想性能。|
|**2025-06-05**|**The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text**|Nikhil Kandpal et.al.|[2506.05209](http://arxiv.org/abs/2506.05209)|null|大型语言模型（LLMs）通常在大量未经许可的文本上训练，这种做法由于可能侵犯知识产权和引发伦理担忧而受到审查。在公开许可的文本上训练LLMs是解决这些问题的第一步，但先前数据收集工作产生的数据集太小或质量太低，无法生成性能良好的LLMs。为了填补这一空白，我们收集、整理并发布了Common Pile v0.1，这是一个用于LLM预训练的公开许可文本的八太字节集合。Common Pile包含了来自30个来源的内容，涵盖了包括研究论文、代码、书籍、百科全书、教育材料、音频转录等在内的多个领域。关键的是，我们通过在Common Pile的文本上训练两个70亿参数的LLMs来验证我们的努力：Comma v0.1-1T和Comma v0.1-2T，分别训练了100亿和200亿个标记。这两个模型在类似的计算预算下达到了与未经许可文本训练的LLMs（如Llama 1和2 7B）的竞争力。除了发布Common Pile v0.1本身，我们还发布了其创建所使用的代码以及Comma v0.1模型的训练混合和检查点。|
|**2025-06-05**|**RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation**|Tianjiao Li et.al.|[2506.05070](http://arxiv.org/abs/2506.05070)|null|大型语言模型（LLMs）拥有强大的多语言能力，将人类反馈强化学习（RLHF）与翻译任务相结合展现出巨大潜力。然而，我们观察到当将这种范式应用于口语字幕翻译任务时，其表现出乎意料地差。在这项工作中，我们研究了这一问题，并发现由于分布偏移，离线奖励模型（RM）逐渐与在线LLM脱节，最终导致不理想的训练结果。为了解决这个问题，我们提出了RIVAL，一个对抗性训练框架，将过程表述为RM和LLM之间的min-max博弈。RIVAL迭代地更新这两个模型，其中RM训练以区分强弱翻译（定性偏好奖励），而LLM训练以增强其翻译以缩小这一差距。为了稳定训练并提高泛化能力，我们还将在RM中结合定量偏好奖励（例如BLEU），实现与人类评估相一致的无参考质量建模。通过大量实验，我们证明了所提出的对抗性训练框架显著优于翻译基线。|
|**2025-06-05**|**HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training**|Geon-Woo Kim et.al.|[2506.04531](http://arxiv.org/abs/2506.04531)|null|随着大型语言模型（LLMs）的训练越来越依赖于地理分布式的加速器，区域间的通信成本变得高昂，且异构硬件的利用率不均。我们提出了HALoS，一个分层异步优化框架，通过在每个区域内引入本地参数服务器（LPSs）以及一个合并区域间更新的全局参数服务器（GPS）来解决这些问题。这种分层设计最小化了昂贵的跨区域通信，减少了拖累效应，并利用了区域内的快速连接。我们为HALoS在非凸目标下的收敛性提供了严格的收敛分析，包括异步训练中分层动能在其中的理论保证。在实证研究中，HALoS在地理分布式的LLMs训练中比同步基线快7.5倍，并且比现有的异步方法快2.1倍。关键的是，HALoS在标准语言建模和下游基准测试中保持了与完全同步的SGD匹配或超过的精度，同时大幅降低了总训练时间。这些结果表明，分层、服务器端更新累积和全局模型合并是可扩展、高效训练新时代LLMs在异构、地理分布式环境中的强大工具。|
|**2025-06-03**|**PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs**|Ze Yu Zhang et.al.|[2506.02965](http://arxiv.org/abs/2506.02965)|null|混合专家（MoE）因其成功适应大型语言模型（LLMs）而越来越受欢迎。在这项工作中，我们引入了隐私保护协同混合专家（PC-MoE），它利用MoE架构的稀疏性，实现内存高效的去中心化协同LLMs训练，使多个拥有有限GPU内存和数据资源的参与者能够集体训练出比各自单独训练更强大的LLMs。同时，这种方法通过在每个参与者内部保持训练数据以及前向传递信号和梯度的部分来保护每个参与者的训练数据隐私。按设计，PC-MoE协同结合了分布式计算的优点和强大的保密保证。与大多数以降低任务准确性为代价来换取保密性的隐私保护方案不同，我们的框架打破了这种权衡：在七个流行的LLM基准测试中，它几乎匹配（有时甚至超过）完全集中式模型的性能和收敛速度，实现了接近70%的峰值GPU RAM减少，同时对重建攻击具有完全的鲁棒性。|
|**2025-06-03**|**QKV Projections Require a Fraction of Their Memory**|Malik Khalaf et.al.|[2506.02939](http://arxiv.org/abs/2506.02939)|null|多头注意力机制是大型语言模型（LLM）运行的核心，许多研究针对其训练过程中的计算和内存效率进行了优化。尽管大多数研究集中于近似缩放点积，但计算输入 $x$ 的 $Q$、$K$ 和 $V$ 张量的线性投影的内存消耗常常被忽视。为了解决这个问题，我们提出了点近似矩阵乘法（PAMM），这是一种新颖的张量压缩技术，通过将注意力层中的 $Q,K,V$ 投影的内存消耗减少到原来的 $\times 512$ ，有效地消除了它们的内存占用，同时达到了类似甚至更好的最终困惑度。PAMM 与高效的注意力技术，如 FlashAttention，完全兼容，使其成为内存高效 LLM 训练的一个实用且互补的方法。|
|**2025-06-03**|**Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning**|Yin Fang et.al.|[2506.02911](http://arxiv.org/abs/2506.02911)|**[link](https://github.com/ncbi-nlp/cell-o1)**|**细胞类型注释是分析单细胞RNA测序数据异质性的关键任务。尽管最近的基础模型自动化了这个过程，但它们通常独立地对细胞进行注释，不考虑批次的细胞背景或提供解释性推理。相比之下，人类专家通常根据他们的专业知识，根据不同的细胞簇注释不同的细胞类型。为了模拟这种工作流程，我们引入了CellPuzzles任务，其目标是给一批细胞分配唯一的细胞类型。这个基准涵盖了多种组织、疾病和供体条件，需要跨越批次的细胞背景进行推理以确保标签的唯一性。我们发现现成的语言大模型（LLMs）在CellPuzzles上表现不佳，最佳基线（OpenAI的o1）仅达到19.0%的批次准确率。为了填补这一空白，我们提出了Cell-o1，这是一个通过在蒸馏推理轨迹上进行监督微调训练的70亿参数LLM，随后通过批次奖励进行强化学习。Cell-o1实现了最先进的性能，比o1高出73%以上，并且在不同情境中表现出良好的泛化能力。对训练动态和推理行为的进一步分析提供了关于批次注释性能和涌现的专家级推理的见解。代码和数据可在https://github.com/ncbi-nlp/cell-o1上找到。**|
|**2025-06-02**|**Why Gradients Rapidly Increase Near the End of Training**|Aaron Defazio et.al.|[2506.02285](http://arxiv.org/abs/2506.02285)|null|在长时间的大型语言模型（LLM）训练运行过程中，梯度范数在训练结束时迅速增加。在这篇简短的笔记中，我们表明这种增加是由于权重衰减、归一化层和学习率调度之间的意外交互所导致的。我们提出了一种简单的校正方法，该方法修复了这种行为，同时在整个训练过程中还导致损失值降低。|
|**2025-06-02**|**Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training**|Pierre-Carl Langlais et.al.|[2506.01732](http://arxiv.org/abs/2506.01732)|null|大型语言模型（LLMs）通常在来自不同来源和领域的海量数据上进行预训练。这些数据往往包含数万亿个标记，其中很大一部分是受版权保护或专有内容，这阻碍了在人工智能法规下使用此类模型。这引发了对真正开放的预训练数据的需求，这些数据符合数据安全法规。在本文中，我们介绍了Common Corpus，这是用于语言模型预训练的最大开放数据集。Common Corpus中收集的数据要么是无版权的，要么是在可接受许可下的，总共有大约两万亿个标记。该数据集包含多种语言，从主要的欧洲语言到在预训练数据集中很少出现的低资源语言；此外，它还包括大量的代码数据。在涵盖的领域和时间周期方面的数据来源多样性为知识各个领域的科研和创业需求开辟了道路。在本技术报告中，我们展示了数据收集的详细来源以及数据集过滤和管理的细节。由于已经被Anthropic等业界领先企业以及多个LLM训练项目使用，我们相信Common Corpus将成为LLMs开放科学研究的关键基础设施。|
|**2025-06-02**|**Fairness Dynamics During Training**|Krishna Patel et.al.|[2506.01709](http://arxiv.org/abs/2506.01709)|null|我们研究了大型语言模型（LLM）训练过程中的公平性动态，以通过训练干预措施（如早期停止）来诊断偏差和缓解；我们发现偏差可能会突然出现，并不总是遵循常见的性能指标。我们引入了两个新的指标来全面评估模型预训练期间的公平性动态：平均排名和部分Jensen-Shannon散度。这些指标为我们提供了关于Pythia模型在WinoBias数据集上对职业性别预测偏差进展的洞察。通过监控这些动态，我们发现：（1）Pythia-6.9b倾向于男性；在训练过程中，它预测“男性”的表现和自信度都比预测“女性”更高；（2）通过早期停止，Pythia-6.9b可以将LAMBADA上的1.7%准确率提升92.5%的公平性；（3）更大的模型可能表现出更多的偏差；Pythia-6.9b对性别的假设比Pythia-160m更多，即使当未指定主体的性别时也是如此。|
|**2025-06-02**|**EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation**|Bingqian Lin et.al.|[2506.01551](http://arxiv.org/abs/2506.01551)|null|构建能够根据自然语言指令进行导航的视觉-语言导航（VLN）智能体是机器人与人交互领域的一个长期目标。近期研究揭示了通过训练开源的大型语言模型（LLMs）来释放LLMs推理能力以提升导航性能，并同时减少LLMs训练语料库与VLN任务之间的领域差距的潜力。然而，这些方法主要采用直接输入-输出映射范式，导致映射学习困难，导航决策难以解释。思维链（Chain-of-Thought，CoT）训练是提高导航决策准确性和可解释性的有前景的方法，但导航任务的复杂性使得完美的CoT标签无法获得，且可能导致通过纯CoT监督微调出现过拟合。在本文中，我们提出了一种新颖的自我改进的具身推理框架，用于提升基于LLM的视觉-语言导航，命名为EvolveNav。我们的EvolveNav包括两个阶段：（1）形式化CoT监督微调，我们通过使用形式化的CoT标签来训练模型，旨在激活模型的导航推理能力并提高推理速度；（2）自我反思的后续训练，模型通过迭代训练使用自己的推理输出作为自我丰富化的CoT标签，以增强监督多样性。我们还引入了一个自我反思的辅助任务，通过对比错误推理模式来鼓励学习正确的推理模式。在流行的VLN基准测试上的实验结果表明，EvolveNav优于之前的基于LLM的VLN方法。代码可在https://github.com/expectorlin/EvolveNav获取。|
|**2025-06-01**|**Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons**|Wenshuo Dong et.al.|[2506.00759](http://arxiv.org/abs/2506.00759)|null|大型语言模型（LLMs）在大量数据上训练能够捕捉训练数据中嵌入的丰富信息。然而，这也引入了隐私泄露的风险，尤其是涉及个人身份信息（PII）。尽管先前的研究表明，可以通过隐私神经元等方法来减轻这种风险，但它们都假设（敏感的）训练数据和用户查询都是英语。我们表明，它们无法防御跨语言环境中的隐私泄露：即使训练数据完全使用一种语言，这些（私人）模型在用另一种语言查询时仍可能泄露私人信息。在这项工作中，我们首先研究了跨语言隐私泄露的信息流，以更好地理解这一问题。我们发现，LLMs在中间层处理私人信息，其中表示在语言间大量共享。泄露风险在后续层转换为特定语言空间时达到峰值。基于此，我们确定了隐私通用神经元和语言特定隐私神经元。隐私通用神经元影响所有语言的隐私泄露，而语言特定隐私神经元仅与特定语言相关。通过禁用这些神经元，跨语言隐私泄露风险降低了23.3%-31.6%。|
|**2025-05-31**|**Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing**|Fatemeh Lotfi et.al.|[2506.00574](http://arxiv.org/abs/2506.00574)|null|现代无线网络必须适应动态条件，同时高效地管理多样化的服务需求。传统的深度强化学习（DRL）在这些环境中存在困难，因为分散和演变的反馈使得最优决策变得具有挑战性。大型语言模型（LLMs）通过将无组织的网络反馈结构化为有意义的潜在表示，为强化学习代理更有效地识别模式提供了解决方案。例如，在O-RAN切片中，诸如信噪比、功率水平和吞吐量等概念在语义上是相关的，LLMs可以自然地将它们聚类，提供更可解释的状态表示。为了利用这一能力，我们引入了一种基于上下文化的自适应方法，将可学习的提示整合到LLM增强的DRL框架中。我们不是依赖于模型的全量微调，而是通过特定任务的提示来细化状态表示，这些提示能够动态地调整网络条件。利用在O-RAN知识上训练的LLM ORANSight，我们开发了提示增强的多智能体强化学习（PA-MRL）框架。可学习的提示优化了语义聚类和强化学习目标，使强化代理能够在更少的迭代中获得更高的奖励并更有效地适应。通过结合提示增强的学习，我们的方法使得在O-RAN切片中实现更快的、更可扩展的、更自适应的资源分配成为可能。实验结果表明，它加速了收敛速度，并优于其他基线。|
|**2025-05-30**|**SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training**|Yehonathan Refael et.al.|[2505.24749](http://arxiv.org/abs/2505.24749)|null|低秩梯度优化方法在大型语言模型（LLMs）的训练过程中显著提高了内存效率，使得在受限于硬件的情况下也能进行操作而不会牺牲性能。然而，这些方法主要强调内存节省，常常忽视了由于依赖标准各向同性最速下降技术而可能导致的收敛加速，这些技术在深度网络，尤其是LLMs的高各向异性景观中可能表现不佳。在本文中，我们提出了SUMO（子空间感知矩正交化），这是一种优化器，它采用精确的奇异值分解（SVD）在动态调整的低维子空间中进行矩正交化，从而实现规范诱导的最速下降优化步骤。通过将优化步骤与损失景观的谱特性显式对齐，SUMO有效地缓解了与常用方法（如Newton-Schulz正交化近似）相关的近似误差。我们从理论上建立了一个这些近似误差的上界，并证明了它们依赖于矩的条件数，我们通过解析证明了这些条件在LLMs训练过程中会遇到。此外，我们理论上和经验上都说明了通过SVD进行精确正交化可以显著提高收敛速度，同时降低整体复杂度。经验评估证实，与最先进的方法相比，SUMO可以加速收敛、提高稳定性、提升性能，并将内存需求降低多达20%。|
|**2025-05-30**|**Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling**|Yimin Du et.al.|[2505.24199](http://arxiv.org/abs/2505.24199)|null|人类偏好数据的质量对于训练和评估大型语言模型（LLMs）至关重要，尤其是在基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO）场景中。传统的并行（SBS）标注方法往往难以应对固有的不确定性、标注者之间的不一致性以及偏好判断的复杂性。本文介绍了一种基于直觉模糊集（IFS）的新型框架，用于在LLM数据标注任务中建模和聚合人类偏好。我们的方法不仅捕捉了偏好的程度，还通过隶属度、非隶属度和犹豫度捕捉了人类判断中的不确定性和犹豫。我们提出了一种基于IFS的标注协议，它能够实现更细微的偏好建模，开发了处理标注者不一致性的聚合方法，并引入了用于偏好数据评估的质量指标。在多个数据集上的实验验证表明，与传统的二元和李克特量表方法相比，我们的基于IFS的方法显著提高了标注一致性，减少了标注者疲劳，并产生了更高质量的偏好数据。由此产生的偏好数据集使得下游任务中的模型性能得到提升，与基线模型相比，胜率提高了12.3%，标注时间减少了15.7%。我们的框架为处理人类偏好标注中的不确定性提供了一个原则性的方法，并为大规模LLM训练提供了实际效益。|
|**2025-05-30**|**CodeV-R1: Reasoning-Enhanced Verilog Generation**|Yaoyu Zhu et.al.|[2505.24183](http://arxiv.org/abs/2505.24183)|null|大型语言模型（LLMs）通过可验证奖励的强化学习（RLVR）在具有明确、可自动验证的任务上取得了突破，例如软件编程和数学问题。然而，将RLVR扩展到电子设计自动化（EDA），尤其是从自然语言（NL）规范自动生成硬件描述语言（HDL）如Verilog，面临着三个关键挑战：缺乏自动化和准确的验证环境、高质量NL代码对的稀缺以及RLVR的计算成本过高。为此，我们引入了CodeV-R1，这是一个用于训练Verilog生成LLMs的RLVR框架。首先，我们开发了一个基于规则的测试平台生成器，它可以对黄金参考进行稳健的等价性检查。其次，我们提出了一种往返数据合成方法，将开源Verilog片段与LLM生成的NL描述配对，通过生成的测试平台验证代码-NL代码的一致性，并过滤掉不等价的示例，以生成高质量的语料库。第三，我们采用了一个两阶段的“先蒸馏后RL”训练流程：首先进行蒸馏以启动推理能力，然后使用自适应DAPO，我们的新型RLVR算法，通过自适应调整采样率来降低训练成本。所得到的模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上分别实现了68.6%和72.9%的pass@1，比之前的最先进技术提高了12~20%，并且与671B DeepSeek-R1的性能相当甚至更好。我们将发布我们的模型、训练流程和语料库，以促进EDA和LLM社区的研究。|
|**2025-05-29**|**MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning**|Yong-Cheng Liaw et.al.|[2505.23254](http://arxiv.org/abs/2505.23254)|null|由于生成式人工智能（AI）取得了巨大成功，大型语言模型（LLMs）已成为一个核心子类，支撑着问答、文本生成和代码补全等应用。尽管在特定领域的数据上微调这些模型可以获得显著的性能提升，但这也带来了令人望而生畏的计算挑战，尤其是对于硬件资源有限的科研人员和小型组织。尽管SSD卸载（即ZeRO-Infinity）已成为通过利用系统内存（即CPU DRAM）和存储空间（即固态设备，SSDs）来克服GPU内存障碍的有效策略，但其设计主要针对以模型为中心的性能问题。因此，包括系统内存碎片化、不高效的固定缓冲区分配、CPU峰值使用率激增和文件系统开销等关键系统级问题仍未得到解决，这阻碍了可扩展性并增加了成本。这样的观察促使本文提出MemAscend框架，该框架系统地解决了SSD卸载的LLM训练中未充分探索的系统内存瓶颈，重点关注资源受限的环境。通过简化固定内存分配、消除碎片化和缓解峰值开销，MemAscend回收了大量的系统内存预算，使得在不超出适度硬件限制的情况下，能够使用更大的模型、更长的上下文窗口和更高的批量大小。在多种LLM基准测试中，与标准SSD卸载技术相比，MemAscend将峰值系统内存消耗平均降低了55.7%，降低了微调的硬件门槛，并解锁了在资源有限机器上进行成本效益高的大规模训练的新可能性。|
|**2025-05-29**|**Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification**|Wenjing Xing et.al.|[2505.23177](http://arxiv.org/abs/2505.23177)|null|传统的代码指令数据合成方法存在多样性和逻辑性不足的问题。我们提出了Infinite-Instruct，这是一个用于合成高质量问答对的自动化框架，旨在增强大型语言模型（LLM）的代码生成能力。该框架专注于提高合成问题的内部逻辑和合成代码的质量。首先，“逆向构建”将代码片段转化为多样化的编程问题。然后，通过“反向构建”，将编程问题中的关键词结构化成知识图谱，以重构具有更强内部逻辑的编程问题。最后，一个跨语言的静态代码分析流程过滤无效样本，以确保数据质量。实验表明，在主流代码生成基准测试中，我们的微调模型在70亿参数模型上实现了平均性能提升21.70%，在320亿参数模型上实现了36.95%的提升。使用不到十分之一的指令微调数据，我们实现了与Qwen-2.5-Coder-Instruct相当的性能。Infinite-Instruct为编程中的LLM训练提供了一个可扩展的解决方案。我们开源了实验中使用的数据集，包括未经过滤的版本和通过静态分析过滤的版本。数据可在https://github.com/xingwenjing417/Infinite-Instruct-dataset获取。|
|**2025-05-29**|**Generating Diverse Training Samples for Relation Extraction with Large Language Models**|Zexuan Li et.al.|[2505.23108](http://arxiv.org/abs/2505.23108)|null|使用大型语言模型（LLMs）生成训练数据可能是改进零样本或少量样本NLP任务的优选方法。然而，许多问题仍需对此方向进行研究。对于关系抽取（RE）任务，我们发现直接提示LLMs生成的样本可能很容易彼此具有高度的相似性。它们在表达一对实体之间的关系时，往往使用有限的句式变体。因此，在本文中，我们研究了如何有效地提高LLMs生成的RE训练样本的多样性，同时保持其正确性。我们首先尝试通过直接在情境学习（ICL）提示中给出指令，使LLMs产生不相似的样本。然后，我们提出了一种通过直接偏好优化（DPO）微调LLMs以生成多样性训练样本的方法。我们使用常用RE数据集进行的实验表明，这两种尝试都能提高生成的训练数据质量。我们还发现，与直接使用LLM进行关系抽取相比，使用其生成的样本训练非LLM RE模型可能导致更好的性能。|
|**2025-05-29**|**LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training**|Bo Wu et.al.|[2505.24034](http://arxiv.org/abs/2505.24034)|null|强化学习（RL）已成为提升大型语言模型（LLMs）能力最有效的后训练方法。在实际应用中，由于对延迟和内存的高要求，开发一个能够可靠管理具有数百亿至数千亿参数的政策模型的效率化RL框架尤其具有挑战性。在本文中，我们提出了LlamaRL，这是一个全分布式、异步RL框架，针对在从少量到数千个设备的GPU集群上高效训练各种规模的大型LLMs（8B、70B和405B参数）进行了优化。LlamaRL引入了一种基于原生PyTorch的简化、单控制器架构，实现了模块化、易用性和无缝扩展至数千个GPU。我们还对LlamaRL的效率进行了理论分析，包括一个形式化的证明，表明其异步设计可以导致严格的RL加速。在Llama 3后训练期间，通过利用如模型卸载、异步离策略训练和分布式直接内存访问以进行权重同步等最佳实践，LlamaRL实现了显著的效率提升——与DeepSpeed-Chat类系统相比，在405B参数政策模型上实现了高达10.7倍的速度提升。此外，随着模型规模的增加，效率优势持续增长，这证明了该框架适用于未来的大规模RL训练。|
|**2025-05-28**|**Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition**|Yuan Tseng et.al.|[2505.22251](http://arxiv.org/abs/2505.22251)|null|最近的研究表明，大型语言模型（LLMs）在语音任务上的性能可以优于现有系统。为了支持这一说法，常常引用LibriSpeech和Common Voice的结果。然而，这项研究发现，LibriSpeech和Common Voice评估集中的大量数据出现在公共LLM预训练语料库中。这引发了人们对从这两个数据集中得出的结果的可靠性的质疑。为了衡量这种污染的影响，对比了受污染和不受污染训练的LLMs，结果表明，受污染的LLMs更有可能生成其在训练过程中已见过的测试句子。使用受污染的LLMs的语音识别器在错误率上只有细微的差异，但为在训练过程中看到的转录文本分配了显著更高的概率。结果显示，LLMs的输出可能会受到微小数据污染的影响，突出了用保留数据评估基于LLMs的语音系统的重要性。|
|**2025-05-28**|**Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R $^2$)GRPO**|Ran Li et.al.|[2505.22068](http://arxiv.org/abs/2505.22068)|null|先前的研究表明，使用可验证奖励的强化学习（RLVR）训练的强大大型语言模型（LLMs）在数学任务中仅能优化推理路径，而不会提高推理能力；而监督微调（SFT）与蒸馏结合则能实现这一点。我们从科学信息提取（SciIE）的角度对此进行研究，发现LLMs和推理LLMs在SciIE任务上的表现不如基于Bert的小型模型。SciIE任务需要推理和记忆能力。我们认为，SFT和RLVR都可以基于SciIE以简单的方式优化推理路径并提高推理能力。我们提出了两阶段训练方法，包括1. MimicSFT，使用结构化推理模板，无需高质量的思维链数据；2. R$^2$GRPO，结合相关性和规则诱导的奖励。在科学信息提取基准测试上的实验表明，这两种方法都能提高推理能力。结合MimicSFT的R$^2$ GRPO在关系抽取方面超越了基线LLMs和专业监督模型。我们的代码可在https://github.com/ranlislz/R2GRPO上找到。|
|**2025-05-28**|**Estimating the Effects of Sample Training Orders for Large Language Models without Retraining**|Hao Yang et.al.|[2505.22042](http://arxiv.org/abs/2505.22042)|null|训练样本的顺序在大语言模型（LLMs）中起着至关重要的作用，这不仅显著影响着它们的外部性能，还影响着它们内部的学习动态。传统方法通常需要使用各种样本顺序重新训练模型，这对于LLMs来说在计算上是不可行的。在这项工作中，我们通过设计一个无需重新训练的框架来改进传统方法。通过将Adam优化器的更新近似为一级和二级泰勒展开，并利用随机投影方法来存储中间检查点，我们的框架可以高效地估计任意训练样本顺序下的模型参数。接下来，我们将我们的框架应用于两个下游研究问题：（1）LLMs的训练课程设计——我们基于无需重新训练的框架，提出了一种新颖的课程学习策略，通过增强课程提案与估计的模型性能，实现更明智的样本调度。（2）LLMs的记忆和泛化效果分析——我们使用无需重新训练的框架来估计训练样本的位置如何影响LLMs的记忆和泛化能力。我们进行了广泛的实验来验证我们的无需重新训练的框架在再现真实模型性能方面的有效性，并进一步展示了其在优化LLMs训练课程和分析LLMs的记忆和泛化效果方面的潜力。|
|**2025-05-28**|**cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning**|Maksim Kolodiazhnyi et.al.|[2505.22914](http://arxiv.org/abs/2505.22914)|**[link](https://github.com/col14m/cadrille)**|计算机辅助设计（CAD）在工程和制造业中扮演着核心角色，使得创建精确且可编辑的3D模型成为可能。利用各种传感器或用户提供的作为CAD重建输入的数据，可以使设计应用更加普及。然而，现有的方法通常只关注单一输入模式，如点云、图像或文本，这限制了它们的通用性和鲁棒性。利用视觉语言模型（VLM）的最新进展，我们提出了一种多模态CAD重建模型，该模型同时处理所有三种输入模式。受大型语言模型（LLM）训练方法的启发，我们采用两阶段流程：在大型程序生成数据上执行监督微调（SFT），然后使用在线反馈进行强化学习（RL）微调，该反馈通过编程获得。此外，我们是第一个探索LLM的CAD任务RL微调的人，证明了在线RL算法，如组相对偏好优化（GRPO），优于离线替代方案。在DeepCAD基准测试中，我们的SFT模型在所有三种输入模式上同时优于现有的单一模态方法。更重要的是，经过RL微调后，cadrille在三个具有挑战性的数据集上，包括一个真实世界的数据集上，创造了新的最先进水平。|
|**2025-05-27**|**TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction**|Tommy Xu et.al.|[2505.21807](http://arxiv.org/abs/2505.21807)|null|表格数据的预测建模是许多现实应用的基础。尽管梯度提升机和一些最近的深度模型在表格数据上取得了强大的性能，但它们通常缺乏可解释性。另一方面，大型语言模型（LLMs）已经显示出强大的生成类似人类推理和解释的能力，但在表格数据预测方面表现不佳。在本文中，我们提出了一种新方法，该方法利用基于推理的LLMs，通过强化学习进行训练，以在表格数据上实现更准确和可解释的预测。我们的方法引入了自定义奖励函数，引导模型不仅向高预测精度迈进，而且还向其预测的人类可理解的原因迈进。实验结果表明，我们的模型在金融基准数据集上取得了有希望的性能，超过了大多数现有的LLMs。|
|**2025-05-27**|**R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning**|Yongchao Chen et.al.|[2505.21668](http://arxiv.org/abs/2505.21668)|**[link](https://github.com/yongchao98/r1-code-interpreter)**|**尽管R1-like模型在推理和规划方面取得了进展，但大型语言模型（LLMs）在需要精确计算、符号操作、优化和算法推理的任务上仍然存在困难，因为文本推理缺乏代码执行的严谨性。一个关键挑战是使LLMs能够决定何时使用文本推理而不是代码生成。虽然OpenAI训练模型在需要时调用代码解释器，但公开的研究缺乏指导如何将预训练的LLMs与代码有效地结合并泛化到各种任务中。我们提出了R1-Code-Interpreter，这是通过多轮监督微调（SFT）和强化学习（RL）训练的纯文本LLM的扩展，可以在逐步推理过程中自主生成多个代码查询。我们精心准备了144个推理和规划任务（其中107个用于训练，37个用于测试），每个任务都有超过200个多样化的问题。我们使用各种SFT和RL策略微调Qwen-2.5模型（3B/7B/14B），研究不同的答案格式、推理模型与非推理模型、冷启动与热启动、GRPO与PPO以及带掩码的代码输出与不带掩码的代码输出。与先前在窄领域上的RL工作不同，我们发现由于任务多样性和昂贵的代码执行，代码解释器训练难度显著增加，这突出了SFT阶段的关键作用。我们的最终模型R1-CI-14B在37个测试任务上的平均准确率从44.0%提高到64.1%，超过了GPT-4o（纯文本：58.6%）并接近GPT-4o使用代码解释器的情况（70.9%），通过代码生成出现了自检查的行为。数据集、代码和模型可在https://github.com/yongchao98/R1-Code-Interpreter和https://huggingface.co/yongchao98获取。**|
|**2025-05-26**|**Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations**|Chaoyi Xiang et.al.|[2505.19674](http://arxiv.org/abs/2505.19674)|**[link](https://github.com/chunhualiu596/word_association_generation)**|**随着大型语言模型的影响日益增加，理解它们所反映的道德价值观变得越来越重要。由于可能的人类规范泄漏到模型训练数据中以及它们对提示形式的敏感性，通过直接提示评估这些模型所理解的道德价值观的性质具有挑战性。因此，我们提出使用被证明能反映人类道德推理的词语联想作为低级底层表示，以获得LLM道德推理的更稳健图景。我们研究了来自西方英语社区和主要在英语数据上训练的LLM的关联中的道德差异。首先，我们创建了一个大型的LLM生成的词语联想数据集，类似于现有的一个人类词语联想数据集。接下来，我们提出了一种基于来自道德基础理论的原生词通过人类和LLM生成的关联图传播道德价值观的新方法。最后，我们比较了由此产生的道德概念化，突出了来自英语说话者和LLM关联中道德价值观的详细但系统性的差异。**|
|**2025-05-26**|**CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis**|Ruixiang Feng et.al.|[2505.19484](http://arxiv.org/abs/2505.19484)|**[link](https://github.com/mmadmax/culfit)**|**大型语言模型（LLMs）在各项任务中表现出色，但它们往往存在特定的文化偏见，忽视了低资源地区价值观和语言多样性。这种文化偏见不仅损害了普遍平等，还可能加剧刻板印象和延续歧视。为了解决这个问题，我们提出了CulFiT，这是一种新的文化感知训练范式，它利用多语言数据和细粒度奖励模型来提高文化敏感性和包容性。我们的方法综合了多样化的文化相关问题，在文化相关的语言中构建批判性数据，并采用细粒度奖励将文化文本分解为可验证的知识单元以进行可解释的评估。我们还引入了GlobalCultureQA，这是一个多语言开放式问答数据集，旨在在全球范围内评估文化感知回答。在三个现有基准和我们的GlobalCultureQA上的大量实验表明，CulFiT在文化一致性以及一般推理方面实现了最先进的开源模型性能。**|
|**2025-05-26**|**Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning**|Shenao Zhang et.al.|[2505.20561](http://arxiv.org/abs/2505.20561)|null|大型语言模型（LLMs）通过强化学习（RL）训练展现出强大的推理能力以及诸如回溯和错误纠正等涌现的反思行为。然而，传统的马尔可夫强化学习将探索限制在训练阶段以学习最优的确定性策略，并且仅通过当前状态依赖历史上下文。因此，在马尔可夫强化学习训练期间是否会出现反思推理，以及为什么它们在测试时有益，仍然不清楚。为了解决这个问题，我们将反思探索重新表述为贝叶斯自适应强化学习（Bayes-Adaptive RL）框架内，该框架明确优化了在马尔可夫决策过程的后验分布下的预期回报。这种贝叶斯公式内在地激励了通过信念更新进行奖励最大化的利用和信息收集的探索。我们得到的算法BARL指导LLM根据观察到的结果来拼接和切换策略，为模型何时以及如何进行反思探索提供了原则性的指导。在合成和数学推理任务上的实证结果表明，BARL在测试时间上优于标准的马尔可夫强化学习方法，实现了更高的token效率和改进的探索有效性。我们的代码可在https://github.com/shenao-zhang/BARL上找到。|
|**2025-05-26**|**Towards Fully FP8 GEMM LLM Training at Scale**|Alejandro Hernández-Cano et.al.|[2505.20524](http://arxiv.org/abs/2505.20524)|null|尽管FP8数据格式在大型语言模型（LLM）预训练中具有巨大的潜力，但由于在规模上保持稳定性的挑战，其应用一直有限。现有方法通常依赖于次优的细粒度FP8内核，或者在敏感组件（如注意力投影）中回退到更高精度的矩阵乘法（GEMMs），这会牺牲潜在的吞吐量提升。我们引入了一类新的LLM架构，首次支持在正向和反向传播过程中，在transformer块内的所有GEMMs中执行FP8计算。这实现了前所未有的吞吐量提升，尤其是在规模上，同时保持了标准BF16训练的下层性能。我们的架构设计减少了大的异常激活，促进了稳定的长期FP8训练。此外，我们还确定了关键指标以监控低精度训练并预测潜在的将来发散。|
|**2025-05-25**|**FP4 All the Way: Fully Quantized Training of LLMs**|Brian Chmiel et.al.|[2505.19115](http://arxiv.org/abs/2505.19115)|**[link](https://github.com/anonymous1252022/fp4-all-the-way)**|**我们首次展示了在高达2000亿个标记的数据集上，使用4位浮点数（FP4）精度对大型语言模型（LLMs）进行完全量化训练（FQT）的方法。我们广泛研究了FP4的关键设计选择，包括块大小、缩放格式和舍入方法。我们的分析表明，NVFP4格式，其中每个包含16个FP4值（E2M1）的块共享一个用E4M3表示的缩放因子，提供了最佳结果。我们使用随机舍入进行反向和更新遍历，以及四舍五入到最近值进行正向遍历，以增强稳定性。此外，我们确定了一个理论上的和经验上的有效量化训练阈值：当梯度范数低于约 $\sqrt{3}$ 倍的量化噪声时，量化训练变得不太有效。利用这些见解，我们在256个Intel Gaudi2加速器上成功训练了一个70亿参数的模型。结果FP4训练的模型在下游任务性能上与标准的BF16基线相当，证实了FP4训练是一种实际且高度高效的大规模LLM训练方法。相关参考实现已提供在https://github.com/Anonymous1252022/fp4-all-the-way。**|
|**2025-05-25**|**SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data**|Wenkai Fang et.al.|[2505.20347](http://arxiv.org/abs/2505.20347)|**[link](https://github.com/wantbook-book/serl)**|**近期的研究表明，强化学习（RL）在提升大型语言模型（LLM）推理能力方面具有有效性。然而，现有工作不可避免地依赖于高质量指令和可验证的奖励来进行有效训练，这两者在专业领域往往难以获得。在本文中，我们提出了自博弈强化学习（SeRL），以有限的初始数据启动LLM的训练。具体来说，SeRL包括两个互补的模块：自我指令和自我奖励。前者模块在每个训练步骤基于可用数据生成额外的指令，采用稳健的在线过滤策略来确保指令的质量、多样性和难度。后者模块引入了一种简单而有效的多数投票机制来估计额外指令的响应奖励，消除了对外部标注的需求。最后，SeRL基于生成数据进行传统的强化学习，促进迭代自博弈学习。在多个推理基准和不同LLM基座上进行的大量实验表明，所提出的SeRL产生了优于其对应物的结果，并达到了与高质量数据可验证奖励获得的结果相当的性能。我们的代码可在https://github.com/wantbook-book/SeRL找到。**|
|**2025-05-24**|**Autocomp: LLM-Driven Code Optimization for Tensor Accelerators**|Charles Hong et.al.|[2505.18574](http://arxiv.org/abs/2505.18574)|null|硬件加速器，尤其是专为张量处理设计的加速器，在当今的计算领域中无处不在。然而，尽管在构建编译器方面付出了巨大的努力，编程这些张量加速器仍然具有挑战性，导致其潜力未得到充分利用。最近，经过大量代码训练的大型语言模型（LLMs）在代码生成和优化任务中显示出巨大的潜力，但生成低资源语言如专用张量加速器代码仍然是一个重大挑战。我们通过Autocomp方法解决了这一挑战，该方法使加速器程序员能够利用领域知识和硬件反馈，通过自动的LLM驱动搜索来优化代码。我们通过以下方式实现这一目标：1）将每个优化步骤制定为一个结构化的两阶段提示，分为规划和代码生成阶段；2）通过简洁且可适应的优化菜单在规划阶段插入领域知识；3）在每个搜索迭代中，将硬件的准确性和性能指标作为反馈。在三个代表性工作负载类别和两种不同的加速器上，我们证明了Autocomp优化的代码比供应商提供的库快5.6倍（GEMM）和2.7倍（卷积），并且比专家级手动调整的代码快1.4倍（GEMM）、1.1倍（卷积）和1.3倍（细粒度线性代数）。此外，我们还证明了从Autocomp生成的优化计划可以在类似的张量操作中重用，在固定的样本预算下，速度提升可达24%。|
|**2025-05-24**|**The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models**|Kefan Yu et.al.|[2505.18497](http://arxiv.org/abs/2505.18497)|null|当前大型语言模型（LLMs）在社交智能任务中展现出新兴的能力，包括隐含意义解析（Sravanthi等，2024年）和心智理论推理（Shapira等，2024年），这两项任务都需要大量的语用理解。然而，LLMs在整个训练过程中如何获得这种能力仍不太清楚。在这项工作中，我们引入了ALTPRAG，这是一个基于替代语用概念的语料库，旨在评估不同训练阶段的LLMs能否准确推断细微的说话者意图。每个实例都配对两个在语境上适当但在语用上不同的后续内容，从而能够对语用解释和对比推理进行细致的评估。我们系统地评估了22个LLMs在关键训练阶段的表现：预训练、监督微调（SFT）和偏好优化，以检查语用能力的发展。我们的结果表明，即使是基础模型也表现出对语用线索的显著敏感性，并且随着模型和数据规模的增加而持续改进。此外，SFT和RLHF进一步提升了性能，尤其是在认知-语用推理方面。这些发现突出了语用能力是LLM训练中涌现的和组合性的属性，并为将模型与人类交流规范相一致提供了新的见解。|
|**2025-05-23**|**ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction**|Yan Yu et.al.|[2505.17691](http://arxiv.org/abs/2505.17691)|null|大型语言模型（LLMs）被广泛用作开放性任务的评估者，而先前的研究已经强调了LLM评估中的偏差问题，但在成对比较中的非传递性问题仍未得到解决：成对比较中的非传递性偏好，即评估者偏好A胜过B，B胜过C，但C胜过A。我们的结果表明，低质量的训练数据可能会降低评估者LLM生成的偏好的传递性。为了解决这个问题，我们提出了一种图论框架，通过将成对偏好建模为锦标赛图来分析和缓解这一问题。我们量化了非传递性，并引入了有向图结构熵来衡量偏好的整体清晰度。我们的分析揭示了高级评估者LLMs（其中Qwen2.5-Max达到67.96%）中存在显著的非传递性，以及高熵值（Qwen2.5-Max为0.8095），反映了偏好整体清晰度较低。为了解决这个问题，我们设计了一种过滤策略，ELSPR，以消除引起非传递性的偏好数据，仅保留一致且传递的偏好数据用于模型微调。实验表明，使用过滤数据进行微调的模型将非传递性降低了13.78%（从64.28%降至50.50%），将结构熵降低了0.0879（从0.8113降至0.7234），并且与人类评估者更加一致（人类一致性率提高了0.6%，Spearman相关系数增加了0.01）。|
|**2025-05-23**|**H2:Towards Efficient Large-Scale LLM Training on Hyper-Heterogeneous Cluster over 1,000 Chips**|Ding Tang et.al.|[2505.17548](http://arxiv.org/abs/2505.17548)|null|近期大型语言模型（LLMs）的进步需要大量的计算资源，这促使了使用来自多个供应商的各种硬件加速器。然而，传统的分布式训练框架由于软件栈、算子实现、通信库和硬件能力的显著差异，难以有效地利用由数千个芯片组成的超异构集群。为了解决这些挑战，我们提出了H2，即HyperHetero，这是一个系统框架，能够使LLMs在拥有超过1000个异构芯片的集群上高效训练。H2集成了DiTorch，这是一个统一的PyTorch兼容接口，确保了芯片间的程序一致性，以及DiComm，这是一个针对异构环境优化的设备直接RDMA通信库。此外，我们引入了HeteroPP与HeteroAuto，这是一种自适应的管道并行策略，能够动态平衡计算负载、内存限制和通信开销。在1000亿参数的LLM上的评估表明，我们的方法在实验中始终实现了超线性的加速，比基线同构训练解决方案快了高达16.37%。这些发现验证了在前所未有的规模上超异构训练的可行性和效率。|
|**2025-05-23**|**L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models**|Xiaohao Liu et.al.|[2505.17505](http://arxiv.org/abs/2505.17505)|null|大型语言模型（LLMs）取得了显著的进展。尽管如此，由于本质上具有顺序性，主流的LLMs训练和推理方法——下一个标记预测（NTP）——在上下文覆盖范围和推理效率方面都存在限制。为了克服这些挑战，我们提出了跳跃多标记预测（L-MTP），这是一种创新的标记预测方法，通过引入跳跃机制扩展了多标记预测（MTP）的能力。与传统的MTP不同，它不是在相邻位置生成多个标记，而是L-MTP有策略地跳过中间标记，在一次前向传递中预测非顺序标记。这种结构化的跳跃不仅增强了模型捕捉长距离依赖的能力，还实现了一种专门针对非顺序跳跃标记生成的解码策略，有效地加速了推理。我们从理论上证明了L-MTP在提高推理效率方面的优势。在多个基准测试中的实验验证了它在提升LLMs性能和推理速度方面的优势。源代码将公开提供。|
|**2025-05-23**|**PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training**|Matan Haroush et.al.|[2505.18313](http://arxiv.org/abs/2505.18313)|null|当使用数十亿参数的大语言模型（LLM）进行训练时，加速器内存和网络限制已成为主要瓶颈。现有的低秩梯度估计器，如GaLoRE和FLORA，通过将权重梯度投影到秩为r的子空间来压缩梯度和优化器张量，从而使得LLM能够在消费级硬件上进行训练。然而，这些方法要么存在偏差，要么受到高估计器方差的影响。此外，基于前一次子空间中第一和第二矩估计的优化器状态，在投影更新时会发生错位，导致训练过程中的不稳定。我们提出了PLUMAGE：概率低秩无偏最小方差梯度估计器。PLUMAGE是现有低秩梯度估计器的直接替代品。它除了选择的秩r和更新间隔外，不引入新的超参数。此外，我们解决了优化器状态错位问题，以防止虚假的权重更新并增强训练稳定性。我们通过实证表明，PLUMAGE在模型预训练评估损失上平均缩小了全秩优化的差距33%，在GLUE基准的平均训练损失上缩小了28%，同时在计算和内存占用方面与GaLoRE相似。|
|**2025-05-22**|**URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training**|Dongyang Fan et.al.|[2505.16570](http://arxiv.org/abs/2505.16570)|null|大型语言模型（LLMs）通常在未利用上下文元数据（如来源、质量或主题）的广泛文本语料库上进行预训练，导致无上下文学习范式。虽然最近的研究表明，将URL信息等元数据作为上下文（即辅助输入，不用于损失计算）可以改善训练效率和下游性能，但它们对哪些类型的元数据真正有效以及何种条件下有效的理解有限。在这项工作中，我们进行了系统的评估，发现并非所有类型的元数据都同等贡献。只有URL上下文可以加快训练速度，而质量评分和主题/格式领域信息没有明显的益处。此外，当在推理时间使用较长的提示时，URL条件下的下游性能改进才会出现。此外，我们证明，具有上下文感知的预训练在无分类器指导方式下比无上下文预训练更能控制生成。尽管主题和格式元数据不会加速训练，但它们在引导输出方面是有效的，为生成提供了人类可解释的控制。|
|**2025-05-22**|**JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model**|Qihao Duan et.al.|[2505.17257](http://arxiv.org/abs/2505.17257)|null|大型语言模型（LLMs）已经彻底改变了自然语言处理，并且越来越多地应用于其他序列数据类型，包括遗传序列。然而，将LLMs应用于基因组学面临着重大挑战。捕捉复杂的基因组相互作用需要建模DNA序列中的长距离依赖关系，即使是在单个基因内，相互作用也常常跨越超过10,000个碱基对，这在传统的模型架构和训练范式下带来了巨大的计算负担。此外，标准的LLM训练方法对DNA来说并不理想：自回归训练虽然高效，但只支持单向理解。然而，DNA本质上是双向的，例如，双向启动子可以在两个方向上调节转录，并占人类基因表达的近11%。掩码语言模型（MLMs）允许双向理解，但效率低下，因为只有掩码的标记才在每个步骤中贡献损失。为了解决这些限制，我们引入了JanusDNA，这是第一个基于新颖的预训练范式构建的双向DNA基础模型，该范式结合了自回归建模的优化效率和掩码建模的双向理解。JanusDNA采用混合的Mamba、Attention和专家混合（MoE）架构，结合了Attention的长距离建模和Mamba的高效序列学习。MoE层通过稀疏激活进一步扩展模型容量，同时保持计算成本低。值得注意的是，JanusDNA在单个80GB GPU上以单核苷酸分辨率处理高达100万个碱基对。大量的实验和消融实验表明，JanusDNA在三个基因组表示基准上实现了新的SOTA结果，超过了具有250倍更多激活参数的模型。代码：https://github.com/Qihao-Duan/JanusDNA|
|**2025-05-21**|**Reverse Engineering Human Preferences with Reinforcement Learning**|Lisa Alazraki et.al.|[2505.15795](http://arxiv.org/abs/2505.15795)|null|大型语言模型（LLMs）的能力通常通过训练来预测人类偏好的其他LLMs进行评估。这个框架，被称为“LLM作为裁判”框架，具有高度可扩展性和相对较低的成本。然而，它也容易受到恶意利用，因为LLM的回答可以被调整以过度拟合裁判的偏好。先前的研究表明，候选-LLM生成的答案可以在事后编辑以最大化裁判-LLM分配给它们的分数。在本研究中，我们采用了一种不同的方法，利用裁判-LLMs提供的信号作为奖励，来对抗性地调整生成文本序言的模型，旨在提升下游性能。我们发现，与现有框架相比，这些模型与冻结的LLMs流水线连接后能够获得更高的LLM评估分数。关键的是，与直接干预模型响应的其他框架不同，我们的方法几乎无法被检测到。我们还证明，当候选-LLM和裁判-LLM被替换为训练过程中未使用的模型时，调整序言生成器的有效性仍然存在。这些发现对更可靠的LLM作为裁判评估设置的设计提出了重要问题。它们还表明，通过流水线LLMs以强化学习的方式优化上游序言，可以有效地逆向工程人类偏好——这种方法可能在未来对抗攻击之外的任务和领域找到应用。|
|**2025-05-21**|**Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications**|Dikshit Chauhan et.al.|[2505.15741](http://arxiv.org/abs/2505.15741)|null|将大型语言模型（LLMs）和进化计算（EC）相结合，代表了一条通过结合强大的自然语言理解能力与优化和搜索能力来推进人工智能的很有前景的途径。本文探讨了LLMs和EC的协同潜力，回顾了它们的交集、互补优势以及新兴应用。我们确定了关键机会，在这些机会中，EC可以增强LLMs的训练、微调、提示工程和架构搜索，而LLMs反过来又能帮助自动化EC的设计、分析和解释。本文探讨了EC和LLMs的协同整合，强调了它们在推进人工智能方面的双向贡献。首先，本文考察了EC技术如何通过优化关键组件（如提示工程、超参数调整和架构搜索）来增强LLMs，展示了进化方法如何自动化和优化这些过程。其次，调查研究了LLMs如何通过自动化元启发式设计、调整进化算法和生成自适应启发式规则来改进EC，从而提高效率和可扩展性。本文讨论了新兴的协同进化框架，展示了其在不同领域的应用，同时也承认了计算成本、可解释性和算法收敛等挑战。调查最后通过确定开放的研究问题，并倡导结合EC和LLMs优势的混合方法。|
|**2025-05-20**|**JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation**|Ghasem Pasandi et.al.|[2505.14978](http://arxiv.org/abs/2505.14978)|null|本文提出了一种名为JARVIS的新型多智能体框架，该框架利用大型语言模型（LLMs）和领域专业知识来生成针对专用电子设计自动化（EDA）任务的高质量脚本。通过结合使用合成数据训练的特定领域LLM、用于结构验证、规则执行、代码修复功能以及高级检索机制的定制编译器，我们的方法在性能上显著优于现有的特定领域模型。我们的框架解决了LLMs中的数据稀缺性和幻觉错误问题，展示了LLMs在特定工程领域的潜力。我们在多个基准测试中评估了我们的框架，并表明它在准确性和可靠性方面优于现有模型。我们的工作为LLMs在EDA领域的应用树立了新的先例，并为该领域的未来创新铺平了道路。|
|**2025-05-20**|**Subquadratic Algorithms and Hardness for Attention with Any Temperature**|Shreya Gupta et.al.|[2505.14840](http://arxiv.org/abs/2505.14840)|null|尽管Transformer架构广受欢迎，但用于计算Attention的标准算法在上下文长度 $n$方面存在二次时间复杂度。Alman和Song[NeurIPS 2023]表明，当头维度$d = \Theta(\log n)$时，在强指数时间假设（$\mathsf{SETH}$）下，如果输入的绝对值有小的入口，受限于$B = o(\sqrt{\log n})$，则可以实现次二次Attention。等价地，对于$d=\Theta(\log n)$，只有在高温度下应用softmax才能实现次二次Attention。这些算法的运行时间与$B$呈指数关系，因此它们不能导致在特定范围之外的$B$上甚至多项式时间的算法。这自然引出问题：在没有对温度的强假设的情况下，何时可以有效地计算Attention？是否存在与入口大小$B$成多项式对数比例的快速Attention算法？在本工作中，我们解决了这个问题，并确定了何时可以实现任意温度的快速Attention。首先，对于所有常数$d = O(1)$，我们给出了第一个对于大$B$的次二次$\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$时间Attention算法。我们的结果即使在头维度大的矩阵具有低秩的情况下也成立。在此范围内，我们还给出了类似的时间复杂度用于Attention梯度计算，因此对于完整的LLM训练过程也是如此。此外，我们表明对我们的算法的任何实质性改进都不太可能。特别是，我们表明即使当$d = 2^{\Theta(\log^* n)}$时，在$\mathsf{SETH}$下Attention也需要$n^{2 - o(1)}$时间。最后，在$d = \mathrm{poly}(n)$ 的范围内，我们表明在流行的细粒度复杂度假设下，标准算法是最优的。|
|**2025-05-20**|**Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models**|Tingchen Fu et.al.|[2505.14810](http://arxiv.org/abs/2505.14810)|**[link](https://github.com/tingchenfu/mathif)**|**遵循指令对于将大型语言模型（LLMs）与用户意图对齐至关重要。尽管最近以推理为导向的模型在复杂数学问题上的表现令人印象深刻，但它们遵循自然语言指令的能力仍被低估。在这项工作中，我们引入了MathIF，这是一个专门用于评估数学推理任务中指令遵循的基准。我们的实证分析揭示了一个持续的矛盾，即在扩大推理能力与保持可控性之间，因为推理更有效的模型往往难以遵守用户指令。我们发现，在提炼的长链式思维上调整或使用推理导向强化学习训练的模型，在指令遵循方面往往会出现退化，尤其是在生成长度增加时。此外，我们表明，即使简单的干预也可以部分恢复服从性，但这是以推理性能为代价的。这些发现突出了当前LLM训练范式中的基本矛盾，并激发了需要更多指令感知推理模型的需求。我们在https://github.com/TingchenFu/MathIF发布了代码和数据。**|
|**2025-05-19**|**Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)**|Artem Riabinin et.al.|[2505.13416](http://arxiv.org/abs/2505.13416)|**[link](https://github.com/artem-riabinin/Experiments-estimating-smoothness-for-NanoGPT-and-CNN)**|**近年来，深度学习优化领域的最新进展带来了基于线性最小化算子（LMO）框架的全新算法，例如 $\sf Muon$和$\sf Scion$。在$\sf Adam$主导超过十年之后，这些基于LMO的方法正逐渐成为可行的替代品，提供了诸如提高内存效率、更好的超参数迁移性以及最重要的是，在包括大型语言模型训练在内的任务中表现出更优越的实证性能。然而，它们在实际应用与当前理论理解之间仍然存在显著差距：以往的分析（1）忽略了这些优化器在实际中逐层应用LMO，以及（2）依赖于一个不切实际的平滑性假设，导致步长过小。为了解决这两个问题，我们提出了一种新的基于LMO的方法，称为$\sf Gluon$，它将先前理论分析的方法作为特例，并引入了一个新的精细化的广义平滑性模型，该模型捕捉了神经网络的逐层几何结构，与$\sf Muon$和$\sf Scion$ 的逐层实际实现相匹配，并导致了具有强大实际预测能力的收敛保证。与以往的结果不同，我们的理论步长与Pethick等人（2025年）报道的微调值非常接近。我们在NanoGPT和CNN上的实验证实，我们的假设在优化轨迹上成立，最终缩小了理论与实践之间的差距。**|
|**2025-05-19**|**On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding**|Haoyuan Wu et.al.|[2505.12723](http://arxiv.org/abs/2505.12723)|null|大型语言模型（LLMs）在代码生成任务中取得了显著性能。然而，在流行编程语言（如Python、C++）与其他语言之间，仍然存在显著的性能差距。为了解决这一能力差距，我们利用代码翻译任务来训练LLMs，从而促进跨多种编程语言的编码能力的迁移。此外，我们引入了一种新的强化学习（RL）框架OORL用于训练，该框架集成了在线策略和离线策略。在OORL中，在线策略RL在代码翻译过程中被应用，由来自单元测试的基于规则的奖励信号引导。补充这种粗粒度的基于规则的奖励，我们提出了组等效偏好优化（GEPO），一种新的偏好优化方法。具体来说，GEPO通过使用中间表示（IRs）组来训练LLMs。LLMs可以被引导区分与源代码等效的IRs和不等效的IRs，同时利用关于组内IR间相互等效性的信号。这个过程使LLMs能够捕捉代码功能的细微方面。通过采用OORL进行代码翻译任务的训练，LLMs提高了对代码功能的识别能力以及对不同编程语言中实现的代码之间关系的理解能力。大量实验表明，我们的OORL在LLMs训练中的代码翻译任务实现了在多个编程语言代码基准上的显著性能提升。|
|**2025-05-19**|**RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs**|Soumya Rani Samineni et.al.|[2505.13697](http://arxiv.org/abs/2505.13697)|null|基于强化学习的大语言模型（LLMs）的后续训练最近引起了广泛关注，尤其是在DeepSeek R1发布之后，它采用了GRPO进行微调。在围绕RL后训练提升推理能力的炒作日益升温之际，我们批判性地审视了这些方法的公式和假设。我们首先强调在将LLMs的训练建模为马尔可夫决策过程（MDP）时普遍采用的结构性假设，并展示这些假设如何导致一个退化了的MDP，而这个MDP几乎不需要RL/GRPO工具。这两个关键的结构性假设包括：（1）使MDP状态仅是动作与状态的拼接，在LLMs中动作成为上下文窗口，而动作成为标记；（2）将状态-动作轨迹的奖励均匀分配到整个轨迹上。通过全面分析，我们表明这些简化假设使该方法实际上等同于结果驱动的监督学习。我们在包括GSM8K和Countdown在内的基准测试中使用Qwen-2.5基模型进行的实验表明，迭代监督微调，结合正负样本，可以达到与基于GRPO的训练相当的性能。我们还将论证，结构性假设间接激励RL生成更长的中间标记序列——这反过来又进入了“RL生成更长思考轨迹”的叙事。虽然RL可能是一个非常有用的技术，用于提高LLMs的推理能力，但我们的分析表明，在建模基本MDP时做出的简单结构性假设使流行的LLM RL框架及其解释变得可疑。|
|**2025-05-18**|**A Survey of Attacks on Large Language Models**|Wenrui Xu et.al.|[2505.12567](http://arxiv.org/abs/2505.12567)|null|大型语言模型（LLMs）和基于LLM的智能体已在现实世界的广泛领域中得到了应用，包括医疗诊断、金融分析、客户支持、机器人和自动驾驶，扩展了它们理解和生成自然语言的能力。然而，LLM应用的广泛部署暴露了关键的安全和可靠性风险，如恶意滥用、隐私泄露和服务中断等，这些风险削弱了用户信任并损害了社会安全。本文对针对LLMs和基于LLM的智能体的对抗性攻击的细节进行了系统概述。这些攻击被组织为LLMs中的三个阶段：训练阶段攻击、推理阶段攻击和可用性与完整性攻击。对于每个阶段，我们分析了代表性攻击方法和最近引入的攻击方法的细节，以及相应的防御措施。我们希望我们的调查能提供一个良好的教程，并全面理解LLM的安全性，特别是针对LLM的攻击。我们希望引起对广泛部署的基于LLM的应用中固有的风险的注意，并强调对不断发展的威胁采取强大缓解策略的紧迫需求。|
|**2025-05-18**|**HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models**|Weixuan Wang et.al.|[2505.12300](http://arxiv.org/abs/2505.12300)|**[link](https://github.com/weixuan-wang123/hbo)**|**由于数据不平衡和异质性，在大规模语言模型（LLMs）上对多种数据集进行微调带来了挑战。现有方法通常在数据集之间（全局）解决这些问题，但忽略了单个数据集内部的不平衡和异质性，这限制了它们的有效性。我们引入了分层平衡优化（HBO），这是一种新颖的方法，可以使LLMs在微调过程中自主调整数据分配，无论是在数据集之间（全局）还是在每个单个数据集内部（局部）。HBO采用双层优化策略，包括两种类型的参与者：一个全局参与者，它平衡不同训练混合子集间的数据采样，以及多个局部参与者，它们根据难度级别优化每个子集中的数据使用。这些参与者由从LLM的训练状态中推导出的奖励函数引导，这些函数衡量学习进度和相对性能提升。我们在多语言和多任务设置中，对三个LLM基座在九个不同的任务上评估了HBO。结果表明，HBO始终优于现有基线，实现了显著的精度提升。我们深入的分析进一步表明，HBO的全局参与者和局部参与者有效地调整了微调过程中的数据使用。HBO为LLM微调中数据不平衡和异质性的挑战提供了一个全面解决方案，使跨多种数据集的训练更加有效。**|
|**2025-05-17**|**Multilingual Collaborative Defense for Large Language Models**|Hongliang Li et.al.|[2505.11835](http://arxiv.org/abs/2505.11835)|**[link](https://github.com/hliang-lee/mcd)**|**大型语言模型（LLMs）的鲁棒性和安全性已成为一个突出的研究领域。其中一个显著的安全漏洞是能够通过将有害查询翻译成稀有或代表性不足的语言来绕过LLMs的安全措施，这是一种简单而有效的方法，称为“越狱”这些模型。尽管人们对这一问题越来越关注，但针对多语言场景下LLMs安全防护的研究却有限，这突显了提高多语言安全性的紧迫需求。在这项工作中，我们研究了不同语言间各种攻击特征之间的相关性，并提出了多语言协作防御（MCD），这是一种新颖的学习方法，它可以自动优化连续的、软性安全提示，以促进LLMs的多语言安全防护。MCD方法具有三个优点：首先，它有效地提高了跨多种语言的安全防护性能；其次，MCD在保持强大的泛化能力的同时，最大限度地减少了误拒绝率；第三，MCD减轻了由LLM训练语料库不平衡造成的语言安全错位。为了评估MCD的有效性，我们手动构建了常见越狱基准（如MaliciousInstruct和AdvBench）的多语言版本，以评估各种安全防护方法。此外，我们将这些数据集引入代表性不足（零样本）的语言中，以验证MCD的语言迁移能力。结果表明，MCD在防御多语言越狱尝试方面优于现有方法，同时表现出强大的语言迁移能力。我们的代码可在https://github.com/HLiang-Lee/MCD上找到。**|
|**2025-05-16**|**MergeBench: A Benchmark for Merging Domain-Specialized LLMs**|Yifei He et.al.|[2505.10833](http://arxiv.org/abs/2505.10833)|**[link](https://github.com/uiuctml/mergebench)**|模型合并提供了一种可扩展的替代方案，通过参数算术结合专门微调的模型，实现了无需联合训练或访问所有任务数据的快速部署。尽管最近的方法显示出前景，但现有的评估在模型规模和任务多样性方面都有限，对它们在大型、领域特定的LLMs中的应用提出了疑问。为了应对这些挑战，我们引入了MergeBench，这是一个综合评估套件，旨在评估大规模模型合并。MergeBench基于最先进的开源语言模型，包括2B到9B规模的Llama和Gemma系列，涵盖了五个关键领域：指令遵循、数学、多语言理解、编码和安全。我们标准化了微调和评估协议，并评估了八种代表性的合并方法在多任务性能、遗忘和运行效率方面的表现。基于广泛的实验，我们提供了算法选择的实用指南，并分享了显示模型合并往往在更强的基模型上表现更好的见解，例如合并系数调整和稀疏化技术提高了知识保留。然而，仍然存在几个挑战，包括大型模型上的计算成本、与多任务模型相比的领域性能差距，以及模型合并在标准LLM训练流程中未充分探索的作用。我们希望MergeBench为未来研究提供基础，以推进对模型合并的理解和实践应用。我们在GitHub上开源了我们的代码：[https://github.com/uiuctml/MergeBench](https://github.com/uiuctml/MergeBench)。|
|**2025-05-16**|**REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning**|Pawin Taechoyotin et.al.|[2505.11718](http://arxiv.org/abs/2505.11718)|null|基于AI的同行评审系统往往比人工反馈产生更浅显和过度赞誉的建议。在这里，我们评估了一个使用多目标强化学习（REMOR）训练的推理LLM克服这些局限性的效果。首先，我们设计了一个多方面奖励函数，使其与人类对评审的评价相一致。这些方面与评审本身（例如，批评、创新）以及评审与稿件之间的关系（即，相关性）有关。首先，我们在PeerRT上使用LoRA对DeepSeek-R1-Distill-Qwen-7B进行监督微调，PeerRT是一个包含推理轨迹的高质量顶级AI会议评审的新数据集。然后，我们应用组相对策略优化（GRPO）来训练两个模型：REMOR-H（具有与人类对齐的奖励）和REMOR-U（具有统一奖励）。有趣的是，与人类对齐的奖励惩罚了通常与优秀评审相关联的方面，导致REMOR-U产生更实质性的反馈。我们的结果表明，REMOR-U和REMOR-H的奖励平均比人类评审高出两倍以上，非推理状态的艺术代理多模态AI评审系统和通用商业LLM基线。我们发现，尽管最佳AI和人类评审在质量上相当，但REMOR避免了低质量人类评审的长尾。我们讨论了推理如何实现这些改进，并发布了与人类对齐的同行评审奖励（HPRR）函数、同行评审推理增强轨迹（PeerRT）数据集和REMOR模型，我们相信这些可以帮助推动该领域的进步。|
|**2025-05-15**|**Neural Thermodynamic Laws for Large Language Model Training**|Ziming Liu et.al.|[2505.10559](http://arxiv.org/abs/2505.10559)|null|关于大型语言模型（LLMs）背后的规律，除了神经可扩展定律之外，了解甚少。我们提出了神经网络热力学定律（NTL）——一个提供对LLM训练动态新见解的新框架。在理论方面，我们证明了在河谷损失景观假设下，关键热力学量（例如温度、熵、比热容、热传导）和经典热力学原理（例如热力学三大定律和能量均分定理）自然会涌现。在实践方面，这种科学视角为设计学习率调度提供了直观的指导。|
|**2025-05-13**|**Strategy-Augmented Planning for Large Language Models via Opponent Exploitation**|Shuai Xu et.al.|[2505.08459](http://arxiv.org/abs/2505.08459)|**[link](https://github.com/hsushuai/SAP)**|在对抗领域中，高效地对对手进行建模和利用是一个长期存在的挑战。近年来，基于大量文本数据训练的大语言模型（LLMs）在一般任务中表现出卓越的性能，为对手建模引入了新的研究方向。一些研究主要关注直接使用LLMs根据包含对手描述的详细提示上下文生成决策，但这些方法仅限于LLMs拥有足够领域专业知识的情况下。为了解决这个问题，我们引入了一个两阶段策略增强规划（SAP）框架，通过利用一个关键组件——策略评估网络（SEN），显著增强了基于LLM的代理的对手利用能力。具体来说，在离线阶段，我们构建一个显式的策略空间，并随后收集策略-结果对数据来训练SEN网络。在在线阶段，SAP动态识别对手的策略，并通过对经过良好训练的SEN进行贪婪搜索最佳反应策略来利用它们，最终通过精心设计的提示将策略转化为一系列行动。实验结果表明，SAP表现出强大的泛化能力，不仅能够有效地应对之前遇到的对手策略，而且能够应对新颖的、未见的策略。在MicroRTS环境中，SAP比基线方法提高了85.35%的性能，并与最先进的（SOTA）基于规则的AI方法的竞争力相当。|
|**2025-05-13**|**Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training**|Yangyi Chen et.al.|[2505.08971](http://arxiv.org/abs/2505.08971)|**[link](https://github.com/yangyi-chen/prior)**|**在标准的大视觉-语言模型（LVLMs）预训练中，模型通常通过下一个标记预测（NTP）来最大化图像条件下的字幕联合概率；然而，由于只有一小部分字幕标记与视觉内容直接相关，这种简单的NTP无意识地使模型适应噪声并增加了幻觉的风险。我们提出了PRIOR，这是一种简单的视觉-语言预训练方法，通过在NTP损失中进行差异加权，优先考虑图像相关的标记，并借鉴了重要性抽样框架来解决这个问题。PRIOR引入了一个参考模型——一个仅使用字幕进行训练的大语言模型（LLM），以根据其LVLMs训练的概率对每个标记进行加权。直观地说，与视觉输入直接相关的标记在没有图像的情况下更难以预测，因此从仅使用文本的参考LLM中获得较低的概率。在训练过程中，我们根据重要性分数实现了一个特定于标记的重新加权项，以调整每个标记的损失。我们将PRIOR应用于两种不同的设置：带有视觉编码器的LVLMs和没有视觉编码器的LVLMs。与NTP相比，在多个视觉-语言基准测试中观察到19%和8%的平均相对改进。此外，PRIOR表现出优越的扩展特性，如显著更高的扩展系数所示，这表明随着计算和数据量的增加，与NTP相比，具有更大的性能提升潜力。**|
|**2025-05-12**|**Domain Regeneration: How well do LLMs match syntactic properties of text domains?**|Da Ju et.al.|[2505.07784](http://arxiv.org/abs/2505.07784)|null|近期大型语言模型性能的改进，很可能伴随着它们在近似其训练数据分布方面的提升。在本工作中，我们探讨了以下问题：LLM们忠实近似哪些文本领域的特性，以及它们做得如何？我们采用来自语料库语言学的观察方法，提示一个常用的开源LLM从两个通常包含在LLM训练数据中的、许可宽松的英文文本领域——维基百科和新闻文本——中重新生成文本。这种重新生成范式使我们能够调查LLM是否可以在一个相对语义控制的设置中忠实匹配原始的人类文本领域。我们研究了不同层次的句法抽象，从更简单的句子长度和文章可读性等属性，到更复杂和更高阶的属性，如依存标签分布、解析深度和解析复杂性。我们发现，与人类原始文本相比，大多数重新生成的分布显示出均值偏移、标准差降低和长尾减少的现象。|
|**2025-05-12**|**Large Language Models and Arabic Content: A Review**|Haneh Rhel et.al.|[2505.08004](http://arxiv.org/abs/2505.08004)|null|在过去三年里，大型语言模型（LLMs）的快速发展对人工智能（AI）的多个领域产生了深远的影响，尤其是在自然语言处理（NLP）领域，包括阿拉伯语在内的多种语言。尽管阿拉伯语被认为是阿拉伯世界27个国家中最广泛使用的语言之一，并在一些非阿拉伯国家也被用作第二语言，但阿拉伯语资源、数据集和工具仍然稀缺。由于阿拉伯语的复杂性，包括其丰富的形态学、复杂的结构和多样的书写标准等因素，阿拉伯语NLP任务面临着各种挑战。研究人员一直在积极应对这些挑战，证明了在多语言语料库上预训练的大型语言模型（LLMs）在各种阿拉伯语NLP任务中取得了显著的成功。本研究概述了使用大型语言模型（LLMs）进行阿拉伯语语言处理的方法，强调了早期预训练的阿拉伯语言模型在各种NLP应用中的表现及其处理多样化阿拉伯语内容和方言的能力。它还概述了如何通过微调和提示工程等技术来提升这些模型的表现。此外，该研究总结了常见的阿拉伯语基准和数据集，并展示了我们对LLMs采用持续上升趋势的观察。|
|**2025-05-11**|**RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models**|Hanzheng Dai et.al.|[2505.07089](http://arxiv.org/abs/2505.07089)|null|基于大型语言模型（LLMs）的自动化渗透测试（AutoPT）因其能够利用LLMs的内在知识自动化道德黑客过程并识别目标系统中的漏洞而受到关注。然而，现有的基于LLM的AutoPT框架在挑战性任务中往往表现不佳，原因有以下几点：LLM训练中使用的知识不平衡、规划过程中的短视规划和在命令生成过程中的幻觉。此外，渗透测试（PT）过程由于其试错性质，受到现有框架的限制，这些框架缺乏从先前失败操作中学习的机制，限制了PT策略的适应性改进。为了解决这些局限性，我们提出了一种由LLMs驱动的知识启发式自我反思PT框架，称为RefPentester，这是一个AutoPT框架，旨在协助人类操作员识别PT过程的当前阶段，选择该阶段的适当策略和技术，选择建议的行动，提供逐步的操作指导，并从先前失败的操作中学习。我们还把PT过程建模为一个七状态的阶段机器，以有效地整合所提出的框架。评估结果显示，RefPentester能够成功地在Hack The Box的Sau机器上揭示凭证，比基线GPT-4o模型的表现高出16.7%。在PT阶段之间，RefPentester在PT阶段转换上也显示出更高的成功率。|
|**2025-05-09**|**Understanding Stragglers in Large Model Training Using What-if Analysis**|Jinkun Lin et.al.|[2505.05713](http://arxiv.org/abs/2505.05713)|**[link](https://github.com/bytedance-seed/straggleranalysis)**|**大型语言模型（LLM）的训练是目前最耗资源的分布式计算之一，通常需要成千上万的GPU，并且需要在机器间进行频繁的同步。这种工作负载模式使其容易受到“慢速工作节点”（stragglers）的影响，训练可能会因少数慢速节点而停滞。在字节跳动，我们发现慢速工作节点并非总是由硬件故障引起，而是可能由多个复杂因素导致。本研究旨在通过对从字节跳动LLM训练集群收集的五个月追踪数据进行全面分析，来探讨LLM训练中的慢速工作节点问题。核心方法是“假设分析”，它模拟没有任何慢速工作节点的场景，并与实际情况进行对比。我们采用这种方法来研究以下问题：（1）慢速工作节点多久影响一次训练作业，以及它们对作业性能有何影响；（2）慢速工作节点是否表现出时间或空间模式；（3）慢速工作节点的潜在根本原因是什么？**|
|**2025-05-08**|**Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data**|Yudong Wang et.al.|[2505.05427](http://arxiv.org/abs/2505.05427)|null|数据质量在提升大型语言模型（LLMs）性能中已成为关键因素。模型驱动的数据过滤正逐渐成为获取高质量数据的主要方法。然而，它仍面临两个主要挑战：（1）缺乏高效的数据验证策略，使得对数据质量的及时反馈变得困难；（2）训练分类器的种子数据选择缺乏明确标准，高度依赖人工专业知识，引入了一定的主观性。为应对第一个挑战，我们引入了一种高效的验证策略，能够在最低的计算成本下快速评估数据对LLMs训练的影响。为解决第二个挑战，基于高质量种子数据有利于LLMs训练的假设，通过整合提出的验证策略，我们优化了正负样本的选择，并提出了一种高效的数据过滤流程。此流程不仅提高了过滤效率、分类器质量和鲁棒性，还显著降低了实验和推理成本。此外，为了高效过滤高质量数据，我们采用了一种基于fastText的轻量级分类器，并成功将该过滤流程应用于两个广泛使用的预训练语料库——FineWeb和中国FineWeb数据集，从而创建了高质量的Ultra-FineWeb数据集。Ultra-FineWeb包含约1000亿个英文标记和1200亿个中文标记。实证结果表明，在Ultra-FineWeb上训练的LLMs在多个基准任务上表现出显著的性能提升，验证了我们的流程在提高数据质量和训练效率方面的有效性。|
|**2025-05-08**|**QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation**|Mengze Hong et.al.|[2505.05225](http://arxiv.org/abs/2505.05225)|null|中国大型语言模型（LLMs）的快速发展突显了进行特定领域评估的必要性，以确保可靠的应用。然而，现有的基准测试在垂直领域覆盖不足，且对中国工作环境的洞察有限。利用资格考试作为人类专业知识评估的统一框架，我们引入了QualBench，这是第一个针对中国LLMs本地化评估的多领域中文问答基准。该数据集包含超过17,000个问题，涵盖六个垂直领域，数据选择基于24项中国资格考试，以紧密契合国家政策和工作标准。通过全面评估，Qwen2.5模型的表现优于更先进的GPT-4o，中国LLMs持续超越非中文模型，突显了在满足资格要求中本地化领域知识的重要性。最佳性能达到75.26%，揭示了模型能力在领域覆盖方面的当前差距。此外，我们展示了LLM与众包机制的协作失败，并提出了多领域RAG知识增强和垂直领域LLM训练的联邦学习机会。|
|**2025-05-07**|**OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models**|Xiaoyu Xu et.al.|[2505.04416](http://arxiv.org/abs/2505.04416)|null|大型语言模型（LLMs）在广泛语料库上进行训练时，可能会记住敏感、受版权保护或有害的内容。为了解决这个问题，我们提出了OBLIVIATE，这是一个强大的逆向学习框架，能够在保留模型效用的情况下移除目标数据。该框架遵循一个结构化的流程：提取目标标记、构建保留集以及使用包含三个组件（掩码、蒸馏和世界事实）的定制损失函数进行微调。通过使用低秩适配器（LoRA），它确保了效率，同时不牺牲逆向学习质量。我们在多个数据集上进行了实验，包括《哈利·波特》系列、WMDP和TOFU，并使用了一系列综合指标：遗忘质量（新文档级别的记忆分数）、模型效用和流畅度。结果表明，它在抵抗成员推理攻击、最小化对保留数据的影响以及在不同场景下保持鲁棒性方面都表现出有效性。|
|**2025-05-07**|**Exploring Influence Factors on LLM Suitability for No-Code Development of End User IoT Applications**|Minghe Wang et.al.|[2505.04710](http://arxiv.org/abs/2505.04710)|null|随着物联网应用的日益普及，终端用户对更加个性化和直观的功能提出了更高的要求。然而，目前定制物联网功能仍然需要一定的编程技能，这成为了一个主要障碍。为了解决这个问题，无代码开发平台被提出作为一种解决方案，以赋予非技术用户创建应用程序的能力。然而，这样的平台仍然需要一定程度的编程知识来构建流程步骤或定义事件-动作关系。LLM（大型语言模型）的出现可以通过实现基于自然语言的交互、自动化复杂任务和动态代码生成来进一步提升无代码平台。通过允许用户用自然语言描述他们的需求，LLM可以显著简化无代码开发。由于LLM在性能、架构、训练数据和目标用例方面存在差异，目前尚不清楚哪些模型最适合，以及决定这种适合性的影响因素。特别是，非技术用户通过无代码开发物联网应用对LLM的需求与例如为更开放的应用生成代码或支持专业开发者相比，将完全不同。在本文中，我们探讨了影响LLM适合于无代码开发物联网应用的因素。我们还考察了输入提示语言对生成应用程序的准确性和质量的影响，以及LLM训练数据的影响。通过使用一系列LLM进行综合实验，我们为优化LLM驱动的无代码平台提供了宝贵的见解，指导了适合LLM的选择及其有效应用。我们的发现有助于提高无代码物联网开发的易用性、效率和用户体验，最终使非专家用户更广泛地采用物联网技术。|
|**2025-05-06**|**A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case**|Haoxiang Luo et.al.|[2505.03196](http://arxiv.org/abs/2505.03196)|null|由于大型语言模型（LLMs）具有先进的推理能力，它们在通信和网络领域的各种任务中展现出强大的潜力。然而，由于不同的LLMs具有不同的模型结构，并且使用不同的语料库和方法进行训练，它们可能为同一网络问题提供不同的优化策略。此外，单个LLM训练数据的局限性，加上其托管设备的潜在恶意性，可能导致响应信心不足甚至存在偏见。为了解决这些挑战，我们提出了一种基于区块链的协作框架，将多个LLMs连接成一个可信赖的多LLM网络（MultiLLMN）。该架构能够实现针对复杂网络优化问题的最可靠和高质量响应的合作评估和选择。具体来说，我们首先回顾了相关研究，并强调了现有LLMs在协作和信任方面的局限性，强调了基于LLM系统的可靠性需求。然后，我们介绍了所提出的可信赖MultiLLMN框架的工作流程和设计。鉴于虚假基站（FBS）攻击在B5G和6G通信系统中的严重性以及通过传统建模技术解决此类威胁的困难，我们将FBS防御作为案例研究，以实证验证我们方法的有效性。最后，我们概述了该新兴领域的有希望的未来研究方向。|
|**2025-05-05**|**HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models**|Zheng Lin et.al.|[2505.02795](http://arxiv.org/abs/2505.02795)|null|近期，大型语言模型（LLMs）取得了显著的突破，颠覆了自然语言处理领域及其它领域。由于参数规模巨大，使用私有数据对LLMs进行微调以适应各种下游任务已成为主流。尽管联邦学习（FL）为在不共享原始数据的情况下微调LLMs提供了一种有希望的解决方案，但其庞大的计算成本阻碍了其普及。此外，在现实场景中，私有客户端设备通常拥有异构的计算资源，这进一步增加了LLMs微调的复杂性。为了应对这些挑战，我们提出了HSplitLoRA，这是一个基于分割学习（SL）和低秩自适应（LoRA）微调的异构参数高效微调（PEFT）框架，用于在异构客户端设备上高效微调LLMs。HSplitLoRA首先根据权重对LLMs训练的贡献识别重要权重。然后，它动态配置所选权重的LoRA适配器的分解秩，并根据客户端设备的计算预算确定模型分割点。最后，设计了一种无噪声的适配器聚合机制，以支持在引入噪声的情况下进行异构适配器聚合。大量实验表明，HSplitLoRA在训练精度和收敛速度方面优于最先进的基准。|
|**2025-05-04**|**Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data**|Zhong Guan et.al.|[2505.02130](http://arxiv.org/abs/2505.02130)|**[link](https://github.com/millioniron/llm_exploration)**|**注意力机制对于大型语言模型（LLMs）的成功至关重要，推动了多个领域的显著进步。然而，对于需要强调拓扑连接的图结构数据，与固定链接上的消息传递机制（如图神经网络GNN所使用的）相比，它们显得不足。这引发了一个问题：“在自然语言设置中，注意力机制是否对图失败了？”受这些观察的启发，我们从注意力机制的角度进行了实证研究，以探索LLMs如何处理图结构数据。目标是更深入地了解LLMs在图结构上的注意力行为。我们发现了一些关于LLMs如何应用注意力到图结构数据上的独特现象，并分析了这些发现，以改进LLMs对这类数据的建模。我们研究的主要发现是：1）虽然LLMs可以识别图数据并捕捉文本节点交互，但由于固有的架构限制，它们在建模图结构中的节点间关系方面存在困难。2）LLMs在图节点上的注意力分布不符合理想的结构模式，表明其未能适应图拓扑的细微差别。3）无论是全连接注意力还是固定连接都不是最优的；每种方法在其应用场景中都有特定的局限性。相反，中间状态注意力窗口可以提高LLMs的训练性能，并在推理期间无缝过渡到全连接窗口。源代码：[LLM4Exploration](https://github.com/millioniron/LLM_exploration)**|
|**2025-05-03**|**Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients**|Yezhen Wang et.al.|[2505.01744](http://arxiv.org/abs/2505.01744)|null|基于低秩适配器（LoRA）的成功，低秩梯度投影（LoRP）已成为一种内存高效的微调解决方案。然而，现有的LoRP方法通常将梯度矩阵的每一行视为默认的投影单元，导致投影粒度的作用未得到充分探索。在这项工作中，我们提出了一种新颖的框架VLoRP，通过引入一个额外的自由度来控制内存效率与性能之间的权衡，从而扩展了低秩梯度投影，这个自由度超越了秩超参数。通过这个框架，我们系统地探讨了投影粒度的影响，表明即使在固定的内存预算下，更细粒度的投影也能提高稳定性和效率。关于VLoRP的优化，我们提出了ProjFactor，一个自适应的内存高效优化器，它显著降低了内存需求，同时确保了有竞争力的性能，即使在梯度累积的情况下也是如此。此外，我们对VLoRP进行了理论分析，证明了其在SGD和ProjFactor下的优化轨迹的下降和收敛性。我们进行了大量的实验来验证我们的发现，涵盖了常识推理、MMLU和GSM8K等任务。|
|**2025-05-02**|**Don't be lazy: CompleteP enables compute-efficient deep transformers**|Nolan Dey et.al.|[2505.01618](http://arxiv.org/abs/2505.01618)|**[link](https://github.com/eleutherai/nanogpt-mup)**|**我们研究了在采用不同参数化方式时LLM训练的计算效率，即随着模型大小变化调整模型和优化器超参数（HP）的规则。一些参数化方法无法在不同模型深度变化时传递最佳基础HP（如学习率），这要求实践者要么在扩展时重新调整这些HP（成本高昂），要么在重新调整不可行时接受次优训练。即使在它们实现了HP传递的情况下，我们也发展了理论来表明参数化可能仍然存在于懒学习状态中，即层仅学习其线性化附近的特征，这阻碍了深度和非线性的有效利用。最后，我们识别并采用了一种独特的参数化方法，我们称之为CompleteP，它在所有层中实现了深度方向上的HP传递和非懒学习。CompleteP使得更广泛范围的模式宽度/深度比率保持计算效率，解锁了更适合不同硬件设置和操作环境的形状。此外，CompleteP在计算效率上比现有最佳方法提高了12-34%。**|
|**2025-05-01**|**LLMPrism: Black-box Performance Diagnosis for Production LLM Training Platforms**|Zhihan Jiang et.al.|[2505.00342](http://arxiv.org/abs/2505.00342)|null|大型语言模型（LLMs）在众多领域带来了革命性的变化，使得LLMs的训练对于现代企业来说至关重要。为了满足这一需求，已经建立了多租户的大型LLMs训练平台来提供LLMs训练服务。然而，由于LLMs训练过程的复杂性和同步性，性能问题经常发生，可能导致大量资源浪费。平台提供商的有限可见性阻碍了现有的分析方法，并对LLMs训练作业的性能监控和诊断提出了挑战。本文首次提出利用底层网络流量数据，根据LLMs训练过程中的独特特征来重建作业的训练时间线。我们设计了LLMPrism，这是首个针对LLMs训练平台的黑盒性能诊断系统。通过逐步识别LLMs训练作业，确定它们的并行策略，并重建训练时间线，LLMPrism实现了对LLMs训练系统的非侵入性、轻量级和连续监控。利用这种监控能力，它还可以有效地诊断潜在的性能问题。自2024年10月起，LLMPrism已部署在我们的大规模生产平台Platform-X上，评估和部署经验表明，LLMPrism可以实现误差在0.3%以内的准确时间线重建，并有效地诊断各种性能问题。|
|**2025-05-01**|**EnronQA: Towards Personalized RAG over Private Documents**|Michael J. Ryan et.al.|[2505.00263](http://arxiv.org/abs/2505.00263)|null|检索增强生成（RAG）由于其能够在推理时引入局部上下文，而不需要微调的成本或数据泄露风险，已成为将知识密集型上下文引入大型语言模型（LLM）中最受欢迎的方法之一。将私有信息与LLM训练的明确分离使得RAG成为许多企业LLM工作负载的基础，因为它允许公司利用客户的私有文档来增强LLM的理解。尽管RAG在企业的私有文档部署中很受欢迎，但当前用于验证和优化RAG管道的基准测试，其语料库来源于公共数据，如维基百科或通用网页，并且提供的个人上下文很少甚至没有。为了使更多个人和私有的RAG更具能力，我们发布了EnronQA基准测试，这是一个包含103,638封电子邮件，跨越150个不同用户邮箱的528,304个问答对的数据集。EnronQA使得对私有数据的RAG管道的基准测试更加完善，并允许在真实数据上引入个性化检索设置的实验。最后，我们使用EnronQA来探讨在推理私有文档时记忆和检索之间的权衡。|
|**2025-05-01**|**Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation**|Vaidehi Patil et.al.|[2505.01456](http://arxiv.org/abs/2505.01456)|**[link](https://github.com/vaidehi99/unlok-vqa)**|**大型数据集上训练的LLMs可能无意中获取到敏感信息，如个人细节和潜在有害内容。这种风险在多模态LLMs中进一步加剧，因为它们会整合来自多个模态（图像和文本）的信息。攻击者可以通过多模态提示利用这种知识来提取敏感细节。评估MLLMs能否有效地忘记这种信息（目标性忘记学习）需要创建高质量、注释良好的图像-文本对。虽然先前关于忘记学习的研究主要集中在文本上，但多模态忘记学习仍然研究不足。为了填补这一空白，我们首先介绍了一个多模态忘记学习基准，UnLOK-VQA（外部知识忘记的视觉问答），以及一个攻击-防御框架来评估从MLLMs中删除特定多模态知识的方法。我们通过一个自动化流程扩展了一个视觉问答数据集，该流程生成不同邻近度的样本以测试泛化性和特异性，随后进行人工筛选以保持高质量。然后，我们评估了六个防御目标针对七个攻击（四个白盒，三个黑盒），包括一个利用隐藏状态可解释性的新型白盒方法。我们的结果表明，多模态攻击优于仅文本或图像的攻击，并且最有效的防御是从内部模型状态中删除答案信息。此外，较大的模型表现出更强的后编辑鲁棒性，这表明规模可以增强安全性。UnLOK-VQA为在MLLMs中推进忘记学习提供了一个严格的基准。**|
|**2025-04-28**|**m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training**|Meng Xiao et.al.|[2504.19565](http://arxiv.org/abs/2504.19565)|null|生物医学研究中大型语言模型（LLMs）的快速进步突显了现有开源标注科学语料库的局限性，这些语料库在数量和质量上通常不足。为了应对生物医学知识复杂层级带来的挑战，我们提出了一种针对生物医学领域LLM训练的知识驱动、多智能体框架，用于科学语料库蒸馏。我们方法的核心是一个协作多智能体架构，其中每个专门智能体都由医学主题词表（MeSH）层级指导，共同自主地从大量科学文献中提取、综合和自我评估高质量文本数据。这些智能体共同生成和细化领域特定的问答对，确保全面覆盖并与生物医学本体保持一致，同时最大限度地减少人工参与。大量的实验结果表明，在多智能体蒸馏数据集上训练的语言模型在生物医学问答任务中实现了显著的改进，超越了强大的生命科学LLM基线和先进的专有模型。值得注意的是，我们的AI-Ready数据集使Llama3-70B在MedPrompt和Med-PaLM-2的辅助下超越了GPT-4，尽管其规模更大。详细的消融研究和案例分析进一步验证了框架中每个智能体的有效性和协同作用，突显了多智能体协作在生物医学LLM训练中的潜力。|
|**2025-04-25**|**The Big Send-off: High Performance Collectives on GPU-based Supercomputers**|Siddharth Singh et.al.|[2504.18658](http://arxiv.org/abs/2504.18658)|null|我们评估了在基于GPU的超计算机上用于大规模语言模型（LLM）训练的集体通信的当前状态。现有的库如RCCL和Cray-MPICH在Frontier等系统上存在关键限制——Cray-MPICH未能充分利用网络和计算资源，而RCCL则存在严重的可扩展性问题。为了解决这些挑战，我们引入了PCCL，这是一个通信库，它针对分布式深度学习工作负载对全聚合和减少散射操作进行了高度优化的实现。PCCL旨在最大限度地利用所有可用的网络和计算资源，并能够高效地扩展到数千个GPU。它实现了显著的性能提升，在Frontier的2048个GCD上进行全聚合时，相较于RCCL提升了6-33倍，相较于Cray-MPICH提升了28-70倍。这些收益直接转化为端到端性能：在大型GPT-3风格训练中，PCCL相较于RCCL为7B和13B参数模型分别提供了高达60%和40%的速度提升。|
|**2025-04-24**|**Cross-region Model Training with Communication-Computation Overlapping and Delay Compensation**|Ying Zhu et.al.|[2504.17672](http://arxiv.org/abs/2504.17672)|null|训练大型语言模型（LLMs）需要庞大的计算资源，通常需要地理分布式的数据中心聚合（即跨区域训练）。然而，广域网络中的高通信延迟严重降低了传统分布式训练的效率。虽然像DiLoCo这样的方法可以减少通信频率，但它们却遭受了阻塞同步的问题。Streaming DiLoCo通过通信-计算重叠来缓解这一问题，但延迟的全局更新和部分同步引入了更新过时和模型不一致的问题。这些因素阻碍了收敛，尤其是在需要强烈的重叠来掩盖高延迟时。我们提出了CoCoDC，一个具有通信-计算重叠和延迟补偿的全新分布式训练框架，以明确解决这些挑战。在CoCoDC框架中，我们特别开发了一种基于泰勒展开的延迟补偿策略，以有效减轻过时问题，并设计了一种自适应传输策略，该策略动态调度模型片段同步，以优化带宽使用并加速收敛。大量实验表明，CoCoDC在最终准确率和训练速度方面均优于DiLoCo和Streaming DiLoCo。具体来说，与Streaming DiLoCo相比，CoCoDC将达到相当困惑度所需的训练步数减少了高达21.0%。我们的工作为可扩展和高效的跨区域LLM训练提供了有效解决方案。|
|**2025-04-22**|**A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment**|Kun Wang et.al.|[2504.15585](http://arxiv.org/abs/2504.15585)|null|大型语言模型（LLMs）的显著成功为学术界和工业界实现通用人工智能开辟了一条有前景的道路，这得益于它们在各种应用中的前所未有的表现。随着LLMs在研究和商业领域的日益突出，它们的安全和安全性问题已成为一个日益关注的问题，不仅对研究人员和企业，而且对每个国家都是如此。目前，现有的关于LLMs安全性的调查主要关注LLMs生命周期的特定阶段，例如部署阶段或微调阶段，缺乏对LLMs整个“生命链”的全面理解。为了解决这一差距，本文首次提出了“全栈”安全的概念，以系统地考虑LLMs训练、部署以及最终商业化的整个过程中的安全问题。与现成的LLMs安全性调查相比，我们的工作显示出几个独特的优势：（I）全面视角。我们将完整的LLMs生命周期定义为包括数据准备、预训练、后训练、部署和最终商业化。据我们所知，这代表了第一个涵盖LLMs整个生命周期的安全性调查。（II）广泛的文献支持。我们的研究基于对800多篇论文的详尽回顾，确保了对安全问题在更全面理解下的全面覆盖和系统组织。（III）独特见解。通过系统的文献分析，我们为每一章开发了可靠的路线图和视角。我们的工作确定了有希望的研究方向，包括数据生成中的安全性、对齐技术、模型编辑以及基于LLMs的智能体系统。这些见解为追求该领域未来工作的研究人员提供了宝贵的指导。|
|**2025-04-21**|**Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs**|Marina Sakharova et.al.|[2504.15210](http://arxiv.org/abs/2504.15210)|null|代码生成大型语言模型（LLMs）已成为现代软件开发中不可或缺的工具，提高了生产效率并加速了开发进程。本文旨在研究使用强化学习和直接偏好优化对代码生成LLMs进行微调的方法，以进一步提高其性能。为此，我们借助符号执行技术增强了奖励模型的训练数据，确保了更全面和客观的数据。通过符号执行，我们创建了一个能够更好地捕捉代码评估细微差异的定制数据集。在该数据集上微调的奖励模型在评估生成代码质量方面相对于基线CodeRL有显著提升。借助奖励模型反馈进行训练的代码生成LLMs，其结果与CodeRL基准相当。|
|**2025-04-21**|**Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL**|Simone Papicchio et.al.|[2504.15077](http://arxiv.org/abs/2504.15077)|null|大型语言模型（LLMs）在将关于关系型数据库的自然语言问题转化为SQL查询方面表现出令人印象深刻的能力。尽管最近有所改进，但小型的LLMs在零样本学习（ZSL）设置下处理涉及多个表和复杂SQL模式的问题时仍然存在困难。监督微调（SFT）部分弥补了预训练模型中的知识缺陷，但在处理涉及多跳推理的查询时却力不从心。为了弥合这一差距，已经提出了不同的LLM训练策略来增强推理能力，这些策略从在ZSL中利用思考过程，包括在SFT中加入推理痕迹，到采用强化学习（RL）策略。然而，推理对Text2SQL性能的影响仍未得到充分探索。本文旨在研究LLMs的推理能力对其在四个基准数据集上Text2SQL性能影响的程度。为此，它考虑了以下LLM设置：（1）ZSL，包括是否包含通用推理；（2）SFT，包括是否包含特定于任务的推理痕迹；（3）RL，探索使用不同的奖励函数，包括已建立的执行精度（EX）和混合使用细粒度奖励函数，这些奖励函数还考虑了部分正确答案的精确度、召回率和基数；（4）SFT+RL，即结合SFT和RL的两个阶段方法。结果显示，在ZSL下的通用推理在解决复杂的Text2SQL案例时证明是无效的。小型LLMs比大型LLMs从具有推理的SFT中获益更多。RL对所有测试模型和数据集总体上都是有益的。使用细粒度度量指标证明是最有效的RL策略。得益于RL和新的text2SQL奖励，7B Qwen-Coder-2.5模型在Bird数据集上的表现与400多亿参数的模型（包括gpt-4o）相当。|
|**2025-04-20**|**AI with Emotions: Exploring Emotional Expressions in Large Language Models**|Shin-nosuke Ishikawa et.al.|[2504.14706](http://arxiv.org/abs/2504.14706)|null|大型语言模型（LLMs）在各种任务中达到人类水平的表现，引发了人们对人工智能（AI）未来可能拥有情感的期待。为了探索当前LLMs在输出中表达情感的能力，我们使用了几种LLMs（OpenAI GPT、Google Gemini、Meta Llama3和Cohere Command R+）来扮演具有指定情感状态的代理，回答问题。我们使用Russell的情感环模型来定义情感状态，这是一个成熟的框架，它通过睡眼-激活（唤醒）和愉悦-不愉悦（效价）轴来表征情感。我们选择这个模型是因为它的简单性，它利用了两个连续参数，这在使用涉及情感状态连续变化的实际应用中提供了更好的可控性。生成的回答使用了一个独立于LLMs、基于GoEmotions数据集训练的情感分析模型进行评估。评估显示，生成的回答的情感状态与指定的一致，证明了LLMs表达情感的能力。这表明基于LLMs的AI代理模拟情感的可能性，为基于情感交互的广泛应用打开了大门，例如可以提供带有个人风格的建议或意见的顾问或顾问。|
|**2025-04-20**|**SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training**|Zhouyang Li et.al.|[2504.14519](http://arxiv.org/abs/2504.14519)|null|管道并行（PP）作为训练大型语言模型（LLM）的关键技术，因其能够以相对较低的通信开销减轻模型状态的内存压力而发挥作用。然而，在长上下文场景中，现有的管道并行方法无法解决大量的激活内存压力，主要是因为多微批次的激活累积导致的峰值内存消耗。此外，这些方法不可避免地引入了相当多的管道气泡，进一步阻碍了效率。为了应对这些挑战，我们提出了一种名为SlimPipe的新颖方法，这是一种细粒度管道并行方法，它采用了统一的序列切片技术以及一前一后（1F1B）调度。该方法将多个微批次累积的激活减少到仅一个，该激活被分割成几个切片。虽然切片被均匀分区，但由于因果注意力，各个切片的计算成本并不相等。我们开发了一种复杂的工作负载重新分配技术来解决这个问题。SlimPipe实现了（1）几乎零内存开销和（2）最小化管道气泡。通过针对各种模型架构、上下文窗口大小和SlimPipe特定配置的全面测试，证明了SlimPipe的有效性。例如，在Llama 70B模型上，与最先进的方法相比，SlimPipe显著提高了模型浮点运算率（MFU）至高达 $1.57\times$ （对于上下文长度为512K）。更重要的是，对于上下文长度为2048K的情况，SlimPipe在256 NVIDIA Hopper 80GB GPU上保持了超过45%的利用率，而其他方法要么由于性能显著下降，要么由于内存限制而完全失败。|
|**2025-04-17**|**GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs**|Kun-Woo Kim et.al.|[2504.12681](http://arxiv.org/abs/2504.12681)|null|大型语言模型（LLMs）在广泛数据集上训练时，往往会学习到敏感信息，这在“被遗忘权”等原则下引发了重大的社会和法律问题。从零开始重新训练整个模型以移除不希望的信息既成本高昂又不切实际。此外，现有的单一领域反学习方法无法解决多领域场景，在这些场景中，知识在隐私和版权等不同领域交织，形成了重叠的表示，导致过度知识移除或性能下降。为了解决这些问题，我们提出了GRAIL（基于梯度的自适应反学习），一个新颖的多领域反学习框架。GRAIL利用来自多个领域的梯度信息来精确地区分反学习范围和保留范围，并应用自适应参数化定位策略，以选择性地移除目标知识，同时保留每个领域的关键参数。在反学习基准实验中，GRAIL实现了与现有方法相当的反学习成功率，同时与之前的最先进方法相比，展示了高达17%的知识保留成功率。我们的发现为有效管理和规范大规模预训练语言模型中的敏感信息建立了一个新的范式。|
|**2025-04-16**|**HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks**|Stefan Abi-Karam et.al.|[2504.12268](http://arxiv.org/abs/2504.12268)|**[link](https://github.com/stefanpie/hls-eval)**|随着大型语言模型（LLM）的训练和推理速度的快速提升，它们在学术界和工业界半导体设计中的应用越来越广泛。虽然之前的大部分工作都是在硬件描述语言（HDL）任务上评估LLM，尤其是Verilog，但设计师越来越多地使用高级综合（HLS）来构建特定领域的加速器和复杂的硬件系统。然而，针对HLS设计任务的全面评估基准和工具仍然很少。为了解决这个问题，我们引入了HLS-Eval，这是第一个针对LLM驱动的HLS设计的完整基准和评估框架。HLS-Eval针对两个核心任务：(1)从自然语言描述中生成HLS代码，以及(2)执行针对HLS的特定代码编辑以优化性能和硬件效率。该基准包括从标准HLS基准和新型来源中提取的94个独特设计。每个案例都通过半自动流程准备，该流程生成自然语言描述和对应的测试平台，用于C模拟和综合验证，确保每个任务都“适用于LLM”。除了基准之外，HLS-Eval还提供了一个模块化的Python框架，用于自动化、并行评估本地和托管LLM。它包括并行评估引擎、直接HLS工具集成以及支持不同LLM交互范式的抽象，从而能够快速原型设计新的基准、任务和LLM方法。我们通过在Vitis HLS上对开源LLM的基线评估来展示HLS-Eval，测量了四个关键指标——可解析性、可编译性、可运行性和可综合性——反映了迭代HLS设计周期。我们还报告了pass@k指标，为LLM-for-hardware的更广泛社区建立了明确的基线和可重用的基础设施。所有基准、框架代码和结果都已开源，可在https://github.com/stefanpie/hls-eval上找到。|
|**2025-04-15**|**Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR**|Yulong Zhang et.al.|[2504.11101](http://arxiv.org/abs/2504.11101)|null|光学字符识别（OCR）任务对于评估视觉-语言模型（VLMs）和提供高质量的数据源用于大型语言模型（LLM）训练数据至关重要。尽管最先进的VLMs在平均OCR准确性方面有所提高，但它们仍然在样本级质量退化问题上挣扎，并且缺乏对低质量输出的可靠自动检测。我们引入了共识熵（CE），这是一种无需训练的推理后方法，通过聚合多个VLM的输出来量化OCR的不确定性。我们的方法利用了一个关键洞察：正确的VLM OCR预测在输出空间中收敛，而错误则发散。我们开发了一个轻量级的多模型框架，能够有效地识别问题样本，选择最佳输出并结合模型的优势。在多个OCR基准和VLM上的实验表明，CE在相同成本下优于VLM作为裁判的方法和单模型基线，并在多个指标上实现了最先进的成果。例如，我们的解决方案显示了以下成果：在质量验证中比VLM作为裁判的方法实现了15.2%更高的F1分数，在数学计算任务上实现了6.0%的准确度提升，并且只需要对7.3%的输入进行重新措辞，同时保持整体性能。值得注意的是，整个过程既不需要训练也不需要监督，同时保持即插即用的功能。|
|**2025-04-14**|**Transferable text data distillation by trajectory matching**|Rong Yao et.al.|[2504.09818](http://arxiv.org/abs/2504.09818)|null|在大型语言模型（LLM）领域，随着大型模型规模的增加，也带来了更高的训练成本。迫切需要最小化LLM训练中的数据量。与数据选择方法相比，数据蒸馏方法旨在合成少量数据样本以实现完整数据集的训练效果，并具有更好的灵活性。尽管数据蒸馏在计算机视觉领域取得了成功，但文本数据的离散性至今阻碍了其在自然语言处理（NLP）领域的探索。在这项工作中，我们提出了一种方法，该方法涉及基于轨迹匹配学习伪提示数据，并找到其最近邻ID以实现跨架构迁移。在蒸馏过程中，我们引入正则化损失来提高我们蒸馏数据的鲁棒性。据我们所知，这是第一个适用于文本生成任务（如指令微调）的数据蒸馏工作。在包括ARC-Easy和MMLU指令微调数据集在内的两个基准上的评估表明，我们的蒸馏方法优于SOTA数据选择方法LESS。此外，我们的方法在LLM结构（即OPT到Llama）上展示了良好的迁移性。|
|**2025-04-13**|**DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training**|Zhenting Wang et.al.|[2504.09710](http://arxiv.org/abs/2504.09710)|**[link](https://github.com/zhentingwang/dump)**|**最近基于强化学习（RL）的后期训练在大型语言模型（LLMs）方面取得了显著进展，尤其是在增强其推理能力以处理复杂任务方面。然而，大多数现有方法将训练数据视为一个统一的整体，忽略了现代LLM训练通常涉及来自不同分布的数据混合——这些数据在来源和难度上都有所不同。这种异质性引入了一个关键挑战：如何自适应地跨分布安排训练以优化学习效率。在本文中，我们提出了一种基于分布级可学习性概念的原理性课程学习框架。我们的核心洞察是，策略优势的大小反映了模型在给定分布上进一步训练可以获得多少益处。基于此，我们提出了一种基于RL的LLM后期训练的分布级课程学习框架，该框架利用了上置信界（UCB）原则来动态调整不同分布的采样概率。这种方法优先考虑具有高平均优势（利用）或低样本计数（探索）的分布，从而产生一个自适应且理论上有根据的训练计划。我们以GRPO作为底层RL算法实例化了我们的课程学习框架，并在具有多个难度和来源的逻辑推理数据集上展示了其有效性。我们的实验表明，我们的框架显著提高了收敛速度和最终性能，突出了在LLM后期训练中分布感知课程策略的价值。代码：https://github.com/ZhentingWang/DUMP。**|
|**2025-04-12**|**Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training**|Mingyu Liang et.al.|[2504.09307](http://arxiv.org/abs/2504.09307)|null|在分布式环境中训练大型语言模型（LLM）面临着模型执行、部署系统复杂性和大量可配置策略空间的重大挑战。尽管存在各种优化技术，但在实践中实现高效率仍然困难。准确的性能模型对于指导优化工作和系统级研究至关重要。我们提出了Lumos，这是一个针对大规模LLM训练的跟踪驱动的性能建模和估计工具包，旨在准确捕捉和预测现代LLM的执行行为。我们使用各种GPT-3变体在具有多达512个NVIDIA H100 GPU的生产级机器学习集群上评估了Lumos，结果表明，它可以在不同模型和配置中，以平均误差仅为3.3%的准确度回放执行时间，并提供其他运行时细节。此外，我们还验证了其从现有跟踪中估计新配置性能的能力，从而促进了模型和部署配置的高效探索。|
|**2025-04-12**|**Privacy Preservation in Gen AI Applications**|Swetha S et.al.|[2504.09095](http://arxiv.org/abs/2504.09095)|null|机器理解并产生与人类相似语言的能力，得益于自然语言处理（NLP）的快速发展，特别是由生成式人工智能（AI）和大型语言模型（LLMs）推动的快速进步，已经彻底改变了客户服务、医疗保健和金融等领域。然而，由于在大型数据集上训练的LLMs可能会无意中吸收并泄露用户交互中的个人可识别信息（PII），这些能力也引发了严重的隐私担忧。深度神经网络的复杂性使得难以追踪或阻止无意中存储和泄露私人信息，这引发了关于AI驱动数据隐私和安全的严重担忧。本研究通过检测生成式AI的弱点，如数据提取、模型反演和成员推理攻击，来应对这些问题。然后开发了一种隐私保护型生成式AI应用，该应用能够抵御这些攻击，通过在处理LLMs之前识别、修改或删除PII，确保隐私而不牺牲功能。为了确定像微软Azure、谷歌云和AWS这样的云平台如何为保护AI应用提供隐私工具，本研究还考察了这些技术。最终，本研究为生成式AI系统提供了一个基本的隐私范式，重点关注数据安全和道德AI实施，并为更安全、更有责任心的使用这些工具打开了大门。|
|**2025-04-10**|**Investigating Vision-Language Model for Point Cloud-based Vehicle Classification**|Yiqiao Li et.al.|[2504.08154](http://arxiv.org/abs/2504.08154)|null|重型卡车由于其体积庞大、操控性有限，与乘用车相比，存在重大的安全挑战。深入理解卡车特性对于提升合作自动驾驶的安全视角至关重要。传统的基于LiDAR的卡车分类方法依赖于大量的手动标注，这使得它们劳动密集且成本高昂。在大量数据集上训练的大语言模型（LLMs）的快速发展为利用它们的少样本学习能力进行卡车分类提供了机会。然而，现有的视觉-语言模型（VLMs）主要在图像数据集上训练，这使得直接处理点云数据变得具有挑战性。本研究引入了一种新的框架，该框架将路边LiDAR点云数据与VLMs相结合，以促进高效且准确的卡车分类，从而支持合作和安全的驾驶环境。本研究引入了三个关键创新：(1)利用现实世界的LiDAR数据集进行模型开发；(2)设计一个预处理管道来适应VLM输入的点云数据，包括点云配准以实现密集的3D渲染和数学形态学技术以增强特征表示；(3)利用上下文学习与少样本提示，以最小化标注训练数据来实现车辆分类。实验结果表明，这种方法具有令人鼓舞的性能，并展示了其减少标注努力同时提高分类准确性的潜力。|
|**2025-04-08**|**Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for Scaled-up LLM Training**|Daiyaan Arfeen et.al.|[2504.06095](http://arxiv.org/abs/2504.06095)|null|通过数据并行（DP）和模型并行（MP）执行的混合，LLM的训练扩展到了10K个GPU。实现效率的关键在于在紧密耦合的GPU子集内进行张量并行（TP；一种MP形式）执行，这些子集被称为扩展域，扩展域越大，性能越好。随着新型数据中心架构的出现，能够在一个扩展域内紧密耦合的GPU数量越来越多，例如从8个GPU扩展到通过NVLink连接的72个GPU。不幸的是，较大的扩展域增加了故障的破坏范围，单个GPU的故障可能会影响整个扩展域的张量并行执行，这可能会大幅降低LLM训练的整体吞吐量。当只有0.1%的GPU处于故障状态时，高TP度的工作可以体验到近10%的LLM训练吞吐量下降。我们提出了非均匀张量并行（NTP）来减轻GPU故障的放大影响。在NTP中，经历GPU故障的DP副本以降低的TP度运行，贡献的吞吐量等于仍然功能正常的GPU的百分比。我们还提出了一种具有改进的电气和热性能的机架设计，以维持经历过故障的扩展域的电力提升；结合NTP，这可以使TP度降低的DP副本（即具有故障GPU的副本）跟上其他副本，从而实现大规模LLM训练的近零吞吐量损失。|
|**2025-04-08**|**On the Impact of Language Nuances on Sentiment Analysis with Large Language Models: Paraphrasing, Sarcasm, and Emojis**|Naman Bhargava et.al.|[2504.05603](http://arxiv.org/abs/2504.05603)|null|大型语言模型（LLMs）在各种任务中，包括情感分析任务中，展现了令人印象深刻的性能。然而，数据质量——尤其是来源于社交媒体的数据——会显著影响其准确性。这项研究探讨了文本细微差别，包括表情符号和讽刺，如何影响情感分析，特别关注通过文本释义技术来提高数据质量。为了解决缺乏标记讽刺数据的不足，作者创建了一个包含5929条推文的、由人类标注的数据集，这有助于评估LLMs在不同讽刺情境下的表现。结果显示，当使用特定主题的数据集，如与核能相关的内容，来微调LLMs时，这些模型由于文本多样性不足，在存在讽刺的情况下无法准确理解情感，需要外部干预，如讽刺移除来提高模型准确性。讽刺移除使得情感准确性提高了多达21%，因为训练于核能相关内容的LLMs在处理讽刺推文时遇到困难，仅达到了30%的准确性。相比之下，在涵盖更广泛主题的一般推文数据集上训练的LLMs在预测讽刺推文的情感方面表现出显著的改进（60%的准确性），这表明纳入通用文本数据可以增强讽刺检测。该研究还使用了对抗性文本增强，表明通过微小改动创建合成文本变体显著提高了模型对讽刺推文的鲁棒性和准确性（大约85%）。此外，对语言碎片化的推文进行文本释义，将大约40%的低置信度标签推文转化为高置信度标签，通过提高LLMs的情感分析准确性6%。|
|**2025-04-07**|**A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models**|Carlos Peláez-González et.al.|[2504.04976](http://arxiv.org/abs/2504.04976)|null|本研究关注大型语言模型（LLMs）在开放世界机器学习中的关键领域。尽管LLMs展现出惊人的自然语言处理能力，但它们也面临着一些挑战，包括一致性、幻觉和越狱漏洞等问题。越狱指的是构建绕过对齐保障的提示，导致不安全输出，损害LLMs的完整性。这项工作特别关注越狱漏洞的挑战，并引入了一种基于LLMs训练领域的越狱攻击新分类法。它通过泛化、目标和鲁棒性差距来描述对齐失败。我们的主要贡献是通过LLMs训练和校准过程中出现的不同语言领域来构建越狱的视角。这一观点突出了现有方法的局限性，并使我们能够根据它们所利用的潜在模型缺陷对越狱攻击进行分类。与基于提示构造方法（例如，提示模板）进行分类的传统分类方法不同，我们的方法提供了对LLMs行为的更深入理解。我们引入了一个包含四个类别的分类法——不匹配的泛化、竞争目标、对抗鲁棒性和混合攻击——揭示了越狱漏洞的基本性质。最后，我们提出了从这个分类研究中得出的关键教训。|
|**2025-04-07**|**Lemmanaid: Neuro-Symbolic Lemma Conjecturing**|Yousef Alhessi et.al.|[2504.04942](http://arxiv.org/abs/2504.04942)|null|自动猜测有用、有趣且新颖的引理将大大提高自动化推理工具的性能，并降低在证明辅助工具中形式化数学的门槛。然而，对于神经和符号方法来说，这都是一项极具挑战性的任务。我们介绍了迈向一个实用的神经-符号引理猜测工具Lemmanaid的第一步，它结合了大型语言模型（LLM）和符号方法，并在Isabelle证明辅助工具的证明库上对其进行了评估。我们训练了一个LLM来生成描述引理形状的引理模板，并使用符号方法来填补细节。我们将Lemmanaid与训练有生成完整引理声明的LLM以及之前完全符号的猜测方法进行了比较。我们的结果表明，神经和符号技术是互补的。通过利用符号和神经方法中的最佳元素，我们可以为广泛的输入领域生成有用的引理，从而促进计算机辅助的理论发展和形式化。|
|**2025-04-03**|**Generative Evaluation of Complex Reasoning in Large Language Models**|Haowei Lin et.al.|[2504.02810](http://arxiv.org/abs/2504.02810)|**[link](https://github.com/linhaowei1/kumo)**|**随着强大的大型语言模型（LLMs）展现出超越人类的推理能力，一个关键问题随之产生：LLMs是真的进行推理，还是仅仅是从它们庞大的、网络爬虫训练数据集中回忆答案？一旦被纳入后续LLMs的训练集，公开发布的基准测试不可避免地会受到影响，从而削弱了它们作为忠实评估的可靠性。为了解决这个问题，我们引入了KUMO，这是一个专门为评估LLMs推理能力而设计的生成评估框架。KUMO将LLMs与符号引擎协同结合，动态地产生多样化的、多轮推理任务，这些任务部分可观察且难度可调整。通过自动化流程，KUMO在开放式领域内持续生成新颖的任务，迫使模型展现出真正的泛化能力，而不是简单的记忆。我们在KUMO创建的100个领域的5,000个任务上评估了23个最先进的LLMs，并将它们的推理能力与大学生进行了基准测试。我们的研究发现，许多LLMs在简单推理任务上已经超越了大学水平的表现，而经过推理能力扩展的LLMs在复杂推理挑战中达到了大学水平的表现。此外，LLMs在KUMO任务上的表现与最新发布的真实世界推理基准测试结果高度相关，这强调了KUMO作为评估真实LLMs推理能力的强大、持久评估工具的价值。**|
|**2025-04-02**|**TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining**|Jeffrey Li et.al.|[2504.02107](http://arxiv.org/abs/2504.02107)|**[link](https://github.com/apple/ml-tic-lm)**|**随着新数据的出现，历史网络数据训练的大语言模型（LLMs）不可避免地会过时。我们研究了LLMs的评价策略和更新方法。我们引入了一个用于LLMs时间连续预训练的Web规模数据集，该数据集由114个Common Crawl（CC）的存档组成，比之前的连续语言建模基准大得多。我们还设计了针对通用CC数据和特定领域（维基百科、StackExchange和代码文档）的时间分层评估，以评估各种连续学习方法在保留以往知识的同时对新数据的适应能力。我们的发现表明，在通用CC数据上，自回归元调度与固定比例重放旧数据相结合可以实现与从头再训练相当的保留损失，同时所需计算量显著减少（2.6倍）。然而，在引入新数据和重放旧数据之间的最佳平衡因重放对避免通用网络数据上的遗忘至关重要而在特定领域则不那么重要而有所不同。**|
|**2025-03-31**|**JudgeLRM: Large Reasoning Models as a Judge**|Nuo Chen et.al.|[2504.00050](http://arxiv.org/abs/2504.00050)|null|大型语言模型（LLMs）作为评估者的兴起为人类标注提供了一种可扩展的替代方案，然而，现有的针对法官的监督微调（SFT）方法在需要复杂推理的领域中往往表现不足。在这项工作中，我们研究了LLM法官是否真正受益于增强的推理能力。通过对评估任务中推理要求的详细分析，我们发现SFT性能提升与推理需求样本比例之间存在负相关关系——突显了SFT在这种场景下的局限性。为了解决这个问题，我们引入了JudgeLRM，这是一系列使用强化学习（RL）和基于法官、结果驱动的奖励进行训练的面向判断的LLMs。JudgeLRM模型在性能上始终优于SFT调整后的模型和最先进的推理模型。值得注意的是，JudgeLRM-3B超越了GPT-4，而JudgeLRM-7B在F1分数上比DeepSeek-R1高出2.79%，特别是在需要深度推理的法官任务中表现出色。|
|**2025-03-28**|**Make Some Noise: Towards LLM audio reasoning and generation using sound tokens**|Shivam Mehta et.al.|[2503.22275](http://arxiv.org/abs/2503.22275)|null|将音频理解与生成整合到大型语言模型（LLMs）中仍然具有挑战性，因为音频的连续性和由此产生的高采样率。在这里，我们介绍了一种新的方法，该方法结合了变分量化（Variational Quantization）与条件流匹配（Conditional Flow Matching），将音频转换为0.23kbps的超低比特率离散标记，使其能够与LLMs中的文本标记无缝集成。我们使用低秩自适应（LoRA）对预训练的基于文本的LLM进行了微调，以评估其在实现真正的多模态能力，即音频理解和生成方面的有效性。我们的标记器在各种具有不同声学事件的数据库中优于传统的VQ-VAE。尽管通过音频标记化损失了大量细节，但使用离散标记训练的多模态LLM在音频理解方面与最先进的方法竞争，尽管音频生成效果较差。我们的结果突出了需要更大的、更多样化的数据集和改进的评估指标来推进多模态LLM的性能。|
|**2025-03-28**|**Post-Incorporating Code Structural Knowledge into LLMs via In-Context Learning for Code Translation**|Yali Du et.al.|[2503.22776](http://arxiv.org/abs/2503.22776)|null|代码翻译将代码库迁移到不同的编程语言。最近，大型语言模型（LLMs）在软件挖掘方面取得了显著进展。然而，处理源代码的语法结构仍然是一个挑战。经典的语法感知方法依赖于复杂的模型架构和损失函数，这使得它们在LLM训练中的集成资源密集。本文采用情境学习（ICL），它直接将任务示例集成到输入上下文中，以在预训练的LLMs后加入代码结构知识。我们从信息论的角度重新审视了ICL中的示例选择，提出基于信息覆盖的列表选择比基于相似性和多样性组合的传统方法更精确和更具普遍性。为了解决量化信息覆盖的挑战，我们引入了一个代理度量，即抽象语法树（CAST）的覆盖范围。此外，我们为示例选择公式化了NP-hard的CAST最大化问题，并证明它是一个标准子模最大化问题。因此，我们提出了一种用于CAST子模最大化的贪婪算法，该算法在多项式时间内理论上保证了（1-1/e）近似的解。我们的方法是将代码结构知识在测试时无训练和模型无关地后加入现有LLMs的第一种方法。实验结果表明，我们的方法显著提高了LLMs的性能，并揭示了两个有意义的见解：1）尽管在训练过程中被忽视，代码结构知识可以在推理期间有效地后加入预训练的LLMs；2）增加模型大小或训练数据不会导致代码结构知识的出现，强调了明确考虑代码语法结构的重要性。|
|**2025-03-27**|**Boosting Large Language Models with Mask Fine-Tuning**|Mingyuan Zhang et.al.|[2503.22764](http://arxiv.org/abs/2503.22764)|**[link](https://github.com/ming-k9/mft)**|**在主流大型语言模型（LLM）微调协议中，模型通常被保持完整。还没有研究质疑保持模型完整性的必要性对性能的影响。在本研究中，我们引入了一种全新的LLM微调范式——掩码微调（MFT），以表明适当打破模型完整性可以意外地提高性能。具体来说，MFT通过典型LLM微调目标学习一组二元掩码。大量实验表明，MFT在各种领域和骨干网络（例如，使用LLaMA2-7B/3.1-8B在编码任务中平均提升1.95%/1.88%）上获得了持续的性能提升。我们提供了详细的步骤来从不同的超参数角度研究提出的MFT，以获得更深入的见解。特别是，MFT通过在完整经过充分训练的模型上部署它，自然地更新了当前的LLM训练协议。这项研究将掩码学习从传统的网络剪枝背景下的模型压缩功能扩展到了更广泛的范围。**|
|**2025-03-27**|**Malicious and Unintentional Disclosure Risks in Large Language Models for Code Generation**|Rafiqul Rabin et.al.|[2503.22760](http://arxiv.org/abs/2503.22760)|null|本文探讨了针对从软件仓库中挖掘的数据进行代码生成的的大型语言模型（LLM）可能生成泄露其训练数据中包含的敏感信息的风险。我们将这种在文献中称为“无意记忆”的风险分解为两个组成部分：无意泄露（LLM在用户未主动寻求的情况下向用户展示秘密）和恶意泄露（LLM向拥有部分训练数据知识的攻击者展示秘密）。我们观察到，尽管现有工作主要预测恶意泄露，无意泄露也是一个关注点。我们描述了评估无意和恶意泄露风险的方法，这些方法可以并行应用于不同版本的训练数据集和模型。我们通过独立评估Open Language Model（OLMo）系列模型及其Dolma训练数据集来展示这些方法。我们的结果表明，首先，数据源和处理方式的变化与无意记忆风险的大幅变化相关；其次，同一组操作变化可能会增加一种风险同时减轻另一种风险；第三，泄露敏感信息的风险不仅与提示策略或测试数据集有关，还与敏感信息的类型有关。这些贡献依赖于数据挖掘，以实现LLM训练数据供应链所需的更大隐私和安全测试。|
|**2025-03-26**|**UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture**|Heng Liao et.al.|[2503.20377](http://arxiv.org/abs/2503.20377)|null|随着大型语言模型（LLMs）的持续扩展，所需的计算能力和带宽也在增加。为了解决这个问题，我们引入了UB-Mesh，这是一种新型的AI数据中心网络架构，旨在提高可扩展性、性能、成本效益和可用性。与提供对称节点间带宽的传统数据中心不同，UB-Mesh采用层次化的本地化nD-FullMesh网络拓扑。这种设计充分利用了LLM训练的数据局部性，优先考虑短距离、直接的互连，以最小化数据移动距离并减少交换机使用。  尽管UB-Mesh的nD-FullMesh拓扑提供了几个理论上的优势，但其具体的架构设计、物理实现和网络系统优化带来了新的挑战。对于UB-Mesh的实际构建，我们首先设计了基于4D-FullMesh拓扑的UB-Mesh-Pod架构。UB-Mesh-Pod通过一系列硬件组件实现，这些组件作为基础构建块，包括专门设计的NPU、CPU、低基数交换机（LRS）、高基数交换机（HRS）、网络接口卡（NIC）等。这些组件通过一种新颖的统一总线（UB）技术相互连接，该技术能够实现灵活的I/O带宽分配和硬件资源池化。  对于网络系统优化，我们提出了名为全路径路由（APR）的高级路由机制，以有效地管理数据流量。这些优化与拓扑感知性能提升和稳健的可靠性措施（如64+1备份设计）相结合，使得成本效益提高了2.04倍，网络可用性比传统的Clos架构提高了7.2%，在各种LLM训练任务中的线性度超过95%。|
|**2025-03-26**|**L4: Diagnosing Large-scale LLM Training Failures via Automated Log Analysis**|Zhihan Jiang et.al.|[2503.20263](http://arxiv.org/abs/2503.20263)|null|随着大型语言模型（LLMs）在各种应用中展现出其能力，为现代企业训练定制的LLMs已成为必要。然而，由于LLM训练的复杂性，这需要大量的计算资源和漫长的训练时间，训练过程中的失败在所难免。这些失败导致了大量的资源和时间浪费，突显出对有效且高效的故障诊断的迫切需求，以降低LLM训练的成本。在本文中，我们展示了从2023年5月到2024年4月，在我们生产平台-X上关于428个LLM训练失败的首次实证研究。我们的研究揭示，硬件和用户错误是主要的根本原因，当前的诊断过程严重依赖训练日志。不幸的是，现有的基于日志的诊断方法在处理LLM训练日志方面存在不足。考虑到LLM训练的独特特征，我们识别出LLM训练日志的三种不同模式：跨作业模式、空间模式和时序模式。然后，我们介绍了我们的基于日志的大规模LLM训练故障诊断框架L4，该框架能够从大量的训练日志中自动提取指示失败的 信息（即日志事件、节点、阶段和迭代），从而减少人工努力并促进故障恢复。在真实世界数据集上的实验结果表明，L4在识别指示失败的日志和定位故障节点方面优于现有方法。此外，L4已在平台-X上应用，并展示了其在实现准确和高效故障诊断方面的有效性。|
|**2025-03-25**|**SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings**|Farhana Keya et.al.|[2503.19257](http://arxiv.org/abs/2503.19257)|null|每项科学发现都始于受先前工作、跨学科概念和新兴挑战启发的想法。最近在科学语料库上训练的大语言模型（LLMs）的进展推动了人工智能辅助想法生成的兴趣。然而，生成具有情境意识、高质量和创新性的想法仍然具有挑战性。我们介绍了SCI-IDEA框架，该框架利用LLM提示策略和“啊哈时刻”检测进行迭代想法细化。SCI-IDEA从研究出版物中提取关键方面，评估生成的想法在新颖性、激动性、可行性和有效性方面的表现。全面实验验证了SCI-IDEA的有效性，分别在新颖性、激动性、可行性和有效性方面实现了平均得分6.84、6.86、6.89和6.84（1-10分）。评估使用了GPT-4o、GPT-4.5、DeepSeek-32B（每个模型在2次提示下运行）和DeepSeek-70B（3次提示下运行），使用分词级嵌入进行“啊哈时刻”检测。同样，使用GPT-4o在5次提示下、GPT-4.5在3次提示下、DeepSeek-32B在零次提示的思维链中、DeepSeek-70B在5次提示下，使用句子级嵌入，它分别实现了6.87、6.86、6.83和6.87的得分。我们还解决了诸如知识产权、潜在误用以及平衡人类创造力和AI驱动创意等伦理考量。我们的结果突出了SCI-IDEA促进情境意识科学想法结构化和灵活探索的潜力，支持创新的同时维护伦理标准。|
|**2025-03-23**|**WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training**|Zheng Wang et.al.|[2503.17924](http://arxiv.org/abs/2503.17924)|null|在这项工作中，我们提出了WLB-LLM，这是一种用于大型语言模型训练的工作负载平衡4D并行方法。我们首先深入分析了LLM训练中工作负载不平衡的问题，并确定了在管道并行和上下文并行层面上的两个主要不平衡来源。然后，为了解决不平衡问题，在管道并行层面，WLB-LLM引入了一种工作负载感知的变长文档打包方法，以平衡微批次的计算和通信工作负载。此外，在上下文并行层面，WLB-LLM提出了一种新颖的按文档细粒度分片策略，确保上下文并行组内的每个工作负载相同。在不同模型规模下的综合实验表明，WLB-LLM显著缓解了4D并行LLM训练中的工作负载不平衡，并在将WLB-LLM应用于我们内部LLM训练框架时实现了平均加速1.23倍。|
|**2025-03-21**|**Modifying Large Language Model Post-Training for Diverse Creative Writing**|John Joon Young Chung et.al.|[2503.17126](http://arxiv.org/abs/2503.17126)|**[link](https://github.com/mj-storytelling/DiversityTuning)**|由于创意写作任务没有唯一的正确答案，训练用于执行这些任务的大型语言模型（LLMs）应该能够生成多样化的有效输出。然而，LLM的后续训练通常侧重于提高生成质量，但往往忽视了促进输出多样性。因此，在创意写作生成中，我们研究了后续训练方法来促进输出多样性和质量。我们的核心思想是将偏差——即训练样本与其他具有相同提示的所有样本之间的差异程度——包含在训练目标中，以促进从罕见的高质量实例中学习。通过采用我们的直接偏好优化（DPO）和优势比偏好优化（ORPO）方法，我们证明了我们可以在最小程度降低质量的同时促进训练模型的输出多样性。我们具有80亿参数的最佳模型能够达到与人类创建的数据集相当的水平，同时其输出质量与我们所检查的最佳指令调整模型GPT-4o和DeepSeek-R1相似。我们进一步通过人工评估、消融实验以及与现有多样化方法DivPO的比较来验证我们的方法。|
|**2025-03-21**|**Large Language Models Can Verbatim Reproduce Long Malicious Sequences**|Sharon Lin et.al.|[2503.17578](http://arxiv.org/abs/2503.17578)|null|后门攻击在机器学习模型中已被广泛研究，主要在计算机视觉领域。最初，这些攻击通过操纵分类器在存在特定、通常微妙的触发器时产生错误的输出。本文重新审视了在大型语言模型（LLMs）背景下后门攻击的概念，重点关注生成长而精确的序列。这种关注至关重要，因为许多LLMs的恶意应用涉及生成长而特定上下文的输出。例如，一个LLM可能被后门化以生成包含硬编码加密密钥的代码，该密钥用于与对手加密通信，因此需要极高的输出精度。我们遵循计算机视觉文献，调整LLM的训练过程，将恶意触发器-响应对纳入更大的良性示例数据集，以生成特洛伊木马模型。我们发现，当被目标输入触发时，可以重现包含≤100个随机字符的任意精确响应，即使在低秩优化设置中也是如此。我们的工作证明了在LoRA微调中注入后门的可能性。在确定漏洞后，我们转向防御此类后门。我们在Gemini Nano 1.8B上进行了实验，表明随后的良性微调有效地禁用了特洛伊木马模型中的后门。|
|**2025-03-21**|**SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia**|Lama Ayash et.al.|[2503.17485](http://arxiv.org/abs/2503.17485)|null|大型语言模型（LLMs）在自然语言处理方面展现出了惊人的能力；然而，它们往往难以准确捕捉和反映文化细微差别。本研究针对这一挑战，聚焦于沙特阿拉伯，一个以其多样的方言和丰富的文化传统为特点的国家。我们引入了SaudiCulture，这是一个旨在评估LLMs在沙特阿拉伯独特地理和文化背景下的文化能力的创新基准。SaudiCulture是一个涵盖五个主要地理区域（如西部、东部、南部、北部和中部）以及适用于所有地区的一般问题的综合数据集。该数据集涵盖了广泛的文化领域，包括食物、服装、娱乐、庆祝活动和手工艺。为确保严格的评估，SaudiCulture包括不同复杂程度的问题，如开放式、单选和多项选择题，其中一些需要多个正确答案。此外，该数据集区分了常见文化知识和专业区域方面。我们对五个LLMs（如GPT-4、Llama 3.3、FANAR、Jais和AceGPT）进行了广泛的评估，分析了它们在不同问题类型和文化背景下的表现。我们的发现表明，所有模型在面对高度专业或特定区域的问题时，尤其是需要多个正确答案的问题时，都会经历显著的性能下降。此外，某些文化类别比其他类别更容易识别，进一步突显了LLMs在文化理解上的不一致性。这些结果强调了将特定区域知识纳入LLMs训练以增强其文化能力的重要性。|
|**2025-03-20**|**Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning**|Chen Li et.al.|[2503.15952](http://arxiv.org/abs/2503.15952)|null|自从DeepSeek-R1普及以来，组相对策略优化（GRPO）已成为推理型大型语言模型训练的核心部分。然而，我们发现了一些影响强化学习稳定性和推理效率的不足。因此，我们提出了自适应组策略优化（AGPO），它包含两个简单但有效的改进：一种改进的收益估计方法，以减轻零方差情况；一种基于长度的奖励，激励模型避免过度思考。实验表明，我们的方法实现了更稳定的训练，在推理步骤中使用了显著更少的标记，且性能相当或更优。|
|**2025-03-20**|**Video-VoT-R1: An efficient video inference model integrating image packing and AoE architecture**|Cheng Li et.al.|[2503.15807](http://arxiv.org/abs/2503.15807)|null|在视频语言预训练领域，现有模型在推理效率和多模态数据处理方面面临诸多挑战。本文提出了一种基于长序列图像编码器的昆仑白泽-VoT-R1视频推理模型及其训练和应用方法。通过整合图像打包技术、专家自主（AoE）架构，并结合思想视频（VoT）——一个使用大规模强化学习训练的大语言模型（LLM），以及多种训练技术，有效提升了模型在视频推理任务中的效率和准确度。实验表明，该模型在多项测试中表现优异，为视频语言理解提供了新的解决方案。|
|**2025-03-19**|**Enhancing Code LLM Training with Programmer Attention**|Yifan Zhang et.al.|[2503.14936](http://arxiv.org/abs/2503.14936)|null|人类注意力为代码LLM训练提供了宝贵且尚未充分利用的信号，提供了超越纯粹机器驱动注意力的视角。尽管收集眼动数据的复杂性和成本很高，但在系统地使用这些信号进行代码LLM训练方面也取得了有限的进展。为了解决这两个问题，我们提出一个涵盖增强和基于奖励的微调的连贯流程。具体来说，我们引入了以下内容：（1）一种眼动路径增强方法，用于扩展程序员注意力数据集；（2）一个模式抽象步骤，将原始注视点精炼为可学习的注意力模式；（3）一种奖励引导策略，将这些见解直接整合到CodeT5监督微调过程中。我们的实验在CodeXGlue基准测试的代码摘要任务上实现了+7.16的CodeBLEU提升，突显了将人类和机器注意力结合如何提升代码智能。我们希望这项工作能够鼓励在下一代AI4SE中更广泛地探索以人为中心的方法。|
|**2025-03-18**|**Empowering LLMs in Decision Games through Algorithmic Data Synthesis**|Haolin Wang et.al.|[2503.13980](http://arxiv.org/abs/2503.13980)|null|大型语言模型（LLMs）在众多领域展现了令人印象深刻的性能，但它们在复杂推理和决策任务上往往表现不佳。决策游戏，其本质需要多方面的推理逻辑，是评估和提升LLMs推理能力的理想沙盒。在本工作中，我们首先探讨LLMs是否可以通过有针对性的训练后提升来掌握复杂的决策游戏。为此，我们设计了数据合成策略，并从两款经典游戏——斗地主和围棋中收集了大量的离线数据集。我们进一步开发了一套技术，以有效地将这些数据整合到LLMs的训练中，从而产生了两个新的智能体：Mastermind-Dou和Mastermind-Go。我们的实验结果表明，这些Mastermind LLMs在其各自的游戏中实现了具有竞争力的表现。此外，我们还探讨了整合决策数据是否能提升LLMs的通用推理能力。我们的研究发现，这种训练后的改进有助于某些推理方面的提升，为优化LLMs数据收集和合成策略提供了有价值的见解。|
|**2025-03-18**|**ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models**|Alexey Karev et.al.|[2503.13923](http://arxiv.org/abs/2503.13923)|null|近年来，大型语言模型（LLMs）是机器学习领域最重要的发现之一。基于LLM的人工智能（AI）助手，如ChatGPT，一直吸引着研究人员、投资者和公众的关注，推动了该行业的快速增长。随着市场上新LLM的频繁推出，区分它们变得越来越困难，从而产生了对新LLM比较方法的需求。在这项研究中，提出了针对生成型大型语言模型的“一致性关注相似度比较框架”（ConSCompF）。该框架比较两个LLM生成的文本，并生成一个相似度分数，表示它们响应之间的整体相似程度。该框架的主要优势是它可以在少量未标记数据上运行，例如聊天机器人指令提示，且不需要LLM开发者披露任何关于其产品的信息。  为了评估ConSCompF的有效性，进行了两个旨在识别多个LLM之间相似性的实验。此外，这些实验还检验了ConSCompF生成的相似度分数与其他基准技术（如ROUGE-L）产生的输出差异之间的相关性。最后，进行了一系列少样本LLM比较实验，以评估ConSCompF在少样本LLM比较场景中的性能。  所提出的框架可用于计算多个LLM的相似度矩阵，这些矩阵可以通过主成分分析（PCA）有效地可视化。ConSCompF的输出可能对了解LLM训练期间可能使用的数据提供有价值的见解，并有助于检测可能的投资欺诈尝试。|
|**2025-03-14**|**Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware**|Insu Jang et.al.|[2503.11367](http://arxiv.org/abs/2503.11367)|**[link](https://github.com/cornstarch-org/cornstarch)**|**多模态大型语言模型（MLLMs）通过结合异构模型架构来扩展大型语言模型（LLMs）的功能，以便处理图像和音频等多种模态。然而，MLLMs模型结构和数据类型的内在异质性使得对现有LLM训练框架的临时扩展不适用于高效的MLLM训练。在本文中，我们提出了Cornstarch，这是第一个通用的分布式MLLM训练框架。Cornstarch简化了模块化MLLM构建，允许构成模型的并行组合，并为高效的分布式MLLM训练引入了MLLM特定的优化，包括管道和上下文并行。我们的评估表明，Cornstarch在训练吞吐量方面优于现有解决方案，最高可达1.57倍。Cornstarch是一个开源项目，可在https://github.com/cornstarch-org/Cornstarch上获取。**|
|**2025-03-14**|**High-Dimensional Interlingual Representations of Large Language Models**|Bryan Wilie et.al.|[2503.11280](http://arxiv.org/abs/2503.11280)|null|大型语言模型（LLMs）在大量多语言数据集上训练时，暗示了形成跨语言结构——表示空间中的一个共享子空间。然而，关于这一现象的证据是混合的，不清楚这些模型是否真正发展出了统一的跨语言表示，或者只是呈现了部分对齐的结构。我们探索了31种资源水平、类型和地理区域各异的多样语言；并发现多语言LLMs表现出不一致的跨语言对齐。为了解决这个问题，我们提出了一种跨语言表示框架，该框架识别了共享的跨语言语义子空间和由于表示限制而存在的碎片化组件。我们引入了跨语言局部重叠（ILO）分数，通过比较高维表示的局部邻域结构来量化跨语言对齐。我们利用ILO来研究单语言微调对多语言LLM中跨语言表示的影响。我们的结果表明，仅在单一语言上训练会破坏早期层的对齐，而冻结这些层则保护了跨语言表示的对齐，从而提高了跨语言泛化能力。这些结果验证了我们的框架和评估跨语言表示的指标，并进一步强调了跨语言对齐对于可扩展的多语言学习至关重要。|
|**2025-03-14**|**Beyond A Single AI Cluster: A Survey of Decentralized LLM Training**|Haotian Dong et.al.|[2503.11023](http://arxiv.org/abs/2503.11023)|null|大型语言模型（LLMs）的出现彻底改变了人工智能的发展，但它们的训练需求超出了单个集群甚至数据中心所能提供的计算资源，限制了其可访问性，仅限于大型组织。去中心化训练作为一种利用集群、数据中心和全球各地区分散资源的有前景的范式应运而生，为更广泛的社区民主化LLMs的发展。作为对这一新兴领域首次全面探索，我们将去中心化LLMs训练视为一种资源驱动范式，并将其分为社区驱动和组织方法。此外，我们的深入分析阐明了去中心化LLMs训练的以下方面：（1）与相关领域概念的比较位置，（2）去中心化资源开发趋势，（3）在新型分类法下的最新进展及其讨论。我们还提供了最新的案例研究，并探讨了未来方向，为去中心化LLMs训练研究的发展做出了贡献。|
|**2025-03-14**|**REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives**|Kun Su et.al.|[2503.11924](http://arxiv.org/abs/2503.11924)|null|本文介绍了一个名为REGEN的新型数据集，旨在评估推荐大型语言模型（LLM）的对话能力，并解决了现有数据集主要关注序列物品预测的局限性。REGEN通过补全两个关键的自然语言特征扩展了亚马逊产品评论数据集：（1）用户评论，表示引导用户选择后续物品的“引导”查询；（2）叙述，与每个推荐物品相关的丰富文本输出，考虑到先前的上下文。这些叙述包括产品推荐、购买解释和用户偏好的总结。此外，我们建立了对话推荐任务的全端建模基准，模型被训练生成基于用户历史（物品和评论）的推荐和相应叙述。为此联合任务，我们引入了一个建模框架LUMEN（基于LLM的统一多任务模型，包含评论、推荐和叙述），它使用LLM作为批评、检索和生成的骨干。我们还使用标准的自动评分技术评估数据集的质量，并通过训练传统和基于LLM的推荐模型来对其进行了基准测试。我们的结果表明，引入评论通过使推荐器能够学习语言理解并将其与推荐信号整合，从而提高了推荐质量。此外，在数据集上训练的LLM有效地生成推荐和上下文叙述，实现了与最先进推荐器和语言模型相当的性能。|
|**2025-03-13**|**SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline Parallel Offloading**|Qiaoling Chen et.al.|[2503.10377](http://arxiv.org/abs/2503.10377)|null|近年来，大型语言模型（LLM）展现出惊人的能力，推动了实际应用的发展。然而，由于GPU内存和计算需求高，在越来越长的输入序列上训练LLM带来了巨大的挑战。现有解决方案面临两个主要限制：（1）内存减少技术，如激活重新计算和CPU卸载，会降低训练效率；（2）分布式并行策略需要过量的GPU资源，限制了输入序列长度的可扩展性。为了解决这些差距，我们提出了自适应序列管道并行卸载（SPPO），这是一种新型的LLM训练框架，旨在优化长序列训练的内存和计算资源效率。SPPO引入了自适应卸载，利用序列感知卸载和二级激活管理来减少GPU内存消耗，同时不降低训练效率。此外，SPPO开发了一种自适应管道调度方法，结合启发式求解器和多路复用序列分区，以提高计算资源效率。实验结果表明，SPPO在Megatron-LM和DeepSpeed的基础上实现了高达3.38倍的吞吐量提升，仅在128个A100 GPU上实现了长达4M个token的7B LLM的高效训练。|
|**2025-03-13**|**DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless Networks: A Survey**|Yu Qiao et.al.|[2503.09956](http://arxiv.org/abs/2503.09956)|null|基于强化学习（RL）的大型语言模型（LLMs），如ChatGPT、DeepSeek和Grok-3，因其在自然语言处理和多模态数据理解方面的卓越能力而受到广泛关注。同时，信息服务领域的迅速扩张推动了智能化、高效和自适应无线网络的日益增长需求。无线网络需要借助基于RL的LLMs的力量，而这些模型也从无线网络中受益，扩展了它们的应用场景。具体而言，基于RL的LLMs可以通过智能资源分配、自适应网络优化和实时决策来增强无线通信系统。相反，无线网络为基于RL的LLMs的高效训练、部署和分布式推理提供了关键基础设施，特别是在去中心化和边缘计算环境中。这种相互增强凸显了深入探索这两个领域之间相互作用的需要。我们首先回顾了无线通信的最新进展，强调了相关挑战和潜在解决方案。然后，我们讨论了基于RL的LLMs的进展，重点关注LLM训练的关键技术、挑战和潜在解决方案。随后，我们探索了这两个领域的相互增强，突出了关键动机、开放挑战和潜在解决方案。最后，我们提供了对未来方向、应用及其社会影响的见解，以进一步探索这一交汇点，为下一代智能通信系统铺平道路。总体而言，这项调查全面概述了基于RL的LLMs与无线网络之间的关系，展现了一个愿景，即这两个领域相互赋能，推动创新。|
|**2025-03-12**|**Global Position Aware Group Choreography using Large Language Model**|Haozhou Pang et.al.|[2503.09645](http://arxiv.org/abs/2503.09645)|null|舞蹈是人类文化深刻而普遍的表达方式，通过与音乐同步的动作传达情感和故事。尽管一些当前的工作在单人舞蹈生成任务中取得了令人满意的结果，但多人舞蹈生成领域仍然相对新颖。在这项工作中，我们提出了一种利用大型语言模型（LLM）最新进展的群体编舞框架，将群体舞蹈生成问题建模为一种序列到序列的翻译任务。我们的框架包括一个标记器，它将连续特征转换为离散标记，以及一个经过微调的LLM，用于根据音频标记预测动作标记。我们表明，通过适当的输入模态标记化和精心设计的LLM训练策略，我们的框架可以生成逼真且多样化的群体舞蹈，同时保持强大的音乐相关性和舞者一致性。大量的实验和评估表明，我们的框架达到了最先进的性能。|
|**2025-03-11**|**EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments**|Dongping Li et.al.|[2503.08604](http://arxiv.org/abs/2503.08604)|**[link](https://github.com/silence143/EMMOE)**|长期以来，开发受自然语言控制的自主家用机器人一直是人类的追求。尽管大型语言模型（LLMs）和具身智能的进步使得这一目标更加接近，但仍然存在一些挑战：缺乏更复杂机器人任务的统一基准，有限的评估方法和指标，LLMs与移动操作轨迹之间的数据不兼容。为了解决这些问题，我们引入了开放环境中的具身移动操作（EMMOE），它要求代理解释用户指令并在连续空间中执行长期日常任务。EMMOE无缝地将高级和低级具身任务整合到一个统一的框架中，并引入了三个新的评估指标。此外，我们收集了EMMOE-100，它具有各种任务属性、详细的过程注释、失败后的重新规划，以及用于LLM训练的两个子数据集。此外，我们设计了HomieBot，这是一个由具有直接偏好优化（DPO）的LLM、轻量级导航和操作模型以及多个错误检测机制组成的复杂代理系统。最后，我们展示了HomieBot的性能以及不同模型和策略的评估。|
|**2025-03-09**|**Privacy Auditing of Large Language Models**|Ashwinee Panda et.al.|[2503.06808](http://arxiv.org/abs/2503.06808)|null|当前用于大型语言模型（LLMs）隐私审计的技术效果有限——它们依赖于生成基本诱饵的方法，导致成员推理攻击能力较弱，进而给出宽松的经验隐私泄露下限。我们开发了一种在涵盖一系列现实场景的威胁模型下，比先前工作中使用的诱饵更有效的诱饵。通过在多个微调LLM系列上进行的大量实验，我们证明了我们的方法在检测隐私泄露方面设定了新的标准。对于测量非隐私训练LLMs的记忆率，我们设计的诱饵超越了先前的方法。例如，在Qwen2.5-0.5B模型上，我们设计的诱饵在1%的误报率（FPR）下实现了49.6%的真正例率（TPR），远超先前方法在1% FPR下的4.2% TPR。我们的方法可用于对具有理论ε为4的训练模型提供ε≈1的隐私审计。据我们所知，这是首次在攻击者无法训练影子模型、插入梯度诱饵或在每个迭代中访问模型的情况下，LLM训练的隐私审计实现了非平凡的审计成功。|
|**2025-03-07**|**PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs**|Roberto Cerina et.al.|[2503.05529](http://arxiv.org/abs/2503.05529)|null|本文介绍了一种名为PoSSUM的开源协议，该协议通过多模态大型语言模型（LLMs）对社交媒体用户进行无干扰式调查。PoSSUM利用用户的实时帖子、图片和其他数字痕迹来创建硅样本，捕捉LLMs训练数据中不存在的信息。为了获得具有代表性的估计值，PoSSUM采用具有结构化先验的分层回归和后分层（MrP）方法，以抵消社交媒体平台可观察的选择偏差。该协议在2024年美国总统选举期间得到验证，期间进行了五次PoSSUM调查，并在GitHub和X上发布。在最后进行的调查中，于10月17日至26日进行，采用1054名X用户的合成样本，PoSSUM准确预测了51个州中的50个州的结果，并将共和党候选人获胜的概率定为0.65。值得注意的是，它还表现出比大多数传统调查机构更低的州级偏差。这些结果证明了PoSSUM作为传统调查方法的完全自动化、无干扰替代品的潜力。|
|**2025-03-06**|**Towards Data-Efficient Language Models: A Child-Inspired Approach to Language Learning**|Mohammad Amin Ghanizadeh et.al.|[2503.04611](http://arxiv.org/abs/2503.04611)|null|在这项工作中，我们解释了在BabyLM挑战赛中采用的方法，该方法使用与传统大型语言模型（LLMs）相比数据量显著更少的方法来训练语言模型（LMs），并受到人类儿童学习方式的启发。尽管人类儿童接触到的语言输入远少于LLM，但他们仍然取得了惊人的语言理解和生成能力。为此，我们开发了一个在经过精选的数据集上训练的模型，该数据集包含1000万个单词，主要来源于儿童指导性转录文本。2024年BabyLM挑战赛初始数据集的1000万个单词经过筛选后变为850万个。接下来，它补充了一个随机选择的TVR数据集的子集，该子集包含150万个电视对话单词。后一个数据集确保模型像儿童一样，也能通过媒体接触语言。此外，我们将词汇量减少到32,000个标记，与儿童在语言习得早期阶段的有限词汇量相匹配。我们使用课程学习，能够在某些基准测试上匹配基线，而在其他基准测试上超越基线。此外，结合常见的LLM训练数据集，如MADLAD-400，会降低性能。这些发现强调了在创建更数据高效的、更好地模仿人类学习过程的语言模型时，数据集选择、词汇缩放和课程学习的重要性。|
|**2025-03-06**|**PokéChamp: an Expert-level Minimax Language Agent**|Seth Karten et.al.|[2503.04094](http://arxiv.org/abs/2503.04094)|null|我们介绍了PokéChamp，这是一个由大型语言模型（LLMs）驱动的用于宝可梦战斗的极大极小值智能体。基于双人竞技游戏的一般框架，PokéChamp利用LLMs的通用能力来增强极大极小值树搜索。具体来说，LLMs替换了三个关键模块：（1）玩家动作采样，（2）对手建模，和（3）价值函数估计，使智能体能够有效地利用游戏历史和人类知识来缩小搜索空间和解决部分可观察性问题。值得注意的是，我们的框架不需要额外的LLM训练。我们在流行的第九代OU格式中评估了PokéChamp。当使用GPT-4o驱动时，它在对最佳现有基于LLM的机器人时达到了76%的胜率，在对最强基于规则的机器人时达到了84%的胜率，展示了其优越的性能。即使使用开源的80亿参数Llama 3.1模型，PokéChamp也持续优于之前基于GPT-4o的LLM最佳机器人Pokellmon，胜率为64%。PokéChamp在宝可梦Showdown在线排行榜上达到了预计的1300-1500分Elo，将其置于人类玩家前30%-10%。此外，这项工作编译了最大的真实玩家宝可梦战斗数据集，包含超过300万场比赛，其中超过50万场高Elo比赛。基于这个数据集，我们建立了一系列战斗基准和谜题来评估特定的战斗技能。我们还对本地游戏引擎进行了关键更新。我们希望这项工作能够促进进一步的研究，利用宝可梦战斗作为基准，将LLM技术与解决一般多智能体问题的博弈论算法相结合。视频、代码和数据集可在https://sites.google.com/view/pokechamp-llm获取。|
|**2025-03-06**|**Continual Pre-training of MoEs: How robust is your router?**|Benjamin Thérien et.al.|[2503.05029](http://arxiv.org/abs/2503.05029)|null|稀疏激活的专家混合（MoE）变压器是基础模型的很有前途的架构。与每个前向传递需要相同数量的浮点运算（FLOPs）的密集变压器相比，MoE在训练时间上具有更高的样本效率，并实现了更强的性能。因此，许多闭源和开源的前沿语言模型都采用了MoE架构。自然地，实践者将希望在不完全重新训练的情况下，使用大量新收集的数据扩展这些模型的能力。先前的研究表明，简单的重放和学习率重新加热以及重新衰减的组合可以使密集的仅解码器变压器进行持续的预训练（CPT），与完全重新训练相比，性能下降最小。然而，对于仅解码器的MoE变压器，尚不清楚路由算法会如何影响持续的预训练性能：1）MoE变压器的路由器是否会相对于密集模型加剧遗忘？2）路由器在CPT后是否在先前分布上保持平衡的负载？3）应用于密集模型相同的策略是否足够用于持续预训练MoE大型语言模型？在以下内容中，我们针对四个MoE变压器进行了一项大规模（>2B参数切换和DeepSeek MoE LLMs，训练了600B个标记）实证研究，以回答这些问题。我们的结果表明，即使在没有重放的MoE中进行持续的预训练，Sinkhorn-Balanced和Z-and-Aux-loss-balanced路由算法对分布变化的鲁棒性也令人惊讶。此外，我们表明MoE LLMs在CPT期间保持了它们的样本效率（相对于FLOP匹配的密集模型），并且它们可以在成本的一小部分内匹配完全重新训练的MoE的性能。|
|**2025-03-04**|**PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset**|Haider Asif et.al.|[2503.02497](http://arxiv.org/abs/2503.02497)|null|大型语言模型（LLMs）在代码生成、自然语言处理和特定领域推理方面展现出惊人的能力。它们在辅助量子软件开发方面的潜力尚未得到充分探索，尤其是对于PennyLane框架——一个领先的混合量子-经典计算平台。为了填补这一空白，我们引入了一个新颖的高质量数据集，包含3,347个针对PennyLane的量子电路及其上下文描述的代码样本，这些样本专门用于训练/微调基于LLM的量子代码辅助。我们的主要贡献有三点：（1）利用量子计算教科书、官方文档和开源存储库自动创建并开源发布了一个全面的PennyLane数据集；（2）开发了一种系统性的数据精炼、标注和格式化方法，以优化LLM训练效率；（3）基于检索增强生成（RAG）框架进行了全面评估，证明了我们的数据集在简化PennyLane代码生成和改善量子开发工作流程方面的有效性。与主要关注Qiskit的现有努力相比，我们的数据集显著扩大了AI驱动代码辅助中涵盖的量子框架范围。通过填补这一差距并提供可重复的数据集创建方法，我们旨在推进AI辅助量子编程领域，使量子计算对新手和经验丰富的开发者都更加易于接触。|
|**2025-03-04**|**Making Better Mistakes in CLIP-Based Zero-Shot Classification with Hierarchy-Aware Language Prompts**|Tong Liang et.al.|[2503.02248](http://arxiv.org/abs/2503.02248)|null|近期的研究利用在大规模互联网爬取文本数据上训练的大型语言模型（LLMs）来生成CLIP基于零样本图像分类中下游类别的文本描述。尽管大多数这些方法旨在提高准确性，但我们的工作专注于“犯更好的错误”，其中错误的严重性来自下游任务的给定标签层次结构。由于CLIP的图像编码器是用语言监督信号训练的，它隐式地捕捉了不同类别之间的层次语义关系。这激励了我们在零样本分类中“犯更好的错误”的目标，而CLIP对于这个任务来说天生非常适合。我们的方法（HAPrompts）通过查询语言模型为给定类别生成文本表示，作为CLIP的零样本分类器，在下游任务上进行图像分类。据我们所知，这是第一个在CLIP基于零样本分类中引入“犯更好的错误”的工作。在我们的实验中，我们的方法在跨越五个不同规模的数据集、具有不同高度标签层次结构的整体比较中优于相关方法。我们的代码和LLM生成的图像提示：\href{https://github.com/ltong1130ztr/HAPrompts}{https://github.com/ltong1130ztr/HAPrompts}。|
|**2025-03-04**|**Learning from Failures in Multi-Attempt Reinforcement Learning**|Stephen Chung et.al.|[2503.04808](http://arxiv.org/abs/2503.04808)|null|最近，强化学习（RL）在大型语言模型（LLMs）领域的进展，以DeepSeek R1为例，表明即使是简单的问答任务也能显著提升LLMs的推理能力。在这项工作中，我们通过将任务修改为多尝试设置来扩展这种方法。模型不是对每个问题只生成一个响应，而是给予多次尝试，并在错误响应后提供反馈。多尝试任务鼓励模型改进其先前的尝试并提高搜索效率。实验结果表明，即使在多尝试任务上训练的小型LLMs，在评估时尝试次数增加也能显著提高准确率，从数学基准测试中的1次尝试的45.6%提高到2次尝试的52.5%。相比之下，在标准单轮任务上训练的同一LLMs，在评估时给予更多尝试时，仅显示出微小的改进，从42.3%增加到43.2%。结果表明，与标准单轮任务相比，在多尝试任务上训练的LLMs在数学基准测试上表现略好，同时也学会了根据用户反馈更有效地改进其响应。完整代码可在https://github.com/DualityRL/multi-attempt上找到。|
|**2025-03-04**|**Call for Rigor in Reporting Quality of Instruction Tuning Data**|Hyeonseok Moon et.al.|[2503.04807](http://arxiv.org/abs/2503.04807)|null|指令微调对于适应用户意图的大型语言模型至关重要。许多研究强调了指令微调（IT）数据质量的重要性，揭示了IT数据质量与LLM对齐性能之间的强相关性。在这些研究中，IT数据的质量通常通过评估使用该数据训练的LLM的性能来评估。然而，我们发现这种做法中存在一个普遍问题：训练模型的超参数往往被随意选择，缺乏充分的理由。我们观察到，即使在用相同的数据训练相同模型的情况下，不同研究应用的超参数也存在显著差异。在本研究中，我们展示了这种做法可能带来的问题，并强调了在验证数据质量时需要仔细考虑。通过我们对LIMA数据质量和所选1000个Alpaca数据点的实验，我们证明了随意的超参数决策可能导致任何随意的结论。|
|**2025-03-03**|**Machine Learners Should Acknowledge the Legal Implications of Large Language Models as Personal Data**|Henrik Nolte et.al.|[2503.01630](http://arxiv.org/abs/2503.01630)|null|GPT是否了解你？答案取决于你的公众知名度；然而，如果你的信息已存在于网站上，答案可能是肯定的。所有大型语言模型（LLMs）在某种程度上都会记住训练数据。如果一个LLM的训练语料库包括个人数据，它也会记住这些个人数据。开发LLM通常涉及处理个人数据，这直接属于数据保护法的范畴。如果一个人被识别或可识别，其影响是深远的：即使在训练阶段结束后，AI系统也必须遵守欧盟通用数据保护条例的要求。为了支持我们的论点：（1）我们重申，LLMs在推理时输出训练数据，无论是原封不动还是以概括的形式。（2）我们表明，一些LLMs本身可以被视为个人数据。这引发了一系列数据保护影响，包括数据主体的权利，包括访问、更正或删除的权利。这些权利扩展到嵌入在AI模型中的信息。（3）本文认为，机器学习研究人员必须在整个ML开发生命周期中承认LLMs作为个人数据的法律影响，从数据收集和整理到在GitHub或Hugging Face上提供模型等。（4）我们提出了ML研究社区处理这些法律影响的多种方式。我们的论文是改善数据保护法与LLMs技术能力之间一致性的起点。我们的发现强调了法律领域与ML社区之间更多互动的必要性。|
|**2025-03-02**|**Towards An Efficient LLM Training Paradigm for CTR Prediction**|Allen Lin et.al.|[2503.01001](http://arxiv.org/abs/2503.01001)|null|大型语言模型（LLMs）在下一代基于排名的推荐系统中展现出巨大的潜力。许多近期的研究表明，LLMs在点击率（CTR）预测方面可以显著超越传统的预测方法。尽管这些成果很有希望，但当前训练范式固有的计算低效性使得在大型数据集上训练LLMs用于基于排名的推荐任务特别具有挑战性。为了训练LLMs进行CTR预测，大多数现有研究采用流行的“滑动窗口”范式。给定一个包含 $m$次用户交互的序列，通过指定每次交互及其 preceding $n$次交互作为上下文，为每次交互构建一个独特的训练提示，将其作为预测目标。然而，滑动窗口范式导致整体复杂度为$O(mn^2)$，与用户交互的长度成线性关系。因此，直接采用这种策略训练LLMs可能导致随着交互长度的增加而出现不可接受的高昂训练成本。为了缓解计算低效性，我们提出了一种新颖的训练范式，称为动态目标隔离（DTI），它结构性地并行化了$k$（其中$k >> 1$）个目标交互的训练。此外，我们确定了两个主要瓶颈——隐藏状态泄露和位置偏差过拟合——这些瓶颈限制了DTI只能扩展到较小的$k$值（例如，5），并提出了一个计算轻量级的解决方案来有效解决每个问题。通过在三个广泛采用的公共CTR数据集上进行的大量实验，我们经验性地表明，DTI将训练时间平均减少了$\textbf{92\%}$（例如，从$70.5$小时减少到$5.31$ 小时），同时没有牺牲CTR预测性能。|
|**2025-03-02**|**Toward Stable and Consistent Evaluation Results: A New Methodology for Base Model Evaluation**|Hongzhi Luan et.al.|[2503.00812](http://arxiv.org/abs/2503.00812)|null|本文提出了在评估基础模型（未经后训练）时存在的两个关键问题：（1）训练过程中的不稳定评估：在预训练的早期阶段，模型缺乏满足要求回答问题的能力，导致评估结果不稳定。这种不稳定性使得难以得出指导训练的可靠结论，尤其是在数据消融和缩放规律等关键实验中。（2）基础模型与指令模型的差异：与相应的指令模型相比，基础模型的评估性能通常较差。这一差距对评估一个评估表现更好的基础模型是否能真正导致更好的指令模型提出了挑战。为了解决这些问题，我们提出了以基础模型为导向的系统评估方法（BOSE），这是一种专门设计来优化基础模型评估的方法。具体来说，BOSE引入了两个关键创新：用于开放式任务的上下文轻量级指令提示（ICLiP）和用于具有候选选项的多选题的空白ppl，将标准困惑度（ppl）指标转化为填空格式，以减轻早期评估的波动。此外，我们首先提出了肯德尔等级相关系数来定量衡量评估的稳定性和一致性。实验结果表明，BOSE显著增强了预训练期间的评估稳定性和基础模型与指令模型之间的一致性，从而为LLMs的训练提供了更可靠的指导。|
|**2025-03-01**|**Functional multi-armed bandit and the best function identification problems**|Yuriy Dorn et.al.|[2503.00509](http://arxiv.org/abs/2503.00509)|null|赌徒优化通常指的是一类具有有限反馈的在线优化问题，即决策者仅使用当前点的目标值来做出新的决策，并且无法访问目标函数的梯度。尽管这个名字准确地捕捉到了反馈的限制，但它某种程度上具有误导性，因为它与多臂老虎机（MAB）问题类没有任何关联。我们提出了两类新问题：函数型多臂老虎机问题（FMAB）和最佳函数识别问题。它们分别是多臂老虎机问题和最佳臂识别问题的修改版，其中每个臂代表一个未知的黑盒函数。这些问题类对于模拟现实世界问题（如竞争性LLM训练）来说是一个惊人的好选择。为了解决这些类的问题，我们提出了一种新的简化方案来构建UCB型算法，即基于具有已知收敛率的非线性优化算法的F-LCB算法。我们基于基本算法的收敛率提供了该简化方案的后悔上限。我们添加了数值实验，以展示所提方案的性能。|
|**2025-02-28**|**ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs**|Hao Ge et.al.|[2502.21231](http://arxiv.org/abs/2502.21231)|null|扩展长上下文能力对于大型语言模型（LLMs）至关重要。为了在长上下文训练中分散多个设备上的内存消耗，通常使用跨数据分区（即数据并行）和跨数据分区（即上下文并行）。当前的训练框架主要将这两种技术视为正交的，并建立静态通信组来组织设备作为静态网格（例如，二维网格）。然而，LLM训练的序列长度通常不同，无论是文本、多模态还是强化学习。数据异质性与静态网格之间的不匹配导致冗余通信和不平衡计算，降低了训练效率。在本工作中，我们引入了ByteScale，这是一个高效、灵活且可扩展的LLM训练框架，用于大规模混合训练长序列和短序列。ByteScale的核心是一种新颖的并行策略，即混合数据并行（HDP），它通过动态网格设计将跨数据分区和跨数据分区统一起来。特别是，我们构建了一个通信优化器，通过数据感知的细粒度分片和动态通信消除短序列的冗余通信，并通过选择性卸载进一步压缩长序列的通信成本。此外，我们还开发了一个平衡调度器，通过并行感知的数据分配来减轻不平衡计算。我们在一个拥有超过12,000个GPU的生产集群上，对7B到141B的模型大小、256K到2048K的上下文长度进行了ByteScale的评估。实验结果表明，ByteScale的性能优于最先进的训练系统，最高可达7.89倍。|
|**2025-02-28**|**ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer**|Omer Goldman et.al.|[2502.21228](http://arxiv.org/abs/2502.21228)|null|为了在多种语言中实现公平的性能，多语言大型语言模型（LLMs）必须能够超越获取知识时的语言来抽象知识。然而，现有文献中缺乏可靠的方法来衡量LLMs的跨语言知识迁移能力。为此，我们提出了ECLeKTic，这是一个多语言闭卷问答（CBQA）数据集，以简单、黑盒的方式评估跨语言知识迁移。我们通过控制12种语言中维基百科文章的存在与否，检测到了语言间信息覆盖不均的情况。我们使用源语言生成寻求知识的问题，答案出现在相关的维基百科文章中，并将这些问题翻译成所有其他11种语言，这些语言的维基百科中缺乏相应的文章。假设维基百科反映了LLMs训练数据中的突出知识，为了解决ECLeKTic的CBQA任务，模型需要在语言间迁移知识。通过实验8个LLMs，我们发现即使SOTA模型能够很好地预测相同语言中的查询答案，它们也很难有效地在语言间共享知识。|
|**2025-02-27**|**Stochastic Rounding for LLM Training: Theory and Practice**|Kaan Ozkara et.al.|[2502.20566](http://arxiv.org/abs/2502.20566)|null|随着大型语言模型（LLMs）的参数规模扩展到数百亿，对高效训练方法的需求——在保证计算速度的同时减少内存使用，且不牺牲精度——比以往任何时候都更加迫切。近年来，提出了各种混合精度策略，这些策略涉及优化组件的不同精度级别，以在最小化精度损失的情况下提高训练速度。然而，这些策略通常需要手动调整，且缺乏理论依据。在本工作中，我们利用随机舍入（SR）来解决使用低精度表示进行训练时的数值误差。我们提供了在利用SR时，Adam优化器下隐式正则化和收敛的理论分析。从这些分析中获得见解后，我们将之前的BF16 + SR策略扩展到分布式环境，从而提高了大规模训练的稳定性和性能。来自预训练模型（参数量高达67亿）的实证结果表明，我们的BF16 + SR策略首次优于（BF16，FP32）混合精度策略，实现了更好的验证困惑度，吞吐量高达1.54倍，内存使用量减少30%。|
|**2025-02-27**|**AutoHete: An Automatic and Efficient Heterogeneous Training System for LLMs**|Zihao Zeng et.al.|[2503.01890](http://arxiv.org/abs/2503.01890)|null|基于Transformer的大语言模型（LLM）在序列建模和文本生成方面展现了非凡的能力，其性能提升与模型规模的增加呈正比。然而，GPU内存的限制限制了众多研究者对LLM训练的访问。现有的异构训练方法虽然显著扩大了可训练模型的规模，但引入了大量的通信开销和CPU负载。在本研究中，我们提出了AutoHete，一个适用于单GPU和多GPU环境的自动高效异构训练系统。AutoHete根据具体的硬件配置和LLM训练需求，动态调整激活检查点、参数卸载和优化器卸载。此外，我们还设计了一个基于优先级的调度机制，以最大化训练迭代间操作的叠加，提高吞吐量。与最先进的异构训练系统相比，AutoHete在各种模型规模和训练配置上实现了1.32倍至1.91倍的吞吐量提升。|
|**2025-02-26**|**The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training**|Jinbo Wang et.al.|[2502.19002](http://arxiv.org/abs/2502.19002)|null|本文由多个构建模块组成，如嵌入层、归一化层、自注意力机制和点对点前馈网络。因此，了解这些模块之间的差异和相互作用非常重要。在本文中，我们揭示了这些模块之间存在明显的锐度差异，这种差异在训练早期出现，并且在整个训练过程中持续存在。受此发现启发，我们提出了块状学习率（Blockwise LR）策略，该策略根据每个模块的锐度调整学习率，从而加速大型语言模型（LLM）的预训练。通过将块状学习率集成到AdamW中，我们实现了与标准AdamW相比的更低终端损失和近2倍的加速。我们在GPT-2和LLaMA上展示了这种加速，模型规模从0.12B到1.1B不等，数据集包括OpenWebText和MiniPile。最后，我们将块状学习率集成到最近提出的内存高效变体Adam-mini（张等人，2024年）中，实现了2倍的加速和2倍的内存节省。这些结果强调了利用锐度差异来提高LLM训练潜力的可能性。|
|**2025-02-26**|**LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training via Efficient Zeroth-Order Adaptive SAM**|Yehonathan Refael et.al.|[2502.19571](http://arxiv.org/abs/2502.19571)|null|我们研究了旨在在严格的计算和内存硬件约束下提高准确性和泛化能力的鲁棒参数高效微调（PEFT）技术，特别关注大型语言模型（LLMs）。现有的PEFT方法往往缺乏鲁棒性，且无法在多样化的任务中有效泛化，导致在实际场景中表现不佳。为了解决这个问题，我们提出了一种名为AdaZo-SAM的新的高计算效率框架，它结合了Adam和Sharpness-Aware Minimization（SAM），并且每次迭代只需要进行单梯度计算。这是通过使用随机零阶估计来找到SAM的上升扰动实现的。我们为AdaZo-SAM提供了收敛保证，并展示了它提高了最先进的PEFT方法的泛化能力。此外，我们设计了一种名为LORENZA的低秩梯度优化方法，它是AdaZo-SAM的内存高效版本。LORENZA利用随机SVD方案来高效地计算子空间投影矩阵，并在选定的子空间上应用优化步骤。这项技术实现了具有自适应低秩梯度更新的全参数微调，达到了与梯度低秩投影方法相同的内存消耗减少。我们为LORENZA提供了收敛分析，并展示了它在预训练和微调LLMs方面的优势。|
|**2025-02-25**|**NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms**|Yashan Wang et.al.|[2502.18008](http://arxiv.org/abs/2502.18008)|null|我们介绍了NotaGen，这是一个符号音乐生成模型，旨在探索生产高质量古典乐谱的潜力。受到大型语言模型（LLMs）成功的启发，NotaGen采用了预训练、微调和强化学习范式（以下简称LLM训练范式）。它在160万首音乐上进行预训练，然后根据“时期-作曲家-乐器”提示对大约9000首高质量古典作品进行微调。对于强化学习，我们提出了CLaMP-DPO方法，该方法在不要求人工标注或预定义奖励的情况下，进一步提高了生成质量和可控性。我们的实验证明了CLaMP-DPO在具有不同架构和编码方案的符号音乐生成模型中的有效性。此外，主观的A/B测试表明，NotaGen在人类作品基准模型上表现更优，极大地推进了符号音乐生成中的音乐美学。|
|**2025-02-24**|**Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam**|Tianjin Huang et.al.|[2502.17055](http://arxiv.org/abs/2502.17055)|**[link](https://github.com/tianjinyellow/stablespam)**|**本文全面评估了针对4位训练最近提出的几种优化器，发现低位精度放大了对学习率的敏感性，并经常导致梯度范数不稳定，在高学习率下导致发散。在这些优化器中，SPAM（一种具有动量重置和峰值梯度裁剪功能的最新优化器）在各种位级别上实现了最佳性能，但难以稳定梯度范数，需要仔细调整学习率。为了解决这些局限性，我们提出了Stable-SPAM，它结合了增强的梯度归一化和裁剪技术。具体来说，Stable-SPAM（1）通过跟踪其历史最大值自适应地更新峰值梯度的裁剪阈值；（2）根据其历史 $l_2$ -范数统计对整个梯度矩阵进行归一化；（3）从SPAM继承动量重置，定期重置Adam的第一和第二矩，以减轻峰值梯度的累积。大量实验表明，Stable-SPAM有效地稳定了4位LLM训练中的梯度范数，与Adam和SPAM相比，性能更优。值得注意的是，我们用Stable-SPAM训练的4位LLaMA-1B模型在困惑度方面优于用Adam训练的BF16 LLaMA-1B模型高达2倍。此外，当两种模型都在4位下进行训练时，Stable-SPAM达到了与Adam相同的损失，但所需的训练步骤仅为其一半左右。代码可在https://github.com/TianjinYellow/StableSPAM.git上找到。**|
|**2025-02-24**|**Muon is Scalable for LLM Training**|Jingyuan Liu et.al.|[2502.16982](http://arxiv.org/abs/2502.16982)|**[link](https://github.com/KellerJordan/Muon)**|最近，基于矩阵正交化的 Muon 优化器在训练小型语言模型方面表现出色，但其扩展到更大模型的可行性尚未得到证实。我们确定了提升 Muon 扩展性的两个关键技术：（1）添加权重衰减；（2）仔细调整每个参数的更新尺度。这些技术使得 Muon 能够在大型训练中无需调整超参数即可直接使用。扩展性实验表明，与 AdamW 相比，在计算最优训练条件下，Muon 实现了约 2 倍的计算效率。基于这些改进，我们引入了 Moonlight，这是一个使用 Muon 训练的 3B/16B 参数混合专家（MoE）模型，使用 5.7T 个标记进行训练。我们的模型改进了当前的帕累托前沿，与先前模型相比，在训练 FLOPs 更少的情况下实现了更好的性能。我们开源了内存最优且通信高效的分布式 Muon 实现。我们还发布了预训练、指令微调和中间检查点，以支持未来的研究。|
|**2025-02-22**|**Echo: A Large Language Model with Temporal Episodic Memory**|WenTao Liu et.al.|[2502.16090](http://arxiv.org/abs/2502.16090)|null|关于大型语言模型（LLMs）的研究在数学、编程和文学创作等领域表现出卓越的性能。然而，大多数研究都集中在基于语义记忆的问答上，忽视了LLMs处理与情景记忆（EM）相关查询的潜力。这种疏忽导致了在需要EM的应用中，如情感陪伴、个人AI助手和AI教师等领域的性能不佳。为了解决这一差距，我们引入了Echo，这是一个增强了时间情景记忆的LLMs。我们提出了一种多智能体数据生成框架，该框架指导模型生成多轮、复杂情景的情景记忆对话数据（EM-Train）。创新地将时间信息纳入LLMs的训练过程中，并使用EM-Train对Echo进行训练。此外，我们开发了一个专门设计用于评估LLMs情景记忆能力的EM-Test基准。EM-Test评估了跨越各种时间跨度和国度难度的性能，提供了多轮情景记忆对话的全面评估。我们的实验表明，Echo在EM-Test上显著优于最先进的LLMs。此外，定性分析揭示了Echo展现出类似人类情景记忆能力的潜力。我们将开源所有数据集、代码和模型权重。|
|**2025-02-21**|**Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training**|Jaydeep Borkar et.al.|[2502.15680](http://arxiv.org/abs/2502.15680)|**[link](https://github.com/jaydeepborkar/Assisted-Memorization)**|**由于个人身份信息（PII）的敏感性，其所有者可能有权控制其在大型语言模型（LLM）训练中的包含或请求其删除。除此之外，PII可能会因为数据集的演进式整理技术、因重新训练而新抓取或因包含在新的下游微调阶段而添加或删除。我们发现，PII的记忆量和易记性是模型在训练管道中演变的一个动态属性，并依赖于常见的设计选择。我们描述了三种新颖现象：（1）在训练后期出现的类似PII可以引发先前看到的序列的记忆，我们称之为辅助记忆，这在我们的设置中是一个重要因素（高达1/3）；（2）添加PII可以显著增加其他PII的记忆（在我们的设置中，高达约7.5倍）；（3）删除PII可能导致其他PII被记忆。模型创建者在训练模型时应该考虑这些一阶和二阶隐私风险，以避免新PII重复的风险。**|
|**2025-02-21**|**Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models**|Ya Wang et.al.|[2502.15499](http://arxiv.org/abs/2502.15499)|**[link](https://github.com/kaihemo/sdd)**|**在大型语言模型（LLMs）的预训练过程中，训练稳定性是一个持续存在的挑战，尤其是在Post-Norm Transformers等易于发生梯度爆炸和梯度消失的架构中。在本文中，我们提出了一种名为Scale-Distribution Decoupling（SDD）的新方法，通过显式地解耦全连接层中权重矩阵的规模和分布来稳定训练。SDD通过应用归一化机制来调节激活值，并使用可学习的缩放向量来保持良好的梯度条件，有效防止了梯度爆炸和梯度消失。这种分离提高了优化效率，尤其是在深度网络中，通过确保稳定的梯度传播。实验结果表明，我们的方法在各种LLM架构中稳定了训练，并在不同的归一化配置下优于现有技术。此外，所提出的方法轻量级且与现有框架兼容，使其成为稳定LLM训练的实用解决方案。代码可在https://github.com/kaihemo/SDD上找到。**|
|**2025-02-20**|**SR-LLM: Rethinking the Structured Representation in Large Language Model**|Jiahuan Zhang et.al.|[2502.14352](http://arxiv.org/abs/2502.14352)|null|结构化表示，如抽象意义表示（AMR），长期以来在计算语言学中扮演着关键角色。然而，在大型语言模型（LLMs）时代，它们的作用仍然模糊。最初尝试通过零样本设置将结构化表示集成到LLMs中，结果并不理想。我们假设这种下降源于结构信息以LLMs训练语料库不熟悉的代码格式传递给LLMs。因此，我们提出了SR-LLM，一个具有两种设置的创新框架，从无训练和有训练的角度探索将结构化表示与LLMs集成的一种更优方式。前者通过LLM提示中的自然语言描述整合结构信息，而其对应版本则通过在语言描述的结构化表示上微调来增强模型的推理能力。在广泛的后处理数据集上观察到性能提升，PAWS中的特别显著增益为3.17%和12.38%。据我们所知，这项工作代表了利用结构化表示可以显著增强LLMs推理能力的开创性演示。我们希望我们的工作为通过结构数据增强LLMs的推理和互操作性提供启示并鼓励未来的研究。|
|**2025-02-20**|**Multi-Faceted Studies on Data Poisoning can Advance LLM Development**|Pengfei He et.al.|[2502.14182](http://arxiv.org/abs/2502.14182)|**[link](https://github.com/PengfeiHePower/awesome-LLM-data-poisoning)**|**大型语言模型（LLMs）的生命周期远比传统机器学习模型复杂，涉及多个训练阶段、多样化的数据来源和多样的推理方法。尽管先前对数据中毒攻击的研究主要集中在LLMs的安全漏洞上，但这些攻击在实际中面临重大挑战。安全的数据收集、严格的数据清洗以及LLMs训练的多阶段特性使得注入中毒数据或可靠地影响LLMs行为变得困难。鉴于这些挑战，这篇立场论文提出了重新思考数据中毒的作用，并认为对数据中毒的多方面研究可以推动LLMs的发展。从威胁角度来看，数据中毒攻击的实用策略可以帮助评估和解决LLMs的真正安全风险。从可信度角度来看，数据中毒可以被用来构建更健壮的LLMs，通过揭示和减轻隐藏的偏见、有害输出和幻觉。此外，从机制角度来看，数据中毒可以为LLMs提供宝贵的见解，尤其是数据与模型行为之间的相互作用，推动对它们内在机制的更深入理解。**|
|**2025-02-19**|**Aligned Multi Objective Optimization**|Yonathan Efroni et.al.|[2502.14096](http://arxiv.org/abs/2502.14096)|null|截至目前，多目标优化文献主要关注相互冲突的目标，研究帕累托前沿，或者要求用户平衡权衡。然而，在机器学习实践中，存在许多不存在此类冲突的场景。来自多任务学习、强化学习和LLMs训练的最新发现表明，相关的多样化任务可以同时提升各个目标的表现。尽管有这些证据，但这种现象尚未从优化角度进行考察。这导致缺乏通用的基于梯度的方法，无法扩展到具有大量相关目标的情况。为了解决这一差距，我们引入了对齐的多目标优化框架，为这一设置提出了新的算法，并提供了其相比朴素方法具有优越性能的理论保证。|
|**2025-02-18**|**R.R.: Unveiling LLM Training Privacy through Recollection and Ranking**|Wenlong Meng et.al.|[2502.12658](http://arxiv.org/abs/2502.12658)|**[link](https://github.com/meng-wenlong/rr)**|**大型语言模型（LLMs）存在重大的隐私风险，由于隐式记忆，可能会泄露训练数据。现有的隐私攻击主要集中于成员推理攻击（MIAs）或数据提取攻击，但在LLMs的训练数据中重建特定的个人信息（PII）仍然具有挑战性。在本文中，我们提出了R.R.（回忆与排序），一种新颖的两步隐私窃取攻击，使得攻击者能够从已经对个人信息（PII）实体进行掩盖的清洗训练数据中重建PII实体。在第一阶段，我们引入了一种名为“回忆”的提示范式，指导LLM重复一个被掩盖的文本并填充掩盖部分。然后我们可以使用PII标识符来提取回忆出的PII候选者。在第二阶段，我们设计了一种新的标准来评分每个PII候选者并对它们进行排序。受成员推理的启发，我们利用参考模型作为对我们标准的校准。在三个流行的PII数据集上的实验表明，R.R.与基线相比实现了更好的PII识别性能。这些结果突显了即使训练数据已被清洗，LLMs仍然容易受到PII泄露的脆弱性。我们已在链接处发布了R.R.的复制品包。**|
|**2025-02-17**|**Efficient Response Generation Method Selection for Fine-Tuning Large Language Models**|Xuan Ren et.al.|[2502.11779](http://arxiv.org/abs/2502.11779)|null|在微调大型语言模型（LLM）的训练数据通常是输入-输出对的格式。然而，对于许多任务，对于相同的输入可能存在多个同等有效的输出变体。近期的研究观察到，训练中使用的输出变体选择可能会影响模型的表现。这引发了一个重要问题：我们如何从众多可能的响应生成策略选项中生成最有效的输出？而不是依赖于传统且资源密集的训练与评估方法，本文提出了一种可扩展的近似方法来估计从相同输入派生的小部分生成训练数据的质量。然后，我们评估这一小部分生成输出与我们要训练的目标模型之间的契合度。我们提供了一个大规模的基准，涵盖了多样化的基于推理的数据集来支持我们的研究。核心观点是，一个好的输出应该与目标LLM生成的输出非常相似。我们将这种“相似度”形式化为候选输出与从目标LLM采样的输出之间的期望对齐分数。我们将这一测量与先前文献中使用的困惑度指标联系起来，并证明利用基于对齐的指标可以提供对模型性能的更好预测。使用这种策略，我们可以评估每个响应生成策略选项生成输出的一个小子集，然后选择最有效的策略。我们表明，使用所选策略生成数据训练的LLM在许多情况下会导致显著的性能提升。|
|**2025-02-17**|**Understanding Silent Data Corruption in LLM Training**|Jeffrey Ma et.al.|[2502.12340](http://arxiv.org/abs/2502.12340)|null|随着大型语言模型（LLM）训练规模的扩大，一种新兴的故障是静默数据损坏（SDC），即硬件在没有明确故障信号的情况下产生错误的计算。在这项工作中，我们首次通过比较健康的生产节点和表现出SDC的不健康节点之间的模型训练，研究了现实世界中的SDC对LLM训练的影响。在云计算平台的帮助下，我们访问了被自动化车队管理从生产中清除的不健康节点。通过XLA编译器的确定性执行和我们所提出的同步机制，我们在三个层面上隔离和分析SDC错误对这些节点的影响：在每个子模块计算、单个优化器步骤和训练周期中。我们的结果表明，SDC对计算的影响在不同不健康节点上有所不同。尽管在大多数情况下，SDC对子模块计算和梯度的扰动相对较小，但SDC可能导致模型收敛到具有不同权重甚至导致训练损失激增的不同最优解。我们的分析有助于进一步理解和减轻SDC的影响。|
|**2025-02-17**|**Learning to Reason at the Frontier of Learnability**|Thomas Foster et.al.|[2502.12272](http://arxiv.org/abs/2502.12272)|null|强化学习现在被广泛采用作为大型语言模型训练的最后阶段，尤其是在数学问题等推理式任务中。通常，模型在单个训练步骤中尝试每个问题多次，并试图从成功和失败中学习。然而，我们通过在两个广泛使用的数据集上使用两种流行的算法（PPO和VinePPO）进行训练，发现许多问题要么在所有尝试中都被解决——这意味着它们已经被学习——要么一个也解决不了——没有提供有意义的训练信号。为了解决这个问题，我们从强化学习文献中借鉴了一种方法——可学习性采样，并将其应用于LLM训练的强化学习阶段。我们的课程优先考虑那些成功方差高的题目，即那些代理有时成功但并不总是成功的题目。我们的研究结果证明，这种课程在多个算法和数据集上持续提高训练性能，为LLMs中更高效和有效的强化学习铺平了道路。|
|**2025-02-16**|**Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest**|Letian Peng et.al.|[2502.11275](http://arxiv.org/abs/2502.11275)|**[link](https://github.com/komeijiforce/cuckoo)**|**大规模的高质量数据，包括预训练的原始文本和训练后的标注数据，被精心准备来培育高级大型语言模型（LLMs）。相比之下，对于信息提取（IE）而言，预训练数据，如BIO标记的序列，难以扩大规模。我们表明，IE模型可以通过将下一标记的\emph{预测}重新构造成对上下文中已存在的标记的\emph{提取}来成为LLM资源的免费搭车者。具体来说，我们提出的下一标记提取（NTE）范式学习了一个通用的IE模型，名为\emph{Cuckoo}，该模型将来自LLM预训练和训练后数据的1.026亿条提取数据进行了转换。在少量样本设置下，Cuckoo能够有效适应传统的和复杂的指令跟随IE，其性能优于现有的预训练IE模型。作为免费搭车者，Cuckoo可以自然地随着LLM数据准备方面的持续进步而进化，无需额外的人工努力即可从LLM训练流程的改进中受益。**|
|**2025-02-16**|**Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training**|Yao-Ching Yu et.al.|[2502.11191](http://arxiv.org/abs/2502.11191)|null|大型语言模型（LLMs）在金融、法律和医学等特定领域取得了显著进展。然而，在网络安全领域，我们发现开源数据集相对匮乏，尤其是高质量的网络安全预训练语料库，尽管许多研究表明LLMs的知识主要是在预训练阶段获得的。为了解决这个问题，我们提出了一套全面的语料库，涵盖了所有主要的训练阶段，包括预训练、指令微调和带有网络安全特定自我反思数据的推理蒸馏。广泛的消融研究表明，它们在公共网络安全基准测试中的有效性。特别是，在我们的数据集上进行持续预训练使综合得分提高了15.88%，而推理蒸馏使得安全认证（CISSP）提高了10%。我们将根据ODC-BY和MIT许可证发布所有数据集和训练好的网络安全LLMs，以鼓励社区进一步的研究。要访问所有数据集和模型权重，请参考https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243。|
|**2025-02-16**|**DreamDDP: Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization**|Zhenheng Tang et.al.|[2502.11058](http://arxiv.org/abs/2502.11058)|null|大型语言模型（LLMs）的增长增加了跨多个数据中心的多个GPU加速分布式训练的挑战。此外，对数据隐私和数据耗尽问题的担忧使得对地理分布式数据中心产生了浓厚兴趣。在地理分布式数据并行训练（DDP）中使用随机梯度下降（S-SGD）进行通信是低带宽环境中的主要瓶颈。局部SGD通过减少同步频率来减轻通信开销，近期研究已成功将其应用于地理分布式预训练LLMs。然而，我们发现其模型同步机制阻止了通信和计算的并行执行，导致系统失去了并行执行通信和计算的机会。  为了克服这一限制，我们通过层级解耦模型同步来扩展局部SGD的设计空间。在每个迭代中，仅同步部分层而不是在特定次数迭代后同步整个模型。利用这种方法，我们引入了DreamDDP，一个用于加速低带宽分布式训练的训练框架，具有三个关键创新：（1）具有与S-SGD相当收敛速率保证的部分局部SGD；（2）在不占用额外GPU内存的情况下，重叠参数同步与计算；（3）通过识别和利用三个特性，根据层间通信和计算时间的细粒度分析来调度通信和计算，从而减少训练时间。在32个GPU上使用包括ResNet-18、ResNet-50、GPT-2和Llama-2在内的杰出深度学习模型进行的实证评估表明，DreamDDP增强了局部SGD（以及Adam）的收敛特性，并且与领先的基线方法相比，实现了从1.49倍到3.91倍的速度提升。|
|**2025-02-14**|**POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning**|Jiawei Cheng et.al.|[2502.10038](http://arxiv.org/abs/2502.10038)|null|POI表示学习在处理与用户移动数据相关的任务中起着至关重要的作用。最近的研究表明，通过丰富POI表示的多模态信息可以显著提升其任务性能。之前，融入POI表示中的文本信息通常仅涉及POI类别或签到内容，导致现有方法中文本特征相对较弱。相比之下，在大量文本数据上训练的大型语言模型（LLMs）被发现拥有丰富的文本知识。然而，利用这种知识来增强POI表示学习面临两个关键挑战：首先，如何有效地从LLMs中提取与POI相关的知识；其次，如何将提取的信息整合以增强POI表示。为了解决这些挑战，我们提出了POI-Enhancer，一个可移植的框架，利用LLMs来改进经典POI学习模型产生的POI表示。我们首先设计了三个专门的提示，以有效地从LLMs中提取语义信息。然后，双特征对齐模块增强了提取信息的质量，而语义特征融合模块则保留了其完整性。交叉注意力融合模块随后将此类高质量信息完全自适应地整合到POI表示中，多视角对比学习进一步将这些表示中注入人类可理解的语义信息。在三个真实世界数据集上的大量实验证明了我们框架的有效性，显示出所有基线表示的显著改进。|
|**2025-02-13**|**On LLM-generated Logic Programs and their Inference Execution Methods**|Paul Tarau et.al.|[2502.09209](http://arxiv.org/abs/2502.09209)|null|大型语言模型（LLMs）在PB级数据上训练，是迄今为止积累和提炼的知识的密集压缩库。本文研究了提取这种知识为几类逻辑程序的技术，包括命题Horn子句、双重Horn子句、关系三元组和确定性子句语法。将这种知识以逻辑程序的形式呈现，可以启用合理的推理方法，以验证LLM输出与其预期用途的一致性，并扩展其推理能力。我们研究了生成程序的新的执行方法，包括针对存储在向量数据库中的LLM生成内容的可导事实的软统一，以及支持与大型LLM生成程序推理的最小模型计算的GPU加速。|
|**2025-02-12**|**Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers**|Siddharth Singh et.al.|[2502.08145](http://arxiv.org/abs/2502.08145)|null|训练和微调具有数百亿至数千亿参数的大型语言模型（LLMs）需要数万个GPU和高度可扩展的软件堆栈。在这项工作中，我们提出了一种新型的四维混合并行算法，该算法在一个高度可扩展、便携、开源的框架AxoNN中实现。我们描述了AxoNN中的一些性能优化，以提高矩阵乘法内核性能、将非阻塞集体操作与计算重叠，以及性能建模以选择性能最优的配置。这些优化导致了在Perlmutter（620.1 Petaflop/s）、Frontier（1.381 Exaflop/s）和Alps（1.423 Exaflop/s）上训练GPT风格的变压器模型前所未有的扩展性和峰值flop/s（bf16）。虽然LLMs的能力随着可训练参数数量的增加而提高，但由训练数据记忆带来的隐私和版权风险也相应增加，这可能导致在推理时泄露敏感或私人信息。我们通过探索“灾难性记忆”这一副作用来强调这一规模效应，其中模型足够大，可以在单次遍历中记忆训练数据，并提出了一种防止这种情况的方法。作为这项研究的一部分，我们展示了在Frontier上使用AxoNN对4050亿参数的LLM进行微调。|
|**2025-02-10**|**Gradient Multi-Normalization for Stateless and Scalable LLM Training**|Meyer Scetbon et.al.|[2502.06742](http://arxiv.org/abs/2502.06742)|null|训练大型语言模型（LLMs）通常依赖于像Adam（Kingma & Ba，2015）这样的自适应优化器，这些优化器存储额外的状态信息以加速收敛，但会带来显著的内存开销。近期的研究，如SWAN（Ma等，2024），通过消除对优化器状态的需求，并通过应用于瞬时梯度的多步预处理程序实现与Adam相当的性能来解决这个问题。受SWAN成功的启发，我们提出了一种新的框架，用于设计根据多个范数对随机梯度进行归一化的无状态优化器。为了实现这一点，我们提出了一种简单的交替方案来强制执行对这些范数的梯度归一化。我们表明，我们的程序可以产生，直到任意精度，问题的不动点，并且SWAN是我们方法的一个特例，具有精心选择的范数，从而提供了对其设计的更深入理解。然而，SWAN的计算成本高昂的漂白/正交化步骤限制了其在大型LLMs中的实用性。利用我们的原则性观点，我们开发了一种更高效、可扩展且实用的无状态优化器。我们的算法放宽了SWAN的性质，显著降低了其计算成本，同时保留了其内存效率，使其适用于大规模模型的训练。在预训练具有高达10亿个参数的LLaMA模型上的实验表明，与Adam相比，速度提高了3倍，同时内存需求显著降低，优于其他内存高效的基线。|
|**2025-02-09**|**$μ$ nit Scaling: Simple and Scalable FP8 LLM Training**|Saaketh Narayan et.al.|[2502.05967](http://arxiv.org/abs/2502.05967)|null|使用8位浮点数（FP8）格式进行大型语言模型训练承诺了显著的效率提升，但降低的数值精度使得训练变得具有挑战性。目前，只有在愿意调整各种超参数、减少模型规模或接受计算动态缩放因子的开销的情况下，才可能使用FP8进行训练。我们展示了简单、可扩展的FP8训练方法，即使在大型模型规模下，也不需要动态缩放因子或特殊超参数。我们的方法，μnit Scaling（μS），还使得模型宽度间的简单超参数迁移、训练和推理中的数值匹配以及其他期望的特性成为可能。μnit Scaling易于实现，它基于对常见Transformer操作的原理分析，由一系列最小干预措施组成。我们通过训练从10亿到130亿参数的模型来验证我们的方法，所有隐藏线性层计算都在FP8下进行。我们在质量上达到了比更高精度基线相当的水平，同时训练速度提高了高达33%。|
|**2025-02-08**|**XPUTimer: Anomaly Diagnostics for Divergent LLM Training in GPU Clusters of Thousand-Plus Scale**|Weihao Cui et.al.|[2502.05413](http://arxiv.org/abs/2502.05413)|null|随着大型语言模型的快速普及，对高效的GPU训练集群的需求日益增长。然而，由于软件硬件交互的复杂性和训练异常的频繁发生，确保这些集群的高性能训练具有挑战性。由于现有的诊断工具针对特定问题进行了狭窄的定制，它们在解决跨越整个训练栈的异常方面的能力存在不足。为此，我们引入了XPUTimer，这是一个针对大规模分布式LLM训练的实时诊断框架。XPUTimer首先集成了一个轻量级的跟踪守护进程，以最小的开销监控关键代码段。此外，它还具备一个诊断引擎，该引擎采用新颖的内核内跟踪和全面的聚合指标，以有效地识别和解决异常。XPUTimer在八个月内部署到6000个GPU上，在整个训练栈上实现了显著的改进，验证了其在实际场景中的有效性。|
|**2025-02-07**|**Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books**|Sangmitra Madhusudan et.al.|[2502.05331](http://arxiv.org/abs/2502.05331)|**[link](https://github.com/Sangmitra-06/BookPAGE)**|**书籍虽然常常富含文化洞察力，但也可能映射出它们时代的社会偏见——这些偏见可能在大型语言模型（LLMs）的训练过程中被学习并延续。我们介绍了一种新的方法，使用微调的LLMs来追踪和量化这些偏见。我们开发了BookPAGE语料库，包含从1950年到2019年七十年间的593本虚构书籍，以追踪偏见的发展。通过在每个十年对书籍进行微调LLMs并使用针对性提示，我们研究了与性别、性取向、种族和宗教相关的偏见的变化。我们的发现表明，在特定十年书籍上训练的LLMs表现出与其时代相符的偏见，既有渐进的趋势也有显著的变化。例如，模型对女性领导角色的描绘从20世纪50年代到21世纪的百分比从8%增加到22%，1990年代有显著增长（从4%到12%），可能符合第三波女权主义的趋势。同性恋关系的提及从20世纪80年代到21世纪初显著增加（从0%到10%），反映了LGBTQ+可见性的增长。令人担忧的是，对伊斯兰教的负面描绘在21世纪初急剧上升（从26%到38%），可能反映了9/11事件后的情绪。重要的是，我们证明了这些偏见主要来自书籍的内容，而不是模型的结构或初始训练。我们的研究通过连接AI、文学研究和社会科学研究，为社会偏见趋势提供了一个新的视角。**|
|**2025-02-07**|**Efficient Knowledge Feeding to Language Models: A Novel Integrated Encoder-Decoder Architecture**|S Santosh Kumar et.al.|[2502.05233](http://arxiv.org/abs/2502.05233)|null|本文介绍了一种在预测过程中高效向语言模型（LLMs）注入知识的新方法，通过在统一框架内整合检索和生成过程。虽然检索增强生成（RAG）模型解决了LLMs训练数据缺口和知识限制的问题，但它受到令牌限制和检索系统准确度依赖的阻碍。我们提出的架构采用上下文向量（ICV）来克服这些挑战。ICV通过使用LLMs的潜在嵌入来创建一个向量，以捕捉关键任务信息，从而重新解释上下文学习。然后，该向量用于改变LLMs的潜在状态，增强生成过程，而无需在提示中添加示例。ICV直接将信息整合到模型中，使其更有效地处理这些信息。我们广泛的实验评估表明，ICV在问答、信息检索和其他任务中优于标准上下文学习和微调。这种方法减轻了当前RAG模型的限制，并为处理大量和多样化的数据集提供了更稳健的解决方案。尽管参数使用比例较低，但我们的ICV增强模型在LLaMA-3、Gemma和Phi-3等模型上取得了具有竞争力的性能，显著降低了计算成本和内存需求。ICV缩短了提示长度，易于控制，超越了令牌限制，与微调相比计算效率更高。|
|**2025-02-06**|**InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers**|Chenchen Shou et.al.|[2502.03885](http://arxiv.org/abs/2502.03885)|null|大规模语言模型（LLM）的训练依赖于多维并行性，其中高带宽域（HBD）对于通信密集型并行性，如张量并行（TP）和专家并行（EP）至关重要。然而，现有的HBD架构在可扩展性、成本和容错性方面存在根本性的限制：以交换机为中心的HBD（例如，NVL-72）因可扩展成本过高而受限，而以GPU为中心的HBD（例如，TPUv3/Dojo）则因故障传播严重而受影响。TPUv4等交换机-GPU混合HBD通过利用光电路交换（OCS）采取折中方案，但在立方级（例如，64个TPU）的故障爆炸半径仍然很大。我们提出了InfinitePOD，这是一种以收发器为中心的HBD架构，它通过使用光电路交换（OCS）在收发器级别统一连接性和动态交换。通过在每个收发器内嵌入OCS，InfinitePOD实现了可重构的点对多点连接，允许拓扑适应为可变大小的环形。这种设计提供了：i）数据中心级别的可扩展性而不引发成本爆炸；ii）通过将故障隔离到单个节点来实现容错性；iii）对于无故障GPU的全带宽利用率。关键创新包括基于硅光子（SiPh）的低成本OCS收发器（OCSTrx）、与节点内/节点间通信协同设计的可重构k跳环拓扑，以及最大化GPU利用率同时最小化跨ToR数据中心网络流量的HBD-DCN编排算法。评估表明，InfinitePOD实现了NVL-72成本的31%，GPU浪费率接近零（比NVL-72和TPUv4低一个数量级以上），当节点故障率低于7%时，跨ToR流量接近零，并且与NVIDIA DGX（每个节点8个GPU）相比，模型FLOPs利用率提高了3.37倍。|
|**2025-02-05**|**Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization**|Chanhui Lee et.al.|[2502.02810](http://arxiv.org/abs/2502.02810)|null|近期在大语言模型（LLMs）领域的进步推动了通用LLMs在分子任务中的应用。虽然一些研究表明，经过微调的LLMs能够达到令人印象深刻的基准性能，但它们因缺乏对分子结构的根本理解而远未成为真正的通用分子LLMs。具体来说，当给予分子任务指令时，使用简单下一个标记预测训练的LLMs会赋予原始分子和负向干扰分子相似的可能性分数，揭示了它们缺乏对分子结构的理解，这对可靠和通用的分子LLMs至关重要。为了克服这一限制并获得真正的通用分子LLM，我们引入了一种新颖的多模态训练方法，该方法基于彻底的多模态指令调整以及在选定的和拒绝的图之间的分子结构偏好优化。在各种分子基准测试中，所提出的通用分子LLM（称为Mol-LLM）在大多数任务上达到了通用LLMs中的最先进性能，同时，在大多数情况下超过了或与最先进的专家LLMs相当。此外，Mol-LLM在反应预测任务中也表现出优越的泛化性能，证明了分子结构理解对泛化视角的影响。|
|**2025-02-04**|**Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search**|Maohao Shen et.al.|[2502.02508](http://arxiv.org/abs/2502.02508)|null|大型语言模型（LLMs）在各个领域展现了惊人的推理能力。最近的研究表明，增加测试时的计算量可以增强LLMs的推理能力。这通常涉及在推理时进行大量的采样，并由外部LLM验证器引导，形成一个两人系统。尽管有外部指导，但该系统的有效性证明了单个LLM处理复杂任务的可能性。因此，我们提出了一个新的研究问题：我们能否将搜索能力内化，从而从根本上增强单个LLMs的推理能力？这项工作探索了一个不同的方向，专注于针对自回归搜索（即具有自我反思和探索新策略的扩展推理过程）的LLMs进行后训练。为了实现这一点，我们提出了动作-思维链（COAT）推理和一种两阶段训练范式：1）一个小规模格式调整阶段，以内化COAT推理格式；2）一个大规模自我改进阶段，利用强化学习。我们的方法产生了Satori，这是一个基于开源模型和数据进行训练的70亿参数LLMs。大量的实证评估表明，Satori在数学推理基准测试中实现了最先进的性能，同时表现出对域外任务的强大泛化能力。代码、数据和模型将全部开源。|
|**2025-02-04**|**AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement**|Shivam Singh et.al.|[2502.02067](http://arxiv.org/abs/2502.02067)|**[link](https://github.com/sssshivvvv/adaptbot)**|**具有身体的人类助手通常被要求在新的场景中完成新的任务。一个根据已知食谱在厨房准备特定菜肴的代理可能被要求准备新的菜肴或在储藏室执行清洁任务。可能没有足够的资源，例如时间或标记的示例，来训练代理适应这些新情况。在许多领域受过大量知识训练的大型语言模型（LLMs）能够预测此类新任务和场景的抽象动作序列，尽管由于任务、代理或领域特定的约束，代理可能无法执行此动作序列。我们的框架通过利用LLM提供的通用预测以及编码在知识图谱（KG）中的先验领域特定知识来解决这些挑战，使代理能够快速适应新的任务和场景。机器人还需要根据需要征求和利用人类输入来完善其现有知识。基于在模拟域中的烹饪和清洁任务的实验评估，我们证明了LLM、KG和人类输入之间的相互作用与仅使用LLM输出相比，导致显著的性能提升。**|
|**2025-02-03**|**SubTrack your Grad: Gradient Subspace Tracking for Memory and Time Efficient Full-Parameter LLM Training**|Sahar Rajabi et.al.|[2502.01586](http://arxiv.org/abs/2502.01586)|null|由于大型语言模型（LLMs）的模型规模和优化器状态庞大，训练LLMs需要大量的时间和计算资源。为了克服这些挑战，最近的方法，如BAdam，通过部分权重更新来提高时间和内存效率，尽管有时会牺牲性能。其他方法，如GaLore，则专注于在优化内存使用的同时保持性能，但可能需要更高的时间复杂度。通过利用梯度的低秩结构和Grassmannian几何，我们提出了SubTrack-Grad，这是一种基于子空间跟踪的优化方法，通过结合估计误差和先前识别的子空间，有效地跟踪不断演变的梯度子空间。与GaLore相比，SubTrack-Grad提供了更好或相当的结果，而显著优于BAdam，尽管BAdam在时间效率上有所妥协。在GLUE任务上，SubTrack-Grad将墙时减少了高达20.57%（平均减少15%），在SuperGLUE任务上减少了高达65%（平均减少22%）。值得注意的是，对于3B参数模型，与全秩训练相比，GaLore的墙时增加了157%，而SubTrack-Grad仅增加了31%，这代表了49%的墙时减少，同时享受与GaLore相同的内存减少。|
|**2025-02-02**|**MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for Efficient LLM Training Across Morphologies**|Ehsaneddin Asgari et.al.|[2502.00894](http://arxiv.org/abs/2502.00894)|null|分词是自然语言处理（NLP）的基础，直接影响模型效率和语言忠实度。尽管字节对编码（BPE）在大型语言模型（LLMs）中得到广泛应用，但它往往忽视了词素边界，导致子词分词不理想，尤其是在形态丰富的语言中。我们引入了形态BPE，这是一种BPE的形态感知扩展，它将语言学结构整合到子词分词中，同时保持统计效率。此外，我们还提出了两个基于形态的评价指标：（i）形态一致性F1分数，它量化了词素共享与分词共享之间的一致性，有助于LLM训练收敛；（ii）形态编辑距离，它衡量了词素与分词在可解释性方面的对齐。在英语、俄语、匈牙利语和阿拉伯语上进行的实验，涵盖300M和1B参数的LLMs，表明形态BPE可以持续降低交叉熵损失，加速收敛并提高形态对齐分数。形态BPE与现有的LLM流程完全兼容，集成时需要的修改最少。形态BPE代码库和分词游乐场将在以下网址提供：https://github.com/llm-lab-org/MorphBPE 和 https://tokenizer.llm-lab.org|
|**2025-02-01**|**Enhancing Token Filtering Efficiency in Large Language Model Training with Collider**|Di Chai et.al.|[2502.00340](http://arxiv.org/abs/2502.00340)|null|本文提出了一种名为Collider的系统，旨在充分发挥在大语言模型（LLM）训练中token过滤的效率。该系统通过在所有层中过滤掉不重要的token的激活来维持稀疏性。此外，它还具备一个自动工作流程，将稀疏的GEMM（通用矩阵乘法）转换为降维的密集GEMM，以优化效率。在TinyLlama-1.1B、Qwen2.5-1.5B和Phi1.5-1.4B这三种LLM上的评估表明，当过滤掉40%的token时，Collider可以将反向传播时间减少高达35.1%，端到端训练时间减少高达22.0%。在TinyLlama在150亿token上的训练中进行的效用评估表明，与常规训练相比，Collider通过相对提高模型效用16.3%来维持token过滤的效用提升，并使用8个GPU将训练时间从4.7天减少到3.5天。Collider被设计为易于集成到现有的LLM训练框架中，使得已经使用token过滤的系统只需一行代码即可加速训练。|
|**2025-01-31**|**Scaling Laws for Differentially Private Language Models**|Ryan McKenna et.al.|[2501.18914](http://arxiv.org/abs/2501.18914)|null|随着大型语言模型（LLM）训练中规模定律的重要性日益凸显，因为它们可以通过规模预测性能提升，并为重要超参数选择提供指导，这些选择在其他情况下可能代价高昂。LLM还依赖于来自（有时敏感的）用户数据的大型、高质量训练数据集。在敏感用户数据上训练模型需要仔细的隐私保护，如差分隐私（DP）。然而，DP训练的动态与传统的训练方式显著不同，因此其规模定律尚未完全理解。在本研究中，我们建立了能够准确模拟DP LLM训练复杂性的规模定律，为许多场景提供了计算-隐私-效用权衡的完整图景以及最优训练配置。|
|**2025-01-31**|**Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities**|Yaping Chai et.al.|[2501.18845](http://arxiv.org/abs/2501.18845)|null|随着预训练语言模型规模和复杂性的增加，它们在许多应用中展现出了卓越的性能，但它们通常需要大量的训练数据集才能得到充分的训练。训练数据集不足可能会意外地导致模型过拟合，并无法应对复杂任务。在大规模语料库上训练的大型语言模型（LLMs）具有突出的文本生成能力，这提高了数据和数量的质量，并在数据增强中发挥着至关重要的作用。具体来说，在个性化任务中给出了独特的提示模板，以引导LLMs生成所需的内容。最近，基于检索的令人鼓舞的技术通过引入外部知识来提高LLMs在数据增强中的表达能力，使它们能够生成更多基于事实的数据。本调查对LLMs中的数据增强进行了深入分析，将技术分为简单增强、基于提示的增强、基于检索的增强和混合增强。我们总结了数据增强中的后处理方法，这极大地促进了增强数据的精炼，并使模型能够过滤掉不真实的内容。然后，我们提供了常见的任务和评估指标。最后，我们介绍了现有挑战和未来机遇，这些机遇有望进一步改善数据增强。|
|**2025-01-28**|**Optimizing Large Language Model Training Using FP4 Quantization**|Ruizhe Wang et.al.|[2501.17116](http://arxiv.org/abs/2501.17116)|null|随着大型语言模型（LLMs）训练计算需求的不断增长，需要更高效的方法。量化训练通过允许低比特算术运算来降低这些成本，是一种有希望的解决方案。虽然FP8精度已经证明了可行性，但由于量化误差显著和表示能力有限，利用FP4仍然是一个挑战。这项工作引入了第一个针对LLMs的FP4训练框架，通过两个关键创新解决了这些挑战：一个用于精确权重更新的可微分量化估计器和一个异常值钳位和补偿策略，以防止激活崩溃。为了确保稳定性，该框架集成了混合精度训练方案和向量量化。实验结果表明，我们的FP4框架在准确度上与BF16和FP8相当，最小化退化，并有效扩展到在多达100B个标记上训练的13B参数LLMs。随着支持FP4的下一代硬件的出现，我们的框架为高效的超低精度训练奠定了基础。|
|**2025-01-28**|**Fine-Tuned Language Models as Space Systems Controllers**|Enrico M. Zucchelli et.al.|[2501.16588](http://arxiv.org/abs/2501.16588)|null|大型语言模型（LLMs）或基础模型（FMs）是经过预训练的能够连贯地自动回归补全句子的变换器。在本文中，我们表明LLMs在经过一些额外的训练，即微调后，可以控制简化的空间系统。我们考察了相对较小的语言模型，其参数量在70亿到130亿之间。我们聚焦于四个问题：三维弹簧玩具问题、低推力轨道转移、低推力地月控制以及有动力下降导航。经过微调的LLMs能够通过生成足够精确的输出控制系统，这些输出是多维向量，精度高达10位有效数字。我们显示，对于几个问题，进行微调所需的数据量小于传统深度神经网络（DNNs）通常所需的数据量，并且微调后的LLMs擅长在训练数据集之外的泛化。此外，同一LLM可以使用来自不同问题的数据进行微调，与为单一应用训练的LLMs相比，性能下降非常轻微。这项工作旨在成为开发通用空间系统控制器的第一步。|
|**2025-01-27**|**LLM-powered Multi-agent Framework for Goal-oriented Learning in Intelligent Tutoring System**|Tianfu Wang et.al.|[2501.15749](http://arxiv.org/abs/2501.15749)|**[link](https://github.com/geminilight/gen-mentor)**|**智能辅导系统（ITSs）通过提供个性化的学习体验，颠覆了教育领域。然而，随着以目标为导向的学习在专业环境中变得越来越重要，强调高效实现特定目标的这种学习方法，现有的ITSs往往难以提供这种针对性的学习体验。在本文中，我们提出了GenMentor，这是一个由大型语言模型（LLM）驱动的多智能体框架，旨在在ITS中提供目标导向的个性化学习。GenMentor首先使用在自定义目标-技能数据集上微调的LLM，准确地将学习者的目标映射到所需的技能。在确定技能差距后，它利用一个综合且动态的学习者多方面状态配置文件驱动的演变优化方法，安排一个高效的学习路径。此外，GenMentor通过探索-草拟-整合机制定制学习内容，以符合个别学习者的需求。广泛的自动和人工评估证明了GenMentor在学习指导和内容质量方面的有效性。此外，我们已经在实践中部署了它，并将其实现为一个应用程序。与专业学习者的实际人类研究进一步突出了它在目标对齐和资源定位方面的有效性，从而提高了个性化水平。补充资源可在https://github.com/GeminiLight/gen-mentor获取。**|
|**2025-01-25**|**Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training**|Xunxin Cai et.al.|[2501.15108](http://arxiv.org/abs/2501.15108)|null|大型语言模型（LLMs）在生物医学领域的快速发展突显了它们潜力与现有开源标注文本数据集规模有限、质量通常较低之间的差距。此外，生物医学知识体系的固有复杂性极大地阻碍了弥合这一差距的努力。LLMs自身能否在克服这一限制中发挥关键作用？受此启发，我们在本研究中调查了这一挑战。我们提出了一种框架，该框架能够从广泛的科学文献中自动提取高质量的文本训练数据。我们的方法通过生物医学知识体系（通过医学主题词表MeSH）自我评估并生成与生物医学领域更紧密相关的问题。这个综合框架建立了一个自动化的工作流程，从而消除了手动干预的需要。此外，我们进行了全面的实验，以评估我们框架生成的数据对大小不同的下游语言模型的影响。与生命科学领域的预训练模型以及由GPT-4等强大闭源模型代表的高效模型相比，我们的方法在问答任务上显著提高了性能。值得注意的是，生成的AI-Ready数据集使得Llama3-70B基础模型在MedPrompt的帮助下，使用多倍参数数量超越了GPT-4。详细的案例研究和消融实验突出了我们框架中每个组件的重要性。|
|**2025-01-24**|**Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation**|Sadegh Mahdavi et.al.|[2501.14275](http://arxiv.org/abs/2501.14275)|**[link](https://github.com/dsl-lab/aops)**|**大型语言模型（LLMs）的进展激起了人们对它们解决奥林匹克级数学问题能力的研究兴趣。然而，这些模型的训练和评估受到可用数据集大小和质量有限的限制，因为为这类高级问题创建大规模数据需要大量来自人类专家的努力。此外，当前的基准测试容易受到污染，导致评估结果不可靠。在本文中，我们提出了一种自动化流程，该流程利用了《问题解决艺术》（AoPS）论坛丰富的资源，该论坛主要展示奥林匹克级问题和社区驱动的解决方案。使用开源LLMs，我们开发了一种从论坛中提取问答对的方法，从而产生了AoPS-Instruct，一个包含超过60万个高质量问答对的数据库。我们的实验表明，在AoPS-Instruct上微调LLMs可以提升它们在各种基准测试中的推理能力。此外，我们构建了一个自动流程，引入了LiveAoPSBench，这是一个基于最新论坛数据的具有时间戳的演变评估集，为评估LLM性能提供了一个抗污染的基准。值得注意的是，我们发现LLM性能随着时间的推移显著下降，这表明它们在旧例题上的成功可能源于预训练的暴露，而不是真正的推理能力。我们的工作提出了一种可扩展的方法来创建和维护用于高级数学推理的大规模、高质量数据集，为LLMs在这一领域的功能和局限性提供了宝贵的见解。我们的基准和代码可在https://github.com/DSL-Lab/aops上获取。**|
|**2025-01-22**|**Kimi k1.5: Scaling Reinforcement Learning with LLMs**|Kimi Team et.al.|[2501.12599](http://arxiv.org/abs/2501.12599)|null|使用下一词预测进行语言模型预训练已被证明在扩展计算方面非常有效，但其受限于可用训练数据量。通过扩展强化学习（RL）为人工智能的持续改进开辟了新的维度，并承诺大型语言模型（LLMs）可以通过学习使用奖励进行探索来扩展其训练数据。然而，先前发表的工作尚未产生具有竞争力的结果。鉴于这种情况，我们报告了我们最新的多模态LLM Kimi k1.5的训练实践，包括其RL训练技术、多模态数据配方和基础设施优化。长上下文扩展和改进的政策优化方法是我们的方法的关键组成部分，我们建立了一个简单有效的RL框架，而不依赖于更复杂的技术，如蒙特卡洛树搜索、价值函数和过程奖励模型。值得注意的是，我们的系统在多个基准和模态上实现了最先进的推理性能——例如，在AIME上达到77.5，在MATH 500上达到96.2，在Codeforces上达到94百分位数，在MathVista上达到74.9，与OpenAI的o1相匹配。此外，我们提出了有效的长到短方法，使用长-CoT技术来改进短-CoT模型，实现了最先进的短-CoT推理结果——例如，在AIME上达到60.8，在MATH500上达到94.6，在LiveCodeBench上达到47.3，大幅超越现有的短-CoT模型，如GPT-4o和Claude Sonnet 3.5（高达+550%）。|
|**2025-01-21**|**FOCUS: First Order Concentrated Updating Scheme**|Yizhou Liu et.al.|[2501.12243](http://arxiv.org/abs/2501.12243)|null|大型语言模型（LLMs）展现出惊人的性能，而提升它们的预训练过程似乎是进一步增强其能力的关键。基于Adam、学习率衰减和权重衰减的成功记录，我们假设预训练损失景观具有狭窄的谷地结构。通过在合成损失函数上的实验，我们发现当梯度查询噪声相对于谷地的尖锐度较高时，Adam的表现不如Signum，因为Adam会大幅度减少有效步长。这一观察促使我们开发了FOCUS，这是一种通过引入对移动平均参数的吸引力来增强Signum的优化器，使其在处理噪声的同时保持更大的步长。在训练GPT-2时，FOCUS证明比Signum更稳定，比Adam更快。这些结果表明，梯度噪声可能是LLM训练中一个被低估的限制因素，而FOCUS提供了有希望的解决方案。|
|**2025-01-20**|**Trustformer: A Trusted Federated Transformer**|Ali Abbasi Tadi et.al.|[2501.11706](http://arxiv.org/abs/2501.11706)|null|本文介绍了一种新颖的联邦学习方法，该方法在保持与现有最先进基线相当的有效性的同时，降低了通信开销。我们的方法通过在本地模拟全局模型来避免共享完整模型权重。我们对每个Transformer层应用k-means聚类，本地计算质心，然后将这些质心传输到服务器，而不是完整的权重或梯度。为了增强安全性，我们利用Intel SGX来安全地传输质心。在翻译任务上的评估表明，我们的方法在通信成本显著降低的同时，实现了与最先进基线相当的功效。这为Transformer模型提供了一种更高效且具有隐私保护性的联邦学习解决方案。|
|**2025-01-20**|**Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas**|Nishant Balepur et.al.|[2501.11549](http://arxiv.org/abs/2501.11549)|**[link](https://github.com/pinafore/alignment-personalization)**|**LLMs通过学习用户对提示的两个输出中哪一个更偏好来调整以遵循指令（对齐）。然而，这种偏好数据格式并没有传达用户为什么偏好选择或拒绝的响应，因此在这些数据集上训练的LLM无法根据不同的用户需求定制响应。为了揭示这些个性化的参数，我们应用了溯因推理对偏好数据进行处理，推断出可能偏好每个输出的用户需求和兴趣，即用户角色。我们通过两个步骤测试了这个想法：角色推断（PI）-通过溯因推理推断偏好选择或拒绝输出的用户角色，以及角色定制（PT）-训练模型根据PI推断的角色定制响应。我们发现：1）LLMs可以准确地推断角色，解释为什么不同的用户可能会偏好选择或拒绝的输出；2）通过PT对PI角色进行增强的偏好数据训练提升了个性化，使模型能够支持用户编写的角色；3）被拒绝的响应角色形成了更难的个性化评估，表明PT在帮助具有非典型偏好的用户方面比典型的对齐方法更有优势。我们主张对于个性化而言，采用溯因推理的观点，不仅要问哪个响应更好，还要问什么时候、为什么以及为了谁。**|
|**2025-01-19**|**GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation**|Shashikant Ilager et.al.|[2501.11006](http://arxiv.org/abs/2501.11006)|**[link](https://github.com/large-scale-sustainable-computing-lsc/green-code)**|大型语言模型（LLMs）正成为日常生活不可或缺的一部分，展现出它们在自然语言处理（NLP）任务中的巨大潜力。除了NLP之外，LLMs在软件开发任务中的应用也越来越广泛，例如代码补全、修改、错误修复和代码翻译。软件工程师广泛使用GitHub Copilot和Amazon Q等工具，通过高精度自动化任务和简化工作流程。尽管LLMs训练的资源消耗和能源强度经常被强调，但随着时间的推移，推理的资源消耗甚至可能更加密集，因为它是具有大量调用的持续过程。因此，为LLMs推理开发资源高效的替代方案对于可持续性至关重要。这项工作提出了GREEN-CODE，这是一个针对LLMs能源感知代码生成的框架。GREEN-CODE在LLMs推理过程中执行动态早期退出。我们训练了一个强化学习（RL）智能体，使其学会在准确性、延迟和能源消耗之间平衡权衡。我们的方法在两个开源LLMs Llama 3.2 3B和OPT 2.7B上进行了评估，使用了JavaCorpus和PY150数据集。结果显示，我们的方法在代码生成任务中平均降低了23-50%的能源消耗，同时没有显著影响准确性。|
|**2025-01-18**|**Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback**|Yen-Ting Lin et.al.|[2501.10799](http://arxiv.org/abs/2501.10799)|null|大语言模型（LLMs）最近在数学推理方面表现出显著的成功。尽管在思维链提示和自洽采样等方法的进步中取得了进展，但这些进展通常关注最终的正确性，而未确保底层推理过程的一致性和可靠性。本文介绍了Step-KTO，这是一个训练框架，它结合了过程级和结果级的二元反馈，以引导LLMs朝着更可信赖的推理轨迹发展。通过为中间推理步骤和最终答案提供二元评估，Step-KTO鼓励模型遵循逻辑进展而不是依赖表面的捷径。在具有挑战性的数学基准测试上的实验表明，Step-KTO显著提高了最终答案的准确性和中间推理步骤的质量。例如，在MATH-500数据集上，Step-KTO在Pass@1准确率方面相对于强基线实现了显著的提升。这些结果凸显了将步骤级过程反馈整合到LLM训练中的潜力，为更可解释和可靠的推理能力铺平了道路。|
|**2025-01-15**|**The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities**|Irina Bigoulaeva et.al.|[2501.08716](http://arxiv.org/abs/2501.08716)|**[link](https://github.com/ukplab/arxiv2025-inherent-limits-plms)**|**大型语言模型（LLMs）经过广泛的网络规模语料库训练，在各个任务上展现出惊人的能力，尤其是在规模扩大后。然而，即使是最先进的模型在某些情况下也难以应对，有时连小孩子都能解决的问题，这表明传统的任务复杂性概念不足以解释LLMs的能力。然而，由于大多数广泛使用的模型也被“指令调整”以适当地响应提示，因此探索LLMs的能力变得复杂。为了解构影响LLMs表现的因素，我们研究了指令调整模型是否具有与使用情境示例提示的基模型根本不同的能力。通过在各个模型家族、规模和任务类型上进行的广泛实验，包括对90个不同的LLMs进行指令调整，我们证明了指令调整模型的性能与其基模型的情境性能显著相关。通过阐明指令调整的贡献，我们扩展了之前关于情境学习的的研究，该研究认为基模型使用预训练数据中的先验知识来解决任务。具体来说，我们将这种理解扩展到指令调整模型，表明它们的预训练数据同样为它们可以解决的任务设定了一个限制边界，并增加了指令调整数据集的影响。**|
|**2025-01-14**|**OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training**|Yijiong Yu et.al.|[2501.08197](http://arxiv.org/abs/2501.08197)|**[link](https://github.com/yuyijiong/fineweb-edu-chinese)**|**大型语言模型（LLMs）展现了非凡的能力，但它们的成功很大程度上依赖于预训练语料库的质量。对于中文LLMs来说，高质量中文数据集的稀缺性构成了一个重大挑战，通常限制了它们的性能。为了解决这个问题，我们提出了OpenCSG中文语料库，这是一系列专为LLM预训练、后训练和微调设计的高质量数据集。该语料库包括Fineweb-edu-chinese、Fineweb-edu-chinese-v2、Cosmopedia-chinese和Smoltalk-chinese，每个数据集都有其独特的特点：Fineweb-edu数据集专注于从各种中文网络来源过滤出的高质量内容；Cosmopedia-chinese提供用于知识密集型训练的合成、教科书式数据；Smoltalk-chinese强调风格多样和格式丰富的聊天数据。OpenCSG中文语料库的特点是其高质量的文本、跨领域的广泛覆盖以及可扩展、可重复的数据整理流程。此外，我们还进行了广泛的实验分析，包括对较小参数模型的评估，这证明了该语料库在C-Eval等任务中的显著性能提升，展示了该语料库在训练中文LLMs方面的有效性。**|
|**2025-01-14**|**Quantifying the Importance of Data Alignment in Downstream Model Performance**|Krrish Chawla et.al.|[2501.08496](http://arxiv.org/abs/2501.08496)|null|与传统的关注数据集大小不同，我们探讨了数据对齐——数据质量经常被忽视的方面——在训练有能力的巨型语言模型（LLMs）中的作用。为此，我们使用基于Task2Vec的对齐系数，这是一种衡量两个数据集之间相似性的定量指标，来量化训练数据与评估数据对齐对下游性能的影响。具体来说，我们对两种设置进行了受控的干预实验：1. 预训练（pt）数据与评估数据集之间对齐系数增加的影响，以及2. 领域特定微调（ft）与领域特定评估之间对齐系数增加的影响。我们探索的特定领域任务是自动形式化——自然语言和代码之间的机器翻译任务，用于形式化验证。在这两种设置中，我们发现模型训练数据与评估数据对齐系数与模型在相应下游任务上的损失/困惑度之间存在强烈的、可预测的负相关性。这些发现表明，有必要重新评估LLM的训练方法，证明了数据对齐相对于数据量的相关性，尤其是在自动形式化等特定的下游任务中。|
|**2025-01-14**|**Towards Best Practices for Open Datasets for LLM Training**|Stefan Baack et.al.|[2501.08365](http://arxiv.org/abs/2501.08365)|null|许多AI公司未经版权所有者许可，在数据上训练大型语言模型（LLMs）。这样做是否合法因司法管辖权而异：在欧盟和日本等国家，在一定限制下允许这样做，而在美国，法律环境更为模糊。无论法律状况如何，创意生产者的担忧导致了多起高调的版权诉讼，诉讼威胁通常被引述为近年来企业和公共利益行动者减少共享训练数据集信息的趋势的原因。限制数据信息这一趋势通过拒绝研究人员、审计人员和受影响的个人获取理解AI模型所需的信息，阻碍了透明度、问责制和整个生态系统的创新，造成了损害。虽然通过在开放获取和公共领域数据上训练语言模型可以减轻这种影响，但截至写作之时，由于在组装必要语料库方面的巨大技术和社会挑战，尚无此类模型（以有意义规模训练）。这些挑战包括不完整和不可靠元数据、数字化物理记录的成本和复杂性，以及确保在快速变化的领域中的相关性及责任所需的各种法律和技术技能。建立一个未来，其中AI系统可以在开放许可的数据上进行训练，这些数据得到负责任地管理和治理，需要法律、技术和政策领域的跨学科合作，以及投资于元数据标准、数字化，并培育开放文化。|
|**2025-01-13**|**Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training**|Ziqing Wen et.al.|[2501.07237](http://arxiv.org/abs/2501.07237)|**[link](https://github.com/zqouo/gwt)**|**大型语言模型（LLMs）在各种自然语言处理任务中展现出了令人印象深刻的性能。然而，它们庞大的参数数量在训练过程中带来了显著的内存挑战，尤其是在使用像Adam这样的内存密集型优化器时。现有的内存高效算法通常依赖于如奇异值分解投影或权重冻结等技术。尽管这些方法有助于缓解内存限制，但与全秩更新相比，它们通常会产生次优的结果。在本文中，我们研究了低秩训练之外的内存高效方法，提出了一种名为梯度小波变换（GWT）的新解决方案，该方法通过对梯度应用小波变换来显著减少维持优化器状态所需的内存需求。我们证明了GWT可以无缝地集成到内存密集型优化器中，从而实现高效训练而不会牺牲性能。通过在预训练和微调任务上的广泛实验，我们展示了与先进的内存高效优化器和全秩方法相比，GWT在内存使用和训练性能方面都达到了最先进的水平。**|
|**2025-01-13**|**LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch**|Zhengzhong Liu et.al.|[2501.07124](http://arxiv.org/abs/2501.07124)|null|我们详细介绍了LLM360 K2-65B模型的训练过程，将我们的360度开源方法扩展到项目LLM360下最大和最强大的模型。尽管开源LLM持续进步，但“最大的LLM是如何训练的？”这一问题在社区中仍然不明确。由于与这些高容量模型相关的商业考虑，其实施细节通常受到保护。这种缺乏透明度阻碍了LLM研究人员利用先前经验中的宝贵见解，例如“解决损失突变的最佳实践是什么？”LLM360 K2项目通过提供LLM最大规模训练过程中积累的全面透明度和资源访问来填补这一空白。本报告重点介绍了K2项目的关键要素，包括我们的第一个模型K2 DIAMOND，这是一个65亿参数的LLM，其性能超过了LLaMA-65B，并与LLaMA2-70B相媲美，同时需要的浮点运算和标记更少。我们详细介绍了实施步骤，并展示了K2 DIAMOND在其训练过程中的纵向能力分析。我们还概述了正在进行的项目，如TXT360，为该系列未来的模型奠定基础。通过提供之前不可用的资源，K2项目也与360度开源原则——透明度、可重复性和可访问性——产生共鸣，我们认为这些原则在资源密集型AI研究时代至关重要。|
|**2025-01-12**|**SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training**|Tianjin Huang et.al.|[2501.06842](http://arxiv.org/abs/2501.06842)|**[link](https://github.com/tianjinyellow/spam-optimizer)**|**大型语言模型（LLMs）在各种任务中表现出色，但它们的训练仍然非常资源密集，并且容易受到训练不稳定等关键挑战的影响。这种不稳定的一个主要来源是梯度损失峰值，它们会干扰学习过程，通常导致昂贵的干预措施，如检查点恢复和实验重启，进一步放大了低效性。本文对LLM训练过程中观察到的梯度峰值进行了全面研究，揭示了它们在多个架构和数据集上的普遍性。我们的分析表明，这些峰值可以比典型梯度大1000倍，严重恶化模型性能。为了解决这个问题，我们提出了Spike-Aware Adam with Momentum Reset SPAM，这是一种新型优化器，旨在通过动量重置和峰值感知梯度裁剪来对抗梯度峰值。广泛的实验，包括预训练和微调，表明SPAM在各种任务中（1）从6000万到10亿的LLM预训练，（2）4位LLM预训练，（3）强化学习，和（4）时间序列预测中，始终优于Adam及其变体。此外，SPAM通过允许稀疏动量来促进内存高效训练，其中只维护和更新动量项的子集。在内存受限的情况下，SPAM优于GaLore和Adam-Mini等最先进的内存高效优化器。我们的工作强调了缓解LLM训练中梯度峰值的重要性，并介绍了一种有效的优化策略，该策略在规模上提高了训练稳定性和资源效率。代码可在https://github.com/TianjinYellow/SPAM-Optimizer.git上找到。**|
|**2025-01-12**|**Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity**|Jun Wan et.al.|[2501.06802](http://arxiv.org/abs/2501.06802)|null|2020年，OpenAI提出了第一种规模定律，描述了模型性能与参数、数据和计算之间的关系。2024年，OpenAI提出了第二种规模定律，描述了模型推理性能与推理计算之间的关系。在本文中，我们使用条件柯尔莫哥洛夫复杂度从无损压缩的角度分析LLM的训练和推理过程，并将这两种类型的规模定律统一起来。我们发现，这两种规模定律都通过增加执行步骤 $t$来提高条件柯尔莫哥洛夫复杂度的近似。第一种规模定律通过增加模型参数$y$来增加$t$。第二种规模定律通过增加输出标记的数量来增加$t$ 。|
|**2025-01-11**|**The Internet of Large Language Models: An Orchestration Framework for LLM Training and Knowledge Exchange Toward Artificial General Intelligence**|Wilson Wei et.al.|[2501.06471](http://arxiv.org/abs/2501.06471)|null|本文探讨了在大型语言模型（LLMs）开发过程中面临的多元挑战，包括模型参数和文件规模的庞大、开发环境配置的复杂性、模型功能的独特性以及计算资源的昂贵成本。为了解决这些挑战，本文提出了三个核心技术解决方案：LLM共享协议、LLM通用环境框架和代理最优路径模块。为了解决研究早期阶段的计算资源限制，我们进一步创新性地提出了一种联合挖掘机制，实现了计算能力提供者和模型设计者之间的双向价值共享，包括对最优模型路径的突破性奖励和长期利润分配，从而为研究人员提供成本优化的计算资源支持，并促进LLM研究和应用的持续发展。|
|**2025-01-10**|**Navigating Tomorrow: Reliably Assessing Large Language Models Performance on Future Event Prediction**|Petraq Nako et.al.|[2501.05925](http://arxiv.org/abs/2501.05925)|null|预测未来事件是一项重要的活动，其应用遍及多个领域和学科。例如，预测股市趋势、自然灾害、商业发展或政治事件的能力可以促进早期预防措施并揭示新的机会。已经提出了多种不同的计算方法来尝试预测未来，包括预测分析、时间序列预测和模拟。本研究评估了几个大型语言模型（LLMs）在支持未来预测任务方面的性能，这是一个尚未充分探索的领域。我们针对三个场景评估这些模型：肯定与可能性提问、推理和反事实分析。为此，我们通过根据实体类型及其流行度查找和分类新闻文章来创建一个数据集。我们收集了LLMs训练截止日期前后新闻文章，以彻底测试和比较模型性能。我们的研究突出了LLMs在预测建模中的潜力和局限性，为未来的改进奠定了基础。|
|**2025-01-09**|**TreeKV: Smooth Key-Value Cache Compression with Tree Structures**|Ziwei He et.al.|[2501.04987](http://arxiv.org/abs/2501.04987)|null|高效的关键值（KV）缓存压缩对于在长序列和资源受限环境中扩展基于Transformer的大语言模型（LLM）至关重要。现有方法根据位置或重要性分数驱逐标记，但基于位置的策略可能会遗漏预定义区域外的关键信息，而依赖于全局重要性分数的方法则会导致强烈的区域偏差，限制KV缓存的整体上下文保留，并可能损害LLM在复杂任务上的性能。我们的小波分析揭示，随着标记接近序列末尾，其对生成的贡献逐渐增加，并倾向于与相邻标记更加分离，这表明从遥远到附近上下文的复杂性和变化性随着增加而平滑过渡。受此观察启发，我们提出了一种名为TreeKV的直观、无需训练的方法，该方法使用树结构进行平滑缓存压缩。TreeKV保持固定的缓存大小，即使在长文本场景中也能使LLM提供高质量的输出。与大多数压缩方法不同，TreeKV适用于生成和预填充阶段。在PG19和OpenWebText2上的语言建模任务中，TreeKV在所有基线模型中表现最出色，使训练有短上下文窗口的LLM能够以16倍的缓存减少泛化到更长窗口。在Longbench基准测试中，TreeKV以最佳效率实现了最佳性能，仅使用了6%的预算。|
|**2025-01-09**|**Exploring Large Language Models for Translating Romanian Computational Problems into English**|Adrian Marius Dumitran et.al.|[2501.05601](http://arxiv.org/abs/2501.05601)|null|近期研究表明，大型语言模型（LLMs）在将数学和计算机科学任务从罗马尼亚语翻译成英语时，其表现不如在原始罗马尼亚语格式下。准确的翻译对于从编程竞赛中的自动翻译到高质量教育材料的创建等应用至关重要，同时也有助于减少人工翻译中的错误或欺诈。本研究表明，当给予良好的结构化提示时，鲁棒的大型语言模型（LLMs）可以在翻译较少见语言时保持或甚至提高其性能。我们的研究结果表明，在适当的监督下，LLMs可以可靠地用于IOI（国际信息学奥林匹克）风格任务的自动翻译。我们评估了多个LLMs的多种翻译方法，包括OpenRoLLM、Llama 3.1 8B、Llama 3.2 3B和GPT-4o，通过重复运行来评估它们的翻译准确性和性能稳定性。此外，我们将OJI（罗马尼亚县级信息学奥林匹克）罗马尼亚语数据集与准确的英语翻译相结合，提高了其用于未来LLM培训和评估的实用性。通过详细的句法和语义分析，我们证实了在人工监督下，LLMs可以作为多语言问题解决的可行解决方案。我们还通过认证专家的评估，比较了LLMs与人工翻译者的翻译质量，强调了LLMs在现实场景中的潜力。|
|**2025-01-08**|**Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning**|Lang Xu et.al.|[2501.04266](http://arxiv.org/abs/2501.04266)|null|将大规模语言模型（LLM）的训练规模扩大涉及到在有限数量的工作者上调整庞大的训练参数。然而，像ZeRO-3这样的方法虽然大幅降低了GPU内存压力，但往往需要大量的通信来确保全局同步和一致性。现有的努力，如ZeRO++，使用辅助分区以避免节点间通信，因为节点内GPU-GPU传输通常比节点间连接具有更高的带宽和更低的延迟。然而，随着像Frontier这样的更强大的基础设施（配备AMD GPU）的出现，并展现出令人印象深刻的计算能力，有必要对硬件拓扑进行研究，并开发针对性的策略来提高训练效率。在本工作中，我们提出了一系列针对ZeRO++的通信和优化策略，以减少通信成本并提高内存利用率。在本文中，我们提出了一种针对当前顶级超级计算机集群Frontier的3级分层分区方法，旨在利用通信层（GCD-GCD、GPU-GPU和节点间）的多种带宽来减少通信开销。对于20B GPT模型，我们观察到与ZeRO++相比，当达到384个GCD时，每个GPU的TFLOPS提高了1.71倍，并且对于384个GCD的扩展效率为0.94。据我们所知，我们的工作也是首次在Frontier AMD GPU上高效优化LLM工作负载的努力。|
|**2025-01-06**|**Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**|Alhassan Mumuni et.al.|[2501.03151](http://arxiv.org/abs/2501.03151)|null|基于大规模预训练基础模型（PFMs）的生成式人工智能（AI）系统，如视觉-语言模型、大型语言模型（LLMs）、扩散模型和视觉-语言-行动（VLA）模型，已在众多领域和情境中展示了解决复杂且真正非平凡AI问题的能力。特别是，多模态大型语言模型（MLLMs）从广泛多样的数据源中学习，使世界有了丰富细腻的表征，从而提供了广泛的能力，包括推理、进行有意义的对话；与人类和其他代理共同解决复杂问题；以及理解人类的社会和情感方面。尽管这一成就令人印象深刻，但基于大规模数据集训练的最先进LLMs的认知能力仍然肤浅且脆弱。因此，通用LLMs在它们的通用能力方面受到了严重限制。为了使LLMs达到人类水平的通用智能，需要解决一系列基础问题——具身化、符号接地、因果关系和记忆。这些概念与人类认知更为一致，并为LLMs提供了固有的类似人类的认知属性，支持实现物理上可行、语义上有意义、灵活且更具可推广性的知识和智能。在本工作中，我们讨论了上述基础问题，并概述了在LLMs中实现这些概念的最先进方法。具体而言，我们讨论了如何利用具身化、符号接地、因果关系和记忆的原则，以有机的方式实现人工通用智能（AGI）。|
|**2025-01-05**|**HALO: Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning**|Saleh Ashkboos et.al.|[2501.02625](http://arxiv.org/abs/2501.02625)|**[link](https://github.com/ist-daslab/halo)**|量化大型语言模型（LLMs）的训练仍然是一个未解决的问题，因为在低精度下执行所有矩阵乘法以保持准确性的尝试已经证明是困难的。这在微调预训练模型时尤为如此，因为它们通常已经具有很大的权重和激活异常值，这使得量化优化变得困难。我们提出了HALO，这是一种针对Transformer的新型量化感知训练方法，通过结合以下三个方面，实现了准确且高效的低精度训练：1）在正向和反向传播过程中战略性地放置Hadamard旋转，以减轻低精度计算中的异常值；2）集成FSDP进行低精度通信；3）高性能内核支持。我们的方法确保正向和反向传播过程中所有的大矩阵乘法都在较低精度下执行。将HALO应用于LLAMA家族模型，在微调各种任务时，HALO实现了接近全精度等效的结果，同时为RTX 4090 GPU上的完整微调提供了高达1.31倍的端到端加速。我们的方法支持标准参数高效微调（PEFT）方法，两者都基于高效的内核实现。我们的结果表明，这是第一个将全量化LLM微调保持FP8精度准确性的实用方法，同时提供了性能优势。|
|**2025-01-05**|**Scaling Laws for Floating Point Quantization Training**|Xingwu Sun et.al.|[2501.02423](http://arxiv.org/abs/2501.02423)|null|低精度训练被认为是降低训练和下游推理成本的有效策略。之前的精度缩放法则主要关注整数量化，对浮点量化中的组成部分关注较少，因此无法很好地适应此场景下的LLM损失。相比之下，尽管浮点量化训练在生产中更常被实施，但对其的研究相对较浅。在本文中，我们彻底探讨了浮点量化目标、指数位、尾数位以及浮点量化训练中缩放因子的计算粒度对LLM模型性能的影响。在提出一个准确的浮点量化统一缩放法则的同时，我们还为社区提供了有价值的建议：（1）指数位对模型性能的贡献略大于尾数位。我们为不同位数提供了最优的指数-尾数位比例，可供硬件制造商未来参考；（2）我们发现低精度LLM训练中临界数据大小的形成。超过临界数据大小的过多训练数据会反向导致LLM性能下降；（3）最优浮点量化精度与计算能力成正比，但在广泛的计算能力范围内，我们估计最佳性价比精度位于4-8位之间。|
|**2025-01-03**|**CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis**|Bohan Zhang et.al.|[2501.01668](http://arxiv.org/abs/2501.01668)|**[link](https://github.com/ruckbreasoning/cot-based-synthesizer)**|**当前推理缩放方法，如自洽性和最佳N项，已被证明在提高LLMs在复杂推理任务上的准确性方面是有效的。然而，这些方法高度依赖候选回答的质量，当所有候选回答都不正确时，它们无法产生正确的答案。在本文中，我们提出了一种新颖的推理缩放策略，即基于CoT的合成器，它利用CoT推理通过分析多个候选回答的互补信息来合成更优的答案，即使所有候选回答都有缺陷。为了实现轻量级和成本效益的实施，我们引入了一个自动数据生成管道，该管道创建多样化的训练数据。这使得在训练数据上训练的小型LLMs可以提高大型模型的推理准确性，包括基于API的LLMs。在四个基准数据集和七个策略模型上的实验结果表明，我们的方法显著提升了性能，在MATH数据集上Llama3-8B提升了11.8%，GPT-4o提升了10.3%。相应的训练数据和代码可在https://github.com/RUCKBReasoning/CoT-based-Synthesizer上公开获取。**|
|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越多地依赖于网络爬取的数据集，对于未经授权使用受版权保护或个人内容进行训练的担忧日益加剧。尽管有如通用数据保护条例（GDPR）等法规，数据所有者仍然对其内容在模型训练中的使用控制有限。为了解决这个问题，我们提出了ExpShield，这是一种主动的自我保护机制，使内容所有者能够将不可见的扰动嵌入到他们的文本中，限制LLMs训练中的数据滥用，同时不影响可读性。这种预防性方法使数据所有者能够直接保护敏感内容，而无需依赖第三方进行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的合理性。我们进一步通过识别记忆触发器和创建陷阱来更集中地偏离模型记忆，从而提高效率。为了验证我们防御措施的有效性，我们提出了一个新颖的实例利用度量，该度量捕捉了模型训练引起的个体风险。实验结果表明，我们的方法有效，MIA AUC从0.95降至0.55，实例利用接近于零。这表明训练后个体风险并未增加，突出了在保护受版权数据方面主动防御的重要性。|
|**2024-12-27**|**Gradient Weight-normalized Low-rank Projection for Efficient LLM Training**|Jia-Hong Huang et.al.|[2412.19616](http://arxiv.org/abs/2412.19616)|**[link](https://github.com/jhhuangkay/gradient-weight-normalized-low-rank-projection-for-efficient-llm-training)**|**大型语言模型（LLMs）在各种任务上表现出色，但日益增长的计算资源需求带来了重大挑战，尤其是在广泛使用全微调进行下游任务时。为了解决这个问题，我们开发了一种参数高效微调（PEFT）方法，但它们通常比全微调表现不佳，并且内存效率较低。在这项工作中，我们介绍了一种名为梯度权重归一化低秩投影（GradNormLoRP）的新方法，该方法在保持与全微调相当的性能的同时，提高了参数和内存效率。GradNormLoRP通过归一化权重矩阵来改善梯度条件，从而在优化过程中促进更好的收敛。此外，它对权重和梯度矩阵应用低秩近似，显著降低了训练过程中的内存使用。广泛的实验表明，我们的8位GradNormLoRP将优化器内存使用量减少了高达89.5%，并使得在消费级GPU（如NVIDIA RTX 4090）上预训练大型LLMs（如LLaMA 7B）成为可能，而无需额外的推理成本。此外，GradNormLoRP在微调任务中优于现有的低秩方法。例如，当使用8个秩对RoBERTa模型在所有GLUE任务上进行微调时，GradNormLoRP的平均得分为80.65，超过了LoRA的79.23分。这些结果突显了GradNormLoRP作为高效LLM预训练和微调的有前景替代方案。源代码：https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training**|
|**2024-12-23**|**Tracking the Feature Dynamics in LLM Training: A Mechanistic Study**|Yang Xu et.al.|[2412.17626](http://arxiv.org/abs/2412.17626)|null|理解大型语言模型（LLMs）的训练动态和特征演变对于其机制可解释性至关重要。尽管稀疏自编码器（SAEs）已被用于识别LLMs中的特征，但这些特征在训练过程中的演变情况仍然难以清晰描绘。在本研究中，我们：（1）引入了SAE-Track，一种高效获取连续SAEs序列的方法；（2）对特征形成过程进行了阐述并进行机制分析；（3）分析和可视化了训练过程中的特征漂移。我们的工作为LLMs中特征动态提供了新的见解，增强了我们对训练机制和特征演变的理解。|
|**2024-12-20**|**TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use**|Junjie Ye et.al.|[2412.15495](http://arxiv.org/abs/2412.15495)|**[link](https://github.com/junjie-ye/tl-training)**|**大语言模型（LLMs）通过利用工具与环境交互，实现了显著的进步，这是通向通用人工智能的关键一步。然而，标准的有监督微调（SFT）方法依赖于大规模数据集，往往忽视了工具使用中的特定任务特征，导致性能瓶颈。为了解决这个问题，我们分析了三种现有的LLMs，并揭示了关键见解：训练数据可能会无意中阻碍工具使用行为，标记的重要性分布不均，工具调用错误落入一小组独特的类别中。基于这些发现，我们提出了TL-Training，这是一个基于任务特征的框架，可以减轻不理想训练数据的影响，动态调整标记权重以在SFT期间优先考虑关键标记，并整合了一个针对错误类别的稳健奖励机制，通过近端策略优化进行优化。我们通过训练CodeLLaMA-2-7B并在四个不同的开源测试集上评估它来验证TL-Training。我们的结果表明，我们方法训练的LLM在仅使用1,217个训练数据点的情况下，其工具使用性能与开源和闭源LLMs相当甚至更好。此外，我们的方法增强了在噪声环境中的鲁棒性，并提高了通用任务性能，为LLMs中的工具使用训练提供了一个可扩展且高效的范式。代码和数据可在https://github.com/Junjie-Ye/TL-Training上找到。**|
|**2024-12-19**|**Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters**|Zihan Chang et.al.|[2412.14479](http://arxiv.org/abs/2412.14479)|null|现有工作仅适用于一定数量的GPU，常常忽略了手动确定所需GPU的具体类型和数量的复杂性，这对开发者来说可能是一个巨大的负担。为了解决这个问题，我们提出了Frenzy，这是一种针对异构GPU集群的内存感知无服务器计算方法。Frenzy允许用户提交模型时无需担心底层硬件资源。首先，Frenzy通过估算LLM的GPU内存使用量来预测所需的GPU数量和类型。然后，它采用低开销的异构感知调度方法来优化训练效率。我们通过在具有三种不同GPU类型的异构GPU集群上进行的LLM多任务训练测试验证了Frenzy的性能。结果显示，Frenzy的内存使用量预测准确率超过92%，调度开销降低了10倍，与最先进的方法相比，它将平均作业完成时间缩短了12%至18%。|
|**2024-12-19**|**Conceptual In-Context Learning and Chain of Concepts: Solving Complex Conceptual Problems Using Large Language Models**|Nishtha N. Vaidya et.al.|[2412.15309](http://arxiv.org/abs/2412.15309)|null|科学和工程问题属于需要特定概念信息（CI）如数学/逻辑相关知识、流程信息或工程指南来解决的复杂概念问题。大型语言模型（LLMs）由于其在对工程和科学任务如辅助问题解决方面的潜力，是解决此类复杂概念问题的有希望的代理。但是，在开放世界数据上训练的vanilla LLMs缺乏必要的CI。在这项工作中，我们专门探索了LLMs的浅层定制方法（SCMs）来解决复杂概念问题。我们为LLM提出了两种新颖的SCM算法，以增强LLMs的CI并使其能够解决复杂概念问题：概念性情境学习（C-ICL）和概念链（CoC）。本文解决的问题是基于数据建模指南中的概念信息在工程/行业领域生成专有数据模型。我们评估了我们的算法在OpenAI LLM的不同大小上，与四个与句法和语义正确性、时间和成本相关的评估指标。所提出的算法在性能上优于目前流行的LLM SCMs，如情境学习（ICL）和思维链（CoT）。观察到，与CoT相比，新SCM C-ICL和CoC的响应正确性分别提高了30.6%和29.88%。定性分析表明，所提出的新的SCMs在LLMs中激活了之前在现有SCMs中未见的能力。它们使问题解决过程更加透明，并减少了幻觉以及模型响应复制提示（鹦鹉学舌）的趋势。|
|**2024-12-18**|**Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes**|Katarzyna Kobalczyk et.al.|[2412.13998](http://arxiv.org/abs/2412.13998)|**[link](https://github.com/kasia-kobalczyk/few-shot-steerable-alignment)**|**随着大型语言模型（LLMs）在日常应用中越来越广泛地嵌入，确保它们与个体用户的多样化偏好保持一致已成为一项关键挑战。目前部署的方法通常假设用户目标具有同质性，并依赖于单目标微调。然而，人类偏好本质上具有异质性，受各种不可观测因素的影响，导致偏好数据中出现冲突的信号。现有解决这一多样性的解决方案通常需要昂贵的、针对特定目标标记的数据集，并涉及训练多个奖励模型或LLM策略，这既计算成本高又不切实际。在本工作中，我们提出了一种新颖的少量样本可操控对齐框架，从用户选择的少量样本中推断其潜在偏好。为此，我们将Bradley-Terry-Luce模型扩展到处理具有不可观测变异因素的异质偏好，并提出了其实际的奖励建模和LLM微调实现。得益于我们提出的功能参数空间条件化方法，使用我们的框架训练的LLMs可以在推理时适应个体偏好，生成跨越行为模式连续体的输出。我们通过实证验证了方法的有效性，证明了它们能够以数据高效的方式捕捉并适应多样化的人类偏好。我们的代码可在以下网址获得：https://github.com/kasia-kobalczyk/few-shot-steerable-alignment。**|
|**2024-12-18**|**AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge**|Xiaobao Wu et.al.|[2412.13670](http://arxiv.org/abs/2412.13670)|**[link](https://github.com/bobxwu/antileak-bench)**|数据污染阻碍了公平的LLM评估，因为它会将测试数据引入新模型的训练集中。现有研究通过更新基准数据集来解决这一挑战。然而，它们无法保证评估的无污染性，因为新收集的数据可能包含现有知识，并且它们的基准更新依赖于大量的人工劳动。为了解决这些问题，我们在本文中提出了AntiLeak-Bench，一个自动化的反泄露基准测试框架。我们不是简单地使用新收集的数据，而是构建了包含LLM训练集中未明确出现的新知识的样本，从而确保严格的无污染评估。我们进一步设计了一个完全自动化的工作流程，用于构建和更新我们的基准，无需人工劳动。这显著降低了基准维护的成本，以适应新兴的LLM。通过广泛的实验，我们强调数据污染很可能在LLM截止时间之前存在，并证明了AntiLeak-Bench有效地克服了这一挑战。|
|**2024-12-18**|**ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling**|William Han et.al.|[2412.14373](http://arxiv.org/abs/2412.14373)|**[link](https://github.com/willxxy/ecg-byte)**|**大型语言模型（LLMs）在除文本以外的领域，特别是心电图（ECGs）领域，展现出惊人的适应性。更具体地说，有一批研究正在探索从多通道ECG和相应的文本提示生成文本的任务。目前的方法通常涉及使用自监督学习（SSL）目标预训练一个ECG特定的编码器，并使用预训练编码器输出的特征微调LLM以进行自然语言生成（NLG）。然而，这些方法受限于1）两阶段训练的效率低下和2）编码器生成的特征的可解释性挑战。为了解决这些限制，我们引入了ECG-Byte，这是一种用于ECG自回归语言模型的适应性字节对编码（BPE）标记化流水线。这种方法将ECG信号压缩并编码成标记，通过直接结合ECG和文本标记实现端到端LLM训练，同时由于ECG标记可以直接映射回原始信号，因此具有更高的可解释性。使用ECG-Byte，我们在NLG任务中实现了与两阶段方法相当的性能，但所需时间仅为后者的一半，所需数据量也减少了约48%。**|
|**2024-12-18**|**A Systematic Examination of Preference Learning through the Lens of Instruction-Following**|Joongwon Kim et.al.|[2412.15282](http://arxiv.org/abs/2412.15282)|null|偏好学习是一种广泛采用的后训练技术，它使大型语言模型（LLMs）与人类偏好相一致，并提高特定下游任务的能力。在本研究中，我们系统地调查了偏好数据集的特定属性如何影响LLMs在指令跟随任务中的对齐和下游性能。我们使用一种新颖的合成数据生成流程，生成48,000个独特的指令跟随提示，这些提示由23个可验证的约束组合而成，这些约束能够实现模型响应的精细和自动化质量评估。使用我们的合成提示，我们采用两种偏好数据集整理方法——拒绝采样（RS）和蒙特卡洛树搜索（MCTS）——来获得（选择，拒绝）响应对。然后，我们进行实验，研究以下因素的影响：（1）选择和拒绝响应之间共享前缀的存在，（2）选择、拒绝响应的对比度和质量，（3）训练提示的复杂性。我们的实验表明，由MCTS生成的偏好对中的共享前缀提供了边际但一致的改进，并在具有挑战性的训练配置中提供了更大的稳定性。高对比度偏好对通常优于低对比度对；然而，结合两者通常通过平衡多样性和学习效率来实现最佳性能。此外，与过于具有挑战性的提示相比，在中等难度的提示上进行训练导致在任务之间更好的泛化，即使在更复杂的评估场景中也是如此。我们的发现为优化指令跟随任务的偏好数据整理提供了可操作的见解，提供了一种可扩展且有效的框架，用于增强LLMs的训练和对齐。|
|**2024-12-17**|**SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba，2015）对于大型语言模型的成功至关重要。然而，它们通常需要在训练过程中维护优化器状态，这可能导致内存需求比模型足迹大几倍。这种开销对可扩展性和计算效率提出了限制。与此相反，随机梯度下降（SGD）是一种无状态优化器，因为它在训练过程中不跟踪状态变量。因此，它实现了最佳内存效率。然而，它在LLM训练中的能力有限（Zhao等，2024b）。在这项工作中，我们表明以无状态方式预处理SGD可以达到与Adam优化器相同的性能，同时大幅降低内存成本。具体来说，我们建议使用归一化和白化预处理瞬时随机梯度。我们表明归一化稳定了梯度分布，而白化抵消了损失景观的局部曲率。这导致了SWAN（带有白化和归一化的SGD），这是一种随机优化器，消除了存储任何优化器状态的需求。经验表明，SWAN的内存占用与SGD相同，与Adam相比，总端到端内存减少了约50%。在语言建模任务中，SWAN表现出与Adam相当甚至更好的性能：当使用350M和1.3B参数预训练LLaMA模型时，SWAN通过使用一半的标记达到相同的评估困惑度，实现了2倍的速度提升。|
|**2024-12-16**|**Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods**|Diana Bar-Or Nirman et.al.|[2412.11625](http://arxiv.org/abs/2412.11625)|null|虽然大型语言模型（LLMs）已成为各个领域的核心工具，但它们往往提供不准确或错误的信息。本研究考察了用户对LLMs错误信息回应的偏好。具体来说，我们评估了用户对LLMs中明确标记的虚假陈述与未标记回应的偏好，以及对比LLM承认缺乏知识的声明和自信的虚假陈述的偏好。此外，我们还调查了要求用户评估陈述真实性如何影响这些偏好。令人惊讶的是，61%的用户更喜欢未标记的错误回应，而69%的用户更喜欢自信的虚假陈述而不是LLM承认缺乏知识。在我们的所有实验中，共有300名用户参与，为我们的分析和结论提供了宝贵的数据。当用户需要评估陈述的真实性时，对未标记和错误回应的偏好略有下降，但仍然很高。这些发现表明，用户偏好可能会通过反馈机制影响LLM的训练，并无意中鼓励生成虚假信息。未来的研究应解决将LLM行为与这种偏好对齐的伦理和实际影响。|
|**2024-12-15**|**Empowering LLMs to Understand and Generate Complex Vector Graphics**|Ximing Xing et.al.|[2412.11102](http://arxiv.org/abs/2412.11102)|null|大型语言模型（LLMs）的空前进步深刻影响了自然语言处理，但尚未完全涉及可扩展矢量图形（SVG）生成的领域。虽然LLMs在训练过程中编码了来自网页的SVG数据的部分知识，但近期研究发现，LLMs中的语义模糊和分词表示可能会导致矢量原初预测中的幻觉。此外，LLM的训练通常缺乏对矢量路径渲染序列的建模和理解，这可能导致输出矢量原初之间的遮挡。在本文中，我们提出了LLM4SVG，这是填补这一差距的初步但重要的一步，使LLMs能够更好地理解和生成矢量图形。LLM4SVG通过可学习的语义标记促进了对SVG组件的深入了解，这些标记精确地编码了这些标记及其相应的属性，以生成语义一致的SVG输出。通过一系列可学习的语义标记，开发了一个用于指令遵循的结构化数据集，以支持两个主要任务的理解和生成。我们的方法引入了一个模块化架构，将语义标签、矢量指令编码器、微调命令和强大的LLMs紧密结合起来，以紧密结合几何、外观和语言信息。为了克服SVG-text指令数据的稀缺性，我们开发了一个自动化的数据生成管道，收集了超过250k SVG数据和580k SVG-text指令的庞大数据集，这促进了LLM开发中流行的两阶段训练策略的采用。通过探索各种训练策略，我们开发了LLM4SVG，它在人类评估任务中显著超越了基于优化渲染的方法和基于语言模型的基线，取得了显著成果。|
|**2024-12-11**|**NAT-NL2GQL: A Novel Multi-Agent Framework for Translating Natural Language to Graph Query Language**|Yuanyuan Liang et.al.|[2412.10434](http://arxiv.org/abs/2412.10434)|**[link](https://github.com/leonyuancode/stockgql)**|**大型语言模型（LLMs）的出现颠覆了许多领域，不仅限于传统的自然语言处理（NLP）任务。最近，将LLMs应用于数据库领域的研究蓬勃发展，作为典型的非关系型数据库，LLMs在图数据库研究中的应用自然引起了广泛关注。近期的研究越来越多地集中于利用LLMs将自然语言翻译成图查询语言（NL2GQL）。尽管取得了一些进展，但这些方法存在明显的局限性，例如它们依赖于简化的流程，这些流程往往忽略了LLMs自主规划和与其他LLMs协作解决复杂NL2GQL挑战的潜力。为了解决这一差距，我们提出了NAT-NL2GQL，一个将自然语言翻译成图查询语言的创新多智能体框架。具体来说，我们的框架由三个协同的智能体组成：预处理智能体、生成智能体和精炼智能体。预处理智能体负责管理作为上下文的数据处理，包括诸如命名实体识别、查询重写、路径链接和提取与查询相关的模式等任务。生成智能体是一个在NL-GQL数据上微调的LLMs，负责根据查询及其相关模式生成相应的GQL语句。精炼智能体负责使用从GQL执行结果中获得的错误信息精炼GQL或上下文。鉴于基于nGQL语法的优质开源NL2GQL数据集稀缺，我们开发了StockGQL，一个由金融市场图数据库构建的数据集。它可在以下网址获取：https://github.com/leonyuancode/StockGQL。在StockGQL和SpCQL数据集上的实验结果表明，我们的方法在性能上显著优于基线方法，突显了其在推进NL2GQL研究方面的潜力。**|
|**2024-12-10**|**The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model**|Jiawei Chen et.al.|[2412.07298](http://arxiv.org/abs/2412.07298)|null|大型语言模型（LLMs）展现出显著的多语言能力。然而，这些能力在预训练过程中发展的机制尚不清楚。在本文中，我们利用代码LLMs作为实验平台，探讨LLMs在预训练过程中多语言能力的演变。基于我们的观察，我们提出了巴别塔假说，该假说描述了LLMs获取新语言能力的整个过程。在学习过程中，多种语言最初共享一个由主要语言主导的知识系统，并逐渐发展出特定于语言的知识系统。然后，我们通过识别工作语言和语言转换神经元来追踪LLMs的内部状态，以验证上述假说。实验结果表明，LLMs的内部状态变化与我们的巴别塔假说一致。基于这些洞察，我们提出了一种新的方法来构建针对多语言代码LLMs的优化预训练语料库，其性能显著优于在原始语料库上训练的LLMs。所提出的巴别塔假说为设计预训练数据分布以实现LLMs最佳多语言能力提供了新的见解。|
|**2024-12-10**|**EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models**|Jialiang Cheng et.al.|[2412.07210](http://arxiv.org/abs/2412.07210)|**[link](https://github.com/intelligent-machine-learning/atorch)**|分布式训练方法对于大型语言模型（LLMs）至关重要。然而，现有的分布式训练方法往往受到通信瓶颈、执行缓慢和弹性有限等问题的影响。为了解决这些问题，我们提出了EDiT，这是一种创新的、高效的分布式训练方法，它结合了定制化的局部随机梯度下降（Local SGD）方法和模型分片技术，以提高大规模训练的效率。EDiT在正向传播过程中执行层级的参数同步，减少通信和内存开销，并实现计算与通信的并行。此外，EDiT采用伪梯度惩罚策略来抑制损失波动，确保训练稳定性并提升性能。此外，我们引入了A-EDiT，这是EDiT的完全异步变体，适用于异构集群。基于EDiT/A-EDiT，我们进行了一系列实验，以验证LLMs的大规模异步训练，并伴随全面的分析。实验结果表明，EDiT/A-EDiT的性能优越，成为不同计算生态系统中分布式LLM训练的稳健解决方案。|
|**2024-12-09**|**Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit**|Joshua Freeman et.al.|[2412.06370](http://arxiv.org/abs/2412.06370)|null|近期，由于2023年12月提起的《纽约时报》诉OpenAI诉讼，前沿大型语言模型（LLM）的版权侵权问题受到了广泛关注。纽约时报声称，GPT-4通过在LLM训练中使用复制文章并在其输出中记忆这些输入，从而侵犯了其版权。我们的研究旨在衡量OpenAI的LLM相对于其他LLM在其输出中表现出逐字记忆的倾向，特别是针对新闻文章。我们发现，GPT和Claude模型都使用拒绝训练和输出过滤器来防止记忆文章的逐字输出。我们应用了一个基本的提示模板来绕过拒绝训练，并显示OpenAI模型目前比Meta、Mistral和Anthropic的模型更不容易被激发记忆。我们发现，随着模型规模的增加，特别是超过1000亿参数后，它们表现出显著更强的记忆能力。我们的发现对训练具有实际意义：必须更加关注在非常大型模型中防止逐字记忆。我们的发现也具有法律意义：在评估OpenAI的LLM相对记忆能力时，我们探究了《纽约时报》版权侵权主张的强度和OpenAI的法律辩护，同时强调了生成式AI、法律和政策交叉领域的问题。|
|**2024-12-09**|**Large Language Models: An Applied Econometric Framework**|Jens Ludwig et.al.|[2412.07031](http://arxiv.org/abs/2412.07031)|null|大型语言模型（LLMs）正在被用于经济学研究以形成预测、标注文本、模拟人类反应、生成假设，甚至为不存在数据的时间和地点生成数据。虽然这些应用具有创新性，但它们是否有效？我们何时可以忽略LLMs的内部运作，仅仅依赖它们的输出？我们开发了一个计量经济学框架来回答这个问题。我们的框架区分了两种类型的实证任务。在以下条件下，使用LLMs的输出进行预测问题（包括假设生成）是有效的：LLMs的训练数据集与研究人员样本之间没有“泄露”。使用LLMs的输出进行估计问题以自动化某些经济概念（由某些文本或人类受试者表达）的测量需要额外的假设：LLMs的输出必须与它们所取代的黄金标准测量一样好。否则，即使LLMs的输出高度准确但并非完美，估计也可能存在偏差。我们在金融和政治经济学中的实例应用中记录了这些条件被违反的程度及其对研究发现的含义。我们还为实证研究人员提供了指导。确保没有训练泄露的唯一方法是使用具有记录的训练数据和发布权重的开源LLMs。处理LLMs测量误差的唯一方法是收集验证数据和建模误差结构。一个推论是，如果无法满足候选LLMs应用的条件，我们强烈建议：不要使用。|
|**2024-12-06**|**Code generation and runtime techniques for enabling data-efficient deep learning training on GPUs**|Kun Wu et.al.|[2412.04747](http://arxiv.org/abs/2412.04747)|null|随着深度学习模型的规模扩大，其训练成本显著增加。由于硬件的进步和当前软件堆栈的限制，数据效率的需求日益上升。数据效率是指有效隐藏数据访问延迟和避免不必要的数据处理。由于GPU内存带宽与计算吞吐量之间的差距越来越大，GPU内存容量即将达到限制，以及PyTorch软件堆栈中的不效率（包括缺乏特定设备的PCIe传输优化和高级领域特定抽象），主要挑战随之产生。为了有效地缓解这些数据不效率，本论文分析了代表性深度训练任务中的数据不效率，特别是在图神经网络（GNN）和大型语言模型（LLM）中。然后，它提出了新的运行时和代码生成技术来缓解这些挑战，并在PyTorch堆栈中无缝实现这些优化，同时保持强大的可编程性和互操作性。首先，设计PyTorch-Direct以在PyTorch中集成以GPU为中心的PCIe数据传输范式用于GNN训练。接下来，提出了Hector中间表示（IR）及其代码生成器，以引入领域特定的顶层抽象并系统地解决关系型GNN的内存密集型性能挑战。最后，在LLM训练中，吞吐量越来越受限于GPU内存容量。为了缓解这一问题，设计了并实现了SSDTrain卸载框架。总之，这些贡献表明，代码生成和运行时技术可以系统地缓解深度学习训练中的数据管理瓶颈，这些瓶颈源于工作负载的数据密集性以及深度学习训练软件堆栈中的固有简化。|
|**2024-12-06**|**Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation**|Xiaoyu Wang et.al.|[2412.05342](http://arxiv.org/abs/2412.05342)|null|大型语言模型（LLM）通常经过微调以参与二元或双方面对面的对话，但它们在多方面对话（MPD）中的适应能力较差，这阻碍了它们在多个人会议、讨论和日常交流等场景中的应用。以往基于LLM的研究主要关注多智能体框架，而其基础LLM仍然是成对微调的。在本工作中，我们设计了一个针对多方面对话数据集的LLM多方面微调框架（MuPaS），并证明这样一个简单的框架可以让LLM高效有效地与多方面对话风格保持一致。我们还设计了两种训练策略，可以将MuPaS转化为MPD模拟器。大量实验表明，MuPaS可以实现最先进的多人回答，更高的一轮预测准确率，更高的人机和自动评估的说话质量，甚至能够在分布外场景、主题和角色描述中生成合理的内容。MuPaS框架将LLM的训练与更复杂的多人应用（如对话生成、虚拟排练或元宇宙）相连接。|
|**2024-12-02**|**MALT: Improving Reasoning with Multi-Agent LLM Training**|Sumeet Ramesh Motwani et.al.|[2412.01928](http://arxiv.org/abs/2412.01928)|null|使大型语言模型（LLMs）之间实现有效协作是实现能够解决复杂问题的自主系统的重要一步。尽管LLMs通常被用作单模型生成器，其中人类对其输出进行批判和改进，但联合训练的协作模型的潜力在很大程度上尚未得到探索。尽管在多智能体通信和辩论环境中取得了有希望的结果，但在训练模型共同完成任务方面进展甚微。在本文中，我们提出了一种针对推理问题的“多智能体LLM训练”（MALT）的第一步。我们的方法采用了一种序列多智能体设置，其中分配给异构LLMs专门的角色：生成器、验证器和改进模型迭代解决问题。我们提出了一种基于轨迹扩展的合成数据生成过程和由基于联合结果的奖励驱动的信用分配策略。这使得我们的后训练设置能够利用正负轨迹来自主地提高每个模型的专门能力，作为联合序列系统的一部分。我们在MATH、GSM8k和CQA上评估了我们的方法，其中MALT在Llama 3.1 8B模型上相对于同一基线模型分别实现了14.14%、7.12%和9.40%的相对改进。这展示了在数学和常识推理问题上的多智能体协作能力的早期进步。更广泛地说，我们的工作为围绕多智能体LLM训练方法的研究提供了具体方向。|
|**2024-12-02**|**Addressing Data Leakage in HumanEval Using Combinatorial Test Design**|Jeremy S. Bradbury et.al.|[2412.01526](http://arxiv.org/abs/2412.01526)|null|大型语言模型（LLMs）在众多领域得到广泛应用，包括软件工程，它们已被用于自动化诸如程序生成和测试分类等任务。随着基于LLM的方法不断进化，定义清晰且稳健的方法以公平评估性能变得尤为重要。基准测试是评估LLMs解决特定任务能力以及评估LLM不同版本随时间解决任务能力的一种常见方法。例如，HumanEval基准测试由164个手工制作的任务组成，已成为评估基于LLM的程序生成的重要工具。然而，使用如HumanEval等基准测试对LLM进行公平评估的一个主要障碍是基准任务和解决方案数据泄露到训练数据集中的问题。这种障碍由于LLM训练数据的黑盒性质而加剧，这使得甚至难以知道是否发生了数据泄露。为了解决数据泄露问题，我们提出了一种新的基准构建方法，该方法将基准测试构建为由模板任务组成，可以使用组合测试设计实例化为新具体任务。对于同一模板任务的具体任务必须足够不同，以至于数据泄露的影响最小，并且足够相似，以至于在性能评估方面可以互换。为了评估我们的基准构建方法，我们提出了HumanEval_T，这是一个使用模板任务和组合测试设计构建的HumanEval的替代基准测试。|
|**2024-12-02**|**Data-Centric and Heterogeneity-Adaptive Sequence Parallelism for Efficient LLM Training**|Yujie Wang et.al.|[2412.01523](http://arxiv.org/abs/2412.01523)|null|扩展LLM的上下文长度（即最大支持的序列长度）具有极其重要的意义。为了促进LLM的长时间上下文训练，序列并行化已成为一项关键技术，它将每个输入序列分散到多个设备上，并需要通信来处理序列。本质上，现有的序列并行化方法假设序列长度均匀（即所有输入序列长度相同），因此为所有输入序列利用单一、静态的散列策略。然而，在现实中，LLM训练语料库中的序列长度表现出显著的差异，通常遵循长尾分布，导致工作负载异质性。在本文中，我们表明采用单一、静态的策略会导致效率低下和资源利用率不足，强调了需要自适应方法来处理序列间的异质工作负载。为此，我们提出了一种异质性自适应序列并行化方法。对于每个训练步骤，我们的方法捕捉序列长度的变化，并根据工作负载特征分配最优的散列策略组合。我们将此问题建模为线性规划优化，并设计了一种高效有效的求解器以找到最优解。此外，我们在支持分布式LLM训练的自适应并行化的高性能系统中实现了我们的方法。实验结果表明，我们的系统在性能上优于最先进的训练框架，最高可达1.98倍。|
|**2024-12-02**|**Scaling Law for Language Models Training Considering Batch Size**|Xian Shuai et.al.|[2412.01505](http://arxiv.org/abs/2412.01505)|null|近年来，大型语言模型（LLMs）取得了显著进步，其中规模法则在快速进展中发挥了关键作用。在本文中，我们实证研究了关键超参数，即全局批量大小，如何影响LLM的训练过程。我们首先训练了参数量从1.25亿到26亿的语模，使用了高达3000亿的高质量标记。通过这些实验，我们建立了一个关于模型大小和训练数据量的基本规模法则。然后，我们考察了批量大小和学习率的变化如何影响这些模型的收敛性和泛化能力。我们的分析得出两种情况下的批量大小规模法则：固定计算预算和固定训练数据量。对不断增大的模型进行的外推实验验证了我们的预测法则，为在特定资源限制下优化LLM训练策略提供了指导。|
|**2024-11-26**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们通过观察发现，在应用低比特量化时，规模较大或训练词数较少的语言模型（LLMs）的量化诱导退化（QiD）较小，而训练词数较多的小型模型则会遭受显著的QiD。为了更深入地了解这一趋势，我们在一个受控环境下研究了1500多个不同规模和训练水平（未训练或完全训练）的量化LLMs检查点，从而推导出QiD与训练词数、模型大小和比特宽度等因素之间的关系定律。利用这些定律，我们提出了一种新视角：可以使用QiD来衡量LLMs的训练水平，并确定不同规模LLMs完全训练所需的训练词数。此外，我们使用这些定律来预测使用100万亿个训练词训练的不同规模LLMs的量化性能。我们的预测表明，未来预期使用超过100万亿个训练词训练的低比特量化模型性能可能并不理想。这为未来的低比特量化提出了潜在的挑战，并突出了在评估低比特量化研究时需要关注模型训练水平。为了促进对此问题的未来研究，我们将本次工作中使用的1500多个量化检查点发布在https://huggingface.co/Xu-Ouyang上。|
|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|**将文本分割成标记的编码技术，如字节对编码（BPE）和字节级BPE（BBPE），通过将文本分割成标记，显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割通常掩盖了标记内部的字符结构和序列，导致模型在训练期间无法完全学习这些复杂的细节。因此，LLMs在理解标记内部的字符组成和位置关系方面存在困难，尤其是在使用有限数据的下游任务中进行微调时。在本文中，我们引入了一种名为标记内部位置感知（TIPA）的新方法，通过使用分词器自己的词汇进行反向字符预测任务来训练模型，从而增强LLMs对内部标记结构的理解。这种方法使模型能够有效地学习和泛化字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测标记级别的字符位置方面优于基线模型。此外，当应用于中文拼写纠正（CSC）的下游任务时，TIPA不仅加速了模型收敛，而且显著提高了任务性能。**|
|**2024-11-26**|**Using Large Language Models for Expert Prior Elicitation in Predictive Modelling**|Alexander Capstick et.al.|[2411.17284](http://arxiv.org/abs/2411.17284)|**[link](https://github.com/alexcapstick/llm-elicited-priors)**|**大型语言模型（LLMs）通过训练不同领域的数据，有效地获取了广泛的信息。然而，它们的计算复杂度、成本和缺乏透明度阻碍了它们在特定任务中的直接应用。在临床研究等领域的预测模型中，获取专家注释或先验知识通常是昂贵且耗时的。本研究提出使用LLMs来获取预测模型的专家先验分布。这种方法也为情境学习提供了一种替代方案，其中语言模型直接负责做出预测。我们比较了LLM获取的先验和无关先验，评估了LLM是否真实地生成了参数分布，并提出了情境学习和先验获取的模型选择策略。我们的研究发现，与无关先验相比，在数据量少的情况下，LLM获取的先验参数分布显著降低了预测误差。应用于临床问题，这意味着所需的生物样本更少，降低了成本和资源。先验获取也始终优于情境学习，且成本更低，因此在我们的环境中成为一种更受欢迎的替代方案。我们展示了该方法在各种用例中的实用性，包括临床应用。在感染预测中，使用LLM获取的先验，在研究中提前200天，以相同的准确度减少了55%所需的标签数量。**|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，基于Transformer的大型语言模型（LLM）在长序列上的推理既昂贵又缓慢。我们引入了星形注意力，这是一种两阶段块稀疏逼近，通过在多个主机之间分散注意力来提高计算效率，同时最小化通信开销。在第一阶段，通过主机间的块局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星形注意力可以无缝集成到大多数使用全局注意力训练的基于Transformer的LLM中，通过减少内存需求和提高推理速度至多11倍，同时保持95-100%的准确率。**|
|**2024-11-25**|**The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C**|Mikita Balesni et.al.|[2411.16353](http://arxiv.org/abs/2411.16353)|null|在论文摘要中，作者首先指出大型语言模型（LLMs）在采用思维链（CoT）进行多跳推理（如“Imagine表演者的配偶是谁？”）时表现出色，但在被迫进行内部推理（不使用CoT）时则表现不佳。作者提到之前关于这一差距大小和性质的研究产生了不一致的证据。在这篇论文中，作者介绍了一种控制环境，用于研究LLMs中的两跳推理，其中超出随机水平的性能构成了潜在推理不可否认的证据。作者在虚构事实上进行微调LLMs（包括Llama 3 8B Instruct和GPT-4o），并确认它们可以推广到使用CoT回答有关它们的两跳问题。作者发现，当事实在训练过程中或提示中一起出现时，模型可以进行潜在推理。然而，出乎意料的是，当学习的事实仅出现在不同的文档中时，模型在两跳推理上完全失败，达到了随机水平准确性和测试损失。作者将这种完全无法组合单独学习的事实称为“两跳诅咒”。此外，作者在真实事实上评估了9个前沿LLMs，发现模型在超过一半的问题类别上完全无法进行无CoT的两跳推理，而在大多数类别上仍然部分成功使用CoT。这些结果表明，LLMs缺乏一种独立于问题类型的一般能力来进行潜在的跨跳推理。|
|**2024-11-24**|**Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution**|Haiquan Wang et.al.|[2411.15871](http://arxiv.org/abs/2411.15871)|null|随着大型语言模型（LLMs）的发展，大规模分布式训练变得必要。然而，高度优化的框架由于通信量大，在模型FLOPS利用率上仍然有显著的损失（通常低于50%）。同时，我们的全面分析显示，计算和通信密集型操作的重叠性很好。本文介绍了一种名为DHelix的新型微观结构，它受到DNA结构的启发，显著提高了LLM训练的效率。DHelix设计的核心是链式交错（SI），它将训练微批次连续流通过GPU视为两条链。DHelix并置两条链的前向和后向传递，并通过对称调度来自相对链的操作进行系统优化，这得益于操作级别的重叠分析结果和基于动态规划的搜索算法。同时，DHelix允许两条链共享模型状态和激活数据的空间，有效地容纳了额外内存空间低于3%的两个微批次。DHelix无缝集成到所有现有数据/模型并行形式，其中最具有挑战性的是管道并行，得益于其独特的模型折叠设计，形成了W型管道。我们使用流行的Llama和GPT密集模型，以及Phi混合专家（MoE）模型，在3个GPU集群（A40、A800和H100）上评估了DHelix的训练。结果显示，它在64-A40和64-A800集群上分别实现了12-40%（最高达到58%MFU）和2-29%（最高达到71%MFU）的提高，显著优于现有方法。在H100集群上，虽然更快的网络降低了DHelix的利润空间，但它使得跨节点张量并行成为可能，这在由于通信成本而目前难以实现的情况下是一种实践。|
|**2024-11-23**|**Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai**|Parinthapat Pengpun et.al.|[2411.15484](http://arxiv.org/abs/2411.15484)|**[link](https://github.com/parinzee/seed-free-synthetic-instruct)**|**我们提出了一种针对低资源语言（特别是泰语）的大语言模型（LLM）指令微调的合成数据方法，以数据高效的方式实现。我们确定了三个有助于指令微调数据集有效性的关键属性：流畅性、多样性和文化背景。我们提出了一种无需种子数据框架，用于生成包含这些基本属性的合成指令微调数据。我们的框架使用LLM生成多样化主题，从维基百科中检索相关上下文，并为各种任务（如问答、摘要和对话）创建指令。实验结果表明，我们的最佳表现合成数据集，结合了这三个关键属性，在仅使用5,000条指令的情况下，与在数万条指令上训练的顶尖泰语LLM相比，实现了具有竞争力的性能。我们的代码和数据集可在https://github.com/parinzee/seed-free-synthetic-instruct上公开获取。**|
|**2024-11-21**|**Exploring Accuracy-Fairness Trade-off in Large Language Models**|Qingquan Zhang et.al.|[2411.14500](http://arxiv.org/abs/2411.14500)|null|大型语言模型（LLMs）在人工智能领域取得了显著进展，展示了它们与人类互动和通过信息传播影响人类认知的能力。然而，最近的研究揭示了这些LLMs内含的偏见问题，这成为一个需要关注的重大问题。在我们的研究中，我们深入研究在LLMs增强过程中，如何协调准确性和公平性的复杂挑战。虽然提高准确性确实可以提升LLMs的整体性能，但这往往是以牺牲公平性为代价的。过度强调一个指标的优化必然会导致另一个指标的显著下降。这强调了在设计优化LLMs阶段时考虑多个因素的重要性。因此，我们主张将LLMs的训练过程重新定义为多目标学习任务。我们的研究揭示了多目标进化学习（MOEL）方法在应对这一挑战方面具有前景。我们的MOEL框架能够同时优化准确性和公平性指标，从而产生一组帕累托最优的LLMs。总之，我们的研究为LLMs中准确性和公平性之间的微妙平衡提供了宝贵的见解，这对于它们在现实世界中的应用越来越重要。通过利用MOEL，我们展示了一条通往更公平和更有效的AI技术的可行途径。|
|**2024-11-20**|**Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics**|Tetiana Bas et.al.|[2411.13738](http://arxiv.org/abs/2411.13738)|**[link](https://github.com/tetianabas/llm_biases)**|**本研究通过比较大型语言模型（LLMs）对性别的感知与人类受访者、美国劳工统计局数据和50%无偏见基准的性别感知，来探讨LLMs中的性别偏见。我们使用职业数据和特定角色的句子创建了一个新的评估集。与LLMs训练数据中常见的基准不同，我们的集合是全新开发的，防止了数据泄露和测试集污染。我们对五个LLMs进行了测试，以使用单词答案预测每个角色的性别。我们使用Kullback-Leibler（KL）散度来比较模型输出与人类感知、统计数据和50%中性基准之间的差异。所有LLMs都显示出与性别中性的显著偏差，并且更符合统计数据，仍然反映了固有的偏见。**|
|**2024-11-20**|**Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training**|Jared Fernandez et.al.|[2411.13055](http://arxiv.org/abs/2411.13055)|null|近年来，神经网络模型能力的显著提升是由模型规模、训练数据和相应的计算资源扩展驱动的。为了开发现代应用（如大型语言模型）所需的无尽大型网络，模型训练需要在数万台硬件加速器（例如GPU）上分布进行，这要求在大规模计算集群中协调计算和通信。在本工作中，我们证明了仔细考虑硬件配置和并行化策略对于有效（即计算和成本高效）地扩展模型规模、训练数据和总计算量至关重要。我们对大规模LLM训练工作负载在模型规模、硬件配置和分布式并行化策略方面的性能进行了广泛的实证研究。我们证明了：（1）超过一定规模后，某些分布式通信策略带来的开销导致之前被认为次优的并行化策略实际上变得更为可取；（2）即使硬件和并行化策略得到适当优化，增加加速器的总数来扩大大型模型训练也会迅速产生递减回报，这意味着每增加一个单位的功率或GPU时长的边际性能较差。|

<p align=right>(<a href=#updated-on-20251017>back to top</a>)</p>

