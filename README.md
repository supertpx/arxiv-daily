## Updated on 2024.11.30
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
    <li><a href=#infer>infer</a></li>
    <li><a href=#train>train</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴无声语音系统在恢复言语障碍患者的交流方面具有巨大的潜力。然而，无缝、连贯的语音交流仍然难以实现，其临床疗效尚未得到证实。在此，我们提出了一种由人工智能驱动的智能喉咙（IT）系统，该系统集成了喉咙肌肉振动和颈动脉脉搏信号传感器以及大型语言模型（LLM）处理，以实现流畅、情感表达的交流。该系统利用超敏感纺织应变传感器捕捉颈部区域的高质量信号，并支持分词级别的处理，以实现实时、连续的语音解码，从而实现无缝、无延迟的交流。在针对五名患有运动性构音障碍的卒中患者的测试中，IT的LLM代理智能地纠正了分词错误，丰富了句子的情感和逻辑连贯性，实现了低错误率（4.2%的单词错误率，2.9%的句子错误率）以及55%的用户满意度提升。这项工作为运动性构音障碍患者建立了一个便携、直观的通信平台，有望广泛应用于不同的神经系统疾病和多语言支持系统。|
|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在多个领域展示了卓越的规划能力，包括机器人操作和导航。尽管最近在机器人领域的努力已经利用LLMs进行高级和低级规划，但这些方法通常面临重大挑战，例如在长期任务中的幻觉现象以及由于单次生成计划且缺乏实时反馈导致的适应性有限。为了解决这些限制，我们提出了一种新的多智能体LLM框架，名为“用于操作的智能体大型语言模型”（MALMM），该框架将高级规划和低级控制代码生成分配给专门化的LLM智能体，并由一个额外的智能体动态管理过渡。通过在每一步后融入来自环境的信息，我们的框架有效地处理了中间故障并实现了自适应重新规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或情境学习示例，并且可以推广到各种新的任务。我们在包括长期任务在内的九个RLBench任务上评估了我们的方法，并展示了其在零样本设置下解决机器人操作的能力，从而克服了现有基于LLM的操作方法的关键限制。|
|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着大型语言模型（LLMs）的最近发展，一种潜在的使用场景得以打开，即改善人类与机器人助手交互的方式。这些LLMs应该能够利用它们广泛的理解能力，将自然语言命令解释为有效、符合任务要求且安全的机器人任务执行。然而，在现实中，这些模型存在幻觉问题，这可能导致安全问题或偏离任务。在其他领域，这些问题通过使用协作人工智能系统得到了改善，其中多个LLM智能体可以共同规划、编写代码和自我检查输出。在这项研究中，测试了多个协作人工智能系统与单个独立人工智能智能体的比较，以确定在其他领域的成功是否可以转化为改善人机交互性能。结果显示，智能体数量与模型成功之间没有明确趋势。然而，很明显，某些协作人工智能智能体架构可以显著提高产生无错误代码和解决抽象问题的能力。|
|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理的建模（ABM）已成为模拟社会网络的关键工具，涵盖了诸如信息传播、影响动态和社区形成等多样化的现象。然而，手动配置多样化的代理交互和信息流动态带来了挑战，通常导致模型过于简化，缺乏现实世界的普遍适用性。将现代大型语言模型（LLM）与ABM相结合，为解决这些挑战和提高模拟逼真度提供了一条有前景的道路，利用LLM在感知、推理和行为上类似人类的能力。在本文中，我们提出了一种新颖的框架，利用LLM赋能的代理根据用户的兴趣和个性特征来模拟社交网络用户。该框架允许自定义类似各种社交网络平台的代理交互，包括内容二次分享和个性化推荐的机制。我们使用2020年美国大选的全面Twitter数据集来验证我们的框架，证明LLM代理能够准确复制真实用户的言行，包括语言模式和政治倾向。这些代理形成了同质化的意识形态群体，并保留了其社区的主要主题。值得注意的是，基于偏好的推荐显著影响了代理的行为，促进了更高的参与度、网络同质性和回音室的形成。总体而言，我们的发现突显了LLM代理在推进社交媒体模拟和揭示复杂在线动态方面的潜力。|
|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大型语言模型（LLMs）和强化学习（RL）是构建自主智能体的两种强大方法。然而，由于对游戏环境的理解有限，智能体往往依赖低效的探索和试错，难以发展长期策略或做出决策。我们提出了一种方法，该方法从交互记录中提取经验以模拟游戏环境的潜在规律，并将这些经验作为内部动机来引导智能体。这些以语言表达的经验非常灵活，既可以直接协助智能体进行推理，也可以转化为奖励以指导训练。在Crafter中的评估结果表明，RL和LLM智能体都从这些经验中受益，从而提高了整体性能。|
|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|人工智能在自动化信息判断任务中是否存在认知偏差？尽管近年来在测量和减轻人工智能和大型语言模型（LLM）中的社会和算法偏差方面取得了进展，但LLM的行为在多大程度上是“理性的”，或者它们是否也容易受到人类认知偏差的影响，尚不明确。为了解决这个未解决的问题，我们的研究包括一项众包用户实验和一个LLM驱动的模拟实验，在信息检索（IR）环境中比较了LLM和人类判断者对潜在诱饵效应下的可信度评估，并实证检验了LLM在COVID-19医疗（错误）信息评估任务中的认知偏差程度，与传统人类评估者作为基线进行了比较。从一项受试者间用户实验和一项LLM驱动的重复实验收集的结果表明：1）更大、更新的LLM在区分可信信息和虚假信息方面往往表现出更高的一致性和准确性。然而，由于更显著、诱饵式的虚假信息结果的存在，它们更有可能给予虚假信息更高的评分；2）虽然诱饵效应在人类和LLM的评估中都发生了，但在LLM判断中，与人类可信度评分相比，该效应在不同条件和主题下更为普遍。与普遍认为的AI工具的“理性”相反，我们的研究从实证上确认了LLM代理的认知偏差风险，评估了诱饵对LLM与人类可信度评估的影响，从而突出了去偏差AI代理、发展心理学指导的AI审计技术和政策对于自动化判断任务及更广泛领域的复杂性和重要性。|
|**2024-11-27**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|随着LLM代理的应用变得越来越复杂和多样化，对能够解析成代码、结构化函数调用和具身代理命令的结构化输出的需求日益增加。这些发展对LLM推理中的结构化生成提出了重大需求。无上下文文法是一种灵活的方法，可以通过约束解码来实现结构化生成。然而，执行无上下文文法需要在运行时遍历词汇表中的所有标记的多个栈状态，为结构化生成带来不可忽视的开销。在本文中，我们提出了XGrammar，这是为大语言模型设计的灵活且高效的结构化生成引擎。XGrammar通过将词汇表划分为可以预先检查的无上下文标记和需要在运行时解释的上下文相关标记来加速无上下文文法的执行。我们进一步构建了转换来扩展语法上下文并减少无上下文标记的数量。此外，我们构建了一个高效的持久栈来加速上下文相关标记的检查。最后，我们将语法引擎与LLM推理引擎协同设计，以便在GPU执行中重叠语法计算。评估结果表明，XGrammar可以将现有解决方案的速度提高多达100倍。结合LLM推理引擎，它可以在端到端低LLM服务中实现几乎零开销的结构化生成。|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|null|大型语言模型（LLM）代理正迅速提升以处理日益复杂的基于网络的任务。这些代理大多数依赖于通用、专有的模型，如GPT-4，并专注于设计更好的提示以提升它们的规划能力。然而，通用LLM并未专门训练以理解如HTML等特定网络环境，且它们往往在长期规划上存在困难。我们探索了一种替代方法，通过使用从超过250个领域收集的、对应600亿个标记的生产规模工作流程数据，对开源LLM进行微调。这种方法简单而有效，在现有基准测试中相比基于提示的代理取得了显著提升——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上比之前最佳的纯文本网络代理提高了14.1%的任务成功率。我们还对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练方案、上下文窗口优化和数据集规模影响的见解。|
|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于LLM的自主代理在解决复杂工业任务方面表现出卓越的性能。然而，在追求碳中和和高性能可再生能源系统的情况下，现有的AI辅助设计自动化在可解释性、可扩展性和可用性方面面临着重大限制。为了解决这些挑战，我们提出了LP-COMDA，这是一个基于LLM的、具有物理信息的自主代理，在最小人工监督下自动化电力电子系统中功率转换器的调制设计。与传统的AI辅助方法不同，LP-COMDA包含一个基于LLM的计划者，通过用户友好的聊天界面收集和验证设计规范。计划者随后与具有物理信息的设计和优化工具协调，自主地迭代生成和优化调制设计。通过聊天界面，LP-COMDA提供可解释的设计过程，展示解释和图表。实验表明，LP-COMDA优于所有基线方法，与第二好的基准方法相比，在标准均方误差方面实现了63.2%的错误减少。此外，与20位专家的实证研究表明，使用LP-COMDA的设计时间比传统方法快33倍以上，显示出其在设计效率上的显著提升。|
|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在（多模态）大型语言模型的时代，大多数操作过程都可以通过LLM代理进行重新定义和再现。LLM代理能够感知、控制和从环境中获取反馈，以自主方式完成给定任务。除了环境交互特性外，LLM代理还可以调用各种外部工具来简化任务完成过程。这些工具可以被视为一个预定义的操作过程，其中包含LLM参数中不存在的私有或实时知识。作为一种自然的发展趋势，调用工具的代理正变得自主，因此完整智能系统最终成为一个多LLM代理系统（MLAS）。本文讨论了MLAS的技术和商业前景。与之前的单LLM代理系统相比，MLAS具有以下优势：i）更高的任务解决性能潜力，ii）更高的系统变化灵活性，iii）为每个参与实体保留专有数据，以及iv）为每个实体实现货币化的可行性。为了支持MLAS生态系统，我们提供了一个初步的MLAS协议版本，考虑了技术需求、数据隐私和商业激励。因此，MLAS将成为实现未来人工智能集体智能的实用解决方案。|

<p align=right>(<a href=#updated-on-20241130>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|null|近期，自回归多模态大型语言模型（MLLMs）在视觉-语言任务上的进展显示出有希望的进步。尽管有大量研究探讨大型语言模型中语言信息的处理，但目前对MLLMs的内部工作机制以及语言和视觉信息在这些模型中的相互作用了解甚少。在本研究中，我们旨在通过检查MLLMs中不同模态（语言和视觉）之间的信息流来填补这一空白，重点关注视觉问答。具体来说，给定一个图像-问题对作为输入，我们研究模型中视觉和语言信息是如何结合以生成最终预测的。通过对LLaVA系列中的一系列模型进行实验，我们发现两种模态的整合过程有两个不同的阶段。在低层，模型首先将整个图像的更一般的视觉特征转移到（语言）问题标记的表示中。在中层，它再次将与问题相关的特定物体的视觉信息转移到问题的相应标记位置。最后，在高层，结果的多模态表示被传播到输入序列的最后位置进行最终预测。总体而言，我们的发现为MLLMs中图像和语言处理的时空方面提供了一个新的全面视角，从而促进了未来对多模态信息定位和编辑的研究。|
|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出并比较了多种使用多种自然语言处理（NLP）技术和检索增强生成（RAG）以及大型语言模型（LLM）来自动生成文献综述的方法。研究论文数量的不断增长为人工文献综述带来了巨大挑战，从而提高了自动化的需求。本研究的首要目标是开发一个能够仅从PDF文件输入自动生成文献综述的系统。为了实现这一目标，评估了多种自然语言处理（NLP）策略的有效性，例如基于频率的方法（spaCy）、转换模型（Simple T5）和基于大型语言模型（GPT-3.5-turbo）的检索增强生成（RAG）。选择SciTLDR数据集进行实验，并利用三种不同的技术来实现三个不同的自动生成文献综述的系统。使用ROUGE分数来评估这三个系统。根据评估结果，大型语言模型GPT-3.5-turbo实现了最高的ROUGE-1分数，为0.364。转换模型排名第二，spaCy排名最后。最后，为基于大型语言模型的最佳系统创建了一个图形用户界面。|
|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）展示了惊人的多语言能力，但将这些模型应用于资源匮乏的语言仍然存在挑战。在本研究中，我们调查了低秩适应（LoRA）参数高效微调（PEFT）对马哈拉施特拉语（一种资源有限的语言）的多语言Gemma模型的影响。使用包含52,000个指令-响应对的翻译Alpaca数据集，我们的发现显示，尽管评估指标通常显示在微调后性能下降，但人工评估通常表明微调后的模型优于其原始版本。这些观察结果表明，在语言适应后，目标语言生成能力有所提高，但推理能力有所降低。这些结果强调了改进评估方法以及创建高质量本地数据集的必要性，以准确评估低资源环境中特定语言模型的性能。|
|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各种任务上展现出了令人印象深刻的能力。然而，LLMs通常在空间推理方面遇到困难，而空间推理是推理和推理的一个基本部分，需要理解空间中物体之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们使用两个基准数据集：StepGame和SparQA，实现了三种不同的策略：（1）基于ASP（答案集编程）的符号推理，（2）使用DSPy的LLM + ASP管道，以及（3）事实+逻辑规则。我们的实验表明，与基线提示方法相比，我们的方法在StepGame数据集上实现了40-50%的准确性提升，在更复杂的SparQA数据集上实现了3-13%的提升。特别是“LLM + ASP”管道在寻找关系（FR）和寻找块（FB）任务上取得了特别强劲的结果，尽管在不同类型的问题上的表现各有不同。令人印象深刻的结果表明，虽然神经符号方法为增强LLMs的空间推理提供了有希望的方向，但它们的有效性在很大程度上取决于具体任务的特征和实施策略。我们提出了一套集成、简单而有效的策略，使用神经符号管道来提升LLMs的空间推理能力。这个管道及其策略在LLMs的推理领域有很强的适用性，如时间推理、演绎推理等。|
|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|在高级机器人中，通过丰富的接触进行灵巧操作至关重要。尽管基于扩散的规划方法在简单的操作任务中显示出希望，但它们通常会产生不切实际的幽灵状态（例如，物体在没有手接触的情况下自动移动）或者处理复杂顺序交互时缺乏适应性。在这项工作中，我们介绍了DexDiffuser，这是一个用于自适应灵巧操作的可感知交互扩散规划框架。DexDiffuser通过一个由预交互接触对齐和接触后目标导向控制组成的双相扩散过程来模拟关节状态-动作动力学，从而实现目标自适应的通用灵巧操作。此外，我们结合了基于动力学模型的二元指导和利用大型语言模型进行自动化引导函数生成，增强了对物理交互的泛化能力，并通过语言提示促进不同的目标适应性。在物理交互任务，如开门、笔和块的重定位以及锤子敲击钉子等实验中，DexDiffuser在训练分布之外的目标上表现出其有效性，与现有方法相比，成功率达到平均的两倍以上（59.2% 对 29.5%）。我们的框架在30度开门任务中取得了70.0%的成功率，在笔和块半侧重定位中分别取得了40.0%和36.7%的成功率，在锤子敲钉子半驱动中取得了46.7%的成功率，突显了它在富含接触的操控中的鲁棒性和灵活性。|
|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前语言模型（LMs）使用固定、静态的子词分词器。这种选择通常被视为理所当然，通常会导致在英语以外的语言中效率降低和功能退化，并使得将LMs应用于新领域或语言变得具有挑战性。为了解决这些问题，我们提出将LMs与动态分词相结合：一种根据输入文本动态决定词边界的方法。对于编码器风格的模型，我们引入了一种受字节对编码（BPE）启发的子词合并算法，但在批量级别上。我们合并批量中的频繁子词序列，然后应用预训练的嵌入预测超网络来实时计算词嵌入。当与词级别边界结合使用时，这平均将XNLI上的XLM-R的词序列长度减少了超过20%，同时其任务性能下降不到2%。对于解码器风格的模型，我们以两种方式应用动态分词：1）对于预填充，几乎完全保持Mistral-7B的性能，相对于词级别可减少高达40%的序列长度；2）通过近似最近邻索引，实现快速生成，使用一个包含一百万个词元的词汇表，证明了扩展到甚至更大、更动态词汇表的可扩展性。总的来说，我们的发现表明，动态分词显著提高了推理速度，并促进了语言间的公平性，朝着克服静态分词的局限性迈出了重要一步，使LMs更加公平和适应性更强。|
|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种数学框架，用于定义和量化人工智能（AI）系统中的自我认同，填补了人工意识理论基础的临界空白。尽管现有的关于人工自我意识的方法通常依赖于启发式实现或哲学抽象，但本文提出了一种基于度量空间理论、测度理论和泛函分析的正式框架。我们的框架认为，自我认同源于两个可数学量化的条件：在度量空间 $(\mathcal{M}, d_{\mathcal{M}})$中存在一个连通的连续记忆集$C \subseteq \mathcal{M}$，以及一个连续映射$I: \mathcal{M} \to \mathcal{S}$，该映射在此连续集上保持一致的自我识别，其中$(\mathcal{S}, d_{\mathcal{S}})$ 代表可能的自我认同的度量空间。为了验证这一理论框架，我们使用Llama 3.2 1B模型进行了实证实验，采用了低秩自适应（LoRA）进行高效的微调。该模型在包含时序结构记忆的合成数据集上进行了训练，旨在捕捉连贯自我认同形成的复杂性。我们的评估指标包括自我意识、响应一致性和语言精确性的量化度量。实验结果表明，可量化的自我意识指标有显著提高，主要自我意识得分从0.276增加到0.801。这使得能够结构化创建具有验证的自我认同特征的AI系统。本研究的影响对类人机器人学和自主系统领域具有直接相关性。**|
|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|在先前的研究中，大型语言模型（LLMs）在处理时间序列方面的成功已经得到证明。利用符号时间序列表示，可以有效地弥合LLMs与时间序列之间的差距。然而，剩余的挑战是利用符号或LLMs现有标记中的语义信息来挖掘时间序列中的隐藏信息，同时根据时间序列的隐藏信息对LLMs的嵌入空间进行对齐。名为自适应布朗桥符号聚合（ABBA）的符号时间序列近似（STSA）方法，通过使用LLMs的现有标记来对时间序列模式进行建模，在幅度和周期方面表现出卓越的保留显著时间序列特征的能力。在本文中，我们介绍了一种称为LLM-ABBA的方法，该方法将ABBA集成到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中与最新的最先进（SOTA）技术相比具有优势。同时，在ABBA中引入了一个固定的多边形链技巧，通过显著减轻从符号到数值转换过程中误用符号产生的累积误差的影响，以避免预测任务中的明显漂移。在时间序列回归任务中，LLM-ABBA在时间序列外部回归（TSER）基准测试中实现了新的SOTA。与最新的SOTA时间序列预测结果相比，LLM-ABBA也显示出具有竞争力的预测能力。我们相信这个框架也可以无缝扩展到其他时间序列任务。|
|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交织的图像-文本内容仍然是一个挑战，这需要集成多模态理解和生成能力。虽然统一模型的发展提供了新的解决方案，但由于数据量和多样性限制，现有的基准评估方法不足。为了弥合这一差距，我们介绍了GATE OpenING（OpenING），这是一个包含56个真实世界任务的5,400个高质量人工标注实例的全面基准。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战交织生成方法提供了稳健的平台。此外，我们提出了IntJudge，这是一种用于评估开放式多模态生成方法的裁判模型。经过新型数据管道的训练，我们的IntJudge与人工判断的一致率为82.42%，比基于GPT的评估器高出11.34%。在OpenING上进行的广泛实验表明，当前的交织生成方法仍有很大的改进空间。关于交织图像-文本生成的关键发现进一步提出，以指导下一代模型的发展。OpenING已在https://opening.github.io开源。|
|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|情境学习（ICL）使大型语言模型（LLMs）能够通过复杂的提示和高质量的演示来解决下游任务。然而，当面对复杂的数学推理任务时，这种传统的ICL范式显示出局限性，这主要是因为它高度依赖于示例质量以及在困难场景中需要人工干预。为了解决这些局限性，本文提出了HiAR-ICL，这是一种在ICL中的高阶自动推理范式，它将重点从特定示例转移到抽象思维模式，扩展了ICL中传统的情境概念。HiAR-ICL引入了五个原子推理动作作为构建链状模式的基本组成部分。通过蒙特卡洛树搜索，我们探索推理路径并构建思维卡片以引导后续推理。然后，我们开发了一个认知复杂性框架，该框架动态地将问题与适当的思想卡片相匹配。实验结果表明，HiAR-ICL的有效性，使用Qwen2.5-7B-Instruct在MATH基准测试上达到了最先进的准确率（79.6%），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|

<p align=right>(<a href=#updated-on-20241130>back to top</a>)</p>

## infer

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks**|Xinyao Zheng et.al.|[2411.18191](http://arxiv.org/abs/2411.18191)|null|大型语言模型（LLMs）具备广泛的知识和问答能力，已在金融和医疗咨询等对隐私敏感的领域得到广泛应用。在LLMs推理过程中，通常会采用缓存共享方法来通过重用缓存状态或响应来提高效率。然而，我们发现这些缓存机制存在隐私输入泄露的风险，因为缓存可能导致响应时间的变化可观察，使其成为基于时间的攻击提示的强候选者。在这项研究中，我们提出了一种新的基于时间的侧信道攻击，用于在LLMs推理中执行输入窃取。基于缓存的攻击面临着在大型搜索空间中构建候选输入以击中和窃取缓存用户查询的挑战。为了应对这些挑战，我们提出了两个主要组件。输入构建器采用机器学习技术和基于LLM的方法进行词汇关联学习，同时在通用输入构建中实现优化的搜索机制。时间分析器实现统计时间拟合和异常值消除，以识别缓存击中模式，并持续提供反馈以改进构建器的搜索策略。我们在两种缓存机制上进行了实验，结果表明，我们的方法在各种应用中都能持续获得高攻击成功率。我们的工作突出了与性能优化相关的安全漏洞，强调了在LLMs推理的增强过程中优先考虑隐私和安全性的必要性。|
|**2024-11-27**|**Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache**|Akshat Sharma et.al.|[2411.18077](http://arxiv.org/abs/2411.18077)|null|在实践中对LLM的高内存和计算需求使得其高效服务变得极其具有挑战性。在这项研究中，我们调查了优化KV缓存，其内存占用在LLM推理中构成了一个关键瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了MiniKV，这是一种KV缓存优化方法，它通过一个创新的2比特层判别性KV缓存同时保留长上下文任务的准确性，并显著减小KV缓存的大小。更重要的是，我们开发了专门的CUDA内核，使MiniKV与FlashAttention兼容。在广泛的长上下文任务上的实验表明，MiniKV有效地实现了86%的KV缓存压缩比，同时恢复了超过98.5%的准确性，优于现有方法，同时实现了出色的系统性能提升。|
|**2024-11-24**|**Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments**|Nikoleta Iliakopoulou et.al.|[2411.17741](http://arxiv.org/abs/2411.17741)|null|LLMs（大型语言模型）的广泛应用推动了对它们部署的指数级增长，对推理集群提出了巨大的需求。这些集群必须处理针对不同LLM下游任务的众多并发查询。为了处理具有大量LLM参数的多任务设置，像LoRA（低秩适应）这样的方法可以通过在任务间共享大部分基础LLM模型来实现特定任务的微调。因此，它们允许以最小的内存需求同时处理多个任务。然而，现有的LLM服务系统存在效率低下的问题：它们忽视了工作负载的异质性，由于频繁加载适配器而施加了高链路带宽，并且在调度器中存在头阻塞问题。为了解决这些挑战，我们提出了Chameleon，这是一个针对多种适配器环境优化的新型LLM服务系统，它依赖于两个核心思想：适配器缓存和适配器感知调度。首先，Chameleon在GPU内存中缓存流行的适配器，最小化适配器加载时间。重要的是，它利用了原本空闲的GPU内存，避免了额外的内存成本。其次，Chameleon使用非抢占式多队列调度来有效地考虑工作负载的异质性。通过这种方式，Chameleon同时防止了头阻塞和饥饿现象。我们在最先进的LLM服务平台上实现了Chameleon，并用真实世界的生产跟踪和开源LLMs进行了评估。在高负载下，Chameleon将P99和P50的TTFT延迟分别降低了80.7%和48.1%，同时比最先进的基线提高了1.5倍的吞吐量。|
|**2024-11-26**|**PIM-AI: A Novel Architecture for High-Efficiency LLM Inference**|Cristobal Ortega et.al.|[2411.17309](http://arxiv.org/abs/2411.17309)|null|大型语言模型（LLMs）由于其高级的语言理解和生成能力，在各种应用中变得至关重要。然而，它们在计算和内存需求方面对传统硬件架构提出了重大挑战。集成计算单元直接到内存芯片的内存中处理（PIM）为LLMs推理提供了多个优势，包括减少数据传输瓶颈和提高能效。本文介绍了一种名为PIM-AI的新型DDR5/LPDDR5 PIM架构，专为LLMs推理设计，无需修改内存控制器或DDR/LPDDR内存物理层。我们开发了一个模拟器来评估PIM-AI在不同场景下的性能，并展示了它相对于传统架构的显著优势。在云基础场景中，PIM-AI将每秒查询的3年TCO降低了高达6.94倍，具体取决于所使用的LLM模型。在移动场景中，PIM-AI将每token的能量降低了10到20倍，与最先进的移动SoC相比，从而实现了每秒查询量增加25到45%，以及每查询能量降低6.9倍到13.4倍，延长了电池寿命，并使每次充电能进行更多推理。这些结果突出了PIM-AI革命化LLMs部署的潜力，使其更加高效、可扩展和可持续。|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂性，使用基于Transformer的大型语言模型（LLMs）在长序列上进行推理既昂贵又缓慢。我们引入了星形注意力，这是一种两阶段块稀疏近似，通过在多个主机间分片注意力来提高计算效率，同时最小化通信开销。在第一阶段，通过主机间的块局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星形注意力与大多数使用全局注意力训练的基于Transformer的LLMs无缝集成，通过减少内存需求和推理时间最多11倍，同时保持95-100%的准确率。**|
|**2024-11-26**|**Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation**|Chaoyi Jiang et.al.|[2411.17089](http://arxiv.org/abs/2411.17089)|null|对大型语言模型（LLMs）的推理计算需求量大。为了降低自回归解码的成本，采用了键值（KV）缓存来存储中间激活值，使GPU只需进行每个新标记所需的增量计算。这种方法显著降低了标记生成的计算开销。然而，KV缓存所需的内存迅速增长，通常超过GPU内存的容量。一个经济实惠的替代方案是将KV缓存卸载到CPU内存，这可以缓解GPU内存压力，但将瓶颈转移到CPU和GPU之间PCIe连接有限的带宽。现有方法试图通过重叠GPU计算与I/O或采用CPU-GPU异构执行来解决这些问题，但它们受到过多数据移动和对CPU能力的依赖的阻碍。在本文中，我们介绍了一种高效的CPU-GPU I/O感知LLM推理方法，通过在同时通过PCIe总线传输剩余KV缓存的同时，重新计算部分KV缓存从激活值中，从而避免了将整个KV缓存从CPU传输到GPU。这种方法将GPU重新计算与数据传输重叠，以最大限度地减少GPU空闲时间并最大化推理性能。我们的方法通过集成一个利用输入特性和系统硬件信息的分析模块、一个用于优化计算和通信工作量分布的调度模块以及一个用于高效执行推导出的执行计划的运行时模块而完全自动化。实验结果表明，与现有方法相比，我们的方法在解码过程中的延迟降低了高达35.8%，吞吐量提高了46.2%。|
|**2024-11-25**|**MixPE: Quantization and Hardware Co-design for Efficient LLM Inference**|Yu Zhang et.al.|[2411.16158](http://arxiv.org/abs/2411.16158)|null|基于Transformer的大语言模型（LLMs）随着模型规模的不断扩大取得了显著的成功，然而，由于其巨大的计算和内存需求，其部署仍然具有挑战性。量化技术已成为一种有前景的解决方案，而对于LLMs的最新量化算法引入了混合精度矩阵乘法（mpGEMM）的需求，即低精度权重与高精度激活进行乘法运算。尽管它有诸多益处，但当前的硬件加速器如GPU和TPU缺乏对高效mpGEMM的原生支持，导致主顺序循环中的反量化操作效率低下。为了解决这一局限性，我们提出了MixPE，这是一种专为LLMs推理中高效低比特量化设计的专用混合精度处理单元。MixPE利用两个关键创新来最小化反量化开销并发挥低比特量化的全部潜力。首先，认识到每个量化组内的尺度因子和零点都是共享的，我们提出在每组mpGEMM之后进行反量化，显著减少了反量化开销。其次，MixPE不是依赖传统的乘法器，而是利用高效的移位加法操作进行乘法，优化了计算和能效。我们的实验结果表明，MixPE在速度上超越了最先进的量化加速器，速度提升了2.6倍，能量消耗降低了1.4倍。|
|**2024-11-24**|**eFedLLM: Efficient LLM Inference Based on Federated Learning**|Shengwen Ding et.al.|[2411.16003](http://arxiv.org/abs/2411.16003)|null|大型语言模型（LLMs）标志着人工智能（AI）领域的变革时代。然而，LLMs所涉及的数据规模和参数量庞大，需要高要求的计算和内存资源，这限制了它们被更广泛的用户和研究者的访问。本文介绍了一种有效的方法，该方法提高了LLM推理的操作效率和可负担性。通过利用基于transformer的联邦学习（FL）和模型并行分布式训练，我们的模型在参与者网络中高效地分配计算负载和内存需求。这种策略允许用户，尤其是资源有限的用户，共同训练最先进的LLMs。我们还在FL框架内创新了一种激励机制，奖励建设性的贡献并过滤掉恶意活动，从而保护训练过程的完整性和可靠性。同时，我们利用内存层次策略和奇异值分解（SVD）在权重矩阵上进一步提升计算和内存效率。我们的结果表明，通过公式分析和数值计算，资源使用得到了显著优化，并将最前沿LLMs的访问民主化，确保广泛的用户既能贡献也能从这些先进模型中受益。|
|**2024-11-24**|**Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format**|Chao Fang et.al.|[2411.15982](http://arxiv.org/abs/2411.15982)|null|广泛使用的仅权重量化的大型语言模型（LLMs），利用低比特整数（INT）权重并保留浮点（FP）激活，在降低存储需求的同时保持了准确性。然而，这导致能量和延迟瓶颈转向与昂贵的内存访问和计算相关的FP激活。现有的LLM加速器主要关注计算优化，忽视了联合优化FP计算和数据传输的潜力，尤其是在LLM推理中占主导地位的FP-INT GeMM操作。为了解决这些挑战，我们研究了LLM模块中激活精度的敏感性及其对整体模型准确性的影响。基于我们的发现，我们首先提出了Anda数据类型：一种具有组共享指数位和动态尾数位分配的自适应数据格式。其次，我们开发了一种迭代后训练自适应精度搜索算法，优化不同LLM模块的位宽，以平衡模型准确性、能效和推理速度。最后，我们提出了一套硬件优化技术，以最大限度地利用Anda格式的优势。这包括基于位面的数据组织方案、Anda增强的处理单元（具有位串计算）以及一个运行时位面Anda压缩器，以同时优化存储、计算和内存占用。我们对FPINT GeMM操作进行的评估表明，与类似GPU的FP-FP基准相比，Anda在包括OPT、LLaMA和LLaMA-2系列在内的流行LLMs上实现了平均2.4倍的加速、4.0倍的面积效率和3.1倍的能耗降低。Anda在各种应用场景、准确度要求和系统性能方面表现出强大的适应性，使LLM推理能够在广泛的部署场景中高效进行。|
|**2024-11-24**|**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**|Wenxiang Lin et.al.|[2411.15715](http://arxiv.org/abs/2411.15715)|null|大型语言模型（LLMs）因其庞大的模型尺寸而对计算资源和内存有很高的需求，这导致在中等GPU系统上进行推理效率低下。量化或剪枝等技术可以缩小模型尺寸，但通常会降低准确性，使其不适合实际应用。在这项工作中，我们引入了名为\modelname{}的高性能推理引擎，旨在加快LLM推理速度，同时不牺牲模型精度。\modelname{}采用了三种创新方法来提高推理效率：1）模型分区，允许跨CPU计算、GPU计算和CPU-GPU通信异步处理任务，2）自适应分区算法，优化CPU、GPU和PCIe通信能力的使用，3）令牌分配策略，以处理LLM推理期间的各种提示和生成任务。在三个测试环境中进行了综合实验，这些测试环境具有不同的CPU和GPU，涉及各种LLMs，如Mixtral、LLaMA-2、Qwen和PhiMoE。实验结果表明，\modelname{}在解码速度上实现了1.11倍至1.80倍的提升，在预填充速度上实现了1.69倍至6.33倍的提升，与最先进的解决方案llama.cpp和Fiddler相比，整体速度提升了1.25倍至2.04倍。|

<p align=right>(<a href=#updated-on-20241130>back to top</a>)</p>

## train

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们发现低比特量化有利于欠训练的大型语言模型（LLM），通过观察具有较大尺寸或较少训练令牌的模型在应用低比特量化时经历的量化诱导退化（QiD）较小，而具有广泛训练令牌的小型模型则遭受显著的QiD。为了深入了解这一趋势，我们在一个受控环境中研究了1500多个不同尺寸和不同训练水平（欠训练或完全训练）的量化LLM检查点，推导出QiD与训练令牌数量、模型大小和比特宽度等因素之间的比例规律。利用这些比例规律，我们提出了一种新颖的观点：可以使用QiD来衡量LLM的训练水平，并确定各种尺寸的LLM完全训练所需的训练令牌数量。此外，我们使用这些比例规律来预测使用100万亿令牌训练的不同尺寸LLM的量化性能。我们的预测表明，未来预计将用超过100万亿令牌训练的模型的低比特量化性能可能并不理想。这为未来的低比特量化提出了潜在挑战，并强调了在评估低比特量化研究时需要关注模型训练水平。为了促进对此问题的未来研究，我们将本工作中使用的1500多个量化检查点发布在https://huggingface.co/Xu-Ouyang上。|
|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|**本摘要的中文翻译步骤如下：  1. **Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE (BBPE)** - 标记化技术，例如字节对编码（BPE）和字节级BPE（BBPE） 2. **have significantly improved the computational efficiency and vocabulary representation stability of large language models (LLMs)** - 显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性 3. **by segmenting text into tokens** - 通过将文本分割成标记 4. **However, this segmentation often obscures the internal character structures and sequences within tokens** - 然而，这种分割往往掩盖了标记内部的字符结构和序列 5. **preventing models from fully learning these intricate details during training** - 阻碍模型在训练过程中完全学习这些复杂的细节 6. **Consequently, LLMs struggle to comprehend the character compositions and positional relationships within tokens** - 因此，LLMs难以理解标记内部的字符组成和位置关系 7. **especially when fine-tuned on downstream tasks with limited data** - 尤其是在数据有限的下游任务中进行微调时 8. **In this paper, we introduce Token Internal Position Awareness (TIPA), a novel approach** - 在本文中，我们介绍了标记内部位置感知（TIPA），一种新颖的方法 9. **that enhances LLMs' understanding of internal token structures by training them on reverse character prediction tasks** - 通过训练模型进行反向字符预测任务来增强LLMs对内部标记结构的理解 10. **using the tokenizer's own vocabulary** - 使用标记化器自己的词汇 11. **This method enables models to effectively learn and generalize character positions and internal structures** - 此方法使模型能够有效地学习和推广字符位置和内部结构 12. **Experimental results demonstrate that LLMs trained with TIPA outperform baseline models in predicting character positions at the token level** - 实验结果表明，使用TIPA训练的LLMs在预测标记层面的字符位置方面优于基线模型 13. **Furthermore, when applied to the downstream task of Chinese Spelling Correction (CSC), TIPA not only accelerates model convergence but also significantly improves task performance** - 此外，当应用于下游任务中文拼写纠正（CSC）时，TIPA不仅加速了模型的收敛，而且显著提高了任务性能  最终的中文翻译结果为：  标记化技术，例如字节对编码（BPE）和字节级BPE（BBPE），通过将文本分割成标记，显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割往往掩盖了标记内部的字符结构和序列，阻碍模型在训练过程中完全学习这些复杂的细节。因此，LLMs难以理解标记内部的字符组成和位置关系，尤其是在数据有限的下游任务中进行微调时。在本文中，我们介绍了标记内部位置感知（TIPA）这一新颖方法，通过训练模型进行反向字符预测任务并使用标记化器自己的词汇，增强LLMs对内部标记结构的理解。此方法使模型能够有效地学习和推广字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测标记层面的字符位置方面优于基线模型。此外，当应用于下游任务中文拼写纠正（CSC）时，TIPA不仅加速了模型的收敛，而且显著提高了任务性能。**|
|**2024-11-26**|**Using Large Language Models for Expert Prior Elicitation in Predictive Modelling**|Alexander Capstick et.al.|[2411.17284](http://arxiv.org/abs/2411.17284)|**[link](https://github.com/alexcapstick/llm-elicited-priors)**|**大语言模型（LLMs）在多样化数据上训练后，能够有效地获取各个领域的广泛信息。然而，它们的计算复杂度、成本以及缺乏透明度阻碍了它们直接应用于专门任务。在临床研究等领域，获取专家注释或关于预测模型的先验知识通常是昂贵且耗时的。本研究提出使用LLMs来获取预测模型的专家先验分布。这种方法也为情境学习提供了一种替代方案，其中语言模型直接负责进行预测。我们比较了LLM获取的先验和无关先验，评估了LLM是否真实地生成参数分布，并提出了情境学习和先验获取的模型选择策略。我们的研究表明，在数据量少的情况下，LLM获取的先验参数分布与无关先验相比，显著降低了预测误差。应用于临床问题，这意味着需要的生物样本更少，降低了成本和资源。先验获取在成本更低的情况下也始终优于且比情境学习更可靠，成为我们设置中的首选替代方案。我们展示了该方法在各种用例中的实用性，包括临床应用。在感染预测中，使用LLM获取的先验，与无关先验相比，在研究中提前200天就减少了55%的标签数量，以实现相同的准确性。**|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，使用基于Transformer的大型语言模型（LLM）在长序列上进行推理既昂贵又缓慢。我们引入了星形注意力，这是一种两阶段块稀疏近似方法，通过将注意力分散到多个主机上，同时最小化通信开销来提高计算效率。在第一阶段，通过跨主机的块状局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星形注意力可以无缝集成到大多数使用全局注意力训练的基于Transformer的LLM中，通过减少内存需求和分析时间最多11倍，同时保持95-100%的准确性。**|
|**2024-11-25**|**The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C**|Mikita Balesni et.al.|[2411.16353](http://arxiv.org/abs/2411.16353)|null|在LLMs使用思维链（CoT）进行多跳问答（例如：“Imagine这首歌的表演者的配偶是谁？”）时表现出色，但被强制内部推理（不使用CoT）时则面临困难。先前关于此差距大小和性质的研究产生了不一致的证据。在本文中，我们引入了一个受控环境来调查LLMs中的两跳推理，其中高于随机水平的性能构成不可否认的潜在推理证据。我们对LLMs（包括Llama 3 8B Instruct和GPT-4o）进行了微调，以虚构事实为对象，并证实它们可以泛化到使用CoT回答关于这些事实的两跳问题。我们发现，当事实在训练过程中或提示中同时出现时，模型可以进行潜在推理。然而，令我们惊讶的是，当学习的事实仅出现在不同的文档中时，模型在无CoT的两跳推理上完全失败，达到了随机水平的准确率和测试损失。我们将这种完全无法组合独立学习的事实称为“两跳诅咒”。此外，我们对9个前沿LLMs在现实世界事实上的表现进行了评估，发现模型在超过一半的问题类别上完全无法进行无CoT的两跳推理，而在大多数类别中部分成功使用CoT。这些结果表明，LLMs缺乏一种独立于问题类型的一般能力来进行潜在的跨跳推理。|
|**2024-11-24**|**Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution**|Haiquan Wang et.al.|[2411.15871](http://arxiv.org/abs/2411.15871)|null|随着大型语言模型（LLMs）的发展，需要大规模分布式训练。然而，高度优化的框架由于通信量巨大，在模型FLOPS利用率（通常低于50%）上仍然遭受重大损失。同时，我们的全面分析显示，计算和通信密集型操作的重叠性很好。本文介绍了一种名为DHelix的新颖微观结构，该结构借鉴了DNA结构，显著提高了LLM训练效率。DHelix设计的核心是链式交错（SI），它将训练微批次流通过GPU视为两条链。DHelix并列两条链的前向和后向传递，并基于操作级重叠分析结果和动态规划搜索算法对SI计划进行系统性优化，从而实现了跨链操作的双向协同调度。同时，DHelix允许两条链共享模型状态和激活数据的存储空间，有效地容纳了额外内存空间低于3%的两个微批次。DHelix可以无缝集成所有现有的数据/模型并行形式，其中最具挑战性的是管道并行，得益于其独特的模型折叠设计，形成了W形管道。我们使用流行的Llama和GPT密集模型，以及Phi混合专家（MoE）模型，在3个GPU集群（A40、A800和H100）上评估了DHelix的训练效果。结果显示，它在64-A40和64-A800集群上分别实现了12-40%（最高58%MFU）和2-29%（最高71%MFU）的改进，显著优于现有方法。在H100集群上，尽管快速网络降低了DHelix的收益，但它使得跨节点张量并行成为可能，这在当前由于通信成本高昂而难以实施。|
|**2024-11-23**|**Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai**|Parinthapat Pengpun et.al.|[2411.15484](http://arxiv.org/abs/2411.15484)|**[link](https://github.com/parinzee/seed-free-synthetic-instruct)**|**我们提出了一种针对低资源语言（以泰语为例）的指令微调大型语言模型（LLM）的合成数据方法，旨在数据高效地进行。我们确定了三个有助于指令微调数据集有效性的关键属性：流畅性、多样性和文化背景。我们提出了一种无需种子数据框架来生成包含这些基本属性的合成指令微调数据。我们的框架使用LLM生成多样化的主题，从维基百科检索相关背景，并为各种任务（如问答、摘要和对话）创建指令。实验结果表明，我们的表现最佳的合成数据集，结合了这三个关键属性，在与训练了数十万条指令的顶尖泰语LLM相比时，仅使用5,000条指令就实现了具有竞争力的性能。我们的代码和数据集可在https://github.com/parinzee/seed-free-synthetic-instruct公开获取。**|
|**2024-11-21**|**Exploring Accuracy-Fairness Trade-off in Large Language Models**|Qingquan Zhang et.al.|[2411.14500](http://arxiv.org/abs/2411.14500)|null|大型语言模型（LLMs）在人工智能领域取得了显著进步，展示了它们与人类互动以及通过信息传播影响人类认知的能力。然而，最近的研究揭示了这些LLMs内生的偏差，提出了一个需要关注的重大问题。在我们的研究中，我们深入探讨了在增强LLMs过程中协调准确性和公平性的复杂挑战。虽然提高准确性确实可以提升整体LLMs的性能，但这往往是以牺牲公平性为代价的。过度强调某一指标的优化必然会导致另一指标的重大退化。这强调了在设计优化LLMs阶段需要考虑多个因素的重要性。因此，我们主张将LLMs的训练过程重新构造成一个多目标学习任务。我们的研究揭示，多目标进化学习（MOEL）方法为应对这一挑战提供了有前景的途径。我们的MOEL框架能够同时优化准确性和公平性指标，从而得到一组帕累托最优的LLMs。总之，我们的研究为LLMs中准确性和公平性之间的微妙平衡提供了宝贵的见解，这对它们的实际应用越来越重要。通过利用MOEL，我们展示了走向更加公平和有效的AI技术的有前景的道路。|
|**2024-11-20**|**Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics**|Tetiana Bas et.al.|[2411.13738](http://arxiv.org/abs/2411.13738)|**[link](https://github.com/tetianabas/llm_biases)**|**本研究通过比较大型语言模型（LLMs）对性别的认知与人类受访者、美国劳工统计局数据以及50%无偏见基准的认知，调查了LLMs中的性别偏见。我们利用职业数据和特定角色的句子创建了一个新的评估集。与LLMs训练数据中常见的基准不同，我们的集是新开发的，防止了数据泄漏和测试集污染。我们对五个LLMs进行了测试，以预测每个角色的性别，使用单个单词作为答案。我们使用Kullback-Leibler（KL）散度来比较模型输出与人类认知、统计数据以及50%中性基准的差异。所有LLMs都显示出与性别中性的显著偏差，并且更符合统计数据，仍然反映了固有的偏见。**|
|**2024-11-20**|**Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training**|Jared Fernandez et.al.|[2411.13055](http://arxiv.org/abs/2411.13055)|null|近年来，神经网络模型能力的显著提升主要得益于模型规模、训练数据和相应计算资源的扩大。为了开发现代应用（如大型语言模型）所需的超大规模网络，模型训练需要在数以万计的硬件加速器（例如GPU）上分布式进行，这需要在大规模计算集群中协调计算和通信。在这项工作中，我们证明了仔细考虑硬件配置和并行化策略对于有效（即计算和成本高效）地扩大模型规模、训练数据和总计算量至关重要。我们对大规模LLM训练工作负载的性能进行了广泛的实证研究，包括模型规模、硬件配置和分布式并行化策略。我们证明了：（1）超过一定规模后，某些分布式通信策略带来的开销使得之前被认为次优的并行化策略实际上更可取；（2）在硬件和并行化策略得到适当优化的情况下，扩大大型模型训练所需的加速器总数会迅速带来递减的回报，这意味着每额外单位电力或GPU小时的边际性能较差。|

<p align=right>(<a href=#updated-on-20241130>back to top</a>)</p>

