## Updated on 2024.12.02
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
    <li><a href=#infer>infer</a></li>
    <li><a href=#train>train</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴无声语音系统在恢复言语障碍患者的沟通能力方面具有巨大的潜力。然而，流畅、连贯的语音仍然难以实现，临床效果也尚未得到证实。在此，我们提出了一种基于人工智能的智能喉部（IT）系统，该系统集成了喉部肌肉振动和颈动脉脉搏信号传感器，并与大型语言模型（LLM）处理相结合，以实现流畅、富有情感表达的沟通。系统利用超灵敏的纺织品应变传感器捕捉颈部区域的高质量信号，并支持token级别的处理，以实现实时、连续的语音解码，从而实现无缝、无延迟的沟通。在测试了五位患有运动性构音障碍的卒中患者后，IT的LLM代理智能地纠正了token错误，丰富了句子层面的情感和逻辑连贯性，实现了低错误率（4.2%的单词错误率，2.9%的句子错误率）和用户满意度55%的提升。这项工作为运动性构音障碍患者建立了一个便携、直观的沟通平台，具有广泛应用于不同神经系统疾病和多语言支持系统的潜力。|
|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在各种领域，包括机器人操作和导航，展现出了惊人的规划能力。虽然最近在机器人领域的努力已经利用LLMs进行高级和低级规划，但这些方法通常面临重大挑战，如长周期任务中的幻觉以及由于一次生成计划且缺乏实时反馈而导致的适应性有限。为了解决这些限制，我们提出了一种新的多智能体LLM框架，即用于操作的多智能体大型语言模型（MALMM），它将高级规划和低级控制代码生成分布在专门的LLM智能体之间，并由一个额外的智能体动态管理过渡。通过在每个步骤后结合环境观察，我们的框架有效地处理中间失败，并实现适应性重新规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或情境学习示例，并能泛化到各种新任务。我们在包括长周期任务的九个RLBench任务上评估了我们的方法，并展示了其在零样本设置下解决机器人操作的能力，从而克服了现有基于LLM的操作方法的关键限制。|
|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着自然语言生成模型——称为大型语言模型（LLMs）——的最近发展，一个潜在的应用场景被打开，以改善人类与机器人助手互动的方式。这些LLMs应该能够利用其广泛的理解能力，将自然语言命令解释为有效、任务相关且安全的机器人任务执行。然而，在现实中，这些模型存在幻觉问题，这可能会导致安全问题的出现或任务偏离。在其他领域，这些问题通过使用协作人工智能系统得到了改善，在这些系统中，多个LLM代理可以共同规划、编码和自我检查输出。在本研究中，测试了多个协作人工智能系统与单个独立人工智能代理之间的比较，以确定在其他领域的成功是否可以转化为改进的人机交互性能。结果表明，代理数量与模型成功之间没有明确的趋势。然而，很明显，一些协作人工智能代理架构可以显著提高产生无错误代码和解决抽象问题的能力。|
|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理建模（ABM）已成为模拟社交网络的必要工具，涵盖了信息传播、影响动态和社区形成等多种现象。然而，手动配置各种代理交互和信息流动态具有挑战性，往往导致模型过于简化，缺乏现实世界的普适性。将现代大型语言模型（LLM）与ABM相结合，为解决这些挑战和提升模拟精度提供了一种有前景的方法，利用LLM在感知、推理和行为方面类似人类的能力。在本文中，我们提出了一种新颖的框架，利用LLM赋能的代理根据用户的兴趣和个性特征来模拟社交网络用户。该框架允许自定义代理交互，类似于各种社交网络平台，包括内容再分享和个性化推荐的机制。我们使用2020年美国选举的综合Twitter数据集验证了我们的框架，结果表明，LLM代理能够准确地复制真实用户的行为，包括语言模式和政治倾向。这些代理形成了同质化的意识形态集群，并保留了他们社区的主要主题。值得注意的是，基于偏好的推荐显著影响了代理的行为，促进了更高的参与度、网络同质性和回音室的形成。总体而言，我们的发现强调了LLM代理在推进社交媒体模拟和揭示复杂在线动态中的潜力。|
|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大语言模型（LLMs）和强化学习（RL）是构建自主智能体的两种强大方法。然而，由于对游戏环境的理解有限，智能体往往依赖于低效的探索和试错，难以发展长期策略或做出决策。我们提出了一种方法，通过从交互记录中提取经验来模拟游戏环境的潜在规律，并将这些经验作为内部动机来引导智能体。这些以语言表达的经验具有高度灵活性，既可以直接协助智能体进行推理，也可以转化为奖励以指导训练。在我们的Crafter评估结果中显示，RL和LLM智能体都从这些经验中获益，从而提高了整体性能。|
|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|人工智能在自动信息判断任务中是否会出现认知偏差？尽管近年来在测量和缓解人工智能和大型语言模型（LLM）中的社会和算法偏差方面取得了进展，但LLM的行为是否“理性”，或者它们是否也容易受到人类认知偏差的影响，尚不清楚。为了解决这个未解决的问题，我们的研究包括一个众包用户实验和一个LLM驱动的模拟实验，在信息检索（IR）环境中，比较了LLM和人类评判者在潜在诱饵效应下的可信度评估，并从经验上考察了LLM在COVID-19医疗（误）信息评估任务中相对于传统的人类评估者的认知偏差程度。从被试间用户实验和LLM驱动的复制实验收集的结果表明：1）更大、更新的LLM在区分可信信息和虚假信息方面往往表现出更高的一致性和准确性。然而，由于存在更显著的诱饵虚假信息结果，它们更有可能给予虚假信息更高的评分；2）虽然诱饵效应在人类和LLM评估中都发生了，但在LLM判断中，这种效应在不同条件和主题上比人类的可信度评分更为普遍。与普遍认为的AI工具的“理性”不同，我们的研究从经验上确认了LLM代理中嵌入的认知偏差风险，评估了诱饵对LLM的影响与人类可信度评估的关系，从而突出了去偏AI代理、开发心理学指导的AI审计技术和政策对于自动判断任务及其重要性的复杂性和必要性。|
|**2024-11-27**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|随着LLM代理的应用变得越来越复杂和多样化，对能够解析为代码、结构化函数调用和具身代理命令的格式化输出的需求越来越高。这些发展对LLM推理中的结构化生成提出了重大要求。上下文无关语法是一种通过限制解码来启用结构化生成的灵活方法。然而，执行上下文无关语法需要遍历词汇表中所有标记的多个栈状态，在结构化生成中带来不可忽视的开销。在本文中，我们提出了XGrammar，这是一个为大型语言模型设计的灵活且高效的结构化生成引擎。XGrammar通过将词汇表划分为可以预先检查的上下文无关标记和需要在运行时解释的上下文相关标记来加速上下文无关语法的执行。我们进一步构建了转换来扩展语法上下文并减少上下文无关标记的数量。此外，我们构建了一个高效的持久栈以加速上下文相关标记的检查。最后，我们与LLM推理引擎协同设计语法引擎，以重叠语法计算与GPU执行。评估结果表明，XGrammar可以将现有解决方案的速度提高高达100倍。结合LLM推理引擎，它可以在端到端的低LLM服务中实现几乎零开销的结构化生成。|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|null|大型语言模型（LLM）代理正在快速进步，以处理越来越复杂的基于网络的任务。大多数这些代理依赖于通用、专有的模型如GPT-4，并专注于设计更好的提示来提高它们的规划能力。然而，通用LLM并未专门训练以理解如HTML等特定网络上下文，并且它们通常在长期规划方面遇到困难。我们探索了一种替代方法，即使用从250多个领域收集的生产规模工作流程数据来微调开源LLM，这些领域对应60亿个标记。这种方法简单但有效，在现有基准测试中相对于基于提示的代理显示出显著优势——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上相对于之前的最佳纯文本网络代理提高了14.1%的任务成功率。我们进一步对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练方案、上下文窗口优化以及数据集规模影响等方面的见解。|
|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于大型语言模型（LLM）的自主代理在解决复杂工业任务方面展现了卓越的性能。然而，在追求碳中和和高性能可再生能源系统的过程中，现有的AI辅助设计自动化在可解释性、可扩展性和可用性方面面临重大局限。为了解决这些挑战，我们提出了一种名为LP-COMDA的基于LLM、具有物理信息的自主代理，它以最小的人为监督自动化了电力电子系统中电力转换器的调制设计。与传统AI辅助方法不同，LP-COMDA包含一个基于LLM的计划者，通过用户友好的聊天界面收集和验证设计规范。计划者随后与物理信息设计优化工具协调，自主迭代生成和细化调制设计。通过聊天界面，LP-COMDA提供了一个可解释的设计过程，展示解释和图表。实验表明，LP-COMDA优于所有基线方法，在标准均方根误差方面，与第二好的基准方法相比，错误减少了63.2%。此外，与20位专家进行的实证研究表明，使用LP-COMDA的设计时间比传统方法快33倍以上，显示出其在设计效率上的显著提升。|
|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在（多模态）大型语言模型的时代，大多数操作流程都可以通过LLM代理进行重新定义和再现。LLM代理能够感知、控制和从环境中获取反馈，以自主方式完成给定任务。除了环境交互特性外，LLM代理还可以调用各种外部工具以简化任务完成过程。这些工具可以被视为包含私有或实时知识的预定义操作流程，而这些知识在LLM的参数中并不存在。作为发展的自然趋势，调用工具的机制正变成自主代理，因此，完整的智能系统最终成为一个多LLM代理系统（MLAS）。本文讨论了MLAS的技术和商业格局。与之前的单LLM代理系统相比，MLAS具有以下优势：i) 任务解决性能的更高潜力，ii) 系统变化的更高灵活性，iii) 为每个参与实体保留专有数据，以及iv) 每个实体货币化的可行性。为了支持MLAS生态系统，我们提供了一个初步版本的MLAS协议，考虑了技术需求、数据隐私和商业激励。因此，MLAS将成为实现近期人工集体智能的实用解决方案。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|null|近年来，自回归多模态大型语言模型（MLLMs）在视觉-语言任务方面取得了令人鼓舞的进展。虽然目前存在许多研究探讨大型语言模型中语言信息的处理，但对于MLLMs的内部工作机制以及语言和视觉信息在这些模型中如何交互的了解还相对有限。在本研究中，我们旨在通过考察MLLMs中不同模态（语言和视觉）之间的信息流，聚焦于视觉问答，来填补这一空白。具体来说，给定一个图像-问题对作为输入，我们研究了在模型中视觉和语言信息是如何结合以生成最终预测的。通过在LLaVA系列的一系列模型上开展实验，我们发现两个模态的整合过程分为两个不同的阶段。在底层，模型首先将整个图像的更一般化的视觉特征转移到（语言）问题标记的表示中。在中层，它再次将与问题相关的特定对象的视觉信息转移到问题的相应标记位置。最后，在高层，所得到的跨模态表示传播到输入序列的最后位置以进行最终预测。总体而言，我们的发现为MLLMs中图像和语言处理的空间和功能方面提供了新的全面视角，从而促进了未来对跨模态信息定位和编辑的研究。|
|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出并比较了多种使用多种自然语言处理（NLP）技术和检索增强生成（RAG）与大型语言模型（LLM）来自动生成文献综述的方法。研究论文数量的不断增长为手动文献综述带来了巨大挑战，从而增加了对自动化的需求。本研究的主要目标是开发一个系统，能够仅从PDF文件输入中自动生成文献综述。为了实现这一目标，评估了几种自然语言处理（NLP）策略的有效性，例如基于频率的方法（spaCy）、Transformer模型（Simple T5）以及带有大型语言模型（GPT-3.5-turbo）的检索增强生成（RAG）。为进行此研究实验，选择了SciTLDR数据集，并利用三种不同的技术实现了三个不同的系统，用于自动生成文献综述。使用ROUGE分数来评估这三个系统。根据评估结果，大型语言模型GPT-3.5-turbo实现了最高的ROUGE-1分数，为0.364。Transformer模型位居第二，spaCy位于最后。最后，基于大型语言模型的最优系统创建了一个图形用户界面。|
|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）在多语言能力方面表现出色，但在将模型应用于资源有限的语言时仍存在挑战。本研究调查了低秩适应（LoRA）参数高效微调（PEFT）对马拉地语Gemma多语言模型的影响，马拉地语是一种资源有限的语言。使用包含52,000条指令-响应对的翻译Alpaca数据集，我们的发现表明，尽管评估指标通常显示在微调后性能下降，但手动评估通常表明微调后的模型优于其原始版本。这些观察结果表明，在语言适应后，目标语言生成能力有所提高，但推理能力有所下降。这些结果强调了改进评估方法以及创建高质量本土数据集的必要性，以准确评估低资源环境下特定语言模型的性能。|
|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各项任务中展现了令人印象深刻的性能。然而，LLMs通常在空间推理方面存在困难，空间推理是推理和推理过程中不可或缺的一部分，需要理解空间中对象之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们在两个基准数据集——StepGame和SparQA上评估了我们的方法，并实施了三种不同的策略：（1）基于ASP（答案集编程）的符号推理，（2）使用DSPy的LLM + ASP管道，以及（3）事实+逻辑规则。我们的实验表明，与基线提示方法相比，我们的方法在StepGame数据集上实现了40-50%的准确率提升，在更复杂的SparQA数据集上提升了3-13%。特别是“LLM + ASP”管道在寻找关系（FR）和寻找块（FB）任务上取得了特别强的结果，尽管在不同问题类型上的性能有所不同。这些令人印象深刻的成果表明，尽管神经符号方法为增强LLMs的空间推理提供了有希望的途径，但其有效性在很大程度上取决于具体任务特征和实施策略。我们提出了一套集成、简单而有效的策略，利用神经符号管道来增强LLMs的空间推理能力。这个管道及其策略在LLMs的其他推理领域（如时间推理、演绎推理等）具有强大的广泛适用性。|
|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|在高级机器人中，具备丰富的接触交互的灵活操作至关重要。虽然基于扩散的规划方法在简单的操作任务上显示出潜力，但它们通常会产生不切实际的效果（例如，物体在没有手部接触的情况下自动移动）或者在处理复杂的连续交互时缺乏适应性。在这项工作中，我们介绍了DexDiffuser，这是一个针对自适应灵活操作的认知交互扩散规划框架。DexDiffuser通过一个双相扩散过程来建模关节状态-动作动力学，该过程包括接触前的接触对齐和接触后的目标导向控制，从而实现目标自适应的通用灵活操作。此外，我们纳入了基于动力学模型的二元指导和利用大型语言模型来自动生成引导函数，增强了物理交互的泛化性，并通过语言提示促进多样化的目标适应。在诸如开门、笔和块重新定位、锤子钉钉等物理交互任务上的实验表明，与现有方法相比，DexDiffuser在训练分布之外的目标上表现出色，成功率达到平均的两倍（59.2%对29.5%）。我们的框架在30度开门任务上取得了70.0%的成功率，在笔和块半侧重新定位任务上分别取得了40.0%和36.7%的成功率，在锤子钉钉半驱动任务上取得了46.7%的成功率，突显了其在丰富接触交互中的鲁棒性和灵活性。|
|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前语言模型（LMs）通常使用一个固定、静态的子词分词器。这个选择往往被理所当然地接受，但通常会导致在英语以外的语言中的效率和能力下降，并使得将LMs应用于新的领域或语言变得具有挑战性。为了解决这些问题，我们提出对LMs进行动态分词改造：一种基于输入文本动态决定分词边界的方法。对于编码器风格模型，我们引入了一种受字节对编码（BPE）启发的子词合并算法，但在批处理级别上。我们合并批处理中的频繁子词序列，然后应用预训练的嵌入预测超网络来即时计算分词嵌入。当与词级边界结合使用时，这在XNLI上的XLM-R中平均减少了20%以上的分词序列长度，同时任务性能下降了不到2%。对于解码器风格模型，我们以两种方式应用动态分词：1）用于预填充，几乎完全维持Mistral-7B的性能，同时序列长度减少了40%以上——相对于词级；2）通过近似最近邻索引，实现快速生成，并使用一个包含一百万个分词的词汇表，展示了扩展到甚至更大、动态词汇表的可扩展性。总体而言，我们的发现表明动态分词显著提高了推理速度并促进了语言的公平性，向克服静态分词的局限性迈进了一大步，并使LMs更加公平和适应性强。|
|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种数学框架，用于在人工智能（AI）系统中定义和量化自我认同，填补了人工意识理论基础的Critical gap。尽管现有的人工自我意识方法通常依赖于启发式实现或哲学抽象，但本文提出了一个基于度量空间理论、测度理论和泛函分析的正式框架。我们的框架认为，自我认同来源于两个可量化的数学条件：在度量空间 $(\mathcal{M}, d_{\mathcal{M}})$中存在一个连通的连续记忆集合$C \subseteq \mathcal{M}$，以及一个连续映射$I: \mathcal{M} \to \mathcal{S}$，该映射在这段连续记忆中保持一致的自我识别，其中$(\mathcal{S}, d_{\mathcal{S}})$ 代表可能的自我认同的度量空间。为了验证这一理论框架，我们使用Llama 3.2 1B模型进行了实证实验，并采用低秩适应（LoRA）进行高效的微调。该模型在包含时序结构记忆的合成数据集上进行了训练，旨在捕捉连贯自我认同形成的复杂性。我们的评估指标包括自我意识、响应一致性和语言精确度的量化措施。实验结果表明，可测量的自我意识指标有显著提高，主要自我意识分数从0.276增加到0.801。这有助于结构化创建具有验证的自我认同特征的AI系统。我们的研究结果对人形机器人和自主系统领域具有直接的相关性。**|
|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|在之前的论文中已经证明了大型语言模型（LLMs）在时间序列上的成功应用。通过使用符号时间序列表示，可以有效地在LLMs和时间序列之间架起桥梁。然而，剩余的挑战是利用时间序列中隐藏的语义信息，通过使用符号或LLMs现有的标记，同时根据时间序列的隐藏信息对LLMs的嵌入空间进行对齐。符号时间序列近似（STSA）方法，即基于自适应布朗桥的符号聚合（ABBA）方法，通过使用LLMs的现有标记，在建模时间序列模式时考虑振幅和周期，显示出在保留显著时间序列特征方面的卓越效果。在本文中，我们介绍了一种方法，称为LLM-ABBA，该方法将ABBA集成到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中与最新的最先进（SOTA）方法相比具有优势。同时，在ABBA中引入了一个固定多边形链技巧，以通过显著减轻在从符号到数值的转换过程中由于误用符号而产生的累积误差的影响，避免在预测任务中的明显漂移。在时间序列回归任务中，LLM-ABBA在时间序列外推回归（TSER）基准测试中实现了新的SOTA。与最近的SOTA时间序列预测结果相比，LLM-ABBA也表现出竞争力的预测能力。我们相信这个框架也可以无缝地扩展到其他时间序列任务。|
|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交织的图像-文本内容仍然是一个挑战，这需要集成的多模态理解和生成能力。虽然统一模型的进展提供了新的解决方案，但现有基准由于数据量和多样性限制，不足以评估这些方法。为了填补这一差距，我们引入了GATE OpenING（OpenING），这是一个包含5,400个高质量人工标注实例的综合基准，涵盖56个现实任务。OpenING覆盖了多样化的日常生活场景，如旅行指南、设计和头脑风暴，为挑战交织生成方法提供了一个稳健的平台。此外，我们提出了IntJudge，这是一个用于评估开放式多模态生成方法的裁判模型。通过一个新颖的数据管道进行训练，我们的IntJudge在与人工判断的一致性率达到82.42%，超过了基于GPT的评估器11.34%。在OpenING上的大量实验表明，当前的交织生成方法仍有很大的改进空间。关于交织图像-文本生成的关键发现进一步提出，以指导下一代模型的发展。OpenING已在https://opening.github.io上开源。|
|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|在上下文学习（ICL）中，通过复杂的提示和高品质的演示，使得大型语言模型（LLMs）能够处理下游任务。然而，当面对复杂的数学推理任务时，这种传统的ICL范式显示出局限性，这主要是因为它高度依赖于示例质量，以及在挑战性场景中需要人类干预。为了解决这些局限性，本文提出了HiAR-ICL，这是一种在ICL中的高阶自动化推理范式，它将重点从具体示例转移到抽象思维模式上，扩展了ICL中传统的上下文概念。HiAR-ICL引入了五个原子推理动作作为构建链式模式的根本组成部分。通过蒙特卡洛树搜索，我们探索推理路径并构建思想卡片以指导后续推理。然后，我们开发了一个认知复杂度框架，该框架可以动态地将问题与适当的思想卡片相匹配。实验结果证明了HiAR-ICL的有效性，在MATH基准测试中，使用Qwen2.5-7B-Instruct达到了最先进的准确率（79.6%），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

## infer

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks**|Xinyao Zheng et.al.|[2411.18191](http://arxiv.org/abs/2411.18191)|null|大型语言模型（LLMs）拥有广泛的知识和问答能力，在隐私敏感领域如金融和医疗咨询中得到广泛应用。在LLMs推理过程中，缓存共享方法通常被采用以提高效率，通过重用缓存状态或响应来处理相同或类似的推理请求。然而，我们发现这些缓存机制存在隐私输入泄露的风险，因为缓存可能导致响应时间的可观察变化，使其成为基于时间攻击的强候选者。在本研究中，我们提出了一种基于时间的侧信道攻击方法，以在LLMs推理中执行输入窃取。基于缓存的攻击面临在大型搜索空间中构建候选输入以击中和窃取缓存用户查询的挑战。为了应对这些挑战，我们提出了两个主要组件。输入构造器采用机器学习技术和基于LLM的方法进行词汇相关性学习，同时实施优化的搜索机制以构建通用输入。时间分析器实现统计时间拟合和异常值消除，以识别缓存命中模式，并持续提供反馈以优化构造器的搜索策略。我们在两种缓存机制上进行了实验，结果表明，我们的方法在各种应用中均能持续获得高攻击成功率。我们的工作突出了与性能优化相关的安全漏洞，强调了在LLMs推理的增强过程中优先考虑隐私和安全的必要性。|
|**2024-11-27**|**Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache**|Akshat Sharma et.al.|[2411.18077](http://arxiv.org/abs/2411.18077)|null|在实践中学效地服务大型语言模型（LLM）变得异常具有挑战性，这是因为它们对内存和计算能力的需求极高。在本研究中，我们调查了优化KV缓存，其在LLM推理中内存占用是一个关键的瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了MiniKV，这是一种KV缓存优化方法，通过一种新颖的2位层判别性KV缓存，在同时保持长上下文任务准确性的同时，显著减小了KV缓存的大小。更重要的是，我们开发了专门的CUDA内核，使MiniKV与FlashAttention兼容。在广泛的长期上下文任务上的实验表明，MiniKV有效地实现了86%的KV缓存压缩比，同时恢复了超过98.5%的准确性，在超越现有方法的同时，实现了卓越的系统性能提升。|
|**2024-11-24**|**Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments**|Nikoleta Iliakopoulou et.al.|[2411.17741](http://arxiv.org/abs/2411.17741)|null|随着大型语言模型（LLM）的广泛应用，其部署呈指数级增长，对推理集群提出了巨大的需求。这些集群必须处理针对不同LLM下游任务的众多并发查询。为了处理具有大量LLM参数的多任务设置，方法如低秩自适应（LoRA）可以在共享大部分基础LLM模型的同时进行任务特定的微调。因此，它们允许以最小的内存需求进行并发任务服务。然而，现有的LLM服务系统存在效率低下的问题：它们忽略了工作负载的异质性，频繁加载适配器导致高链路带宽，以及调度器中的头阻塞问题。为了解决这些挑战，我们提出了Chameleon，一个针对多个适配器环境优化的新型LLM服务系统，它依赖于两个核心思想：适配器缓存和适配器感知调度。首先，Chameleon在GPU内存中缓存流行的适配器，最小化适配器加载时间。重要的是，它使用原本闲置的GPU内存，避免了额外的内存成本。其次，Chameleon使用非抢占式多队列调度，以有效地处理工作负载的异质性。通过这种方式，Chameleon同时防止了头阻塞和饥饿。我们基于最先进的LLM服务平台实现了Chameleon，并使用真实世界的生产跟踪和开源LLM进行了评估。在高负载下，Chameleon将P99和P50的TTFT延迟分别降低了80.7%和48.1%，同时与最先进的基线相比，提高了1.5倍的吞吐量。|
|**2024-11-26**|**PIM-AI: A Novel Architecture for High-Efficiency LLM Inference**|Cristobal Ortega et.al.|[2411.17309](http://arxiv.org/abs/2411.17309)|null|大型语言模型（LLMs）因其先进的语言理解和生成能力，在各种应用中变得至关重要。然而，它们的计算和内存需求对传统硬件架构构成了重大挑战。内存中计算（PIM）将计算单元直接集成到内存芯片中，为LLM推理提供了多个优势，包括减少数据传输瓶颈和提高能效。本文介绍了一种名为PIM-AI的新型DDR5/LPDDR5 PIM架构，专为LLM推理设计，无需修改内存控制器或DDR/LPDDR内存物理层。我们开发了一个模拟器来评估PIM-AI在各种场景下的性能，并展示了其在传统架构上的显著优势。在基于云的场景中，PIM-AI将每秒查询的三年TCO降低最多6.94倍，具体取决于所使用的LLM模型。在移动场景中，与最先进的移动SoC相比，PIM-AI将每令牌的能耗降低了10到20倍，从而实现每秒查询增加25到45%，每查询能耗减少6.9倍到13.4倍，延长了电池寿命，并允许每次充电进行更多推理。这些结果突显了PIM-AI在LLM部署中的革命性潜力，使它们更高效、可扩展和可持续。|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂度，使用基于Transformer的大型语言模型（LLMs）在长序列上进行推理既昂贵又缓慢。我们引入了星形注意力，这是一种两阶段块稀疏逼近方法，通过在多个主机之间分片注意力来提高计算效率，同时最大限度地减少通信开销。在第一阶段，通过并行地在主机之间使用块局部注意力处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。星形注意力与大多数使用全局注意力训练的基于Transformer的LLMs无缝集成，通过减少内存需求和提高推理时间（最多11倍）的同时，保持95-100%的准确度。**|
|**2024-11-26**|**Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation**|Chaoyi Jiang et.al.|[2411.17089](http://arxiv.org/abs/2411.17089)|null|对于大型语言模型（LLMs）的推理计算量很大。为了降低自回归解码的成本，使用键值（KV）缓存存储中间激活值，使得GPU只需要执行每个新标记所需的增量计算。这种方法显著降低了标记生成的计算开销。然而，KV缓存所需的内存会迅速增长，通常超过GPU内存的容量。一种成本效益更高的替代方案是将KV缓存卸载到CPU内存中，这可以缓解GPU内存压力，但将瓶颈转移到了CPU和GPU之间有限的PCIe连接带宽。现有方法试图通过重叠GPU计算与I/O操作或采用CPU-GPU异构执行来解决这个问题，但它们受到数据传输过多和对CPU能力的依赖的限制。在本文中，我们介绍了一种高效的CPU-GPU I/O感知LLM推理方法，该方法通过在同时通过PCIe总线传输剩余的KV缓存的同时，重新计算部分KV缓存从激活值，从而避免了将整个KV缓存从CPU转移到GPU。这种方法将GPU重新计算与数据传输重叠，以最小化GPU空闲时间并最大化推理性能。我们的方法通过集成一个利用输入特性和系统硬件信息的分析模块、一个用于优化计算和通信工作负载分配的调度模块以及一个用于高效执行推导出的执行计划的运行时模块，实现了完全自动化。实验结果表明，与最先进的方法相比，我们的方法在解码过程中实现了高达35.8%的更低延迟和46.2%的更高吞吐量。|
|**2024-11-25**|**MixPE: Quantization and Hardware Co-design for Efficient LLM Inference**|Yu Zhang et.al.|[2411.16158](http://arxiv.org/abs/2411.16158)|null|基于Transformer的大型语言模型（LLMs）在模型规模不断增大的同时取得了显著的成功，但它们的部署仍然具有挑战性，这是因为它们对计算和内存的巨大需求。量化技术已出现并成为一项有希望的解决方案，对于LLMs的先进量化算法引入了对混合精度矩阵乘法（mpGEMM）的需求，即使用低精度权重与高精度激活进行乘法运算。尽管这种方法有其优势，但当前的硬件加速器，如GPU和TPU，缺乏对高效mpGEMM的原生支持，导致在主顺序循环中的去量化操作效率低下。为了解决这一限制，我们引入了MixPE，这是一种专门用于LLMs推理中高效低比特量化的混合精度处理单元。MixPE利用两个关键创新来最小化去量化开销并释放低比特量化的全部潜力。首先，认识到缩放因子和零点在每个量化组内是共享的，我们提出在每个组mpGEMM之后执行去量化，从而显著减少去量化开销。其次，MixPE不是依赖于传统的乘法器，而是使用高效的移位和加法操作进行乘法运算，优化了计算和能效。我们的实验结果表明，MixPE比最先进的量化加速器快2.6倍，并降低了1.4倍的能耗。|
|**2024-11-24**|**eFedLLM: Efficient LLM Inference Based on Federated Learning**|Shengwen Ding et.al.|[2411.16003](http://arxiv.org/abs/2411.16003)|null|大型语言模型（LLMs）标志着人工智能（AI）领域的一个变革时代。然而，LLMs所需要的大量数据和参数规模，对计算和内存资源提出了高要求，这限制了它们被更广泛的用户和研究人员的访问。本文介绍了一种有效的方法，该方法提高了LLMs推理的操作效率和成本效益。通过利用基于transformer的联邦学习（FL）和模型并行分布式训练，我们的模型能够高效地在参与者网络中分配计算负载和内存需求。这种策略允许用户，尤其是资源有限的用户，共同训练最先进的LLMs。我们还创新性地在FL框架内引入了激励机制，奖励建设性的贡献并过滤掉恶意活动，从而保护训练过程的完整性和可靠性。同时，我们利用内存层次策略和权重矩阵上的奇异值分解（SVD）进一步提升了计算和内存效率。我们的结果，基于公式分析和数值计算，展示了资源使用的显著优化，并使最前沿的LLMs的访问民主化，确保广泛的用户既能贡献也能从这些先进模型中受益。|
|**2024-11-24**|**Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format**|Chao Fang et.al.|[2411.15982](http://arxiv.org/abs/2411.15982)|null|广泛使用的仅权重量化的大型语言模型（LLMs），利用低比特整数（INT）权重并保留浮点（FP）激活，在降低存储需求的同时保持准确性。然而，这导致能量和延迟瓶颈转向与昂贵的内存访问和计算相关的FP激活。现有的LLM加速器主要关注计算优化，忽略了联合优化FP计算和数据移动的潜力，特别是在LLM推理中占主导地位的FP-INT GeMM操作。为了解决这些挑战，我们研究了激活精度在各个LLM模块中的敏感性及其对整体模型准确性的影响。基于我们的发现，我们首先提出了Anda数据类型：一种具有组共享指数位和动态尾数位分配的自适应数据格式。其次，我们开发了一种迭代后训练自适应精度搜索算法，优化不同LLM模块的位宽，以平衡模型准确性、能量效率和推理速度。最后，提出了一套硬件优化技术，以最大限度地发挥Anda格式的优势。这些技术包括基于位平面的数据组织方案、Anda增强的处理单元（具有位串计算）和运行时位平面Anda压缩器，以同时优化存储、计算和内存占用。我们对于FPINT GeMM操作的评估显示，对于包括OPT、LLaMA和LLaMA-2系列在内的流行LLMs，与GPU类似的FP-FP基准相比，Anda平均实现了2.4倍的加速、4.0倍的区域效率提升和3.1倍的能量效率提升。Anda在各种应用场景、精度要求和系统性能方面表现出强大的适应性，使LLM推理能够高效地应用于广泛的部署场景。|
|**2024-11-24**|**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**|Wenxiang Lin et.al.|[2411.15715](http://arxiv.org/abs/2411.15715)|null|大型语言模型（LLMs）因其庞大的模型尺寸而对计算资源和内存的需求很高，这导致在中等GPU系统上的推理效率低下。量化或剪枝等技术的应用可以缩小模型尺寸，但通常会损害准确性，使其不适用于实际应用。在这项工作中，我们引入了\modelname{}，这是一个高性能的推理引擎，旨在加快LLMs的推理速度，同时不牺牲模型准确性。\modelname{}包含了三种创新的方法来提高推理效率：1）模型分区，允许异步处理CPU计算、GPU计算和CPU-GPU通信的任务；2）自适应分区算法，以优化CPU、GPU和PCIe通信能力的使用；3）令牌分配策略，以处理LLMs推理期间的多样化和生成任务。在三个测试环境中对各种LLMs（如Mixtral、LLaMA-2、Qwen和PhiMoE）进行了全面的实验，这些测试环境具有不同的CPU和GPU。实验结果表明，\modelname{}在解码速度上达到了 $1.11\times$到$1.80\times$的加速，在预填充速度上达到了$1.69\times$到$6.33\times$的加速，与最先进的解决方案llama.cpp和Fiddler相比，整体速度提高了$1.25\times$到$2.04\times$ 。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

## train

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-11-27**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们发现在应用低比特量化时，大型的语言模型（LLMs）或训练词汇较少的模型经历的量化诱导退化（QiD）较少，而训练词汇较多的小型模型则遭受显著的QiD。为了更深入地了解这一趋势，我们在一个受控环境中研究了1500多个不同大小和不同训练水平（未训练或完全训练）的低比特量化LLM检查点，推导出QiD与训练词汇数量、模型大小和比特宽度等因素之间的缩放定律。通过推导出的缩放定律，我们提出了一种新的观点，即我们可以利用QiD来衡量LLMs的训练水平，并确定各种大小LLMs完全训练所需的训练词汇数量。此外，我们使用缩放定律来预测训练了100万亿个词汇的不同大小LLMs的量化性能。我们的预测表明，未来预期用超过100万亿个词汇进行训练的模型的低比特量化性能可能并不理想。这为未来低比特量化提出了潜在挑战，并突出了在评估低比特量化研究时对模型训练水平的关注需求。为了促进对此问题的未来研究，我们将本工作中使用的1500多个量化检查点发布在https://huggingface.co/Xu-Ouyang。|
|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|**摘要翻译步骤：  1. 识别关键术语和概念：Tokenization techniques, Byte-Pair Encoding (BPE), Byte-Level BPE (BBPE), large language models (LLMs), text segmentation, tokens, internal character structures, sequences, Token Internal Position Awareness (TIPA), reverse character prediction tasks, tokenizer's own vocabulary, character positions, internal structures, Chinese Spelling Correction (CSC), model convergence, task performance。  2. 理解句子结构和逻辑关系：介绍背景技术 -> 提出问题 -> 提出解决方案 -> 实验结果 -> 总结效果。  3. 翻译关键术语和概念：    - Tokenization techniques: 分词技术    - Byte-Pair Encoding (BPE): 字节对编码    - Byte-Level BPE (BBPE): 字节级BPE    - large language models (LLMs): 大型语言模型    - text segmentation: 文本分割    - tokens: 标记    - internal character structures: 内部字符结构    - sequences: 序列    - Token Internal Position Awareness (TIPA): 标记内部位置感知    - reverse character prediction tasks: 反向字符预测任务    - tokenizer's own vocabulary: 分词器自身的词汇表    - character positions: 字符位置    - internal structures: 内部结构    - Chinese Spelling Correction (CSC): 中文拼写纠错    - model convergence: 模型收敛    - task performance: 任务性能  4. 将句子结构和逻辑关系翻译成中文，同时保持术语的准确性：  分词技术如字节对编码（BPE）和字节级BPE（BBPE）通过将文本分割成标记显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割通常掩盖了标记内部的字符结构和序列，阻止模型在训练过程中充分学习这些复杂的细节。因此，LLMs在处理标记内的字符组成和位置关系时遇到困难，尤其是在数据有限的下游任务中微调时。在本文中，我们引入了一种名为标记内部位置感知（TIPA）的新方法，通过使用分词器自身的词汇表训练模型进行反向字符预测任务，从而增强了LLMs对内部标记结构的理解。这种方法使模型能够有效地学习和泛化字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测标记级别的字符位置方面优于基线模型。此外，当应用于中文拼写纠错（CSC）等下游任务时，TIPA不仅加速了模型收敛，还显著提高了任务性能。**|
|**2024-11-26**|**Using Large Language Models for Expert Prior Elicitation in Predictive Modelling**|Alexander Capstick et.al.|[2411.17284](http://arxiv.org/abs/2411.17284)|**[link](https://github.com/alexcapstick/llm-elicited-priors)**|**大型语言模型（LLMs）通过在多样化数据上训练，有效地获取了各个领域的广泛信息。然而，它们的计算复杂性、成本以及缺乏透明度阻碍了它们在特定任务中的直接应用。在临床研究等领域，获取关于预测模型的专业标注或先验知识通常是昂贵且耗时的。本研究提出使用LLMs来获取预测模型的专家先验分布。这种方法也为情境学习提供了另一种选择，在这种情境下，语言模型被要求直接进行预测。我们比较了LLM获取的先验和无关先验，评估LLM是否真实地生成参数分布，并提出了情境学习和先验获取的模型选择策略。我们的研究发现，与无关先验相比，LLM获取的先验参数分布显著降低了低数据环境下的预测误差。应用于临床问题，这意味着需要更少的生物样本，降低了成本和资源。先验获取也始终优于情境学习，且成本更低，因此在我们的环境中成为更受欢迎的选择。我们展示了这种方法在包括临床应用在内的各种用例中的实用性。对于感染预测，使用LLM获取的先验在研究中提前200天，将实现与无关先验相同准确率的所需标签数量减少了55%。**|
|**2024-11-26**|**Star Attention: Efficient LLM Inference over Long Sequences**|Shantanu Acharya et.al.|[2411.17116](http://arxiv.org/abs/2411.17116)|**[link](https://github.com/NVIDIA/Star-Attention)**|**由于自注意力机制的二次复杂性，在长序列上使用基于Transformer的大语言模型（LLMs）进行推理既昂贵又缓慢。我们引入了Star Attention，这是一种两阶段块稀疏近似方法，通过在多个主机之间分片注意力以提高计算效率，同时最小化通信开销。在第一阶段，使用主机间的块局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。Star Attention与大多数使用全局注意力训练的基于Transformer的LLMs无缝集成，通过减少内存需求和推理时间最多11倍，同时保持95-100%的准确率。**|
|**2024-11-25**|**The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C**|Mikita Balesni et.al.|[2411.16353](http://arxiv.org/abs/2411.16353)|null|尽管在采用思维链（CoT）进行多跳问题（例如：“Imagine这首歌的表演者的配偶是谁？”）推理时，大型语言模型（LLMs）表现出色，但当被迫内部推理（不使用CoT）时，它们会遇到困难。先前关于这个差距的大小和性质的研究产生了不一致的证据，结果并不明确。在本文中，我们引入了一个受控环境来研究LLMs中的两跳推理，其中高于随机水平的性能构成对潜在推理不可否认的证据。我们对LLMs（包括Llama 3 8B Instruct和GPT-4o）在虚构事实上的进行了微调，并证实它们能够通过使用CoT来泛化回答关于它们的两跳问题。我们发现，当事实在训练过程中或在提示中同时出现时，模型可以进行潜在推理。然而，出人意料的是，当学习到的事实仅出现在不同的文档中时，模型在没有CoT的情况下完全无法进行两跳推理，达到了随机水平准确性和测试损失。我们将这种完全无法组合分别学习到的事实称为两跳诅咒。此外，我们对9个前沿LLMs在现实世界事实上的表现进行了评估，发现模型在超过一半的问题类别中完全无法进行没有CoT的两跳推理，而在大多数类别中通过CoT保持部分成功。这些结果表明，LLMs缺乏一种独立于问题类型的一般能力来进行潜在的、多跳推理。|
|**2024-11-24**|**Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution**|Haiquan Wang et.al.|[2411.15871](http://arxiv.org/abs/2411.15871)|null|随着大型语言模型（LLMs）的发展，需要大规模的分布式训练。然而，高度优化的框架由于通信量巨大，在模型FLOPS利用率上仍然存在显著损失（通常低于50%）。同时，我们的全面分析表明，计算和通信密集型操作重叠得很好。本文介绍了一种名为DHelix的新型微观结构，该结构受DNA结构启发，显著提高了LLMs训练的效率。DHelix设计中的核心是链式交错（SI），它将训练微批次的连续流视为两个链。DHelix将两个链的前向和后向传递并置，并针对一个SI计划进行系统优化，该计划通过操作级重叠分析结果和基于动态规划搜索算法协同调度来自不同链的操作。同时，DHelix使两个链能够共享模型状态和激活数据的空间，有效地容纳两个小于3%额外内存空间的微批次。DHelix无缝集成到现有所有形式的数据/模型并行性中，其中最具有挑战性的是流水线并行性，这得益于其独特的模型折叠设计，产生了W形流水线。我们使用流行的Llama和GPT密集模型以及Phi混合专家（MoE）模型，在3个GPU集群（A40、A800和H100）上评估了DHelix训练。结果表明，它在64-A40和64-A800集群上分别实现了12-40%（高达58%MFU）和2-29%（高达71%MFU）的改进，显著优于现有方法。在H100集群上，虽然更快网络降低了DHelix的利润空间，但它使跨节点张量并行性变得可行，这在由于通信成本高昂而目前无法实现的实践中具有潜力。|
|**2024-11-23**|**Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai**|Parinthapat Pengpun et.al.|[2411.15484](http://arxiv.org/abs/2411.15484)|**[link](https://github.com/parinzee/seed-free-synthetic-instruct)**|**我们提出了一种针对低资源语言（特别是泰语）的大语言模型（LLM）指令微调的合成数据方法，以数据高效的方式实现。我们确定了三个对指令微调数据集有效性的关键属性：流畅性、多样性和文化背景。我们提出了一种无需种子数据框架，用于生成具有这些基本属性的合成指令微调数据。我们的框架利用LLM生成多样化的主题，从维基百科检索相关背景，并为各种任务（如问答、摘要和对话）创建指令。实验结果表明，我们的性能最佳的合成数据集，结合了这三个关键属性，在仅使用5000条指令的情况下，与在数十万条指令上训练的顶尖泰语LLM相比，取得了具有竞争力的表现。我们的代码和数据集可在https://github.com/parinzee/seed-free-synthetic-instruct公开获取。**|
|**2024-11-21**|**Exploring Accuracy-Fairness Trade-off in Large Language Models**|Qingquan Zhang et.al.|[2411.14500](http://arxiv.org/abs/2411.14500)|null|大型语言模型（LLMs）在人工智能领域取得了显著进展，展示了它们与人类互动以及通过信息传播影响人类认知的能力。然而，近期的研究揭示了LLMs中固有的偏见实例，提出了一个需要关注的关键问题。在我们的研究中，我们深入探讨了在增强LLMs过程中实现准确性和公平性之间和谐统一的复杂挑战。尽管提高准确性确实可以提升LLMs的整体性能，但通常是以牺牲公平性为代价的。过分强调一个指标的优化必然会导致另一个指标的重大退化。这强调了在设计优化LLMs阶段时需要考虑多个因素的重要性。因此，我们主张将LLMs的训练过程重新构造成一个多目标学习任务。我们的调查发现，多目标进化学习（MOEL）方法为应对这一挑战提供了有希望的途径。我们的MOEL框架能够同时优化准确性和公平性指标，从而产生一组Pareto最优的LLMs。总的来说，我们的研究为LLMs中准确性和公平性之间的微妙平衡提供了宝贵的见解，这对于它们在现实世界中的应用越来越重要。通过利用MOEL，我们展示了走向更加公平和有效的AI技术的有希望的道路。|
|**2024-11-20**|**Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics**|Tetiana Bas et.al.|[2411.13738](http://arxiv.org/abs/2411.13738)|**[link](https://github.com/tetianabas/llm_biases)**|**这项研究通过比较大型语言模型（LLMs）的性别感知与人类受访者、美国劳工统计局数据和50%无偏见基准，来调查LLMs中的性别偏见。我们使用职业数据和特定角色的句子创建了一个新的评估集。与LLMs训练数据中常见的基准不同，我们的集是新开发的，防止数据泄露和测试集污染。我们测试了五个LLMs，使用单词回答来预测每个角色的性别。我们使用Kullback-Leibler（KL）距离来比较模型输出与人类感知、统计数据和50%中立性基准。所有LLMs都显示出与性别中立性有显著偏差，并且更多地与统计数据一致，仍然反映了固有的偏见。**|
|**2024-11-20**|**Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training**|Jared Fernandez et.al.|[2411.13055](http://arxiv.org/abs/2411.13055)|null|近年来，神经网络模型能力的显著提升得益于模型规模、训练数据和相应计算资源的扩展。为了开发现代应用（如大型语言模型）所需的超大型网络，模型训练需要在数以万计的硬件加速器（例如GPU）上分布式进行，这要求在大规模计算集群中进行计算和通信的编排。在这项工作中，我们证明了仔细考虑硬件配置和并行化策略对于有效（即计算和成本高效）地扩展模型规模、训练数据和总计算量至关重要。我们对大规模LLM训练工作负载在不同模型规模、硬件配置和分布式并行化策略下的性能进行了广泛的实证研究。我们证明了：1）超过一定规模后，某些分布式通信策略带来的开销使得之前认为次优的并行化策略实际上变得更可取；2）在硬件和并行化策略得到适当优化的情况下，大规模模型训练扩展加速器总数很快就会产生边际效益递减的现象，这意味着每增加一个计算单元或GPU小时所带来的性能提升较差。|

<p align=right>(<a href=#updated-on-20241202>back to top</a>)</p>

